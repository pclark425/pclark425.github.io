<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5249 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5249</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5249</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-111.html">extraction-schema-111</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-265842153</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2311.00444v1.pdf" target="_blank">Form follows Function: Text-to-Text Conditional Graph Generation based on Functional Requirements</a></p>
                <p><strong>Paper Abstract:</strong> This work focuses on the novel problem setting of generating graphs conditioned on a description of the graph's functional requirements in a downstream task. We pose the problem as a text-to-text generation problem and focus on the approach of fine-tuning a pretrained large language model (LLM) to generate graphs. We propose an inductive bias which incorporates information about the structure of the graph into the LLM's generation process by incorporating message passing layers into an LLM's architecture. To evaluate our proposed method, we design a novel set of experiments using publicly available and widely studied molecule and knowledge graph data sets. Results suggest our proposed approach generates graphs which more closely meet the requested functional requirements, outperforming baselines developed on similar tasks by a statistically significant margin.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5249.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5249.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SGG-LLM serialization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Serialized Graph Generator - Large Language Model serialization (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An injective, reversible serialization method that linearizes a text-graph as a bag-of-edges string with special delimiters and a node-disambiguation token, designed for fine-tuning autoregressive LLMs to generate graphs conditioned on text; combined with interleaved message-passing layers inside the LLM to introduce structural bias.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>bag-of-edges serialized graph with special tokens and node-disambiguation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each edge is serialized as a delimited string: <PN> predecessor-node-string <E> edge-string <SN> successor-node-string, and edges are listed in a predefined order (bag-of-edges). Nodes that share identical textual features are disambiguated by appending a special token <D> and a unique integer (e.g., C<D>0). Additional special tokens (<PN>, <E>, <SN>, <D>) are added to the tokenizer/embeddings. Serialization function g(·) is injective (reversible) by construction; the deserialization g^{-1}(·) recovers the graph. For integration with message-passing (MP) layers, the model selects the last token embedding describing a node/edge (because of causal attention) and constructs auxiliary graphs (edge-graph or correspondence graph) to run MP without leaking future information.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>text graphs (molecular graphs, knowledge graphs / triples), general directed labeled graphs where node/edge features are text strings</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Injective/reversible serialization (expressive enough to represent any graph), explicit token delimiters for node/edge boundaries, node-disambiguation to handle repeated identical node labels, compatible with causal autoregressive generation, designed to permit insertion of structural signals via interleaved MP layers; allows multimodal outputs (non-injective mapping from function to many graphs) and preserves information needed for deterministic deserialization.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Conditional graph generation from functional textual descriptions: (1) molecule generation conditioned on functional requirements (number of valency electrons and QED) constructed from PCQM4M (Valency and QED datasets); (2) knowledge graph generation from imperative text (WebNLG+ 2020 benchmark).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Parsability (fraction of generated samples that parse/deserialise without error), Mean Absolute Error (MAE) between generated graph functional property and requested value (for QED and Valency), Diversity (binary measure if two samples and ground-truth differ in node+edge sets), and for WebNLG+ standard KG metrics (precision/recall/F1). Reported numeric results (mean ± std): MAE on 100k train set: SGG-LLM (no MP) QED 0.044 ± 0.011, Valency 0.060 ± 0.018; SGG-LLM (edge-based MP) QED 0.036 ± 0.005, Valency 0.035 ± 0.014; SGG-LLM (correspondence MP) QED 0.039 ± 0.007, Valency 0.045 ± 0.017. Parsability near 1.0 for all SGG-LLM variants on molecule tasks (e.g., SGG-LLM none: QED 1.000 ± 0.000, Valency 0.999 ± 0.001). On WebNLG+ SGG-LLM without MP achieved state-of-the-art triple-generation metrics comparable to or better than regen/grapher (F1/precision/recall reported in paper; exact numbers in Appendix E).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compared against prior bag-of-edges baselines (regen and grapher) adapted to molecules (with node-disambiguation): SGG-LLM variants (especially those with message passing) significantly outperform regen and grapher on the molecule functional-generation MAE metric (e.g., regen MAE QED 0.149 ± 0.018 & Valency 2.282 ± 1.156; grapher MAE QED 0.157 ± 0.004 & Valency 1.268 ± 0.229 for 100k training), and produce higher diversity on Valency. Parsability of baselines is also high (~1.0) but functional accuracy (MAE) is much worse for baselines. Message-passing variants of SGG-LLM outperform the no-MP SGG-LLM; SGG-LLM outperforms regen likely due to both the modified training objective and the single joint model design; SGG-LLM also outperformed grapher (which uses a dual-module approach). On WebNLG+ imperative KG generation, adding MP did not improve and did not degrade performance; SGG-LLM (no MP) achieved competitive SOTA.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires fine-tuning large autoregressive LLMs (computationally expensive and slow at generation), scaling issues for larger graphs or long feature strings, necessitates gating when incorporating MP outputs back into token embeddings (a learned tanh gating factor was required otherwise models failed to fine-tune), need to ensure message passing never aggregates from future tokens (causality constraint), special handling needed for nodes appearing multiple times (edge-graph and correspondence-graph constructions), and the method focuses on interpolation (functional descriptions within training range) not extrapolation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Form follows Function: Text-to-Text Conditional Graph Generation based on Functional Requirements', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5249.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5249.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>bag-of-edges serialization (prior)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bag-of-edges serialized graph representation (edge-list linearization used in prior LLM graph generation work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A linearization that represents a graph as a sequence of edge-descriptions (edge tuples) encoded as delimited text strings; used by prior LLM-based knowledge-graph and explanation-graph generators and adapted here as baseline serialization for molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>bag-of-edges serialization / edge-list linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each edge is written as a textual record combining predecessor node string, edge string, and successor node string, separated by special tokens (e.g., <PN>, <E>, <SN>) and then concatenated into a sequence listing edges in some predefined order (order may be arbitrary or domain-specific like DFS for SMILES); used directly as conditioning/target text for an autoregressive LLM fine-tuned to output the serialized graph.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>knowledge graphs (triples), explanation graphs, adapted here to molecular graphs (with modifications such as node disambiguation)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Simple and widely-used; interpretable and straightforward to tokenize; injectivity depends on ordering and may require extra tokens to be reversible; prior methods did not handle multiple nodes with identical text labels (a limitation highlighted by this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Used in prior knowledge-graph generation benchmarks (e.g., WebNLG+ 2020) and here as implemented baselines (regen, grapher) for molecule generation conditioned on functional text (Valency and QED datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>When implemented as baselines in this paper and evaluated on molecule tasks: parsability (≈1.0 for baselines), MAE: regen (100k) QED 0.149 ± 0.018, Valency 2.282 ± 1.156; grapher (100k) QED 0.157 ± 0.004, Valency 1.268 ± 0.229. Diversity reported lower than some SGG-LLM variants on Valency.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>SGG-LLM (their serialization with MP and training changes) produced significantly better functional accuracy (lower MAE) and higher diversity than bag-of-edges baselines (regen, grapher) on molecule functional generation; for imperative KG generation (WebNLG+), bag-of-edges approaches (e.g., regen variants) are strong baselines, but SGG-LLM (no MP) achieved state-of-the-art performance according to the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Does not natively handle identical node-labels / repeated node feature strings (requires disambiguation), ordering can leak future information unless causal masking and careful architecture are used, and prior bag-of-edges methods often trained with standard per-token next-token objective which the authors show can hurt performance unless corrected (they propose an alternative loss weighting).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Form follows Function: Text-to-Text Conditional Graph Generation based on Functional Requirements', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5249.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5249.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SMILES</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SMILES (Simplified Molecular Input Line Entry System) linearization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A widely used molecule-specific textual linearization that produces a string encoding molecular structure by traversing bonds (commonly via depth-first search) and emitting atom and bond tokens; cited here as an example of domain-specific serialization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>SMILES depth-first traversal linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>SMILES orders atoms/bonds using a traversal (often depth-first) and emits a linear string where atoms and bonds are represented by characters/symbols; canonicalization or specific ordering can be applied to make mapping one-to-one. The paper references SMILES as a domain example of a serialization (depth-first ordering of edges/bonds).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>molecular graphs (chemical structures)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Compact and domain-optimized for chemistry; widely supported and reversible with canonical forms, but domain-specific (not general for arbitrary text-labeled graphs) and can conflate different graph embeddings unless canonicalization is applied.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Mentioned in context of prior serialized molecule representations; not directly used in experiments in this paper (authors instead adapt bag-of-edges style serialization with disambiguation for molecules).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>SMILES is a common molecule linearization but the authors argue prior LLM serialized-graph work (bag-of-edges) used similar ideas; the paper adopts a more general bag-of-edges + disambiguation approach to be reversible for arbitrary text-labeled graphs rather than SMILES-specific encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Domain-specific to chemistry and may not generalize to graphs whose node/edge features are arbitrary text; standard SMILES requires canonicalization for bijectivity and may not be directly compatible with the autoregressive LLM training considerations discussed (causality/MP interleaving) without adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Form follows Function: Text-to-Text Conditional Graph Generation based on Functional Requirements', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5249.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5249.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>edge-graph / correspondence-graph deserialization for MP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Edge-graph and correspondence-graph constructions for message-passing inside an LLM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two auxiliary graph constructions used to map the serialized token sequence into node/edge feature matrices for message-passing layers embedded inside the LLM while preserving causal (autoregressive) constraints: the edge-graph (treat edges as nodes) and the correspondence-graph (treat each node occurrence in the serialization as its own node).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>edge-graph and correspondence-graph derived from serialized sequence</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Edge-graph: treat each original graph edge as a node in a new graph (V_edge = E) and connect two edge-nodes if they share an original node, with directed edges constrained so indices j->k only if k>j (to avoid passing information backward in sequence). Correspondence-graph: treat each occurrence of a node in the serialized sequence as a separate node and connect occurrences that refer to the same original node if they are adjacent in the serialization and point from earlier to later occurrences. For both, the feature vector for a node/edge is taken from the embedding of the last token that described that element in the serialized sequence; these dense vectors are fed to MP layers (GraphSAGE in experiments). MP outputs are then gated (elementwise multiply by tanh(a)) and added back to the following token embedding in the sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>applies to any serialized text graphs; used for molecular graphs in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Enables message-passing structural information to be computed without violating autoregressive causality (information only aggregated from earlier sequence elements), maintains reversibility because operations are performed with known mappings from sequence indices to graph elements, uses gating to stabilize fine-tuning and gradual incorporation of structural signals.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Used within SGG-LLM variants to generate molecules conditioned on functional properties (QED, Valency) and for WebNLG+ knowledge graph generation as an ablation/variant.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>When used (edge-based MP), SGG-LLM edges achieved MAE (100k train) QED 0.036 ± 0.005 and Valency 0.035 ± 0.014, outperforming no-MP variants on MAE and diversity; correspondence-based MP also improved over no-MP but edge-MP was best in some settings. Parsability with MP was near 1.0.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Variants that interleave MP using these constructions outperform the no-MP SGG-LLM in functional accuracy (MAE) and diversity on molecule tasks; the paper notes MP did not help for the imperative knowledge-graph task (WebNLG+), indicating task dependence of the benefit. Also, MP requires gating and careful causal-constrained graph construction—naive insertion of MP without gating caused models to fail to produce any parsable molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Must enforce index ordering constraints (edges only point forward in the serialization) to avoid backward information flow; mapping from serialized tokens to MP node/edge indices is non-trivial (requires f_index mapping); requires learned gating (tanh a) to stabilize fine-tuning — without gating models with MP failed to generate parsable outputs; increases model complexity and computational cost.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Form follows Function: Text-to-Text Conditional Graph Generation based on Functional Requirements', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Regen: Reinforcement learning for text and knowledge base generation using pretrained language models. <em>(Rating: 2)</em></li>
                <li>Explanation graph generation via pretrained language models: An empirical study with contrastive learning. <em>(Rating: 2)</em></li>
                <li>Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. <em>(Rating: 2)</em></li>
                <li>WebNLG+ 2020 <em>(Rating: 2)</em></li>
                <li>grapher <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5249",
    "paper_id": "paper-265842153",
    "extraction_schema_id": "extraction-schema-111",
    "extracted_data": [
        {
            "name_short": "SGG-LLM serialization",
            "name_full": "Serialized Graph Generator - Large Language Model serialization (this paper)",
            "brief_description": "An injective, reversible serialization method that linearizes a text-graph as a bag-of-edges string with special delimiters and a node-disambiguation token, designed for fine-tuning autoregressive LLMs to generate graphs conditioned on text; combined with interleaved message-passing layers inside the LLM to introduce structural bias.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "bag-of-edges serialized graph with special tokens and node-disambiguation",
            "representation_description": "Each edge is serialized as a delimited string: &lt;PN&gt; predecessor-node-string &lt;E&gt; edge-string &lt;SN&gt; successor-node-string, and edges are listed in a predefined order (bag-of-edges). Nodes that share identical textual features are disambiguated by appending a special token &lt;D&gt; and a unique integer (e.g., C&lt;D&gt;0). Additional special tokens (&lt;PN&gt;, &lt;E&gt;, &lt;SN&gt;, &lt;D&gt;) are added to the tokenizer/embeddings. Serialization function g(·) is injective (reversible) by construction; the deserialization g^{-1}(·) recovers the graph. For integration with message-passing (MP) layers, the model selects the last token embedding describing a node/edge (because of causal attention) and constructs auxiliary graphs (edge-graph or correspondence graph) to run MP without leaking future information.",
            "graph_type": "text graphs (molecular graphs, knowledge graphs / triples), general directed labeled graphs where node/edge features are text strings",
            "representation_properties": "Injective/reversible serialization (expressive enough to represent any graph), explicit token delimiters for node/edge boundaries, node-disambiguation to handle repeated identical node labels, compatible with causal autoregressive generation, designed to permit insertion of structural signals via interleaved MP layers; allows multimodal outputs (non-injective mapping from function to many graphs) and preserves information needed for deterministic deserialization.",
            "evaluation_task": "Conditional graph generation from functional textual descriptions: (1) molecule generation conditioned on functional requirements (number of valency electrons and QED) constructed from PCQM4M (Valency and QED datasets); (2) knowledge graph generation from imperative text (WebNLG+ 2020 benchmark).",
            "performance_metrics": "Parsability (fraction of generated samples that parse/deserialise without error), Mean Absolute Error (MAE) between generated graph functional property and requested value (for QED and Valency), Diversity (binary measure if two samples and ground-truth differ in node+edge sets), and for WebNLG+ standard KG metrics (precision/recall/F1). Reported numeric results (mean ± std): MAE on 100k train set: SGG-LLM (no MP) QED 0.044 ± 0.011, Valency 0.060 ± 0.018; SGG-LLM (edge-based MP) QED 0.036 ± 0.005, Valency 0.035 ± 0.014; SGG-LLM (correspondence MP) QED 0.039 ± 0.007, Valency 0.045 ± 0.017. Parsability near 1.0 for all SGG-LLM variants on molecule tasks (e.g., SGG-LLM none: QED 1.000 ± 0.000, Valency 0.999 ± 0.001). On WebNLG+ SGG-LLM without MP achieved state-of-the-art triple-generation metrics comparable to or better than regen/grapher (F1/precision/recall reported in paper; exact numbers in Appendix E).",
            "comparison_to_other_representations": "Compared against prior bag-of-edges baselines (regen and grapher) adapted to molecules (with node-disambiguation): SGG-LLM variants (especially those with message passing) significantly outperform regen and grapher on the molecule functional-generation MAE metric (e.g., regen MAE QED 0.149 ± 0.018 & Valency 2.282 ± 1.156; grapher MAE QED 0.157 ± 0.004 & Valency 1.268 ± 0.229 for 100k training), and produce higher diversity on Valency. Parsability of baselines is also high (~1.0) but functional accuracy (MAE) is much worse for baselines. Message-passing variants of SGG-LLM outperform the no-MP SGG-LLM; SGG-LLM outperforms regen likely due to both the modified training objective and the single joint model design; SGG-LLM also outperformed grapher (which uses a dual-module approach). On WebNLG+ imperative KG generation, adding MP did not improve and did not degrade performance; SGG-LLM (no MP) achieved competitive SOTA.",
            "limitations_or_challenges": "Requires fine-tuning large autoregressive LLMs (computationally expensive and slow at generation), scaling issues for larger graphs or long feature strings, necessitates gating when incorporating MP outputs back into token embeddings (a learned tanh gating factor was required otherwise models failed to fine-tune), need to ensure message passing never aggregates from future tokens (causality constraint), special handling needed for nodes appearing multiple times (edge-graph and correspondence-graph constructions), and the method focuses on interpolation (functional descriptions within training range) not extrapolation.",
            "uuid": "e5249.0",
            "source_info": {
                "paper_title": "Form follows Function: Text-to-Text Conditional Graph Generation based on Functional Requirements",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "bag-of-edges serialization (prior)",
            "name_full": "Bag-of-edges serialized graph representation (edge-list linearization used in prior LLM graph generation work)",
            "brief_description": "A linearization that represents a graph as a sequence of edge-descriptions (edge tuples) encoded as delimited text strings; used by prior LLM-based knowledge-graph and explanation-graph generators and adapted here as baseline serialization for molecules.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "bag-of-edges serialization / edge-list linearization",
            "representation_description": "Each edge is written as a textual record combining predecessor node string, edge string, and successor node string, separated by special tokens (e.g., &lt;PN&gt;, &lt;E&gt;, &lt;SN&gt;) and then concatenated into a sequence listing edges in some predefined order (order may be arbitrary or domain-specific like DFS for SMILES); used directly as conditioning/target text for an autoregressive LLM fine-tuned to output the serialized graph.",
            "graph_type": "knowledge graphs (triples), explanation graphs, adapted here to molecular graphs (with modifications such as node disambiguation)",
            "representation_properties": "Simple and widely-used; interpretable and straightforward to tokenize; injectivity depends on ordering and may require extra tokens to be reversible; prior methods did not handle multiple nodes with identical text labels (a limitation highlighted by this paper).",
            "evaluation_task": "Used in prior knowledge-graph generation benchmarks (e.g., WebNLG+ 2020) and here as implemented baselines (regen, grapher) for molecule generation conditioned on functional text (Valency and QED datasets).",
            "performance_metrics": "When implemented as baselines in this paper and evaluated on molecule tasks: parsability (≈1.0 for baselines), MAE: regen (100k) QED 0.149 ± 0.018, Valency 2.282 ± 1.156; grapher (100k) QED 0.157 ± 0.004, Valency 1.268 ± 0.229. Diversity reported lower than some SGG-LLM variants on Valency.",
            "comparison_to_other_representations": "SGG-LLM (their serialization with MP and training changes) produced significantly better functional accuracy (lower MAE) and higher diversity than bag-of-edges baselines (regen, grapher) on molecule functional generation; for imperative KG generation (WebNLG+), bag-of-edges approaches (e.g., regen variants) are strong baselines, but SGG-LLM (no MP) achieved state-of-the-art performance according to the paper.",
            "limitations_or_challenges": "Does not natively handle identical node-labels / repeated node feature strings (requires disambiguation), ordering can leak future information unless causal masking and careful architecture are used, and prior bag-of-edges methods often trained with standard per-token next-token objective which the authors show can hurt performance unless corrected (they propose an alternative loss weighting).",
            "uuid": "e5249.1",
            "source_info": {
                "paper_title": "Form follows Function: Text-to-Text Conditional Graph Generation based on Functional Requirements",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "SMILES",
            "name_full": "SMILES (Simplified Molecular Input Line Entry System) linearization",
            "brief_description": "A widely used molecule-specific textual linearization that produces a string encoding molecular structure by traversing bonds (commonly via depth-first search) and emitting atom and bond tokens; cited here as an example of domain-specific serialization.",
            "citation_title": "Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules.",
            "mention_or_use": "mention",
            "representation_name": "SMILES depth-first traversal linearization",
            "representation_description": "SMILES orders atoms/bonds using a traversal (often depth-first) and emits a linear string where atoms and bonds are represented by characters/symbols; canonicalization or specific ordering can be applied to make mapping one-to-one. The paper references SMILES as a domain example of a serialization (depth-first ordering of edges/bonds).",
            "graph_type": "molecular graphs (chemical structures)",
            "representation_properties": "Compact and domain-optimized for chemistry; widely supported and reversible with canonical forms, but domain-specific (not general for arbitrary text-labeled graphs) and can conflate different graph embeddings unless canonicalization is applied.",
            "evaluation_task": "Mentioned in context of prior serialized molecule representations; not directly used in experiments in this paper (authors instead adapt bag-of-edges style serialization with disambiguation for molecules).",
            "performance_metrics": null,
            "comparison_to_other_representations": "SMILES is a common molecule linearization but the authors argue prior LLM serialized-graph work (bag-of-edges) used similar ideas; the paper adopts a more general bag-of-edges + disambiguation approach to be reversible for arbitrary text-labeled graphs rather than SMILES-specific encodings.",
            "limitations_or_challenges": "Domain-specific to chemistry and may not generalize to graphs whose node/edge features are arbitrary text; standard SMILES requires canonicalization for bijectivity and may not be directly compatible with the autoregressive LLM training considerations discussed (causality/MP interleaving) without adaptation.",
            "uuid": "e5249.2",
            "source_info": {
                "paper_title": "Form follows Function: Text-to-Text Conditional Graph Generation based on Functional Requirements",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "edge-graph / correspondence-graph deserialization for MP",
            "name_full": "Edge-graph and correspondence-graph constructions for message-passing inside an LLM",
            "brief_description": "Two auxiliary graph constructions used to map the serialized token sequence into node/edge feature matrices for message-passing layers embedded inside the LLM while preserving causal (autoregressive) constraints: the edge-graph (treat edges as nodes) and the correspondence-graph (treat each node occurrence in the serialization as its own node).",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "edge-graph and correspondence-graph derived from serialized sequence",
            "representation_description": "Edge-graph: treat each original graph edge as a node in a new graph (V_edge = E) and connect two edge-nodes if they share an original node, with directed edges constrained so indices j-&gt;k only if k&gt;j (to avoid passing information backward in sequence). Correspondence-graph: treat each occurrence of a node in the serialized sequence as a separate node and connect occurrences that refer to the same original node if they are adjacent in the serialization and point from earlier to later occurrences. For both, the feature vector for a node/edge is taken from the embedding of the last token that described that element in the serialized sequence; these dense vectors are fed to MP layers (GraphSAGE in experiments). MP outputs are then gated (elementwise multiply by tanh(a)) and added back to the following token embedding in the sequence.",
            "graph_type": "applies to any serialized text graphs; used for molecular graphs in experiments",
            "representation_properties": "Enables message-passing structural information to be computed without violating autoregressive causality (information only aggregated from earlier sequence elements), maintains reversibility because operations are performed with known mappings from sequence indices to graph elements, uses gating to stabilize fine-tuning and gradual incorporation of structural signals.",
            "evaluation_task": "Used within SGG-LLM variants to generate molecules conditioned on functional properties (QED, Valency) and for WebNLG+ knowledge graph generation as an ablation/variant.",
            "performance_metrics": "When used (edge-based MP), SGG-LLM edges achieved MAE (100k train) QED 0.036 ± 0.005 and Valency 0.035 ± 0.014, outperforming no-MP variants on MAE and diversity; correspondence-based MP also improved over no-MP but edge-MP was best in some settings. Parsability with MP was near 1.0.",
            "comparison_to_other_representations": "Variants that interleave MP using these constructions outperform the no-MP SGG-LLM in functional accuracy (MAE) and diversity on molecule tasks; the paper notes MP did not help for the imperative knowledge-graph task (WebNLG+), indicating task dependence of the benefit. Also, MP requires gating and careful causal-constrained graph construction—naive insertion of MP without gating caused models to fail to produce any parsable molecules.",
            "limitations_or_challenges": "Must enforce index ordering constraints (edges only point forward in the serialization) to avoid backward information flow; mapping from serialized tokens to MP node/edge indices is non-trivial (requires f_index mapping); requires learned gating (tanh a) to stabilize fine-tuning — without gating models with MP failed to generate parsable outputs; increases model complexity and computational cost.",
            "uuid": "e5249.3",
            "source_info": {
                "paper_title": "Form follows Function: Text-to-Text Conditional Graph Generation based on Functional Requirements",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Regen: Reinforcement learning for text and knowledge base generation using pretrained language models.",
            "rating": 2,
            "sanitized_title": "regen_reinforcement_learning_for_text_and_knowledge_base_generation_using_pretrained_language_models"
        },
        {
            "paper_title": "Explanation graph generation via pretrained language models: An empirical study with contrastive learning.",
            "rating": 2,
            "sanitized_title": "explanation_graph_generation_via_pretrained_language_models_an_empirical_study_with_contrastive_learning"
        },
        {
            "paper_title": "Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules.",
            "rating": 2,
            "sanitized_title": "smiles_a_chemical_language_and_information_system_1_introduction_to_methodology_and_encoding_rules"
        },
        {
            "paper_title": "WebNLG+ 2020",
            "rating": 2,
            "sanitized_title": "webnlg_2020"
        },
        {
            "paper_title": "grapher",
            "rating": 2
        }
    ],
    "cost": 0.016142999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>FORM FOLLOWS FUNCTION: TEXT-TO-TEXT CONDI-TIONAL GRAPH GENERATION BASED ON FUNCTIONAL REQUIREMENTS
1 Nov 2023</p>
<p>Peter A Zachares 
OATML Oxford University</p>
<p>Vahan Hovhannisyan 
OATML Oxford University</p>
<p>Alan Mosca 
OATML Oxford University</p>
<p>Yarin Gal yaringal@gmail.com 
OATML Oxford University</p>
<p>FORM FOLLOWS FUNCTION: TEXT-TO-TEXT CONDI-TIONAL GRAPH GENERATION BASED ON FUNCTIONAL REQUIREMENTS
1 Nov 202381DD060E46B763E5E6386F5C9AE7B6E4arXiv:2311.00444v1[cs.LG]
This work focuses on the novel problem setting of generating graphs conditioned on a description of the graph's functional requirements in a downstream task.We pose the problem as a text-to-text generation problem and focus on the approach of finetuning a pretrained large language model (LLM) to generate graphs.We propose an inductive bias which incorporates information about the structure of the graph into the LLM's generation process by incorporating message passing layers into an LLM's architecture.To evaluate our proposed method, we design a novel set of experiments using publicly available and widely studied molecule and knowledge graph data sets.Results suggest our proposed approach generates graphs which more closely meet the requested functional requirements, outperforming baselines developed on similar tasks by a statistically significant margin.</p>
<p>INTRODUCTION</p>
<p>Many concepts can be described by graphs; including molecules (19), abstract syntax trees (18), knowledge graphs (26), and project schedules (17).Each of these concepts is used in downstream tasks where graph structure has a direct impact on task performance.For example, some molecules can be used as medicine for a disease while others cannot and this is partially determined by the molecules' graphs.It would be useful if we could describe the functional requirements of a graph using natural language and query a model to conditionally generate a graph which meets these requirements.For example, a model using the prompt "generate a molecule with 47 valency electrons" would generate a valid molecule graph with 47 valency electrons.With such models we could speed up and improve the processes of drug discovery, software development, project management as well as many other applications of graphs.</p>
<p>In this work, we focus on the problem of generating graphs where node and edge features are strings of text and term this type of graph a text graph.Text graphs are a fairly flexible data format and can be used in most applications of graph-structured data including those listed above.To the best of our knowledge, text graph generation conditioned on text has only been studied in the fields of knowledge graph generation (8; 12; 13; 25; 26; 38) and explanation graph generation (32).In both setups the conditional text explicitly describes the graph.These explicit descriptions give an imperative "recipe" with step-by-step instructions of how to generate the graph.Contrary to giving explicit imperative descriptions, this work focuses on the case where the conditional text is a functional description of the graph, specifically for a downstream task.Additionally, in the case of imperative conditional text, there is an injective mapping between conditional text and the graph that should be generated, which means that methods can be evaluated by comparing the generated graph to the ground truth graph in terms of its structure.By contrast, many graphs might correspond to the same functional description and hence a new experimental design is required to study the problem.</p>
<p>Preprint</p>
<p>We propose solving this problem by fine tuning a large language model (LLM) to generate a serialized text description of a graph.This is motivated by the strong performance pre-trained LLMs have shown when fine-tuned to perform a specific task (6; 16; 33), including the task of text-to-serialized graph generation (8).However, prior work on serialized graph generation does not generalize to all domains containing text graph structured data.To further this approach, we propose a serialization method which is expressive enough to reversibly serialize any graph and a method to incorporate graph structure into the LLM's generation process motivated by the observation that the structures of generated graphs have a direct impact on their functional properties and consequently on model performance.While LLMs are probably expressive enough to learn to incorporate graph structure into their generation process, it is more efficient to provide the model with the ability to do so.However, it is also a non-trivial challenge because modern LLMs perform autoregressive sampling to generate text (4; 5; 6; 16; 27; 30; 33).This involves a multi-step sequential process, where at each step a forward pass is performed and then a new token is sampled.A token is an atomic unit of text like 'dog', ' ', or 'ch'.The generation process exhibits high computational complexity and cost due to the requirement of performing a forward model pass for each generated token.Consequently, LLMs are typically trained to generate text without actually performing generation at training time.Only a single forward pass is performed at each training step without sampling and the entire sequence (conditional plus desired text) is fed into the LLM for that forward pass.As such, an LLM has access to the full sequence at training time, while at generation time it only has access to the conditional text and the part of the sequence that has already been generated.This means that during training an LLM could learn to rely on information that it will not have access to at generation time.This issue is overcome by using causal masking, which guarantees the LLM cannot pass information backwards in the token sequence ensuring that generation conditions are simulated at training time.</p>
<p>In Section 3, we propose a method of incorporating graph structure into an LLM's generation process which passes information between tokens in a sequence.Our key contribution is demonstrating how to do so without passing information backwards in the sequence.Specifically, we propose to extend LLMs to process and generate sequences of text and graphs.For this we provide the LLM with the ability to deserialize graphs incorporated into input token sequences and introduce message passing layers into the LLM to calculate representations of graph structure in a manner that is conducive to autoregressive generation.</p>
<p>In this work, we specifically focus on the problem of generating text graphs from functional requirements within the range of those seen at training time (interpolation), instead of extrapolating outside the training set's domain, which we leave as future work.We propose a novel experiment design to evaluate methods in this new problem setting using the publicly available data sets WebNLG+ 2020 (10) and PCQM4M (19).Results suggest that our proposed model outperforms previous work on text graph generation conditioned on text by a statistically significant margin.And that our proposed approach for incorporating graph structure into the language model's generation process leads to models which generate examples which on average more closely meet their conditional functional requirements.All code used to perform experiments is publicly available at ...</p>
<p>PRELIMINARIES</p>
<p>A text graph G = {V, E} is composed of a set of N nodes v i ∈ V each with an associated text string describing the node and a set of M directed edges (v i , v j ) ∈ E each with an associated text string describing the edge.Each text graph G is associated with text D f containing a functional description of G. Our goal is to train a model to accurately predict p θ (G|D f ) with parameters θ, describing the distribution over text graphs G conditioned on a specific functional description D f .Importantly, this distribution may be multimodal as multiple graphs may have the same functional properties.This work focuses on the case where the parameterized model p θ (•) is a language model that generates a serialized text description of the graph.Hence, if D G is the serialized graph, then the language model predicts p θ (D G |D f ).This requires an injective serialization function g : G → D G with a known inverse mapping g −1 : D G → G from the serialized description D G back to the graph G such that G = g −1 (g(G)).g(•) is used at training time to transform graphs G into serialized graphs D G and the deserialization function is used at generation time to recover the graph G from the serialized graph D G (see Figure 1 top row).With this approach, we pose the problem of generating graphs Prior state-of-the-art work on serialized graph generation uses a bag-of-edges approach (8; 32), which describes each edge using a string with special tokens for delimiting graph elements and then lists out the edges in a predefined order.For example, the popular smiles molecule serialisation algorithm uses depth-first search traversal (39) to order chemical bonds (edges) in a molecule.Then each edge is described with the syntax <PN>predecessor node feature string<E>edge feature string<SN>successor node feature string, where <PN>,<E>,<SN> are special tokens used to delimit the different components of the edge.</p>
<p>Using LLMs, a description D is represented in three ways: 1) as a string (text), 2) as a sequence of token indicators, and 3) as a sequence of feature vectors / embeddings.To distinguish between these three representations, we use D S to denote the string representation, D T to denote the sequence of token indicators and D F to denote the sequence of feature vectors.For the serialized graph D G , its string representation is denoted by D G,S , its sequence of token indicators representation by D G,T and its sequence of feature vector representation by D G,F .</p>
<p>METHOD</p>
<p>The datasets LLMs are pre-trained on do not contain much, if any, data on generating serialized graphs (4; 6; 30; 33).Consequently, it is unlikely they could generate text graphs without further training.We provide evidence supporting this hypothesis in experiments where we evaluate a pretrained LLM without further fine-tuning.As shown in the results in Appendix D, an LLM without fine-tuning did not generate a single valid serialized graph.Hence, we propose fine-tuning an LLM on a training and validation set of (functional description, graph) pairs to generate serialized graphs conditioned on functional descriptions.While LLMs are probably expressive enough to learn to incorporate graph structure into their generation process, it is more efficient to provide the model with the ability to do so.State-of-the-art models for performing inference on graph structured data use message passing (MP) layers (14; 35).As such, we propose interleaving MP layers between language modelling layers in an LLM (Figure 1 block A).Any message passing layer can be used with our proposed approach1 .</p>
<p>FINE-TUNING OBJECTIVE</p>
<p>For fine-tuning, we propose minimizing the negative log-likelihood of serialized graph sequences D G as described in equation ( 1):
Loss = 1 n batch n batch i=1 − log p θ (d 1,i G,T |D i f ) + n i seq j=2 − log p θ (d j,i G,T |d 1:(j−1),i G,T , D i f )(1)
where
d i,j G,T ∈ D i G,T
, the superscript i is the index in a batch, the superscript j indicates the position of an element d i,j G,T in the sequence D i G,T , n batch is the number of elements in the batch, and n i seq is the number of elements in the sequence of example i in the batch.This objective differs from the traditional LLM fine-tuning objective of maximizing the average likelihood of the next token for each token in a batch of sequences as described in equation 2;
Loss = 1 n batch i=1 n i seq n batch i=1 − log p θ (d 1,i G,T |D i f ) + n i seq j=2 − log p θ (d j,i G,T |d 1:(j−1),i G,T , D i f )(2)
The difference between the two objectives is the red term in both equations.We can view the reciprocal of this term as a weight placed on the importance of an example in the training set with respect to the training objective.For the standard training objective in equation 2, the term
n batch i=1
n i seq in red changes from batch to batch, because batches contain randomly sampled examples which differ in A comparison in the performance of models trained with these objectives can be seen in Tables 2 and  3.The baseline regen only differs from our proposed model SGG-LLM without message passing in that it was trained using equation 2 instead of 1.We also report additional experiments shown Appendix D which suggests what is causing the difference in performance.</p>
<p>NODE DISAMBIGUATION DURING SERIALIZATION</p>
<p>One special case of graph-structured data, which is not handled by prior work on generating serialized graphs, is when a subset of nodes in a graph are described by the same feature string2 .However, this special case occurs in many domains, such as molecule data.For example, the molecule with multiple carbon atoms depicted in Figure 2 top row.To address this issue, we add a disambiguation token <D> to the feature strings of nodes with identical feature strings followed by a unique integer.An example of node disambiguation is shown the bottom row of Figure 2, where the three carbon atoms' feature strings are modified from C, C, C to C<D>0, C<D>1, C<D>2.With the addition of a disambiguation token, the serialization method becomes expressive enough to reversibly serialize any graph.In conjunction with this serialization function and special token <D>, we also add special tokens <PN> (predecessor node) <E> (edge) and <SN> (successor node) to the tokenizer of the pre-trained LLM, as well as additional rows to the embedding layer of the LLM for all four special tokens before fine-tuning.See our publicly available code for implementations of the proposed serialization g(•) and deserialization functions g −1 (•).</p>
<p>MESSAGE PASSING WITHIN A LANGUAGE MODEL</p>
<p>Our proposed architecture requires passing information from an LLM layer to an MP layer, then to the following LLM layer, and so on as shown in block B in Figure 1.As such, we need to convert the serialized graph representation outputted by an LLM layer into a representation of the graph that can be fed into an MP layer and vice versa.The inputs to an MP layer are node feature vectors D V,F ∈ R N ×H for each node in a graph, where H is the dimensionality of a feature vector, and the graph's adjacency matrix A ∈ R N ×N .An obvious choice for constructing D V,F would be simply Preprint using the feature vectors of D G,F .However, in D G,T , each node in the graph is described by multiple tokens and consequently multiple feature vectors in D G,F .For example, in Figure 2, the predecessor node of edge 1 is described by four tokens [<PN>, C, <D>, 0].We can calculate a single node vector from its multiple token feature vectors in D G,F by selecting the feature vector of the last element describing the node in D G,F because the last vector contains information about all previous tokens in the sequence including all those describing the node because of the causal attention layers in an LLM.</p>
<p>In addition, if a node has a degree greater than one, it will be described more than once in D G3 .While a node may occur more than once in a serialized graph, using our proposed serialization method, an edge will only occur once.In a graph G = {V, E}, let there be a set e k for each edge (v i , v j ) ∈ E where the superscript k indicates the position of the edge in D G and the set contains the two nodes the edge connects {v i , v j } ∈ e k .For the graph G and graph serialization D G pair depicted in Figure 2, e 2 would be the set of nodes {C<D>1, N} contained in edge 2 because edge 2 is the second edge (k = 2) described in the serialization D G .As a slight abuse of notation, we denote elements of a graph's edge set E either by (v i , v j ) or by e k .Hence, we define a new graph G edge = {V edge , E edge } where the edges in the original graph are treated as nodes V edge = E and nodes in the original graph define edges in the new graph E edge = {(e j , e k )|e j , e k ∈ E ∧ e j ∩ e k ̸ = ∅ ∧ j ̸ = k}.We call this new graph an edge graph.Figure 2 shows an example of transforming a graph into its edge graph where the four edges in the original graph become the nodes in the edge graph.Let f index : Z + → Z + be a mapping from the index of an edge in the sequence D V edge ,F to the index of the last token describing the edge in D G,F .To construct the sequence of feature vectors for the nodes in an edge graph G edge , for each MP layer we use the last token's feature vector of each edge in the serialized graph, so that
d k V edge ,F = d f index (k) G,F
.</p>
<p>MP layers pass information between nodes in a graph by aggregating feature vectors from neighboring nodes.To ensure that an MP layer does not pass information backwards in D G,F , the MP layer should only aggregate information for a node from nodes that are earlier in the sequence as depicted in causal graph attention in block D in Figure 1.causal attention, shown in block C in Figure 1, only attends to elements previous to the query element in the sequence.graph attention only attends to elements in the sequence which are neighboring nodes to the query element in the sequence based on a reference graph.causal graph attention respects both of these constraints.We propose MP layers to pass information on a graph's edge graph.To ensure information is not passed backwards in D G,F we must add an additional constraint when constructing the edge set of an edge graph which is ∀(e j , e k ) ∈ E edge , k &gt; j.This constraint has been applied to the edge graph shown in Figure 2.</p>
<p>After passing an edge graph through an MP layer, we add each resultant edge feature vector to the token feature vector immediately after its description in the serialized graph as described in equation 3;
d t G,F = d t G,F + d k V edge ,F tanh a, if f index (k) = t − 1; d t G,F , otherwise.(3)
During the development of this method, we found it necessary to multiply element-wise the output of an MP layer by a gating term tanh a when incorporating the output back into D G,F as shown in equation 3. a is a learned gating parameter of the MP layer initialized to 0. Without the gating term, we could not fine-tune an LLM with message passing incorporated to generate valid graphs as demonstrated by the results shown in Appendix D. We hypothesize this is because the gating term allows an LLM to gradually learn to take into account graph structure.Without it, the model most likely starts fine-tuning at a position on the loss landscape from which it cannot converge to a useful local minima.The use of the gating term is inspired by the work of (1), which faced a similar issue.</p>
<p>Preprint 4 RELATED WORK 4.1 INCORPORATING OTHER MODALITIES INTO TEXT GENERATION</p>
<p>This work proposes incorporating graph structure, an additional modality, into an LLM's text generation process (where the generated text is a serialized graph).There have been many works on incorporating other modalities into the text generation process.Speech-to-text can be posed as a sequence to sequence problem (15) and so similar methods to those used for text-to-text generation have been used to incorporate the modality of sound into language generation (2; 29; 37).Another widely studied problem is image-to-text generation, where most works focus on calculating dense sequence representations of images (20; 21; 23; 28; 40; 1) to pose image-to-text generation as a sequence-to-sequence generation problem as well.From this field of work, our proposed method is inspired by (1) which incorporates image information into the text generation process by representing sequences of images as sequences of dense vectors and interleaving special layers for combining the modalities of image and text between language modelling layers in an LLM.Inspired by this approach, we propose interleaving additional layers in-between language modelling layers in a pretrained LLM and incorporating additional tokens for representing the other modality.Unlike (1), we incorporate the modality of graphs as opposed to images into the generation process, and use a modified LLM to generate graphs rather than text.</p>
<p>GENERATING GRAPHS CONDITIONED ON TEXT</p>
<p>In the field of natural language processing, there have been many works on parsing natural language into syntax trees, some of which pose the problem as a serialized graph generation conditioned on text (9; 36).However, these works use a domain specific serialization method and do not incorporate graph structure into their proposed models' generation processes.There is one recent work (32) focused on generating explanation graphs from text which proposes a contrastive learning objective for fine-tuning the LLM T5 (30) to generate graphs, however the contrastive objective requires additional labels created by an expert.There is also a recent work published on generating protein graphs conditioned on text (24), however it cannot generalize past the domain of protein graph generation because the proposed method generates sequences of characters describing amino acid sequences.</p>
<p>Most prior work on text to graph generation focuses on generating knowledge graphs from text (8; 12; 13; 25; 26; 38).The best performing method on benchmark tasks is regen (8), which uses the LLM T5 (30) to generate knowledge graphs and is fine-tuned using the standard objective of predicting the next token in a sequence (equation 2).The second best performing method on benchmark knowledge graph generation tasks is grapher (26) which uses a combination of T5 and a recurrent neural network to generate graphs.This method differs from ours in the use of an additional model besides the LLM to predict edges and generate edge features as well as a training objective similar to equation 2.</p>
<p>EXPERIMENTS</p>
<p>Prior work on text graph generation conditioned on text does not provide an experimental design for evaluating methods that generate graphs based on functional descriptions.To do so, a dataset containing (functional requirements, graph) pairs is required for a task where it is possible to automatically calculate the functional properties of newly generated graphs.To generate such a dataset, we took the open graph benchmark large scale molecule dataset PCQM4M (19) and used the open source Python package rdkit (34) to generate two functional descriptions for each molecule: number of valency electrons in it and quantitative estimated-likeness (QED) (3).These are commonly used functional properties of a molecule's graph and are described in more detail in Appendix C.</p>
<p>DATASETS</p>
<p>For evaluation we created two datasets, each dataset is composed of all the molecules in the original PCQM4M (19)  called Valency dataset.The other dataset contains (QED functional description, graph) pairs and is called QED dataset.To evaluate the impact of amount of data on model performance, we created three subsets for each of the datasets.Each subset had 1000 randomly selected molecules in its test and validation sets.Importantly, these were the same across the three subsets and then the training sets of the three subsets were another 25, 000, 100, 000 and 400, 000 randomly chosen examples.</p>
<p>The training set of the smaller subsets was contained in the training set of the larger subset sets.See Figure 2 for examples of input output pairs for the constructed data sets.</p>
<p>METRICS AND EVALUATION</p>
<p>To evaluate models, we calculated three metrics on the test sets of the datasets described above: parsability, diversity, and most importantly mean absolute error (MAE) with respect to the conditional functional property.A generated graph is parsable if no error is thrown when calculating its functional property.If a molecule is parsable (i.e. it follows correct serialization syntax and doesn't violate basic laws of physics), it is given a score of 1, otherwise 0. In all experiments we use the following metrics:</p>
<p>• Parsability is the mean parsability score of samples in the test set.</p>
<p>• MAE is the mean absolute error between the generated graph's functional property value and the requested property value averaged over samples in the test set.To interpret the magnitudes reported for MAE results see Table 1 describing some summary statistics about functional properties of graphs in the datasets used in experiments.</p>
<p>• Diversity is a measure of the multimodality of the distribution p θ (D G |D f ) that the LLM learns.A sampled graph is assigned a diversity score of 1, if it, a second sampled graph and the ground truth do not share the same node and edge sets; and is assigned a score of 0 otherwise.Diversity is the average of diversity scores of samples in the test set.</p>
<p>CURRENT STATE-OF-THE-ART</p>
<p>We implemented current state-of-the-art models from prior work as baselines: specifically we implemented the version of grapher (26), which generates edge features instead of classifying them and regen (8).We had to make two modifications to both approaches so they could generalize to molecule data: we added node disambiguation from Section 3.2 to their serialization methods and updated their language models to a more recent model BLOOM (33) 100,000 1.000 ± 0.000 0.999 ± 0.001 0.845 ± 0.017 0.506 ± 0.031 SGG-LLM edges 100,000 0.998 ± 0.001 0.999 ± 0.000 0.836 ± 0.029 0.540 ± 0.016 SGG-LLM correspondences 100,000 0.995 ± 0.001 0.998 ± 0.000 0.839 ± 0.008 0.608 ± 0.054 SGG-LLM none 25,000 0.997 ± 0.002 0.991 ± 0.003 0.799 ± 0.008 0.518 ± 0.078 SGG-LLM none 400,000 0.998 ± 0.001 1.000 ± 0.000 0.857 ± 0.006 0.542 ± 0.051 grapher N/A 100,000 1.000 ± 0.000 1.000 ± 0.000 0.745 ± 0.038 0.410 ± 0.045 regen N/A 100,000 0.984 ± 0.008 0.991 ± 0.007 0.854 ± 0.031 0.446 ± 0.124 Table 3: Model performance in terms of parsability and diversity on QED and Valency datasets.The first, second, and third best performing models are highlighted using the colors shown here.</p>
<p>Experiments were repeated three times to estimate standard error.Models below the boldface line are from prior work.All models achieved a parsability near 1.0.</p>
<p>experiments by our proposed approach.See Appendix A for a description of the training process used to train grapher, regen and variants of our proposed approach.</p>
<p>RESULTS</p>
<p>On the task of generating graphs to meet functional requirements, the ideal model can generate a diverse set of parsable graphs with functional properties equal to the requested functional property.Tables 2 and 3 describe the results of our proposed approach on the QED and Valency datasets.Our proposed approach is referred to as SGG-LLM standing for serialized graph generator large language model.All variants of our approach and the baselines grapher, and regen achieve a parsability score near or at 1.The SGG-LLM variant with correspondences message passing (described in Appendix B) is another method of incorporating the MP layers into an LLM by passing messages in D G based on node correspondences rather than edges.</p>
<p>Results in table 2 suggest that all variants of SGG-LLM outperform baselines by a statistically significant margin, using the unpaired t-student test and a threshold p-value of 0.05, in terms of generating examples with functional properties close those requested.In addition, results suggest that all variants of SGG-LLM outperform grapher by a statistically significant margin in terms of generating a diverse set of candidates on the Valency dataset.Finally, the two variants of SGG-LLM that use MP layers outperform the variant that does not utilise MP layers.</p>
<p>The high performance of all variants of our approach over regen suggest the importance of the proposed loss function and of weighting tokens equally across batches during training.The high performance of all variants of our approach over grapher could be for the same reasons as regen, as it is trained with a similar loss, or it could be that single model which jointly estimates the existence of nodes, edges, and their features is more effective at generating graphs than a dual module model.</p>
<p>To demonstrate the generality of our proposed approach beyond generating molecules, we also evaluate it on the benchmark knowledge graph generation dataset WebNLG+ 2020 (10) using a different language model T5 (30).Note this task is generating graphs conditioned on an imperative description of the graph, so is not directly linked to the focus of this paper.See Appendix E for a description of the experiments and discussion of the results.</p>
<p>DISCUSSION</p>
<p>The main limitation of our proposed approach is its reliance on fine-tuning a pre-trained autoregressive LLMs, which are computationally expensive, slow at generation time and require substantial computational resources even to fine-tune.This limitation would become even more difficult when applying this method to tasks containing larger graphs or graphs containing elements with long feature strings.Hence an interesting next step from this method would be using quantized pre-trained models and low rank adapters on LLM layers for more efficient fine tuning (7).n i seq ] which is the expected value of the differing term in equation 2 from equation 1.This is to determine whether it is the magnitude of the differing term causing the difference in performance or the fact that n batch i=1 n i seq changes from batch to batch.In experiments, some summary statistics for the term n batch i=1 n i seq in equation 2 were mean = 4053, max = 4676, median = 4060, and inter-quartile range = 3915 − 4199.There was a marginal difference in performance in terms of all three metrics when comparing the performance of SGG-LLM w/ special loss and SGG-LLM without message passing.The marginal difference in performance suggests that the changing weighting between batches and across epochs is what hurts the performance of models trained with the objective described in equation 1. regen is a model trained with equation 1.
Preprint</p>
<p>Model</p>
<p>E RESULTS OF KNOWLEDGE GRAPH GENERATION EXPERIMENTS</p>
<p>To demonstrate the generality of our proposed approach beyond generating molecules, we also evaluate it on the benchmark knowledge graph generation dataset WebNLG+ 2020 (10) using a different language model T5 (30).Like in prior work, model performance is evaluated using the</p>
<p>Figure 1 :
1
Figure 1: At generation time (top) the input to the model is the graph's functional description D f and the output is a serialised description of the graph D G .During training time (below top) the input is a functional description, serialized graph pair (D f , D g ); the output is the probability p(D g ).Graph serialization method g(•) is used to serialize input graphs before training, while the deserialization method g −1 (•) is used to deserialize the generated serialized graph D g as well as to help perform message passing within the LLM.Block A describes the proposed architecture interleaving message passing layers between LLM layers.Block B depicts how information is passed between LLM and MP layers.The bottom visualization depicts the masked matrices used in the types of attention performed within LLM and message passing (MP) layers.</p>
<p>Figure 2 :
2
Figure 2: Top: the correspondence between a graph and its edge graph.Middle: the graph's bag-of-edges serialisation with special tokens <PN>, <SN>, <E> and <D>.Bottom: Examples of functional requirements used for condition graph generation for the molecule above.</p>
<p>Table 1 :
1
Summary statistics about functional requirements in datasets used in experiments.The standard deviations reported for both data sets provide context for evaluating Table2below.
PreprintMinimum Maximum MeanStandard DeviationMedianInterquantile RangeNumber of Valency Electrons212277.313.38070 -88QED0.060.980.7640.1330.780.68 -0.88Model Message Passing Training Set SizeMean Absolute Error QED ValencySGG-LLM none100,0000.044 ± 0.011 0.060 ± 0.018SGG-LLM edges100,0000.036 ± 0.005 0.035 ± 0.014SGG-LLM correspondences100,0000.039 ± 0.007 0.045 ± 0.017SGG-LLM none25,0000.062 ± 0.008 1.703 ± 0.074SGG-LLM none400,0000.020 ± 0.001 0.076 ± 0.034grapher N/A100,0000.157 ± 0.004 1.268 ± 0.229regenN/A100,0000.149 ± 0.018 2.282 ± 1.156
dataset with less than 20 atoms, which is still more than 2 million examples.One of the two datasets contains (number of valency electrons functional description, graph) pairs and is</p>
<p>Table 2 :
2
Model performance in terms of MAE on QED and Valency datasets.The first, second, and third best performing models are highlighted using the colors shown here.Experiments were repeated three times to estimate standard error.All fine-tuned variants of SGG-LLM outperform baselines by a statistically significant margin.Models below the boldface line are from prior work.</p>
<p>, which is the same LLM used in
PreprintModelMessage PassingTraining Set SizeQEDParsability ValencyQEDDiversity ValencySGG-LLM none</p>
<p>(3)&gt;0 edge 3 in the correspondence graph.Edges are constructed by connecting nodes in the correspondence graph which correspond to the same node in the original graph i.e. in the graphic C<D>0 edge 1 is connected to C<D>0 edge 3 because they refer to the node C<D>0 in the original graph.Nodes in the correspondence graph are only connected if they are adjacent occurrences in the serialized graph.By constructing edges such that they always point from an earlier instance of a node to a later instance we ensure the message passing layer does not pass information backwards in the serialized graph sequence at training time.As additional method, we propose incorporating MP layers into an LLM where the MP layers pass information based on a graph's correspondence graph.C FUNCTIONAL DESCRIPTIONS OF MOLECULESWe used two functional descriptions of molecules to generate datasets for our experiments -number of valency electrons and QED.The number of valency electrons in a molecule is the sum of the number of valency electrons in its atoms.Importantly, this property is a result of only a graph's node composition, but not its full structure.To generate descriptions of a functional property of a graph's full structure, we calculate a metric called quantitative estimated drug-likeness (QED)(3).QED quantifies multiple properties of a molecule which are attractive for applications in drug design and then calculates a single metric from these properties using a weighted logarithmic sum.The properties used to calculate QED are molecular weight, octanol-water partition coefficient, topological polar surface area, number of hydrogen bond donors and acceptors, the number of aromatic rings and rotatable bonds, and the presence of unwanted chemical functionalities.For experiments we set the weight of the properties: molecular weight, number of hydrogen bond donors and acceptors, and presence of unwanted chemical functionalities to zero in the QED calculation, because they are mainly determined by node composition instead of the entire graph structure.The functional descriptions generated were of the format "a molecule with number of valence electrons equal to ..." and "a molecule with a weighted quantitative estimation of drug-likeness equal to ...".D ADDITIONAL RESULTS OF MOLECULE GENERATION EXPERIMENTSBelow, we provide three tables describing the full results of experiments on the molecule data sets including ablation studies to empirically justify some design choices in our proposed method.Our proposed model is referred to as SGG-LLM.There are three additional models in the tables below; 1) SGG-LLM w/out fine-tuning, 2) SGG-LLM w/ special loss, and 3) SGG-LLM with edge based message passing without a gating term.The table below describing performance of models in terms of the parsability of generated examples shows that SGG-LLM w/out fine-tuning and SGG-LLM with edge based message passing without a gating term both did not produce a single parsable example on molecule datasets' test sets.These results suggests that both fine-tuning and a gating term (when incorporating message passing into an LLM) are required to achieve good performance with our proposed method.Note if a model cannot generate parsable examples, then the metric mean absolute error cannot be calculated and the diversity of generated examples is not a useful measure of model performance.Consequently, we do not report mean absolute error or diversity for SGG-LLM w/out fine-tuning and SGG-LLM with edge based message passing without a gating term.The model SGG-LLM w/ special loss is a version of SGG-LLM without message passing that was trained using equation 1, but instead of letting n batch = batch size as proposed in section 3.1, we equal n batch = E[
PreprintModelMessage Passing Training Set SizeQEDParsability ValencySGG-LLMnone100,0001.000 ± 0.000 0.999 ± 0.001SGG-LLM w/out fine-tuningnone100,0000.000 ± 0.000 0.000 ± 0.000SGG-LLM w/ special lossnone100,0000.986 ± 0.003-SGG-LLMedges100,0000.998 ± 0.001 0.999 ± 0.000SGG-LLM w/out gating termedges100,0000.000 ± 0.000 0.000 ± 0.000SGG-LLMcorrespondences100,0000.995 ± 0.001 0.998 ± 0.000SGG-LLMnone25,0000.997 ± 0.002 0.991 ± 0.003SGG-LLMnone400,0000.998 ± 0.001 1.000 ± 0.000grapherN/A100,0001.000 ± 0.000 1.000 ± 0.000regenN/A100,0000.984 ± 0.008 0.991 ± 0.007n batchi=1
We used GraphSAGE(14) in all experiments
Prior work on generating graphs conditioned on text focused on tasks requiring graphs that did not contain sets of nodes with the same feature string(8; 26;<br />
)
This makes it difficult to introduce graph information into the early elements in the graph serialization sequence. For a node with a degree greater than one, if we passed the feature vector outputted from an MP layer into its earliest instance in the graph serialization, we would be passing information backwards in the sequence
PreprintIn terms of societal impact, our proposed approach might help speed up and improve processes such as drug discovery, software development and project planning.But at the same time requires oversight to ensure it is not used for nefarious applications like the design of chemical weapons.In addition, if this approach is applied to a task in the social sciences, analysis should be required to ensure that the biases learned by the model are understood and any unfair preferences learned for a certain demographic or group should be mitigated.A HYPERPARAMETERS AND TRAININGFor experiments on the molecule data sets, all variants of our proposed approach and the implemented baseline grapher(26)and regen (8) used the pretrained version of BLOOM(33)with 560 million parameters as their language model.For experiments on the WebNLG2020+ data set(10), all models used the pretrained version of T5(30)with 770 million parameters as their language model.All models were trained for up to ten epochs and checkpointed based on minimizing validation loss.Model's were trained using the ADAM optimizer(22)with a learning rate of 3e − 5, a β 1 of 0.9, a β 2 of 0.999 and a regularization weight of 1e − 7 as well as a linear learning rate schedule.During training, model parameters' gradients norms were clipped to a value of 1.0.The models were trained with a batch size of 18 using stage 2 data parallelism using the method described in(31).All models were trained and evaluated on machines with 3 NVIDIA A100 GPUs.Variants of our proposed approach which incorporated message passing into the LLM by interleaving message passing layers in the LLM used a single GraphSage (14) layer in each message passing layer with an embedding size equal to the token embedding size of the LLM they were incorporated into.B MESSAGE PASSING BETWEEN NODE CORRESPONDENCESMost nodes appear at least twice in a bag-of-edges serialization.We can define a correspondence graph from a serialized graph D G by treating each instance of a node in a serialized graph D G as its own node in the correspondence graph.Then the correspondences between instances define edges in the correspondence graphs.An example of a correspondence graph is shown in the graphic below;In the graphic, the node C<D>0 occurs more than once in D G and each instance is treated as its own node in the correspondence graph, so the node C<D>0 corresponds to C<D>0 edge 1 and to Preprint metrics of F1-score, precision and recall when comparing a generated graph to the ground truth knowledge graph.See(8)for a more detailed explanation of these metrics.We compare our proposed method to the three best performing models on this data set; regen (8), grapher (26), and bt5(11).On the WebNLG+ 2020 data set, results in the table below suggest that the incorporating message passing into an LLM is not useful to the task of knowledge graph generation from an imperative description, but also that using edge message passing does not degrade performance.Interestingly, SGG-LLM without message passing was able to achieve state-of-the-art performance on the benchmark task of generating triples in a knowledge graph.regen's implementation for experiments on WebNLG2020+ was identical to SGG-LLM without message passing, except that SGG-LLM was trained with a different training objective (see equation 1 in the main paper), a more aggressive learning rate and a linear learning rate schedule.So the state-of-the-art performance of SG-LLM on WebNLG2020+ may be attributed to better hyperparameter selection or the modified training objective.Model performance on knowledge graph data set WebNLG+ 2020.Note: baseline model results do not report standard deviations because they were not reported in prior work and we felt it was more appropriate to report baseline results based their published results as opposed to reimplementing the baselines ourselves.Experiments with variants of our proposed approach were repeated three times to estimate standard error.Exact
Flamingo: a visual language model for few-shot learning. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, Karen Simonyan, ArXiv, abs/2204.141982022</p>
<p>SpeechT5: Unified-modal encoder-decoder pre-training for spoken language processing. Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, Furu Wei, 10.18653/v1/2022.acl-long.393Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational LinguisticsMay 20221</p>
<p>Quantifying the chemical beauty of drugs. Richard Bickerton, Gaia Paolini, Jérémy Besnard, Andrew Sorel Muresan, Hopkins, 10.1038/nchem.1243Nature chemistry. 42012</p>
<p>Language models are few-shot learners. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam Mccandlish, Alec Radford, Ilya Sutskever, Dario Amodei, 2020</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet ; Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. Felipe Petroski Such. Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang,; Andrew N. Carr; Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrewJan Leike. 2021</p>
<p>. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Emily Vinodkumar Prabhakaran, Nan Reif, Ben Du, Reiner Hutchinson, James Pope, Jacob Bradbury, Michael Austin, Guy Isard, Pengcheng Gur-Ari, Toju Yin, Anselm Duke, Sanjay Levskaya, Sunipa Ghemawat, Henryk Dev, Xavier Michalewski, Vedant Garcia, Kevin Misra, Liam Robinson, Denny Fedus, Daphne Zhou, David Ippolito, Hyeontaek Luan, Barret Lim, Alexander Zoph, Ryan Spiridonov, David Sepassi, Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck2022Jeff Dean, Slav Petrovand Noah Fiedel. Palm: Scaling language modeling with pathways</p>
<p>Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer, Qlora, arXiv:2305.14314Efficient finetuning of quantized llms. 2023arXiv preprint</p>
<p>Regen: Reinforcement learning for text and knowledge base generation using pretrained language models. Pierre L Dognin, Inkit Padhi, Igor Melnyk, Payel Das, 2021</p>
<p>Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, Noah A Smith, Recurrent neural network grammars. 2016</p>
<p>Creating training corpora for NLG micro-planners. Claire Gardent, Anastasia Shimorina, Shashi Narayan, Laura Perez-Beltrachini, 10.18653/v1/P17-1017Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational LinguisticsJuly 20171Long Papers)</p>
<p>Machine translation aided bilingual data-to-text generation and semantic parsing. Heming Ge, Mihir Sanjay Kale, Oshin Agarwal, Rami Al-Rfou, Siamak Shakeri, Yunhsuan Sung, 20203rd Workshop on Natural Language Generation from the Semantic Web</p>
<p>Machine translation aided bilingual data-to-text generation and semantic parsing. Heming Ge, Mihir Sanjay Kale, Oshin Agarwal, Rami Al-Rfou, Siamak Shakeri, Yunhsuan Sung, 20203rd Workshop on Natural Language Generation from the Semantic Web</p>
<p>Cyclegt: Unsupervised graph-to-text and text-to-graph generation via cycle training. Qipeng Guo, Zhijing Jin, Xipeng Qiu, Weinan Zhang, David Wipf, Zheng Zhang, 2020</p>
<p>Inductive representation learning on large graphs. Will Hamilton, Zhitao Ying, Jure Leskovec, 201730Advances in neural information processing systems</p>
<p>Deep speech: Scaling up end-to-end speech recognition. Y Awni, Carl Hannun, Jared Case, Bryan Casper, Greg Catanzaro, Erich Diamos, Ryan Elsen, Sanjeev Prenger, Shubho Satheesh, Adam Sengupta, Andrew Y Coates, Ng, CoRR, abs/1412.55672014</p>
<p>Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego De Las, Lisa Anne Casas, Johannes Hendricks, Aidan Welbl, Tom Clark, Eric Hennigan, Katie Noland, George Millican, Bogdan Van Den Driessche, Aurelia Damoc, Simon Guy, Karen Osindero, Erich Simonyan, Jack W Elsen, Rae, 2022</p>
<p>Data-driven schedule risk forecasting for construction mega-projects. Vahan Hovhannisyan, Peter Zachares, Yael Grushka-Cockayne, Alan Mosca, Carlos Ledezma, Available at SSRN. 44961192023</p>
<p>Open graph benchmark: Datasets for machine learning on graphs. Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, Jure Leskovec, arXiv:2005.006872020arXiv preprint</p>
<p>Ogblsc: A large-scale challenge for machine learning on graphs. Weihua Hu, Matthias Fey, Hongyu Ren, Maho Nakata, Yuxiao Dong, Jure Leskovec, arXiv:2103.094302021arXiv preprint</p>
<p>Scaling up visual and vision-language representation learning with noisy text supervision. Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V Le, Yun-Hsuan Sung, Zhen Li, Tom Duerig, CoRR, abs/2102.059182021</p>
<p>Vilt: Vision-and-language transformer without convolution or region supervision. Wonjae Kim, Bokyung Son, Ildoo Kim, 2021</p>
<p>Adam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, 3rd International Conference on Learning Representations, ICLR 2015. San Diego, CA, USAMay 7-9, 2015. 2015Conference Track Proceedings</p>
<p>Visualbert: A simple and performant baseline for vision and language. Liunian Harold, Li , Mark Yatskar, Cho-Jui Da Yin, Kai-Wei Hsieh, Chang, CoRR, abs/1908.035572019</p>
<p>A text-guided protein design framework. Shengchao Liu, Yutao Zhu, Jiarui Lu, Zhao Xu, Weili Nie, Anthony Gitter, Chaowei Xiao, Jian Tang, Hongyu Guo, Anima Anandkumar, 2023</p>
<p>Unified structure generation for universal information extraction. Yaojie Lu, Qing Liu, Dai Dai, Xinyan Xiao, Hongyu Lin, Xianpei Han, Le Sun, Hua Wu, 2022</p>
<p>Knowledge graph generation from text. Igor Melnyk, Pierre Dognin, Payel Das, 2022</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, ; , Ryan Lowe, Jan Leike,. 2022</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever, CoRR, abs/2103.000202021</p>
<p>Robust speech recognition via large-scale weak supervision. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine Mcleavey, Ilya Sutskever, 2022</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, J. Mach. Learn. Res. 1532-4435211jan 2020</p>
<p>Zero: Memory optimization towards training A trillion parameter models. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, Yuxiong He, CoRR, abs/1910.020542019</p>
<p>Explanation graph generation via pretrained language models: An empirical study with contrastive learning. Swarnadeep Saha, Prateek Yadav, Mohit Bansal, 2022</p>
<p>Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, Jonathan Tow, Alexander M Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Benoît Sagot, Niklas Muennighoff, Albert Villanova Del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina Mcmillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Laurençon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, David Daniel Van Strien, Dragomir Ifeoluwa Adelani, Eduardo González Radev, Efrat Ponferrada, Ethan Levkovizh, Eyal Kim, Francesco De Bar Natan, Gérard Toni, Germán Dupont, Giada Kruszewski, Hady Pistilli, Hamza Elsahar, Hieu Benyamina, Ian Tran, Idris Yu, Isaac Abdulmumin, Itziar Johnson, Javier Gonzalez-Dios, Jenny De La Rosa, Jesse Chim, Jian Dodge, Jonathan Zhu, Jörg Chang, Joseph Frohberg, Joydeep Tobing, Khalid Bhattacharjee, Kimbo Almubarak, Kyle Chen, Leandro Lo, Leon Von Werra, Long Weber, Loubna Phan, Ludovic Ben Allal, Manan Tanguy, Manuel Dey, Maraim Romero Muñoz, María Masoud, Mario Grandury, Max Šaško, Maximin Huang, Mayank Coavoux, Mike Singh, Preprint Tian-Jian, Minh Chien Jiang, Mohammad A Vu, Mustafa Jauhar, Nishant Ghaleb, Nora Subramani, Nurulaqilla Kassner, Olivier Khamis, Omar Nguyen, Ona Espejel, Paulo De Gibert, Peter Villegas, Pierre Henderson, Priscilla Colombo, Quentin Amuok, Rheza Lhoest, Rishi Harliman, Roberto Bommasani, Rui Luis López, Salomey Ribeiro, Sampo Osei, Sebastian Pyysalo, Shamik Nagel, Shamsuddeen Bose, Shanya Hassan Muhammad, Shayne Sharma, Somaieh Longpre, Stanislav Nikpoor, Suhas Silberberg, Sydney Pai, Tiago Zink, Timo Timponi Torrent, Tristan Schick, Valentin Thrush, Vassilina Danchev, Veronika Nikoulina, Violette Laippala, Vrinda Lepercq, Zaid Prabhu, Zeerak Alyafeai, Arun Talat, Benjamin Raja, Chenglei Heinzerling, Davut Emre Si, Elizabeth Taşar, Sabrina J Salesky, Wilson Y Mielke, Abheesht Lee, Andrea Sharma, Antoine Santilli, Arnaud Chaffin, Debajyoti Stiegler, Eliza Datta, Gunjan Szczechla, Han Chhablani, Harshit Wang, Hendrik Pandey, Jason Strobelt, Alan Fries, Jos Rozen, Leo Gao, Lintang Sutawika, Saiful Bari, Maged S Al-Shaibani, Matteo Manica, Nihal Nayak, Ryan Teehan, Samuel Albanie, Sheng Shen, Srulik Ben-David, Stephen H Bach, Taewoon Kim, Tali Bers, Thibault Fevry, Trishala Neeraj, Conglong Li, Deepak Narayanan, Hatim Bourfoune, Jared Casper, Jeff Rasley, Max Ryabinin, Mayank Mishra, Minjia Zhang, Mohammad Shoeybi, Myriam Peyrounette, Nicolas Patry, Nouamane Tazi, Omar Sanseviero, Pierre Patrick Von Platen, Pierre Cornette, Rémi François Lavallée, Samyam Lacroix, Sanchit Rajbhandari, Shaden Gandhi, Stéphane Smith, Suraj Requena, Tim Patil, Ahmed Dettmers, Amanpreet Baruwa, Anastasia Singh, Anne-Laure Cheveleva, Arjun Ligozat, Aurélie Subramonian, Charles Névéol, Dan Lovering, Deepak Garrette, Ehud Tunuguntla, Ekaterina Reiter, Ekaterina Taktasheva, Eli Voloshina, Genta Bogdanov, Hailey Indra Winata, Jan-Christoph Schoelkopf, Jekaterina Kalo, Jessica Novikova, Jordan Zosa Forde, Jungo Clive, Ken Kasai, Liam Kawamura, Marine Hazan, Miruna Carpuat, Najoung Clinciu, Newton Kim, Oleg Cheng, Omer Serikov, Oskar Antverg, Rui Van Der Wal, Ruochen Zhang, Sebastian Zhang, Shachar Gehrmann, Shani Mirkin, Tatiana Pais, Thomas Shavrina, Tian Scialom, Tomasz Yun, Verena Limisiewicz, Vitaly Rieser, Vladislav Protasov, Yada Mikhailov, Yonatan Pruksachatkun, Zachary Belinkov, Zdeněk Bamberger, Alice Kasner, Amanda Rueda, Amir Pestana, Ammar Feizpour, Amy Khan, Ana Faranak, Anthony Santos, Silas Hevia, Sourav Wang, Sylvain Roy, Thanh Viguier, Tobi Le, Trieu Oyebade, Yoyo Le, Zach Yang, Nguyen ; Chenxi, Chirag Zhou, Chuxin Jain, Clémentine Xu, Fourrier, Daniel Daniel León Periñán, Dian Molano, Enrique Yu, Fabio Manjavacas, Florian Barth, Gabriel Fuhrimann, Altay ; Jihyun, John Kang, Jonas Giorgi, Jose Golde, Karthik David Posada, Lokesh Rangasai Sivaraman, Lu Bulchandani, Luisa Liu, Shinzato, Giyaseddin Bayrak, Gully Burns, Helena U. Vrabec, Imane Bello, Ishani Dash. Aycha Abdollahi, Azadeh Tammour, Bahareh Hajihosseini, Benjamin Behroozi, Bharat Ajibade, Carlos Saxena, Danish Muñoz Ferrandis, David Contractor, Davis Lansky, Douwe David, Kiela, A Duong, Edward Nguyen, Emi Tan, Ezinwanne Baylor, Fatima Ozoani, Frankline Mirza, Habib Ononiwu, Hessie Rezanejad, Indrani Jones, Irene Bhattacharya, Irina Solaiman, Isar Sedenko, Jesse Nejadgholi, Josh Passmore, Julio Bonis Seltzer, Livia Sanz, Mairon Dutra, Maraim Samagaio, Margot Elbadri, Marissa Mieskes, Martha Gerchick, Michael Akinlolu, Mike Mckenna, Muhammed Qiu, Mykola Ghauri, Nafis Burynok, Nazneen Abrar, Nour Rajani, Nour Elkott, Olanrewaju Fahmy, Ran Samuel, Rasmus An, Ryan Kromann, Samira Hao, Sarmad Alizadeh, Shubber, Urmish Thakker, Vikas Raunak, Xiangru Tang, Zheng-Xin Yong, Zhiqing Sun, Shaked Brody, Yallow Uri, Hadar Tojarieh, Adam Roberts; Alfredo Palasciano, Alison Callahan, Anima Shukla, Antonio Miranda-Escalada, Ayush Singh, Benjamin Beilharz; Bo Wang, Caio Brito,; Maria A Castillo; Marianna Nezhurina, Mario Sänger, Matthias Samwald, Michael Cullan; Nathan Dahlberg, Nicholas Michio Broad, Nikolaus Muellner, Pascale Fung, Patrick Haller, Ramya Chandrasekhar, Renata Eisenberg, Robert Martin, Rodrigo CanalliMaiko Takeuchi, Marc Pàmies2022Michael Weinberg, Michiel De Wolf, Mina Mihaljcic, Minna Liu, Moritz Freidank, Myungsun Kang, Natasha Seelam ; Sushil Bharati, Tanmay Laud, Théo Gigant, Tomoya Kainuma, Wojciech Kusa, Yanis LabrakHyung Won Chung, Jaesung Tae, Jason Phang. Rosaline Su, Ruisi Su, Samuel Cahyawijaya, Samuele Garda, Shlok S Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Kumar, Stefan Schweter. Yash Shailesh Bajaj, Yash Venkatraman, Yifan Xu, Yingxin Xu, Yu Xu, Zhe Tan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes Belkada, and Thomas Wolf. Bloom: A 176b-parameter open-access multilingual language model</p>
<p>Gianluca Sforna, deric4 (github handle). 2023Greg Landrum, and Hans De Winter. rdkit</p>
<p>Everything is connected: Graph neural networks. Petar Veličković, Current Opinion in Structural Biology. 791025382023</p>
<p>Grammar as a foreign language. Oriol Vinyals, Łukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, Geoffrey Hinton, Advances in neural information processing systems. 282015</p>
<p>Large-scale self-and semi-supervised learning for speech translation. Changhan Wang, Anne Wu, Juan Miguel Pino, Alexei Baevski, Michael Auli, Alexis Conneau, CoRR, abs/2104.066782021</p>
<p>Language models are open knowledge graphs. Chenguang Wang, Xiao Liu, Dawn Song, 2020</p>
<p>Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. David Weininger, 10.1021/ci00057a005J. Chem. Inf. Comput. Sci. 0095- 2338281feb 1988</p>
<p>Lit: Zero-shot transfer with locked-image text tuning. Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, Lucas Beyer, CoRR, abs/2111.079912021</p>            </div>
        </div>

    </div>
</body>
</html>
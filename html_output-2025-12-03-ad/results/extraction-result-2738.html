<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2738 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2738</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2738</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-70.html">extraction-schema-70</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <p><strong>Paper ID:</strong> paper-208512978</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1911.12511v1.pdf" target="_blank">Algorithmic Improvements for Deep Reinforcement Learning applied to Interactive Fiction</a></p>
                <p><strong>Paper Abstract:</strong> Text-based games are a natural challenge domain for deep reinforcement learning algorithms. Their state and action spaces are combinatorially large, their reward function is sparse, and they are partially observable: the agent is informed of the consequences of its actions through textual feedback. In this paper we emphasize this latter point and consider the design of a deep reinforcement learning agent that can play from feedback alone. Our design recognizes and takes advantage of the structural characteristics of text-based games. We first propose a contextualisation mechanism, based on accumulated reward, which simplifies the learning problem and mitigates partial observability. We then study different methods that rely on the notion that most actions are ineffectual in any given situation, following Zahavy et al.'s idea of an admissible action. We evaluate these techniques in a series of text-based games of increasing difficulty based on the TextWorld framework, as well as the iconic game Zork. Empirically, we find that these techniques improve the performance of a baseline deep reinforcement learning agent applied to text-based games.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2738.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2738.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LSTM-DRQN (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LSTM Deep Recurrent Q-Network (baseline agent used in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recurrent deep Q-learning agent that encodes the history of textual observations and previous actions with an LSTM (history encoder) and scores actions with a feed-forward action scorer; used as the paper's baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LSTM-DRQN (modified baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A Deep Recurrent Q-Network variant that concatenates a sentence representation and the previous action representation, passes these through an LSTM history module to produce a history embedding h_t, and scores actions with a feed-forward action scorer Φ_A; trained end-to-end with Q-learning.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>TextWorld (SaladWorld suite) and ZORK I</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>TextWorld-derived synthetic benchmark 'SaladWorld' (seven levels of increasing rooms/objects/subtasks where rewards are sparse and require multi-step object manipulation) and the human-authored interactive fiction game ZORK I.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>working memory implemented by recurrent network (LSTM) over histories</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>sequential hidden-state buffer (LSTM hidden state) encoding recent history; training uses truncated sequences (sequence length l = 15, minimum history n = 6) for updates</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>recent observations (textual feedback), previous actions, cumulative score u_t is available to other modules (but baseline uses only LSTM history)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>implicit recurrent capacity; training and target computations use fixed-length transition sequences of length 15 (l = 15) and minimum backprop history n = 6</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>implicit retrieval via the LSTM hidden state (recency-biased sequential encoding); no explicit retrieval mechanism</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>updated every timestep by feeding new observation and previous action into the LSTM; training uses truncated backprop (no backprop for earliest n entries in sequence)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>to mitigate partial observability by summarizing past textual feedback and actions into a history embedding used for Q-value estimation and policy selection</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Baseline LSTM-DRQN attains good performance on simpler SaladWorld levels (levels 1–3) but performance degrades on harder levels with more subtasks and larger action sets; on ZORK it achieves lower scores than the augmented SC+masking agent (see paper figures). Specific numeric scores are reported in paper figures (fraction of subtasks completed; learning curves) but exact percentages are not listed in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Tabular agents that ignore history (hash most recent feedback) perform poorly once multiple subtasks are present; a tabular agent that uses explicit LOOK/INVENTORY state (LI-tabular) also performs poorly on harder levels (paper reports qualitative and figure-based comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Recurrent LSTM memory suffices for simpler tasks but struggles as tasks require longer histories and more precise credit assignment; implicit contextualisation via LSTM alone is harder to learn in partially observable domains with large action spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Learning an implicit memory/contextualisation purely via LSTM is difficult for harder levels; truncated training sequences limit gradient flow for long-horizon dependencies; baseline without additional contextualisation or action gating fails beyond level 3.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td>Within the experiments, augmenting the LSTM baseline with score contextualisation (K = 5 heads) and action gating (masking) produced the best results compared to the pure LSTM baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Algorithmic Improvements for Deep Reinforcement Learning applied to Interactive Fiction', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2738.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2738.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Score Contextualisation (SC)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Score Contextualisation with multiple Q-heads</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A memory-context mechanism that conditions the action-value function on cumulative episode reward (score) by using multiple network heads (K independent LSTM heads or value heads), one selected by the current cumulative score, to simplify credit assignment and mitigate partial observability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Score-contextualised LSTM-DRQN (SC)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same base LSTM-DRQN architecture augmented with K separate LSTM heads and corresponding action scorers; a mapping J(u_t) selects which head to use given the current cumulative (undiscounted) score u_t, so the Q-function approximator is Q(h_t, a_t, u_t).</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>TextWorld (SaladWorld suite) and ZORK I</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>SaladWorld: multi-room object-collection subtasks with sparse rewards; ZORK I: human-designed interactive fiction with larger, more complex state-space and sparse rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>task-progress-conditioned working memory (LSTM history plus score-indexed network heads)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>multiple independent LSTM heads (K heads) and corresponding feed-forward action scorers; the current cumulative score indexes which head is active (paper uses K = 5)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>history embedding from LSTM (past observations/actions) plus cumulative score used to pick a head; each head implicitly focuses on features relevant to that score/phase</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>same LSTM implicit capacity as baseline; training uses sequences of length 15 (l = 15) and minimum history n = 6; K = 5 heads used in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>implicit via selecting the LSTM head indexed by the current cumulative score (J(u_t)) and reading the current LSTM hidden state for that head</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>each active head's LSTM state is updated on each timestep when its head is selected; weights are trained end-to-end via Q-learning losses per-head</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>to decompose the long task into score-indexed subtasks so each head can learn a separate value function for a portion of the task, improving credit assignment and mitigating partial observability</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Score contextualisation improves learning speed and final performance on the SaladWorld levels compared to the single-head baseline: SC learns faster and solves simpler levels that the baseline fails to solve without admissible-action information; combined with masking it recovers near-oracle performance on Level 3 and outperforms the baseline across the first three levels; on ZORK SC reaches a score comparable to AE-DQN in about half the training steps (qualitative statement reported). Exact numeric percentages are shown in paper figures (fraction of tasks solved, learning curves) rather than tabulated in text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Baseline LSTM-DRQN (single-head) performs worse — slower learning and lower final fraction of subtasks solved on SaladWorld; tabular and LI-tabular baselines also perform worse (figures in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Score contextualisation mitigates partial observability by using cumulative score as a surrogate state variable, simplifying credit assignment; it lets separate heads focus on subtask-specific features and accelerates learning, especially when combined with action gating.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Does not fully solve harder SaladWorld levels (levels 4+); requires a suitable mapping J(u) and an appropriate choice of K (paper used K = 5); only as good as the ability to learn per-head value functions and still needs better exploration to handle larger problems.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td>In experiments, K = 5 heads was used and score contextualisation combined with masking (admissible-action gating) produced the strongest empirical gains (masking threshold c = 0.001 was selected by parameter sweep).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Algorithmic Improvements for Deep Reinforcement Learning applied to Interactive Fiction', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2738.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2738.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Action Gating / Φ_C (admissibility classifier)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Auxiliary admissibility classifier and action gating (Φ_C)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An auxiliary LSTM-based binary classifier that predicts for each candidate action the probability ξ(h,a) that the action is admissible given the history; its outputs are used to gate actions via masking, dropout, or in a history-adapted consistent Q-learning update (CQLH).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LSTM-based admissibility classifier (Φ_C) used for action gating</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Φ_C is a sigmoid-output classifier taking the LSTM history embedding h_t as input and producing per-action admissibility probabilities ξ(h_t,a); learned from bandit feedback indicating whether executed actions were admissible (binary signal).</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>TextWorld (SaladWorld suite) and ZORK I</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>SaladWorld: tasks require remembering prior actions to make actions admissible; admissibility depends on history (e.g., 'put lettuce on counter' only after 'take lettuce'). ZORK: human-authored IF with complex admissibility structure.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>classifier uses LSTM-encoded history as its memory input (no separate external memory store)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>per-action admissibility probabilities computed from the LSTM history embedding; no explicit key-value store — memory is the LSTM hidden state and learned classifier weights</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>encoded recent history (observations and past actions) used to infer admissibility; classifier learns mapping from histories to admissibility probabilities</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>implicit LSTM capacity; training uses sequences of length 15 for target computation and classifier training across sampled transitions</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>implicit reading of the LSTM history embedding to produce ξ(h,a); gating decisions use thresholding (masking) or stochastic inclusion (dropout) based on ξ</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Φ_C is trained online from bandit feedback: after chosen action a_t the agent receives a binary admissibility signal e_t and updates Φ_C using binary cross-entropy loss; replay buffer sampling used for training</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>to reduce exploration/action selection over vast action spaces by eliminating or down-weighting inadmissible (ineffectual) actions, thereby improving sample efficiency and credit assignment</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Action gating alone gives modest benefits on the simplest level but little improvement beyond Level 1; when combined with score contextualisation, masking (thresholding) significantly improves results (on Level 3 it nearly matches SC with oracle gating); overall, SC + Masking outperforms baseline in most early levels and speeds learning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Without admissibility gating, SC still helps but learning is slower; baseline without gating suffers in levels where many actions are ineffectual. Exact numeric metrics are plotted in figures (fraction of tasks solved, learning curves).</td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Masking (deterministic thresholding of ξ) performs best among tested gating methods; dropout and CQLH are less effective in experiments. Learning admissibility from bandit feedback is possible but harder than oracle gating — oracle gating (true admissible set) yields faster learning because it gives perfect credit assignment.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Learning ξ(h,a) from bandit feedback is challenging due to state aliasing and partial observability; CQLH can underestimate action-values when ξ ≈ 0.5 and yields smaller action gaps than state-based consistent Q-learning; masking requires choosing a threshold c (paper used c = 0.001).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td>Masking (A_t := {a : ξ(h_t,a) ≥ c}) combined with score contextualisation yielded the best empirical results; c = 0.001 was selected for masking in experiments. Among backup strategies, the paper's CQLH (history-adapted consistent Q-learning) was more stable than an alternate simpler variant (ACQLH).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Algorithmic Improvements for Deep Reinforcement Learning applied to Interactive Fiction', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2738.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2738.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LI-tabular</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Look+Inventory tabular agent (LI-tabular)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A comparator tabular agent that constructs a state description via information-gathering commands (LOOK and INVENTORY) and hashes this description to Q-values — an explicit, hand-crafted memory-like state construction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LI-tabular (LOOK/INVENTORY tabular agent)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A tabular Q-learning agent that explicitly performs LOOK and INVENTORY actions to gather descriptive state information and then treats that latest description as the state (hashing description-action pairs to Q-values).</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>TextWorld (SaladWorld suite)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>SaladWorld levels require remembering objects and subtasks; LI-tabular uses explicit info-gathering to construct a disambiguated state.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>explicit episodic-like state constructed via information-gathering commands (implicit external memory: the LOOK/INVENTORY textual description becomes the state)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>hashed mapping from the most recent description (including LOOK and INVENTORY outputs) to Q-values (tabular store of description-action pairs)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>the immediate textual descriptions returned by LOOK and INVENTORY commands (used as the current state representation)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>limited to the most recent LOOK/INVENTORY-derived description (no long sequential memory beyond that single description)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>direct use of the current LOOK/INVENTORY textual description as the state (no retrieval beyond reading the last gathered description)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>updates are standard tabular Q-learning updates to the hash entry corresponding to the current description-action pair when rewards are observed</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>to make the partially observable environment effectively observable by actively querying for state descriptions, simplifying Q-learning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>LI-tabular performs better than memoryless tabular but performs poorly as the number of subtasks increases and in more complex levels; empirical figures show it suffers when many subtasks are present (see paper figures).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>The memoryless tabular agent (hashing only most recent feedback) performs significantly worse than LI-tabular and both perform worse than LSTM-based approaches on more complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Explicit info-gathering to build a state helps in some settings (LI-tabular beats purely memoryless tabular), but can be problematic if information-gathering actions have negative consequences; also scales poorly as task complexity and number of subtasks grows.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Relies on handcrafted information-gathering and only uses the most recent gathered description as state; not robust when LOOK/INVENTORY are costly or when tasks require long-range memory across many steps.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td>Not applicable (single hand-crafted strategy evaluated). The paper notes that although LI-tabular can be successful as a heuristic, it is brittle and less general than learned recurrent memory.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Algorithmic Improvements for Deep Reinforcement Learning applied to Interactive Fiction', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2738.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2738.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AE-DQN (Zahavy et al. 2018)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AE-DQN (Action Elimination DQN from Zahavy et al. 2018)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work that learns to eliminate inadmissible actions via an auxiliary network and uses oracle-like information-gathering (LOOK/INVENTORY) to simplify admissibility prediction; referenced as a comparison point in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learn what not to learn: Action elimination with deep reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>AE-DQN (action-elimination DQN)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An agent that trains an auxiliary network to predict which actions are inadmissible and uses this to prune the action space during RL; the approach in Zahavy et al. is a state-based action elimination algorithm (their admissibility threshold is derived from confidence intervals using linear assumptions).</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>ZORK I (in prior work comparisons) and other IF domains</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Interactive fiction domains where many actions are ineffectual; AE-DQN prunes action space to speed learning.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>This paper reports that their score-contextualising agent reaches a ZORK score comparable to AE-DQN in about half the training steps (qualitative comparison); AE-DQN itself had shown improved convergence with admissible action pruning in Zahavy et al.'s experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>AE-DQN shows that pruning inadmissible actions can speed convergence; this paper builds on that idea but learns admissibility from bandit feedback and adapts elimination to the history-based (partially observable) setting.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Algorithmic Improvements for Deep Reinforcement Learning applied to Interactive Fiction', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2738.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2738.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ammanabrolu & Riedl (2019)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-based memory agent (Ammanabrolu and Riedl 2019)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Related work that represents state as a knowledge graph that is updated after every game step (hand-coded update rules) to handle partial observability in text-adventure games; referenced in Related Work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Playing text-adventure games with graph-based deep reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Graph-based knowledge representation agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An approach that uses a continuously updated knowledge graph as state representation; the graph encodes discovered entities, relations, and locations and is updated after each step using handcoded rules.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>Text-adventure games (choice-based and parser-based IF domains)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>IF domains where maintaining an explicit, structured memory (knowledge graph) of objects, locations, and relations can aid decision-making under partial observability.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>structured knowledge-graph (explicit long-term memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>graph with nodes (objects/locations) and edges (relations/ownership/locations); updates are hand-coded in the referenced work</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>discovered objects, relations, room connections, inventory, and inferred state facts</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>not specified in this paper; in the referenced work capacity is effectively bounded by the number of discovered entities and relations</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>explicit graph queries (in the referenced work) to form state representations for downstream policy/value networks</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>updated after every game step according to hand-coded graph-update rules (per Ammanabrolu & Riedl 2019, as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>to address partial observability by maintaining an explicit world model/knowledge graph for planning and decision making</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Mentioned as an effective approach in prior work; this paper notes the graph update rules are hand-coded and suggests interest in learning such structures during gameplay (no direct comparative metrics provided here).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Paper cites this work to note that explicit structured memories (knowledge graphs) can address partial observability but that the update rules in that prior work are hand-coded rather than learned.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>According to the paper's related-work discussion, the graph updates in Ammanabrolu & Riedl are hand-coded, which may limit generality; this paper suggests learning such updates would be interesting future work.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Algorithmic Improvements for Deep Reinforcement Learning applied to Interactive Fiction', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learn what not to learn: Action elimination with deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Language understanding for text-based games using deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Playing text-adventure games with graph-based deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>What can you do with a rock? affordance extraction via word embeddings <em>(Rating: 1)</em></li>
                <li>Towards solving text-based games by producing adaptive action spaces <em>(Rating: 1)</em></li>
                <li>Counting to explore and generalize in text-based games <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2738",
    "paper_id": "paper-208512978",
    "extraction_schema_id": "extraction-schema-70",
    "extracted_data": [
        {
            "name_short": "LSTM-DRQN (baseline)",
            "name_full": "LSTM Deep Recurrent Q-Network (baseline agent used in this paper)",
            "brief_description": "A recurrent deep Q-learning agent that encodes the history of textual observations and previous actions with an LSTM (history encoder) and scores actions with a feed-forward action scorer; used as the paper's baseline.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "LSTM-DRQN (modified baseline)",
            "agent_description": "A Deep Recurrent Q-Network variant that concatenates a sentence representation and the previous action representation, passes these through an LSTM history module to produce a history embedding h_t, and scores actions with a feed-forward action scorer Φ_A; trained end-to-end with Q-learning.",
            "base_model_size": null,
            "game_benchmark_name": "TextWorld (SaladWorld suite) and ZORK I",
            "game_description": "TextWorld-derived synthetic benchmark 'SaladWorld' (seven levels of increasing rooms/objects/subtasks where rewards are sparse and require multi-step object manipulation) and the human-authored interactive fiction game ZORK I.",
            "uses_memory": true,
            "memory_type": "working memory implemented by recurrent network (LSTM) over histories",
            "memory_structure": "sequential hidden-state buffer (LSTM hidden state) encoding recent history; training uses truncated sequences (sequence length l = 15, minimum history n = 6) for updates",
            "memory_content": "recent observations (textual feedback), previous actions, cumulative score u_t is available to other modules (but baseline uses only LSTM history)",
            "memory_capacity": "implicit recurrent capacity; training and target computations use fixed-length transition sequences of length 15 (l = 15) and minimum backprop history n = 6",
            "memory_retrieval_strategy": "implicit retrieval via the LSTM hidden state (recency-biased sequential encoding); no explicit retrieval mechanism",
            "memory_update_strategy": "updated every timestep by feeding new observation and previous action into the LSTM; training uses truncated backprop (no backprop for earliest n entries in sequence)",
            "memory_usage_purpose": "to mitigate partial observability by summarizing past textual feedback and actions into a history embedding used for Q-value estimation and policy selection",
            "performance_with_memory": "Baseline LSTM-DRQN attains good performance on simpler SaladWorld levels (levels 1–3) but performance degrades on harder levels with more subtasks and larger action sets; on ZORK it achieves lower scores than the augmented SC+masking agent (see paper figures). Specific numeric scores are reported in paper figures (fraction of subtasks completed; learning curves) but exact percentages are not listed in the main text.",
            "performance_without_memory": "Tabular agents that ignore history (hash most recent feedback) perform poorly once multiple subtasks are present; a tabular agent that uses explicit LOOK/INVENTORY state (LI-tabular) also performs poorly on harder levels (paper reports qualitative and figure-based comparisons).",
            "has_memory_ablation": true,
            "memory_effectiveness_findings": "Recurrent LSTM memory suffices for simpler tasks but struggles as tasks require longer histories and more precise credit assignment; implicit contextualisation via LSTM alone is harder to learn in partially observable domains with large action spaces.",
            "memory_limitations": "Learning an implicit memory/contextualisation purely via LSTM is difficult for harder levels; truncated training sequences limit gradient flow for long-horizon dependencies; baseline without additional contextualisation or action gating fails beyond level 3.",
            "comparison_with_other_memory_types": true,
            "best_memory_configuration": "Within the experiments, augmenting the LSTM baseline with score contextualisation (K = 5 heads) and action gating (masking) produced the best results compared to the pure LSTM baseline.",
            "uuid": "e2738.0",
            "source_info": {
                "paper_title": "Algorithmic Improvements for Deep Reinforcement Learning applied to Interactive Fiction",
                "publication_date_yy_mm": "2019-11"
            }
        },
        {
            "name_short": "Score Contextualisation (SC)",
            "name_full": "Score Contextualisation with multiple Q-heads",
            "brief_description": "A memory-context mechanism that conditions the action-value function on cumulative episode reward (score) by using multiple network heads (K independent LSTM heads or value heads), one selected by the current cumulative score, to simplify credit assignment and mitigate partial observability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Score-contextualised LSTM-DRQN (SC)",
            "agent_description": "Same base LSTM-DRQN architecture augmented with K separate LSTM heads and corresponding action scorers; a mapping J(u_t) selects which head to use given the current cumulative (undiscounted) score u_t, so the Q-function approximator is Q(h_t, a_t, u_t).",
            "base_model_size": null,
            "game_benchmark_name": "TextWorld (SaladWorld suite) and ZORK I",
            "game_description": "SaladWorld: multi-room object-collection subtasks with sparse rewards; ZORK I: human-designed interactive fiction with larger, more complex state-space and sparse rewards.",
            "uses_memory": true,
            "memory_type": "task-progress-conditioned working memory (LSTM history plus score-indexed network heads)",
            "memory_structure": "multiple independent LSTM heads (K heads) and corresponding feed-forward action scorers; the current cumulative score indexes which head is active (paper uses K = 5)",
            "memory_content": "history embedding from LSTM (past observations/actions) plus cumulative score used to pick a head; each head implicitly focuses on features relevant to that score/phase",
            "memory_capacity": "same LSTM implicit capacity as baseline; training uses sequences of length 15 (l = 15) and minimum history n = 6; K = 5 heads used in experiments",
            "memory_retrieval_strategy": "implicit via selecting the LSTM head indexed by the current cumulative score (J(u_t)) and reading the current LSTM hidden state for that head",
            "memory_update_strategy": "each active head's LSTM state is updated on each timestep when its head is selected; weights are trained end-to-end via Q-learning losses per-head",
            "memory_usage_purpose": "to decompose the long task into score-indexed subtasks so each head can learn a separate value function for a portion of the task, improving credit assignment and mitigating partial observability",
            "performance_with_memory": "Score contextualisation improves learning speed and final performance on the SaladWorld levels compared to the single-head baseline: SC learns faster and solves simpler levels that the baseline fails to solve without admissible-action information; combined with masking it recovers near-oracle performance on Level 3 and outperforms the baseline across the first three levels; on ZORK SC reaches a score comparable to AE-DQN in about half the training steps (qualitative statement reported). Exact numeric percentages are shown in paper figures (fraction of tasks solved, learning curves) rather than tabulated in text.",
            "performance_without_memory": "Baseline LSTM-DRQN (single-head) performs worse — slower learning and lower final fraction of subtasks solved on SaladWorld; tabular and LI-tabular baselines also perform worse (figures in paper).",
            "has_memory_ablation": true,
            "memory_effectiveness_findings": "Score contextualisation mitigates partial observability by using cumulative score as a surrogate state variable, simplifying credit assignment; it lets separate heads focus on subtask-specific features and accelerates learning, especially when combined with action gating.",
            "memory_limitations": "Does not fully solve harder SaladWorld levels (levels 4+); requires a suitable mapping J(u) and an appropriate choice of K (paper used K = 5); only as good as the ability to learn per-head value functions and still needs better exploration to handle larger problems.",
            "comparison_with_other_memory_types": true,
            "best_memory_configuration": "In experiments, K = 5 heads was used and score contextualisation combined with masking (admissible-action gating) produced the strongest empirical gains (masking threshold c = 0.001 was selected by parameter sweep).",
            "uuid": "e2738.1",
            "source_info": {
                "paper_title": "Algorithmic Improvements for Deep Reinforcement Learning applied to Interactive Fiction",
                "publication_date_yy_mm": "2019-11"
            }
        },
        {
            "name_short": "Action Gating / Φ_C (admissibility classifier)",
            "name_full": "Auxiliary admissibility classifier and action gating (Φ_C)",
            "brief_description": "An auxiliary LSTM-based binary classifier that predicts for each candidate action the probability ξ(h,a) that the action is admissible given the history; its outputs are used to gate actions via masking, dropout, or in a history-adapted consistent Q-learning update (CQLH).",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "LSTM-based admissibility classifier (Φ_C) used for action gating",
            "agent_description": "Φ_C is a sigmoid-output classifier taking the LSTM history embedding h_t as input and producing per-action admissibility probabilities ξ(h_t,a); learned from bandit feedback indicating whether executed actions were admissible (binary signal).",
            "base_model_size": null,
            "game_benchmark_name": "TextWorld (SaladWorld suite) and ZORK I",
            "game_description": "SaladWorld: tasks require remembering prior actions to make actions admissible; admissibility depends on history (e.g., 'put lettuce on counter' only after 'take lettuce'). ZORK: human-authored IF with complex admissibility structure.",
            "uses_memory": true,
            "memory_type": "classifier uses LSTM-encoded history as its memory input (no separate external memory store)",
            "memory_structure": "per-action admissibility probabilities computed from the LSTM history embedding; no explicit key-value store — memory is the LSTM hidden state and learned classifier weights",
            "memory_content": "encoded recent history (observations and past actions) used to infer admissibility; classifier learns mapping from histories to admissibility probabilities",
            "memory_capacity": "implicit LSTM capacity; training uses sequences of length 15 for target computation and classifier training across sampled transitions",
            "memory_retrieval_strategy": "implicit reading of the LSTM history embedding to produce ξ(h,a); gating decisions use thresholding (masking) or stochastic inclusion (dropout) based on ξ",
            "memory_update_strategy": "Φ_C is trained online from bandit feedback: after chosen action a_t the agent receives a binary admissibility signal e_t and updates Φ_C using binary cross-entropy loss; replay buffer sampling used for training",
            "memory_usage_purpose": "to reduce exploration/action selection over vast action spaces by eliminating or down-weighting inadmissible (ineffectual) actions, thereby improving sample efficiency and credit assignment",
            "performance_with_memory": "Action gating alone gives modest benefits on the simplest level but little improvement beyond Level 1; when combined with score contextualisation, masking (thresholding) significantly improves results (on Level 3 it nearly matches SC with oracle gating); overall, SC + Masking outperforms baseline in most early levels and speeds learning.",
            "performance_without_memory": "Without admissibility gating, SC still helps but learning is slower; baseline without gating suffers in levels where many actions are ineffectual. Exact numeric metrics are plotted in figures (fraction of tasks solved, learning curves).",
            "has_memory_ablation": true,
            "memory_effectiveness_findings": "Masking (deterministic thresholding of ξ) performs best among tested gating methods; dropout and CQLH are less effective in experiments. Learning admissibility from bandit feedback is possible but harder than oracle gating — oracle gating (true admissible set) yields faster learning because it gives perfect credit assignment.",
            "memory_limitations": "Learning ξ(h,a) from bandit feedback is challenging due to state aliasing and partial observability; CQLH can underestimate action-values when ξ ≈ 0.5 and yields smaller action gaps than state-based consistent Q-learning; masking requires choosing a threshold c (paper used c = 0.001).",
            "comparison_with_other_memory_types": true,
            "best_memory_configuration": "Masking (A_t := {a : ξ(h_t,a) ≥ c}) combined with score contextualisation yielded the best empirical results; c = 0.001 was selected for masking in experiments. Among backup strategies, the paper's CQLH (history-adapted consistent Q-learning) was more stable than an alternate simpler variant (ACQLH).",
            "uuid": "e2738.2",
            "source_info": {
                "paper_title": "Algorithmic Improvements for Deep Reinforcement Learning applied to Interactive Fiction",
                "publication_date_yy_mm": "2019-11"
            }
        },
        {
            "name_short": "LI-tabular",
            "name_full": "Look+Inventory tabular agent (LI-tabular)",
            "brief_description": "A comparator tabular agent that constructs a state description via information-gathering commands (LOOK and INVENTORY) and hashes this description to Q-values — an explicit, hand-crafted memory-like state construction.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "LI-tabular (LOOK/INVENTORY tabular agent)",
            "agent_description": "A tabular Q-learning agent that explicitly performs LOOK and INVENTORY actions to gather descriptive state information and then treats that latest description as the state (hashing description-action pairs to Q-values).",
            "base_model_size": null,
            "game_benchmark_name": "TextWorld (SaladWorld suite)",
            "game_description": "SaladWorld levels require remembering objects and subtasks; LI-tabular uses explicit info-gathering to construct a disambiguated state.",
            "uses_memory": true,
            "memory_type": "explicit episodic-like state constructed via information-gathering commands (implicit external memory: the LOOK/INVENTORY textual description becomes the state)",
            "memory_structure": "hashed mapping from the most recent description (including LOOK and INVENTORY outputs) to Q-values (tabular store of description-action pairs)",
            "memory_content": "the immediate textual descriptions returned by LOOK and INVENTORY commands (used as the current state representation)",
            "memory_capacity": "limited to the most recent LOOK/INVENTORY-derived description (no long sequential memory beyond that single description)",
            "memory_retrieval_strategy": "direct use of the current LOOK/INVENTORY textual description as the state (no retrieval beyond reading the last gathered description)",
            "memory_update_strategy": "updates are standard tabular Q-learning updates to the hash entry corresponding to the current description-action pair when rewards are observed",
            "memory_usage_purpose": "to make the partially observable environment effectively observable by actively querying for state descriptions, simplifying Q-learning",
            "performance_with_memory": "LI-tabular performs better than memoryless tabular but performs poorly as the number of subtasks increases and in more complex levels; empirical figures show it suffers when many subtasks are present (see paper figures).",
            "performance_without_memory": "The memoryless tabular agent (hashing only most recent feedback) performs significantly worse than LI-tabular and both perform worse than LSTM-based approaches on more complex tasks.",
            "has_memory_ablation": true,
            "memory_effectiveness_findings": "Explicit info-gathering to build a state helps in some settings (LI-tabular beats purely memoryless tabular), but can be problematic if information-gathering actions have negative consequences; also scales poorly as task complexity and number of subtasks grows.",
            "memory_limitations": "Relies on handcrafted information-gathering and only uses the most recent gathered description as state; not robust when LOOK/INVENTORY are costly or when tasks require long-range memory across many steps.",
            "comparison_with_other_memory_types": true,
            "best_memory_configuration": "Not applicable (single hand-crafted strategy evaluated). The paper notes that although LI-tabular can be successful as a heuristic, it is brittle and less general than learned recurrent memory.",
            "uuid": "e2738.3",
            "source_info": {
                "paper_title": "Algorithmic Improvements for Deep Reinforcement Learning applied to Interactive Fiction",
                "publication_date_yy_mm": "2019-11"
            }
        },
        {
            "name_short": "AE-DQN (Zahavy et al. 2018)",
            "name_full": "AE-DQN (Action Elimination DQN from Zahavy et al. 2018)",
            "brief_description": "Prior work that learns to eliminate inadmissible actions via an auxiliary network and uses oracle-like information-gathering (LOOK/INVENTORY) to simplify admissibility prediction; referenced as a comparison point in this paper.",
            "citation_title": "Learn what not to learn: Action elimination with deep reinforcement learning",
            "mention_or_use": "mention",
            "agent_name": "AE-DQN (action-elimination DQN)",
            "agent_description": "An agent that trains an auxiliary network to predict which actions are inadmissible and uses this to prune the action space during RL; the approach in Zahavy et al. is a state-based action elimination algorithm (their admissibility threshold is derived from confidence intervals using linear assumptions).",
            "base_model_size": null,
            "game_benchmark_name": "ZORK I (in prior work comparisons) and other IF domains",
            "game_description": "Interactive fiction domains where many actions are ineffectual; AE-DQN prunes action space to speed learning.",
            "uses_memory": null,
            "memory_type": null,
            "memory_structure": null,
            "memory_content": null,
            "memory_capacity": null,
            "memory_retrieval_strategy": null,
            "memory_update_strategy": null,
            "memory_usage_purpose": null,
            "performance_with_memory": "This paper reports that their score-contextualising agent reaches a ZORK score comparable to AE-DQN in about half the training steps (qualitative comparison); AE-DQN itself had shown improved convergence with admissible action pruning in Zahavy et al.'s experiments.",
            "performance_without_memory": null,
            "has_memory_ablation": null,
            "memory_effectiveness_findings": "AE-DQN shows that pruning inadmissible actions can speed convergence; this paper builds on that idea but learns admissibility from bandit feedback and adapts elimination to the history-based (partially observable) setting.",
            "memory_limitations": null,
            "comparison_with_other_memory_types": null,
            "best_memory_configuration": null,
            "uuid": "e2738.4",
            "source_info": {
                "paper_title": "Algorithmic Improvements for Deep Reinforcement Learning applied to Interactive Fiction",
                "publication_date_yy_mm": "2019-11"
            }
        },
        {
            "name_short": "Ammanabrolu & Riedl (2019)",
            "name_full": "Graph-based memory agent (Ammanabrolu and Riedl 2019)",
            "brief_description": "Related work that represents state as a knowledge graph that is updated after every game step (hand-coded update rules) to handle partial observability in text-adventure games; referenced in Related Work.",
            "citation_title": "Playing text-adventure games with graph-based deep reinforcement learning",
            "mention_or_use": "mention",
            "agent_name": "Graph-based knowledge representation agent",
            "agent_description": "An approach that uses a continuously updated knowledge graph as state representation; the graph encodes discovered entities, relations, and locations and is updated after each step using handcoded rules.",
            "base_model_size": null,
            "game_benchmark_name": "Text-adventure games (choice-based and parser-based IF domains)",
            "game_description": "IF domains where maintaining an explicit, structured memory (knowledge graph) of objects, locations, and relations can aid decision-making under partial observability.",
            "uses_memory": true,
            "memory_type": "structured knowledge-graph (explicit long-term memory)",
            "memory_structure": "graph with nodes (objects/locations) and edges (relations/ownership/locations); updates are hand-coded in the referenced work",
            "memory_content": "discovered objects, relations, room connections, inventory, and inferred state facts",
            "memory_capacity": "not specified in this paper; in the referenced work capacity is effectively bounded by the number of discovered entities and relations",
            "memory_retrieval_strategy": "explicit graph queries (in the referenced work) to form state representations for downstream policy/value networks",
            "memory_update_strategy": "updated after every game step according to hand-coded graph-update rules (per Ammanabrolu & Riedl 2019, as cited)",
            "memory_usage_purpose": "to address partial observability by maintaining an explicit world model/knowledge graph for planning and decision making",
            "performance_with_memory": "Mentioned as an effective approach in prior work; this paper notes the graph update rules are hand-coded and suggests interest in learning such structures during gameplay (no direct comparative metrics provided here).",
            "performance_without_memory": null,
            "has_memory_ablation": null,
            "memory_effectiveness_findings": "Paper cites this work to note that explicit structured memories (knowledge graphs) can address partial observability but that the update rules in that prior work are hand-coded rather than learned.",
            "memory_limitations": "According to the paper's related-work discussion, the graph updates in Ammanabrolu & Riedl are hand-coded, which may limit generality; this paper suggests learning such updates would be interesting future work.",
            "comparison_with_other_memory_types": true,
            "best_memory_configuration": null,
            "uuid": "e2738.5",
            "source_info": {
                "paper_title": "Algorithmic Improvements for Deep Reinforcement Learning applied to Interactive Fiction",
                "publication_date_yy_mm": "2019-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learn what not to learn: Action elimination with deep reinforcement learning",
            "rating": 2,
            "sanitized_title": "learn_what_not_to_learn_action_elimination_with_deep_reinforcement_learning"
        },
        {
            "paper_title": "Language understanding for text-based games using deep reinforcement learning",
            "rating": 2,
            "sanitized_title": "language_understanding_for_textbased_games_using_deep_reinforcement_learning"
        },
        {
            "paper_title": "Playing text-adventure games with graph-based deep reinforcement learning",
            "rating": 2,
            "sanitized_title": "playing_textadventure_games_with_graphbased_deep_reinforcement_learning"
        },
        {
            "paper_title": "What can you do with a rock? affordance extraction via word embeddings",
            "rating": 1,
            "sanitized_title": "what_can_you_do_with_a_rock_affordance_extraction_via_word_embeddings"
        },
        {
            "paper_title": "Towards solving text-based games by producing adaptive action spaces",
            "rating": 1,
            "sanitized_title": "towards_solving_textbased_games_by_producing_adaptive_action_spaces"
        },
        {
            "paper_title": "Counting to explore and generalize in text-based games",
            "rating": 1,
            "sanitized_title": "counting_to_explore_and_generalize_in_textbased_games"
        }
    ],
    "cost": 0.0177245,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Algorithmic Improvements for Deep Reinforcement Learning applied to Interactive Fiction</p>
<p>Vishal Jain vishal.jain@mail.mcgill.ca 
William Fedus 
Hugo Larochelle hugolarochelle@google.com.dprecup@cs.mcgill.ca 
CIFAR Fellow</p>
<p>Doina Precup 
CIFAR Fellow</p>
<p>Marc G Bellemare bellemare@google.com 
CIFAR Fellow</p>
<p>Mila 
Google Brain </p>
<p>McGill University
4 DeepMind</p>
<p>Algorithmic Improvements for Deep Reinforcement Learning applied to Interactive Fiction</p>
<p>Text-based games are a natural challenge domain for deep reinforcement learning algorithms. Their state and action spaces are combinatorially large, their reward function is sparse, and they are partially observable: the agent is informed of the consequences of its actions through textual feedback. In this paper we emphasize this latter point and consider the design of a deep reinforcement learning agent that can play from feedback alone. Our design recognizes and takes advantage of the structural characteristics of textbased games. We first propose a contextualisation mechanism, based on accumulated reward, which simplifies the learning problem and mitigates partial observability. We then study different methods that rely on the notion that most actions are ineffectual in any given situation, following Zahavy et al.'s idea of an admissible action. We evaluate these techniques in a series of text-based games of increasing difficulty based on the TextWorld framework, as well as the iconic game ZORK. Empirically, we find that these techniques improve the performance of a baseline deep reinforcement learning agent applied to text-based games.</p>
<p>Introduction</p>
<p>In a text-based game, also called interactive fiction (IF), an agent interacts with its environment through a natural language interface. Actions consist of short textual commands, while observations are paragraphs describing the outcome of these actions ( Figure 1). Recently, interactive fiction has emerged as an important challenge for AI techniques (Atkinson et al. 2018), in great part because the genre combines natural language with sequential decision-making.</p>
<p>From a reinforcement learning perspective, IF domains pose a number of challenges. First, the state space is typically combinatorial in nature, due to the presence of objects and characters with which the player can interact. Since any natural language sentence may be given as a valid command, the action space is similarly combinatorial. The player observes its environment through feedback in natural language, making this a partially observable problem. The reward structure is usually sparse, with non-zero rewards only received when the agent accomplishes something meaningful, Copyright c 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. We are particularly interested in bringing deep reinforcement learning techniques to bear on this problem. In this paper, we consider how to design an agent architecture that can learn to play text adventure games from feedback alone. Despite the inherent challenges of the domain, we identify three structural aspects that make progress possible:</p>
<p>• Rewards from subtasks. The optimal behaviour completes a series of subtasks towards the eventual game end;</p>
<p>• Transition structure. Most actions have no effect in a given state;</p>
<p>• Memory as state.</p>
<p>Remembering key past events is often sufficient to deal with partial observability.</p>
<p>While these properties have been remarked on in previous work (Narasimhan, Kulkarni, and Barzilay 2015; Zahavy et al. 2018), here we relax some of the assumptions previously made and provide fresh tools to more tractably solve IF domains. More generally, we believe these tools to be useful in partially observable domains with similar structure. Our first contribution takes advantage of the special reward structure of IF domains. In IF, the accumulated reward within an episode correlates with the number of completed subtasks and provides a good proxy for an agent's progress. Our score contextualisation architecture makes use of this fact by defining a piecewise value function composed of different deep network heads, where each piece corresponds to a particular level of cumulative reward. This separation allows the network to learn separate value functions for different portions of the complete task; in particular, when the problem is linear (i.e., there is a fixed ordering in which subtasks must be completed), our method can be used to learn a separate value function for each subtask.</p>
<p>Our second contribution extends the work of Zahavy et al. (2018) on action elimination. We make exploration and action selection more tractable by determining which actions are admissible in the current state. Formally, we say that an action is admissible if it leads to a change in the underlying game state. While the set of available actions is typically large in IF domains, there are usually few commands that are actually admissible in any particular context. Since the state is not directly observable, we first learn an LSTMbased auxiliary classifier that predicts which actions are admissible given the agent's history of recent feedback. We use the predicted probability of an action being admissible to modulate or gate which actions are available to the agent at each time step. We propose and compare three simple modulation methods: masking, drop out, and finally consistent Q-learning (Bellemare et al. 2016b). Compared to Zahavy et al.'s algorithm, our techniques are simpler in spirit and can be learned from feedback alone.</p>
<p>We show the effectiveness of our methods on a suite of seven IF problems of increasing difficulty generated using the TextWorld platform . We find that combining the score contextualisation approach to an otherwise standard recurrent deep RL architecture leads to faster learning than when using a single value function. Furthermore, our action gating mechanism enables the learning agent to progress on the harder levels of our suite of problems.</p>
<p>Problem Setting</p>
<p>We represent an interactive fiction environment as a partially observable Markov decision process (POMDP) with deterministic observations. This POMDP is summarized by the tuple (S, A, P, r, O, ψ, γ), where S is the state space, A the action space, P is the transition function, r : S × A → R is the reward function, and γ ∈ [0, 1) is the discount factor. The function ψ : S × A × S → O describes the observation o = ψ(s, a, s ) provided to the agent when action a is taken in state s and leads to state s . Throughout we will make use of standard notions from reinforcement learning (Sutton and Barto 1998) as adapted to the POMDP literature (McCallum 1995;Silver and Veness 2010). At time step t, the agent selects an action according to a policy π which maps a history h t := o 1 , a 1 , . . . , o t to a distribution over actions, denoted π(· | h t ). This history is a sequence of observations and actions which, from the agent's perspective, replaces the unobserved environment state s t . We denote by B(s | h t ) the probability or belief of being in state s after observing h t . Finally, we will find it convenient to rely on time indices to indicate the relationship between a history h t and its successor, and denote by h t+1 the history resulting from taking action a t in h t and observing o t+1 as emitted by the hidden state s t+1 .</p>
<p>The action-value function Q π describes the expected discounted sum of rewards when choosing action a after observing history h t , and subsequently following policy π:
Q π (h t , a) = E i≥0 γ i r(s t+i , a t+i ) | h t , a ,
where we assume that the action at time t + j is drawn from π(· | h t+j ); note that the reward depends on the sequence of hidden states s t+1 , s t+2 , . . . implied by the belief state B(· | h t ). The action-value function satisfies the Bellman equation over histories
Q π (h t , a) = E st,st+1 r(s t , a) + γ max a ∈A Q π (h t+1 , a ) .
When the state is observed at each step (O = S), this simplifies to the usual Bellman equation for Markov decision processes:
Q π (s t , a) = r(s t , a) + γ E st+1∼P max a ∈A Q π (s t+1 , a ). (1)
In the fully observable case we will conflate s t and h t .</p>
<p>The Q-learning algorithm (Watkins 1989) over histories maintains an approximate action-value function Q which is updated from samples h t , a t , r t , o t+1 using a step-size parameter α ∈ [0, 1):
Q(h t , a t ) ← Q(h t , a t ) + αδ t δ t = r t + γ max a∈A Q(h t+1 , a) − Q(h t , a t ).(2)
Q-learning is used to estimate the optimal action-value function attained by a policy which maximizes Q π for all histories. In the context of our work, we will assume that this policy exists. Storing this action-value function in a lookup table is impractical, as there are in general an exponential number of histories to consider. Instead, we use recurrent neural networks approximate the Q-learning process.</p>
<p>Consistent Q-Learning</p>
<p>Consistent Q-learning (Bellemare et al. 2016b) learns a value function which is consistent with respect to a local form of policy stationarity. Defined for a Markov decision process, it replaces the term δ t in (2) by
δ CQL t = rt + γ maxa∈A Q(st+1, a) − Q(st, at) st+1 = st (γ − 1)Q(st, at) st+1 = st.(3)
Consistent Q-learning can be shown to decrease the actionvalue of suboptimal actions while maintaining the actionvalue of the optimal action, leading to larger action gaps and a potentially easier value estimation problem.</p>
<p>Observe that consistent Q-learning is not immediately adaptable to the history-based formulation, since h t+1 and h t are sequences of different lengths (and therefore not comparable). One of our contributions in this paper is to derive a related algorithm suited to the history-based setting.</p>
<p>Admissible Actions</p>
<p>We will make use of the notion of an admissible action, following terminology by Zahavy et al. (2018). 1 Definition 1. An action a is admissible in state s if P (s | s, a) &lt; 1.</p>
<p>That is, a is admissible in s if its application may result in a change in the environment state. When P (s | s, a) = 1, we say that an action is inadmissible.</p>
<p>We extend the notion of admissibility to histories as follows. We say that an action a is admissible given a history h if it is admissible in some state that is possible given h, or equivalently:
s∈S B(s | h)P (s | s, a) &lt; 1.
We denote by ξ(s) ⊆ A the set of admissible actions in state s. Abusing notation, we define the admissibility function We write A t for the set of admissible actions given history h t , i.e. the actions whose admissibility in h t is strictly greater than zero. In IF domains, inadmissible actions are usually dominated, and we will deprioritize or altogether rule them out based on our estimate of ξ(h, a).</p>
<p>More Efficient Learning for IF Domains</p>
<p>We are interested in learning an action-value function which is close to optimal and from which can be derived a nearoptimal policy. We would also like learning to proceed in a sample-efficient manner. In the context of IF domains, this is hindered by both the partially observable nature of the environment and the size of the action space. In this paper we propose two complementary ideas that alleviate some of the issues caused by partial observability and large action sets. The first idea contextualizes the action-value function on a surrogate notion of progress based on total reward so far, while the second seeks to eliminate inadmissible actions from the exploration and learning process.</p>
<p>Although our ideas are broadly applicable, for concreteness we describe their implementation in a deep reinforcement learning framework. Our agent architecture ( Figure 2) is derived from the LSTM-DRQN agent  and the work of Narasimhan, Kulkarni, and Barzilay (2015).</p>
<p>Score Contextualisation</p>
<p>In applying reinforcement learning to games, it is by now customary to translate the player's score differential into rewards (Bellemare et al. 2013;OpenAI 2018). Our setting is similar to Arcade Learning Environment in the sense that the environment provides the score. In IF, the player is  Figure 2: Our IF architecture consists of three modules: a representation generator Φ R that learns an embedding for a sentence, an action scorer Φ A that chooses a network head i (a feed-forward network) conditional on score u t , learns its Q-values and outputs Q(h t , :, u t ) and finally, an auxilliary classifier Φ C that learns an approximate admissibility functionξ(h t , :). The architecture is trained end-to-end.
Φ Φ − 1 C o n c a t e n a t e L S T M H ( 1 ) ( 1 ) Φ Φ … … … … L S T M H ( K ) ( ) Φ … … … …
awarded points for acquiring an important object, or completing some task relevant to progressing through the game. These awards occur in a linear, or almost linear structure, reflecting the agent's progression through the story, and are relatively sparse. We emphasize that this is in contrast to the more general reinforcement learning setting, which may provide reward for surviving, or achieving something at a certain rate. In the video game SPACE INVADERS, for example, the notion of "finishing the game" is ill-defined: the player's objective is to keep increasing their score until they run out of lives. We make use of the IF reward structure as follows. We call score the agent's total (undiscounted) reward since the beginning of an episode, remarking that the term extends beyond game-like domains. At time step t, the score u t is
u t := t−1 i=0 r i .
In IF domains, where the score reflects the agent's progress, it is reasonable to treat it as a state variable. We propose maintaining a separate action-value function for each possible score. This action-value function is denoted Q(h t , a t , u t ). We call this approach score contextualisation.  2018)). First, credit assignment becomes easier since the score provides clues as to the hidden state. Second, in settings with function approximation we expect optimization to be simpler since for each u, the function Q(·, ·, u) needs only be trained on a subset of the data, and hence can focus on features relevant to this part of the environment.</p>
<p>In a deep network, we implement score contextualisation using K network heads and a map J : N → {1, . . . , K} such that the J (u t ) th head is used when the agent has received a score of u t at time t. This provides the flexibility to either map each score to a separate network head, or multiple scores to one head. Taking K = 1 uses one monolothic network for all subtasks, and fully relies on this network to identify state from feedback. In our experiments, we assign scores to networks heads using a round-robin scheme with a fixed K. Using Narasimhan, Kulkarni, and Barzilay (2015)'s terminology, our architecture consists of a shared representation generator Φ R with K independent LSTM heads, followed by a feed-forward action scorer Φ A (i) which outputs the action-values ( Figure 2).</p>
<p>Action Gating Based on Admissibility</p>
<p>In this section we revisit the idea of using the admissibility function to eliminate or more generally gate actions. Consider an action a which is inadmissible in state s. By definition, taking this action does not affect the state. We further assume that inadmissible actions produce a constant level of reward, which we take to be 0 without loss of generality:</p>
<p>a inadmissible in s =⇒ r(s, a) = 0. This assumption is reasonable in IF domains, and more generally holds true in domains that exhibit subtask structure, such as the video game MONTEZUMA'S REVENGE (Bellemare et al. 2016a). We can combine knowledge of P and r for inadmissible actions with Bellman's equation (1) to deduce that for any policy π, a inadmissible in
s =⇒ Q π (s, a) ≤ max a ∈A Q π (s, a ) (4)
If we know that a is inadmissible, then we do not need to learn its action-value. We propose learning a classifier whose purpose is to predict the admissibility function. Given a history h, this classifier outputs, for each action a, the probabilityξ(h, a) that this action is admissible. Because of state aliasing, this probability is in general strictly between 0 and 1; furthermore, it may be inaccurate due to approximation error. We therefore consider action gating schemes that are sensitive to intermediate values ofξ(h, a). The first two schemes produce an approximately admissible setÂ t which varies from time step to time step; the third directly uses the definition of admissibility in a history-based implementation of the consistent Bellman operator.</p>
<p>Dropout. The dropout method randomly adds each action a toÂ t with probabilityξ(h t , a).</p>
<p>Masking. The masking method uses an elimination threshold c ∈ [0, 1). The setÂ t contains all actions a whose estimated admissibility is at least c:
A t := {a :ξ(h t , a) ≥ c}.
The masking method is a simplified version of Zahavy et al. (2018)'s action elimination algorithm, whose threshold is adaptively determined from a confidence interval, itself derived from assuming a value function and admissibility functions that can be expressed linearly in terms of some feature vector.</p>
<p>In both the dropout and masking methods, we use the action setÂ t in lieu of the the full action set A when selecting exploratory actions.</p>
<p>Consistent Q-learning for histories (CQLH). The third method leaves the action set unchanged, but instead drives the action-values of purportedly inadmissible actions to 0. This is done by adapting the consistent Bellman operator (3) to the history-based setting. First, we replace the indicator I [st+1 =st] by the probabilityξ t :=ξ(h t , a t ). Second, we drive Q(s t , a t ) to 0 in the case when we believe the state is unchanged, following the argumentation of (4). This yields a version of consistent Q-learning which is adapted to histories, and makes use of the predicted admissibility:
δ CQLH t = r t + γ max a∈A Q(h t+1 , a)ξ t + γQ(h t , a t )(1 −ξ t ) − Q(h t , a t ).(5)
One may ask whether this method is equivalent to a beliefstate average of consistent Q-learning whenξ(h t , a t ) is accurate, i.e. equals ξ(h t , a t ). In general, this is not the case: the admissibility of an action depends on the hidden state, which in turns influences the action-value at the next step. As a result, the above method may underestimate actionvalues when there is state aliasing (e.g.,ξ(h t , a t ) ≈ 0.5), and yields smaller action gaps than the state-based version whenξ(h t , a t ) = 1. However, when a t is known to be inadmissible (ξ(h t , a t ) = 0), the methods do coincide, justifying its use as an action gating scheme.</p>
<p>We implement these ideas using an auxiliary classifier Φ C . For each action a, this classifier outputs the estimated probabilityξ(h t , a), parametrized as a sigmoid function. These probabilities are learned from bandit feedback: after choosing a from history h t , the agent receives a binary signal e t as to whether a was admissible or not. In our setting, learning this classifier is particularly challenging because the agent must predict admissibility solely based on the history h t . As a point of comparison, using the informationgathering commands LOOK and INVENTORY to establish the state, as proposed by Zahavy et al. (2018), leads to a simpler learning problem, but one which does not consider the full history. The need to learnξ(h t , a) from bandit feedback also encourages methods that generalize across histories and textual descriptions.</p>
<p>Both score contextualisation and action gating are tailored to domains that exhibit the structure typical of interactive fiction. To assess how useful these methods are, we will make use of a synthetic benchmark based on the TextWorld framework . TextWorld provides a reinforcement learning interface to text-based games along with an environment specification language for designing new environments. Environments provide a set of locations, or rooms, objects that can picked up and carried between locations, and a reward function based on interacting with these objects. Following the genre, special key objects are used to access parts of the environment.</p>
<p>Our benchmark provides seven environments of increasing complexity, which we call levels. We control complexity by adding new rooms and/or objects to each successive level. Each level also requires the agent to complete a number of subtasks (Table 1), most of which involve carrying one or more items to a particular location. Reward is provided only when the agent completes one of these subtasks. Thematically, each level involves collecting food items to make a salad, inspired by the first TextWorld competition. Example objects include an apple and a head of lettuce, while example actions include get apple and slice lettuce with knife. Accordingly we call our benchmark Salad-World.</p>
<p>SaladWorld provides a graded measure of an agent architecture's ability to deal with both partial observability and large action spaces. Indeed, completing each subtasks requires memory of what has previously been accomplished, along with where different objects are. Together with this, each level in the SaladWorld involves some amount of history-dependent admissibility i.e the admissibility of the action depends on the history rather than the state. For example, put lettuce on counter can only be accomplished once take lettuce (in a different room) has happened. Keys pose an additional difficulty as they do not themselves provide reward. As shown in Table 1, the number of possible actions rapidly increases with the number of objects in a given level. Even the small number of rooms and objects considered here preclude the use of tabular representations, as the state space for a given level is the exponentially-sized cross-product of possible object and agent locations. In fact, we have purposefully designed Sal-adWorld as a small challenge for IF agents, and even our best method falls short of solving the harder levels within the allotted training time. Full details are given in Table 2 in the appendix.</p>
<p>Empirical Analysis</p>
<p>In the first set of experiments, we use SaladWorld to establish that both score contextualisation and action gating provide positive benefits in the context of IF domain. We then validate these findings on the celebrated textbased game ZORK used in prior work (Fulda et al. 2017;Zahavy et al. 2018).</p>
<p>Our baseline agent is the LSTM-DRQN agent (Yuan et al. 2018) but with a different action representation. We augment  1  4  2  2  8  2  7  4  3  15  3  7  4  3  15  4  9  8  4  50  5  11  15  5  141  6  12  20  6  283  7  12  20  7  295 this baseline with either or both score contextualisation and action gating, and observe the resulting effect on agent performance in SaladWorld. We measure this performance as the fraction of subtasks completed during an episode, averaged over time. In all cases, our results are generated from 5 independent trials of each condition. To smooth the results, we use moving average with a window of 20,000 training steps. The graphs and the histograms report average ± std. deviation across the trials. Score contextualisation uses K = 5 network heads; the baseline corresponds to K = 1. Each head is trained using the Adam optimizer (Kingma and Ba 2015) with a learning rate α = 0.001 to minimize a Q-learning loss (Mnih et al. 2015) with a discount factor of γ = 0.9. The auxiliary classifier Φ C is trained with the binary cross-entropy loss over the selected action's admissibility (recall that our agent only observes the admissibility function for the selected action). Training is done using a balanced form of prioritized replay which we found improves baseline performance appreciably. Specifically, we use the sampling mechanism described in Hausknecht and Stone (2015) with prioritization i.e we sample τ p fraction of episodes that had atleast one positive reward, τ n fraction with atleast one negative reward and 1 − τ p − τ n from whole episodic memory D. Section C.1 in the Appendix compares the baseline agent with and without prioritization. For prioritization, τ p = τ n = 0.25.</p>
<p>Actions are chosen from the estimated admissible setÂ t according to an -greedy rule, with annealed linearly from 1.0 to 0.1 over the first million training steps. To simplify exploration, our agent further takes a forced LOOK action every 20 steps. Each episode lasts for a maximum T steps. For Level 1 game, T = 100, whereas for rest of the levels T = 200.To simplify exploration, our agent further takes a forced LOOK action every 20 steps (Section C.1).Full details are given in the Appendix (Section A).</p>
<p>Score Contextualisation</p>
<p>We first consider the effect of score contextualisation on our agents' ability to complete tasks in SaladWorld. We ask, Does score contextualisation mitigate the negative effects of partial observability?</p>
<p>We begin in a simplified setting where the agent knows the admissible set A t . We call this setting oracle gating. This setting lets us focus on the impact of contextualisation alone. Following subtasks with reward and fulfilling condition:</p>
<p>• 10 points when the agent first enters the vegetable market.</p>
<p>• 5 points when the agent gets lettuce from the vegetable market and puts lettuce on the counter. All subtasks from previous level plus this subtask:</p>
<p>• 5 points when the agent takes the red key from the supermarket, goes to the playroom, opens the red door with the red key, gets the apple from cookhouse and puts it into the fridge in the kitchen. We compare our score contextualisation (SC) to the baseline and also to two "tabular" agents. The first tabular agent treats the most recent feedback as state, and hashes each unique description-action pair to a Q-value. This results in a memoryless scheme that ignores partial observability. The second tabular agent performs the information-gathering actions LOOK and INVENTORY to construct its state description, and also hashes these to unique Q-values. Accordingly, we call this the "LI-tabular" agent. This latter scheme has proved to be a successful heuristic in the design of IF agents (Fulda et al. 2017), but can be problematic in domains where taking information-gathering actions can have negative consequences (as is the case in ZORK). Figure 3 shows the performance of the four methods across SaladWorld levels, after 1.3 million training steps. We observe that the tabular agents' performance suffers as soon as there are multiple subtasks, as expected. The baseline agent performs well up to the third level, but then shows significantly reduced performance. We hypothesize that this occurs because the baseline agent must estimate the hidden state from longer history sequences and effectively learn an implicit contextualisation. Beyond the fourth level, the performance of all agents suffers, suggesting the need for a better exploration strategy, for example using expert data (Tessler et al. 2019).</p>
<p>We find that score contextualisation performs better than the baseline when the admissible set is unknown. Figure 4 compares learning curves of the SC and baseline agents with oracle gating and using the full action set, respectively, in the simplest of levels (Level 1 and 2). We find that score contextualisation can learn to solve these levels even without access to A t , whereas the baseline cannot. Our results also show that oracle gating simplifies the problem, and illustrate the value in handling inadmissible actions differently.</p>
<p>We hypothesize that score contextualisation results in a simpler learning problem in which the agent can more easily learn to distinguish which actions are relevant to the task, and hence facilitate credit assignment. Our result indicates that it might be unreasonable to expect contextualisation to arise naturally (or easily) in partially observable domains with large actions sets. We conclude that score contextualisation mitigates the negative effects of partial observability.</p>
<p>Score Contextualisation with Learned Action Gating</p>
<p>The previous experiment (in particular, Figure 4) shows the value of restricting action selection to admissible actions. With the goal in mind of designing an agent that can operate from feedback alone, we now ask:</p>
<p>0IZIP 0IZIP 0IZIP 0IZIP 0IZIP 0IZIP 0IZIP %ZKJVEGXMSRSJXSXEPWYFXEWOWWSPZIH )JJIGXSJ%GXMSR+EXMRK 1EWOMRK (VSTSYX '50, 2SKEXMRK Figure 5: Fraction of tasks solved by each method at the end of training for 1.3 million steps. Except in Level 1, action gating by itself does not improve end performance.</p>
<p>Masking</p>
<p>No gating Dropout CQLH Figure 6: Effectiveness of action gating with score contextualisation in Level 3. Of the three methods, masking performs best.</p>
<p>Can an agent learn more efficiently when given bandit feedback about the admissibility of its chosen actions? We address this question by comparing our three action gating mechanisms. As discussed in Section 3.2, the output of the auxiliary classifier describes our estimate of an action's admissibility for a given history.</p>
<p>As an initial point of comparison, we tested the performance of the baseline agent when using the auxiliary classifier's output to gate actions. For the masking method, we selected c = 0.001 from a larger initial parameter sweep. The results are summarized in Figure 5. While action gating alone provides some benefits in the first level, performance is equivalent for the rest of the levels.</p>
<p>However, when combined with score contextualisation (see Fig 6, 7), we observe some performance gains. In Level 3 in particular, we almost recover the performance of the SC agent with oracle gating. From our results we conclude that masking with the right threshold works best, but leave as an open question whether the other action gating schemes can be improved. Figure 8 shows the final comparison between the baseline LSTM-DRQN and our new agent architecture which incorporates action gating and score contextualisation (full learning curves are provided in the appendix, Figure 14). Our results show that the augmented method significantly outperforms the baseline, and is able to handle more complex IF domains. From level 4 onwards, the learning curves in the appendix show that combining score contextualisation with masking results in faster learning, even though final performance is unchanged. We posit that better exploration schemes are required for further progress in SaladWorld.</p>
<p>Zork</p>
<p>As a final experiment, we evaluate our agent architecture on the interactive fiction ZORK I, the first installment of the popular trilogy. ZORK provides an interesting point of comparison for our methods, as it is designed by and for humans -following the ontology of Bellemare et al. (2013), it is a domain which is both interesting and independent. Our main objective is to compare the different methods studied with Zahavy et al. (2018)'s AE-DQN agent. Following their experimental setup, we take γ = 0.8 and train for 2 million steps. All agents use the smaller action set (131 actions). Unlike AE-DQN, however, our agent does not use informationgathering actions (LOOK and INVENTORY) to establish the state. Figure 9 shows the corresponding learning curves. Despite operating in a harder regime than AE-DQN, the score contextualizing agent reaches a score comparable to AE-DQN, in about half of the training steps. All agents eventually fail to pass the 35-point benchmark, which corresponds to a particularly difficult in-game task (the "troll quest") which involves a timing element, and we hypothesize requires a more intelligent exploration strategy. 6 Related Work RL applied to Text Adventure games: LSTM-DQN by Narasimhan, Kulkarni, and Barzilay (2015) deals with parser-based text adventure games and uses an LSTM to generate feedback representation. The representation is then used by an action scorer to generate scores for the action verb and objects. The two scores are then averaged to determine Q-value for the state-action pair. In the realm of choice-based games, He et al. (2016) uses two separate deep neural nets to generate representation for feedback and action respectively. Q-values are calculated by dot-product of these representations. None of the above approaches deals with partial observability in text adventure games.</p>
<p>Admissible action set learning: Tao et al. (2018) approach the issue of learning admissible set given context as a supervised learning one. They train their model on (input, label) pairs where input is context (concatenation of feedbacks by LOOK and INVENTORY) and label is the list of admissible commands given this input. AE-DQN (Zahavy et al. 2018) employs an additional neural network to prune inadmissible actions from action set given a state. Although the paper doesn't deal with partial observability in text adventure games, authors show that having a tractable admissible action set led to faster convergence. Fulda et al. (2017) work on bounding the action set through affordances. Their agent is trained through tabular Q-Learning.</p>
<p>Partial Observability: Yuan et al. (2018) replace the shared MLP in Narasimhan, Kulkarni, and Barzilay (2015) with an LSTM cell to calculate context representation. However, they use concatenation of feedbacks by LOOK and IN-VENTORY as the given state to make the game more observable. Their work also doesn't focus on pruning in-admissible actions given a context. Finally, Ammanabrolu and Riedl (2019) deal with partial observability by representing state as a knowledge graph and continuously updating it after every game step. However, the graph update rules are handcoded; it would be interesting to see they can be learned during gameplay.</p>
<p>Conclusions and Future work</p>
<p>We introduced two algorithmic improvements for deep reinforcement learning applied to interactive fiction (IF). While naturally rooted in IF, we believe our ideas extend more generally to partially observable domains and large discrete action spaces. Our results on SaladWorld and ZORK show the usefulness of these improvements. Going forward, we believe better contextualisation mechanisms should yield further gains. In ZORK, in particular, we hypothesize that going beyond the 35-point limit will require more tightly coupling exploration with representation learning.</p>
<p>Acknowledgments</p>
<p>This work was funded by the CIFAR Learning in Machines and Brains program. Authors thank Compute Canada for providing the computational resources.</p>
<p>SC + Masking SC(oracle) Figure 10: Comparing the effect of oracle gating versus learning admissibility from bandit feedback. Learning is faster in case of oracle gating since agent is given admissible action set resulting in overall better credit assignment.</p>
<p>A Training Details</p>
<p>A.1 Hyper-parameters</p>
<p>Training hyper-parameters: For all the experiments unless specified, γ = 0.9. Weights for the learning agents are updated every 4 steps. Agents with score contextualisation architecture have K = 5 network heads. Parameters of score contextualisation architecture are learned end to end with Adam optimiser (Kingma and Ba 2015) with learning rate α = 0.001. To prevent imprecise updates for the initial states in the transition sequence due to in-sufficient history, we use updating mechanism proposed by Lample and Chaplot (2017). In this mechanism, considering the transition sequence of length l, o 1 , o 2 , . . . , o l , errors from o 1 , o 2 , . . . , o n aren't back-propagated through the network. In our case, the sequence length l = 15 and minimum history size for a state to be updated n = 6 for all experiments. Score contextualisation heads are trained to minimise the Q-learning loss over the whole transition sequence. On the other hand, Φ C minimises the BCE (binary cross-entropy) loss over the predicted admissibility probability and the actual admissibility signal for every transition in the transition sequence. The behavior policy during training is −greedy over the admissible setÂ t . Each episode lasts for a maximum T steps. For Level 1 game, we anneal = 1 to 0.1 over 1000000 steps and T = 100. For rest of the games in the suite, we anneal = 1 to 0.1 over 1000000 steps and T = 200. Architectural hyper-parameters: In Φ R , word embedding size is 20 and the number of hidden units in encoder LSTM is 64. For a network head k, the number of hidden units in context LSTM is 512; Φ A (k) is a two layer MLP: sizes of first and second layer are 128 and |A| respectively. Φ C has the same configuration as Φ A (k).</p>
<p>A.2 Action Gating Implementation</p>
<p>For dropout and masking when selecting actions, we set Q(h t , a t ) = −∞ for a / ∈Â t . Sinceξ t is basically an estimate for admissibility for action a given history h t , we use (5) to implement consistent Q value backups: Q(h t , a t ) ← 0IZIP 0IZIP 0IZIP 0IZIP 0IZIP 0IZIP 0IZIP %ZKSJXSXEPWYFXEWOWWSPZIH *MREPGSQTEVMWMSR 7'QEWOMRK &amp;EWIPMRI Figure 11: Final comparison shows that our algorithmic enhancements improve the baseline. We show number of tasks solved by each method at the end of training for 1.3 million steps.
Q(h t , a t ) + αδ t where δ t = r t + γ max a∈A Q(h t+1 , a)ξ t + Q(h t+1 , a t )(1 −ξ t ) − Q(h t , a t )
We notice by using the above equation, that for an action a inadmissible in s, it's value indeed reduces to 0 over time.</p>
<p>A.3 Baseline Modifications</p>
<p>We modify LSTM-DRQN  in two ways. First, we concatenate the representations Φ R (o t ) and Φ R (a t−1 ) before sending it to the history LSTM, in contrast Yuan et al. (2018) concatenates the inputs o t and a t−1 first and then generates Φ R ([o t ; a t−1 ]). Second, we modify the action scorer as action scorer in the LSTM-DRQN could only handle commands with two words.</p>
<p>B Notations and Algorithm</p>
<p>Following are the notations important to understand the algorithm:</p>
<p>• o t , r t , e t : observation (i.e. feedback), reward and admissibility signal received at time t. • a t : command executed in game-play at time t. • u t : cumulative rewared/score at time t. • Φ R : representation generator. • Φ C : auxiliary classifier.</p>
<p>• K : number of network heads in score contextualisation architecture. • J : dictionary mapping cumulative rewards to network heads. • H(k) : LSTM corresponding to network head k.</p>
<p>• Φ A (k) : Action scorer corresponding to network head k.</p>
<p>• h t : agent's context/history state at time t.</p>
<p>• T : maximum steps for an episode.</p>
<p>• p i : boolean that determines whether +ve reward was received in episode i.  • q i : boolean that determines whether -ve reward was received in episode i. • τ p : fraction of episodes where ∃t &lt; T : r t &gt; 0 • τ n : fraction of episodes where ∃t &lt; T : r t &lt; 0 • l : sequence length.</p>
<p>• n : minimum history size for a state to be updated. • A : action set. •Â t : admissible set generated at time t.</p>
<p>• I target : update interval for target network • : parameter for −greedy exploration strategy.</p>
<p>• 1 : softness parameter i.e. 1 fraction of timesÂ t = A. • c : threshold parameter for action elimination strategy Masking. • G max : maximum steps till which training is performed. Full training procedure is listed in Algorithm 1.</p>
<p>C More Empirical Analysis</p>
<p>C.1 Prioritised Sampling &amp; Infrequent LOOK</p>
<p>Our algorithm uses prioritised sampling and executes a LOOK action every I look = 20 steps. The baseline agent LSTM-DRQN follows this algorithm. We now ask, Does prioritised sampling and an infrequent LOOK play a significant role in the baseline's performance? For this experiment, we compare the Baseline to two agents. The first agent is the Baseline without prioritised sampling and the second is the one without an infrequent look. Accordingly, we call them "No-priority (NP)" and "No-LOOK (NL)" respectively. We use Zork as the testing domain.</p>
<p>From Fig 12, we observe that the Baseline performs better than the NP agent. This is because prioritised sampling helps the baseline agent to choose the episodes in which rewards are received in, thus assigning credit to the relevant states faster and overall better learning. In the same figure, the Baseline performs slightly better than the NL agent. We hypothesise that even though LOOK command is executed infrequently, it helps the agent in exploration and do credit assignment better. ACQLH CQLH Figure 13: Learning curves for CQLH ablation study.</p>
<p>C.2 CQLH</p>
<p>Our algorithm uses CQLH implementation as described in Section A.2. An important case that CQLH considers is s t+1 = s t . This manifests in (1 −ξ) term in equation (5). We now ask whether ignoring the case s t+1 = s t worsen the agent's performance? For this experiment, we compare CQLH agent with the agent which uses this error for update:
δ t =r t + γ max a∈A Q(h t+1 , a)ξ t − Q(h t , a t ).
Accordingly, we call this new agent as "alternate CQLH (ACQLH)" agent. We use Zork as testing domain. From Fig  13, we observe that although ACQLH has a simpler update rule, its performance seems more unstable compared to the CQLH agent.</p>
<p>Initialize score u 1 = 0, hidden State of H, h 0 = 0 and get start textual description o 1 and initial command a 0 = 'look'. Set p k ← 0, q k ← 0.  Figure 16: Game map shows the progression from Level 3 to Level 7 in terms of number of rooms and objects. Besides every object in the map, there is a tuple which shows in which levels the object is available. We observe that with successive levels, the number of rooms and objects increase making it more difficult to solve these levels.</p>
<p>Figure 1 :
1The introductory gameplay from ZORK.such as retrieving an important object or unlocking a new part of the domain.</p>
<p>ξ(s, a) := I [a∈ξ(s)] ξ(h, a) := Pr{a ∈ ξ(S)}, S ∼ B(· | h).</p>
<p>The use of additional context variables has by now been demonstrated in a number of settings (Rakelly et al. (2019); Icarte et al. (2018); Ghosh et al. (</p>
<p>from previous level plus this subtask:• 5 points when the agent takes the blue key from open space, opens the blue door, gets tomato from the supermarket and puts it on the counter in the kitchen. from level 1 plus this subtask: • 5 points when the agent takes the blue key from open space, goes to the garden, opens the blue door with the blue key, gets tomato from the supermarket and puts it on the counter in the kitchen. Remark: Level 3 game differs from Level 2 game in terms of number of steps required to complete the additional sub-task (which is greater in case of Level from previous level plus this subtask:• 5 points when the agent takes parsley from the backyard and knife from the cutlery shop to the kitchen, puts parsley into fridge and knife on the counter. from previous level plus this subtask: • 5 points when the agent goes to fruit shop, takes chest key, opens container with chest key, takes the banana from the chest and puts it into the fridge in the kitchen.</p>
<p>Figure 3 :Figure 4 :
34Fraction of tasks solved by each method at the end of training for 1.3 million steps. The tabular agents, which do not take history into account, perform quite poorly. LI stands for "look, inventory" (see text for details). Comparing whether score contextualisation as an architecture provides a useful representation for learning to act optimally. Row 1 and 2 correspond to Level 1 and 2 respectively.</p>
<p>Figure 7 :Figure 8 :
78Fraction of tasks solved by each method at the end of training for 1.3 million steps. For first 3 levels, SC + Masking is better or equivalent to SC. For levels 4 and beyond, better exploration strategies are required. Score contextualisation and masking compared to the baseline agent. We show the fraction of tasks solved by each method at the end of training for 1.3 million steps.</p>
<p>Figure 9 :
9Learning curves for different agents in Zork.</p>
<p>Figure 12 :
12Learning curves for baseline ablation study.</p>
<p>, h t ← ACT(o t , a t−1 , u t , h t−1 , J , , 1 , c, θ) 26: a t ← 'look' if t mod 20 == 0 27:Execute action a t , observe {r t+1 , o t+1 , e t+1 }.</p>
<p>Figure 15 :
15← 1 if r t &gt; 0; q k ← 1 if r t &lt; 0; u t+1 ← u t + r t29:Sample minibatch of transition sequences f 30:y b,: , E b,: ← TARGETS(f, γ, θ − ) 31: Perform gradient descent on L(θ) = j+l i=j+n−1 [y b,i − Q(h b,i , a i , u b,i ; θ) 2 + I [usingΦ C ] BCE(e i , E b,i )] 32:θ − ← θ if t mod I update == Game map shows Level 1 and 2. Simpler levels help us test the effectiveness of score contextualisation architecture.</p>
<p>Table 1 :
1Main characteristics of each level in our synthetic benchmark.LEVEL # ROOMS # OBJECTS # SUB-TASKS |A|</p>
<p>Table 2 :
2Subtasks information and scores possible for each level of the suite.Level 
Subtasks </p>
<p>Note that our definition technically differs from Zahavy et al.(2018)'s, who define an admissible action as one that is not ruled out by the learning algorithm.
Figure 14: Learning curves for Score contextualisation (SC : red), Score contextualisation + Masking (SC + Masking : blue) and Baseline (grey) for all the levels of the SaladWorld. For the simpler levels i.e. level 1 and 2, SC and SC + Masking perform better than Baseline. With difficult level 3, only SC + Masking solves the game. For levels 4 and beyond, we posit that better exploration strategies are required.
Playing text-adventure games with graph-based deep reinforcement learning. P Ammanabrolu, M Riedl, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics1Ammanabrolu, P., and Riedl, M. 2019. Playing text-adventure games with graph-based deep reinforcement learning. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 3557-3565. Minneapolis, Min- nesota: Association for Computational Linguistics.</p>
<p>The text-based adventure AI competition. T Atkinson, H Baier, T Copplestone, S Devlin, J Swan, IEEE Transactions on Games. Atkinson, T.; Baier, H.; Copplestone, T.; Devlin, S.; and Swan, J. 2018. The text-based adventure AI competi- tion. IEEE Transactions on Games.</p>
<p>The arcade learning environment: An evaluation platform for general agents. M G Bellemare, Y Naddaf, J Veness, M Bowling, Journal of Artificial Intelligence Research. 47253279Bellemare, M. G.; Naddaf, Y.; Veness, J.; and Bowl- ing, M. 2013. The arcade learning environment: An evalua- tion platform for general agents. Journal of Artificial Intel- ligence Research 47:253279.</p>
<p>Unifying countbased exploration and intrinsic motivation. M Bellemare, S Srinivasan, G Ostrovski, T Schaul, D Saxton, R Munos, Advances in Neural Information Processing Systems. Lee, D. D.Sugiyama, M.Luxburg, U. V.Guyon, I.and Garnett, R.Curran Associates, Inc29Bellemare, M.; Srinivasan, S.; Ostrovski, G.; Schaul, T.; Saxton, D.; and Munos, R. 2016a. Unifying count- based exploration and intrinsic motivation. In Lee, D. D.; Sugiyama, M.; Luxburg, U. V.; Guyon, I.; and Garnett, R., eds., Advances in Neural Information Processing Systems 29. Curran Associates, Inc. 1471-1479.</p>
<p>Increasing the action gap: New operators for reinforcement learning. M G Bellemare, G Ostrovski, A Guez, P S Thomas, R Munos, Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, AAAI'16. the Thirtieth AAAI Conference on Artificial Intelligence, AAAI'16AAAI PressBellemare, M. G.; Ostrovski, G.; Guez, A.; Thomas, P. S.; and Munos, R. 2016b. Increasing the action gap: New operators for reinforcement learning. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, AAAI'16, 1476-1483. AAAI Press.</p>
<p>Textworld: A learning environment for text-based games. M.-A Côté, A Kádár, X Yuan, B Kybartas, T Barnes, E Fine, J Moore, M Hausknecht, L E Asri, M Adada, W Tay, A Trischler, Côté, M.-A.; Kádár, A.; Yuan, X.; Kybartas, B.; Barnes, T.; Fine, E.; Moore, J.; Hausknecht, M.; Asri, L. E.; Adada, M.; Tay, W.; and Trischler, A. 2018. Textworld: A learning environment for text-based games. CoRR abs/1806.11532.</p>
<p>What can you do with a rock? affordance extraction via word embeddings. N Fulda, D Ricks, B Murdoch, D Wingate, Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence. the Twenty-Sixth International Joint Conference on Artificial IntelligenceFulda, N.; Ricks, D.; Murdoch, B.; and Wingate, D. 2017. What can you do with a rock? affordance extraction via word embeddings. Proceedings of the Twenty-Sixth In- ternational Joint Conference on Artificial Intelligence.</p>
<p>Divide-and-conquer reinforcement learning. D Ghosh, A Singh, A Rajeswaran, V Kumar, S Levine, Ghosh, D.; Singh, A.; Rajeswaran, A.; Kumar, V.; and Levine, S. 2018. Divide-and-conquer reinforcement learn- ing. ICLR 2018.</p>
<p>Deep recurrent q-learning for partially observable mdps. M Hausknecht, P Stone, AAAI Fall Symposium on Sequential Decision Making for Intelligent Agents (AAAI-SDMIA15). Hausknecht, M., and Stone, P. 2015. Deep recur- rent q-learning for partially observable mdps. In AAAI Fall Symposium on Sequential Decision Making for Intelligent Agents (AAAI-SDMIA15).</p>
<p>Deep reinforcement learning with a natural language action space. J He, J Chen, X He, J Gao, L Li, L Deng, M Ostendorf, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. the 54th Annual Meeting of the Association for Computational LinguisticsBerlin, GermanyAssociation for Computational Linguistics1He, J.; Chen, J.; He, X.; Gao, J.; Li, L.; Deng, L.; and Ostendorf, M. 2016. Deep reinforcement learning with a natural language action space. In Proceedings of the 54th Annual Meeting of the Association for Computational Lin- guistics (Volume 1: Long Papers), 1621-1630. Berlin, Ger- many: Association for Computational Linguistics.</p>
<p>Using reward machines for high-level task specification and decomposition in reinforcement learning. R T Icarte, T Q Klassen, R Valenzano, S A Mcilraith, Proceedings of the International Conference on Machine Learning. the International Conference on Machine LearningIcarte, R. T.; Klassen, T. Q.; Valenzano, R.; and McIl- raith, S. A. 2018. Using reward machines for high-level task specification and decomposition in reinforcement learning. In Proceedings of the International Conference on Machine Learning.</p>
<p>Adam: A method for stochastic optimization. D P Kingma, J Ba, International Conference on Learning Representations. ICLRKingma, D. P., and Ba, J. 2015. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR).</p>
<p>Playing fps games with deep reinforcement learning. G Lample, D S Chaplot, Thirty-First AAAI Conference on Artificial Intelligence. Lample, G., and Chaplot, D. S. 2017. Playing fps games with deep reinforcement learning. In Thirty-First AAAI Conference on Artificial Intelligence.</p>
<p>Reinforcement learning with selective perception and hidden state. A K Mccallum, Ph.D. DissertationUniversity of RochesterMcCallum, A. K. 1995. Reinforcement learning with selective perception and hidden state. Ph.D. Dissertation, University of Rochester.</p>
<p>Humanlevel control through deep reinforcement learning. V Mnih, K Kavukcuoglu, D Silver, A A Rusu, J Veness, M G Bellemare, A Graves, M Riedmiller, A K Fidjeland, G Ostrovski, Nature. 5187540529Mnih, V.; Kavukcuoglu, K.; Silver, D.; Rusu, A. A.; Veness, J.; Bellemare, M. G.; Graves, A.; Riedmiller, M.; Fidjeland, A. K.; Ostrovski, G.; et al. 2015. Human- level control through deep reinforcement learning. Nature 518(7540):529.</p>
<p>Language understanding for text-based games using deep reinforcement learning. K Narasimhan, T Kulkarni, R Barzilay, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingNarasimhan, K.; Kulkarni, T.; and Barzilay, R. 2015. Language understanding for text-based games using deep re- inforcement learning. Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing.</p>
<p>Gym retro. Openai, OpenAI. 2018. Gym retro. https://github.com/openai/ retro.</p>
<p>Efficient off-policy meta-reinforcement learning via probabilistic context variables. K Rakelly, A Zhou, D Quillen, C Finn, S Levine, Rakelly, K.; Zhou, A.; Quillen, D.; Finn, C.; and Levine, S. 2019. Efficient off-policy meta-reinforcement learning via probabilistic context variables. ICML 2019.</p>
<p>Monte-carlo planning in large pomdps. D Silver, J Veness, Advances in Neural Information Processing Systems. Silver, D., and Veness, J. 2010. Monte-carlo plan- ning in large pomdps. In Advances in Neural Information Processing Systems.</p>
<p>Reinforcement learning: An introduction. R S Sutton, A G Barto, MIT PressSutton, R. S., and Barto, A. G. 1998. Reinforcement learning: An introduction. MIT Press.</p>
<p>Towards solving text-based games by producing adaptive action spaces. R Y Tao, M.-A Côté, X Yuan, L E Asri, Tao, R. Y.; Côté, M.-A.; Yuan, X.; and Asri, L. E. 2018. Towards solving text-based games by producing adap- tive action spaces.</p>
<p>Sparse imitation learning for text based games with combinatorial action spaces. C Tessler, T Zahavy, D Cohen, D J Mankowitz, S Mannor, The Multidisciplinary Conference on Reinforcement Learning and Decision Making (RLDM). 2019Tessler, C.; Zahavy, T.; Cohen, D.; Mankowitz, D. J.; and Mannor, S. 2019. Sparse imitation learning for text based games with combinatorial action spaces. The Multi- disciplinary Conference on Reinforcement Learning and Decision Making (RLDM) 2019.</p>
<p>Learning from delayed rewards. C J C H Watkins, Cambridge, EnglandCambridge UniversityPh.D. DissertationWatkins, C. J. C. H. 1989. Learning from delayed rewards. Ph.D. Dissertation, Cambridge University, Cam- bridge, England.</p>
<p>Counting to explore and generalize in text-based games. X Yuan, M.-A Côté, A Sordoni, R Laroche, R T Des Combes, M Hausknecht, A Trischler, Yuan, X.; Côté, M.-A.; Sordoni, A.; Laroche, R.; des Combes, R. T.; Hausknecht, M.; and Trischler, A. 2018. Counting to explore and generalize in text-based games.</p>
<p>Learn what not to learn: Action elimination with deep reinforcement learning. T Zahavy, M Haroush, N Merlis, D J Mankowitz, S Mannor, Proceedings of the 32Nd International Conference on Neural Information Processing Systems, NIPS'18. the 32Nd International Conference on Neural Information Processing Systems, NIPS'18USACurran Associates IncZahavy, T.; Haroush, M.; Merlis, N.; Mankowitz, D. J.; and Mannor, S. 2018. Learn what not to learn: Action elimination with deep reinforcement learning. In Proceed- ings of the 32Nd International Conference on Neural In- formation Processing Systems, NIPS'18, 3566-3577. USA: Curran Associates Inc.</p>
<p>. ← Lstm H(k, w t , h t−1h t ← LSTM H(k)[w t , h t−1 ].</p>
<p>. ← Φ C, h tξ(h t , a; θ) ← Φ C (h t ).</p>
<p>o l , a l , r l+1 , e l+1 , u l+1 ) ← f ; h b,0 ← 0 11: Pass transition sequence through H to get h b,1 , h b,2 , . . . , h b,l 12: E b,i ← ξ(h b,i , a i ; θ − ) 13: y b,i ← max a∈A Q(h b,i+1 , a, u b,i+1 ; θ − ) 14: y b,i ← E b,i y b,i + (1 − E b,i ) Q(h b,i+1 , a i , u b,i+1 ; θ − ) if using CQLH. 15: y b,i ← r i+1 if o i is terminal else y b,i ← r i+1 + γy b,i 16: return y b,: , E b. else a t ← argmax a∈Ât Q(h t , a, u t ; θ) 7: return a t , h t 8: end function 9: function TARGETS(f, γ, θ − ) 10: (a 0. I look , I update , γ, 1 , , c, K, I2017: end function 18: 19: Input: G max. Initialize episodic replay memory D, global step counter G ← 0, dictionary J = {}. 21: Initialize parameters θ of the network, target network parameter θ − ← θ. 22: while G &lt; G max doGenerateÂ t (see Section 3.2). 6: With probability , a t ← Uniform(Â t ), else a t ← argmax a∈Ât Q(h t , a, u t ; θ) 7: return a t , h t 8: end function 9: function TARGETS(f, γ, θ − ) 10: (a 0 , o 1 , a 1 , r 2 , u 2 , e 2 , o 2 , . . . , o l , a l , r l+1 , e l+1 , u l+1 ) ← f ; h b,0 ← 0 11: Pass transition sequence through H to get h b,1 , h b,2 , . . . , h b,l 12: E b,i ← ξ(h b,i , a i ; θ − ) 13: y b,i ← max a∈A Q(h b,i+1 , a, u b,i+1 ; θ − ) 14: y b,i ← E b,i y b,i + (1 − E b,i ) Q(h b,i+1 , a i , u b,i+1 ; θ − ) if using CQLH. 15: y b,i ← r i+1 if o i is terminal else y b,i ← r i+1 + γy b,i 16: return y b,: , E b,: 17: end function 18: 19: Input: G max , I look , I update , γ, 1 , , c, K, I [usingΦ C ] , n 20: Initialize episodic replay memory D, global step counter G ← 0, dictionary J = {}. 21: Initialize parameters θ of the network, target network parameter θ − ← θ. 22: while G &lt; G max do 23:</p>            </div>
        </div>

    </div>
</body>
</html>
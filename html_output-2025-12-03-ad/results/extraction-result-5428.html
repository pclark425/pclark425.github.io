<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5428 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5428</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5428</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-109.html">extraction-schema-109</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <p><strong>Paper ID:</strong> paper-02cea70eb2c250682de6fade9486aefc0f746629</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/02cea70eb2c250682de6fade9486aefc0f746629" target="_blank">Tracking prototype and exemplar representations in the brain across learning</a></p>
                <p><strong>Paper Venue:</strong> eLife</p>
                <p><strong>Paper TL;DR:</strong> These findings indicate that, under the right circumstances, individuals may form representations at multiple levels of specificity, potentially facilitating a broad range of future decisions.</p>
                <p><strong>Paper Abstract:</strong> There is a long-standing debate about whether categories are represented by individual category members (exemplars) or by the central tendency abstracted from individual members (prototypes). Neuroimaging studies have shown neural evidence for either exemplar representations or prototype representations, but not both. Presently, we asked whether it is possible for multiple types of category representations to exist within a single task. We designed a categorization task to promote both exemplar and prototype representations and tracked their formation across learning. We found only prototype correlates during the final test. However, interim tests interspersed throughout learning showed prototype and exemplar representations across distinct brain regions that aligned with previous studies: prototypes in ventromedial prefrontal cortex and anterior hippocampus and exemplars in inferior frontal gyrus and lateral parietal cortex. These findings indicate that, under the right circumstances, individuals may form representations at multiple levels of specificity, potentially facilitating a broad range of future decisions.</p>
                <p><strong>Cost:</strong> 0.042</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5428.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5428.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prototype model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prototype model / prototype theory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Conceptual categories are represented functionally by an abstract central tendency (the prototype) that summarizes the most typical features of category members; classification of new items is based on their similarity to category prototypes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>On the genesis of abstract ideas</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Prototype model</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>Proposes that a category is represented by a single abstracted representation (the prototype) formed by integrating across exemplars; classification of a test item depends on its similarity to each category prototype and choosing the category with the higher prototype similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>Prototype / abstracted, feature-averaged representation</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>Abstraction (central tendency), compressed representation, discards exemplar-specific detail, supports broad generalization, typicality gradients (items closer to prototype classified more accurately).</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>In this study: behavioral typicality gradients (accuracy decreases with distance from prototype) and better overall behavioral model fits for the prototype model across phases; fMRI: VMPFC and anterior hippocampus tracked prototype model predictors (representational match) during learning and final test. Prior work cited (Posner & Keele 1968; Bowman & Zeithamova 2018) also found prototype correlates in VMPFC and hippocampus.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>Prototype model does not predict old-item advantages (training item advantage over new items at same distance), and exemplar-predicting neural signatures found in other regions during learning indicate prototype abstraction may coexist with exemplar memory; prototype vs exemplar behavioral fits can be correlated making dissociation difficult.</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>Category learning and generalization, recognition/classification tasks, concept formation and abstraction across repeated exposures.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Contrasted with exemplar models: prototype uses a single abstracted representation vs exemplar stores multiple individual items; prototype model better explains typicality gradients and often fits behavior well when categories are coherent; exemplar models better explain advantages for old/trained items. Paper shows both models can co-exist and neural data can dissociate them even when behavioral fits are similar.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Compute similarity between test item and prototype via exponential decay of feature distance (Shepard-like); attention weights modulate feature importance; produce trial-by-trial probability by normalizing prototype similarity against other category prototype similarities (softmax-like normalization). Prototype extraction is described functionally as integration across exemplars (memory integration) leading to an abstracted representation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>Does not explain advantages for exemplar-specific judgments; unclear whether VMPFC representations are strictly prototype-specific or mixed; dynamics across learning (when prototypes dominate vs coexistence with exemplars) remain unresolved; how prototypes are encoded mechanistically (e.g., via recurrent connections or as byproduct of exemplar retrieval) is an open question.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Tracking prototype and exemplar representations in the brain across learning', 'publication_date_yy_mm': '2020-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5428.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5428.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Exemplar model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Exemplar model / exemplar theory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Conceptual categories are functionally represented by stored individual exemplars; classification of new items is determined by summed similarity to stored exemplars (with attention weighting), and decisions rely on retrieving and comparing exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Attention, similarity, and the identification–categorization relationship</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Exemplar model</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>Proposes categories consist of memories of individual category members; classification of a novel item is based on its summed similarity to all stored exemplars in each category (more similar exemplars contribute more), often modulated by attention weights to features.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>Exemplar-based, episodic/memory-trace representations (multiple stored exemplars)</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>High-fidelity memory for individual instances, supports item-specific judgments (old-item advantage), flexible retrieval-driven classification, attention-weighted feature importance, similarity-based decision via summed exemplar similarities.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>In this study: behavioral evidence of old-item advantage during interim tests (training items > new items at same distance) and model-based fMRI showing exemplar predictors tracked in inferior frontal gyrus and lateral parietal cortex during learning (significant exemplar correlates). Prior neuroimaging (Mack et al., 2013) also found exemplar-tracking regions in lateral occipital, lateral prefrontal and lateral parietal cortices.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>Prototype model often fits behavior better overall (as in the final test here); exemplar correlates were weaker or absent in the final test in this study, suggesting exemplar representations can weaken over time or under some task framings; exemplar and prototype model predictions can be correlated, complicating behavioral dissociation.</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>Categorization and recognition tasks, tasks requiring specificity (discrimination among similar items), studies of memory specificity and recollection.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Contrasted with prototype models: exemplar stores multiple specific traces vs prototype compresses to a central tendency; exemplar explains training-item advantages and item-specific effects better; prototype explains typicality gradients more compactly. The paper demonstrates they can co-exist and be supported by distinct brain regions.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Similarity computed for each test item to each exemplar using exponential distance transform with attention weights; category evidence is sum of exemplar similarities for each category; decision probability is normalized ratio of summed similarities (softmax/relative-similarity rule). Retrieval of exemplars and interference resolution mechanisms (IFG) support exemplar-based categorization.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>How exemplar representations evolve across learning and consolidation (e.g., weakening vs persistence) is unresolved; the conditions that determine when exemplar vs prototype representations dominate are not fully specified; interaction with attention processes and selective weighting across perceptual features needs further clarification.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Tracking prototype and exemplar representations in the brain across learning', 'publication_date_yy_mm': '2020-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5428.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5428.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ALCOVE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ALCOVE: an exemplar-based connectionist model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A connectionist implementation of exemplar theory that formalizes exemplar storage, attention learning (feature weights), and generalization via activation and gradient-based parameter learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ALCOVE: an exemplar-based connectionist model of category learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>ALCOVE (exemplar-based connectionist model)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>A computational, connectionist exemplar model that implements exemplar storage and attention-weight learning via associative network dynamics and error-driven learning; uses exemplar activations (similarity-transformed) to drive category responses and updates feature attention through learning.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>Exemplar-based, connectionist/distributed with attention-weight adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>Stores exemplars as reference points within a similarity space; attention weights adapt through learning to emphasize diagnostic dimensions; gradient-based parameter updating; explains shifts in feature attention and exemplar influence on categorization.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>Cited as formal exemplar implementation (Kruschke, 1992). The paper references formal models (including exemplar formulations like ALCOVE) used to generate behavioral predictions and to parameterize model-based fMRI regressors (attention weights w and sensitivity c employed in similarity functions).</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>Connectionist exemplar implementations can produce similar behavioral fits to prototype models under many stimuli configurations; disentangling attention-weight effects from prototype abstraction remains challenging; not directly fit as a separate connectionist instantiation in this paper (only standard exemplar similarity formulation used).</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>Formal analyses of categorization, simulations of learning-related attention shifts, accounting for exemplar effects and rule-plus-exception phenomena.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>ALCOVE operationalizes exemplar theory with learned attention weights and gradient learning, giving explicit mechanisms for attention and exemplar influence, compared to prototype models which compress input to a summary representation; ALCOVE can account for exemplar-specific advantages and attentional modulation.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Similarity-based exemplar activation transformed by an exponential decay; attention weights are learned via error-driven learning rules to maximize categorization accuracy; summed exemplar activations feed response selection mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>How connectionist exemplar learning scales to naturalistic categories and integrates with systems that form abstractions (prototypes/schemas) is open; neural mapping of ALCOVE components to brain systems (which regions implement attention-weight learning vs exemplar storage) needs further elucidation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Tracking prototype and exemplar representations in the brain across learning', 'publication_date_yy_mm': '2020-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5428.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5428.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Complementary Learning Systems</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Complementary Learning Systems (CLS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Framework proposing complementary roles: hippocampus stores specific episodic exemplars rapidly while neocortex gradually learns integrated, abstracted representations, accounting for both specificity and generalization in memory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Why there are complementary learning systems in the Hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Complementary Learning Systems (CLS)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>Functional-level theory that memory involves two complementary systems: a fast, episodic system (hippocampus) that stores individual experiences (supporting exemplar-like representations) and a slow, integrative neocortical system that abstracts across experiences to form generalized knowledge (supporting prototype/schema formation).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>Dual-system: episodic exemplar traces (hippocampal) + integrated/abstracted neocortical representations (prototype/schema)</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>Division of labor by timescale and function, rapid encoding of specifics vs slow integration enabling abstraction, interactions during consolidation and retrieval-mediated learning, explains coexistence of specificity and generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>Cited in paper as motivation: hippocampus and VMPFC tracked prototype predictors (integration/abstraction) while hippocampal subregions and cortical areas tracked exemplar predictors (specificity); matches CLS prediction that separate systems support different representational formats. Paper replicates VMPFC & anterior hippocampus prototype signals consistent with integration and CLS ideas.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>CLS predicts a time-dependent shift from specific to abstract representations; in this study, both representations emerged in parallel during learning (no strict trade-off), and exemplar signals weakened by the final test, suggesting more complex dynamics than a simple competition or unidirectional shift.</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>Memory consolidation, category learning, episodic inference, generalization, retrieval-mediated learning and schema formation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>CLS provides a systems-level account reconciling exemplar and prototype accounts by allocating them to different learning systems; contrasts with single-process exemplar or prototype-only theories by positing complementary mechanisms and timescales.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Rapid hippocampal encoding of episodes (exemplars) that can be replayed to neocortex over time to support gradual extraction of structured knowledge (prototypes/schemas); retrieval-mediated integration may accelerate abstraction.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>Exact mapping between CLS components and observed cortical ROIs (e.g., VMPFC as neocortical integrator) needs more evidence; CLS's predictions about temporal dynamics (when exemplars vs prototypes dominate) are not fully validated here as both representations can form in parallel.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Tracking prototype and exemplar representations in the brain across learning', 'publication_date_yy_mm': '2020-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5428.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5428.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Schema abstraction / Multiple-trace</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>"Schema abstraction" in a multiple-trace memory model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multiple-trace framework proposing that abstract representations (schemas/prototypes) arise from accumulation and interaction of multiple episodic traces rather than single-instance encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>"Schema abstraction" in a multiple-trace memory model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Schema abstraction / multiple-trace model</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>Functional account where abstraction (schemas/prototypes) emerges from multiple stored traces through processes of similarity-based retrieval and integration across experiences; prototypes may be byproducts of recurring exemplar retrievals that coalesce into abstracted representations.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>Multiple episodic traces (distributed exemplar traces) that give rise to emergent abstract/schema representations</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>Abstraction arises from accumulation of traces, allows coexistence of exemplar-specific and abstracted knowledge, supports both specificity and generalization, emphasis on emergent summary representations without erasing exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>Discussed as plausible mechanism in paper (Hintzman 1986 cited): prototype representations may emerge as byproduct of retrieving exemplars and become encoded via recurrent connections; aligns with observed co-existence of exemplar and prototype correlates during learning.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>Difficult to distinguish whether prototypes are separate stored entities or emergent from exemplar retrieval; neural evidence here supports both separate region correlates and possible emergent processes, leaving mechanistic details unresolved.</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>Category learning, episodic memory integration, schema formation, generalization tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Differs from pure prototype model by explaining prototype emergence as an outcome of multiple exemplar traces rather than a separate dedicated abstract store; bridges exemplar and prototype accounts by offering a process-level route from episodic traces to abstraction.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Retrieval of related exemplars leads to integrative encoding (e.g., via hippocampal recurrence) that can instantiate abstracted features or prototypes; recurrent and consolidation processes increase robustness of abstracted representations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>Mechanistic details of how and when exemplar retrieval produces stable prototype representations are underspecified; how this process maps onto distinct brain regions (separate representations vs emergent coding in same regions) requires further work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Tracking prototype and exemplar representations in the brain across learning', 'publication_date_yy_mm': '2020-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Decoding the brain's algorithm for categorization from its neural implementation <em>(Rating: 2)</em></li>
                <li>Abstract memory representations in the ventromedial prefrontal cortex and Hippocampus support concept generalization <em>(Rating: 2)</em></li>
                <li>ALCOVE: an exemplar-based connectionist model of category learning <em>(Rating: 2)</em></li>
                <li>Attention, similarity, and the identification–categorization relationship <em>(Rating: 2)</em></li>
                <li>On the genesis of abstract ideas <em>(Rating: 2)</em></li>
                <li>Why there are complementary learning systems in the Hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory <em>(Rating: 2)</em></li>
                <li>"Schema abstraction" in a multiple-trace memory model <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5428",
    "paper_id": "paper-02cea70eb2c250682de6fade9486aefc0f746629",
    "extraction_schema_id": "extraction-schema-109",
    "extracted_data": [
        {
            "name_short": "Prototype model",
            "name_full": "Prototype model / prototype theory",
            "brief_description": "Conceptual categories are represented functionally by an abstract central tendency (the prototype) that summarizes the most typical features of category members; classification of new items is based on their similarity to category prototypes.",
            "citation_title": "On the genesis of abstract ideas",
            "mention_or_use": "use",
            "theory_or_model_name": "Prototype model",
            "theory_or_model_description": "Proposes that a category is represented by a single abstracted representation (the prototype) formed by integrating across exemplars; classification of a test item depends on its similarity to each category prototype and choosing the category with the higher prototype similarity.",
            "representation_format_type": "Prototype / abstracted, feature-averaged representation",
            "key_properties": "Abstraction (central tendency), compressed representation, discards exemplar-specific detail, supports broad generalization, typicality gradients (items closer to prototype classified more accurately).",
            "empirical_support": "In this study: behavioral typicality gradients (accuracy decreases with distance from prototype) and better overall behavioral model fits for the prototype model across phases; fMRI: VMPFC and anterior hippocampus tracked prototype model predictors (representational match) during learning and final test. Prior work cited (Posner & Keele 1968; Bowman & Zeithamova 2018) also found prototype correlates in VMPFC and hippocampus.",
            "empirical_challenges": "Prototype model does not predict old-item advantages (training item advantage over new items at same distance), and exemplar-predicting neural signatures found in other regions during learning indicate prototype abstraction may coexist with exemplar memory; prototype vs exemplar behavioral fits can be correlated making dissociation difficult.",
            "applied_domains_or_tasks": "Category learning and generalization, recognition/classification tasks, concept formation and abstraction across repeated exposures.",
            "comparison_to_other_models": "Contrasted with exemplar models: prototype uses a single abstracted representation vs exemplar stores multiple individual items; prototype model better explains typicality gradients and often fits behavior well when categories are coherent; exemplar models better explain advantages for old/trained items. Paper shows both models can co-exist and neural data can dissociate them even when behavioral fits are similar.",
            "functional_mechanisms": "Compute similarity between test item and prototype via exponential decay of feature distance (Shepard-like); attention weights modulate feature importance; produce trial-by-trial probability by normalizing prototype similarity against other category prototype similarities (softmax-like normalization). Prototype extraction is described functionally as integration across exemplars (memory integration) leading to an abstracted representation.",
            "limitations_or_open_questions": "Does not explain advantages for exemplar-specific judgments; unclear whether VMPFC representations are strictly prototype-specific or mixed; dynamics across learning (when prototypes dominate vs coexistence with exemplars) remain unresolved; how prototypes are encoded mechanistically (e.g., via recurrent connections or as byproduct of exemplar retrieval) is an open question.",
            "uuid": "e5428.0",
            "source_info": {
                "paper_title": "Tracking prototype and exemplar representations in the brain across learning",
                "publication_date_yy_mm": "2020-11"
            }
        },
        {
            "name_short": "Exemplar model",
            "name_full": "Exemplar model / exemplar theory",
            "brief_description": "Conceptual categories are functionally represented by stored individual exemplars; classification of new items is determined by summed similarity to stored exemplars (with attention weighting), and decisions rely on retrieving and comparing exemplars.",
            "citation_title": "Attention, similarity, and the identification–categorization relationship",
            "mention_or_use": "use",
            "theory_or_model_name": "Exemplar model",
            "theory_or_model_description": "Proposes categories consist of memories of individual category members; classification of a novel item is based on its summed similarity to all stored exemplars in each category (more similar exemplars contribute more), often modulated by attention weights to features.",
            "representation_format_type": "Exemplar-based, episodic/memory-trace representations (multiple stored exemplars)",
            "key_properties": "High-fidelity memory for individual instances, supports item-specific judgments (old-item advantage), flexible retrieval-driven classification, attention-weighted feature importance, similarity-based decision via summed exemplar similarities.",
            "empirical_support": "In this study: behavioral evidence of old-item advantage during interim tests (training items &gt; new items at same distance) and model-based fMRI showing exemplar predictors tracked in inferior frontal gyrus and lateral parietal cortex during learning (significant exemplar correlates). Prior neuroimaging (Mack et al., 2013) also found exemplar-tracking regions in lateral occipital, lateral prefrontal and lateral parietal cortices.",
            "empirical_challenges": "Prototype model often fits behavior better overall (as in the final test here); exemplar correlates were weaker or absent in the final test in this study, suggesting exemplar representations can weaken over time or under some task framings; exemplar and prototype model predictions can be correlated, complicating behavioral dissociation.",
            "applied_domains_or_tasks": "Categorization and recognition tasks, tasks requiring specificity (discrimination among similar items), studies of memory specificity and recollection.",
            "comparison_to_other_models": "Contrasted with prototype models: exemplar stores multiple specific traces vs prototype compresses to a central tendency; exemplar explains training-item advantages and item-specific effects better; prototype explains typicality gradients more compactly. The paper demonstrates they can co-exist and be supported by distinct brain regions.",
            "functional_mechanisms": "Similarity computed for each test item to each exemplar using exponential distance transform with attention weights; category evidence is sum of exemplar similarities for each category; decision probability is normalized ratio of summed similarities (softmax/relative-similarity rule). Retrieval of exemplars and interference resolution mechanisms (IFG) support exemplar-based categorization.",
            "limitations_or_open_questions": "How exemplar representations evolve across learning and consolidation (e.g., weakening vs persistence) is unresolved; the conditions that determine when exemplar vs prototype representations dominate are not fully specified; interaction with attention processes and selective weighting across perceptual features needs further clarification.",
            "uuid": "e5428.1",
            "source_info": {
                "paper_title": "Tracking prototype and exemplar representations in the brain across learning",
                "publication_date_yy_mm": "2020-11"
            }
        },
        {
            "name_short": "ALCOVE",
            "name_full": "ALCOVE: an exemplar-based connectionist model",
            "brief_description": "A connectionist implementation of exemplar theory that formalizes exemplar storage, attention learning (feature weights), and generalization via activation and gradient-based parameter learning.",
            "citation_title": "ALCOVE: an exemplar-based connectionist model of category learning",
            "mention_or_use": "mention",
            "theory_or_model_name": "ALCOVE (exemplar-based connectionist model)",
            "theory_or_model_description": "A computational, connectionist exemplar model that implements exemplar storage and attention-weight learning via associative network dynamics and error-driven learning; uses exemplar activations (similarity-transformed) to drive category responses and updates feature attention through learning.",
            "representation_format_type": "Exemplar-based, connectionist/distributed with attention-weight adaptation",
            "key_properties": "Stores exemplars as reference points within a similarity space; attention weights adapt through learning to emphasize diagnostic dimensions; gradient-based parameter updating; explains shifts in feature attention and exemplar influence on categorization.",
            "empirical_support": "Cited as formal exemplar implementation (Kruschke, 1992). The paper references formal models (including exemplar formulations like ALCOVE) used to generate behavioral predictions and to parameterize model-based fMRI regressors (attention weights w and sensitivity c employed in similarity functions).",
            "empirical_challenges": "Connectionist exemplar implementations can produce similar behavioral fits to prototype models under many stimuli configurations; disentangling attention-weight effects from prototype abstraction remains challenging; not directly fit as a separate connectionist instantiation in this paper (only standard exemplar similarity formulation used).",
            "applied_domains_or_tasks": "Formal analyses of categorization, simulations of learning-related attention shifts, accounting for exemplar effects and rule-plus-exception phenomena.",
            "comparison_to_other_models": "ALCOVE operationalizes exemplar theory with learned attention weights and gradient learning, giving explicit mechanisms for attention and exemplar influence, compared to prototype models which compress input to a summary representation; ALCOVE can account for exemplar-specific advantages and attentional modulation.",
            "functional_mechanisms": "Similarity-based exemplar activation transformed by an exponential decay; attention weights are learned via error-driven learning rules to maximize categorization accuracy; summed exemplar activations feed response selection mechanisms.",
            "limitations_or_open_questions": "How connectionist exemplar learning scales to naturalistic categories and integrates with systems that form abstractions (prototypes/schemas) is open; neural mapping of ALCOVE components to brain systems (which regions implement attention-weight learning vs exemplar storage) needs further elucidation.",
            "uuid": "e5428.2",
            "source_info": {
                "paper_title": "Tracking prototype and exemplar representations in the brain across learning",
                "publication_date_yy_mm": "2020-11"
            }
        },
        {
            "name_short": "Complementary Learning Systems",
            "name_full": "Complementary Learning Systems (CLS)",
            "brief_description": "Framework proposing complementary roles: hippocampus stores specific episodic exemplars rapidly while neocortex gradually learns integrated, abstracted representations, accounting for both specificity and generalization in memory.",
            "citation_title": "Why there are complementary learning systems in the Hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory",
            "mention_or_use": "mention",
            "theory_or_model_name": "Complementary Learning Systems (CLS)",
            "theory_or_model_description": "Functional-level theory that memory involves two complementary systems: a fast, episodic system (hippocampus) that stores individual experiences (supporting exemplar-like representations) and a slow, integrative neocortical system that abstracts across experiences to form generalized knowledge (supporting prototype/schema formation).",
            "representation_format_type": "Dual-system: episodic exemplar traces (hippocampal) + integrated/abstracted neocortical representations (prototype/schema)",
            "key_properties": "Division of labor by timescale and function, rapid encoding of specifics vs slow integration enabling abstraction, interactions during consolidation and retrieval-mediated learning, explains coexistence of specificity and generalization.",
            "empirical_support": "Cited in paper as motivation: hippocampus and VMPFC tracked prototype predictors (integration/abstraction) while hippocampal subregions and cortical areas tracked exemplar predictors (specificity); matches CLS prediction that separate systems support different representational formats. Paper replicates VMPFC & anterior hippocampus prototype signals consistent with integration and CLS ideas.",
            "empirical_challenges": "CLS predicts a time-dependent shift from specific to abstract representations; in this study, both representations emerged in parallel during learning (no strict trade-off), and exemplar signals weakened by the final test, suggesting more complex dynamics than a simple competition or unidirectional shift.",
            "applied_domains_or_tasks": "Memory consolidation, category learning, episodic inference, generalization, retrieval-mediated learning and schema formation.",
            "comparison_to_other_models": "CLS provides a systems-level account reconciling exemplar and prototype accounts by allocating them to different learning systems; contrasts with single-process exemplar or prototype-only theories by positing complementary mechanisms and timescales.",
            "functional_mechanisms": "Rapid hippocampal encoding of episodes (exemplars) that can be replayed to neocortex over time to support gradual extraction of structured knowledge (prototypes/schemas); retrieval-mediated integration may accelerate abstraction.",
            "limitations_or_open_questions": "Exact mapping between CLS components and observed cortical ROIs (e.g., VMPFC as neocortical integrator) needs more evidence; CLS's predictions about temporal dynamics (when exemplars vs prototypes dominate) are not fully validated here as both representations can form in parallel.",
            "uuid": "e5428.3",
            "source_info": {
                "paper_title": "Tracking prototype and exemplar representations in the brain across learning",
                "publication_date_yy_mm": "2020-11"
            }
        },
        {
            "name_short": "Schema abstraction / Multiple-trace",
            "name_full": "\"Schema abstraction\" in a multiple-trace memory model",
            "brief_description": "A multiple-trace framework proposing that abstract representations (schemas/prototypes) arise from accumulation and interaction of multiple episodic traces rather than single-instance encoding.",
            "citation_title": "\"Schema abstraction\" in a multiple-trace memory model",
            "mention_or_use": "mention",
            "theory_or_model_name": "Schema abstraction / multiple-trace model",
            "theory_or_model_description": "Functional account where abstraction (schemas/prototypes) emerges from multiple stored traces through processes of similarity-based retrieval and integration across experiences; prototypes may be byproducts of recurring exemplar retrievals that coalesce into abstracted representations.",
            "representation_format_type": "Multiple episodic traces (distributed exemplar traces) that give rise to emergent abstract/schema representations",
            "key_properties": "Abstraction arises from accumulation of traces, allows coexistence of exemplar-specific and abstracted knowledge, supports both specificity and generalization, emphasis on emergent summary representations without erasing exemplars.",
            "empirical_support": "Discussed as plausible mechanism in paper (Hintzman 1986 cited): prototype representations may emerge as byproduct of retrieving exemplars and become encoded via recurrent connections; aligns with observed co-existence of exemplar and prototype correlates during learning.",
            "empirical_challenges": "Difficult to distinguish whether prototypes are separate stored entities or emergent from exemplar retrieval; neural evidence here supports both separate region correlates and possible emergent processes, leaving mechanistic details unresolved.",
            "applied_domains_or_tasks": "Category learning, episodic memory integration, schema formation, generalization tasks.",
            "comparison_to_other_models": "Differs from pure prototype model by explaining prototype emergence as an outcome of multiple exemplar traces rather than a separate dedicated abstract store; bridges exemplar and prototype accounts by offering a process-level route from episodic traces to abstraction.",
            "functional_mechanisms": "Retrieval of related exemplars leads to integrative encoding (e.g., via hippocampal recurrence) that can instantiate abstracted features or prototypes; recurrent and consolidation processes increase robustness of abstracted representations.",
            "limitations_or_open_questions": "Mechanistic details of how and when exemplar retrieval produces stable prototype representations are underspecified; how this process maps onto distinct brain regions (separate representations vs emergent coding in same regions) requires further work.",
            "uuid": "e5428.4",
            "source_info": {
                "paper_title": "Tracking prototype and exemplar representations in the brain across learning",
                "publication_date_yy_mm": "2020-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Decoding the brain's algorithm for categorization from its neural implementation",
            "rating": 2
        },
        {
            "paper_title": "Abstract memory representations in the ventromedial prefrontal cortex and Hippocampus support concept generalization",
            "rating": 2
        },
        {
            "paper_title": "ALCOVE: an exemplar-based connectionist model of category learning",
            "rating": 2
        },
        {
            "paper_title": "Attention, similarity, and the identification–categorization relationship",
            "rating": 2
        },
        {
            "paper_title": "On the genesis of abstract ideas",
            "rating": 2
        },
        {
            "paper_title": "Why there are complementary learning systems in the Hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory",
            "rating": 2
        },
        {
            "paper_title": "\"Schema abstraction\" in a multiple-trace memory model",
            "rating": 2
        }
    ],
    "cost": 0.04246775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><!doctype html>

<html lang="en" prefix="og: http://ogp.me/ns#">

<head>

    <meta charset="utf-8">

    <title>Tracking prototype and exemplar representations in the brain across learning | eLife</title>

    <meta name="viewport" content="width=device-width,initial-scale=1,shrink-to-fit=no">

    <meta name="format-detection" content="telephone=no">

                <script id="Cookiebot" src="https://consent.cookiebot.com/uc.js" data-cbid="0a5c50d8-fcf9-47b1-8f4f-1eaadb13941b" type="text/javascript"></script>

    <script>
        document.querySelector('html').classList.add('js');
    </script>

    <style>
                @font-face{font-display:fallback;font-family:"Noto Sans";src:url(/assets/patterns/fonts/NotoSans-Regular-webfont-custom-2-subsetting.6031b15b.woff2) format("woff2")}@font-face{font-display:fallback;font-family:"Noto Sans";src:url(/assets/patterns/fonts/NotoSans-SemiBold-webfont-custom-2-subsetting.8b9e80d5.woff2) format("woff2");font-weight:700}@font-face{font-display:fallback;font-family:"Noto Serif";src:url(/assets/patterns/fonts/NotoSerif-Regular-webfont-custom-2-subsetting.e6069232.woff2) format("woff2")}@font-face{font-display:fallback;font-family:"Noto Serif";src:url(/assets/patterns/fonts/NotoSerif-Bold-webfont-basic-latin-subsetting.592fd0d5.woff2) format("woff2");font-weight:700}html{font-family:sans-serif;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0}aside,header,main,nav,section{display:block}picture{display:inline-block;vertical-align:baseline}a{background-color:transparent}b{font-weight:inherit}b{font-weight:bolder}h1{font-size:2em;margin:.67em 0}img{border:0}svg:not(:root){overflow:hidden}button,input{font:inherit;margin:0}button{overflow:visible}button{text-transform:none}button{-webkit-appearance:button}button::-moz-focus-inner,input::-moz-focus-inner{border:0;padding:0}button:-moz-focusring,input:-moz-focusring{outline:ButtonText dotted 1px}input{line-height:normal}input[type=checkbox]{box-sizing:border-box;padding:0}input[type=search]{-webkit-appearance:textfield}input[type=search]::-webkit-search-cancel-button,input[type=search]::-webkit-search-decoration{-webkit-appearance:none}fieldset{border:1px solid silver;margin:0 2px;padding:.35em .625em .75em}*,:after,:before{box-sizing:border-box}body,html{height:100%}body{background-color:#fff;color:#212121;text-rendering:optimizeLegibility}h1{font-family:"Noto Sans",Arial,Helvetica,sans-serif;font-weight:700;font-size:2.25rem;line-height:1.33333;font-size:2.25rem;margin:0}h2{font-family:"Noto Sans",Arial,Helvetica,sans-serif;font-weight:700;font-size:1.625rem;line-height:1.15385;margin:0;padding-bottom:21px;padding-bottom:1.3125rem;padding-top:21px;padding-top:1.3125rem}h6{font-family:"Noto Sans",Arial,Helvetica,sans-serif;font-weight:700;font-size:1rem;line-height:1.5;margin:0;padding-top:10px;padding-top:.625rem;padding-bottom:14px;padding-bottom:.875rem}p{font-family:"Noto Serif",serif;font-size:1rem;line-height:1.5;font-weight:400;margin:0;margin-bottom:24px;margin-bottom:1.5rem}a{color:#087acc;text-decoration:none}p a:not(.additional-asset__link--download):not(.asset-viewer-inline__download_all_link):not(.asset-viewer-inline__open_link):not(.reference__title):not(.doi):not(.reference__authors_link):not(.trigger):not(.popup__link):not(.see-more-link){border-bottom:1px dotted #212121;color:#212121;text-decoration:none}b{font-weight:700}ol,ul{margin-bottom:24px;margin-bottom:1.5rem;margin-top:0;padding-left:48px;padding-left:3rem}dl{margin-bottom:24px;margin-bottom:1.5rem;margin-top:0}dd,dt,li{font-family:"Noto Serif",serif;font-size:1rem;line-height:1.5;font-weight:400}dt{font-weight:700}dd{margin-left:0}.hidden{display:none}.visuallyhidden{border:0;clip:rect(0 0 0 0);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}.clearfix{zoom:1}.clearfix:after,.clearfix:before{content:"";display:table}.clearfix:after{clear:both}.global-inner:after{content:"";display:block;clear:both}img{max-height:100%;max-width:100%}input[type=checkbox]{margin-right:6px;margin-right:.375rem}::-webkit-input-placeholder{color:#bdbdbd}::-moz-placeholder{color:#bdbdbd}:-ms-input-placeholder{color:#bdbdbd}:-moz-placeholder{color:#bdbdbd}.grid-column{margin-bottom:48px;margin-bottom:3rem}@media only all and (min-width:45.625em){.grid-column{margin-bottom:72px;margin-bottom:4.5rem}}.grid-secondary-column__item{margin-bottom:48px;margin-bottom:3rem}@media only all and (min-width:45.625em){.grid-secondary-column__item{margin-bottom:72px;margin-bottom:4.5rem}}.wrapper{box-sizing:content-box;margin:auto;max-width:1114px;max-width:69.625rem;padding-left:24px;padding-right:24px}@media only screen and (min-width:30em){.wrapper{padding-left:48px;padding-right:48px}}.wrapper.wrapper--content{padding-top:48px;padding-top:3rem}.content-header-image-wrapper+.wrapper.wrapper--listing,.content-header-simple+.wrapper.wrapper--listing{padding-top:0}.grid{list-style:none;margin:0;padding:0;margin-left:-1.6%;margin-right:-1.6%;zoom:1}.grid:after,.grid:before{content:"";display:table}.grid:after{clear:both}.grid__item{float:left;padding-left:1.6%;padding-right:1.6%;width:100%;box-sizing:border-box}@media only screen and (min-width:900px){.large--eight-twelfths{min-height:1px;width:66.666%}.large--ten-twelfths{min-height:1px;width:83.333%}.push--large--one-twelfth{left:8.333%}}@media only screen and (min-width:1200px){.x-large--two-twelfths{min-height:1px;width:16.666%}.x-large--seven-twelfths{min-height:1px;width:58.333%}.x-large--eight-twelfths{min-height:1px;width:66.666%}.push--x-large--zero{left:0}.push--x-large--two-twelfths{left:16.666%}}.altmetric-container-without-details{padding-top:30px;padding-top:1.875rem;display:none;gap:10px;-ms-flex-pack:justify;justify-content:space-between}@media only screen and (min-width:62.5em){.altmetric-container-without-details{display:-ms-flexbox;display:flex}:root{--banner-min-height:var(--banner-min-height--wider)}}.altmetric-embed{margin-bottom:30px;margin-bottom:1.875rem;padding-top:15px;padding-top:.9375rem}div.altmetric-container-without-details.altmetric-container-without-details.altmetric-container-without-details.altmetric-container-without-details.altmetric-container-without-details.altmetric-container-without-details.altmetric-container-without-details p.altmetric-text--small{font-family:"Noto Sans",Arial,Helvetica,sans-serif;font-size:.875rem;line-height:1.42857}div.altmetric-container-without-details.altmetric-container-without-details.altmetric-container-without-details.altmetric-container-without-details.altmetric-container-without-details.altmetric-container-without-details.altmetric-container-without-details p.altmetric-text--small a.altmetric-link--small{border-bottom:none;color:#212121;font-weight:700}.altmetric-embed:empty+.altmetric-text--small{display:none}.breadcrumbs{margin:0;margin:0;padding:0;padding:0}.breadcrumbs .breadcrumb-item{font-size:.875rem;line-height:1.71429;margin:0;margin:0;padding:0;padding:0;display:inline;font-family:"Noto Sans",Arial,Helvetica,sans-serif;list-style:none}.breadcrumbs .breadcrumb-item:after{content:" | "}.breadcrumbs .breadcrumb-item:last-child:after{content:""}.breadcrumbs .breadcrumb-item .breadcrumb-item__link{color:#212121}.button{border:none;border-radius:4px;color:#fff;display:inline-block;font-family:"Noto Sans",Arial,Helvetica,sans-serif;font-size:.875rem;line-height:1;font-weight:500;padding:17px 40px 16px;padding:1.0625rem 2.5rem 1rem;text-align:center;text-decoration:none;text-transform:uppercase}.button--default{background-color:#087acc;border:1px solid #087acc;color:#fff;padding:15px 36px 14px;padding:.9375rem 2.25rem .875rem}.button--secondary{background-color:#f7f7f7;border:1px solid #e0e0e0;color:#212121;padding:15px 36px 14px;padding:.9375rem 2.25rem .875rem}.button--extra-small{border-radius:3px;font-size:.6875rem;line-height:2.1818181818;padding:0 6px;padding:0 .375rem;height:24px;height:1.5rem}.button--action{padding:9px 20px;padding:.5625rem 1.25rem;background-color:#edeff4;border:none;color:#212121;display:-ms-inline-flexbox;display:inline-flex;font-weight:700;min-height:40px;line-height:24px;text-transform:initial}.button--action.icon{padding-left:40px;padding-left:2.5rem;background-position:11px;background-repeat:no-repeat}.button--action.icon-comment{background-image:url(/assets/patterns/img/icons/commenting.1b2facba.svg)}.button--action.icon-citation{background-image:url(/assets/patterns/img/icons/cite.5f3c4fab.svg)}.button--action.icon-download{background-image:url(/assets/patterns/img/icons/download.ecfa2d98.svg)}.button--action.icon-share{background-image:url(/assets/patterns/img/icons/share.5cbd86ce.svg)}.date{font-size:.875rem;line-height:1.71429;font-family:"Noto Sans",Arial,Helvetica,sans-serif;font-weight:400;color:inherit;font-size:.75rem;line-height:2}.definition-list--timeline dd,.definition-list--timeline dt{padding-left:28px;padding-left:1.75rem;font-family:"Noto Sans",Arial,Helvetica,sans-serif;font-weight:700;position:relative}.definition-list--timeline dt{font-size:1rem;line-height:1.5;margin-top:18px;margin-top:1.125rem}.definition-list--timeline dt:first-child{margin-top:0;margin-top:0}.definition-list--timeline dd{font-size:.875rem;line-height:1.71429;color:#757575}.definition-list--timeline dd:before,.definition-list--timeline dt:before{background-color:#edeff4;content:"";height:100%;left:0;position:absolute;top:0;width:8px}.definition-list--timeline dt:before{border-radius:6px 6px 0 0}.definition-list--timeline dd:before{border-radius:0 0 6px 6px}.doi{font-size:.875rem;line-height:1.71429;font-family:"Noto Sans",Arial,Helvetica,sans-serif;font-weight:400;color:#757575}.doi a.doi__link{border-bottom:none;color:#757575;text-decoration:none;text-transform:none}.doi--article-section{color:#212121;display:block;font-size:.875rem;margin-bottom:24px;margin-bottom:1.5rem}.doi--article-section a.doi__link{color:#212121}.form-item{margin-bottom:24px;margin-bottom:1.5rem}.form-item>:last-child{margin-bottom:0}@supports (display:flex){.form-item__label_container{display:-ms-flexbox;display:flex;-ms-flex-pack:justify;justify-content:space-between}}.form-item__label{display:block;color:#212121;font-family:"Noto Sans",Arial,Helvetica,sans-serif;font-size:.875rem;line-height:1.42857;font-weight:700;text-align:left;margin-bottom:4px;margin-bottom:.25rem}.info-bar{background-color:#087acc;color:#fff;font-family:"Noto Sans",Arial,Helvetica,sans-serif;font-size:.875rem;font-weight:600;padding:14px 8px;padding:.875rem .5rem;text-align:center}.info-bar__text{background-position:0 50%;background-repeat:no-repeat;display:inline-block;padding:2px 0 0 28px;padding:.125rem 0 0 1.75rem;font-size:.875rem;line-height:1.3}.info-bar--warning .info-bar__text{padding-left:0;padding-left:0}.info-bar--announcement{padding:0;padding:0;background-color:#f5f7fa;color:#212121}.info-bar--announcement a{color:#087acc;font-weight:600;text-decoration:none}.info-bar--announcement .info-bar__container{padding:28px;padding:1.75rem;margin-bottom:24px;margin-bottom:1.5rem}.info-bar--announcement .info-bar__text{padding-top:0;padding-top:0;background:url(/assets/patterns/img/icons/info-blue.857b9409.svg) no-repeat,linear-gradient(transparent,transparent);font-weight:400;line-height:1.7;text-align:left}.info-bar--info .info-bar__text{background-image:url(/assets/patterns/img/icons/info.96b39fb1.png);background-image:url(/assets/patterns/img/icons/info.6662c626.svg),linear-gradient(transparent,transparent)}.info-bar--multiple-versions{background-color:#087acc}.info-bar--multiple-versions .info-bar__text{background-image:url(/assets/patterns/img/icons/multiple-versions-article.336e926c.png);background-image:url(/assets/patterns/img/icons/multiple-versions-article.96fdc4b8.svg),linear-gradient(transparent,transparent);background-size:17px}.info-bar--success{background-color:#629f43}.info-bar--success .info-bar__text{background-image:url(/assets/patterns/img/icons/confirmed.79ce7ac4.png);background-image:url(/assets/patterns/img/icons/confirmed.c30e8709.svg),linear-gradient(transparent,transparent)}.info-bar--attention{background-color:#cf0c4e}.info-bar--attention .info-bar__text{background-image:url(/assets/patterns/img/icons/attention.de73644c.png);background-image:url(/assets/patterns/img/icons/attention.77e4036b.svg),linear-gradient(transparent,transparent)}.info-bar--correction{background-color:#cf0c4e}.info-bar--correction .info-bar__text{background-image:url(/assets/patterns/img/icons/corrected-article.7be00f7a.png);background-image:url(/assets/patterns/img/icons/corrected-article.9cd6ecfe.svg),linear-gradient(transparent,transparent);background-size:17px}.info-bar--dismissible{padding-top:24px;padding-top:1.5rem;box-sizing:content-box;margin:auto;max-width:1114px;max-width:69.625rem;padding-left:24px;padding-right:24px;background-color:inherit;max-width:1114px}@media only screen and (min-width:30em){.info-bar--dismissible{padding-left:48px;padding-right:48px}}@media only screen and (min-width:56.25em){.info-bar--dismissible:not(.hidden){display:-ms-grid;display:grid;-ms-grid-columns:(1fr)[12];grid-template-columns:repeat(12,1fr)}:root{--banner-min-height:var(--banner-min-height--wide)}}.js .info-bar--dismissible:not([data-behaviour-initialised]){display:none}.info-bar--dismissible .info-bar__container{padding:14px 15px 9px;padding:.875rem .9375rem .5625rem;background-color:#f5f7fa;color:#212121;display:-ms-flexbox;display:flex;grid-column:2/12}@media only screen and (min-width:75em){.info-bar--dismissible .info-bar__container{grid-column:1/13}}.info-bar--dismissible .info-bar__text{font-size:.875rem;line-height:1.71429;padding:0 8px 0 30px;padding:0 .5rem 0 1.875rem;background:url(/assets/patterns/img/icons/info-blue.857b9409.svg) no-repeat,linear-gradient(transparent,transparent);background-size:24px;font-weight:400;text-align:left;width:100%}.info-bar--dismissible .info-bar__text a{padding-left:2px;padding-left:.125rem;color:#087acc;font-weight:600;text-decoration:none}.info-bar--dismissible .dismiss-button{padding:5px;padding:.3125rem;background:url(/assets/patterns/img/icons/close.f00467a1.svg) center no-repeat;border:0;border-radius:2px;display:inline-block;float:right;height:24px;text-indent:-10000px;width:24px}.info-bar--dismissible .dismiss-button:hover{background-color:rgba(0,0,0,.15)}.info-bar--warning{background-color:#d14600}.main-menu .list-heading{padding-left:0;padding-right:0;padding-top:24px;padding-top:1.5rem;padding-bottom:24px;padding-bottom:1.5rem;text-align:center}.js .main-menu .list-heading{border:0;clip:rect(0 0 0 0);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}.media-source__fallback_link{font-family:"Noto Sans",Arial,Helvetica,sans-serif;font-size:.875rem;text-decoration:none}.modal-container{background-color:rgba(255,255,255,.9);height:100%;left:0;position:fixed;top:0;width:100%;visibility:hidden}.modal-content{background-color:#fff;border-radius:3px;box-shadow:0 0 20px rgba(0,0,0,.22);display:-ms-grid;display:grid;font-family:"Noto Sans",Arial,Helvetica,sans-serif;margin:0;padding:36px}@media only all and (min-width:30em){.modal-content{margin:24px}}@media only all and (min-width:48.625em){.modal-content{margin:180px auto 24px;max-width:730px}}.modal-content h6{font-size:1rem;line-height:1.5;padding-bottom:6px;padding-bottom:.375rem;grid-column:1/3;-ms-grid-row:1;grid-row:1;justify-self:start;padding-top:0}.modal-content .social-media-sharers{margin-bottom:0;margin-bottom:0;display:-ms-flexbox;display:flex;grid-column:1/3;-ms-grid-row:4;grid-row:4;-ms-flex-pack:center;justify-content:center}@media only all and (min-width:45.625em){.modal-content .social-media-sharers{-ms-grid-row:unset;grid-row:unset;-ms-flex-pack:left;justify-content:left}.modal-content .social-media-sharers li{margin:0 4px;margin:0 .25rem}.modal-content .social-media-sharers li:first-child{margin-left:0;margin-left:0}.modal-content .social-media-sharers li:first-child a{margin-left:0;margin-left:0}}.modal-content .reference{font-family:"Noto Sans",Arial,Helvetica,sans-serif;grid-column:1/3}.modal-content .reference .doi{margin-top:12px;margin-top:.75rem;margin-bottom:36px;margin-bottom:2.25rem;display:-ms-flexbox;display:flex}.modal-content .button-collection{display:unset;-ms-flex-direction:column;flex-direction:column;grid-column:1/3}.modal-content .button-collection .button-collection__item{float:none}.modal-content .button-collection .button{margin-bottom:12px;margin-bottom:.75rem;width:100%}@media only all and (min-width:45.625em){.modal-content .button-collection{grid-column:1/3}.modal-content .button-collection .button-collection__item{float:left}.modal-content .button-collection .button{margin-bottom:12px;margin-bottom:.75rem;padding:15px 24px 14px;padding:.9375rem 1.5rem .875rem;line-height:1.5;text-decoration:none;width:auto}}.modal-content__body{margin-top:30px;margin-top:1.875rem;display:-ms-grid;display:grid;grid-column:1/3;-ms-grid-columns:1fr 224px;grid-template-columns:1fr 224px}.modal-content__body .button{margin-bottom:36px;margin-bottom:2.25rem;justify-self:center;grid-column:1/3;-ms-grid-row:3;grid-row:3}.modal-content__body .button.button--default{padding:15px 18px;padding:.9375rem 1.125rem}.modal-content__body .form-item{margin-bottom:36px;margin-bottom:2.25rem;grid-column:1/3}@media only all and (min-width:45.625em){.modal-content__body .button.button--default{justify-self:right;grid-column:2/3;-ms-grid-row:unset;grid-row:unset}.modal-content__body .form-item{grid-column:1/2}}.modal-content__close-button{padding:0 25px 0 0;padding:0 1.5625rem 0 0;background:url(/assets/patterns/img/icons/close.f00467a1.svg) right 4px no-repeat;color:#212121;font-family:"Noto Sans",Arial,Helvetica,sans-serif;font-weight:700;grid-column:2/3;-ms-grid-row:1;grid-row:1;justify-self:end}.modal-nojs{visibility:hidden}.reference__title{font-family:"Noto Sans",Arial,Helvetica,sans-serif;font-size:1rem;line-height:1.5;font-weight:700;display:inline;text-decoration:none}.reference__authors_list{display:inline;list-style:none;margin:0;padding:0}.reference__author{font-size:1rem;line-height:1.5;display:inline;font-family:"Noto Sans",Arial,Helvetica,sans-serif;margin:0;padding:0}.reference__author:after{content:", "}.reference__author:last-of-type:after{content:""}.reference__authors_list_suffix{font-family:"Noto Sans",Arial,Helvetica,sans-serif;font-size:1rem;line-height:1.5}.reference__origin{font-family:"Noto Sans",Arial,Helvetica,sans-serif;font-size:1rem;line-height:1.5;font-size:1rem;line-height:1.5;display:inline;padding-bottom:0}.reference .doi__reference-spacing{margin-bottom:6px;margin-bottom:.375rem}.see-more-link{display:block;color:#212121;font-family:"Noto Sans",Arial,Helvetica,sans-serif;font-size:1rem;line-height:1.5;text-decoration:none}.social-media-sharers{-ms-flex-positive:0;flex-grow:0;-ms-flex-preferred-size:24px;flex-basis:24px}.content-header-journal .social-media-sharers,.modal-content .social-media-sharers{-ms-flex-preferred-size:auto;flex-basis:auto;margin:0;margin:0;margin-bottom:12px;margin-bottom:.75rem;padding:0;padding:0;-ms-flex-pack:center;justify-content:center}.content-header-journal .social-media-sharers li,.modal-content .social-media-sharers li{margin-left:0;margin-left:0;margin-right:0;margin-right:0;list-style:none}.content-header-journal .social-media-sharers li:last-child,.modal-content .social-media-sharers li:last-child{margin-right:0}.content-header__one-column-container .social-media-sharers{margin-bottom:0;margin-bottom:0}.social-media-sharer{display:inline-block}.social-media-sharer{background-color:#212121;border-radius:3px;color:#fff;margin:0 8px;height:24px;padding:2px 0;text-decoration:none;width:24px}.content-header-journal .social-media-sharer,.modal-content .social-media-sharer{background-color:transparent;border-radius:0;margin:0 9px}.content-header-journal .social-media-sharer:hover svg path{fill:#087acc}.content-header--image .social-media-sharer{background-color:transparent;border:1px solid #fff;padding:1px 0}.content-header:not(.content-header--image):not(.content-header-journal) .social-media-sharer:active,.content-header:not(.content-header--image):not(.content-header-journal) .social-media-sharer:hover{background-color:#087acc}.speech-bubble{background-color:#087acc;border:1px solid #087acc;color:#fff;border-radius:3px;display:block;font-family:"Noto Sans",Arial,Helvetica,sans-serif;font-size:.875rem;line-height:2.57143;height:36px;height:2.25rem;padding:0;position:relative;text-align:center;text-decoration:none;width:42px;width:2.625rem}.speech-bubble[data-behaviour~=HypothesisOpener]{display:none}.speech-bubble:after{border-style:solid;border-width:20px;border-color:transparent;border-left-color:#087acc;border-right-width:0;content:"";height:0;width:0;left:8px;position:absolute;top:8px;z-index:-1}.speech-bubble__inner{display:inline-block}.speech-bubble--wrapped{font-size:.6875rem;line-height:1.36364;padding:13px 18px 14px;padding:.8125rem 1.125rem .875rem;display:block;font-weight:300;height:auto;min-width:2em;width:auto;justify-self:end}.speech-bubble--wrapped .speech-bubble--wrapped__prefix{font-size:.875rem;line-height:1.28571;display:inline-block;font-family:"Noto Sans",Arial,Helvetica,sans-serif;font-weight:300;text-transform:uppercase}.speech-bubble--wrapped .speech-bubble__inner{margin-left:9px;margin-left:.5625rem;padding:3px 5px 0;padding:.1875rem .3125rem 0;font-size:.875rem;line-height:1.28571;background:#fff;border-radius:3px;color:#0769b0;font-weight:700;letter-spacing:.2px;min-width:26px;position:relative}.speech-bubble--wrapped .speech-bubble__inner:after{background:url(/assets/patterns/img/icons/speech-bubble-wrapped-triangle.65674a5a.svg) no-repeat;content:"";width:10px;height:10px;bottom:-10px;left:5px;position:absolute}.speech-bubble--has-placeholder.speech-bubble--wrapped .speech-bubble__inner{background:url(/assets/patterns/img/icons/speech-bubble-wrapped-placeholder.2fee63c5.svg) 8px 5px no-repeat #fff;text-indent:-9999px}.text-field,.text-field[type=text]{-webkit-appearance:none;-moz-appearance:none;appearance:none}.text-field{border:1px solid #e0e0e0;border-radius:3px;color:#212121;display:block;font-family:"Noto Sans",Arial,Helvetica,sans-serif;font-size:1rem;line-height:1.5;padding:11px 12px;padding:.6875rem .75rem;margin-bottom:6px;margin-bottom:.375rem;width:100%}.js .main-menu .to-top-link{display:none}.tabbed-navigation:not(.hidden)~.main-content-grid .article-section{scroll-margin-top:72px}.tabbed-navigation:not(.hidden)~.main-content-grid .article-section .article-download-links-list__heading{scroll-margin-top:72px}.article-section--first{border:none;padding-top:0}.article-section--first .article-section__header:first-child h2{margin-top:0;padding-top:0}.wrapper--content-with-header-and-aside .article-section--first .article-section__header:first-child h2{padding-top:21px;padding-top:1.3125rem}.article-section__header{position:relative}.article-section__header_text{color:#212121;margin:0;-ms-flex:1 0 80%;flex:1 0 80%;text-decoration:none}.article-section__body{font-family:"Noto Serif",serif;font-size:1rem;line-height:1.5;font-weight:400}.divider{border-bottom:1px solid #e0e0e0;grid-column:2/12;height:1px}@media only all and (min-width:75em){.divider{grid-column:1/13}.content-header__one-column .divider{grid-column:3/11;margin-left:-24px;margin-right:-24px}}@media only all and (min-width:45.625em){.main-content-grid{grid-column:2/12;-ms-grid-row:2;grid-row:2;min-width:0}.global-wrapper.social-media-page--wrapper .main .content-header:not(.content-header-journal){display:-ms-grid;display:grid;-ms-grid-columns:(1fr)[12];grid-template-columns:repeat(12,1fr);grid-column-gap:24px}}@media only all and (min-width:56.25em){.main-content-grid{grid-column:2/9}}@media only all and (min-width:75em){.main-content-grid{margin-left:48px;margin-left:3rem;grid-column:3/10;-ms-grid-row:1;grid-row:1}.global-wrapper.social-media-page--wrapper .main .content-header:not(.content-header-journal)>*{margin-left:0;margin-left:0;padding-right:0;padding-right:0}}@media only all and (min-width:45.625em){.global-wrapper.social-media-page--wrapper .main .content-header:not(.content-header-journal)>*{grid-column:2/12}.global-wrapper.social-media-page--wrapper .main .content-header:not(.content-header-journal) .main-content-grid,.global-wrapper.social-media-page--wrapper .main .content-header:not(.content-header-journal) .secondary-column-grid{margin-top:0;margin-top:0;-ms-grid-row:auto;grid-row:auto}}@media only all and (min-width:75em){.global-wrapper.social-media-page--wrapper .main .content-header:not(.content-header-journal) .main-content-grid{grid-column:3/9;grid-row:2/5;margin-right:-24px}.global-wrapper.social-media-page--wrapper .main .content-header:not(.content-header-journal) .social-media-sharers--wrapper~.main-content-grid{grid-column:3/9}.global-wrapper.social-media-page--wrapper .main .content-header:not(.content-header-journal) .social-media-sharers,.global-wrapper.social-media-page--wrapper .main .content-header:not(.content-header-journal) .social-media-sharers--wrapper{display:block}.global-wrapper.social-media-page--wrapper .main .content-header:not(.content-header-journal) .social-media-sharers--wrapper{grid-column:10/11;grid-row:2/3;justify-self:end}.global-wrapper.social-media-page--wrapper .main .content-header:not(.content-header-journal) .social-media-sharers--wrapper .svg-background-image{margin-left:8px;margin-left:.5rem}.global-wrapper.social-media-page--wrapper .main .content-header:not(.content-header-journal) .side-section-wrapper__link{display:block}.global-wrapper.social-media-page--wrapper .main .content-header:not(.content-header-journal) .secondary-column-grid{grid-column:10/13;grid-row:2/3}.global-wrapper.social-media-page--wrapper .main .content-header:not(.content-header-journal) .social-media-sharers--wrapper~.secondary-column-grid{grid-row:3/5}}.authors{margin-bottom:24px;margin-bottom:1.5rem}.content-header-grid__main .authors{margin:0 0 36px}@media only all and (max-width:45.563rem){.content-header-journal .authors{display:contents}.authors .institution_list{border:0;clip:rect(0 0 0 0);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}}.author_list{margin:0;padding:0}@media only all and (max-width:45.625em){.content-header-journal .author_list{padding:0}}.content-header-grid__main .author_list{padding:0 0 24px}.author_list_item{display:inline;font-family:"Noto Sans",Arial,Helvetica,sans-serif;font-size:1rem;line-height:1.5;list-style-type:none;padding:0;text-align:center}.author_list_item:first-of-type:before{content:""}.author_list_item:before{content:", "}.author_suffix{white-space:nowrap}li.institution_list_item:last-child .institution_separator{display:none}.author_link{color:inherit;text-decoration:inherit}.author_icon{padding-top:1px;vertical-align:text-top}.institution_list{margin:0;padding:0}.institution_list_item{display:inline;font-family:"Noto Sans",Arial,Helvetica,sans-serif;font-size:.875rem;line-height:1.71429;font-weight:500;list-style-type:none;padding:0}.button-collection{display:table;margin:0;padding:0;margin-bottom:24px;margin-bottom:1.5rem;margin-top:-24px;margin-top:-1.5rem}@supports (display:flex){.button-collection{display:-ms-flexbox;display:flex;-ms-flex-wrap:wrap;flex-wrap:wrap;-ms-flex-align:center;align-items:center}}.button-collection.button-collection--inline{margin-top:0;margin-top:0;display:inline-block}.button-collection.button-collection--inline .button-collection__item{margin-top:0;margin-top:0;margin-right:12px;margin-right:.75rem;position:relative}.button-collection.button-collection--inline .button-collection__item:last-child{margin-right:0;margin-right:0}.button-collection__item{float:left;list-style:none;margin-top:24px;margin-top:1.5rem;margin-right:10px;margin-right:.625rem}.compact-form__container{border:none;margin:0 auto;max-width:440px;max-width:27.5rem;padding:0;position:relative}.search-box__inner .compact-form__container{max-width:none}.compact-form__input{background-color:#fff;border:1px solid #e0e0e0;border-right:none;border-radius:3px;display:block;font-family:"Noto Sans",Arial,Helvetica,sans-serif;font-size:1rem;line-height:1.5;padding:11px 55px 11px 12px;padding:.6875rem 3.4375rem .6875rem .75rem;width:100%}.compact-form__submit{background:url(/assets/patterns/img/icons/arrow-forward.7f8fc46b.png);background:url(/assets/patterns/img/icons/arrow-forward.663dc5c2.svg),linear-gradient(transparent,transparent);background-color:#087acc;background-position:50% 50%;background-repeat:no-repeat;border:none;border-radius:0 3px 3px 0;color:#fff;height:48px;height:3rem;position:absolute;right:0;top:0;width:47px;width:2.9375rem}.compact-form__reset{border:0;clip:rect(0 0 0 0);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}.contextual-data{font-size:.875rem;line-height:1.71429;font-family:"Noto Sans",Arial,Helvetica,sans-serif;font-weight:400;color:#757575;font-size:.75rem;line-height:2}.content-header .contextual-data{border-bottom:none}.content-header__one-column .contextual-data{display:-ms-flexbox;display:flex}.content-header__one-column .contextual-data:after{margin-left:3px;margin-left:.1875rem;margin-right:3px;margin-right:.1875rem;content:"\00a0\2022\00a0";font-weight:700;white-space:pre}.contextual-data__list{border-bottom:1px solid #e0e0e0;margin:0;padding:11px 0;padding:.6875rem 0;text-align:center}.content-header .contextual-data__list{border-bottom:none;padding:0;padding:0;text-align:right}.content-aside .contextual-data__list{margin-bottom:24px;margin-bottom:1.5rem;padding:0;padding:0;border:none;display:-ms-flexbox;display:flex}.content-aside .contextual-data__list .contextual-data__item{padding:0;padding:0;font-size:.875rem;line-height:1.71429}.content-aside .contextual-data__list .contextual-data__item:before{content:", "}.content-aside .contextual-data__list .contextual-data__item:first-child:before{content:""}.contextual-data__item{display:inline-block;font-size:.875rem;line-height:1.71429;font-family:"Noto Sans",Arial,Helvetica,sans-serif;font-weight:400;color:#757575;font-size:.75rem;line-height:2;margin:0;padding:0 5px 0 0;padding:0 .3125rem 0 0}.contextual-data__item .contextual-data__counter,.contextual-data__item a{color:inherit}.contextual-data__item a:hover{color:#087acc}.content-header .contextual-data__item{font-size:.875rem;line-height:1.71429;display:list-item;padding:0;list-style:none}.contextual-data__item__hypothesis_opener{display:none}.js .contextual-data__item__hypothesis_opener{color:#087acc;display:inline-block}.contextual-data__cite_wrapper{border-bottom:1px solid #e0e0e0;padding-top:12px;padding-top:.75rem;padding-bottom:11px;padding-bottom:.6875rem;padding-left:0;padding-right:0;text-align:center}.contextual-data__cite{display:none}.contextual-data__counter{color:#212121;font-weight:700}.content-header__footer .contextual-data__counter{font-weight:400}@media only screen and (min-width:56.25rem){.contextual-data{border-bottom:1px solid #e0e0e0}.contextual-data__list{-ms-flex-item-align:center;-ms-grid-row-align:center;align-self:center;border-bottom:none;display:inline-block;text-align:left}.content-header__one-column .contextual-data__list{-ms-flex-item-align:auto;-ms-grid-row-align:auto;align-self:auto}.contextual-data__cite_wrapper{border-bottom:none;float:right;margin-left:auto;padding:11px 0;padding:.6875rem 0;text-align:start}.contextual-data__cite{-ms-flex-item-align:center;-ms-grid-row-align:center;align-self:center;display:inline-block;-ms-flex:1;flex:1;text-align:right;padding:0 5px 0 0;padding:0 .3125rem 0 0}}.highlight-item{max-width:1114px;max-width:69.625rem;margin-bottom:24px;margin-bottom:1.5rem;color:#212121;display:-ms-grid;display:grid;grid-auto-flow:dense;-ms-grid-columns:6fr 6fr;grid-template-columns:6fr 6fr;list-style-type:none;overflow:hidden;position:relative}@media only screen and (min-width:30em){.highlight-item{margin-bottom:36px;margin-bottom:2.25rem}}.highlight-item__meta .meta{font-size:.875rem;line-height:1.71429;color:inherit}@media only all and (min-width:45.625em){.highlight-item__meta .meta{display:-ms-flexbox;display:flex}}.highlight-item__meta .meta__type:hover{color:inherit}:root{--site-header-height:5.9375rem;--subjects-height--narrow:4.75rem;--subjects-height--medium:6.375rem;--subjects-height--x-wide:22.75rem;--subjects-height:var(--subjects-height--narrow);--subjects-height-js:var(--subjects-height);--banner-min-height--narrow:17.25rem;--banner-min-height--medium:21.125rem;--banner-min-height--wide:21.6875rem;--banner-min-height--wider:29.375rem;--banner-min-height:var(--banner-min-height--narrow);--max-height-of-banner-and-subjects-and-site-header--narrow:33.75rem;--max-height-of-banner-and-subjects-and-site-header:var(--max-height-of-banner-and-subjects-and-site-header--narrow);--max-height-of-banner-and-subjects-and-site-header-js:var(--max-height-of-banner-and-subjects-and-site-header);--min-height-of-banner-and-subjects-and-site-header:calc(var(--banner-min-height) + var(--subjects-height) + var(--site-header-height));--min-height-of-banner-and-subjects-and-site-header-js:calc(var(--banner-min-height) + var(--subjects-height-js) + var(--site-header-height));--max-height-of-banner-and-subjects:calc(var(--max-height-of-banner-and-subjects-and-site-header) - var(--site-header-height));--max-height-of-banner-and-subjects-js:calc(var(--max-height-of-banner-and-subjects-and-site-header-js) - var(--site-header-height));--min-height-of-banner-and-subjects:calc(var(--min-height-of-banner-and-subjects-and-site-header) - var(--site-header-height));--min-height-of-banner-and-subjects-js:calc(var(--min-height-of-banner-and-subjects-and-site-header-js) - var(--site-header-height))}@media only screen and (min-width:45.625em){.highlight-item{display:-ms-grid;display:grid;grid-auto-flow:column;-ms-grid-columns:8fr 4fr;grid-template-columns:8fr 4fr}:root{--subjects-height:var(--subjects-height--medium);--banner-min-height:var(--banner-min-height--medium)}}@media only screen and (min-width:75em){.highlight-item{margin-bottom:0;margin-bottom:0;grid-auto-flow:row;-ms-grid-columns:auto;grid-template-columns:auto}:root{--subjects-height-js:var(--subjects-height--x-wide);--max-height-of-banner-and-subjects-js:62.5rem}}.jump-menu__wrapper{margin-top:24px;margin-top:1.5rem;background-color:#fff}.jump-menu__list{border-left:2px solid #edeff4;list-style:none;margin:0;padding:0;position:-webkit-sticky;position:sticky;top:110px}.jump-menu__item{margin:0;margin:0;margin-bottom:24px;margin-bottom:1.5rem;font-size:.875rem;line-height:1.71429;font-family:"Noto Sans",Arial,Helvetica,sans-serif}.jump-menu{padding-left:18px;padding-left:1.125rem;color:#757575;display:block;text-decoration:none}.wrapper--content .jump-menu__wrapper{display:none}.main-menu__section{padding-bottom:15px;padding-bottom:.9375rem}.main-menu__title{font-family:"Noto Sans",Arial,Helvetica,sans-serif;font-size:.875rem;line-height:1;font-weight:700;text-transform:uppercase;margin:0;padding:0;padding-bottom:5px;padding-bottom:.3125rem;text-transform:uppercase}.main-menu__title-container{padding-top:30px;padding-top:1.875rem;padding-bottom:30px;padding-bottom:1.875rem}@media only screen and (min-width:56.25em){:root{--site-header-height:7.4375rem}.main-menu__title-container{padding-top:42px;padding-top:2.625rem}}@media only screen and (min-width:75em){.wrapper--content .jump-menu__wrapper{display:block;grid-column:1/3;-ms-grid-row:3;grid-row:3}.main-menu__title-container{padding-top:54px;padding-top:3.375rem}}.main-menu__title-container .site-header__title{float:none}.main-menu__list{padding-left:0;padding-left:0}.main-menu__list_item{font-family:"Noto Sans",Arial,Helvetica,sans-serif;font-size:1rem;line-height:1.5;font-size:1rem;line-height:3;margin:0;padding:0;text-align:center;padding-bottom:24px;padding-bottom:1.5rem;display:block;line-height:0;text-align:left}@media only screen and (min-width:56.25em){.main-menu__list_item.hidden-wide{display:none}}.main-menu__list_item.end-of-group{margin-bottom:24px;margin-bottom:1.5rem;border-bottom:1px solid #e0e0e0}.main-menu__list_item:last-child{padding-bottom:48px;padding-bottom:3rem}.main-menu__list_link{font-size:.875rem;line-height:1.71429;color:#212121;font-weight:700;text-decoration:none}.main-menu__list_link:hover{color:#087acc}.main-menu__close_control{font-size:.875rem;line-height:1.71429;padding:0;padding:0;margin-top:8px;margin-top:.5rem;background:url(/assets/patterns/img/icons/close.f00467a1.svg) 49px 3px no-repeat;background-size:14px;border:none;color:#212121;font-family:"Noto Sans",Arial,Helvetica,sans-serif;font-weight:700;float:right;text-align:left;width:68px}.main-menu__close_control:hover{background-image:url(/assets/patterns/img/icons/close-blue.35ee140b.svg);color:#087acc}.main-menu--js{display:none}.main-menu--js .main-menu__container{display:block}.main-menu--js.main-menu--shown{background-color:#fff;box-sizing:border-box;color:#212121;display:block;float:left;height:100vh;left:-3000px;max-width:100%;overflow:auto;position:fixed;top:0;transform:translate3d(3000px,0,0);width:100%;z-index:40}@media only screen and (min-width:56.25em){.main-menu--js.main-menu--shown{width:320px}.nav-primary{border-left:1px solid #e0e0e0;height:24px}}.main-menu--js .main_menu__quit{display:none}.main-menu__container:hover{color:#087acc}.main-menu__container .site-header__logo_link{display:block;float:none;height:35px;width:88px}.meta{font-size:.875rem;line-height:1.71429;font-family:"Noto Sans",Arial,Helvetica,sans-serif;font-weight:400;color:#757575;font-size:.75rem;line-height:2}.content-header-journal .meta{font-size:.875rem;line-height:1.71429}.highlights .meta{display:block;overflow:hidden;text-overflow:ellipsis;white-space:nowrap}.meta__type{color:inherit;text-decoration:none}.meta__type+.date::before{content:"\00a0\2022\00a0";font-weight:700}.teaser--main .meta__type{font-size:.875rem;line-height:1.28571}.teaser--main .meta__type+.date::before{content:""}@media only all and (min-width:30em){.content-header-journal .social-media-sharers li,.modal-content .social-media-sharers li{margin-left:8px;margin-left:.5rem;margin-right:8px;margin-right:.5rem}.teaser--main .meta__type+.date::before{content:"\00a0\2022\00a0";font-weight:700}}.teaser .meta__type+.date::before{content:""}a.meta__type:hover{color:#0769b0}.meta>a.meta__type:last-child:after{content:""}.teaser .meta__status,.teaser .meta__type,.teaser .meta__version{font-size:.875rem;line-height:1.42857;color:#757575;display:inline-block}.teaser .meta__status:after,.teaser .meta__type:after,.teaser .meta__version:after{color:#757575;content:"\00a0\2022\00a0";display:inline-block;font-weight:700;width:13px}.teaser .meta__status{color:#212121}.teaser .meta__status-circle{margin-bottom:1px;margin-bottom:.0625rem;margin-right:8px;margin-right:.5rem;margin-left:1px;margin-left:.0625rem;background-color:#edeff4;border-radius:50%;display:inline-block;height:8px;width:8px}.teaser .meta__status-circle-not-revised{background-color:#f29524}.teaser .meta__status-circle-revised{background-color:#42aea4}.teaser .meta__status-circle-vor{background-color:#087acc}@media only screen and (min-width:45.625em){.button-collection__item{margin-left:0;margin-left:0;margin-right:24px;margin-right:1.5rem}}.search-box{position:relative}.search-box:not(.search-box--js){padding-top:48px;padding-top:3rem}.search-box__inner{max-width:1114px;padding:0 6%;position:relative}.wrapper .search-box__inner{padding-left:0;padding-right:0}@media only all and (min-width:1114px){.search-box__inner{margin:0 auto;padding:0 66px;padding:0 4.125rem}}.nav-primary{margin-top:8px;margin-top:.5rem;background-color:#fff;clear:right;position:relative;z-index:10}.nav-primary__list{padding-left:0;padding-left:0;margin-bottom:0;margin-bottom:0}.nav-primary__item{font-family:"Noto Sans",Arial,Helvetica,sans-serif;font-size:.875rem;line-height:1.71429;font-weight:700;color:#212121;float:left;list-style-type:none}@supports (display:flex){.nav-primary__list{-ms-flex-align:center;align-items:center;display:-ms-flexbox;display:flex;padding-top:0}.nav-primary__item{padding-top:0}.nav-secondary__item{padding-top:0}}.nav-primary a:link,.nav-primary a:visited{color:#212121;text-decoration:none}.nav-primary__item{display:none}.nav-primary__item--first{display:list-item}.nav-primary__item--first a{background:url(/assets/patterns/img/patterns/molecules/nav-primary-menu-ic.ac4e582f.svg) 50px -2px no-repeat;background-size:24px;display:-ms-flexbox;display:flex;width:74px}@media only all and (max-width:21.25rem){.nav-primary__item--first{padding:0}}@media only screen and (min-width:56.25em){.nav-primary__item{margin-left:24px;margin-left:1.5rem;display:list-item}.nav-primary__item a{color:#212121}.nav-primary__item--first{margin-left:21px;margin-left:1.3125rem;margin-top:-2px;margin-top:-.125rem}.nav-primary__item--first a{background-position-x:0;background-position-y:0;text-indent:-9999px;width:24px}}.nav-secondary{margin-top:8px;margin-top:.5rem;background-color:#fff;display:none;margin-left:auto;position:relative;z-index:15}.nav-secondary__list{height:40px;height:2.5rem;margin:0;margin:0;padding:0;padding:0}.nav-secondary__item{font-family:"Noto Sans",Arial,Helvetica,sans-serif;font-size:.875rem;line-height:1.71429;font-weight:700;color:#212121;float:left;list-style-type:none}.nav-secondary__list .nav-secondary__item.nav-secondary__item--search a{display:none}@media only screen and (min-width:56.25em){.nav-secondary{display:inline-block}.nav-secondary__list .nav-secondary__item.nav-secondary__item--search a{margin-right:24px;margin-right:1.5rem;background:url(/assets/patterns/img/patterns/molecules/nav-secondary-search-black-ic.a3f2d6fa.svg) no-repeat;background-size:21px;display:block;text-indent:-9999px;width:24px}}@media only screen and (min-width:75em){.nav-secondary__list .nav-secondary__item.nav-secondary__item--search a{background-position-x:50px;background-position-y:0;text-indent:0;width:70px}}.nav-secondary__list .nav-secondary__item.nav-secondary__item--alert a{display:none}.nav-secondary__item--hide-narrow{display:none}.nav-secondary__item--hide-narrow .button--default.button--extra-small{padding-left:8px;padding-left:.5rem;padding-right:8px;padding-right:.5rem;font-weight:400}.nav-secondary__item a:not(.login-control__non_js_control_link){text-decoration:none}.nav-secondary__item a:not(.login-control__non_js_control_link):link,.nav-secondary__item a:not(.login-control__non_js_control_link):visited{color:#212121}.nav-secondary__item a:not(.login-control__non_js_control_link).button:link,.nav-secondary__item a:not(.login-control__non_js_control_link).button:visited{color:#fff}@media only screen and (min-width:56.25em){.nav-secondary__list .nav-secondary__item.nav-secondary__item--alert a{margin-right:24px;margin-right:1.5rem;background:url(/assets/patterns/img/patterns/molecules/nav-secondary-alert-black-ic.e9bb80fe.svg) no-repeat;background-size:21px;display:block;text-indent:-9999px;width:24px}.nav-secondary__item--hide-narrow{display:list-item}}.tabbed-navigation{margin-bottom:24px;margin-bottom:1.5rem;background-color:#fff;border-bottom:1px solid #e0e0e0;border-top:1px solid #e0e0e0;grid-column:2/12;-ms-grid-row:1;grid-row:1;-ms-flex-pack:center;justify-content:center;position:-webkit-sticky;position:sticky;top:-1px;z-index:9}@media only all and (min-width:62.5em){.tabbed-navigation{grid-column:1/9}}.tabbed-navigation:not(.hidden)~.main-content-grid{-ms-grid-row:2;grid-row:2}.wrapper .tabbed-navigation{margin-left:-24px;margin-left:-1.5rem;margin-right:-24px;margin-right:-1.5rem}@media only all and (min-width:30em){.tabbed-navigation{margin-bottom:36px;margin-bottom:2.25rem}.wrapper .tabbed-navigation{margin-left:-48px;margin-left:-3rem;margin-right:-48px;margin-right:-3rem}}@media only all and (min-width:45.625em){.wrapper .tabbed-navigation{margin-left:0;margin-left:0;margin-right:0;margin-right:0}.content-aside .content-aside__second-column{width:100%}}@media only all and (min-width:75em){.tabbed-navigation{grid-column:1/10}.wrapper .tabbed-navigation{margin-right:36px;margin-right:2.25rem}}.tabbed-navigation__tab-label{font-size:1rem;line-height:1.5;display:inline}.tabbed-navigation__tab-label.tabbed-navigation__tab-label--active{border-bottom:4px solid #087acc;font-weight:600}.tabbed-navigation__tab-label.tabbed-navigation__tab-label--active a{color:#212121}.tabbed-navigation__tab-label a{padding:24px 14px;padding:1.5rem .875rem;color:#757575;display:block;font-family:"Noto Sans",Arial,Helvetica,sans-serif;text-decoration:none}.tabbed-navigation__tab-label a:hover{color:#212121}.tabbed-navigation__tab-label--long{display:none}@media only all and (min-width:30em){.tabbed-navigation__tab-label a{padding:24px 18px 20px;padding:1.5rem 1.125rem 1.25rem}.tabbed-navigation__tab-label--long{display:inline}}.tabbed-navigation__tab-label--side-by-side{display:none}@media only all and (min-width:30em){.tabbed-navigation__tab-label--side-by-side{display:inline}}.tabbed-navigation__tabs{display:-ms-inline-flexbox;display:inline-flex;list-style:none;margin:0;padding:0;width:100%;-ms-flex-pack:center;justify-content:center}@media only all and (min-width:30em){.tabbed-navigation__tabs{display:-ms-flexbox;display:flex;-ms-flex-wrap:wrap;flex-wrap:wrap;max-height:72px;overflow:hidden}}.content-aside{padding-top:12px;padding-top:.75rem}@media only screen and (min-width:30em){.content-aside{padding-top:24px;padding-top:1.5rem}}@media only screen and (min-width:45.625em){.content-aside{display:-ms-grid;display:grid;-ms-grid-columns:(1fr)[12];grid-template-columns:repeat(12,1fr)}}.content-aside .button-collection.button-collection--inline{margin-bottom:24px;margin-bottom:1.5rem}.content-aside .button-collection.button-collection--inline .button-collection__item{margin-bottom:12px;margin-bottom:.75rem}.content-aside .contextual-data{border:none}.content-aside .contextual-data__list{border:none;display:inline-block;text-align:left}.content-aside .contextual-data__list .contextual-data__item{padding:0;padding:0}.content-aside .contextual-data__list .contextual-data__item:before{content:""}.content-aside .contextual-data__list .contextual-data__item:after{content:"\00a0\2022\00a0";display:inline-block;width:13px}.content-aside .contextual-data__list .contextual-data__item:last-child:after{content:""}.content-aside .contextual-data__list .contextual-data__item.no-separator:after{content:""}.content-aside .content-aside__second-column{padding-bottom:24px;padding-bottom:1.5rem}.content-aside .content-aside__second-column .divider{display:none}@media only all and (min-width:62.5em){.content-aside .content-aside__second-column{padding-bottom:0;padding-bottom:0}.content-aside .content-aside__second-column .divider{display:block}}@media only screen and (min-width:45.625em){.content-aside .content-aside__column-wrapper{display:-ms-flexbox;display:flex;grid-column:2/12}.content-aside .content-aside__column-wrapper .content-aside__first-column{padding-right:48px;padding-right:3rem;width:50%}.content-aside .content-aside__column-wrapper .content-aside__first-column+.content-aside__second-column{width:50%}}.content-aside .content-aside__definition-list{margin-bottom:36px;margin-bottom:2.25rem}@media only screen and (min-width:62.5em){.content-aside{padding-top:86px;padding-top:5.375rem;-ms-flex-align:end;align-items:flex-end;-ms-flex-item-align:start;align-self:flex-start;display:-ms-flexbox;display:flex;-ms-flex-direction:column;flex-direction:column;z-index:5}.content-aside .content-aside__column-wrapper{display:block;max-width:260px;padding-left:10px;padding-left:.625rem}.content-aside .content-aside__column-wrapper .content-aside__first-column{padding-right:0;padding-right:0;width:100%}.content-aside .content-aside__column-wrapper .content-aside__first-column+.content-aside__second-column{width:100%}.content-aside .content-aside__definition-list{margin-bottom:48px;margin-bottom:3rem;padding-left:6px;padding-left:.375rem}}.content-aside .definition-list{margin:0;margin:0;-ms-grid-row:1;grid-row:1;overflow:hidden}@media only screen and (min-width:62.5em){.wrapper--content-with-header-and-aside{display:-ms-grid;display:grid;grid-column-gap:24px;-ms-grid-columns:(1fr)[12];grid-template-columns:repeat(12,1fr)}.wrapper--content-with-header-and-aside .content-header.content-header__has-aside{grid-column:1/9;-ms-grid-row:2;grid-row:2}.wrapper--content-with-header-and-aside .content-header .content-container-grid{display:block}.wrapper--content-with-header-and-aside .content-aside{grid-column:9/13;grid-row:2/4}}@media only screen and (min-width:75em){.nav-secondary__list .nav-secondary__item.nav-secondary__item--alert a{background-position-x:50px;background-position-y:0;text-indent:0;width:70px}.wrapper--content-with-header-and-aside .content-header.content-header__has-aside{padding-right:36px;padding-right:2.25rem;grid-column:1/10}.wrapper--content-with-header-and-aside .content-aside{grid-column:10/13}}.wrapper--content-with-header-and-aside>.wrapper{padding:0;padding:0}.wrapper--content-with-header-and-aside .wrapper--content{grid-column:1/13;-ms-grid-row:3;grid-row:3}@media only screen and (min-width:45.625em){.wrapper--content-with-header-and-aside .wrapper--content .main-content-grid{grid-column:2/12;-ms-grid-row:3;grid-row:3}}@media only screen and (min-width:62.5em){.wrapper--content-with-header-and-aside .wrapper--content{padding-top:24px;padding-top:1.5rem}.wrapper--content-with-header-and-aside .wrapper--content .main-content-grid{grid-column:1/9}}@media only screen and (min-width:75em){.wrapper--content-with-header-and-aside .wrapper--content{padding-top:36px;padding-top:2.25rem}.wrapper--content-with-header-and-aside .wrapper--content .main-content-grid{margin-left:36px;margin-left:2.25rem;margin-right:36px;margin-right:2.25rem;grid-column:3/10;-ms-grid-row:3;grid-row:3}}.content-header-grid-top{margin-top:24px;margin-top:1.5rem;margin-bottom:24px;margin-bottom:1.5rem}.content-header-grid-top .content-header__subject_list{margin-bottom:0;margin-bottom:0;text-align:left}@media only all and (min-width:45.625em){.content-container-grid{display:-ms-grid;display:grid;-ms-grid-columns:(1fr)[12];grid-template-columns:repeat(12,1fr);grid-column-gap:24px;max-width:1114px;padding:0}.content-header-grid-top{grid-column:2/12}}@media only all and (min-width:75em){.content-header-grid-top{grid-column:1/13}.page--magazine .content-header-grid-top{grid-column:2/12}.content-header__one-column .content-header-grid-top{grid-column:3/11;margin-left:-24px}}.content-header-grid__main{margin-bottom:24px;margin-bottom:1.5rem}.social-media-page--wrapper .content-header-grid__main{margin-bottom:36px;margin-bottom:2.25rem}.social-media-page--wrapper[data-item-type=digest] .content-header-grid__main{margin-bottom:24px;margin-bottom:1.5rem}.content-header__one-column .content-header-grid__main .content-header__footer{display:block}.content-header__one-column .content-header-grid__main .content-header__footer .content-header__footer-column-left{margin-bottom:24px;margin-bottom:1.5rem;display:-ms-flexbox;display:flex}.content-header-grid__main .content-header__icons li{display:inline-block}.content-header-grid__main .date{font-size:.875rem;line-height:1.71429}@media only all and (min-width:30em){.content-header-grid__main .content-header__footer{display:-ms-flexbox;display:flex;width:100%}.content-header-grid__main .doi:before{margin-left:3px;margin-left:.1875rem;content:"\00a0\2022\00a0";font-weight:700}.content-header-grid__main{margin-bottom:36px;margin-bottom:2.25rem;grid-column:1/10;width:100%}}@media only all and (min-width:45.625em){.social-media-page--wrapper .content-header-grid__main{margin-bottom:48px;margin-bottom:3rem}.content-header__one-column .content-header-grid__main .content-header__footer{display:-ms-flexbox;display:flex}.content-header__one-column .content-header-grid__main .content-header__footer .content-header__footer-column-left{margin-bottom:0;margin-bottom:0}.content-header-grid__main{grid-column:2/12;-ms-grid-row:2;grid-row:2}}@media only all and (min-width:56.25em){.global-wrapper.social-media-page--wrapper .main .content-header:not(.content-header-journal)>*{grid-column:3/11}.content-header .contextual-data__list{text-align:left}.content-header-grid__main{margin-bottom:48px;margin-bottom:3rem;grid-column:2/10;padding-right:48px}.content-header__one-column .content-header-grid__main{grid-column:2/12;padding-right:0}.wrapper--content-with-header-and-aside .content-header-grid__main{grid-column:2/12}}@media only all and (min-width:75em){.content-header-grid__main{grid-column:1/10;padding-right:0}.page--magazine .content-header-grid__main{grid-column:2/10;padding-right:48px}.content-header__one-column .content-header-grid__main{grid-column:3/12;margin-left:-24px;padding-right:46px}}.content-header__footer-inner{font-size:.875rem;line-height:1.71429;display:-ms-flexbox;display:flex}.content-header__one-column .social-media-sharers{display:inline-block}.content-header__one-column .social-media-sharers li{display:inline;margin-left:0;margin-right:0}.content-header__one-column .social-media-sharers li a:last-child{margin-right:0}.content-header__footer .content-header__one-column{display:-ms-flexbox;display:flex;width:100%}.content-header-grid__main .content-header__one-column{padding-right:0;grid-column:1/13}.content-header__one-column-container{display:-ms-flexbox;display:flex;margin-left:auto}.content-header__has-aside .content-container-grid{display:block}@media only all and (min-width:45.625em){.content-header__has-aside .content-container-grid{display:-ms-grid;display:grid}}.content-header__has-aside .content-container-grid .content-header-grid-top,.content-header__has-aside .content-container-grid .content-header-grid__main,.content-header__has-aside .content-container-grid .divider{margin-left:0;margin-left:0}.content-header__has-aside .content-container-grid .divider{display:none}.content-header__has-aside .content-header__icons{vertical-align:middle}@media only all and (min-width:20em){.content-header__has-aside .content-header__icons{display:inline-block}}.content-header__has-aside .content-header-grid-top{margin-bottom:12px;margin-bottom:.75rem}@media only all and (min-width:30em){.content-header__has-aside .content-header-grid-top{margin-bottom:24px;margin-bottom:1.5rem}}@media only all and (min-width:45.625em){.content-header__has-aside .content-header-grid-top{margin-bottom:12px;margin-bottom:.75rem}.content-header__has-aside .content-header-grid__main .content-header__footer .content-header__footer-column-left{margin-bottom:24px;margin-bottom:1.5rem}}.content-header__has-aside .content-header-grid__main{margin-bottom:24px;margin-bottom:1.5rem}.content-header__has-aside .content-header-grid__main .content-header__body{margin-bottom:24px;margin-bottom:1.5rem}.content-header__has-aside .content-header-grid__main .content-header__footer .content-header__footer-column-left{display:inline-block}.content-header__has-aside .content-header-grid__main .authors{margin-bottom:24px;margin-bottom:1.5rem}.content-header__has-aside .content-header-grid__main .author_link_highlight{padding-top:0;padding-top:0}.content-header__has-aside .content-header-grid__main .author_list{padding-bottom:12px;padding-bottom:.75rem}.content-header-profile{padding-top:48px;padding-top:3rem;padding-bottom:24px;padding-bottom:1.5rem;box-sizing:content-box;margin:auto;max-width:1114px;max-width:69.625rem;font-family:"Noto Sans",Arial,Helvetica,sans-serif;padding-left:6%;padding-right:6%;position:relative;text-align:center}@media only all and (min-width:45.625em){.content-header-profile{padding-bottom:48px;padding-bottom:3rem}}.content-header-profile__display_name{font-size:1.25rem;line-height:2.4;font-weight:700;margin:0;padding:0}.content-header-profile__details{font-size:1rem;line-height:1.5}.content-header-profile__affiliations{margin:0;padding:0;list-style:none}.content-header-profile__affiliations:empty{display:none}.content-header-profile__affiliation{display:inline;font-family:"Noto Sans",Arial,Helvetica,sans-serif}.content-header-profile__affiliation:after{content:"; "}.content-header-profile__affiliation:last-child:after{content:""}.content-header-profile__orcid .orcid__id{color:inherit}.content-header-profile__email{word-break:break-all}.content-header-profile__links{list-style:none;margin:0;padding:0}.js .content-header-profile__links{display:none}.content-header-profile__link{color:#212121;font-family:"Noto Sans",Arial,Helvetica,sans-serif;font-size:.875rem;line-height:1.71429;font-weight:400;text-decoration:underline;text-transform:none}.content-header-profile__link:hover{text-decoration:underline}.content-header-profile__link--logout{position:absolute;right:24px;top:24px}.content-header-simple{padding-top:48px;padding-top:3rem;padding-bottom:24px;padding-bottom:1.5rem;padding-left:6%;padding-right:6%;text-align:center}@media only all and (min-width:45.625em){.content-header-simple{padding-bottom:48px;padding-bottom:3rem}.social-media-page--wrapper .content-header.wrapper:after{grid-column:2/12}}.content-header-simple__title{font-family:"Noto Sans",Arial,Helvetica,sans-serif;font-weight:700;font-size:1.625rem;line-height:1.15385;color:#212121;font-size:1.25rem;line-height:1.2;margin:0;padding:0}.content-header-simple__strapline{color:#212121;font-family:"Noto Serif",serif;font-size:1rem;line-height:1.5;font-weight:400;margin:0;padding:0}.clean .content-header-simple{padding-left:0;padding-left:0;text-align:left}.header-wrapper .content-header-simple{padding:0;padding:0}.header-wrapper .content-header-simple__title{font-size:2rem;line-height:1.25}@media only screen and (min-width:56.25em){.header-wrapper .content-header-simple__title{font-size:2.625rem;line-height:1.2381}}.header-wrapper .content-header-simple__strapline{margin-top:24px;margin-top:1.5rem;font-family:"Noto Sans",Arial,Helvetica,sans-serif;font-weight:700}.content-header{box-sizing:content-box;margin:auto;max-width:1114px;max-width:69.625rem;color:#212121;padding-top:0;padding-bottom:23px;padding-bottom:1.4375rem;position:relative;text-align:center}.content-header.wrapper{padding-bottom:0}.content-header.wrapper:after{border-bottom:1px solid #e0e0e0;content:"";display:block;width:100%}.content-header-journal{text-align:left;position:static}.content-header-journal.wrapper:after{border-bottom:none}@media only all and (min-width:75em){.content-header__has-aside .content-header-grid__main{margin-bottom:0;margin-bottom:0}.wrapper--content-with-header-and-aside .content-header-journal{margin-left:0;margin-left:0}}.content-header--read-more .content-header__subject_list{padding-left:0;padding-left:0;padding-right:0;padding-right:0;width:100%}.content-header-image-wrapper--no-credit{padding-bottom:48px;padding-bottom:3rem}.content-header__top{margin-top:12px;margin-top:.75rem;text-align:left}.content-header__top~.content-header__body{margin-top:24px;margin-top:1.5rem;margin-bottom:48px;margin-bottom:3rem}.content-header__body{margin-top:48px;margin-top:3rem;margin-bottom:24px;margin-bottom:1.5rem}.content-header--header .content-header__body{margin-top:12px;margin-top:.75rem}.content-header--image{border-bottom:none;color:#fff;height:264px;overflow:hidden;padding-bottom:0}@media only all and (min-width:56.25em){.content-header__has-aside .content-header-grid__main{padding-right:0;padding-right:0}.social-media-page--wrapper .content-header.wrapper:after{grid-column:3/11}.content-header--image{min-height:336px}}.content-header--image .content-header__body{height:132px;display:-ms-flexbox;display:flex;-ms-flex-direction:column;flex-direction:column;-ms-flex-align:center;align-items:center;-ms-flex-line-pack:center;align-content:center;-ms-flex-pack:center;justify-content:center;padding:0 12px;padding:0 .75rem}.content-header-journal .content-header--image .content-header__body{height:168px}.content-header--has-social-media-sharers .content-header--image .content-header__body{min-height:192px}.content-header-journal .content-header--has-social-media-sharers .content-header--image .content-header__body{height:216px}@media only all and (min-width:45.625em){.content-header--header .content-header__body{margin-top:24px;margin-top:1.5rem}.content-header-grid__main .content-header__body{margin-bottom:36px;margin-bottom:2.25rem;margin-top:0;margin-top:0}.content-header--image{height:288px}.content-header--image .content-header__body{display:-ms-flexbox;display:flex;-ms-flex-direction:column;flex-direction:column;-ms-flex-align:center;align-items:center;-ms-flex-line-pack:center;align-content:center;-ms-flex-pack:center;justify-content:center;padding:0 48px;padding:0 3rem;margin-top:48px;margin-top:3rem;margin-bottom:24px;margin-bottom:1.5rem}.content-header-journal .content-header--image .content-header__body{height:120px}}.content-header--image .social-media-sharers{position:absolute;left:0;right:0;bottom:52px}.content-header-journal .content-header--image .social-media-sharers{bottom:28px}@media only all and (max-width:45.5625em){.content-header--image.content-header--has-profile .content-header__body{display:block;margin-top:0;margin-bottom:0}}.content-header__title{font-size:2.25rem;line-height:1.33333;margin-top:0;margin-top:0;margin-bottom:24px;margin-bottom:1.5rem}.content-header-journal .content-header__title{margin-top:6px;margin-top:.375rem;margin-bottom:12px;margin-bottom:.75rem}@media only all and (min-width:45.625em){.content-header__title{font-size:2.5625rem;line-height:1.17073}}@media only all and (min-width:56.25em){.content-header--image .content-header__body{height:168px}.content-header--has-social-media-sharers .content-header--image .content-header__body{min-height:216px}.content-header__title{font-size:2.875rem;line-height:1.56522}}.content-header--header .content-header__title,.content-header--read-more .content-header__title{font-size:1.8125rem;line-height:1.24138}.content-header__title_link{color:inherit;text-decoration:inherit}@media only all and (min-width:45.625em){.content-header--header .content-header__title,.content-header--read-more .content-header__title{font-size:2.25rem;line-height:1.33333}.content-header--image .content-header__body{margin-top:72px;margin-top:4.5rem}}.content-header--image .content-header__title{font-size:2.5625rem;line-height:1.17073;margin-bottom:0;height:132px;display:-ms-flexbox;display:flex;-ms-flex-pack:center;justify-content:center;-ms-flex-item-align:center;align-self:center;-ms-flex-align:center;align-items:center}@media only all and (min-width:45.625em){.content-header--image .content-header__title{font-size:3.25rem;height:auto;display:block}}.content-header--image .content-header__title.content-header__title--xx-short{font-size:2.875rem}@media only all and (min-width:30em){.content-header__has-aside .content-header-grid__main .content-header__footer .content-header__footer-column-left{display:-ms-flexbox;display:flex}.content-header--image .content-header__title.content-header__title--xx-short{font-size:3.25rem}}.content-header--image .content-header__title.content-header__title--x-short{font-size:2.5625rem}@media only all and (min-width:45.625em){.content-header--image .content-header__title.content-header__title--x-short{font-size:2.875rem}}@media only all and (min-width:56.25em){.content-header--image .content-header__title{font-size:3.625rem;line-height:1.24138}.content-header--image .content-header__title.content-header__title--x-short{font-size:3.25rem}}.content-header--image .content-header__title.content-header__title--short{font-size:1.875rem}@media only all and (min-width:30em){.content-header--image .content-header__title.content-header__title--short{font-size:2.25rem}}@media only all and (min-width:45.625em){.content-header--image .content-header__title.content-header__title--short{font-size:2.5625rem}}@media only all and (min-width:56.25em){.content-header--image .content-header__title.content-header__title--short{font-size:2.875rem}}@media only all and (min-width:75em){.content-header--image .content-header__title.content-header__title--short{font-size:3.25rem}}.content-header--image .content-header__title.content-header__title--medium{font-size:1.625rem}@media only all and (min-width:30em){.content-header--image .content-header__title.content-header__title--medium{font-size:1.875rem}}@media only all and (min-width:45.625em){.content-header--image .content-header__title.content-header__title--medium{font-size:2.25rem}}@media only all and (min-width:56.25em){.content-header--image .content-header__title.content-header__title--medium{font-size:2.5625rem}}@media only all and (min-width:75em){.content-header--image .content-header__title.content-header__title--medium{font-size:3.25rem}}.content-header--image .content-header__title.content-header__title--long{font-size:1.25rem}@media only all and (min-width:30em){.content-header--image .content-header__title.content-header__title--long{font-size:1.625rem}}@media only all and (min-width:45.625em){.content-header--image .content-header__title.content-header__title--long{font-size:2.25rem}}@media only all and (min-width:75em){.content-header--image .content-header__title.content-header__title--long{font-size:2.5625rem}}.content-header--image .content-header__title.content-header__title--x-long{font-size:1.25rem}@media only all and (min-width:45.625em){.content-header--image .content-header__title.content-header__title--x-long{font-size:1.625rem}}@media only all and (min-width:56.25em){.content-header--image .content-header__title.content-header__title--x-long{font-size:1.625rem}}@media only all and (min-width:75em){.content-header--image .content-header__title.content-header__title--x-long{font-size:1.875rem}}.content-header--image .content-header__title.content-header__title--xx-long{font-size:1.125rem}@media only all and (min-width:30em){.content-header--image .content-header__title.content-header__title--xx-long{font-size:1.25rem}}.content-header__picture{position:absolute;top:0;right:0;bottom:0;left:0;z-index:-1}.content-header__picture:after{content:"";position:absolute;top:0;right:0;bottom:0;left:0;z-index:-1;background-color:rgba(0,0,0,.4)}.content-header__image{z-index:-2;position:absolute;left:50%;top:50%;height:100%;min-width:100%;max-width:none;-ms-transform:translate(-50%,-50%);transform:translate(-50%,-50%)}.content-header__image:after{content:"";background-color:#fff;position:absolute;top:0;left:0;width:100%;height:100%}.content-header__profile_wrapper{padding:18px 0 6px;padding:1.125rem 0 .375rem;font-size:.75rem;line-height:1}.content-header__profile{text-decoration:none}.content-header__profile .content-header__profile_data,.content-header__profile .content-header__profile_label,.content-header__profile dl{display:inline-block;margin:0;font-size:.75rem;line-height:1}@media only all and (min-width:45.625em){.content-header__profile_wrapper{position:absolute;left:0;right:0;line-height:normal}.content-header__profile .content-header__profile_data,.content-header__profile .content-header__profile_label,.content-header__profile dl{display:block;font-size:.6875rem;line-height:2.18182}}.content-header__profile_label{font-size:.875rem;line-height:1.71429;font-family:"Noto Sans",Arial,Helvetica,sans-serif;font-weight:400;color:#fff}.content-header__profile_data{font-size:.875rem;line-height:1.71429;font-family:"Noto Sans",Arial,Helvetica,sans-serif;font-weight:400;color:#fff}.content-header__profile_image{display:none}@supports (display:flex){@media only all and (min-width:45.625em){.content-header__profile--has-image{display:-ms-inline-flexbox;display:inline-flex;-ms-flex-pack:center;justify-content:center;text-align:left;width:100%}.content-header__profile--has-image .content-header__profile_image{display:block;border-radius:24px;height:48px;width:48px;margin-right:12px;margin-right:.75rem}.content-header__profile--has-image dd,.content-header__profile--has-image dl,.content-header__profile--has-image dt{display:block}.content-header__profile--has-image .content-header__profile_data{color:#fff;font-family:"Noto Sans",Arial,Helvetica,sans-serif;font-size:.875rem;line-height:1.71429}.content-header__profile_wrapper{padding:24px 0 0;padding:1.5rem 0 0}}}.content-header__component-assets{display:-ms-grid;display:grid;-ms-grid-columns:50px auto 50px;grid-template-columns:50px auto 50px}.content-header__subject_list{padding-top:24px;padding-top:1.5rem;font-size:.875rem;line-height:1.71429;font-family:"Noto Sans",Arial,Helvetica,sans-serif;font-weight:400;color:#087acc;padding-left:0;padding-left:0;padding-right:0;padding-right:0;-ms-grid-row:1;grid-row:1;margin:0;-ms-flex-order:2;order:2;text-align:center}@media only all and (min-width:45.625rem){.content-header__subject_list{padding-left:72px;padding-left:4.5rem;padding-right:72px;padding-right:4.5rem}.content-header-journal .content-header__subject_list{padding-left:0;padding-left:0;padding-right:0;padding-right:0}}.content-header--image .content-header__subject_list{color:inherit}.content-header__subject_list:before{color:#757575}.content-header-journal .content-header__subject_list{padding-top:0;padding-top:0}.content-header__subject_list_item{font-size:.875rem;line-height:1.71429;font-family:"Noto Sans",Arial,Helvetica,sans-serif;font-weight:400;color:#087acc;font-size:.8125rem;line-height:1.84615;display:inline;list-style-type:none;padding:0}.content-header__subject_list_item .content-header__subject_link{font-weight:700}.content-header__subject_list_item .content-header__subject_link:after{content:", "}.content-header--image .content-header__subject_list_item{color:inherit}.content-header__subject_list_item:last-child .content-header__subject_link:after{content:""}.content-header__subject_link{font-size:.875rem;line-height:1.71429;font-family:"Noto Sans",Arial,Helvetica,sans-serif;font-weight:400;color:#087acc;font-size:.8125rem;line-height:1.84615;text-decoration:none}.content-header-journal .content-header__subject_link{font-size:.875rem;line-height:1.71429}.content-header__subject_link:hover{color:#0769b0;text-decoration:underline;text-underline-offset:3px}.content-header--image .content-header__subject_link{color:inherit}.content-header--image .content-header__subject_link:hover{color:inherit}.content-header__icons{margin:14px 0 0;margin:.875rem 0 0;padding:0;padding:0;-ms-grid-row:1;grid-row:1;list-style:none;-ms-flex-order:1;order:1}.content-header-journal .content-header__icons{margin:0 0 0 6px;margin:0 0 0 .375rem;-ms-flex-order:0;order:0}@media only all and (max-width:29.9375em){.content-header-journal.content-header__has-aside .content-header__icons{margin:0;margin:0}}@media only all and (min-width:75em){.content-header--image .content-header__icons{left:16px}.content-header--image .side-section-wrapper__download_link{right:16px}}.content-header--image .content-header__icons{left:12px;top:12px}.content-header__one-column .content-header__icons{-ms-grid-row:auto;grid-row:auto}.content-header__icon{background-repeat:no-repeat;background-position:left bottom;display:block;height:22px;width:17px}.content-header-journal .content-header__icon{background-position:0 0;height:20px}.content-header__icon--cc{background-image:url(/assets/patterns/img/icons/cc.ec7b6e9c.svg),linear-gradient(transparent,transparent)}.content-header__icon--cc:hover{background-image:url(/assets/patterns/img/icons/cc-hover.7a693c5e.svg),linear-gradient(transparent,transparent)}.content-header-journal .content-header__icon--cc:hover{background-image:url(/assets/patterns/img/icons/cc-new.508bd042.svg)}.content-header-journal .content-header__icon--cc{background-image:url(/assets/patterns/img/icons/cc-new.508bd042.svg)}.content-header__icon--oa{background-image:url(/assets/patterns/img/icons/oa.f53eb8bd.svg),linear-gradient(transparent,transparent)}.content-header__icon--oa:hover{background-image:url(/assets/patterns/img/icons/oa-hover.ec1c5229.svg),linear-gradient(transparent,transparent)}.content-header-journal .content-header__icon--oa:hover{background-image:url(/assets/patterns/img/icons/oa-new.9b599d77.svg)}.content-header-journal .content-header__icon--oa{background-image:url(/assets/patterns/img/icons/oa-new.9b599d77.svg)}.content-header-journal .content-header__icon--cc{width:18px}.content-header-journal .content-header__icon--oa{width:16px}.content-header--image .side-section-wrapper__download_link{right:12px;position:absolute;top:-12px}.content-header__download_icon{width:20px}.side-section .content-header__download_icon{width:20px}.content-header__impact-statement{font-family:"Noto Sans",Arial,Helvetica,sans-serif;font-size:1rem;line-height:1.5;font-weight:500;margin-bottom:24px;margin-bottom:1.5rem;max-width:100%}.content-header__impact-statement a{border-bottom:1px dotted #212121;color:#212121;text-decoration:none}.content-header__impact-statement a:hover{border-bottom-color:#212121;color:#212121}.content-header__impact-statement a:active,.content-header__impact-statement a:hover{color:inherit}.content-header--image .content-header__impact-statement{margin-bottom:0;margin-bottom:0;display:none}.content-header--image .content-header__impact-statement a{border-bottom:1px dotted #fff;color:#fff;text-decoration:none}.content-header--image .content-header__impact-statement a:hover{border-bottom-color:#fff;color:#fff}@media only all and (min-width:56.25em){.content-header--image .content-header__title.content-header__title--xx-long{font-size:1.625rem}.content-header--image .content-header__impact-statement{display:block}.content-header--image.content-header--has-social-media-sharers .content-header__impact-statement{display:none}}.content-header__cta{margin-bottom:18px;margin-bottom:1.125rem}.content-header--image .content-header__cta{margin-bottom:0;margin-bottom:0;position:absolute;bottom:44px;left:0;right:0}.content-header__meta{padding-bottom:24px;padding-bottom:1.5rem}.content-header--image .content-header__meta{position:absolute;left:0;right:0;bottom:6px;font-size:.75rem;line-height:1}.content-header-journal .content-header--image .content-header__meta{bottom:12px}@media only all and (min-width:45.625em){.content-header--image .side-section-wrapper__download_link{top:0}.content-header__download_icon{width:44px}.content-header--image .content-header__meta{bottom:0;font-size:.75rem;line-height:2}}.content-header-grid__main .content-header__meta{padding-bottom:0;padding-bottom:0}.content-header--image .meta{color:inherit;font-size:.75rem;line-height:1}.content-header--image .date{color:inherit;font-size:.75rem;line-height:1}.content-header--image .meta__type:hover{color:inherit}.content-header__image-credit{color:#757575;font-family:"Noto Sans",Arial,Helvetica,sans-serif;font-size:.6875rem;line-height:2.18182;padding-top:12px;padding-top:.75rem;padding-bottom:12px;padding-bottom:.75rem;text-align:right;visibility:hidden}.content-header__image-credit a,.content-header__image-credit a:hover{color:inherit;text-decoration:underline}.content-header__image-credit--overlay{color:#fff;position:absolute;bottom:0;right:0;padding-right:12px;padding-right:.75rem}.hero-banner{max-width:1114px;max-width:69.625rem;margin-bottom:24px;margin-bottom:1.5rem;background-color:#fff;color:#212121;height:100%;width:100%}@media only screen and (min-width:75em){.hero-banner{margin-bottom:48px;margin-bottom:3rem;padding-top:24px;padding-top:1.5rem}}.hero-banner__details .hero-banner__meta .meta{color:inherit;font-size:.875rem;line-height:1}.hero-banner__details .hero-banner__meta .meta__type:hover{color:inherit}.hero-banner__picture-wrapper{height:-webkit-fit-content;height:-moz-fit-content;height:fit-content}.highlight{max-width:1114px;max-width:69.625rem;padding:36px 0;padding:2.25rem 0;border-bottom:1px solid #e0e0e0;margin-left:auto;margin-right:auto;overflow:hidden;position:relative}@media only screen and (min-width:45.625em){.hero-banner{display:-ms-grid;display:grid;grid-auto-flow:dense;-ms-grid-columns:7fr 5fr;grid-template-columns:7fr 5fr}.hero-banner__picture-wrapper{-ms-grid-column:2;grid-column:2}.highlight{padding-bottom:48px;padding-bottom:3rem}}@media only screen and (min-width:75em){.highlight{padding-top:48px;padding-top:3rem}}.highlight__items{list-style:none;margin:0;padding:0}@media only all and (min-width:75em){.content-header--image.content-header--has-social-media-sharers .content-header__impact-statement{display:block}.highlight__items{-ms-flex-align:start;align-items:start;-moz-column-gap:3rem;column-gap:3rem;display:-ms-grid;display:grid;-ms-grid-columns:4fr 4fr 4fr;grid-template-columns:4fr 4fr 4fr}}.listing-list--read-more .content-header-divider{border:none}.listing-list--read-more .content-header--read-more{border:none}.site-header{margin:30px 0;margin:1.875rem 0;max-height:96px;position:relative;z-index:20}@media only screen and (min-width:56.25em){.site-header{margin:42px 0;margin:2.625rem 0}}@media only screen and (min-width:75em){.site-header{margin:54px 0 42px;margin:3.375rem 0 2.625rem}}@supports (display:flex){.site-header{display:-ms-flexbox;display:flex}.site-header__logo_link{background:0 0}}.site-header .search-box{background-color:#fff;display:none}.site-header__title{display:inline-block;float:left;position:relative;z-index:21}.site-header__logo_link{display:block;height:35px;width:88px}.site-header__logo_link_image{display:none}@supports (display:flex){.site-header__logo_link_image{display:block}}.site-header__navigation{background-color:#fff;height:35px;display:-ms-flexbox;display:flex;-ms-flex-pack:right;justify-content:right;margin-left:auto;position:relative;width:100%;z-index:20}.site-header__skip_to_content{display:block;position:absolute;top:20px;left:20px;white-space:nowrap}.site-header__skip_to_content__link{border:0;clip:rect(0 0 0 0);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px;padding:15px 36px 14px;padding:.9375rem 2.25rem .875rem;z-index:50}@media only all and (min-width:45.625em){.content-header--image .date{font-size:.75rem;line-height:2}.content-header__image-credit{visibility:visible}.site-header__title{float:left;position:relative;width:88px}.site-header__logo_link{margin:0;margin:0;background:0 0;display:block;float:right;height:35px;position:relative;width:88px;z-index:10}.site-header__logo_link_image{display:block}}@media only screen and (min-width:56.25em){.site-header{max-height:119px}.site-header__navigation{margin-left:20px;margin-left:1.25rem}.site-header__navigation__item{-ms-flex-pack:start;justify-content:flex-start}}@media only screen and (min-width:75em){.site-header{max-height:143px}}    </style>

        <link rel="apple-touch-icon" sizes="57x57" href="/assets/favicons/apple-touch-icon-57x57.c2b7763c.png">
    <link rel="apple-touch-icon" sizes="60x60" href="/assets/favicons/apple-touch-icon-60x60.de8ef708.png">
    <link rel="apple-touch-icon" sizes="72x72" href="/assets/favicons/apple-touch-icon-72x72.329a975f.png">
    <link rel="apple-touch-icon" sizes="76x76" href="/assets/favicons/apple-touch-icon-76x76.bf88533a.png">
    <link rel="apple-touch-icon" sizes="114x114" href="/assets/favicons/apple-touch-icon-114x114.1e1b745a.png">
    <link rel="apple-touch-icon" sizes="120x120" href="/assets/favicons/apple-touch-icon-120x120.ee3353ac.png">
    <link rel="apple-touch-icon" sizes="144x144" href="/assets/favicons/apple-touch-icon-144x144.0a33d19e.png">
    <link rel="apple-touch-icon" sizes="152x152" href="/assets/favicons/apple-touch-icon-152x152.8124b596.png">
    <link rel="apple-touch-icon" sizes="180x180" href="/assets/favicons/apple-touch-icon-180x180.7dd4e110.png">
    <link rel="icon" type="image/svg+xml" href="/assets/favicons/favicon.e086b7f6.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.68f361ed.png">
    <link rel="icon" type="image/png" sizes="192x192" href="/assets/favicons/android-chrome-192x192.9d5990c2.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.c7965709.png">
    <link rel="shortcut icon" href="/assets/favicons/favicon.28c35cbe.ico">
    <link rel="manifest" href="/assets/favicons/manifest.dbfa139f.json">
    <meta name="theme-color" content="#ffffff">
    <meta name="application-name" content="eLife">






                <meta name="dc.format" content="text/html">
                <meta name="dc.language" content="en">
                <meta name="dc.publisher" content="eLife Sciences Publications Limited">

                                    <meta name="dc.title" content="Tracking prototype and exemplar representations in the brain across learning">

                                    <meta name="dc.identifier" content="doi:10.7554/eLife.59360">

                                    <meta name="dc.date" content="2020-11-26">

                                            <meta name="dc.rights" content="© 2020 Bowman et al.. This article is distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use and redistribution provided that the original author and source are credited.">



                                                                                        <meta name="dc.contributor" content="Caitlin R Bowman">
                                                                                                                            <meta name="dc.contributor" content="Takako Iwashita">
                                                                                                                            <meta name="dc.contributor" content="Dagmar Zeithamova">



        <meta property="og:site_name" content="eLife">
        <meta property="og:url" content="https://elifesciences.org/articles/59360">
        <meta property="og:title" content="Tracking prototype and exemplar representations in the brain across learning">
        <meta name="twitter:site" content="@eLife">

                                                <meta property="og:description" content="Concepts can be represented at multiple levels of specificity (individual examples, abstract category averages) within a single task across different regions of the brain.">
            <meta name="description" content="Concepts can be represented at multiple levels of specificity (individual examples, abstract category averages) within a single task across different regions of the brain.">

                    <meta name="twitter:card" content="summary">
            <meta name="twitter:image" content="https://elifesciences.org/assets/images/social/icon-600x600@1.52e87ee6.png">
            <meta property="og:image" content="https://elifesciences.org/assets/images/social/icon-600x600@1.52e87ee6.png">
            <meta property="og:image:width" content="600">
            <meta property="og:image:height" content="600">

                    <meta property="og:type" content="article">

        <link rel="canonical" href="/articles/59360">









    <script type="text/plain" data-cookieconsent="statistics,marketing">
                window.gtmDataLayer = window.gtmDataLayer || [];

                window.gtmDataLayer.push(
            {
                'articleSubjects': 'Neuroscience',
                'articleType': 'Research Article',
                'articlePublishDate': 'Nov 26, 2020'
            }
        );

        (function (w, d, s, l, i) {
            w[l] = w[l] || [];
            w[l].push({
                'gtm.start': new Date().getTime(), event: 'gtm.js'
            });
            var f = d.getElementsByTagName(s)[0],
                j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : '';
            j.async = true;
            j.src =
                'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
            f.parentNode.insertBefore(j, f);
        })(window, document, 'script', 'gtmDataLayer', 'GTM-WVM8KG');
            </script>

            <script type="text/plain" data-cookieconsent="statistics" src="https://www.googleoptimize.com/optimize.js?id=OPT-KJGKNCT"></script>


</head>

<body>

            <noscript>
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WVM8KG" height="0" width="0"
                    style="display:none; visibility:hidden"></iframe>
        </noscript>

    <div class="global-wrapper" data-behaviour="FragmentHandler Math HypothesisLoader"
                    data-item-type="research-article"
            >

        <div class="global-inner">

                            <div>
                    <div class="wrapper wrapper--site-header">
  <header class="site-header clearfix" data-behaviour="SiteHeader" id="siteHeader">
        <div class="site-header__title clearfix" role="banner">
          <div class="site-header__skip_to_content">
            <a href="#maincontent" class="site-header__skip_to_content__link button button--default">Skip to Content</a>
          </div>
          <a href="/" class="site-header__logo_link">
            <img src="/assets/patterns/img/patterns/organisms/elife-logo-xs.fd623d00.svg" alt="eLife logo" class="site-header__logo_link_image"/>
            <span class="visuallyhidden" >eLife home page</span>
          </a>
        </div>
    <div class="site-header__navigation" role="navigation" aria-label="Main navigation">
      <div class="site-header__navigation__item">
          <nav class="nav-primary">
            <ul class="nav-primary__list clearfix">
                <li class="nav-primary__item nav-primary__item--first">


                    <a href="#mainMenu">
                      Menu
                    </a>

                </li>
                <li class="nav-primary__item">


                    <a href="/">
                      Home
                    </a>

                </li>
                <li class="nav-primary__item">


                    <a href="/browse">
                      Browse
                    </a>

                </li>
                <li class="nav-primary__item">


                    <a href="/magazine">
                      Magazine
                    </a>

                </li>
                <li class="nav-primary__item">


                    <a href="/community">
                      Community
                    </a>

                </li>
                <li class="nav-primary__item nav-primary__item--last">


                    <a href="/about">
                      About
                    </a>

                </li>
            </ul>
          </nav>
      </div>
        <nav class="nav-secondary">
          <ul class="nav-secondary__list clearfix">
              <li class="nav-secondary__item nav-secondary__item--first nav-secondary__item--search">


                  <a href="/search" rel="search">
                    Search
                  </a>

              </li>
              <li class="nav-secondary__item nav-secondary__item--alert">


                  <a href="/content-alerts">
                    Alerts
                  </a>

              </li>
              <li class="nav-secondary__item nav-secondary__item--last nav-secondary__item--hide-narrow">

                        <a href="/submit-your-research" class="button button--extra-small button--default" id="submitResearchButton">Submit your research</a>


              </li>
          </ul>
        </nav>
    </div>

      <div class="search-box" data-behaviour="SearchBox">
        <div class="search-box__inner">
            <form class="compact-form" id="search" action="/search" method="GET" novalidate>
              <fieldset class="compact-form__container">
                <label>
                  <span class="visuallyhidden">Search by keyword or author</span>
                  <input type="search" name="for" value="" placeholder="Search by keyword or author"

                     class="compact-form__input"

                  >
                </label>


                <button type="reset" name="reset" class="compact-form__reset"><span class="visuallyhidden">Reset form</span></button>
                <button type="submit" class="compact-form__submit"><span class="visuallyhidden">Search</span></button>
              </fieldset>
            </form>

            <label class="search-box__search_option_label">
              <input type="checkbox" name="subjects[]" value="neuroscience" form="search">Limit my search to Neuroscience
            </label>

        </div>
      </div>
  </header>
</div>

                </div>





            <main role="main" class="main" id="maincontent">


            <div class="wrapper wrapper--content-with-header-and-aside">

        <header
  class="content-header content-header-journal wrapper content-header--header content-header--has-social-media-sharers content-header__has-aside"
  data-behaviour="ContentHeader">

  <div class="content-container-grid">
    <div class="content-header-grid-top">

        <ul class="breadcrumbs">
            <li class="breadcrumb-item">
                <a href="/articles/research-article" class="breadcrumb-item__link">Research Article</a>
            </li>
        </ul>


          <ol class="content-header__subject_list">
              <li class="content-header__subject_list_item">
                <a href="/subjects/neuroscience" class="content-header__subject_link">Neuroscience</a>
              </li>
          </ol>
    </div>
    <div class="content-header-grid__main">

      <div class="content-header__body">
        <h1 class="content-header__title content-header__title--long">Tracking prototype and exemplar representations in the brain across learning</h1>


      </div>

        <div class="authors" data-behaviour="Authors">
          <ol class="author_list" aria-label="Authors of this article">
            <li class="author_list_item"><span class="author"><a href="/articles/59360#xe59294b7" data-behaviour="Popup" class="author_link">Caitlin R Bowman</a><span class="author_suffix">&nbsp;<picture>
                    <source srcset="/assets/patterns/img/icons/corresponding-author.d7eda27b.svg" type="image/svg+xml">
                    <img src="/assets/patterns/img/icons/corresponding-author@1x.075234c5.png"
                        srcset="/assets/patterns/img/icons/corresponding-author@2x.5d6281b5.png 2x, /assets/patterns/img/icons/corresponding-author@1x.075234c5.png 1x"
                        alt="Is a corresponding author" class="author_icon">
                </picture></span></span></li><li class="author_list_item"><span class="author"><a href="/articles/59360#x82a302d4" data-behaviour="Popup" class="author_link">Takako Iwashita</a></span></li><li class="author_list_item"><span class="author"><a href="/articles/59360#x9ea0ce01" data-behaviour="Popup" class="author_link">Dagmar Zeithamova</a><span class="author_suffix">&nbsp;<picture>
                    <source srcset="/assets/patterns/img/icons/corresponding-author.d7eda27b.svg" type="image/svg+xml">
                    <img src="/assets/patterns/img/icons/corresponding-author@1x.075234c5.png"
                        srcset="/assets/patterns/img/icons/corresponding-author@2x.5d6281b5.png 2x, /assets/patterns/img/icons/corresponding-author@1x.075234c5.png 1x"
                        alt="Is a corresponding author" class="author_icon">
                </picture></span></span></li>
          </ol>

            <ol class="institution_list" aria-label="Author institutions">
                <li class="institution_list_item">
                  <span class="institution">Department of Psychology, University of Oregon, United States<span class="institution_separator" aria-hidden="true">;</span>
                  </span>
                </li>
                <li class="institution_list_item">
                  <span class="institution">Department of Psychology, University of Wisconsin-Milwaukee, United States<span class="institution_separator" aria-hidden="true">;</span>
                  </span>
                </li>
            </ol>
        </div>
      <div class="content-header__footer">


          <div class="content-header__meta">
            <div class="meta">



                <span class="date"> <time datetime="2020-11-26">Nov 26, 2020</time></span>

            </div>
          </div>

          <span class="doi">
              <a href="https://doi.org/10.7554/eLife.59360" class="doi__link">
            https://doi.org/10.7554/eLife.59360
              </a>
          </span>

        <ul class="content-header__icons">
          <li>
            <a href="https://en.wikipedia.org/wiki/Open_access"
                class="content-header__icon content-header__icon--oa">
                  <span class="visuallyhidden">Open access</span>
            </a>
          </li>
          <li>
            <a href="#copyright" class="content-header__icon content-header__icon--cc">
              <span class="visuallyhidden">Copyright information</span>
            </a>
          </li>
        </ul>


      </div>
    </div>

    <div class="divider"></div>

  </div>

</header>


                    <aside class="content-aside" data-behaviour="ContentAside">


  <div class="content-aside__column-wrapper">

      <div class="content-aside__first-column">
        <div class="content-aside__definition-list">
          <dl class="definition-list definition-list--timeline" aria-label="Version history">
            <dt class="definition-list--active">Version of Record</dt><dd><time datetime="2020-12-17">December 17, 2020</time> </dd>
<dt><a href="/articles/59360v1">Accepted Manuscript</a></dt><dd><time datetime="2020-11-26">November 26, 2020</time> </dd>
          </dl>
        </div>
      </div>

    <div class="content-aside__second-column">

        <ol class="button-collection button-collection--inline">
            <li class="button-collection__item">    <a href="#downloads" class="button button--default button--action icon icon-download" id="button-action-download">Download</a>
</li>
            <li class="button-collection__item">    <a href="#cite-this-article" class="button button--default button--action icon icon-citation" id="modalContentCitations">Cite</a>
</li>
            <li class="button-collection__item">    <a href="#share" class="button button--default button--action icon icon-share" id="modalContentShare">Share</a>
</li>
            <li class="button-collection__item"><div data-hypothesis-trigger>
    <a href="#comment" class="button button--default button--action icon icon-comment" data-behaviour="HypothesisTrigger">Comment<span aria-hidden='true'><span data-visible-annotation-count></span> </span><span class='visuallyhidden'>Open annotations (there are currently <span data-hypothesis-annotation-count>0</span> annotations on this page). </span></a>
</div>
</li>
        </ol>

        <div class="contextual-data">

            <ul class="contextual-data__list" aria-label="The following contains the number of views, citations and annotations in this article">

                <li class="contextual-data__item"><a href="/articles/59360#metrics"><span class="contextual-data__counter">6,589</span> views</a></li>
                <li class="contextual-data__item"><a href="/articles/59360#metrics"><span class="contextual-data__counter">606</span> downloads</a></li>
                <li class="contextual-data__item"><a href="/articles/59360#metrics"><span class="contextual-data__counter">48</span> citations</a></li>


            </ul>


        </div>

      <div class="divider"></div>

        <div class='
                     altmetric-container-without-details'>
            <script src='https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js'></script>
            <div class='altmetric-embed'
            data-hide-less-than="1" 
            data-badge-type='donut' 

            data-doi=10.7554/eLife.59360></div>
            <p class="altmetric-text--small">Altmetric provides a collated score for online attention across various platforms and media.<br/>
            <a href="#metrics" class='altmetric-link--small'>See more details</a>
            </p>
        </div>

    </div>
  </div>
</aside>


                    <div class="modal modal-nojs" data-behaviour="Modal" data-trigger-id="modalContentShare">
  <div class="modal-container">
      <div class="modal-content">
        <h6>Share this article</h6>
        <a href="" class="modal-content__button modal-content__close-button"></a>
        <div class="modal-content__body">
          <div class="form-item">

  <div class="form-item__label_container">
      <label
          for="modal-share-doi"
          class="form-item__label visuallyhidden">
        Doi
      </label>

  </div>

  <input
      type="text"
      class="text-field text-field--text"
     id="modal-share-doi"
     name="doi"

     value="https://doi.org/10.7554/eLife.59360"




  >


</div>
    <button class="button button--default" type="button" data-behaviour="ButtonClipboard" data-clipboard="https://doi.org/10.7554/eLife.59360">Copy to clipboard</button>

<ul class="social-media-sharers">
  <li>
    <a class="social-media-sharer email" href="mailto:?subject=Tracking%20prototype%20and%20exemplar%20representations%20in%20the%20brain%20across%20learning&amp;body=https%3A%2F%2Fdoi.org%2F10.7554%2FeLife.59360" target="_blank" rel="noopener noreferrer" aria-label="Share by Email">
      <svg width="24px" height="24px" viewBox="0 0 24 24" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
          <g id="email" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
              <rect id="clear-button-bg" x="0" y="0" width="24" height="24"></rect>
              <path d="M20,4 L4,4 C2.9,4 2.01,4.9 2.01,6 L2,18 C2,19.1 2.9,20 4,20 L20,20 C21.1,20 22,19.1 22,18 L22,6 C22,4.9 21.1,4 20,4 Z M20,8 L12,13 L4,8 L4,6 L12,11 L20,6 L20,8 Z" id="Shape" fill="#000000" fill-rule="nonzero"></path>
          </g>
      </svg>
    </a>
  </li>
  <li>
    <a class="social-media-sharer" href="https://twitter.com/intent/tweet/?text=In%20%40eLife%3A%20Tracking%20prototype%20and%20exemplar%20representations%20in%20the%20brain%20across%20learning&amp;url=https%3A%2F%2Fdoi.org%2F10.7554%2FeLife.59360" target="_blank" rel="noopener noreferrer" aria-label="Tweet a link to this page">
      <svg width="24px" height="24px" viewBox="0 0 24 24" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
        <g id="x-24-articles" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
            <rect id="Rectangle" x="0" y="0" width="24" height="24"></rect>
            <path d="M13.6604555,11.4785798 L20.954365,3 L19.225942,3 L12.8926412,10.3618317 L7.83425247,3 L2,3 L9.64927632,14.1323934 L2,23.023486 L3.72852106,23.023486 L10.4166498,15.2491415 L15.7586789,23.023486 L21.5929313,23.023486 L13.660031,11.4785798 L13.6604555,11.4785798 Z M11.293009,14.2304723 L10.5179779,13.1219369 L4.35133136,4.30120576 L7.00623887,4.30120576 L11.9827944,11.4198173 L12.7578255,12.5283527 L19.2267583,21.7814574 L16.5718508,21.7814574 L11.293009,14.2308968 L11.293009,14.2304723 Z" id="Shape" fill="#212121" fill-rule="nonzero"></path>
        </g>
      </svg>
    </a>
  </li>
  <li>
    <a class="social-media-sharer" href="https://facebook.com/sharer/sharer.php?u=https%3A%2F%2Fdoi.org%2F10.7554%2FeLife.59360" target="_blank" rel="noopener noreferrer" aria-label="Share on Facebook">
      <svg width="24px" height="24px" viewBox="0 0 24 24" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
          <g id="facebook" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
              <g>
                  <rect id="clear-button-bg" x="0" y="0" width="24" height="24"></rect>
                  <path d="M11.8969072,2 C6.43099227,2 2,6.41067993 2,11.8515383 C2,16.7687258 5.61915593,20.844338 10.3505155,21.5833958 L10.3505155,14.6992486 L7.83762887,14.6992486 L7.83762887,11.8515383 L10.3505155,11.8515383 L10.3505155,9.68112126 C10.3505155,7.21207948 11.8280541,5.84825714 14.0887242,5.84825714 C15.1715271,5.84825714 16.3041237,6.04067 16.3041237,6.04067 L16.3041237,8.465072 L15.0561469,8.465072 C13.8267075,8.465072 13.443299,9.22446783 13.443299,10.0035475 L13.443299,11.8515383 L16.1881443,11.8515383 L15.7493557,14.6992486 L13.443299,14.6992486 L13.443299,21.5833958 C18.1746585,20.844338 21.7938144,16.7687258 21.7938144,11.8515383 C21.7938144,6.41067993 17.3628222,2 11.8969072,2 Z" id="Fill-1" fill="#212121" fill-rule="nonzero"></path>
              </g>
          </g>
      </svg>
    </a>
  </li>
  <li>
    <a class="social-media-sharer" href="https://www.linkedin.com/shareArticle?title=Tracking%20prototype%20and%20exemplar%20representations%20in%20the%20brain%20across%20learning&amp;url=https%3A%2F%2Fdoi.org%2F10.7554%2FeLife.59360" target="_self" aria-label="Share this page to LinkedIn (opens up email program, if configured on this system)">
      <svg width="24px" height="24px" viewBox="0 0 24 24" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
        <g id="linkedin" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
            <g>
                <rect id="Rectangle" x="0" y="0" width="24" height="24"></rect>
                <path d="M18.9751954,18.9751435 L16.0232549,18.9751435 L16.0232549,14.3522816 C16.0232549,13.2499163 16.0035753,11.8308324 14.4879383,11.8308324 C12.9504693,11.8308324 12.7152366,13.0319032 12.7152366,14.2720257 L12.7152366,18.974836 L9.76329605,18.974836 L9.76329605,9.46835755 L12.5971589,9.46835755 L12.5971589,10.7675189 L12.6368256,10.7675189 C13.2146663,9.77952684 14.2890899,9.18943873 15.4328668,9.23189481 C18.4247815,9.23189481 18.9764254,11.1998552 18.9764254,13.7600486 L18.9751954,18.9751435 Z M6.43252317,8.16888879 C5.48643292,8.16905854 4.71933758,7.40223852 4.7191677,6.45614827 C4.71899794,5.51005802 5.48581795,4.74296266 6.4319082,4.74279278 C7.37799845,4.74262301 8.14509382,5.50944301 8.14526371,6.45553326 C8.14534522,6.90986184 7.96494211,7.34561469 7.64374096,7.66693118 C7.32253981,7.98824766 6.88685175,8.16880719 6.43252317,8.16888879 M7.90849343,18.9751435 L4.95347798,18.9751435 L4.95347798,9.46835755 L7.90849343,9.46835755 L7.90849343,18.9751435 Z M20.4468607,2.00148552 L3.47012787,2.00148552 C2.66777241,1.99243094 2.00979231,2.63513479 2,3.43748159 L2,20.4846305 C2.00945705,21.2873639 2.6673842,21.9307041 3.47012787,21.9223954 L20.4468607,21.9223954 C21.2511925,21.9322515 21.9116962,21.2889485 21.922831,20.4846305 L21.922831,3.43625161 C21.9113617,2.63231978 21.2508045,1.98965267 20.4468607,2" id="Path_2520" fill="#212121" fill-rule="nonzero"></path>
            </g>
        </g>
    </svg>
    </a>
  </li>
  <li>
    <a class="social-media-sharer" href="https://reddit.com/submit/?title=Tracking%20prototype%20and%20exemplar%20representations%20in%20the%20brain%20across%20learning&amp;url=https%3A%2F%2Fdoi.org%2F10.7554%2FeLife.59360" target="_blank" rel="noopener noreferrer" aria-label="Share this page on Reddit">
      <svg width="24px" height="24px" viewBox="0 0 24 24" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
          <g id="reddit" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
              <g>
                  <rect id="Rectangle" x="0" y="0" width="24" height="24"></rect>
                  <path d="M11.9872095,1.051163 C18.0270212,1.051163 22.923256,5.94739779 22.923256,11.9872095 C22.923256,18.0270212 18.0270212,22.923256 11.9872095,22.923256 C5.94739779,22.923256 1.051163,18.0270212 1.051163,11.9872095 C1.051163,5.94739779 5.94739779,1.051163 11.9872095,1.051163 Z M16.5534886,5.37441881 C16.1058142,5.37441881 15.7220932,5.63023277 15.5430235,6.0139537 L12.895349,5.451163 C12.8186049,5.4383723 12.7418607,5.451163 12.6779072,5.48953509 C12.6139537,5.52790719 12.5755816,5.59186067 12.5500002,5.66860486 L11.7441863,9.48023277 C10.0430235,9.53139556 8.52093044,10.0302328 7.42093044,10.8360467 C7.13953509,10.5674421 6.74302347,10.3883723 6.32093044,10.3883723 C5.4383723,10.3883723 4.72209323,11.1046514 4.72209323,11.9872095 C4.72209323,12.6395351 5.10581416,13.1895351 5.66860486,13.445349 C5.64302347,13.5988374 5.63023277,13.7651165 5.63023277,13.9313956 C5.63023277,16.3872095 8.48255835,18.3697677 12.0127909,18.3697677 C15.5430235,18.3697677 18.395349,16.3872095 18.395349,13.9313956 C18.395349,13.7651165 18.3825583,13.6116281 18.356977,13.4581397 C18.8813956,13.2023258 19.2779072,12.6395351 19.2779072,11.9872095 C19.2779072,11.1046514 18.5616281,10.3883723 17.67907,10.3883723 C17.2441863,10.3883723 16.8604653,10.5546514 16.57907,10.8360467 C15.4918607,10.0558142 13.9825583,9.54418626 12.3197677,9.48023277 L13.0488374,6.06511649 L15.4151165,6.5639537 C15.4406979,7.16511649 15.9395351,7.651163 16.5534886,7.651163 C17.1802328,7.651163 17.6918607,7.13953509 17.6918607,6.51279091 C17.6918607,5.88604672 17.1802328,5.37441881 16.5534886,5.37441881 Z M9.72325602,15.7093025 C10.2093025,16.195349 11.2581397,16.3744188 12.0127909,16.3744188 C12.7674421,16.3744188 13.8034886,16.195349 14.3023258,15.7093025 C14.4174421,15.5941863 14.6093025,15.5941863 14.7244188,15.7093025 C14.8139537,15.8372095 14.8139537,16.0162793 14.6988374,16.1313956 C13.9186049,16.9116281 12.4348839,16.9627909 12.0000002,16.9627909 C11.5651165,16.9627909 10.0686049,16.8988374 9.301163,16.1313956 C9.18604672,16.0162793 9.18604672,15.8244188 9.301163,15.7093025 C9.41627928,15.5941863 9.60813974,15.5941863 9.72325602,15.7093025 Z M9.48023277,11.9872095 C10.1069769,11.9872095 10.6186049,12.4988374 10.6186049,13.1255816 C10.6186049,13.7523258 10.1069769,14.2639537 9.48023277,14.2639537 C8.85348858,14.2639537 8.34186067,13.7523258 8.34186067,13.1255816 C8.34186067,12.4988374 8.85348858,11.9872095 9.48023277,11.9872095 Z M14.4941863,11.9872095 C15.1209304,11.9872095 15.6325583,12.4988374 15.6325583,13.1255816 C15.6325583,13.7523258 15.1209304,14.2639537 14.4941863,14.2639537 C13.8674421,14.2639537 13.3558142,13.7523258 13.3558142,13.1255816 C13.3558142,12.4988374 13.8674421,11.9872095 14.4941863,11.9872095 Z" id="Combined-Shape" fill="#212121" fill-rule="nonzero"></path>
              </g>
          </g>
      </svg>
    </a>
  </li>
  <li>
    <a class="social-media-sharer" href="https://toot.kytta.dev/?text=Tracking%20prototype%20and%20exemplar%20representations%20in%20the%20brain%20across%20learning%20https%3A%2F%2Fdoi.org%2F10.7554%2FeLife.59360" target="_blank" rel="noopener noreferrer" aria-label="Share this page on Mastodon">
      <svg width="24px" height="24px">
          <g id="mastodon" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
              <g>
                  <rect id="Rectangle" x="0" y="0" width="24" height="24"></rect>
                  <path d='M11.5412949,0 C14.6936837,0.0250851513 17.7279692,0.357269448 19.4950021,1.1471931 C19.4950021,1.1471931 22.9995752,2.67324958 22.9995752,7.87958222 L23,8.27840764 C22.9948717,9.32900538 22.9399626,12.2394095 22.5108214,14.3877566 C22.1727192,16.0807457 19.4826471,17.9335557 16.3930961,18.2926354 C14.7820287,18.4797396 13.1958041,18.6517151 11.5043627,18.5762011 C8.73816776,18.4528442 6.55544838,17.9335557 6.55544838,17.9335557 C6.55544838,18.1956567 6.57205459,18.4452152 6.605267,18.6786105 C6.96489093,21.335697 9.312211,21.4948713 11.5357152,21.5690923 C13.7799439,21.6438306 15.7782681,21.0305374 15.7782681,21.0305374 L15.8704657,23.0052819 C15.8704657,23.0052819 14.3007146,23.8257215 11.5043627,23.9766203 C9.96237703,24.0591169 8.04774824,23.9388633 5.8177344,23.3643616 C0.981210876,22.1183796 0.149439349,17.1004442 0.022169409,12.0089343 C-0.0166226796,10.4972307 0.00729025582,9.07177343 0.00729025582,7.87958222 C0.00729025582,2.67324958 3.51199627,1.1471931 3.51199627,1.1471931 C5.27916201,0.357269448 8.31145476,0.0250851513 11.4638436,0 L11.5412949,0 Z M15.8599153,5 C14.461897,5 13.4033065,5.46697151 12.7035195,6.40103745 L12.0229667,7.39238391 L11.3425554,6.40103745 C10.642627,5.46697151 9.58403654,5 8.18615964,5 C6.97793807,5 6.00462882,5.36912752 5.26142334,6.08919046 C4.54070495,6.8092534 4.18190148,7.78253068 4.18190148,9.00730143 L4.18190148,15 L6.91358821,15 L6.91358821,9.1834452 C6.91358821,7.95732233 7.50716259,7.33498047 8.69445278,7.33498047 C10.0071899,7.33498047 10.6652556,8.0732355 10.6652556,9.5330285 L10.6652556,12.7167687 L13.3808194,12.7167687 L13.3808194,9.5330285 C13.3808194,8.0732355 14.0387436,7.33498047 15.3514807,7.33498047 C16.5387709,7.33498047 17.1323453,7.95732233 17.1323453,9.1834452 L17.1323453,15 L19.864032,15 L19.864032,9.00730143 C19.864032,7.78253068 19.5052285,6.8092534 18.7846516,6.08919046 C18.0413047,5.36912752 17.0679954,5 15.8599153,5 Z' id='Combined-Shape' fill='#212121' fill-rule='nonzero'></path>
              </g>
          </g>
      </svg>
    </a>
  </li>
</ul>


        </div>
      </div>
  </div>
</div>

                    <div class="modal modal-nojs" data-behaviour="Modal" data-trigger-id="modalContentCitations">
  <div class="modal-container">
      <div class="modal-content">
        <h6>Cite this article</h6>
        <a href="" class="modal-content__button modal-content__close-button"></a>
        <div class="modal-content__body">
          <div class="reference">
    <ol class="reference__authors_list">
        <li class="reference__author">
          Caitlin R Bowman</li>
        <li class="reference__author">
          Takako Iwashita</li>
        <li class="reference__author">
          Dagmar Zeithamova</li>
    </ol>
    <span class="reference__authors_list_suffix">(2020)</span>



      <div class="reference__title">Tracking prototype and exemplar representations in the brain across learning</div>



    <div class="reference__origin"><i>eLife</i> <b>9</b>:e59360.</div>

  <div class="doi__reference-spacing"></div>

    <span class="doi">
      https://doi.org/10.7554/eLife.59360
    </span>



</div>
<ol class="button-collection">
    <li class="button-collection__item">    <button class="button button--default" type="button" data-behaviour="ButtonClipboard" data-clipboard="Caitlin R Bowman, Takako Iwashita, Dagmar Zeithamova (2020) Tracking prototype and exemplar representations in the brain across learning eLife 9:e59360

            https://doi.org/
    ">Copy to clipboard</button>
</li>
    <li class="button-collection__item">    <a href="/articles/59360.bib" class="button button--secondary">Download BibTeX</a>
</li>
    <li class="button-collection__item">    <a href="/articles/59360.ris" class="button button--secondary">Download .RIS</a>
</li>
</ol>

        </div>
      </div>
  </div>
</div>





    <div data-behaviour="DelegateBehaviour" data-delegate-behaviour="Popup" data-selector=".article-section:not(#abstract) a" class="wrapper wrapper--content" id="content">


    <div class="content-container-grid">


            <nav class="tabbed-navigation" data-behaviour="TabbedNavigation">
  <ul class="tabbed-navigation__tabs">
      <li class="tabbed-navigation__tab-label tabbed-navigation__tab-label--active">
        <a href="/articles/59360#content">Full text</a>
      </li>
      <li class="tabbed-navigation__tab-label">
        <a href="/articles/59360/figures#content">Figures<span class="tabbed-navigation__tab-label--long"> and data</span></a>
      </li>
      <li class="tabbed-navigation__tab-label">
        <a href="/articles/59360/peer-reviews#content">Peer review</a>
      </li>
      <li class="tabbed-navigation__tab-label tabbed-navigation__tab-label--side-by-side" data-side-by-side-link="https://lens.elifesciences.org/59360">
        <a href="https://lens.elifesciences.org/59360">Side by side</a>
      </li>
  </ul>
</nav>





                <div class="jump-menu__wrapper" data-behaviour="JumpMenu">
  <ul class="jump-menu__list">
      <li class="jump-menu__item">
        <a href="#abstract" class="jump-menu">Abstract</a>
      </li>
      <li class="jump-menu__item">
        <a href="#s1" class="jump-menu">Introduction</a>
      </li>
      <li class="jump-menu__item">
        <a href="#s2" class="jump-menu">Results</a>
      </li>
      <li class="jump-menu__item">
        <a href="#s3" class="jump-menu">Discussion</a>
      </li>
      <li class="jump-menu__item">
        <a href="#s4" class="jump-menu">Materials and methods</a>
      </li>
      <li class="jump-menu__item">
        <a href="#data" class="jump-menu">Data availability</a>
      </li>
      <li class="jump-menu__item">
        <a href="#references" class="jump-menu">References</a>
      </li>
      <li class="jump-menu__item">
        <a href="#info" class="jump-menu">Article and author information</a>
      </li>
      <li class="jump-menu__item">
        <a href="#metrics" class="jump-menu">Metrics</a>
      </li>
  </ul>
</div>



        <div class="main-content-grid">






                    <section
    class="article-section article-section--first"
   id="abstract"
  data-behaviour="ArticleSection"

>

    <header class="article-section__header">
      <h2 class="article-section__header_text">Abstract</h2>

    </header>

  <div class="article-section__body">
      <p class="paragraph">There is a long-standing debate about whether categories are represented by individual category members (exemplars) or by the central tendency abstracted from individual members (prototypes). Neuroimaging studies have shown neural evidence for either exemplar representations or prototype representations, but not both. Presently, we asked whether it is possible for multiple types of category representations to exist within a single task. We designed a categorization task to promote both exemplar and prototype representations and tracked their formation across learning. We found only prototype correlates during the final test. However, interim tests interspersed throughout learning showed prototype and exemplar representations across distinct brain regions that aligned with previous studies: prototypes in ventromedial prefrontal cortex and anterior hippocampus and exemplars in inferior frontal gyrus and lateral parietal cortex. These findings indicate that, under the right circumstances, individuals may form representations at multiple levels of specificity, potentially facilitating a broad range of future decisions.</p>







  </div>

</section>



                    <section
    class="article-section "
   id="s1"
  data-behaviour="ArticleSection"

>

    <header class="article-section__header">
      <h2 class="article-section__header_text">Introduction</h2>

    </header>

  <div class="article-section__body">
      <p class="paragraph">The ability to form new conceptual knowledge is a key aspect of healthy memory function. There has been a longstanding debate about the nature of the representations underlying conceptual knowledge, which is exemplified in the domain of categorization. Some propose that categories are represented by their individual category members and that generalizing the category label to new examples involves joint retrieval and consideration of individual examples encountered in the past (i.e., exemplar models, <a href="#fig1">Figure 1A</a>; <a href="#bib32">Kruschke, 1992</a>; <a href="#bib39">Medin and Schaffer, 1978</a>; <a href="#bib44">Nosofsky, 1986</a>). Others propose that categories are represented by their central tendency – an abstract prototype containing all the most typical features of the category (i.e., prototype models, <a href="#fig1">Figure 1B</a>; <a href="#bib26">Homa, 1973</a>; <a href="#bib56">Posner and Keele, 1968</a>; <a href="#bib58">Reed, 1972</a>). Category generalization then involves consideration of a new item’s similarity to relevant category prototypes.</p>
    <div
        id="fig1"
        class="asset-viewer-inline asset-viewer-inline-- "
        data-variant=""
        data-behaviour="AssetNavigation AssetViewer ToggleableCaption"
        data-selector=".caption-text__body"
        data-asset-viewer-group="fig1"
        data-asset-viewer-uri="https://iiif.elifesciences.org/lax/59360%2Felife-59360-fig1-v2.tif/full/1500,/0/default.jpg"
        data-asset-viewer-width="1500"
        data-asset-viewer-height="808"
    >

      <div class="asset-viewer-inline__header_panel">
          <div class="asset-viewer-inline__header_text">
            <span class="asset-viewer-inline__header_text__prominent">Figure 1</span>
          </div>


            <div class="asset-viewer-inline__figure_access">
              <a href="https://elifesciences.org/download/aHR0cHM6Ly9paWlmLmVsaWZlc2NpZW5jZXMub3JnL2xheC81OTM2MCUyRmVsaWZlLTU5MzYwLWZpZzEtdjIudGlmL2Z1bGwvZnVsbC8wL2RlZmF1bHQuanBn/elife-59360-fig1-v2.jpg?_hash=TAq0KuRDXZaeJTYVzEa%2FuNskEtCryV%2FMRJrspQKr1Jk%3D" class="asset-viewer-inline__download_all_link" download="Download"><span class="visuallyhidden">Download asset</span></a>
              <a href="https://iiif.elifesciences.org/lax/59360%2Felife-59360-fig1-v2.tif/full/1500,/0/default.jpg" class="asset-viewer-inline__open_link" target="_blank" rel="noopener noreferrer"><span class="visuallyhidden">Open asset</span></a>
            </div>

      </div>

          <figure class="captioned-asset">

              <a href="https://iiif.elifesciences.org/lax/59360%2Felife-59360-fig1-v2.tif/full/1500,/0/default.jpg" class="captioned-asset__link" target="_blank" rel="noopener noreferrer">
              <picture class="captioned-asset__picture">
                  <source srcset="https://iiif.elifesciences.org/lax/59360%2Felife-59360-fig1-v2.tif/full/1234,/0/default.webp 2x, https://iiif.elifesciences.org/lax/59360%2Felife-59360-fig1-v2.tif/full/617,/0/default.webp 1x"
                      type="image/webp"
                      >
                  <source srcset="https://iiif.elifesciences.org/lax/59360%2Felife-59360-fig1-v2.tif/full/1234,/0/default.jpg 2x, https://iiif.elifesciences.org/lax/59360%2Felife-59360-fig1-v2.tif/full/617,/0/default.jpg 1x"
                      type="image/jpeg"
                      >
                  <img src="https://iiif.elifesciences.org/lax/59360%2Felife-59360-fig1-v2.tif/full/617,/0/default.jpg"

                       alt=""
                       class="captioned-asset__image"
                  >
              </picture>
              </a>





              <figcaption class="captioned-asset__caption">

                  <h6 class="caption-text__heading">Category-learning task.</h6>


                <div class="caption-text__body"><p class="paragraph">Conceptual depiction of (<b>A</b>) exemplar and (<b>B</b>) prototype models. Exemplar: categories are represented as individual exemplars. New items are classified into the category with the most similar exemplars. Prototype: categories are represented by their central tendencies (prototypes). New items are classified into the category with the most similar prototype. (<b>C</b>) Example stimuli. The leftmost stimulus is the prototype of category A and the rightmost stimulus is the prototype of category B, which shares no features with prototype A. Members of category A share more features with prototype A than prototype B, and vice versa. (<b>D</b>) During the learning phase, participants completed four study-test cycles while undergoing fMRI. In each cycle, there were two runs of observational study followed by one run of an interim generalization test. During observational study runs, participants saw training examples with their species labels without making any responses. During interim test runs, participants classified training items as well as new items at varying distances. (<b>E</b>) After all study-test cycles were complete, participants completed a final generalization test that was divided across four runs. Participants classified training items as well as new items at varying distances.</p>
</div>


              </figcaption>




          </figure>


    </div>
<p class="paragraph">Both the prototype and exemplar accounts have been formalized as quantitative models and fit to behavioral data for decades, with numerous studies supporting each model (exemplar meta-analysis: <a href="#bib46">Nosofsky, 1988</a>; prototype meta-analysis: <a href="#bib67">Smith and Minda, 2000</a>). Neuroimaging studies have also provided support for these models. Studies using univariate contrasts showed overlap between neural systems supporting categorization and recognition (<a href="#bib48">Nosofsky et al., 2012</a>), as well as medial temporal lobe involvement in categorization (<a href="#bib30">Koenig et al., 2008</a>; <a href="#bib35">Lech et al., 2016</a>; <a href="#bib43">Nomura et al., 2007</a>), both of which have been interpreted as indicating a role of exemplar retrieval in categorization. More recently, studies have used parameters generated from formal prototype and exemplar models with neuroimaging data, but with conflicting results. <a href="#bib36">Mack et al., 2013</a> found similar behavioral fits for the two models, but better fit of the exemplar model to brain data. Parts of the lateral occipital, lateral prefrontal and lateral parietal cortices tracked exemplar model predictors. No region tracked prototype predictors. The authors concluded that categorization decisions are based on memory for individual items rather than abstract prototypes. In contrast, <a href="#bib6">Bowman and Zeithamova, 2018</a> found better fit of the prototype model in both brain and behavior. The ventromedial prefrontal cortex and anterior hippocampus tracked prototype predictors, demonstrating that neural category representations can involve more than representing the individual category members, even in regions like the hippocampus typically thought to support memory for specific episodes.</p>
<p class="paragraph">Interestingly, the different brain regions identified across these two studies aligned well with the larger literature contrasting memory specificity with memory integration and generalization. Lateral prefrontal regions are thought to resolve interference between similar items in memory (<a href="#bib4">Badre and Wagner, 2005</a>; <a href="#bib5">Bowman and Dennis, 2016</a>; <a href="#bib28">Jonides et al., 1998</a>; <a href="#bib33">Kuhl et al., 2007</a>), and lateral parietal cortex supports recollective experience (<a href="#bib72">Vilberg and Rugg, 2008</a>) and maintains high fidelity representations of individual items during memory retrieval (<a href="#bib34">Kuhl and Chun, 2014</a>; <a href="#bib73">Xiao et al., 2017</a>). That these regions also tracked exemplar predictors suggests that these functions may also support categorization by maintaining representations of individual category members as distinct from one another and from non-category members. In contrast, the VMPFC and hippocampus are known to support episodic inference through memory integration of related episodes (<a href="#bib60">Schlichting et al., 2015</a>; <a href="#bib64">Shohamy and Wagner, 2008</a>; <a href="#bib76">Zeithamova et al., 2012</a>) and encoding of new information in light of prior knowledge (<a href="#bib71">van Kesteren et al., 2012</a>). That these regions also tracked prototype predictions suggests that prototype extraction may involve integrating across category exemplars, linking across items sharing a category label to form an integrated, abstract category representation. However, as neural prototype and exemplar representations were identified across studies that differed in both task details and in the categorization strategies elicited, it has not been possible to say whether differences in the brain regions supporting categorization were due to differential strength of prototype versus exemplar representations or some other aspect of the tasks.</p>
<p class="paragraph">It is possible that the seemingly conflicting findings regarding the nature of category representations arose because individuals are capable of forming either type of representation. Prior studies have compared different category structures and task instructions to identify multiple memory systems supporting categorization (e.g., <a href="#bib1">Aizenstein et al., 2000</a>; <a href="#bib2">Ashby et al., 1998</a>; <a href="#bib17">Ell et al., 2010</a>; <a href="#bib75">Zeithamova et al., 2008</a>). While such findings show that the nature of concept representations depend on task demands, it is unclear if both prototype and exemplar representations can co-exist within the same task. Such mixed representations have been identified in episodic memory tasks, with individuals sometimes forming both integrated and separated representations for the same events (<a href="#bib60">Schlichting et al., 2015</a>) and a single episode sometimes represented at multiple levels of specificity, even within the hippocampus (<a href="#bib12">Collin et al., 2015</a>). We also know that individuals sometimes use a mix of strategies in categorization, for example when most category members are classified according to a simple rule while others are memorized as exceptions to that rule (<a href="#bib13">Davis et al., 2012</a>; <a href="#bib47">Nosofsky et al., 1994</a>). These differing representations may emerge because they allow for flexibility in future decision-making, as abstract representations that discard details of individual items are well suited to making generalization judgments but are poorly suited to judgments that require specificity. Alternatively, prototype representations may emerge as a byproduct of retrieving category exemplars, and they may themselves be encoded via recurrent connections, becoming an increasingly robust part of the concept representation (<a href="#bib25">Hintzman, 1986</a>; <a href="#bib31">Koster et al., 2018</a>; <a href="#bib77">Zeithamova and Bowman, 2020</a>). Thus, under some circumstances, both prototype and exemplar representations may be apparent within the same task.</p>
<p class="paragraph">To test this idea, we used fMRI in conjunction with a categorization task designed to balance encoding of individual examples vs. abstract information. This task used a training set with examples relatively close to the prototype, which has been shown to promote prototype abstraction (<a href="#bib6">Bowman and Zeithamova, 2018</a>; <a href="#bib7">Bowman and Zeithamova, 2020</a>). To promote exemplar encoding, we used an observational training task rather than feedback-based training (<a href="#bib11">Cincotta and Seger, 2007</a>; <a href="#bib23">Heindel et al., 2013</a>; <a href="#bib53">Poldrack et al., 2001</a>). We then looked for evidence of prototype and exemplar representations in the brain and in behavioral responses. In behavior, the prototype model assumes that categories are represented by their prototypes and predicts that subjects should be best at categorizing the prototypes themselves, with decreasing accuracy for items with fewer shared features with prototypes. The prototype model does not make differential predictions for new and old (training) items at the same distance from the prototype. The exemplar model assumes that categories are represented by the previously encountered exemplars and predicts that subjects should be best at categorizing old items and new items closest to the old exemplars. The mathematical formalizations of the models further take into account that a participant may not pay equal attention to all stimulus features and that perceived distance increases non-linearly with physical distance (see Methods for more details). We note that it is sometimes possible to observe behavioral evidence for both types of representations. For example, in our prior study (<a href="#bib6">Bowman and Zeithamova, 2018</a>), participants’ behavior was better explained by the prototype model than the exemplar model, but we also observed an advantage for old items relative to new items at the same distance to prototypes, in line with exemplar but not prototype model predictions.</p>
<p class="paragraph">The key behavioral prediction of each model is the trial-by-trial probability of responding category A vs category B. These probabilities are determined for each trial by the relative similarity of the test item to the category A and category B representations proposed by each model. Once these probabilities are generated for each model, they are compared to the participant’s actual responses to determine which model better predicted the subject’s observed behavior. We also used output from the models to generate subject-specific, trial-by-trial fMRI predictions. These were derived from the similarity of each test item to either an exemplar-based or prototype-based category representation (see Methods for details). We then measured the extent to which prototype- and exemplar-tracking brain regions could be identified, focusing on the VMPFC and anterior hippocampus as predicted prototype-tracking regions, and lateral occipital, prefrontal, and parietal regions as predicted exemplar-tracking regions.</p>
<p class="paragraph">We also asked whether there are shifts across learning in the type of concept representation individuals rely on to make categorization judgments. While some have suggested that memory systems compete with one another during learning (<a href="#bib54">Poldrack and Packard, 2003</a>; <a href="#bib62">Seger, 2005</a>), prior studies fitting exemplar and prototype models to fMRI data have done so only during a categorization test that followed extensive training, potentially missing dynamics occurring earlier in concept formation. Notably, memory consolidation research suggests that memories become abstract over time, often at the expense of memory for specific details (<a href="#bib38">McClelland et al., 1995</a>; <a href="#bib41">Moscovitch et al., 2016</a>; <a href="#bib52">Payne et al., 2009</a>; <a href="#bib57">Posner and Keele, 1970</a>), suggesting that early concept representations may be exemplar-based. In contrast, research on schema-based memory shows that abstract knowledge facilitates learning of individual items by providing an organizational structure into which new information can be incorporated (<a href="#bib9">Bransford and Johnson, 1972</a>; <a href="#bib70">Tse et al., 2007</a>; <a href="#bib71">van Kesteren et al., 2012</a>). Thus, early learning may instead emphasize formation of prototype representations, with exemplars emerging later. Finally, abstract and specific representations need not trade-off in either direction. Instead, the brain may form these representations in parallel (<a href="#bib12">Collin et al., 2015</a>; <a href="#bib60">Schlichting et al., 2015</a>) without trade-off between concept knowledge and memory for individual items (<a href="#bib59">Schapiro et al., 2017</a>), generating the prediction that both prototype and exemplar representations may grow in strength over the course of learning.</p>
<p class="paragraph">In the present study, participants underwent fMRI scanning while learning two novel categories or ‘species,’ which were represented by cartoon animals varying on eight binary dimensions (<a href="#fig1">Figure 1C</a>). The learning phase consisted of two types of runs: observational study runs and interim generalization test runs (<a href="#fig1">Figure 1D</a>). During study runs, participants passively viewed individual category members with their accompanying species label (‘Febble’ or ‘Badoon’). All of the items presented during study runs differed by two features from their respective prototypes (for example, exemplars depicted in <a href="#fig1">Figure 1A</a>). After completing two runs of observational study, participants underwent an interim generalization test run in which participants classified cartoon animals into the two species. Test items included the training items as well as new items at varying distances from category prototypes. Across the entire learning phase, there were four study-test cycles, with different new test items at every cycle. The learning phase was followed by a final generalization test, whose structure was similar to the interim test runs but more extensive (<a href="#fig1">Figure 1E</a>).</p>
<p class="paragraph">To test for evidence of prototype and exemplar representations in behavior across the group, we compared accuracy for items varying in distance from category prototypes and for an accuracy advantage for training items relative to new items matched for distance from category prototypes. We also fit formal prototype and exemplar models to behavior in individual subjects, which involves computing the similarity of a given test item to either the prototype of each category (prototype model) or the individual training items from each category (exemplar model), which is then used to make predictions about how likely it is that an item will be classified into one category versus the other. The model whose predictions better match a given subject’s actual classification responses will have better fit. However, it is also possible that evidence for each of the models will be similar, potentially reflecting a mix of representations.</p>
<p class="paragraph">To test for co-existing prototype and exemplar correlates in the brain during interim and final generalization tests, we used latent metrics generated from each model as trial-by-trial predictors of BOLD activation in six regions of interest (<a href="#fig2">Figure 2</a>): ventromedial prefrontal cortex, anterior hippocampus, posterior hippocampus, lateral occipital cortex, inferior frontal gyrus, and lateral parietal cortex. To identify potential changes with learning, we tested these effects separately in the first half of the learning phase (interim tests 1 and 2) and second half of the learning phase (interim tests 3 and 4) as well as in the final test.</p>
    <div
        id="fig2"
        class="asset-viewer-inline asset-viewer-inline-- "
        data-variant=""
        data-behaviour="AssetNavigation AssetViewer ToggleableCaption"
        data-selector=".caption-text__body"
        data-asset-viewer-group="fig2"
        data-asset-viewer-uri="https://iiif.elifesciences.org/lax/59360%2Felife-59360-fig2-v2.tif/full/full/0/default.jpg"
        data-asset-viewer-width="1039"
        data-asset-viewer-height="468"
    >

      <div class="asset-viewer-inline__header_panel">
          <div class="asset-viewer-inline__header_text">
            <span class="asset-viewer-inline__header_text__prominent">Figure 2</span>
          </div>


            <div class="asset-viewer-inline__figure_access">
              <a href="https://elifesciences.org/download/aHR0cHM6Ly9paWlmLmVsaWZlc2NpZW5jZXMub3JnL2xheC81OTM2MCUyRmVsaWZlLTU5MzYwLWZpZzItdjIudGlmL2Z1bGwvZnVsbC8wL2RlZmF1bHQuanBn/elife-59360-fig2-v2.jpg?_hash=km0eZCQgsT%2BxHPj5N9pamlxfgfCmaxMpvJvdBaU3KtM%3D" class="asset-viewer-inline__download_all_link" download="Download"><span class="visuallyhidden">Download asset</span></a>
              <a href="https://iiif.elifesciences.org/lax/59360%2Felife-59360-fig2-v2.tif/full/full/0/default.jpg" class="asset-viewer-inline__open_link" target="_blank" rel="noopener noreferrer"><span class="visuallyhidden">Open asset</span></a>
            </div>

      </div>

          <figure class="captioned-asset">

              <a href="https://iiif.elifesciences.org/lax/59360%2Felife-59360-fig2-v2.tif/full/full/0/default.jpg" class="captioned-asset__link" target="_blank" rel="noopener noreferrer">
              <picture class="captioned-asset__picture">
                  <source srcset="https://iiif.elifesciences.org/lax/59360%2Felife-59360-fig2-v2.tif/full/987,/0/default.webp 1.6x, https://iiif.elifesciences.org/lax/59360%2Felife-59360-fig2-v2.tif/full/617,/0/default.webp 1x"
                      type="image/webp"
                      >
                  <source srcset="https://iiif.elifesciences.org/lax/59360%2Felife-59360-fig2-v2.tif/full/987,/0/default.jpg 1.6x, https://iiif.elifesciences.org/lax/59360%2Felife-59360-fig2-v2.tif/full/617,/0/default.jpg 1x"
                      type="image/jpeg"
                      >
                  <img src="https://iiif.elifesciences.org/lax/59360%2Felife-59360-fig2-v2.tif/full/617,/0/default.jpg"

                       alt=""
                       class="captioned-asset__image"
                  >
              </picture>
              </a>





              <figcaption class="captioned-asset__caption">

                  <h6 class="caption-text__heading">Regions of interest from a representative subject.</h6>


                <div class="caption-text__body"><p class="paragraph">Regions were defined in the native space of each subject using automated segmentation in Freesurfer.</p>
</div>


              </figcaption>




          </figure>


    </div>







  </div>

</section>



                    <section
    class="article-section "
   id="s2"
  data-behaviour="ArticleSection"
  data-initial-state="closed"
>

    <header class="article-section__header">
      <h2 class="article-section__header_text">Results</h2>

    </header>

  <div class="article-section__body">
      <section
    class="article-section "
   id="s2-1"


>

    <header class="article-section__header">
      <h3 class="article-section__header_text">Behavioral</h3>

    </header>

  <div class="article-section__body">
      <section
    class="article-section "
   id="s2-1-1"


>

    <header class="article-section__header">
      <h4 class="article-section__header_text">Accuracy</h4>

    </header>

  <div class="article-section__body">
      <section
    class="article-section "
   id="s2-1-1-1"


>

    <header class="article-section__header">
      <h5 class="article-section__header_text">Interim tests</h5>

    </header>

  <div class="article-section__body">
      <p class="paragraph">Categorization performance across the four interim tests is presented in <a href="#fig3">Figure 3A</a>. We first tested whether generalization accuracy improved across the learning phase and whether generalization of category labels to new items differed across items of varying distance to category prototypes. There was a significant main effect of interim test number [<i>F</i>(3,84)=3.27, p=0.03, <math id="inf1"><mrow><msubsup><mi>η</mi><mi>p</mi><mn>2</mn></msubsup></mrow></math> = 0.11], with a significant linear effect [<i>F</i>(1,28)=9.91, p=0.004, <math id="inf2"><mrow><msubsup><mi>η</mi><mi>p</mi><mn>2</mn></msubsup></mrow></math> = 0.26] driven by increasing generalization accuracy across the interim tests. There was also a significant main effect of item distance [<i>F</i>(3,84)=51.75, p&lt;0.001, <math id="inf3"><mrow><msubsup><mi>η</mi><mi>p</mi><mn>2</mn></msubsup></mrow></math> = 0.65] with a significant linear effect [<i>F</i>(1,28)=126.04, p&lt;0.001, <math id="inf4"><mrow><msubsup><mi>η</mi><mi>p</mi><mn>2</mn></msubsup></mrow></math> = 0.82] driven by better accuracy for items closer to category prototypes. The interim test number x item distance interaction effect was not significant [<i>F</i>(9,252)=0.62, p=0.78, <math id="inf5"><mrow><msubsup><mi>η</mi><mi>p</mi><mn>2</mn></msubsup></mrow></math> = 0.02]. We next tested whether accuracy for old training items was higher than new items of the same distance (i.e., distance 2) and whether that differed over the course of the learning phase. There was a linear effect of interim test number [<i>F</i>(1,28)=16.78, p&lt;0.001, <math id="inf6"><mrow><msubsup><mi>η</mi><mi>p</mi><mn>2</mn></msubsup></mrow></math> = 0.38] driven by increasing accuracy across the tests. There was also a significant main effect of item type (old vs. new) [<i>F</i>(1,28)=8.76, p=0.01, <math id="inf7"><mrow><msubsup><mi>η</mi><mi>p</mi><mn>2</mn></msubsup></mrow></math> = 0.24], driven by higher accuracy for old items (M = 0.83, SD = 0.11) relative to new items of the same distance from the prototypes (M = 0.77, SD = 0.10). The interim test number x item type interaction effect was not significant [<i>F</i>(3,84)=0.35, p=0.79, <math id="inf8"><mrow><msubsup><mi>η</mi><mi>p</mi><mn>2</mn></msubsup></mrow></math> = 0.01], indicating that the advantage for old compared to new items was relatively stable across learning. To summarize, we observed a reliable typicality gradient where accuracy decreased with the distance from the prototypes and both old and new items at the distance two numerically fell between distance 1 and distance three items (<a href="#fig3">Figure 3A</a>). However, within distance two items, we also observed a reliable advantage for the old items compared to new items, an aspect of the data that would not be predicted by the prototype model.</p>
    <div
        id="fig3"
        class="asset-viewer-inline asset-viewer-inline-- "
        data-variant=""
        data-behaviour="AssetNavigation AssetViewer ToggleableCaption"
        data-selector=".caption-text__body"
        data-asset-viewer-group="fig3"
        data-asset-viewer-uri="https://iiif.elifesciences.org/lax/59360%2Felife-59360-fig3-v2.tif/full/,1500/0/default.jpg"
        data-asset-viewer-width="864"
        data-asset-viewer-height="1500"
    >

      <div class="asset-viewer-inline__header_panel">
          <div class="asset-viewer-inline__header_text">
            <span class="asset-viewer-inline__header_text__prominent">Figure 3</span>
          </div>


            <div class="asset-viewer-inline__figure_access">
              <a href="https://elifesciences.org/download/aHR0cHM6Ly9paWlmLmVsaWZlc2NpZW5jZXMub3JnL2xheC81OTM2MCUyRmVsaWZlLTU5MzYwLWZpZzMtdjIudGlmL2Z1bGwvZnVsbC8wL2RlZmF1bHQuanBn/elife-59360-fig3-v2.jpg?_hash=Nre%2BW3%2FgWtAjF3GhQXVpNOnP%2FaHFXpoWkAsp7ZnYGq8%3D" class="asset-viewer-inline__download_all_link" download="Download"><span class="visuallyhidden">Download asset</span></a>
              <a href="https://iiif.elifesciences.org/lax/59360%2Felife-59360-fig3-v2.tif/full/,1500/0/default.jpg" class="asset-viewer-inline__open_link" target="_blank" rel="noopener noreferrer"><span class="visuallyhidden">Open asset</span></a>
            </div>

      </div>

          <figure class="captioned-asset">

              <a href="https://iiif.elifesciences.org/lax/59360%2Felife-59360-fig3-v2.tif/full/,1500/0/default.jpg" class="captioned-asset__link" target="_blank" rel="noopener noreferrer">
              <picture class="captioned-asset__picture">
                  <source srcset="https://iiif.elifesciences.org/lax/59360%2Felife-59360-fig3-v2.tif/full/987,/0/default.webp 1.6x, https://iiif.elifesciences.org/lax/59360%2Felife-59360-fig3-v2.tif/full/617,/0/default.webp 1x"
                      type="image/webp"
                      >
                  <source srcset="https://iiif.elifesciences.org/lax/59360%2Felife-59360-fig3-v2.tif/full/987,/0/default.jpg 1.6x, https://iiif.elifesciences.org/lax/59360%2Felife-59360-fig3-v2.tif/full/617,/0/default.jpg 1x"
                      type="image/jpeg"
                      >
                  <img src="https://iiif.elifesciences.org/lax/59360%2Felife-59360-fig3-v2.tif/full/617,/0/default.jpg"

                       alt=""
                       class="captioned-asset__image"
                  >
              </picture>
              </a>





              <figcaption class="captioned-asset__caption">

                  <h6 class="caption-text__heading">Behavioral accuracy for interim and final tests.</h6>


                <div class="caption-text__body"><p class="paragraph">(<b>A</b>) Mean generalization accuracy across each of four interim tests completed during the learning phase. Source data can be found in <a href="#fig3sdata1">Figure 3—source data 1</a>. (<b>B</b>) Mean categorization accuracy in the final test. Source data can be found in <a href="#fig3sdata2">Figure 3—source data 2</a>. In both cases, accuracies are separated by distance from category prototypes (0–3) and old vs. new (applicable to distance two items only). Error bars represent the standard error of the mean. </p>
</div>


              </figcaption>




          </figure>

        <div class="additional-assets">


          <dl class="additional-assets__list">
              <dt class="additional-asset__text" id="fig3sdata1">
                    <h6 class="caption-text__heading">Figure 3—source data 1</h6>

                    <p class="caption-text__heading">Behavioral accuracy - interim tests.</p>


              </dt>

              <dd class="additional-asset__access">


                  <a class="additional-asset__link" href="https://cdn.elifesciences.org/articles/59360/elife-59360-fig3-data1-v2.csv.zip">https://cdn.elifesciences.org/articles/59360/elife-59360-fig3-data1-v2.csv.zip</a>

                  <div>
                    <a class="additional-asset__link additional-asset__link--download" href="https://elifesciences.org/download/aHR0cHM6Ly9jZG4uZWxpZmVzY2llbmNlcy5vcmcvYXJ0aWNsZXMvNTkzNjAvZWxpZmUtNTkzNjAtZmlnMy1kYXRhMS12Mi5jc3Yuemlw/elife-59360-fig3-data1-v2.csv.zip?_hash=CnJJABmOpwXMI14Kpy7lpISHDeC6KYylGy1eWo69N6w%3D" download="elife-59360-fig3-data1-v2.csv.zip">Download elife-59360-fig3-data1-v2.csv.zip</a>
                  </div>
              </dd>
              <dt class="additional-asset__text" id="fig3sdata2">
                    <h6 class="caption-text__heading">Figure 3—source data 2</h6>

                    <p class="caption-text__heading">Behavioral accuracy - final test.</p>


              </dt>

              <dd class="additional-asset__access">


                  <a class="additional-asset__link" href="https://cdn.elifesciences.org/articles/59360/elife-59360-fig3-data2-v2.csv.zip">https://cdn.elifesciences.org/articles/59360/elife-59360-fig3-data2-v2.csv.zip</a>

                  <div>
                    <a class="additional-asset__link additional-asset__link--download" href="https://elifesciences.org/download/aHR0cHM6Ly9jZG4uZWxpZmVzY2llbmNlcy5vcmcvYXJ0aWNsZXMvNTkzNjAvZWxpZmUtNTkzNjAtZmlnMy1kYXRhMi12Mi5jc3Yuemlw/elife-59360-fig3-data2-v2.csv.zip?_hash=T3UvrcmWnd%2Fk1XdDvSU5wyNqbs%2By6IvIfXBDIAj3yLM%3D" download="elife-59360-fig3-data2-v2.csv.zip">Download elife-59360-fig3-data2-v2.csv.zip</a>
                  </div>
              </dd>
          </dl>

        </div>

    </div>







  </div>

</section>







  </div>

</section>
<section
    class="article-section "
   id="s2-1-2"


>

    <header class="article-section__header">
      <h4 class="article-section__header_text">Final test</h4>

    </header>

  <div class="article-section__body">
      <p class="paragraph">Accuracies for generalization items at each distance from the prototype as well as for training items (all training items were at distance two from the prototypes) are presented in <a href="#fig3">Figure 3B</a>. A repeated measures ANOVA on new items that tested the effect of distance from category prototypes on generalization accuracy showed a main effect of item distance [<i>F</i>(3,84)=53.61, p&lt;0.001, <math id="inf9"><mrow><msubsup><mi>η</mi><mi>p</mi><mn>2</mn></msubsup></mrow></math> = 0.66] that was well characterized by a linear effect [<i>F</i>(1,28)=124.55, p&lt;0.001, <math id="inf10"><mrow><msubsup><mi>η</mi><mi>p</mi><mn>2</mn></msubsup></mrow></math> = 0.82]. Thus, the categorization gradient driven by higher accuracy for items closer to category prototypes observed during learning was also strong during the final test. In contrast, a paired t-test for accuracy on old relative to new items at distance two showed that the numeric advantage for old relative to new items was not statistically significant in the final test [<i>t</i>(28)=0.93, p=0.36, <i>CI</i><sub>95</sub>[−0.03,.08], <i>d</i> = 0.22].</p>







  </div>

</section>







  </div>

</section>
<section
    class="article-section "
   id="s2-2"


>

    <header class="article-section__header">
      <h3 class="article-section__header_text">Behavioral model fits</h3>

    </header>

  <div class="article-section__body">
      <p class="paragraph"><a href="#fig4">Figure 4a-c</a> presents model fits in terms of raw negative log likelihood for each phase (lower numbers mean lower model fit error and thus better fit). Fits from the two models tend to be correlated. If a subject randomly guesses on the majority trials (such as early in learning), neither model will fit the subject’s responses well and the subject will have higher (mis)fit values for both models. As a subject learns and does better on the task, fits of both models will tend to improve because items close to the old exemplars of category A tend to be, on average, closer to the category A prototype than the category B prototype and vice versa. For example, even if a subject had a purely exemplar representation, the prototype model would still fit that subject’s behavior quite well, albeit not as well as the exemplar model. Due to the correlation between model fits, the exact fit value for one model is not sufficient to determine a subject’s strategy, only the relative fit of one model compared to the other. Visually, in <a href="#fig4">Figure 4a–c</a>, subjects above the diagonal are better fit by the exemplar model, participants below the line are better fit by the prototype model, and participants near the line are fit comparably well by both models. Thus, although the model fits tend to be correlated across-subject, the within-subject advantage for one model over another is still detectable and meaningful. To quantify which model fits are comparable and which are reliably different, we took a Monte Carlo approach and compared the observed model fit differences to a null distribution expected by chance alone (see Methods for details). <a href="#fig4">Figure 4d–f</a> presents the percentage of subjects that were classified as having used a prototype strategy, exemplar strategy, or having model fits that were not reliably different from one another (‘similar’ fit). In the first half of learning, the majority of subjects (66%) had similar prototype and exemplar model fits. In the second half of learning and the final test, the majority of subjects (56% and 66%, respectively) were best fit by the prototype model. Prototype and exemplar model fits may not differ reliably for a given subject, such as when the subject’s responses are perfectly consistently with both models (as can happen in high-performing subjects) or when some responses are more consistent with one model while other response are more consistent with the other model. In such cases, a subject may be relying on a single representation but we cannot discern which, or the subject may rely to some extent on both types of representations.</p>
    <div
        id="fig4"
        class="asset-viewer-inline asset-viewer-inline-- "
        data-variant=""
        data-behaviour="AssetNavigation AssetViewer ToggleableCaption"
        data-selector=".caption-text__body"
        data-asset-viewer-group="fig4"
        data-asset-viewer-uri="https://iiif.elifesciences.org/lax/59360%2Felife-59360-fig4-v2.tif/full/1500,/0/default.jpg"
        data-asset-viewer-width="1500"
        data-asset-viewer-height="1082"
    >

      <div class="asset-viewer-inline__header_panel">
          <div class="asset-viewer-inline__header_text">
            <span class="asset-viewer-inline__header_text__prominent">Figure 4</span>
          </div>


            <div class="asset-viewer-inline__figure_access">
              <a href="https://elifesciences.org/download/aHR0cHM6Ly9paWlmLmVsaWZlc2NpZW5jZXMub3JnL2xheC81OTM2MCUyRmVsaWZlLTU5MzYwLWZpZzQtdjIudGlmL2Z1bGwvZnVsbC8wL2RlZmF1bHQuanBn/elife-59360-fig4-v2.jpg?_hash=mCIn0UlitvzMk3OOUXOvgJg7TB1vhpT0fPSTpj4O4bY%3D" class="asset-viewer-inline__download_all_link" download="Download"><span class="visuallyhidden">Download asset</span></a>
              <a href="https://iiif.elifesciences.org/lax/59360%2Felife-59360-fig4-v2.tif/full/1500,/0/default.jpg" class="asset-viewer-inline__open_link" target="_blank" rel="noopener noreferrer"><span class="visuallyhidden">Open asset</span></a>
            </div>

      </div>

          <figure class="captioned-asset">

              <a href="https://iiif.elifesciences.org/lax/59360%2Felife-59360-fig4-v2.tif/full/1500,/0/default.jpg" class="captioned-asset__link" target="_blank" rel="noopener noreferrer">
              <picture class="captioned-asset__picture">
                  <source srcset="https://iiif.elifesciences.org/lax/59360%2Felife-59360-fig4-v2.tif/full/1234,/0/default.webp 2x, https://iiif.elifesciences.org/lax/59360%2Felife-59360-fig4-v2.tif/full/617,/0/default.webp 1x"
                      type="image/webp"
                      >
                  <source srcset="https://iiif.elifesciences.org/lax/59360%2Felife-59360-fig4-v2.tif/full/1234,/0/default.jpg 2x, https://iiif.elifesciences.org/lax/59360%2Felife-59360-fig4-v2.tif/full/617,/0/default.jpg 1x"
                      type="image/jpeg"
                      >
                  <img src="https://iiif.elifesciences.org/lax/59360%2Felife-59360-fig4-v2.tif/full/617,/0/default.jpg"

                       alt=""
                       class="captioned-asset__image"
                  >
              </picture>
              </a>





              <figcaption class="captioned-asset__caption">

                  <h6 class="caption-text__heading">Behavioral model fits.</h6>


                <div class="caption-text__body"><p class="paragraph">Scatter plots indicate the relative exemplar vs. prototype model fits for each subject. Fits are given in terms of negative log likelihood (i.e., model error) such that lower values reflect better model fit. Each dot represents a single subject and the trendline represents equal prototype and exemplar fit. Dots above the line have better exemplar relative to prototype model fit. Dots below the line have better prototype relative to exemplar model fit. Pie charts indicate the percentage of individual subjects classified as best fit by the prototype model (in blue), the exemplar model (in red), and those similarly fit by the two models (in grey). Model fits were computed separately for the 1<sup>st</sup> half of the learning phase (interim tests 1–2, <b>A,D</b>), the 2<sup>nd</sup> half of the learning phase (interim tests 3–4, <b>B,E</b>), and the final test (<b>C,F</b>). Source data for all phases can be found in <a href="#fig4sdata1">Figure 4—source data 1</a>.</p>
</div>


              </figcaption>




          </figure>

        <div class="additional-assets">


          <dl class="additional-assets__list">
              <dt class="additional-asset__text" id="fig4sdata1">
                    <h6 class="caption-text__heading">Figure 4—source data 1</h6>

                    <p class="caption-text__heading">Behavioral model fits - all phases.</p>


              </dt>

              <dd class="additional-asset__access">


                  <a class="additional-asset__link" href="https://cdn.elifesciences.org/articles/59360/elife-59360-fig4-data1-v2.csv.zip">https://cdn.elifesciences.org/articles/59360/elife-59360-fig4-data1-v2.csv.zip</a>

                  <div>
                    <a class="additional-asset__link additional-asset__link--download" href="https://elifesciences.org/download/aHR0cHM6Ly9jZG4uZWxpZmVzY2llbmNlcy5vcmcvYXJ0aWNsZXMvNTkzNjAvZWxpZmUtNTkzNjAtZmlnNC1kYXRhMS12Mi5jc3Yuemlw/elife-59360-fig4-data1-v2.csv.zip?_hash=6YU7gMt0IQzT4bP1Fsf1pDnbfud9avIbeAL%2Fq4OXSDM%3D" download="elife-59360-fig4-data1-v2.csv.zip">Download elife-59360-fig4-data1-v2.csv.zip</a>
                  </div>
              </dd>
          </dl>

        </div>

    </div>
<p class="paragraph">We formally compared model fits for interim tests across the first and second half of the learning phase using a repeated-measures ANOVA on raw model fits. There was a significant main effect of learning phase [<i>F</i>(1,28)=39.74, p&lt;0.001, <math id="inf11"><mrow><msubsup><mi>η</mi><mi>p</mi><mn>2</mn></msubsup></mrow></math> = 0.59] with better model fits (i.e., lower error) in the second half of the learning phase (M = 5.98, SD = 5.81) compared to the first half (M = 10.64, SD = 6.72). There was also a significant main effect of model [<i>F</i>(1,28)=17.50, p&lt;0.001, <math id="inf12"><mrow><msubsup><mi>η</mi><mi>p</mi><mn>2</mn></msubsup></mrow></math> = 0.39] with better fit for the prototype model (M = 7.86, SD = 5.95) compared to the exemplar model (M = 8.77, SD = 6.02). The learning phase x model interaction effect was not significant [<i>F</i>(1,28)=0.01, p=0.91, <math id="inf13"><mrow><msubsup><mi>η</mi><mi>p</mi><mn>2</mn></msubsup></mrow></math> = 0.001], with a similar prototype advantage in the first half (d = 0.13, <i>CI</i><sub>95</sub>[0.31,1.45]) as in the second half (d = 0.16, <i>CI</i><sub>95</sub>[0.22,1.65]). When we compared prototype and exemplar model fits in the final test, we again found a significant advantage for the prototype model over the exemplar model [<i>t</i>(28)=3.53, p=0.001, <i>CI</i><sub>95</sub>[0.89, 3.39], <i>d</i> = 0.23]. Thus, the prototype model provided an overall better fit to behavioral responses throughout the learning phase and final test, and the effect size of the prototype advantage was largest in the final test.</p>







  </div>

</section>
<section
    class="article-section "
   id="s2-3"


>

    <header class="article-section__header">
      <h3 class="article-section__header_text">fMRI</h3>

    </header>

  <div class="article-section__body">
      <section
    class="article-section "
   id="s2-3-1"


>

    <header class="article-section__header">
      <h4 class="article-section__header_text">Model-based MRI</h4>

    </header>

  <div class="article-section__body">
      <p class="paragraph">The behavioral model fitting described above maximizes the correspondence between response probabilities generated by the two models and the actual participants’ patterns of responses. Once the parameters for the best fitting prototype and best fitting exemplar representations were estimated from the behavioral data, we utilized them to construct model-based fMRI predictors, one exemplar-based predictor and one prototype-based predictor for each participant. For each test item, a model prediction was computed as the similarity of the item to the underlying prototype or exemplar representation regardless of category (representational match; see Methods for details). The trial-by-trial model predictions from both models were then used for fMRI analysis to identify regions that have signal consistent with either model. Importantly, even when behavioral fits are comparable between the two models, the neural model predictions can remain dissociable as they more directly index the underlying representations that are different between the models (<a href="#bib36">Mack et al., 2013</a>). For example, the prototypes would be classified into their respective categories with high probability by either model because they are much closer to one category’s representation than the other, generating similar behavioral prediction for that trial. However, the representational match will be much higher for the prototype model than the exemplar model as the prototype is not particularly close to any old exemplars. Thus, the neural predictors can dissociate the models to a greater degree than behavioral predictions (<a href="#bib36">Mack et al., 2013</a>). Furthermore, the neural model fits can help detect evidence of both kinds of representations, even if one dominates the behavior.</p>
<section
    class="article-section "
   id="s2-3-1-1"


>

    <header class="article-section__header">
      <h5 class="article-section__header_text">Learning phase</h5>

    </header>

  <div class="article-section__body">
      <p class="paragraph">We first tested the degree to which prototype and exemplar information was represented across ROIs and across different points of the learning phase. Using the data from the interim generalization tests, we compared neural model fits across our six ROIs across the first and second half of the learning phase. Full ANOVA results are presented in <a href="#table1">Table 1</a>. <a href="#fig5">Figure 5</a> presents neural model fits for each ROI. <a href="#fig5">Figure 5A</a> represents 1<sup>st</sup> half of the learning phase, <a href="#fig5">Figure 5B</a> represents the 2<sup>nd</sup> half of the learning phase, and <a href="#fig5">Figure 5C</a> represents fits collapsed across the entire learning phase (to illustrate the main effects of ROI, model and ROI x model interaction).</p>
    <div
        id="fig5"
        class="asset-viewer-inline asset-viewer-inline-- "
        data-variant=""
        data-behaviour="AssetNavigation AssetViewer ToggleableCaption"
        data-selector=".caption-text__body"
        data-asset-viewer-group="fig5"
        data-asset-viewer-uri="https://iiif.elifesciences.org/lax/59360%2Felife-59360-fig5-v2.tif/full/,1500/0/default.jpg"
        data-asset-viewer-width="921"
        data-asset-viewer-height="1500"
    >

      <div class="asset-viewer-inline__header_panel">
          <div class="asset-viewer-inline__header_text">
            <span class="asset-viewer-inline__header_text__prominent">Figure 5</span>
          </div>


            <div class="asset-viewer-inline__figure_access">
              <a href="https://elifesciences.org/download/aHR0cHM6Ly9paWlmLmVsaWZlc2NpZW5jZXMub3JnL2xheC81OTM2MCUyRmVsaWZlLTU5MzYwLWZpZzUtdjIudGlmL2Z1bGwvZnVsbC8wL2RlZmF1bHQuanBn/elife-59360-fig5-v2.jpg?_hash=I6qRHjS4O4nJXpC88s67Hdm1A03%2FTFqVAaEPyofhp08%3D" class="asset-viewer-inline__download_all_link" download="Download"><span class="visuallyhidden">Download asset</span></a>
              <a href="https://iiif.elifesciences.org/lax/59360%2Felife-59360-fig5-v2.tif/full/,1500/0/default.jpg" class="asset-viewer-inline__open_link" target="_blank" rel="noopener noreferrer"><span class="visuallyhidden">Open asset</span></a>
            </div>

      </div>

          <figure class="captioned-asset">

              <a href="https://iiif.elifesciences.org/lax/59360%2Felife-59360-fig5-v2.tif/full/,1500/0/default.jpg" class="captioned-asset__link" target="_blank" rel="noopener noreferrer">
              <picture class="captioned-asset__picture">
                  <source srcset="https://iiif.elifesciences.org/lax/59360%2Felife-59360-fig5-v2.tif/full/1234,/0/default.webp 2x, https://iiif.elifesciences.org/lax/59360%2Felife-59360-fig5-v2.tif/full/617,/0/default.webp 1x"
                      type="image/webp"
                      >
                  <source srcset="https://iiif.elifesciences.org/lax/59360%2Felife-59360-fig5-v2.tif/full/1234,/0/default.jpg 2x, https://iiif.elifesciences.org/lax/59360%2Felife-59360-fig5-v2.tif/full/617,/0/default.jpg 1x"
                      type="image/jpeg"
                      >
                  <img src="https://iiif.elifesciences.org/lax/59360%2Felife-59360-fig5-v2.tif/full/617,/0/default.jpg"

                       alt=""
                       class="captioned-asset__image"
                  >
              </picture>
              </a>





              <figcaption class="captioned-asset__caption">

                  <h6 class="caption-text__heading">Neural prototype and exemplar model fits.</h6>


                <div class="caption-text__body"><p class="paragraph">Neural model fits for each region of interest for (<b>A</b>) the first half of the learning phase, (<b>B</b>) the second half of the learning phase, (<b>C</b>) the overall learning phase (averaged across the first and second half of learning), and (<b>D</b>) the final test. Prototype fits are in blue, exemplar fits in red. Neural model fit is the effect size: the mean/SD of ß-values within each ROI, averaged across appropriate runs. VMPFC = ventromedial prefrontal cortex, ahip = anterior hippocampus, phip = posterior hippocampus, LO = lateral occipital cortex, IFG = inferior frontal gyrus, and Lat. Par. = lateral parietal cortex. Source data for interim tests is in <a href="#fig5sdata1">Figure 5—source data 1</a> and <a href="#fig5sdata2">Figure 5—source data 2</a> for the final test.</p>
</div>


              </figcaption>




          </figure>

        <div class="additional-assets">


          <dl class="additional-assets__list">
              <dt class="additional-asset__text" id="fig5sdata1">
                    <h6 class="caption-text__heading">Figure 5—source data 1</h6>

                    <p class="caption-text__heading">Neural model fits - interim tests.</p>


              </dt>

              <dd class="additional-asset__access">


                  <a class="additional-asset__link" href="https://cdn.elifesciences.org/articles/59360/elife-59360-fig5-data1-v2.csv.zip">https://cdn.elifesciences.org/articles/59360/elife-59360-fig5-data1-v2.csv.zip</a>

                  <div>
                    <a class="additional-asset__link additional-asset__link--download" href="https://elifesciences.org/download/aHR0cHM6Ly9jZG4uZWxpZmVzY2llbmNlcy5vcmcvYXJ0aWNsZXMvNTkzNjAvZWxpZmUtNTkzNjAtZmlnNS1kYXRhMS12Mi5jc3Yuemlw/elife-59360-fig5-data1-v2.csv.zip?_hash=sDEfsT2adsPF2uEGptxCZplw16A%2FxaL5gJHuMwTV92Q%3D" download="elife-59360-fig5-data1-v2.csv.zip">Download elife-59360-fig5-data1-v2.csv.zip</a>
                  </div>
              </dd>
              <dt class="additional-asset__text" id="fig5sdata2">
                    <h6 class="caption-text__heading">Figure 5—source data 2</h6>

                    <p class="caption-text__heading">Neural model fits - final test.</p>


              </dt>

              <dd class="additional-asset__access">


                  <a class="additional-asset__link" href="https://cdn.elifesciences.org/articles/59360/elife-59360-fig5-data2-v2.csv.zip">https://cdn.elifesciences.org/articles/59360/elife-59360-fig5-data2-v2.csv.zip</a>

                  <div>
                    <a class="additional-asset__link additional-asset__link--download" href="https://elifesciences.org/download/aHR0cHM6Ly9jZG4uZWxpZmVzY2llbmNlcy5vcmcvYXJ0aWNsZXMvNTkzNjAvZWxpZmUtNTkzNjAtZmlnNS1kYXRhMi12Mi5jc3Yuemlw/elife-59360-fig5-data2-v2.csv.zip?_hash=a7OFYnZ%2B4g%2BDdCiJiTis7lLO%2Fsi8tYvMI2eC5B6UIFU%3D" download="elife-59360-fig5-data2-v2.csv.zip">Download elife-59360-fig5-data2-v2.csv.zip</a>
                  </div>
              </dd>
          </dl>

        </div>

    </div>
    <div
        id="table1"
        class="asset-viewer-inline asset-viewer-inline--table "
        data-variant="table"
        data-behaviour="AssetNavigation AssetViewer ToggleableCaption"
        data-selector=".caption-text__body"
        data-asset-viewer-group="table1"
    >

      <div class="asset-viewer-inline__header_panel">
          <div class="asset-viewer-inline__header_text">
            <span class="asset-viewer-inline__header_text__prominent">Table 1</span>
          </div>


            <div class="asset-viewer-inline__figure_access">
            </div>

      </div>

          <figure class="captioned-asset">






              <figcaption class="captioned-asset__caption">

                  <h6 class="caption-text__heading">ANOVA results for model-based fMRI during the learning phase.</h6>





              </figcaption>



              <div class="table">


                  <div class="table__table">

                    <table><thead><tr><th valign="top">Effect</th><th valign="top">df</th><th valign="top">F</th><th valign="top">P</th><th><math id="inf14"><mrow><msubsup><mi>η</mi><mi>p</mi><mn>2</mn></msubsup></mrow></math></th></tr></thead><tbody><tr><td valign="top">ROI</td><td valign="top">3.4,95.6 GG</td><td valign="top">3.90</td><td valign="top">.002</td><td valign="top">.12</td></tr><tr><td valign="top">Model</td><td valign="top">1,28</td><td valign="top">2.60</td><td valign="top">.12</td><td valign="top">.09</td></tr><tr><td valign="top">Learning half</td><td valign="top">1,28</td><td valign="top">2.18</td><td valign="top">.15</td><td valign="top">.07</td></tr><tr><td valign="top">ROI x Model</td><td valign="top">2.9,80.3 GG</td><td valign="top">5.91</td><td valign="top">.001</td><td valign="top">.17</td></tr><tr><td valign="top">ROI x Learning half</td><td valign="top">3.1,86.9 GG</td><td valign="top">0.53</td><td valign="top">.67</td><td valign="top">.02</td></tr><tr><td valign="top">Model x Learning half</td><td valign="top">1,28</td><td valign="top">0.09</td><td valign="top">.76</td><td valign="top">.003</td></tr><tr><td valign="top">ROI x Model x Learning Half</td><td valign="top">3.2,89.6 GG</td><td valign="top">2.31</td><td valign="top">.08</td><td valign="top">.08</td></tr></tbody></table>

                  </div>



              </div>

          </figure>


    </div>
<p class="paragraph">As predicted, there was a significant ROI x Model interaction effect, indicating that there were differences across regions in the type of category information that they tracked. To understand the nature of this interaction, we computed follow-up t-tests on the neural model fits in each ROI, collapsed across the first and second half of the learning phase. Consistent with prior work (<a href="#bib6">Bowman and Zeithamova, 2018</a>), the VMPFC and anterior hippocampus (our predicted prototype regions) significantly tracked prototype information [VMPFC: <i>t</i>(28) = 2.86, p=0.004, <i>CI</i><sub>95</sub>[µ &gt;0.06], <i>d</i> = 0.75]; [anterior hippocampus: <i>t</i>(28) = 1.88, p=0.04, <i>CI</i><sub>95</sub>[µ &gt;0.009], <i>d</i> = 0.49]. Prototype correlates were numerically but not significantly stronger than exemplar correlates in both regions [VMPFC: <i>t</i>(28) = 1.23, p=0.11, <i>d</i> = 0.34, <i>CI</i><sub>95</sub>[µ &gt;−0.03]]; (anterior hippocampus: <i>t</i>(28) = 0.87, p=0.19, <i>d</i> = 0.22, <i>CI</i><sub>95</sub>[µ &gt;−0.05]). For the predicted exemplar regions, we found that both lateral parietal cortex and inferior frontal gyrus significantly tracked exemplar model predictions [lateral parietal: <i>t</i>(28) = 2.06, p=0.02, <i>CI</i><sub>95</sub>[µ &gt;0.02], <i>d</i> = 0.54]; [inferior frontal: <i>t</i>(28) = 2.40, p=0.01, <i>CI</i><sub>95</sub>[µ &gt;0.03], <i>d</i> = 0.63], with numerically positive exemplar correlates in lateral occipital cortex that were not statistically significant [<i>t</i>(28)=0.78, p=0.22, <i>CI</i><sub>95</sub>[µ &gt;−0.05], <i>d</i> = 0.20]. When comparing neural exemplar fits to neural prototype fits, there was a significant exemplar advantage in both lateral parietal cortex [<i>t</i>(28)=3.00, p=0.003, <i>d</i> = 0.71, <i>CI</i><sub>95</sub>[µ &gt;0.09]], and in inferior frontal gyrus [<i>t</i>(28)=2.63, p=0.01, <i>d</i> = 0.67, <i>CI</i><sub>95</sub>[µ &gt;0.06]], that did not reach significance in the lateral occipital cortex [<i>t</i>(28)=1.44, p=0.08, <i>d</i> = 0.36, <i>CI</i><sub>95</sub>[µ &gt;−0.06]].</p>
<p class="paragraph">As in our prior study, the posterior hippocampus showed numerically better fit of the exemplar predictor, but neither the exemplar effect [<i>t</i>(28)=1.88, p=0.07, <i>CI</i><sub>95</sub>[−0.01,.13], <i>d</i> = 0.49] nor the prototype effect reached significance [<i>t</i>(28)=−1.14, p=0.26, <i>CI</i><sub>95</sub>[−0.12,.03], <i>d</i> = 0.30]. Comparing the effects in the two hippocampal regions as part of a 2 (hippocampal ROI: anterior, posterior) x 2 (model: prototype, exemplar) repeated-measures ANOVA, we found a significant interaction [<i>F</i>(1,28)=9.04, p=0.006, <math id="inf15"><mrow><msubsup><mi>η</mi><mi>p</mi><mn>2</mn></msubsup></mrow></math> = 0.24], showing that there is a dissociation along the hippocampal long axis in the type of category information represented. Taken together, we found evidence for different types of category information represented across distinct regions of the brain.</p>
<p class="paragraph">We were also interested in whether there was a shift in representations that could be detected across learning. The only effect that included learning phase that approached significance was the three-way ROI x model x learning phase interaction, likely reflecting the more pronounced region x model differences later in learning (<a href="#fig5">Figure 5A</a> vs. <a href="#fig5">Figure 5B</a>).</p>







  </div>

</section>
<section
    class="article-section "
   id="s2-3-1-2"


>

    <header class="article-section__header">
      <h5 class="article-section__header_text">Final test</h5>

    </header>

  <div class="article-section__body">
      <p class="paragraph"><a href="#fig5">Figure 5D</a> presents neural model fits from each ROI during the final test. We tested whether the differences across ROIs identified during the learning phase were also present in the final test. As during the learning phase, we found a significant main effect of ROI [<i>F</i>(2.9,79.8)=9.13, p&lt;0.001, <math id="inf16"><mrow><msubsup><mi>η</mi><mi>p</mi><mn>2</mn></msubsup></mrow></math> = 0.25, GG] and no main effect of model [<i>F</i>(1,28)=1.65, p=0.21, <math id="inf17"><mrow><msubsup><mi>η</mi><mi>p</mi><mn>2</mn></msubsup></mrow></math> = 0.06]. However, unlike the learning phase, we did not find a significant model x ROI interaction effect [<i>F</i>(3.3,91.2)=1.81, p=0.15, <math id="inf18"><mrow><msubsup><mi>η</mi><mi>p</mi><mn>2</mn></msubsup></mrow></math> = 0.06, GG]. Because this was a surprising finding, we wanted to better understand what had changed from the learning phase to the final test. Thus, although the ROI x model interaction was not significant in the final test, we computed follow-up tests on regions that had significantly tracked prototype and exemplar predictors during the learning phase. As in the learning phase, both the VMPFC and anterior hippocampus continued to significantly track prototype predictors during the final test with effect sizes similar to those observed during learning [VMPFC: <i>t</i>(28) = 2.83, p=0.004, <i>CI</i><sub>95</sub>[µ &gt;0.06], <i>d</i> = 0.74]; [anterior hippocampus: <i>t</i>(28) = 1.98, p=0.03, <i>CI</i><sub>95</sub>[µ &gt;0.01], <i>d</i> = 0.52]. Here, prototype correlates were significantly stronger than exemplar correlates in the anterior hippocampus [<i>t</i>(28)=2.28, p=0.02, <i>d</i> = 0.63], <i>CI</i><sub>95</sub>[µ &gt;0.02] and marginally stronger in the VMPFC [<i>t</i>(28)=1.67, p=0.053, <i>d</i> = 0.46, <i>CI</i><sub>95</sub>[µ &gt; - 0.03]]. However, exemplar correlates did not reach significance in any of the predicted exemplar regions (all t &lt; 1.18, p&gt;0.12, d &lt; 0.31).</p>







  </div>

</section>







  </div>

</section>







  </div>

</section>







  </div>

</section>



                    <section
    class="article-section "
   id="s3"
  data-behaviour="ArticleSection"
  data-initial-state="closed"
>

    <header class="article-section__header">
      <h2 class="article-section__header_text">Discussion</h2>

    </header>

  <div class="article-section__body">
      <p class="paragraph">In the present study, we tested whether exemplar- and prototype-based category representations could co-exist in the brain within a single task under conditions that favor both exemplar memory and prototype extraction. We found signatures of both types of representations across distinct brain regions when participants categorized items during the learning phase. Consistent with predictions based on prior studies, the ventromedial prefrontal cortex and anterior hippocampus tracked abstract prototype information, and the inferior frontal gyrus and lateral parietal cortex tracked specific exemplar information. In addition, we tested whether individuals relied on different types of representations over the course of learning. We did not find evidence of representational shifts either from specific to abstract or vice versa. Instead, results suggested that both types of representations emerged together during learning, although prototype correlates came to dominate by the final test. Together, we show that specific and abstract representations may instead exist in parallel for the same categories.</p>
<p class="paragraph">A great deal of prior work in the domain of category learning has focused on whether classification of novel category members relies on retrieval of individual category exemplars (<a href="#bib32">Kruschke, 1992</a>; <a href="#bib39">Medin and Schaffer, 1978</a>; <a href="#bib44">Nosofsky, 1986</a>; <a href="#bib49">Nosofsky and Stanton, 2005</a>; <a href="#bib74">Zaki et al., 2003</a>) or instead on abstract category prototypes (<a href="#bib16">Dubé, 2019</a>; <a href="#bib26">Homa, 1973</a>; <a href="#bib56">Posner and Keele, 1968</a>; <a href="#bib58">Reed, 1972</a>; <a href="#bib68">Smith and Minda, 2002</a>). These two representations are often pitted against one another with one declared the winner over the other, which is based largely on typical model-fitting procedures for behavioral data. Indeed, fitting exemplar and prototype models to behavioral data in the present study generally showed better fit of the prototype model over the exemplar model. However, using neuroimaging allowed us to detect both types of representations apparent across different parts of the brain. These results thus contribute to the ongoing debate about the nature of category representations in behavioral studies of categorization by showing that individuals may maintain multiple representations simultaneously even when one model shows better overall fit to behavior.</p>
<p class="paragraph">In addition to contributing novel findings to a longstanding debate in the behavioral literature, the present study also helps to resolve between prior neuroimaging studies fitting prototype and exemplar models to brain data. Specifically, two prior studies found conflicting results: one study found only exemplar representations in the brain (<a href="#bib36">Mack et al., 2013</a>) whereas another found only prototype representations (<a href="#bib6">Bowman and Zeithamova, 2018</a>). Notably the brain regions tracking exemplar predictions were different than those identified as tracking prototype predictions, showing that these studies engaged different brain systems in addition to implicating different categorization strategies. However, because the category structures, stimuli and analysis details also differed between these studies, the between-studies differences in the identified neural systems could not be uniquely attributed to the distinct category representations that participants presumably relied on. The present data newly show that neural prototype and exemplar correlates can exist not only across different task contexts but also within the same task, providing evidence that these neural differences reflect distinct category representations rather than different task details.</p>
<p class="paragraph">Moreover, our results aligned with those found separately across two studies, replicating the role of the VMPFC and anterior hippocampus in tracking prototype information (<a href="#bib6">Bowman and Zeithamova, 2018</a>) and replicating the role of inferior prefrontal and lateral parietal cortices in tracking exemplar information (<a href="#bib36">Mack et al., 2013</a>). Prior work has shown that the hippocampus and VMPFC support integration across related experiences in episodic inference tasks (for reviews, see <a href="#bib61">Schlichting and Preston, 2017</a>; <a href="#bib77">Zeithamova and Bowman, 2020</a>). We have now shown for the second time that these same regions also track prototype information during category generalization, suggesting that they may play a common role across seemingly distinct tasks. That is, integrating across experiences may not only link related elements as in episodic inference tasks, but may also serve to derive abstract information such as category prototypes. We also replicated a dissociation within the hippocampus from <a href="#bib6">Bowman and Zeithamova, 2018</a> in which the anterior hippocampus showed significantly stronger prototype representations than the posterior hippocampus. Our findings are consistent with a proposed gradient along the hippocampal long axis, with representations becoming increasingly coarse in spatial and temporal scale moving from posterior to anterior portions of the hippocampus (<a href="#bib10">Brunec et al., 2018</a>; <a href="#bib55">Poppenk et al., 2013</a>). Lastly, we note that while the VMPFC significantly tracked prototype predictions, there was only a marginal difference between prototype and exemplar correlates in this region. Thus, it remains an open question whether representations in VMPFC are prototype specific or instead may reflect some mix of coding.</p>
<p class="paragraph">Our finding that IFG and lateral parietal cortices tracked exemplar predictions is consistent not only with prior work showing exemplar correlates in these regions during categorization (<a href="#bib36">Mack et al., 2013</a>), but also with the larger literature on their role in maintaining memory specificity. In particular, IFG is thought to play a critical role in resolving interference between similar items (<a href="#bib4">Badre and Wagner, 2005</a>; <a href="#bib5">Bowman and Dennis, 2016</a>; <a href="#bib28">Jonides et al., 1998</a>; <a href="#bib33">Kuhl et al., 2007</a>) while lateral parietal cortices often show high fidelity representations of individual items and features necessary for task performance (<a href="#bib34">Kuhl and Chun, 2014</a>; <a href="#bib73">Xiao et al., 2017</a>). The present findings support and further this prior work by showing that regions supporting memory specificity across many memory tasks may also contribute to exemplar-based concept learning.</p>
<p class="paragraph">In addition to IFG and lateral parietal cortex, we predicted that lateral occipital cortex would track exemplar information. This prediction was based both on its previously demonstrated exemplar correlates in the Mack et al. study as well as evidence that representations in visual regions shift with category learning (<a href="#bib18">Folstein et al., 2013</a>; <a href="#bib20">Freedman et al., 2001</a>; <a href="#bib42">Myers and Swan, 2012</a>; <a href="#bib50">Palmeri and Gauthier, 2004</a>). Such shifts are posited to be the result of selective attention to visual features most relevant for categorization (<a href="#bib21">Goldstone and Steyvers, 2001</a>; <a href="#bib39">Medin and Schaffer, 1978</a>; <a href="#bib44">Nosofsky, 1986</a>). Consistent with the selective attention interpretation, Mack et al., showed that LO tracked similarity between items when feature weights estimated by the exemplar model were taken into account, above-and-beyond tracking physical similarity. In the present study, LO showed an overall similar pattern as IFG and lateral parietal cortex, but exemplar correlates did not reach significance during any phase of the experiment, providing only weak evidence for exemplar coding in this region. However, in contrast to this prior work, all stimulus features in our study were equally relevant for determining category membership. This aspect of our task may have limited the role of selective attention in the present study and thus the degree to which perceptual regions tracked category information.</p>
<p class="paragraph">In designing the present study, we aimed to increase exemplar strategy use as compared to our prior study in which the prototype model fit reliably better than the exemplar model in 73% of the sample (<a href="#bib6">Bowman and Zeithamova, 2018</a>). We included a relatively coherent category structure that was likely to promote prototype formation (<a href="#bib6">Bowman and Zeithamova, 2018</a>; <a href="#bib7">Bowman and Zeithamova, 2020</a>), but tried to balance it with an observational rather than feedback-based training task in hopes of emphasizing individual items and promoting exemplar representations. The results suggest some shift in model fits, albeit modest. The prototype strategy was still identified as dominant in the latter half of learning and the final test, but we also observed more participants who were comparably fit by both models. Moreover, we detected exemplar correlates in the brain in the present study, albeit only during the second half of the learning phase. Thus, while the behavioral shift in model fits was modest, it may have been sufficient to make exemplar representations detectable despite prototype dominance in behavior. Notably, our prior study did show some evidence of exemplar-tracking regions (including portions of LO and lateral parietal cortex) but only when we used a lenient, uncorrected threshold. This suggests that exemplar-based representations may form in the brain even though they are not immediately relevant for the task at hand.</p>
<p class="paragraph">It may be that representations form at multiple levels of specificity to promote flexibility in future decision-making because it is not always clear what aspects of current experience will become relevant (<a href="#bib77">Zeithamova and Bowman, 2020</a>). Consistent with this idea, research shows that category representations can spontaneously form alongside memory for individuals, even when task instructions emphasize distinguishing between similar individuals (<a href="#bib3">Ashby et al., 2020</a>). In the present context, accessing prototype representations may be efficient for making generalization judgments, but they cannot on their own support judgments that require discrimination between separate experiences or between members of the same category. Thus, exemplar representations may also form to support judgments requiring specificity. Precedence for co-existing representations also comes from neuroimaging studies of spatial processing (<a href="#bib10">Brunec et al., 2018</a>), episodic inference (<a href="#bib60">Schlichting et al., 2015</a>), and memory for narratives (<a href="#bib12">Collin et al., 2015</a>). These studies have all shown evidence for separate representations of individual experiences alongside representations that integrate across experiences. The present results show that these parallel representations may also be present during category learning.</p>
<p class="paragraph">While co-existing prototype and exemplar representations were clear during the learning phase of our task, they were not present during the final test phase. The VMPFC and anterior hippocampus continued to track prototypes during the final test, but exemplar-tracking regions no longer emerged. The lack of exemplar correlates in the brain was matched by a weaker exemplar effect in behavior. While we observed a reliable advantage for old relative to new items matched for distance during the interim tests, the old advantage was no longer significant in the final test. The effect size for the prototype advantage in model fits was also larger in the final test than in the learning phase. This finding was unexpected, but we offer several possibilities that can be investigated further in future research. One possibility is that exemplar representations were weakened in the absence of further observational study runs that had boosted exemplars in earlier phases. Similarly, framing it as a ‘final test’ may have switched participants from trying to gather multiple kinds of information that might improve later performance (i.e., both exemplar and prototype) to simply deploying the strongest representation that they had, which seems to have been prototype-based. Alternatively, there may be real, non-linear dynamics in how prototype and exemplar representations develop. For example, exemplar representations may increase up to some threshold while individuals are encoding these complex stimuli, then decrease as a result of repetition suppression (<a href="#bib15">Desimone, 1996</a>; <a href="#bib22">Gonsalves et al., 2005</a>; <a href="#bib24">Henson et al., 2002</a>) once individual items are sufficiently well represented. Of course, future studies will be needed to both replicate this finding and directly test these differing possibilities.</p>
<p class="paragraph">In addition to identifying multiple, co-existing types of category representations during learning, we sought to test whether there were representational shifts as category knowledge developed. While there was a prototype advantage in the brain during the final test, we found no evidence for a shift between exemplar and prototype representations over the course of learning. Both prototype and exemplar correlates showed numerical increases across learning in brain and behavior, suggesting strengthening of both types of representations in parallel. Prior work has shown that individuals may use both rules and memory for individual exemplars throughout learning without strong shifts from one to the other (<a href="#bib69">Thibaut et al., 2018</a>). Others have suggested that there may be representational shifts during category learning, but rather than shifting between exemplar and prototype representations, early learning may be focused on detecting simple rules and testing multiple hypotheses (<a href="#bib27">Johansen and Palmeri, 2002</a>; <a href="#bib47">Nosofsky et al., 1994</a>; <a href="#bib51">Paniukov and Davis, 2018</a>), whereas similarity-based representations such as prototype and exemplar representations may develop later in learning (<a href="#bib27">Johansen and Palmeri, 2002</a>). Our findings are consistent with this framework, with strong prototype and exemplar representations emerging across distinct regions primarily in the second half of learning. Our results are also consistent with recent neuroimaging studies showing multiple memory representations forming in parallel without need for competition (<a href="#bib12">Collin et al., 2015</a>; <a href="#bib60">Schlichting et al., 2015</a>), potentially allowing individuals to flexibly use prior experience based on current decision-making demands.</p>
<section
    class="article-section "
   id="s3-1"


>

    <header class="article-section__header">
      <h3 class="article-section__header_text">Conclusion</h3>

    </header>

  <div class="article-section__body">
      <p class="paragraph">In the present study, we found initial evidence that multiple types of category representations may co-exist across distinct brain regions within the same categorization task. The regions identified as prototype-tracking (anterior hippocampus and VMPFC) and exemplar-tracking (IFG and lateral parietal cortex) in the present study align with prior studies that have found only one or the other. These findings shed light on the multiple memory systems that contribute to concept representation and provide novel evidence of how the brain may flexibly represent information at different levels of specificity and that these representations may not always compete during learning.</p>







  </div>

</section>







  </div>

</section>



                    <section
    class="article-section "
   id="s4"
  data-behaviour="ArticleSection"
  data-initial-state="closed"
>

    <header class="article-section__header">
      <h2 class="article-section__header_text">Materials and methods</h2>

    </header>

  <div class="article-section__body">
      <section
    class="article-section "
   id="s4-1"


>

    <header class="article-section__header">
      <h3 class="article-section__header_text">Participants</h3>

    </header>

  <div class="article-section__body">
      <p class="paragraph">Forty volunteers were recruited from the University of Oregon and surrounding community and were financially compensated for their research participation. This sample size was determined based on effect sizes for neural prototype-tracking and exemplar-tracking regions estimated from prior studies (<a href="#bib6">Bowman and Zeithamova, 2018</a>; <a href="#bib36">Mack et al., 2013</a>), allowing for detection of the minimum effect size (prototype correlates in anterior hippocampus, d = 0.43 with n = 29) using a one-tailed, one-sample t-test with at least 80% power. All participants provided written informed consent, and Research Compliance Services at the University of Oregon approved all experimental procedures. All participants were right-handed, native English speakers and were screened for neurological conditions, medications known to affect brain function, and contraindications for MRI.</p>
<p class="paragraph">A total of 11 subjects were excluded. Six subjects were excluded prior to fMRI analyses: three subjects for chance performance (&lt;0.6 by the end of the training phase and/or &lt;0.6 for trained items in the final test), one subject for excessive motion (&gt;1.5 mm within multiple runs), and two subjects for failure to complete all phases. An additional five subjects were excluded for high correlation between fMRI regressors that precluded model-based fMRI analyses of the first or second half of learning phase: three subjects had r &gt; 0.9 for prototype and exemplar regressors and two subjects had a rank deficient design matrix driven by a lack of trial-by-trial variability in the exemplar predictor. In all five participants, attentional weight parameter estimates from both models indicated that most stimulus dimensions were ignored, which in some cases may lead to a lack of variability in model fits. This left 29 subjects (age: <i>M</i> = 21.9 years, <i>SD</i> = 3.3 years, range 18–30 years; 19 females) reported in all analyses. Additionally, we excluded single runs from three subjects who had excessive motion limited to that single run.</p>







  </div>

</section>
<section
    class="article-section "
   id="s4-2"


>

    <header class="article-section__header">
      <h3 class="article-section__header_text">Materials</h3>

    </header>

  <div class="article-section__body">
      <p class="paragraph">Stimuli consisted of cartoon animals that differed on eight binary features: neck (short vs. long), tail (straight vs. curled), foot shape (claws vs. round), snout (rounded vs. pig), head (ears vs. antennae), color (purple vs. red), body shape (angled vs. round), and design on the body (polka dots vs. stripes) (<a href="#bib8">Bozoki et al., 2006</a>; <a href="#bib75">Zeithamova et al., 2008</a>; available for download <a href="http://osf.io/8bph2">osf.io/8bph2</a>). The two possible versions of all features can be seen across the two prototypes shown in <a href="#fig1">Figure 1C</a>. For each participant, the stimulus that served as the prototype of category A was randomly selected from four possible stimuli and all other stimuli were re-coded in reference to that prototype. The stimulus that shared no features with the category A prototype served as the category B prototype. Physical distance between any pair of stimuli was defined by their number of differing features. Category A stimuli were those that shared more features with the category A prototype than the category B prototype. Category B stimuli were those that shared more features with the category B prototype than the category A prototype. Stimuli equidistant from the two prototypes were not used in the study.</p>
<section
    class="article-section "
   id="s4-2-1"


>

    <header class="article-section__header">
      <h4 class="article-section__header_text">Training set</h4>

        <a href="https://bio-protocol.org/eLIFErap59360?item=s4-2-1" class="article-section__header_link">Request a detailed protocol</a>
    </header>

  <div class="article-section__body">
      <p class="paragraph">The training set included four stimuli per category, each differing from their category prototype by two features (see <a href="#table2">Table 2</a> for training set structure). The general structure of the training set with regard to the category prototypes was the same across subjects, but the exact stimuli differed based on the prototypes selected for a given participant. The training set structure was selected to generate many pairs of training items that were four features apart both within the same category and across the two categories. This design ensured that categories could not be learned via unsupervised clustering based on similarity of exemplars alone.</p>
    <div
        id="table2"
        class="asset-viewer-inline asset-viewer-inline--table "
        data-variant="table"
        data-behaviour="AssetNavigation AssetViewer ToggleableCaption"
        data-selector=".caption-text__body"
        data-asset-viewer-group="table2"
    >

      <div class="asset-viewer-inline__header_panel">
          <div class="asset-viewer-inline__header_text">
            <span class="asset-viewer-inline__header_text__prominent">Table 2</span>
          </div>


            <div class="asset-viewer-inline__figure_access">
            </div>

      </div>

          <figure class="captioned-asset">






              <figcaption class="captioned-asset__caption">

                  <h6 class="caption-text__heading">Dimension values for example prototypes and training stimuli from each category.</h6>





              </figcaption>



              <div class="table">


                  <div class="table__table">

                    <table><thead><tr><th valign="top"/><th colspan="8" valign="top">Dimension values</th></tr></thead><tbody><tr><td valign="top">Stimulus</td><td valign="top"><span class="underline">1</span></td><td valign="top"><span class="underline">2</span></td><td valign="top"><span class="underline">3</span></td><td valign="top"><span class="underline">4</span></td><td valign="top"><span class="underline">5</span></td><td valign="top"><span class="underline">6</span></td><td valign="top"><span class="underline">7</span></td><td valign="top"><span class="underline">8</span></td></tr><tr><td valign="top">Prototype A</td><td valign="top">1</td><td valign="top">1</td><td valign="top">1</td><td valign="top">1</td><td valign="top">1</td><td valign="top">1</td><td valign="top">1</td><td valign="top">1</td></tr><tr><td valign="top">A1</td><td valign="top">1</td><td valign="top">1</td><td valign="top">1</td><td valign="top">1</td><td valign="top">1</td><td valign="top">1</td><td valign="top">0</td><td valign="top">0</td></tr><tr><td valign="top">A2</td><td valign="top">0</td><td valign="top">1</td><td valign="top">1</td><td valign="top">1</td><td valign="top">0</td><td valign="top">1</td><td valign="top">1</td><td valign="top">1</td></tr><tr><td valign="top">A3</td><td valign="top">1</td><td valign="top">0</td><td valign="top">1</td><td valign="top">0</td><td valign="top">1</td><td valign="top">1</td><td valign="top">1</td><td valign="top">1</td></tr><tr><td valign="top">A4</td><td valign="top">1</td><td valign="top">1</td><td valign="top">0</td><td valign="top">1</td><td valign="top">1</td><td valign="top">0</td><td valign="top">1</td><td valign="top">1</td></tr><tr><td valign="top">Prototype B</td><td valign="top">0</td><td valign="top">0</td><td valign="top">0</td><td valign="top">0</td><td valign="top">0</td><td valign="top">0</td><td valign="top">0</td><td valign="top">0</td></tr><tr><td valign="top">B1</td><td valign="top">0</td><td valign="top">0</td><td valign="top">0</td><td valign="top">0</td><td valign="top">0</td><td valign="top">0</td><td valign="top">1</td><td valign="top">1</td></tr><tr><td valign="top">B2</td><td valign="top">1</td><td valign="top">0</td><td valign="top">0</td><td valign="top">0</td><td valign="top">1</td><td valign="top">0</td><td valign="top">0</td><td valign="top">0</td></tr><tr><td valign="top">B3</td><td valign="top">0</td><td valign="top">1</td><td valign="top">0</td><td valign="top">1</td><td valign="top">0</td><td valign="top">0</td><td valign="top">0</td><td valign="top">0</td></tr><tr><td valign="top">B4</td><td valign="top">0</td><td valign="top">0</td><td valign="top">1</td><td valign="top">0</td><td valign="top">0</td><td valign="top">1</td><td valign="top">0</td><td valign="top">0</td></tr></tbody></table>

                  </div>



              </div>

          </figure>


    </div>







  </div>

</section>
<section
    class="article-section "
   id="s4-2-2"


>

    <header class="article-section__header">
      <h4 class="article-section__header_text">Interim test sets</h4>

        <a href="https://bio-protocol.org/eLIFErap59360?item=s4-2-2" class="article-section__header_link">Request a detailed protocol</a>
    </header>

  <div class="article-section__body">
      <p class="paragraph">Stimuli in the interim generalization tests included 22 unique stimuli: the eight training stimuli, both prototypes, and two new stimuli at each distance (1, 2, 3, 5, 6, 7) from the category A prototype. Distance 1, 2 and 3 items were scored as correct when participant labeled them as category A members. Items at the distance 5, 6 and 7 from the category A prototype (thus distance 3, 2, and one from the B prototype) were scored as correct when participant labeled them as category B members. While new unique distance 1–3, 5–7 items were selected for each interim test set, the old training stimuli and the prototypes were necessarily the same for each test.</p>







  </div>

</section>
<section
    class="article-section "
   id="s4-2-3"


>

    <header class="article-section__header">
      <h4 class="article-section__header_text">Final test set</h4>

        <a href="https://bio-protocol.org/eLIFErap59360?item=s4-2-3" class="article-section__header_link">Request a detailed protocol</a>
    </header>

  <div class="article-section__body">
      <p class="paragraph">Stimuli in the final test included 58 unique stimuli. Forty-eight of those consisted of 8 new stimuli selected at each distance 1–3, 5–7 from the category A prototype, each presented once during the final test. These new items were distinct from those used in either the training set or the interim test sets with the exception of the items that differed by only one feature from their respective prototypes. Because there are only 8 distance one items for each prototype, they were all used as part of the interim test sets before being used again in the final test set. The final test also included the eight training stimuli and the two prototypes, each presented twice in this phase (<a href="#bib6">Bowman and Zeithamova, 2018</a>; <a href="#bib29">Kéri et al., 2001</a>; <a href="#bib66">Smith et al., 2008</a>). The stimulus structure enabled dissociable behavioral predictions from the two models. While stimuli near the prototypes also tend to be near old exemplars, the correlation is imperfect. For example, when attention is equally distributed across features, the prototype model would make the same response probability prediction for all distance three items. However, some of those distance three items were near an old exemplar while others were farther from all old exemplars, creating distinct exemplar model predictions. Because we varied the test stimuli to include all distances from the prototypes, and because within each distance to the prototype there was variability in how far the stimuli are from the old exemplars, the structure was set up to facilitate dissociation between the model predictions.</p>







  </div>

</section>







  </div>

</section>
<section
    class="article-section "
   id="s4-3"


>

    <header class="article-section__header">
      <h3 class="article-section__header_text">Experimental design</h3>

        <a href="https://bio-protocol.org/eLIFErap59360?item=s4-3" class="article-section__header_link">Request a detailed protocol</a>
    </header>

  <div class="article-section__body">
      <p class="paragraph">The study consisted of two sessions: one session of neuropsychological testing and one experimental session. Only results from the experimental session are reported in the present manuscript. In the experimental session, subjects underwent four cycles of observational study and interim generalization tests (<a href="#fig1">Figure 1D</a>), followed by a final generalization test (<a href="#fig1">Figure 1E</a>), all while undergoing fMRI.</p>
<p class="paragraph">In each run of observational study, participants were shown individual animals on the screen with a species label (Febbles and Badoons) and were told to try to figure out what makes some animals Febbles and others Badoons without making any overt responses. Each stimulus was presented on the screen for 5 s followed by a 7 s ITI. Within each study run, participants viewed the training examples three times in a random order. After two study runs, participants completed an interim generalization test. Participants were shown cartoon animals without their labels and classified them into the two species without feedback. Each test stimulus was presented for 5 s during which time they could make their response, followed by a 7 s ITI. After four study-test cycles, participants completed a final categorization test, split across four runs. As in the interim tests, participants were asked to categorize animals into one of two imaginary species (Febbles and Badoons) using the same button press while the stimulus was on the screen. Following the MRI session, subjects were asked about the strategies they used to learn the categories, if any, and then indicated which version of each feature they thought was most typical for each category. Lastly, subjects were verbally debriefed about the study.</p>







  </div>

</section>
<section
    class="article-section "
   id="s4-4"


>

    <header class="article-section__header">
      <h3 class="article-section__header_text">fMRI Data Acquisition</h3>

        <a href="https://bio-protocol.org/eLIFErap59360?item=s4-4" class="article-section__header_link">Request a detailed protocol</a>
    </header>

  <div class="article-section__body">
      <p class="paragraph">Raw MRI data are available for download via OpenNeuro (<a href="http://openneuro.org/datasets/ds002813">openneuro.org/datasets/ds002813</a>). Scanning was completed on a 3T Siemens MAGNETOM Skyra scanner at the University of Oregon Lewis Center for Neuroimaging using a 32-channel head coil. Head motion was minimized using foam padding. The scanning session started with a localizer scan followed by a standard high-resolution T1-weighted MPRAGE anatomical image (TR 2500 ms; TE 3.43 ms; TI 1100 ms; flip angle 7°; matrix size 256 256; 176 contiguous slices; FOV 256 mm; slice thickness 1 mm; voxel size 1.0 1.0 1.0 mm; GRAPPA factor 2). Then, a custom anatomical T2 coronal image (TR 13,520 ms; TE 88 ms; flip angle 150°; matrix size 512 512; 65 contiguous slices oriented perpendicularly to the main axis of the hippocampus; interleaved acquisition; FOV 220 mm; voxel size 0.4 0.4 2 mm; GRAPPA factor 2) was collected. This was followed by 16 functional runs using a multiband gradient echo pulse sequence [TR 2000 ms; TE 26 ms; flip angle 90°; matrix size 100 100; 72 contiguous slices oriented 15° off the anterior commissure–posterior commissure line to reduced prefrontal signal dropout; interleaved acquisition; FOV 200 mm; voxel size 2.0 2.0 2.0 mm; generalized autocalibrating partially parallel acquisitions (GRAPPA) factor 2]. One hundred and forty-five volumes were collected for each observational study run, 133 volumes for each interim test run, and 103 volumes for each final test run.</p>







  </div>

</section>
<section
    class="article-section "
   id="s4-5"


>

    <header class="article-section__header">
      <h3 class="article-section__header_text">Behavioral accuracies</h3>

    </header>

  <div class="article-section__body">
      <section
    class="article-section "
   id="s4-5-1"


>

    <header class="article-section__header">
      <h4 class="article-section__header_text">Interim tests</h4>

        <a href="https://bio-protocol.org/eLIFErap59360?item=s4-5-1" class="article-section__header_link">Request a detailed protocol</a>
    </header>

  <div class="article-section__body">
      <p class="paragraph">To assess changes in generalization accuracy across train-test cycles, we computed a 4 (interim test run: 1–4) x 4 (distance: 0–3) repeated-measures ANOVA on accuracy for new items only. We were particularly interested in linear effects of interim test run and distance. We also tested whether there was a difference across training in accuracy for the training items themselves versus new items at the same distance from their prototypes, which can index how much participants learn about specific items above-and-beyond what would be expected based on their typicality. We thus computed a 4 (interim test run: 1–4) x 2 (item type: training, new) repeated-measures ANOVA on accuracies for distance two items.</p>







  </div>

</section>
<section
    class="article-section "
   id="s4-5-2"


>

    <header class="article-section__header">
      <h4 class="article-section__header_text">Final test</h4>

        <a href="https://bio-protocol.org/eLIFErap59360?item=s4-5-2" class="article-section__header_link">Request a detailed protocol</a>
    </header>

  <div class="article-section__body">
      <p class="paragraph">First, to assess the effect of item typicality, classification performance in the final test (collapsed across runs) was assessed by computing a one-way, repeated-measures ANOVA across new items at distances (0–3) from either prototype. Second, we assessed whether there was an old-item advantage by comparing accuracy for training items and new items of equal distance from prototypes (distance 2) using a paired-samples t-test. For all analyses (including fMRI analyses described below), a Greenhouse-Geisser correction was applied whenever the assumption of sphericity was violated as denoted by ‘GG’ in the results.</p>







  </div>

</section>







  </div>

</section>
<section
    class="article-section "
   id="s4-6"


>

    <header class="article-section__header">
      <h3 class="article-section__header_text">Prototype and exemplar model fitting</h3>

    </header>

  <div class="article-section__body">
      <p class="paragraph">As no responses were made during the study runs, prototype and exemplar models were only fit to test runs – interim and final tests. As the number of trials in each interim test was kept low to minimize exposure to non-training items during the learning phase, we concatenated across interim tests 1 and 2 and across interim tests 3 and 4 to obtain more robust model fit estimates for the first half vs. second half of the learning phase. Model fits for the final test were computed across all four runs combined. Each model was fit to trial-by-trial data in individual participants.</p>
<section
    class="article-section "
   id="s4-6-1"


>

    <header class="article-section__header">
      <h4 class="article-section__header_text">Prototype similarity</h4>

        <a href="https://bio-protocol.org/eLIFErap59360?item=s4-6-1" class="article-section__header_link">Request a detailed protocol</a>
    </header>

  <div class="article-section__body">
      <p class="paragraph">As in prior studies (<a href="#bib6">Bowman and Zeithamova, 2018</a>; <a href="#bib37">Maddox et al., 2011</a>; <a href="#bib40">Minda and Smith, 2001</a>), the similarity of each test stimulus to each prototype was computed, assuming that perceptual similarity is an exponential decay function of physical similarity (<a href="#bib63">Shepard, 1957</a>), and taking into account potential differences in attention to individual features. Formally, similarity between the test stimulus and the prototypes was computed as follows:</p>
<div class="math-block" id="equ1">

    <span class="math-block__label">(1)</span>

    <span class="math-block__math"><math><mrow><msub><mi>S</mi><mi>A</mi></msub><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>=</mo><mi>exp</mi><mrow><mo>[</mo> <mrow><mo>−</mo><mi>c</mi><munderover><mstyle displaystyle="true" mathsize="140%"><mo movablelimits="false">∑</mo></mstyle><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><msup><mrow><mo>(</mo><msub><mi>w</mi><mi>i</mi></msub><mo>|</mo><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><mo> </mo><mi>p</mi><mi>r</mi><mi>o</mi><mi>t</mi><msub><mi>o</mi><mrow><mi>A</mi><mi>i</mi></mrow></msub><msup><mo>|</mo><mi>r</mi></msup><mo>)</mo></mrow><mrow><mn>1</mn><mo>/</mo><mi>r</mi></mrow></msup></mrow> <mo>]</mo></mrow><mo>,</mo></mrow></math></span>

</div>
<p class="paragraph">where <math id="inf19"><mrow><msub><mi>S</mi><mi>A</mi></msub><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></math> is the similarity of item <i>x</i> to category A, <i>x<sub>i</sub></i> represents the value of the item <i>x</i> on the <i>i</i>th dimension of its <i>m</i> binary dimensions (<i>m</i> = 8 in this study), proto<sub>A</sub> is the prototype of category A, <i>r</i> is the distance metric (fixed at one for the city-block metric for the binary dimension stimuli). Parameters that were estimated from each participant’s pattern of behavioral responses were <i>w</i> (a vector with eight weights, one for each of the eight stimulus features and constrained to sum to 1) and <i>c</i> (sensitivity: the rate at which similarity declines with physical distance, constrained to be 0–100).</p>







  </div>

</section>
<section
    class="article-section "
   id="s4-6-2"


>

    <header class="article-section__header">
      <h4 class="article-section__header_text">Exemplar similarity</h4>

        <a href="https://bio-protocol.org/eLIFErap59360?item=s4-6-2" class="article-section__header_link">Request a detailed protocol</a>
    </header>

  <div class="article-section__body">
      <p class="paragraph">Exemplar models assume that categories are represented by their individual exemplars, and that test items are classified into the category with the highest summed similarity across category exemplars (<a href="#fig1">Figure 1A</a>). As in the prototype model, a nonlinear exponential decay function is used to transform physical similarity into subjective similarity, based on research on how perceived similarity relates to physical similarity (<a href="#bib63">Shepard, 1957</a>). Using a nonlinear function has the effect of weighting the most similar exemplars more heavily than the least similar exemplars, as the similarity value for two items that are physically one feature apart will be more than twice the similarity value of two items that are two features apart. Using the sum of similarity across all exemplars within a category provides an opportunity for multiple highly similar exemplars to be considered in making decisions, which allows the model to generate different predictions when there is a single close exemplar versus when there are multiple close exemplars. Together, this means that the most similar training exemplars drive the summed similarity metric but there is still differentiation in the predictions informed by other exemplars beyond the closest exemplar. This is canonically how an item’s similarity to each category is computed in exemplar models (<a href="#bib45">Nosofsky, 1987</a>; <a href="#bib74">Zaki et al., 2003</a>).Formally, similarity of each test stimulus to the exemplars of each category was computed as follows:</p>
<div class="math-block" id="equ2">

    <span class="math-block__label">(2)</span>

    <span class="math-block__math"><math><mrow><msub><mi>S</mi><mi>A</mi></msub><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>=</mo><mo> </mo><munder><mstyle displaystyle="true" mathsize="140%"><mo movablelimits="false">∑</mo></mstyle><mrow><mi>y</mi><mo>∈</mo><mi>A</mi></mrow></munder><mtext>exp</mtext><mrow><mo>[</mo> <mrow><mo>−</mo><mi>c</mi><munderover><mstyle displaystyle="true" mathsize="140%"><mo movablelimits="false">∑</mo></mstyle><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mo>(</mo><msub><mi>w</mi><mi>i</mi></msub><mrow><mo>|</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><msup><mrow><mrow><mrow><msub><mi>y</mi><mi>i</mi></msub></mrow><mo>|</mo></mrow></mrow><mi>r</mi></msup><msup><mo>)</mo><mrow><mn>1</mn><mo>/</mo><mi>r</mi></mrow></msup></mrow></mrow></mrow> <mo>]</mo></mrow></mrow></math></span>

</div>
<p class="paragraph">where <i>y</i> represents the individual training stimuli from category A, and the remaining notation and parameters are as in <a href="#equ1">Equation 1</a>.</p>







  </div>

</section>
<section
    class="article-section "
   id="s4-6-3"


>

    <header class="article-section__header">
      <h4 class="article-section__header_text">Parameter estimation</h4>

        <a href="https://bio-protocol.org/eLIFErap59360?item=s4-6-3" class="article-section__header_link">Request a detailed protocol</a>
    </header>

  <div class="article-section__body">
      <p class="paragraph">For both models, the probability of assigning a stimulus <i>x</i> to category A is equal to the similarity to category A divided by the summed similarity to categories A and B, formally, as follows:</p>
<div class="math-block" id="equ3">

    <span class="math-block__label">(3)</span>

    <span class="math-block__math"><math><mstyle displaystyle="true" scriptlevel="0"><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>A</mi><mrow><mo stretchy="false">|</mo></mrow><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><msub><mi>S</mi><mrow><mi>A</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><mrow><msub><mi>S</mi><mrow><mi>A</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>+</mo><msub><mi>S</mi><mrow><mi>B</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow></mstyle></math></span>

</div>
<p class="paragraph">Using these equations, the best fitting <i>w</i><sub>1-8</sub> (attention to each feature) and <i>c</i> (sensitivity) parameters were estimated from the behavioral data of each participant, separately for the first half of the learning phase, second half of the learning phase, and the final test, and separately for the prototype and exemplar models. To estimate these parameters for a given model, the trial-by-trial predictions generated by <a href="#equ3">Equation 3</a> were compared with the participant’s actual series of responses, and model parameters were tuned to minimize the difference between predicted and observed responses. An error metric (negative log likelihood of the entire string of responses) was computed for each model by summing the negative of log-transformed probabilities, and this value was minimized by adjusting <i>w</i> and <i>c</i> using standard maximum likelihood methods, implemented in MATLAB (Mathworks, Natick, MA), using the ‘fminsearch’ function.</p>







  </div>

</section>
<section
    class="article-section "
   id="s4-6-4"


>

    <header class="article-section__header">
      <h4 class="article-section__header_text">Group analyses</h4>

        <a href="https://bio-protocol.org/eLIFErap59360?item=s4-6-4" class="article-section__header_link">Request a detailed protocol</a>
    </header>

  <div class="article-section__body">
      <p class="paragraph">After optimization, we computed a 2 (model: prototype, exemplar) x 2 (learning phase half: 1<sup>st</sup>, 2<sup>nd</sup>) repeated-measures ANOVA on the model fit values (i.e., negative log likelihood) to determine which model provided a better fit to behavioral responses at the group level and if there were shifts across learning in which model fit best. We used a paired-samples t-test comparing model fits during the final test to determine whether the group as a whole was better fit by the prototype or exemplar model by the end of the experiment.</p>
<p class="paragraph">We also tested whether individual subjects were reliably better fit by one model or the other using a permutation analysis. For each subject in each phase of the experiment, we created a null distribution of model fits by shuffling the order of stimuli associated with the subject’s actual string of responses, then fitting the prototype and exemplar models to this randomized set of response – stimulus mappings. We repeated this process 10,000 times for each subject in each phase. We first confirmed that the actual prototype and exemplar model fits were reliably better than would be expected if subjects were responding randomly by comparing these real fits to the null distribution of prototype and exemplar model fits (alpha = 0.05, one-tailed). Indeed, both the prototype and exemplar models fit reliably better than chance for all subjects in all phases of the experiment. Next, we tested whether one model reliably outperformed the other model by taking the difference in model fits generated by the permutation analysis. We then compared the observed difference in model fits to the null distribution of model fit differences and determined whether the observed difference appeared with a frequency of less than 5% (alpha = 0.05, two-tailed). Using this procedure, we labeled each subject as having used a prototype strategy, exemplar strategy, or having fits that did not differ reliably from one another (‘similar’ model fits) for each phase of the experiment.</p>







  </div>

</section>
<section
    class="article-section "
   id="s4-6-5"


>

    <header class="article-section__header">
      <h4 class="article-section__header_text">fMRI Preprocessing</h4>

        <a href="https://bio-protocol.org/eLIFErap59360?item=s4-6-5" class="article-section__header_link">Request a detailed protocol</a>
    </header>

  <div class="article-section__body">
      <p class="paragraph">The raw data were converted from dicom files to nifti files using the dcm2niix function from MRIcron (<a href="https://www.nitrc.org/projects/mricron">https://www.nitrc.org/projects/mricron</a>). Functional images were skull-stripped using BET (Brain Extraction Tool), which is part of FSL (<a href="http://www.fmrib.ox.ac.uk/fsl">http://www.fmrib.ox.ac.uk/fsl</a>). Within-run motion correction was computed using MCFLIRT in FSL to realign each volume to the middle volume of the run. Across-run motion correction was then computed using ANTS (Advanced Normalization Tools) by registering the first volume of each run to the first volume of the first functional run (i.e., the first training run). Each computed transformation was then applied to all volumes in the corresponding run. Brain-extracted and motion-corrected images from each run were entered into the FEAT (fMRI Expert Analysis Tool) in FSL for high-pass temporal filtering (100 s) and spatial smoothing (4 mm FWHM kernel).</p>







  </div>

</section>







  </div>

</section>
<section
    class="article-section "
   id="s4-7"


>

    <header class="article-section__header">
      <h3 class="article-section__header_text">Regions of interest</h3>

        <a href="https://bio-protocol.org/eLIFErap59360?item=s4-7" class="article-section__header_link">Request a detailed protocol</a>
    </header>

  <div class="article-section__body">
      <p class="paragraph">Regions of interest (ROIs, <a href="#fig2">Figure 2</a>) were defined anatomically in individual participants’ native space using the cortical parcellation and subcortical segmentation from Freesurfer version 6 (<a href="https://surfer.nmr.mgh.harvard.edu/">https://surfer.nmr.mgh.harvard.edu/</a>) and collapsed across hemispheres to create bilateral masks. Past research has indicated that there may be a functional gradient along the hippocampal long axis, with detailed, find-grained representations in the posterior hippocampus and increasingly coarse, generalized representations proceeding toward the anterior hippocampus (<a href="#bib10">Brunec et al., 2018</a>; <a href="#bib19">Frank et al., 2019</a>; <a href="#bib55">Poppenk et al., 2013</a>). As such, we divided the hippocampal ROI into anterior/posterior portions at the middle slice. When a participant had an odd number of hippocampal slices, the middle slice was assigned to the posterior hippocampus. Based on our prior report (<a href="#bib6">Bowman and Zeithamova, 2018</a>), we expected the anterior portion of the hippocampus to track prototype predictors, together with VMPFC (medial orbitofrontal label in Freesurfer). Based on the prior study by <a href="#bib36">Mack et al., 2013</a>, we expected lateral occipital cortex, inferior frontal gyrus (combination of pars opercularis, pars orbitalis, and pars triangularis freesurfer labels), and lateral parietal cortex (combination of inferior parietal and superior parietal freesurfer labels) to track exemplar predictors. The posterior hippocampus was also included as an ROI, to test for an anterior/posterior dissociation within the hippocampus. While one might expect the posterior hippocampus to track exemplar predictors based on the aforementioned functional gradient, our prior report <a href="#bib6">Bowman and Zeithamova, 2018</a> found only a numeric trend in this direction and <a href="#bib36">Mack et al., 2013</a> did not report any hippocampal findings despite significant exemplar correlates found in the cortex. Thus, we did not have strong predictions regarding the posterior hippocampus, other than being distinct from the anterior hippocampus.</p>







  </div>

</section>
<section
    class="article-section "
   id="s4-8"


>

    <header class="article-section__header">
      <h3 class="article-section__header_text">Model-based fMRI analyses</h3>

        <a href="https://bio-protocol.org/eLIFErap59360?item=s4-8" class="article-section__header_link">Request a detailed protocol</a>
    </header>

  <div class="article-section__body">
      <p class="paragraph">fMRI data were modeled using the GLM. Three task-based regressors were included in the GLM: one for all trial onsets, one that included modulation for each trial by prototype model predictions, and one that included modulation for each trial by exemplar model predictions. Events were modeled with a duration of 5 s, which was the fixed length of the stimulus presentation. Onsets were then convolved with the canonical hemodynamic response function as implemented in in FSL (a gamma function with a phase of 0 s, and SD of 3 s, and a mean lag time of 6 s). The six standard timepoint-by-timepoint motion parameters were included as regressors of no interest.</p>
<p class="paragraph">The regressor for all trial onsets was included to account for activation that is associated with performing a categorization task generally, but does not track either model specifically. The modulation values for each model were computed as the summed similarity across category A and category B (denominator of <a href="#equ3">Equation 3</a>) generated by the assumptions of each model (from <a href="#equ1">Equations 1 and 2</a>). This summed similarity metric indexes how similar the current item is to the existing category representations as a whole (regardless of which category it is closer to) and has been used by prior studies to identify regions that contain such category representations (<a href="#bib6">Bowman and Zeithamova, 2018</a>; <a href="#bib14">Davis and Poldrack, 2014</a>; <a href="#bib36">Mack et al., 2013</a>). Correlations between prototype and exemplar summed similarity values ranged from r = −0.73 to. 82 for included subjects, with a mean of absolute values of r = 0.32. A vast majority (80%) of included runs had prototype and exemplar correlations between +/-. 5. To account for any shared variance between the regressors, we included both model predictors in the same GLM. We verified that the pattern of results remained the same when analyses are limited to participants with absolute correlations r &lt; 0.5 in all runs, with most correlations being quite small.</p>
<p class="paragraph">For region of interest analyses, we obtained an estimate of how much the BOLD signal in each region tracked each model predictor by dividing the mean ROI parameter estimate by the standard deviation of parameter estimates (i.e., computing an effect size measure). Normalizing the beta values by their error of the estimate de-weighs values associated with large uncertainty, similar to how lower level estimates are used in group analyses as implemented in FSL (<a href="#bib65">Smith et al., 2004</a>). These normalized beta values were then averaged across the appropriate runs (interim tests 1–2, interim tests 3–4, all four runs of the final test) and submitted to group analyses.</p>
<p class="paragraph">We tested whether prototype and exemplar correlates emerged across different regions and/or at different points during the learning phase. To do so, we computed a 2 (model: prototype, exemplar) x 2 (learning phase: 1<sup>st</sup> half, 2<sup>nd</sup> half) x 6 (ROI: VMPFC, anterior hippocampus, posterior hippocampus, lateral occipital, lateral prefrontal, and lateral parietal cortices) repeated-measures ANOVA on parameter estimates from the interim test runs. We were interested in a potential model x ROI interaction effect, indicating differences across brain regions in the type of category information represented. Following any significant interaction effect, we computed one-sample t-tests to determine whether each region significantly tracked a given model and paired-samples t-tests to determine whether the region tracked one model reliably better than the other. Given a priori expectations about the nature of these effects, we computed one-tailed tests only on the effects of interest: for example, in hypothesized prototype-tracking ROIs (anterior hippocampus and VMPFC), we computed one-sample t-tests to compare prototype effects to zero and a paired-samples t-test to test whether the prototype correlates were stronger than exemplar correlates. We followed a similar procedure in hypothesized exemplar-tracking ROIs (inferior frontal gyrus, lateral parietal cortex, lateral occipital cortex). We were also interested in potential interactions with the learning phase, which would indicate shift across learning in category representations. Following any such interaction, follow-up ANOVAs or t-tests were performed to better understand drivers of the effect.</p>
<p class="paragraph">We next tested ROI differences in the final generalization phase. To do so, we computed a 2 (model: prototype, exemplar) x 6 (ROI: see above) repeated-measures ANOVA on parameter estimates from the final generalization test. We were particularly interested in the model x ROI interaction effect, which would indicate that regions differ in which model they tracked. Because each participant’s neural model fit is inherently dependent on their behavioral model fit, we focused on group-average analyses and did not perform any brain-behavior individual differences analyses.</p>







  </div>

</section>







  </div>

</section>



                    <button class="speech-bubble speech-bubble--has-placeholder speech-bubble--wrapped "
        data-behaviour="SpeechBubble HypothesisOpener"
aria-live="polite">
  <span class="speech-bubble--wrapped__prefix">Add a comment</span>
  <span class="speech-bubble__inner"><span aria-hidden="true"><span data-visible-annotation-count>+</span></span><span class="visuallyhidden"> Open annotations. The current annotation count on this page is <span data-hypothesis-annotation-count>being calculated</span>.</span></span>
</button>



                    <section
    class="article-section "
   id="data"
  data-behaviour="ArticleSection"

>

    <header class="article-section__header">
      <h2 class="article-section__header_text">Data availability</h2>

    </header>

  <div class="article-section__body">
      <p class="paragraph">Raw MRI data have been deposited at openneuro.org/datasets/ds002813. Source data have been provided for Figures 3-5.</p>
<div class="message-bar">
  The following data sets were generated
</div>
<ol class="reference-list">
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="dataset1" id="dataset1">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Bowman+CR%22" class="reference__authors_link">Bowman CR</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Iwashita+T%22" class="reference__authors_link">Iwashita T</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Zeithamova+D%22" class="reference__authors_link">Zeithamova D</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(2020)</span>



            <a href="https://openneuro.org/datasets/ds002813" class="reference__title">OpenNeuro</a>



          <div class="reference__origin">ID ds002813. Model-based fMRI reveals co-existing specific and generalized concept representations.</div>

        <div class="doi__reference-spacing"></div>



            <div class="reference__link-outer">
              <a href="https://openneuro.org/datasets/ds002813" class="reference__link">https://openneuro.org/datasets/ds002813</a>
            </div>



      </div>
    </li>
</ol>







  </div>

</section>



                    <section
    class="article-section "
   id="references"
  data-behaviour="ArticleSection"
  data-initial-state="closed"
>

    <header class="article-section__header">
      <h2 class="article-section__header_text">References</h2>

    </header>

  <div class="article-section__body">
      <ol class="reference-list">
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib1" id="bib1">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Aizenstein+HJ%22" class="reference__authors_link">Aizenstein HJ</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:MacDonald+AW%22" class="reference__authors_link">MacDonald AW</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Stenger+VA%22" class="reference__authors_link">Stenger VA</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Nebes+RD%22" class="reference__authors_link">Nebes RD</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Larson+JK%22" class="reference__authors_link">Larson JK</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Ursu+S%22" class="reference__authors_link">Ursu S</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Carter+CS%22" class="reference__authors_link">Carter CS</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(2000)</span>


            <a href="https://doi.org/10.1162/08989290051137512" class="reference__title">Complementary category learning systems identified using event-related functional MRI</a>




          <div class="reference__origin"><i>Journal of Cognitive Neuroscience</i> <b>12</b>:977–987.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1162/08989290051137512" class="doi__link">
            https://doi.org/10.1162/08989290051137512
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://www.ncbi.nlm.nih.gov/pubmed/11177418" class="reference__abstract_link">PubMed</a>
          </li>
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Complementary+category+learning+systems+identified+using+event-related+functional+MRI&amp;author=Aizenstein+HJ&amp;author=MacDonald+AW&amp;author%5B2%5D=Stenger+VA&amp;author%5B3%5D=Nebes+RD&amp;author%5B4%5D=Larson+JK&amp;author%5B5%5D=Ursu+S&amp;author%5B6%5D=Carter+CS&amp;publication_year=2000&amp;journal=Journal+of+Cognitive+Neuroscience&amp;volume=12&amp;pages=pp.+977%E2%80%93987&amp;pmid=11177418" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib2" id="bib2">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Ashby+FG%22" class="reference__authors_link">Ashby FG</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Alfonso-Reese+LA%22" class="reference__authors_link">Alfonso-Reese LA</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Turken+AU%22" class="reference__authors_link">Turken AU</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Waldron+EM%22" class="reference__authors_link">Waldron EM</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(1998)</span>


            <a href="https://doi.org/10.1037/0033-295X.105.3.442" class="reference__title">A neuropsychological theory of multiple systems in category learning</a>




          <div class="reference__origin"><i>Psychological Review</i> <b>105</b>:442–481.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1037/0033-295X.105.3.442" class="doi__link">
            https://doi.org/10.1037/0033-295X.105.3.442
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://www.ncbi.nlm.nih.gov/pubmed/9697427" class="reference__abstract_link">PubMed</a>
          </li>
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=A+neuropsychological+theory+of+multiple+systems+in+category+learning&amp;author=Ashby+FG&amp;author=Alfonso-Reese+LA&amp;author%5B2%5D=Turken+AU&amp;author%5B3%5D=Waldron+EM&amp;publication_year=1998&amp;journal=Psychological+Review&amp;volume=105&amp;pages=pp.+442%E2%80%93481&amp;pmid=9697427" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib3" id="bib3">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Ashby+SR%22" class="reference__authors_link">Ashby SR</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Bowman+CR%22" class="reference__authors_link">Bowman CR</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Zeithamova+D%22" class="reference__authors_link">Zeithamova D</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(2020)</span>


            <a href="https://doi.org/10.3758/s13423-020-01754-3" class="reference__title">Perceived similarity ratings predict generalization success after traditional category learning and a new paired-associate learning task</a>




          <div class="reference__origin"><i>Psychonomic Bulletin & Review</i> <b>27</b>:791–800.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.3758/s13423-020-01754-3" class="doi__link">
            https://doi.org/10.3758/s13423-020-01754-3
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://www.ncbi.nlm.nih.gov/pubmed/32472329" class="reference__abstract_link">PubMed</a>
          </li>
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Perceived+similarity+ratings+predict+generalization+success+after+traditional+category+learning+and+a+new+paired-associate+learning+task&amp;author=Ashby+SR&amp;author=Bowman+CR&amp;author%5B2%5D=Zeithamova+D&amp;publication_year=2020&amp;journal=Psychonomic+Bulletin+%26+Review&amp;volume=27&amp;pages=pp.+791%E2%80%93800&amp;pmid=32472329" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib4" id="bib4">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Badre+D%22" class="reference__authors_link">Badre D</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Wagner+AD%22" class="reference__authors_link">Wagner AD</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(2005)</span>


            <a href="https://doi.org/10.1093/cercor/bhi075" class="reference__title">Frontal lobe mechanisms that resolve proactive interference</a>




          <div class="reference__origin"><i>Cerebral Cortex</i> <b>15</b>:2003–2012.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1093/cercor/bhi075" class="doi__link">
            https://doi.org/10.1093/cercor/bhi075
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://www.ncbi.nlm.nih.gov/pubmed/15788702" class="reference__abstract_link">PubMed</a>
          </li>
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Frontal+lobe+mechanisms+that+resolve+proactive+interference&amp;author=Badre+D&amp;author=Wagner+AD&amp;publication_year=2005&amp;journal=Cerebral+Cortex&amp;volume=15&amp;pages=pp.+2003%E2%80%932012&amp;pmid=15788702" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib5" id="bib5">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Bowman+CR%22" class="reference__authors_link">Bowman CR</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Dennis+NA%22" class="reference__authors_link">Dennis NA</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(2016)</span>


            <a href="https://doi.org/10.1162/jocn_a_00961" class="reference__title">The neural basis of recollection rejection: increases in Hippocampal–Prefrontal Connectivity in the Absence of a Shared Recall-to-Reject and Target Recollection Network</a>




          <div class="reference__origin"><i>Journal of Cognitive Neuroscience</i> <b>28</b>:1194–1209.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1162/jocn_a_00961" class="doi__link">
            https://doi.org/10.1162/jocn_a_00961
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=The+neural+basis+of+recollection+rejection%3A+increases+in+Hippocampal%E2%80%93Prefrontal+Connectivity+in+the+Absence+of+a+Shared+Recall-to-Reject+and+Target+Recollection+Network&amp;author=Bowman+CR&amp;author=Dennis+NA&amp;publication_year=2016&amp;journal=Journal+of+Cognitive+Neuroscience&amp;volume=28&amp;pages=pp.+1194%E2%80%931209" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib6" id="bib6">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Bowman+CR%22" class="reference__authors_link">Bowman CR</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Zeithamova+D%22" class="reference__authors_link">Zeithamova D</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(2018)</span>


            <a href="https://doi.org/10.1523/JNEUROSCI.2811-17.2018" class="reference__title">Abstract memory representations in the ventromedial prefrontal cortex and Hippocampus support concept generalization</a>




          <div class="reference__origin"><i>The Journal of Neuroscience</i> <b>38</b>:2605–2614.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1523/JNEUROSCI.2811-17.2018" class="doi__link">
            https://doi.org/10.1523/JNEUROSCI.2811-17.2018
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://www.ncbi.nlm.nih.gov/pubmed/29437891" class="reference__abstract_link">PubMed</a>
          </li>
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Abstract+memory+representations+in+the+ventromedial+prefrontal+cortex+and+Hippocampus+support+concept+generalization&amp;author=Bowman+CR&amp;author=Zeithamova+D&amp;publication_year=2018&amp;journal=The+Journal+of+Neuroscience&amp;volume=38&amp;pages=pp.+2605%E2%80%932614&amp;pmid=29437891" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib7" id="bib7">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Bowman+CR%22" class="reference__authors_link">Bowman CR</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Zeithamova+D%22" class="reference__authors_link">Zeithamova D</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(2020)</span>


            <a href="https://doi.org/10.1037/xlm0000824" class="reference__title">Training set coherence and set size effects on concept generalization and recognition</a>




          <div class="reference__origin"><i>Journal of Experimental Psychology: Learning, Memory, and Cognition</i> <b>46</b>:1442–1464.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1037/xlm0000824" class="doi__link">
            https://doi.org/10.1037/xlm0000824
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Training+set+coherence+and+set+size+effects+on+concept+generalization+and+recognition&amp;author=Bowman+CR&amp;author=Zeithamova+D&amp;publication_year=2020&amp;journal=Journal+of+Experimental+Psychology%3A+Learning%2C+Memory%2C+and+Cognition&amp;volume=46&amp;pages=pp.+1442%E2%80%931464" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib8" id="bib8">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Bozoki+A%22" class="reference__authors_link">Bozoki A</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Grossman+M%22" class="reference__authors_link">Grossman M</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Smith+EE%22" class="reference__authors_link">Smith EE</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(2006)</span>


            <a href="https://doi.org/10.1016/j.neuropsychologia.2005.08.001" class="reference__title">Can patients with Alzheimer's disease learn a category implicitly?</a>




          <div class="reference__origin"><i>Neuropsychologia</i> <b>44</b>:816–827.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1016/j.neuropsychologia.2005.08.001" class="doi__link">
            https://doi.org/10.1016/j.neuropsychologia.2005.08.001
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://www.ncbi.nlm.nih.gov/pubmed/16229868" class="reference__abstract_link">PubMed</a>
          </li>
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Can+patients+with+Alzheimer%27s+disease+learn+a+category+implicitly%3F&amp;author=Bozoki+A&amp;author=Grossman+M&amp;author%5B2%5D=Smith+EE&amp;publication_year=2006&amp;journal=Neuropsychologia&amp;volume=44&amp;pages=pp.+816%E2%80%93827&amp;pmid=16229868" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib9" id="bib9">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Bransford+JD%22" class="reference__authors_link">Bransford JD</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Johnson+MK%22" class="reference__authors_link">Johnson MK</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(1972)</span>


            <a href="https://doi.org/10.1016/S0022-5371(72)80006-9" class="reference__title">Contextual prerequisites for understanding: some investigations of comprehension and recall</a>




          <div class="reference__origin"><i>Journal of Verbal Learning and Verbal Behavior</i> <b>11</b>:717–726.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1016/S0022-5371(72)80006-9" class="doi__link">
            https://doi.org/10.1016/S0022-5371(72)80006-9
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Contextual+prerequisites+for+understanding%3A+some+investigations+of+comprehension+and+recall&amp;author=Bransford+JD&amp;author=Johnson+MK&amp;publication_year=1972&amp;journal=Journal+of+Verbal+Learning+and+Verbal+Behavior&amp;volume=11&amp;pages=pp.+717%E2%80%93726" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib10" id="bib10">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Brunec+IK%22" class="reference__authors_link">Brunec IK</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Bellana+B%22" class="reference__authors_link">Bellana B</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Ozubko+JD%22" class="reference__authors_link">Ozubko JD</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Man+V%22" class="reference__authors_link">Man V</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Robin+J%22" class="reference__authors_link">Robin J</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Liu+ZX%22" class="reference__authors_link">Liu ZX</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Grady+C%22" class="reference__authors_link">Grady C</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Rosenbaum+RS%22" class="reference__authors_link">Rosenbaum RS</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Winocur+G%22" class="reference__authors_link">Winocur G</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Barense+MD%22" class="reference__authors_link">Barense MD</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Moscovitch+M%22" class="reference__authors_link">Moscovitch M</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(2018)</span>


            <a href="https://doi.org/10.1016/j.cub.2018.05.016" class="reference__title">Multiple scales of representation along the hippocampal anteroposterior Axis in humans</a>




          <div class="reference__origin"><i>Current Biology</i> <b>28</b>:2129–2135.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1016/j.cub.2018.05.016" class="doi__link">
            https://doi.org/10.1016/j.cub.2018.05.016
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://www.ncbi.nlm.nih.gov/pubmed/29937352" class="reference__abstract_link">PubMed</a>
          </li>
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Multiple+scales+of+representation+along+the+hippocampal+anteroposterior+Axis+in+humans&amp;author=Brunec+IK&amp;author=Bellana+B&amp;author%5B2%5D=Ozubko+JD&amp;author%5B3%5D=Man+V&amp;author%5B4%5D=Robin+J&amp;author%5B5%5D=Liu+ZX&amp;author%5B6%5D=Grady+C&amp;author%5B7%5D=Rosenbaum+RS&amp;author%5B8%5D=Winocur+G&amp;author%5B9%5D=Barense+MD&amp;author%5B10%5D=Moscovitch+M&amp;publication_year=2018&amp;journal=Current+Biology&amp;volume=28&amp;pages=pp.+2129%E2%80%932135&amp;pmid=29937352" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib11" id="bib11">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Cincotta+CM%22" class="reference__authors_link">Cincotta CM</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Seger+CA%22" class="reference__authors_link">Seger CA</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(2007)</span>


            <a href="https://doi.org/10.1162/jocn.2007.19.2.249" class="reference__title">Dissociation between striatal regions while learning to categorize via feedback and via observation</a>




          <div class="reference__origin"><i>Journal of Cognitive Neuroscience</i> <b>19</b>:249–265.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1162/jocn.2007.19.2.249" class="doi__link">
            https://doi.org/10.1162/jocn.2007.19.2.249
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://www.ncbi.nlm.nih.gov/pubmed/17280514" class="reference__abstract_link">PubMed</a>
          </li>
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Dissociation+between+striatal+regions+while+learning+to+categorize+via+feedback+and+via+observation&amp;author=Cincotta+CM&amp;author=Seger+CA&amp;publication_year=2007&amp;journal=Journal+of+Cognitive+Neuroscience&amp;volume=19&amp;pages=pp.+249%E2%80%93265&amp;pmid=17280514" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib12" id="bib12">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Collin+SH%22" class="reference__authors_link">Collin SH</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Milivojevic+B%22" class="reference__authors_link">Milivojevic B</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Doeller+CF%22" class="reference__authors_link">Doeller CF</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(2015)</span>


            <a href="https://doi.org/10.1038/nn.4138" class="reference__title">Memory hierarchies map onto the hippocampal long Axis in humans</a>




          <div class="reference__origin"><i>Nature Neuroscience</i> <b>18</b>:1562–1564.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1038/nn.4138" class="doi__link">
            https://doi.org/10.1038/nn.4138
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://www.ncbi.nlm.nih.gov/pubmed/26479587" class="reference__abstract_link">PubMed</a>
          </li>
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Memory+hierarchies+map+onto+the+hippocampal+long+Axis+in+humans&amp;author=Collin+SH&amp;author=Milivojevic+B&amp;author%5B2%5D=Doeller+CF&amp;publication_year=2015&amp;journal=Nature+Neuroscience&amp;volume=18&amp;pages=pp.+1562%E2%80%931564&amp;pmid=26479587" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib13" id="bib13">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Davis+T%22" class="reference__authors_link">Davis T</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Love+BC%22" class="reference__authors_link">Love BC</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Preston+AR%22" class="reference__authors_link">Preston AR</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(2012)</span>


            <a href="https://doi.org/10.1093/cercor/bhr036" class="reference__title">Learning the exception to the rule: model-based fMRI reveals specialized representations for surprising category members</a>




          <div class="reference__origin"><i>Cerebral Cortex</i> <b>22</b>:260–273.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1093/cercor/bhr036" class="doi__link">
            https://doi.org/10.1093/cercor/bhr036
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Learning+the+exception+to+the+rule%3A+model-based+fMRI+reveals+specialized+representations+for+surprising+category+members&amp;author=Davis+T&amp;author=Love+BC&amp;author%5B2%5D=Preston+AR&amp;publication_year=2012&amp;journal=Cerebral+Cortex&amp;volume=22&amp;pages=pp.+260%E2%80%93273" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib14" id="bib14">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Davis+T%22" class="reference__authors_link">Davis T</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Poldrack+RA%22" class="reference__authors_link">Poldrack RA</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(2014)</span>


            <a href="https://doi.org/10.1093/cercor/bht014" class="reference__title">Quantifying the internal structure of categories using a neural typicality measure</a>




          <div class="reference__origin"><i>Cerebral Cortex</i> <b>24</b>:1720–1737.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1093/cercor/bht014" class="doi__link">
            https://doi.org/10.1093/cercor/bht014
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://www.ncbi.nlm.nih.gov/pubmed/23442348" class="reference__abstract_link">PubMed</a>
          </li>
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Quantifying+the+internal+structure+of+categories+using+a+neural+typicality+measure&amp;author=Davis+T&amp;author=Poldrack+RA&amp;publication_year=2014&amp;journal=Cerebral+Cortex&amp;volume=24&amp;pages=pp.+1720%E2%80%931737&amp;pmid=23442348" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib15" id="bib15">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Desimone+R%22" class="reference__authors_link">Desimone R</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(1996)</span>


            <a href="https://doi.org/10.1073/pnas.93.24.13494" class="reference__title">Neural mechanisms for visual memory and their role in attention</a>




          <div class="reference__origin"><i>PNAS</i> <b>93</b>:13494–13499.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1073/pnas.93.24.13494" class="doi__link">
            https://doi.org/10.1073/pnas.93.24.13494
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://www.ncbi.nlm.nih.gov/pubmed/8942962" class="reference__abstract_link">PubMed</a>
          </li>
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Neural+mechanisms+for+visual+memory+and+their+role+in+attention&amp;author=Desimone+R&amp;publication_year=1996&amp;journal=PNAS&amp;volume=93&amp;pages=pp.+13494%E2%80%9313499&amp;pmid=8942962" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib16" id="bib16">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Dub%C3%A9+C%22" class="reference__authors_link">Dubé C</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(2019)</span>


            <a href="https://doi.org/10.3758/s13421-019-00900-0" class="reference__title">Central tendency representation and exemplar matching in visual short-term memory</a>




          <div class="reference__origin"><i>Memory & Cognition</i> <b>47</b>:589–602.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.3758/s13421-019-00900-0" class="doi__link">
            https://doi.org/10.3758/s13421-019-00900-0
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://www.ncbi.nlm.nih.gov/pubmed/30830554" class="reference__abstract_link">PubMed</a>
          </li>
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Central+tendency+representation+and+exemplar+matching+in+visual+short-term+memory&amp;author=Dub%C3%A9+C&amp;publication_year=2019&amp;journal=Memory+%26+Cognition&amp;volume=47&amp;pages=pp.+589%E2%80%93602&amp;pmid=30830554" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib17" id="bib17">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Ell+SW%22" class="reference__authors_link">Ell SW</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Weinstein+A%22" class="reference__authors_link">Weinstein A</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Ivry+RB%22" class="reference__authors_link">Ivry RB</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(2010)</span>


            <a href="https://doi.org/10.1016/j.neuropsychologia.2010.06.006" class="reference__title">Rule-based categorization deficits in focal basal ganglia lesion and Parkinson's disease patients</a>




          <div class="reference__origin"><i>Neuropsychologia</i> <b>48</b>:2974–2986.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1016/j.neuropsychologia.2010.06.006" class="doi__link">
            https://doi.org/10.1016/j.neuropsychologia.2010.06.006
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://www.ncbi.nlm.nih.gov/pubmed/20600196" class="reference__abstract_link">PubMed</a>
          </li>
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Rule-based+categorization+deficits+in+focal+basal+ganglia+lesion+and+Parkinson%27s+disease+patients&amp;author=Ell+SW&amp;author=Weinstein+A&amp;author%5B2%5D=Ivry+RB&amp;publication_year=2010&amp;journal=Neuropsychologia&amp;volume=48&amp;pages=pp.+2974%E2%80%932986&amp;pmid=20600196" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib18" id="bib18">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Folstein+JR%22" class="reference__authors_link">Folstein JR</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Palmeri+TJ%22" class="reference__authors_link">Palmeri TJ</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Gauthier+I%22" class="reference__authors_link">Gauthier I</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(2013)</span>


            <a href="https://doi.org/10.1093/cercor/bhs067" class="reference__title">Category learning increases discriminability of relevant object dimensions in visual cortex</a>




          <div class="reference__origin"><i>Cerebral Cortex</i> <b>23</b>:814–823.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1093/cercor/bhs067" class="doi__link">
            https://doi.org/10.1093/cercor/bhs067
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://www.ncbi.nlm.nih.gov/pubmed/22490547" class="reference__abstract_link">PubMed</a>
          </li>
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Category+learning+increases+discriminability+of+relevant+object+dimensions+in+visual+cortex&amp;author=Folstein+JR&amp;author=Palmeri+TJ&amp;author%5B2%5D=Gauthier+I&amp;publication_year=2013&amp;journal=Cerebral+Cortex&amp;volume=23&amp;pages=pp.+814%E2%80%93823&amp;pmid=22490547" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib19" id="bib19">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Frank+LE%22" class="reference__authors_link">Frank LE</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Bowman+CR%22" class="reference__authors_link">Bowman CR</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Zeithamova+D%22" class="reference__authors_link">Zeithamova D</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(2019)</span>


            <a href="https://doi.org/10.1162/jocn_a_01457" class="reference__title">Differential functional connectivity along the long Axis of the Hippocampus aligns with differential role in memory specificity and generalization</a>




          <div class="reference__origin"><i>Journal of Cognitive Neuroscience</i> <b>31</b>:1958–1975.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1162/jocn_a_01457" class="doi__link">
            https://doi.org/10.1162/jocn_a_01457
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://www.ncbi.nlm.nih.gov/pubmed/31397613" class="reference__abstract_link">PubMed</a>
          </li>
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Differential+functional+connectivity+along+the+long+Axis+of+the+Hippocampus+aligns+with+differential+role+in+memory+specificity+and+generalization&amp;author=Frank+LE&amp;author=Bowman+CR&amp;author%5B2%5D=Zeithamova+D&amp;publication_year=2019&amp;journal=Journal+of+Cognitive+Neuroscience&amp;volume=31&amp;pages=pp.+1958%E2%80%931975&amp;pmid=31397613" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib20" id="bib20">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Freedman+DJ%22" class="reference__authors_link">Freedman DJ</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Riesenhuber+M%22" class="reference__authors_link">Riesenhuber M</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Poggio+T%22" class="reference__authors_link">Poggio T</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Miller+EK%22" class="reference__authors_link">Miller EK</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(2001)</span>


            <a href="https://doi.org/10.1126/science.291.5502.312" class="reference__title">Categorical representation of visual stimuli in the primate prefrontal cortex</a>




          <div class="reference__origin"><i>Science</i> <b>291</b>:312–316.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1126/science.291.5502.312" class="doi__link">
            https://doi.org/10.1126/science.291.5502.312
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://www.ncbi.nlm.nih.gov/pubmed/11209083" class="reference__abstract_link">PubMed</a>
          </li>
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Categorical+representation+of+visual+stimuli+in+the+primate+prefrontal+cortex&amp;author=Freedman+DJ&amp;author=Riesenhuber+M&amp;author%5B2%5D=Poggio+T&amp;author%5B3%5D=Miller+EK&amp;publication_year=2001&amp;journal=Science&amp;volume=291&amp;pages=pp.+312%E2%80%93316&amp;pmid=11209083" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib21" id="bib21">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Goldstone+RL%22" class="reference__authors_link">Goldstone RL</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Steyvers+M%22" class="reference__authors_link">Steyvers M</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(2001)</span>


            <a href="https://doi.org/10.1037/0096-3445.130.1.116" class="reference__title">The sensitization and differentiation of dimensions during category learning</a>




          <div class="reference__origin"><i>Journal of Experimental Psychology: General</i> <b>130</b>:116–139.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1037/0096-3445.130.1.116" class="doi__link">
            https://doi.org/10.1037/0096-3445.130.1.116
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=The+sensitization+and+differentiation+of+dimensions+during+category+learning&amp;author=Goldstone+RL&amp;author=Steyvers+M&amp;publication_year=2001&amp;journal=Journal+of+Experimental+Psychology%3A+General&amp;volume=130&amp;pages=pp.+116%E2%80%93139" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib22" id="bib22">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Gonsalves+BD%22" class="reference__authors_link">Gonsalves BD</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Kahn+I%22" class="reference__authors_link">Kahn I</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Curran+T%22" class="reference__authors_link">Curran T</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Norman+KA%22" class="reference__authors_link">Norman KA</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Wagner+AD%22" class="reference__authors_link">Wagner AD</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(2005)</span>


            <a href="https://doi.org/10.1016/j.neuron.2005.07.013" class="reference__title">Memory strength and repetition suppression: multimodal imaging of medial temporal cortical contributions to recognition</a>




          <div class="reference__origin"><i>Neuron</i> <b>47</b>:751–761.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1016/j.neuron.2005.07.013" class="doi__link">
            https://doi.org/10.1016/j.neuron.2005.07.013
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://www.ncbi.nlm.nih.gov/pubmed/16129403" class="reference__abstract_link">PubMed</a>
          </li>
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Memory+strength+and+repetition+suppression%3A+multimodal+imaging+of+medial+temporal+cortical+contributions+to+recognition&amp;author=Gonsalves+BD&amp;author=Kahn+I&amp;author%5B2%5D=Curran+T&amp;author%5B3%5D=Norman+KA&amp;author%5B4%5D=Wagner+AD&amp;publication_year=2005&amp;journal=Neuron&amp;volume=47&amp;pages=pp.+751%E2%80%93761&amp;pmid=16129403" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib23" id="bib23">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Heindel+WC%22" class="reference__authors_link">Heindel WC</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Festa+EK%22" class="reference__authors_link">Festa EK</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Ott+BR%22" class="reference__authors_link">Ott BR</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Landy+KM%22" class="reference__authors_link">Landy KM</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Salmon+DP%22" class="reference__authors_link">Salmon DP</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(2013)</span>


            <a href="https://doi.org/10.1016/j.neuropsychologia.2013.06.001" class="reference__title">Prototype learning and dissociable categorization systems in Alzheimer's disease</a>




          <div class="reference__origin"><i>Neuropsychologia</i> <b>51</b>:1699–1708.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1016/j.neuropsychologia.2013.06.001" class="doi__link">
            https://doi.org/10.1016/j.neuropsychologia.2013.06.001
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://www.ncbi.nlm.nih.gov/pubmed/23751172" class="reference__abstract_link">PubMed</a>
          </li>
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Prototype+learning+and+dissociable+categorization+systems+in+Alzheimer%27s+disease&amp;author=Heindel+WC&amp;author=Festa+EK&amp;author%5B2%5D=Ott+BR&amp;author%5B3%5D=Landy+KM&amp;author%5B4%5D=Salmon+DP&amp;publication_year=2013&amp;journal=Neuropsychologia&amp;volume=51&amp;pages=pp.+1699%E2%80%931708&amp;pmid=23751172" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib24" id="bib24">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Henson+RN%22" class="reference__authors_link">Henson RN</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Shallice+T%22" class="reference__authors_link">Shallice T</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Gorno-Tempini+ML%22" class="reference__authors_link">Gorno-Tempini ML</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Dolan+RJ%22" class="reference__authors_link">Dolan RJ</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(2002)</span>


            <a href="https://doi.org/10.1093/cercor/12.2.178" class="reference__title">Face repetition effects in implicit and explicit memory tests as measured by fMRI</a>




          <div class="reference__origin"><i>Cerebral Cortex</i> <b>12</b>:178–186.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1093/cercor/12.2.178" class="doi__link">
            https://doi.org/10.1093/cercor/12.2.178
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://www.ncbi.nlm.nih.gov/pubmed/11739265" class="reference__abstract_link">PubMed</a>
          </li>
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Face+repetition+effects+in+implicit+and+explicit+memory+tests+as+measured+by+fMRI&amp;author=Henson+RN&amp;author=Shallice+T&amp;author%5B2%5D=Gorno-Tempini+ML&amp;author%5B3%5D=Dolan+RJ&amp;publication_year=2002&amp;journal=Cerebral+Cortex&amp;volume=12&amp;pages=pp.+178%E2%80%93186&amp;pmid=11739265" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib25" id="bib25">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Hintzman+DL%22" class="reference__authors_link">Hintzman DL</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(1986)</span>


            <a href="https://doi.org/10.1037/0033-295X.93.4.411" class="reference__title">"Schema abstraction" in a multiple-trace memory model</a>




          <div class="reference__origin"><i>Psychological Review</i> <b>93</b>:411–428.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1037/0033-295X.93.4.411" class="doi__link">
            https://doi.org/10.1037/0033-295X.93.4.411
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=%22Schema+abstraction%22+in+a+multiple-trace+memory+model&amp;author=Hintzman+DL&amp;publication_year=1986&amp;journal=Psychological+Review&amp;volume=93&amp;pages=pp.+411%E2%80%93428" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib26" id="bib26">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Homa+D%22" class="reference__authors_link">Homa D</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(1973)</span>


            <a href="https://doi.org/10.1037/h0035772" class="reference__title">Prototype abstraction and classification of new instances as a function of number of instances defining the prototype</a>




          <div class="reference__origin"><i>Journal of Experimental Psychology</i> <b>101</b>:116–122.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1037/h0035772" class="doi__link">
            https://doi.org/10.1037/h0035772
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Prototype+abstraction+and+classification+of+new+instances+as+a+function+of+number+of+instances+defining+the+prototype&amp;author=Homa+D&amp;publication_year=1973&amp;journal=Journal+of+Experimental+Psychology&amp;volume=101&amp;pages=pp.+116%E2%80%93122" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib27" id="bib27">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Johansen+MK%22" class="reference__authors_link">Johansen MK</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Palmeri+TJ%22" class="reference__authors_link">Palmeri TJ</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(2002)</span>


            <a href="https://doi.org/10.1016/S0010-0285(02)00505-4" class="reference__title">Are there representational shifts during category learning?</a>




          <div class="reference__origin"><i>Cognitive Psychology</i> <b>45</b>:482–553.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1016/S0010-0285(02)00505-4" class="doi__link">
            https://doi.org/10.1016/S0010-0285(02)00505-4
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://www.ncbi.nlm.nih.gov/pubmed/12480477" class="reference__abstract_link">PubMed</a>
          </li>
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Are+there+representational+shifts+during+category+learning%3F&amp;author=Johansen+MK&amp;author=Palmeri+TJ&amp;publication_year=2002&amp;journal=Cognitive+Psychology&amp;volume=45&amp;pages=pp.+482%E2%80%93553&amp;pmid=12480477" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib28" id="bib28">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Jonides+J%22" class="reference__authors_link">Jonides J</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Smith+EE%22" class="reference__authors_link">Smith EE</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Marshuetz+C%22" class="reference__authors_link">Marshuetz C</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Koeppe+RA%22" class="reference__authors_link">Koeppe RA</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Reuter-Lorenz+PA%22" class="reference__authors_link">Reuter-Lorenz PA</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(1998)</span>


            <a href="https://doi.org/10.1073/pnas.95.14.8410" class="reference__title">Inhibition in verbal working memory revealed by brain activation</a>




          <div class="reference__origin"><i>PNAS</i> <b>95</b>:8410–8413.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1073/pnas.95.14.8410" class="doi__link">
            https://doi.org/10.1073/pnas.95.14.8410
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Inhibition+in+verbal+working+memory+revealed+by+brain+activation&amp;author=Jonides+J&amp;author=Smith+EE&amp;author%5B2%5D=Marshuetz+C&amp;author%5B3%5D=Koeppe+RA&amp;author%5B4%5D=Reuter-Lorenz+PA&amp;publication_year=1998&amp;journal=PNAS&amp;volume=95&amp;pages=pp.+8410%E2%80%938413" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib29" id="bib29">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:K%C3%A9ri+S%22" class="reference__authors_link">Kéri S</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Kelemen+O%22" class="reference__authors_link">Kelemen O</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Benedek+G%22" class="reference__authors_link">Benedek G</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Janka+Z%22" class="reference__authors_link">Janka Z</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(2001)</span>


            <a href="https://doi.org/10.1016/S0920-9964(00)00092-X" class="reference__title">Intact prototype learning in schizophrenia</a>




          <div class="reference__origin"><i>Schizophrenia Research</i> <b>52</b>:261–264.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1016/S0920-9964(00)00092-X" class="doi__link">
            https://doi.org/10.1016/S0920-9964(00)00092-X
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://www.ncbi.nlm.nih.gov/pubmed/11705719" class="reference__abstract_link">PubMed</a>
          </li>
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Intact+prototype+learning+in+schizophrenia&amp;author=K%C3%A9ri+S&amp;author=Kelemen+O&amp;author%5B2%5D=Benedek+G&amp;author%5B3%5D=Janka+Z&amp;publication_year=2001&amp;journal=Schizophrenia+Research&amp;volume=52&amp;pages=pp.+261%E2%80%93264&amp;pmid=11705719" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib30" id="bib30">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Koenig+P%22" class="reference__authors_link">Koenig P</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Smith+EE%22" class="reference__authors_link">Smith EE</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Troiani+V%22" class="reference__authors_link">Troiani V</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Anderson+C%22" class="reference__authors_link">Anderson C</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Moore+P%22" class="reference__authors_link">Moore P</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Grossman+M%22" class="reference__authors_link">Grossman M</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(2008)</span>


            <a href="https://doi.org/10.1093/cercor/bhn043" class="reference__title">Medial temporal lobe involvement in an implicit memory task: evidence of collaborating implicit and explicit memory systems from FMRI and Alzheimer's disease</a>




          <div class="reference__origin"><i>Cerebral Cortex</i> <b>18</b>:2831–2843.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1093/cercor/bhn043" class="doi__link">
            https://doi.org/10.1093/cercor/bhn043
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://www.ncbi.nlm.nih.gov/pubmed/18400793" class="reference__abstract_link">PubMed</a>
          </li>
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Medial+temporal+lobe+involvement+in+an+implicit+memory+task%3A+evidence+of+collaborating+implicit+and+explicit+memory+systems+from+FMRI+and+Alzheimer%27s+disease&amp;author=Koenig+P&amp;author=Smith+EE&amp;author%5B2%5D=Troiani+V&amp;author%5B3%5D=Anderson+C&amp;author%5B4%5D=Moore+P&amp;author%5B5%5D=Grossman+M&amp;publication_year=2008&amp;journal=Cerebral+Cortex&amp;volume=18&amp;pages=pp.+2831%E2%80%932843&amp;pmid=18400793" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib31" id="bib31">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Koster+R%22" class="reference__authors_link">Koster R</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Chadwick+MJ%22" class="reference__authors_link">Chadwick MJ</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Chen+Y%22" class="reference__authors_link">Chen Y</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Berron+D%22" class="reference__authors_link">Berron D</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Banino+A%22" class="reference__authors_link">Banino A</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:D%C3%BCzel+E%22" class="reference__authors_link">Düzel E</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Hassabis+D%22" class="reference__authors_link">Hassabis D</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Kumaran+D%22" class="reference__authors_link">Kumaran D</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(2018)</span>


            <a href="https://doi.org/10.1016/j.neuron.2018.08.009" class="reference__title">Big-Loop recurrence within the hippocampal system supports integration of information across episodes</a>




          <div class="reference__origin"><i>Neuron</i> <b>99</b>:1342–1354.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1016/j.neuron.2018.08.009" class="doi__link">
            https://doi.org/10.1016/j.neuron.2018.08.009
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://www.ncbi.nlm.nih.gov/pubmed/30236285" class="reference__abstract_link">PubMed</a>
          </li>
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Big-Loop+recurrence+within+the+hippocampal+system+supports+integration+of+information+across+episodes&amp;author=Koster+R&amp;author=Chadwick+MJ&amp;author%5B2%5D=Chen+Y&amp;author%5B3%5D=Berron+D&amp;author%5B4%5D=Banino+A&amp;author%5B5%5D=D%C3%BCzel+E&amp;author%5B6%5D=Hassabis+D&amp;author%5B7%5D=Kumaran+D&amp;publication_year=2018&amp;journal=Neuron&amp;volume=99&amp;pages=pp.+1342%E2%80%931354&amp;pmid=30236285" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib32" id="bib32">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Kruschke+JK%22" class="reference__authors_link">Kruschke JK</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(1992)</span>


            <a href="https://doi.org/10.1037/0033-295X.99.1.22" class="reference__title">ALCOVE: an exemplar-based connectionist model of category learning</a>




          <div class="reference__origin"><i>Psychological Review</i> <b>99</b>:22–44.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1037/0033-295X.99.1.22" class="doi__link">
            https://doi.org/10.1037/0033-295X.99.1.22
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://www.ncbi.nlm.nih.gov/pubmed/1546117" class="reference__abstract_link">PubMed</a>
          </li>
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=ALCOVE%3A+an+exemplar-based+connectionist+model+of+category+learning&amp;author=Kruschke+JK&amp;publication_year=1992&amp;journal=Psychological+Review&amp;volume=99&amp;pages=pp.+22%E2%80%9344&amp;pmid=1546117" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib33" id="bib33">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Kuhl+BA%22" class="reference__authors_link">Kuhl BA</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Dudukovic+NM%22" class="reference__authors_link">Dudukovic NM</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Kahn+I%22" class="reference__authors_link">Kahn I</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Wagner+AD%22" class="reference__authors_link">Wagner AD</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(2007)</span>


            <a href="https://doi.org/10.1038/nn1918" class="reference__title">Decreased demands on cognitive control reveal the neural processing benefits of forgetting</a>




          <div class="reference__origin"><i>Nature Neuroscience</i> <b>10</b>:908–914.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1038/nn1918" class="doi__link">
            https://doi.org/10.1038/nn1918
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://www.ncbi.nlm.nih.gov/pubmed/17558403" class="reference__abstract_link">PubMed</a>
          </li>
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Decreased+demands+on+cognitive+control+reveal+the+neural+processing+benefits+of+forgetting&amp;author=Kuhl+BA&amp;author=Dudukovic+NM&amp;author%5B2%5D=Kahn+I&amp;author%5B3%5D=Wagner+AD&amp;publication_year=2007&amp;journal=Nature+Neuroscience&amp;volume=10&amp;pages=pp.+908%E2%80%93914&amp;pmid=17558403" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib34" id="bib34">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Kuhl+BA%22" class="reference__authors_link">Kuhl BA</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Chun+MM%22" class="reference__authors_link">Chun MM</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(2014)</span>


            <a href="https://doi.org/10.1523/JNEUROSCI.4328-13.2014" class="reference__title">Successful remembering elicits event-specific activity patterns in lateral parietal cortex</a>




          <div class="reference__origin"><i>Journal of Neuroscience</i> <b>34</b>:8051–8060.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1523/JNEUROSCI.4328-13.2014" class="doi__link">
            https://doi.org/10.1523/JNEUROSCI.4328-13.2014
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://www.ncbi.nlm.nih.gov/pubmed/24899726" class="reference__abstract_link">PubMed</a>
          </li>
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Successful+remembering+elicits+event-specific+activity+patterns+in+lateral+parietal+cortex&amp;author=Kuhl+BA&amp;author=Chun+MM&amp;publication_year=2014&amp;journal=Journal+of+Neuroscience&amp;volume=34&amp;pages=pp.+8051%E2%80%938060&amp;pmid=24899726" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib35" id="bib35">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Lech+RK%22" class="reference__authors_link">Lech RK</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:G%C3%BCnt%C3%BCrk%C3%BCn+O%22" class="reference__authors_link">Güntürkün O</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Suchan+B%22" class="reference__authors_link">Suchan B</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(2016)</span>


            <a href="https://doi.org/10.1016/j.bbr.2016.05.049" class="reference__title">An interplay of fusiform gyrus and Hippocampus enables prototype- and exemplar-based category learning</a>




          <div class="reference__origin"><i>Behavioural Brain Research</i> <b>311</b>:239–246.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1016/j.bbr.2016.05.049" class="doi__link">
            https://doi.org/10.1016/j.bbr.2016.05.049
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://www.ncbi.nlm.nih.gov/pubmed/27233826" class="reference__abstract_link">PubMed</a>
          </li>
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=An+interplay+of+fusiform+gyrus+and+Hippocampus+enables+prototype-+and+exemplar-based+category+learning&amp;author=Lech+RK&amp;author=G%C3%BCnt%C3%BCrk%C3%BCn+O&amp;author%5B2%5D=Suchan+B&amp;publication_year=2016&amp;journal=Behavioural+Brain+Research&amp;volume=311&amp;pages=pp.+239%E2%80%93246&amp;pmid=27233826" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib36" id="bib36">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Mack+ML%22" class="reference__authors_link">Mack ML</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Preston+AR%22" class="reference__authors_link">Preston AR</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Love+BC%22" class="reference__authors_link">Love BC</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(2013)</span>


            <a href="https://doi.org/10.1016/j.cub.2013.08.035" class="reference__title">Decoding the brain's algorithm for categorization from its neural implementation</a>




          <div class="reference__origin"><i>Current Biology</i> <b>23</b>:2023–2027.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1016/j.cub.2013.08.035" class="doi__link">
            https://doi.org/10.1016/j.cub.2013.08.035
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://www.ncbi.nlm.nih.gov/pubmed/24094852" class="reference__abstract_link">PubMed</a>
          </li>
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Decoding+the+brain%27s+algorithm+for+categorization+from+its+neural+implementation&amp;author=Mack+ML&amp;author=Preston+AR&amp;author%5B2%5D=Love+BC&amp;publication_year=2013&amp;journal=Current+Biology&amp;volume=23&amp;pages=pp.+2023%E2%80%932027&amp;pmid=24094852" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib37" id="bib37">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Maddox+WT%22" class="reference__authors_link">Maddox WT</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Glass+BD%22" class="reference__authors_link">Glass BD</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Zeithamova+D%22" class="reference__authors_link">Zeithamova D</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Savarie+ZR%22" class="reference__authors_link">Savarie ZR</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Bowen+C%22" class="reference__authors_link">Bowen C</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Matthews+MD%22" class="reference__authors_link">Matthews MD</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Schnyer+DM%22" class="reference__authors_link">Schnyer DM</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(2011)</span>


            <a href="https://doi.org/10.1093/sleep/34.3.253" class="reference__title">The effects of sleep deprivation on dissociable prototype learning systems</a>




          <div class="reference__origin"><i>Sleep</i> <b>34</b>:253–260.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1093/sleep/34.3.253" class="doi__link">
            https://doi.org/10.1093/sleep/34.3.253
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://www.ncbi.nlm.nih.gov/pubmed/21358842" class="reference__abstract_link">PubMed</a>
          </li>
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=The+effects+of+sleep+deprivation+on+dissociable+prototype+learning+systems&amp;author=Maddox+WT&amp;author=Glass+BD&amp;author%5B2%5D=Zeithamova+D&amp;author%5B3%5D=Savarie+ZR&amp;author%5B4%5D=Bowen+C&amp;author%5B5%5D=Matthews+MD&amp;author%5B6%5D=Schnyer+DM&amp;publication_year=2011&amp;journal=Sleep&amp;volume=34&amp;pages=pp.+253%E2%80%93260&amp;pmid=21358842" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib38" id="bib38">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:McClelland+JL%22" class="reference__authors_link">McClelland JL</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:McNaughton+BL%22" class="reference__authors_link">McNaughton BL</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:O%27Reilly+RC%22" class="reference__authors_link">O'Reilly RC</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(1995)</span>


            <a href="https://doi.org/10.1037/0033-295X.102.3.419" class="reference__title">Why there are complementary learning systems in the Hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory</a>




          <div class="reference__origin"><i>Psychological Review</i> <b>102</b>:419–457.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1037/0033-295X.102.3.419" class="doi__link">
            https://doi.org/10.1037/0033-295X.102.3.419
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://www.ncbi.nlm.nih.gov/pubmed/7624455" class="reference__abstract_link">PubMed</a>
          </li>
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Why+there+are+complementary+learning+systems+in+the+Hippocampus+and+neocortex%3A+insights+from+the+successes+and+failures+of+connectionist+models+of+learning+and+memory&amp;author=McClelland+JL&amp;author=McNaughton+BL&amp;author%5B2%5D=O%27Reilly+RC&amp;publication_year=1995&amp;journal=Psychological+Review&amp;volume=102&amp;pages=pp.+419%E2%80%93457&amp;pmid=7624455" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib39" id="bib39">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Medin+DL%22" class="reference__authors_link">Medin DL</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Schaffer+MM%22" class="reference__authors_link">Schaffer MM</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(1978)</span>


            <a href="https://doi.org/10.1037/0033-295X.85.3.207" class="reference__title">Context theory of classification learning</a>




          <div class="reference__origin"><i>Psychological Review</i> <b>85</b>:207–238.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1037/0033-295X.85.3.207" class="doi__link">
            https://doi.org/10.1037/0033-295X.85.3.207
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Context+theory+of+classification+learning&amp;author=Medin+DL&amp;author=Schaffer+MM&amp;publication_year=1978&amp;journal=Psychological+Review&amp;volume=85&amp;pages=pp.+207%E2%80%93238" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib40" id="bib40">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Minda+JP%22" class="reference__authors_link">Minda JP</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Smith+JD%22" class="reference__authors_link">Smith JD</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(2001)</span>


            <a href="https://doi.org/10.1037/0278-7393.27.3.775" class="reference__title">Prototypes in category learning: the effects of category size, category structure, and stimulus complexity</a>




          <div class="reference__origin"><i>Journal of Experimental Psychology: Learning, Memory, and Cognition</i> <b>27</b>:775–799.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1037/0278-7393.27.3.775" class="doi__link">
            https://doi.org/10.1037/0278-7393.27.3.775
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://www.ncbi.nlm.nih.gov/pubmed/11394680" class="reference__abstract_link">PubMed</a>
          </li>
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Prototypes+in+category+learning%3A+the+effects+of+category+size%2C+category+structure%2C+and+stimulus+complexity&amp;author=Minda+JP&amp;author=Smith+JD&amp;publication_year=2001&amp;journal=Journal+of+Experimental+Psychology%3A+Learning%2C+Memory%2C+and+Cognition&amp;volume=27&amp;pages=pp.+775%E2%80%93799&amp;pmid=11394680" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib41" id="bib41">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Moscovitch+M%22" class="reference__authors_link">Moscovitch M</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Cabeza+R%22" class="reference__authors_link">Cabeza R</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Winocur+G%22" class="reference__authors_link">Winocur G</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Nadel+L%22" class="reference__authors_link">Nadel L</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(2016)</span>


            <a href="https://doi.org/10.1146/annurev-psych-113011-143733" class="reference__title">Episodic memory and beyond: the Hippocampus and neocortex in transformation</a>




          <div class="reference__origin"><i>Annual Review of Psychology</i> <b>67</b>:105–134.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1146/annurev-psych-113011-143733" class="doi__link">
            https://doi.org/10.1146/annurev-psych-113011-143733
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://www.ncbi.nlm.nih.gov/pubmed/26726963" class="reference__abstract_link">PubMed</a>
          </li>
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Episodic+memory+and+beyond%3A+the+Hippocampus+and+neocortex+in+transformation&amp;author=Moscovitch+M&amp;author=Cabeza+R&amp;author%5B2%5D=Winocur+G&amp;author%5B3%5D=Nadel+L&amp;publication_year=2016&amp;journal=Annual+Review+of+Psychology&amp;volume=67&amp;pages=pp.+105%E2%80%93134&amp;pmid=26726963" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib42" id="bib42">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Myers+EB%22" class="reference__authors_link">Myers EB</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Swan+K%22" class="reference__authors_link">Swan K</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(2012)</span>


            <a href="https://doi.org/10.1162/jocn_a_00243" class="reference__title">Effects of category learning on neural sensitivity to non-native phonetic categories</a>




          <div class="reference__origin"><i>Journal of Cognitive Neuroscience</i> <b>24</b>:1695–1708.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1162/jocn_a_00243" class="doi__link">
            https://doi.org/10.1162/jocn_a_00243
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://www.ncbi.nlm.nih.gov/pubmed/22621261" class="reference__abstract_link">PubMed</a>
          </li>
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Effects+of+category+learning+on+neural+sensitivity+to+non-native+phonetic+categories&amp;author=Myers+EB&amp;author=Swan+K&amp;publication_year=2012&amp;journal=Journal+of+Cognitive+Neuroscience&amp;volume=24&amp;pages=pp.+1695%E2%80%931708&amp;pmid=22621261" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib43" id="bib43">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Nomura+EM%22" class="reference__authors_link">Nomura EM</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Maddox+WT%22" class="reference__authors_link">Maddox WT</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Filoteo+JV%22" class="reference__authors_link">Filoteo JV</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Ing+AD%22" class="reference__authors_link">Ing AD</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Gitelman+DR%22" class="reference__authors_link">Gitelman DR</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Parrish+TB%22" class="reference__authors_link">Parrish TB</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Mesulam+MM%22" class="reference__authors_link">Mesulam MM</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Reber+PJ%22" class="reference__authors_link">Reber PJ</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(2007)</span>


            <a href="https://doi.org/10.1093/cercor/bhj122" class="reference__title">Neural correlates of rule-based and information-integration visual category learning</a>




          <div class="reference__origin"><i>Cerebral Cortex</i> <b>17</b>:37–43.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1093/cercor/bhj122" class="doi__link">
            https://doi.org/10.1093/cercor/bhj122
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://www.ncbi.nlm.nih.gov/pubmed/16436685" class="reference__abstract_link">PubMed</a>
          </li>
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Neural+correlates+of+rule-based+and+information-integration+visual+category+learning&amp;author=Nomura+EM&amp;author=Maddox+WT&amp;author%5B2%5D=Filoteo+JV&amp;author%5B3%5D=Ing+AD&amp;author%5B4%5D=Gitelman+DR&amp;author%5B5%5D=Parrish+TB&amp;author%5B6%5D=Mesulam+MM&amp;author%5B7%5D=Reber+PJ&amp;publication_year=2007&amp;journal=Cerebral+Cortex&amp;volume=17&amp;pages=pp.+37%E2%80%9343&amp;pmid=16436685" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib44" id="bib44">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Nosofsky+RM%22" class="reference__authors_link">Nosofsky RM</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(1986)</span>


            <a href="https://doi.org/10.1037/0096-3445.115.1.39" class="reference__title">Attention, similarity, and the identification–categorization relationship</a>




          <div class="reference__origin"><i>Journal of Experimental Psychology: General</i> <b>115</b>:39–57.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1037/0096-3445.115.1.39" class="doi__link">
            https://doi.org/10.1037/0096-3445.115.1.39
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Attention%2C+similarity%2C+and+the+identification%E2%80%93categorization+relationship&amp;author=Nosofsky+RM&amp;publication_year=1986&amp;journal=Journal+of+Experimental+Psychology%3A+General&amp;volume=115&amp;pages=pp.+39%E2%80%9357" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib45" id="bib45">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Nosofsky+RM%22" class="reference__authors_link">Nosofsky RM</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(1987)</span>


            <a href="https://doi.org/10.1037/0278-7393.13.1.87" class="reference__title">Attention and learning processes in the identification and categorization of integral stimuli</a>




          <div class="reference__origin"><i>Journal of Experimental Psychology: Learning, Memory, and Cognition</i> <b>13</b>:87–108.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1037/0278-7393.13.1.87" class="doi__link">
            https://doi.org/10.1037/0278-7393.13.1.87
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://www.ncbi.nlm.nih.gov/pubmed/2949055" class="reference__abstract_link">PubMed</a>
          </li>
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Attention+and+learning+processes+in+the+identification+and+categorization+of+integral+stimuli&amp;author=Nosofsky+RM&amp;publication_year=1987&amp;journal=Journal+of+Experimental+Psychology%3A+Learning%2C+Memory%2C+and+Cognition&amp;volume=13&amp;pages=pp.+87%E2%80%93108&amp;pmid=2949055" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib46" id="bib46">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Nosofsky+RM%22" class="reference__authors_link">Nosofsky RM</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(1988)</span>


            <a href="https://doi.org/10.1037/0278-7393.14.4.700" class="reference__title">Exemplar-based accounts of relations between classification, recognition, and typicality</a>




          <div class="reference__origin"><i>Journal of Experimental Psychology: Learning, Memory, and Cognition</i> <b>14</b>:700–708.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1037/0278-7393.14.4.700" class="doi__link">
            https://doi.org/10.1037/0278-7393.14.4.700
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Exemplar-based+accounts+of+relations+between+classification%2C+recognition%2C+and+typicality&amp;author=Nosofsky+RM&amp;publication_year=1988&amp;journal=Journal+of+Experimental+Psychology%3A+Learning%2C+Memory%2C+and+Cognition&amp;volume=14&amp;pages=pp.+700%E2%80%93708" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib47" id="bib47">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Nosofsky+RM%22" class="reference__authors_link">Nosofsky RM</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Palmeri+TJ%22" class="reference__authors_link">Palmeri TJ</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:McKinley+SC%22" class="reference__authors_link">McKinley SC</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(1994)</span>


            <a href="https://doi.org/10.1037/0033-295X.101.1.53" class="reference__title">Rule-plus-exception model of classification learning</a>




          <div class="reference__origin"><i>Psychological Review</i> <b>101</b>:53–79.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1037/0033-295X.101.1.53" class="doi__link">
            https://doi.org/10.1037/0033-295X.101.1.53
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://www.ncbi.nlm.nih.gov/pubmed/8121960" class="reference__abstract_link">PubMed</a>
          </li>
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Rule-plus-exception+model+of+classification+learning&amp;author=Nosofsky+RM&amp;author=Palmeri+TJ&amp;author%5B2%5D=McKinley+SC&amp;publication_year=1994&amp;journal=Psychological+Review&amp;volume=101&amp;pages=pp.+53%E2%80%9379&amp;pmid=8121960" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib48" id="bib48">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Nosofsky+RM%22" class="reference__authors_link">Nosofsky RM</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Little+DR%22" class="reference__authors_link">Little DR</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:James+TW%22" class="reference__authors_link">James TW</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(2012)</span>


            <a href="https://doi.org/10.1073/pnas.1111304109" class="reference__title">Activation in the neural network responsible for categorization and recognition reflects parameter changes</a>




          <div class="reference__origin"><i>PNAS</i> <b>109</b>:333–338.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1073/pnas.1111304109" class="doi__link">
            https://doi.org/10.1073/pnas.1111304109
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://www.ncbi.nlm.nih.gov/pubmed/22184233" class="reference__abstract_link">PubMed</a>
          </li>
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Activation+in+the+neural+network+responsible+for+categorization+and+recognition+reflects+parameter+changes&amp;author=Nosofsky+RM&amp;author=Little+DR&amp;author%5B2%5D=James+TW&amp;publication_year=2012&amp;journal=PNAS&amp;volume=109&amp;pages=pp.+333%E2%80%93338&amp;pmid=22184233" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib49" id="bib49">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Nosofsky+RM%22" class="reference__authors_link">Nosofsky RM</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Stanton+RD%22" class="reference__authors_link">Stanton RD</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(2005)</span>


            <a href="https://doi.org/10.1037/0096-1523.31.3.608" class="reference__title">Speeded classification in a probabilistic category structure: contrasting exemplar-retrieval, decision-boundary, and prototype models</a>




          <div class="reference__origin"><i>Journal of Experimental Psychology: Human Perception and Performance</i> <b>31</b>:608–629.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1037/0096-1523.31.3.608" class="doi__link">
            https://doi.org/10.1037/0096-1523.31.3.608
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://www.ncbi.nlm.nih.gov/pubmed/15982134" class="reference__abstract_link">PubMed</a>
          </li>
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Speeded+classification+in+a+probabilistic+category+structure%3A+contrasting+exemplar-retrieval%2C+decision-boundary%2C+and+prototype+models&amp;author=Nosofsky+RM&amp;author=Stanton+RD&amp;publication_year=2005&amp;journal=Journal+of+Experimental+Psychology%3A+Human+Perception+and+Performance&amp;volume=31&amp;pages=pp.+608%E2%80%93629&amp;pmid=15982134" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib50" id="bib50">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Palmeri+TJ%22" class="reference__authors_link">Palmeri TJ</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Gauthier+I%22" class="reference__authors_link">Gauthier I</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(2004)</span>


            <a href="https://doi.org/10.1038/nrn1364" class="reference__title">Visual object understanding</a>




          <div class="reference__origin"><i>Nature Reviews Neuroscience</i> <b>5</b>:291–303.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1038/nrn1364" class="doi__link">
            https://doi.org/10.1038/nrn1364
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://www.ncbi.nlm.nih.gov/pubmed/15034554" class="reference__abstract_link">PubMed</a>
          </li>
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Visual+object+understanding&amp;author=Palmeri+TJ&amp;author=Gauthier+I&amp;publication_year=2004&amp;journal=Nature+Reviews+Neuroscience&amp;volume=5&amp;pages=pp.+291%E2%80%93303&amp;pmid=15034554" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib51" id="bib51">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Paniukov+D%22" class="reference__authors_link">Paniukov D</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Davis+T%22" class="reference__authors_link">Davis T</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(2018)</span>


            <a href="https://doi.org/10.1016/j.neuroimage.2017.10.057" class="reference__title">The evaluative role of rostrolateral prefrontal cortex in rule-based category learning</a>




          <div class="reference__origin"><i>NeuroImage</i> <b>166</b>:19–31.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1016/j.neuroimage.2017.10.057" class="doi__link">
            https://doi.org/10.1016/j.neuroimage.2017.10.057
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://www.ncbi.nlm.nih.gov/pubmed/29107769" class="reference__abstract_link">PubMed</a>
          </li>
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=The+evaluative+role+of+rostrolateral+prefrontal+cortex+in+rule-based+category+learning&amp;author=Paniukov+D&amp;author=Davis+T&amp;publication_year=2018&amp;journal=NeuroImage&amp;volume=166&amp;pages=pp.+19%E2%80%9331&amp;pmid=29107769" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib52" id="bib52">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Payne+JD%22" class="reference__authors_link">Payne JD</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Schacter+DL%22" class="reference__authors_link">Schacter DL</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Propper+RE%22" class="reference__authors_link">Propper RE</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Huang+LW%22" class="reference__authors_link">Huang LW</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Wamsley+EJ%22" class="reference__authors_link">Wamsley EJ</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Tucker+MA%22" class="reference__authors_link">Tucker MA</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Walker+MP%22" class="reference__authors_link">Walker MP</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Stickgold+R%22" class="reference__authors_link">Stickgold R</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(2009)</span>


            <a href="https://doi.org/10.1016/j.nlm.2009.03.007" class="reference__title">The role of sleep in false memory formation</a>




          <div class="reference__origin"><i>Neurobiology of Learning and Memory</i> <b>92</b>:327–334.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1016/j.nlm.2009.03.007" class="doi__link">
            https://doi.org/10.1016/j.nlm.2009.03.007
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://www.ncbi.nlm.nih.gov/pubmed/19348959" class="reference__abstract_link">PubMed</a>
          </li>
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=The+role+of+sleep+in+false+memory+formation&amp;author=Payne+JD&amp;author=Schacter+DL&amp;author%5B2%5D=Propper+RE&amp;author%5B3%5D=Huang+LW&amp;author%5B4%5D=Wamsley+EJ&amp;author%5B5%5D=Tucker+MA&amp;author%5B6%5D=Walker+MP&amp;author%5B7%5D=Stickgold+R&amp;publication_year=2009&amp;journal=Neurobiology+of+Learning+and+Memory&amp;volume=92&amp;pages=pp.+327%E2%80%93334&amp;pmid=19348959" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib53" id="bib53">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Poldrack+RA%22" class="reference__authors_link">Poldrack RA</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Clark+J%22" class="reference__authors_link">Clark J</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Par%C3%A9-Blagoev+EJ%22" class="reference__authors_link">Paré-Blagoev EJ</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Shohamy+D%22" class="reference__authors_link">Shohamy D</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Creso+Moyano+J%22" class="reference__authors_link">Creso Moyano J</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Myers+C%22" class="reference__authors_link">Myers C</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Gluck+MA%22" class="reference__authors_link">Gluck MA</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(2001)</span>


            <a href="https://doi.org/10.1038/35107080" class="reference__title">Interactive memory systems in the human brain</a>




          <div class="reference__origin"><i>Nature</i> <b>414</b>:546–550.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1038/35107080" class="doi__link">
            https://doi.org/10.1038/35107080
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://www.ncbi.nlm.nih.gov/pubmed/11734855" class="reference__abstract_link">PubMed</a>
          </li>
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Interactive+memory+systems+in+the+human+brain&amp;author=Poldrack+RA&amp;author=Clark+J&amp;author%5B2%5D=Par%C3%A9-Blagoev+EJ&amp;author%5B3%5D=Shohamy+D&amp;author%5B4%5D=Creso+Moyano+J&amp;author%5B5%5D=Myers+C&amp;author%5B6%5D=Gluck+MA&amp;publication_year=2001&amp;journal=Nature&amp;volume=414&amp;pages=pp.+546%E2%80%93550&amp;pmid=11734855" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib54" id="bib54">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Poldrack+RA%22" class="reference__authors_link">Poldrack RA</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Packard+MG%22" class="reference__authors_link">Packard MG</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(2003)</span>


            <a href="https://doi.org/10.1016/S0028-3932(02)00157-4" class="reference__title">Competition among multiple memory systems: converging evidence from animal and human brain studies</a>




          <div class="reference__origin"><i>Neuropsychologia</i> <b>41</b>:245–251.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1016/S0028-3932(02)00157-4" class="doi__link">
            https://doi.org/10.1016/S0028-3932(02)00157-4
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://www.ncbi.nlm.nih.gov/pubmed/12457750" class="reference__abstract_link">PubMed</a>
          </li>
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Competition+among+multiple+memory+systems%3A+converging+evidence+from+animal+and+human+brain+studies&amp;author=Poldrack+RA&amp;author=Packard+MG&amp;publication_year=2003&amp;journal=Neuropsychologia&amp;volume=41&amp;pages=pp.+245%E2%80%93251&amp;pmid=12457750" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib55" id="bib55">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Poppenk+J%22" class="reference__authors_link">Poppenk J</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Evensmoen+HR%22" class="reference__authors_link">Evensmoen HR</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Moscovitch+M%22" class="reference__authors_link">Moscovitch M</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Nadel+L%22" class="reference__authors_link">Nadel L</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(2013)</span>


            <a href="https://doi.org/10.1016/j.tics.2013.03.005" class="reference__title">Long-axis specialization of the human Hippocampus</a>




          <div class="reference__origin"><i>Trends in Cognitive Sciences</i> <b>17</b>:230–240.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1016/j.tics.2013.03.005" class="doi__link">
            https://doi.org/10.1016/j.tics.2013.03.005
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://www.ncbi.nlm.nih.gov/pubmed/23597720" class="reference__abstract_link">PubMed</a>
          </li>
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Long-axis+specialization+of+the+human+Hippocampus&amp;author=Poppenk+J&amp;author=Evensmoen+HR&amp;author%5B2%5D=Moscovitch+M&amp;author%5B3%5D=Nadel+L&amp;publication_year=2013&amp;journal=Trends+in+Cognitive+Sciences&amp;volume=17&amp;pages=pp.+230%E2%80%93240&amp;pmid=23597720" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib56" id="bib56">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Posner+MI%22" class="reference__authors_link">Posner MI</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Keele+SW%22" class="reference__authors_link">Keele SW</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(1968)</span>


            <a href="https://doi.org/10.1037/h0025953" class="reference__title">On the genesis of abstract ideas</a>




          <div class="reference__origin"><i>Journal of Experimental Psychology</i> <b>77</b>:353–363.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1037/h0025953" class="doi__link">
            https://doi.org/10.1037/h0025953
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=On+the+genesis+of+abstract+ideas&amp;author=Posner+MI&amp;author=Keele+SW&amp;publication_year=1968&amp;journal=Journal+of+Experimental+Psychology&amp;volume=77&amp;pages=pp.+353%E2%80%93363" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib57" id="bib57">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Posner+MI%22" class="reference__authors_link">Posner MI</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Keele+SW%22" class="reference__authors_link">Keele SW</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(1970)</span>


            <a href="https://doi.org/10.1037/h0028558" class="reference__title">Retention of abstract ideas</a>




          <div class="reference__origin"><i>Journal of Experimental Psychology</i> <b>83</b>:304–308.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1037/h0028558" class="doi__link">
            https://doi.org/10.1037/h0028558
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Retention+of+abstract+ideas&amp;author=Posner+MI&amp;author=Keele+SW&amp;publication_year=1970&amp;journal=Journal+of+Experimental+Psychology&amp;volume=83&amp;pages=pp.+304%E2%80%93308" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib58" id="bib58">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Reed+SK%22" class="reference__authors_link">Reed SK</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(1972)</span>


            <a href="https://doi.org/10.1016/0010-0285(72)90014-X" class="reference__title">Pattern recognition and categorization</a>




          <div class="reference__origin"><i>Cognitive Psychology</i> <b>3</b>:382–407.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1016/0010-0285(72)90014-X" class="doi__link">
            https://doi.org/10.1016/0010-0285(72)90014-X
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Pattern+recognition+and+categorization&amp;author=Reed+SK&amp;publication_year=1972&amp;journal=Cognitive+Psychology&amp;volume=3&amp;pages=pp.+382%E2%80%93407" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib59" id="bib59">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Schapiro+AC%22" class="reference__authors_link">Schapiro AC</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:McDevitt+EA%22" class="reference__authors_link">McDevitt EA</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Chen+L%22" class="reference__authors_link">Chen L</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Norman+KA%22" class="reference__authors_link">Norman KA</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Mednick+SC%22" class="reference__authors_link">Mednick SC</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Rogers+TT%22" class="reference__authors_link">Rogers TT</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(2017)</span>


            <a href="https://doi.org/10.1038/s41598-017-12884-5" class="reference__title">Sleep benefits memory for semantic category structure while preserving Exemplar-Specific information</a>




          <div class="reference__origin"><i>Scientific Reports</i> <b>7</b>:5.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1038/s41598-017-12884-5" class="doi__link">
            https://doi.org/10.1038/s41598-017-12884-5
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://www.ncbi.nlm.nih.gov/pubmed/29093451" class="reference__abstract_link">PubMed</a>
          </li>
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Sleep+benefits+memory+for+semantic+category+structure+while+preserving+Exemplar-Specific+information&amp;author=Schapiro+AC&amp;author=McDevitt+EA&amp;author%5B2%5D=Chen+L&amp;author%5B3%5D=Norman+KA&amp;author%5B4%5D=Mednick+SC&amp;author%5B5%5D=Rogers+TT&amp;publication_year=2017&amp;journal=Scientific+Reports&amp;volume=7&amp;pages=5&amp;pmid=29093451" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib60" id="bib60">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Schlichting+ML%22" class="reference__authors_link">Schlichting ML</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Mumford+JA%22" class="reference__authors_link">Mumford JA</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Preston+AR%22" class="reference__authors_link">Preston AR</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(2015)</span>


            <a href="https://doi.org/10.1038/ncomms9151" class="reference__title">Learning-related representational changes reveal dissociable integration and separation signatures in the Hippocampus and prefrontal cortex</a>




          <div class="reference__origin"><i>Nature Communications</i> <b>6</b>:8151.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1038/ncomms9151" class="doi__link">
            https://doi.org/10.1038/ncomms9151
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://www.ncbi.nlm.nih.gov/pubmed/26303198" class="reference__abstract_link">PubMed</a>
          </li>
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Learning-related+representational+changes+reveal+dissociable+integration+and+separation+signatures+in+the+Hippocampus+and+prefrontal+cortex&amp;author=Schlichting+ML&amp;author=Mumford+JA&amp;author%5B2%5D=Preston+AR&amp;publication_year=2015&amp;journal=Nature+Communications&amp;volume=6&amp;pages=8151&amp;pmid=26303198" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib61" id="bib61">
          <div class="reference__label">Book</div>
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Schlichting+ML%22" class="reference__authors_link">Schlichting ML</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Preston+AR%22" class="reference__authors_link">Preston AR</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(2017)</span>


            <a href="https://doi.org/10.1007/978-3-319-50406-3_13" class="reference__title">The hippocampus and memory integration: Building knowledge to navigate future decisions</a>




          <div class="reference__origin">In: Duff M. C, Hannula D. E, editors. <i>The Hippocampus From Cells to System: Structure, Connectivity, and Functional Contributions to Memory and Flexible Cognition</i>. Springer. pp. 405–437.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1007/978-3-319-50406-3_13" class="doi__link">
            https://doi.org/10.1007/978-3-319-50406-3_13
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=The+hippocampus+and+memory+integration%3A+Building+knowledge+to+navigate+future+decisions&amp;author=Schlichting+ML&amp;author=Preston+AR&amp;publication_year=2017&amp;pages=pp.+405%E2%80%93437" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib62" id="bib62">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Seger+CA%22" class="reference__authors_link">Seger CA</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(2005)</span>


            <a href="https://doi.org/10.1523/JNEUROSCI.3401-04.2005" class="reference__title">The roles of the caudate nucleus in human classification learning</a>




          <div class="reference__origin"><i>Journal of Neuroscience</i> <b>25</b>:2941–2951.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1523/JNEUROSCI.3401-04.2005" class="doi__link">
            https://doi.org/10.1523/JNEUROSCI.3401-04.2005
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=The+roles+of+the+caudate+nucleus+in+human+classification+learning&amp;author=Seger+CA&amp;publication_year=2005&amp;journal=Journal+of+Neuroscience&amp;volume=25&amp;pages=pp.+2941%E2%80%932951" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib63" id="bib63">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Shepard+RN%22" class="reference__authors_link">Shepard RN</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(1957)</span>


            <a href="https://doi.org/10.1007/BF02288967" class="reference__title">Stimulus and response generalization: a stochastic model relating generalization to distance in psychological space</a>




          <div class="reference__origin"><i>Psychometrika</i> <b>22</b>:325–345.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1007/BF02288967" class="doi__link">
            https://doi.org/10.1007/BF02288967
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Stimulus+and+response+generalization%3A+a+stochastic+model+relating+generalization+to+distance+in+psychological+space&amp;author=Shepard+RN&amp;publication_year=1957&amp;journal=Psychometrika&amp;volume=22&amp;pages=pp.+325%E2%80%93345" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib64" id="bib64">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Shohamy+D%22" class="reference__authors_link">Shohamy D</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Wagner+AD%22" class="reference__authors_link">Wagner AD</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(2008)</span>


            <a href="https://doi.org/10.1016/j.neuron.2008.09.023" class="reference__title">Integrating memories in the human brain: hippocampal-midbrain encoding of overlapping events</a>




          <div class="reference__origin"><i>Neuron</i> <b>60</b>:378–389.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1016/j.neuron.2008.09.023" class="doi__link">
            https://doi.org/10.1016/j.neuron.2008.09.023
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://www.ncbi.nlm.nih.gov/pubmed/18957228" class="reference__abstract_link">PubMed</a>
          </li>
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Integrating+memories+in+the+human+brain%3A+hippocampal-midbrain+encoding+of+overlapping+events&amp;author=Shohamy+D&amp;author=Wagner+AD&amp;publication_year=2008&amp;journal=Neuron&amp;volume=60&amp;pages=pp.+378%E2%80%93389&amp;pmid=18957228" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib65" id="bib65">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Smith+SM%22" class="reference__authors_link">Smith SM</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Jenkinson+M%22" class="reference__authors_link">Jenkinson M</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Woolrich+MW%22" class="reference__authors_link">Woolrich MW</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Beckmann+CF%22" class="reference__authors_link">Beckmann CF</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Behrens+TE%22" class="reference__authors_link">Behrens TE</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Johansen-Berg+H%22" class="reference__authors_link">Johansen-Berg H</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Bannister+PR%22" class="reference__authors_link">Bannister PR</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:De+Luca+M%22" class="reference__authors_link">De Luca M</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Drobnjak+I%22" class="reference__authors_link">Drobnjak I</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Flitney+DE%22" class="reference__authors_link">Flitney DE</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Niazy+RK%22" class="reference__authors_link">Niazy RK</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Saunders+J%22" class="reference__authors_link">Saunders J</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Vickers+J%22" class="reference__authors_link">Vickers J</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Zhang+Y%22" class="reference__authors_link">Zhang Y</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:De+Stefano+N%22" class="reference__authors_link">De Stefano N</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Brady+JM%22" class="reference__authors_link">Brady JM</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Matthews+PM%22" class="reference__authors_link">Matthews PM</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(2004)</span>


            <a href="https://doi.org/10.1016/j.neuroimage.2004.07.051" class="reference__title">Advances in functional and structural MR image analysis and implementation as FSL</a>




          <div class="reference__origin"><i>NeuroImage</i> <b>23 Suppl 1</b>:S208–S219.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1016/j.neuroimage.2004.07.051" class="doi__link">
            https://doi.org/10.1016/j.neuroimage.2004.07.051
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://www.ncbi.nlm.nih.gov/pubmed/15501092" class="reference__abstract_link">PubMed</a>
          </li>
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Advances+in+functional+and+structural+MR+image+analysis+and+implementation+as+FSL&amp;author=Smith+SM&amp;author=Jenkinson+M&amp;author%5B2%5D=Woolrich+MW&amp;author%5B3%5D=Beckmann+CF&amp;author%5B4%5D=Behrens+TE&amp;author%5B5%5D=Johansen-Berg+H&amp;author%5B6%5D=Bannister+PR&amp;author%5B7%5D=De+Luca+M&amp;author%5B8%5D=Drobnjak+I&amp;author%5B9%5D=Flitney+DE&amp;author%5B10%5D=Niazy+RK&amp;author%5B11%5D=Saunders+J&amp;author%5B12%5D=Vickers+J&amp;author%5B13%5D=Zhang+Y&amp;author%5B14%5D=De+Stefano+N&amp;author%5B15%5D=Brady+JM&amp;author%5B16%5D=Matthews+PM&amp;publication_year=2004&amp;journal=NeuroImage&amp;volume=23+Suppl+1&amp;pages=pp.+S208%E2%80%93S219&amp;pmid=15501092" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib66" id="bib66">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Smith+JD%22" class="reference__authors_link">Smith JD</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Redford+JS%22" class="reference__authors_link">Redford JS</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Haas+SM%22" class="reference__authors_link">Haas SM</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(2008)</span>


            <a href="https://doi.org/10.1037/0096-3445.137.2.390" class="reference__title">Prototype abstraction by monkeys (Macaca mulatta)</a>




          <div class="reference__origin"><i>Journal of Experimental Psychology: General</i> <b>137</b>:390–401.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1037/0096-3445.137.2.390" class="doi__link">
            https://doi.org/10.1037/0096-3445.137.2.390
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://www.ncbi.nlm.nih.gov/pubmed/18473665" class="reference__abstract_link">PubMed</a>
          </li>
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Prototype+abstraction+by+monkeys+%28Macaca+mulatta%29&amp;author=Smith+JD&amp;author=Redford+JS&amp;author%5B2%5D=Haas+SM&amp;publication_year=2008&amp;journal=Journal+of+Experimental+Psychology%3A+General&amp;volume=137&amp;pages=pp.+390%E2%80%93401&amp;pmid=18473665" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib67" id="bib67">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Smith+JD%22" class="reference__authors_link">Smith JD</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Minda+JP%22" class="reference__authors_link">Minda JP</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(2000)</span>


            <a href="https://doi.org/10.1037/0278-7393.26.1.3" class="reference__title">Thirty categorization results in search of a model</a>




          <div class="reference__origin"><i>Journal of Experimental Psychology: Learning, Memory, and Cognition</i> <b>26</b>:3–27.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1037/0278-7393.26.1.3" class="doi__link">
            https://doi.org/10.1037/0278-7393.26.1.3
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://www.ncbi.nlm.nih.gov/pubmed/10682288" class="reference__abstract_link">PubMed</a>
          </li>
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Thirty+categorization+results+in+search+of+a+model&amp;author=Smith+JD&amp;author=Minda+JP&amp;publication_year=2000&amp;journal=Journal+of+Experimental+Psychology%3A+Learning%2C+Memory%2C+and+Cognition&amp;volume=26&amp;pages=pp.+3%E2%80%9327&amp;pmid=10682288" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib68" id="bib68">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Smith+JD%22" class="reference__authors_link">Smith JD</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Minda+JP%22" class="reference__authors_link">Minda JP</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(2002)</span>


            <a href="https://doi.org/10.1037/0278-7393.28.4.800" class="reference__title">Distinguishing prototype-based and exemplar-based processes in dot-pattern category learning</a>




          <div class="reference__origin"><i>Journal of Experimental Psychology: Learning, Memory, and Cognition</i> <b>28</b>:800–811.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1037/0278-7393.28.4.800" class="doi__link">
            https://doi.org/10.1037/0278-7393.28.4.800
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Distinguishing+prototype-based+and+exemplar-based+processes+in+dot-pattern+category+learning&amp;author=Smith+JD&amp;author=Minda+JP&amp;publication_year=2002&amp;journal=Journal+of+Experimental+Psychology%3A+Learning%2C+Memory%2C+and+Cognition&amp;volume=28&amp;pages=pp.+800%E2%80%93811" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib69" id="bib69">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Thibaut+JP%22" class="reference__authors_link">Thibaut JP</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Gelaes+S%22" class="reference__authors_link">Gelaes S</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Murphy+GL%22" class="reference__authors_link">Murphy GL</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(2018)</span>


            <a href="https://doi.org/10.3758/s13421-017-0782-4" class="reference__title">Does practice in category learning increase rule use or exemplar use-or both?</a>




          <div class="reference__origin"><i>Memory & Cognition</i> <b>46</b>:530–543.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.3758/s13421-017-0782-4" class="doi__link">
            https://doi.org/10.3758/s13421-017-0782-4
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://www.ncbi.nlm.nih.gov/pubmed/29313292" class="reference__abstract_link">PubMed</a>
          </li>
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Does+practice+in+category+learning+increase+rule+use+or+exemplar+use-or+both%3F&amp;author=Thibaut+JP&amp;author=Gelaes+S&amp;author%5B2%5D=Murphy+GL&amp;publication_year=2018&amp;journal=Memory+%26+Cognition&amp;volume=46&amp;pages=pp.+530%E2%80%93543&amp;pmid=29313292" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib70" id="bib70">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Tse+D%22" class="reference__authors_link">Tse D</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Langston+RF%22" class="reference__authors_link">Langston RF</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Kakeyama+M%22" class="reference__authors_link">Kakeyama M</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Bethus+I%22" class="reference__authors_link">Bethus I</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Spooner+PA%22" class="reference__authors_link">Spooner PA</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Wood+ER%22" class="reference__authors_link">Wood ER</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Witter+MP%22" class="reference__authors_link">Witter MP</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Morris+RG%22" class="reference__authors_link">Morris RG</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(2007)</span>


            <a href="https://doi.org/10.1126/science.1135935" class="reference__title">Schemas and memory consolidation</a>




          <div class="reference__origin"><i>Science</i> <b>316</b>:76–82.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1126/science.1135935" class="doi__link">
            https://doi.org/10.1126/science.1135935
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://www.ncbi.nlm.nih.gov/pubmed/17412951" class="reference__abstract_link">PubMed</a>
          </li>
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Schemas+and+memory+consolidation&amp;author=Tse+D&amp;author=Langston+RF&amp;author%5B2%5D=Kakeyama+M&amp;author%5B3%5D=Bethus+I&amp;author%5B4%5D=Spooner+PA&amp;author%5B5%5D=Wood+ER&amp;author%5B6%5D=Witter+MP&amp;author%5B7%5D=Morris+RG&amp;publication_year=2007&amp;journal=Science&amp;volume=316&amp;pages=pp.+76%E2%80%9382&amp;pmid=17412951" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib71" id="bib71">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:van+Kesteren+MT%22" class="reference__authors_link">van Kesteren MT</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Ruiter+DJ%22" class="reference__authors_link">Ruiter DJ</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Fern%C3%A1ndez+G%22" class="reference__authors_link">Fernández G</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Henson+RN%22" class="reference__authors_link">Henson RN</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(2012)</span>


            <a href="https://doi.org/10.1016/j.tins.2012.02.001" class="reference__title">How schema and novelty augment memory formation</a>




          <div class="reference__origin"><i>Trends in Neurosciences</i> <b>35</b>:211–219.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1016/j.tins.2012.02.001" class="doi__link">
            https://doi.org/10.1016/j.tins.2012.02.001
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://www.ncbi.nlm.nih.gov/pubmed/22398180" class="reference__abstract_link">PubMed</a>
          </li>
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=How+schema+and+novelty+augment+memory+formation&amp;author=van+Kesteren+MT&amp;author=Ruiter+DJ&amp;author%5B2%5D=Fern%C3%A1ndez+G&amp;author%5B3%5D=Henson+RN&amp;publication_year=2012&amp;journal=Trends+in+Neurosciences&amp;volume=35&amp;pages=pp.+211%E2%80%93219&amp;pmid=22398180" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib72" id="bib72">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Vilberg+KL%22" class="reference__authors_link">Vilberg KL</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Rugg+MD%22" class="reference__authors_link">Rugg MD</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(2008)</span>


            <a href="https://doi.org/10.1016/j.neuropsychologia.2008.01.004" class="reference__title">Memory retrieval and the parietal cortex: a review of evidence from a dual-process perspective</a>




          <div class="reference__origin"><i>Neuropsychologia</i> <b>46</b>:1787–1799.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1016/j.neuropsychologia.2008.01.004" class="doi__link">
            https://doi.org/10.1016/j.neuropsychologia.2008.01.004
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Memory+retrieval+and+the+parietal+cortex%3A+a+review+of+evidence+from+a+dual-process+perspective&amp;author=Vilberg+KL&amp;author=Rugg+MD&amp;publication_year=2008&amp;journal=Neuropsychologia&amp;volume=46&amp;pages=pp.+1787%E2%80%931799" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib73" id="bib73">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Xiao+X%22" class="reference__authors_link">Xiao X</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Dong+Q%22" class="reference__authors_link">Dong Q</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Gao+J%22" class="reference__authors_link">Gao J</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Men+W%22" class="reference__authors_link">Men W</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Poldrack+RA%22" class="reference__authors_link">Poldrack RA</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Xue+G%22" class="reference__authors_link">Xue G</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(2017)</span>


            <a href="https://doi.org/10.1523/JNEUROSCI.2324-16.2017" class="reference__title">Transformed neural pattern reinstatement during episodic memory retrieval</a>




          <div class="reference__origin"><i>The Journal of Neuroscience</i> <b>37</b>:2986–2998.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1523/JNEUROSCI.2324-16.2017" class="doi__link">
            https://doi.org/10.1523/JNEUROSCI.2324-16.2017
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Transformed+neural+pattern+reinstatement+during+episodic+memory+retrieval&amp;author=Xiao+X&amp;author=Dong+Q&amp;author%5B2%5D=Gao+J&amp;author%5B3%5D=Men+W&amp;author%5B4%5D=Poldrack+RA&amp;author%5B5%5D=Xue+G&amp;publication_year=2017&amp;journal=The+Journal+of+Neuroscience&amp;volume=37&amp;pages=pp.+2986%E2%80%932998" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib74" id="bib74">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Zaki+SR%22" class="reference__authors_link">Zaki SR</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Nosofsky+RM%22" class="reference__authors_link">Nosofsky RM</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Stanton+RD%22" class="reference__authors_link">Stanton RD</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Cohen+AL%22" class="reference__authors_link">Cohen AL</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(2003)</span>


            <a href="https://doi.org/10.1037/0278-7393.29.6.1160" class="reference__title">Prototype and exemplar accounts of category learning and attentional allocation: a reassessment</a>




          <div class="reference__origin"><i>Journal of Experimental Psychology: Learning, Memory, and Cognition</i> <b>29</b>:1160–1173.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1037/0278-7393.29.6.1160" class="doi__link">
            https://doi.org/10.1037/0278-7393.29.6.1160
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Prototype+and+exemplar+accounts+of+category+learning+and+attentional+allocation%3A+a+reassessment&amp;author=Zaki+SR&amp;author=Nosofsky+RM&amp;author%5B2%5D=Stanton+RD&amp;author%5B3%5D=Cohen+AL&amp;publication_year=2003&amp;journal=Journal+of+Experimental+Psychology%3A+Learning%2C+Memory%2C+and+Cognition&amp;volume=29&amp;pages=pp.+1160%E2%80%931173" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib75" id="bib75">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Zeithamova+D%22" class="reference__authors_link">Zeithamova D</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Maddox+WT%22" class="reference__authors_link">Maddox WT</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Schnyer+DM%22" class="reference__authors_link">Schnyer DM</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(2008)</span>


            <a href="https://doi.org/10.1523/JNEUROSCI.2915-08.2008" class="reference__title">Dissociable prototype learning systems: evidence from brain imaging and behavior</a>




          <div class="reference__origin"><i>Journal of Neuroscience</i> <b>28</b>:13194–13201.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1523/JNEUROSCI.2915-08.2008" class="doi__link">
            https://doi.org/10.1523/JNEUROSCI.2915-08.2008
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://www.ncbi.nlm.nih.gov/pubmed/19052210" class="reference__abstract_link">PubMed</a>
          </li>
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Dissociable+prototype+learning+systems%3A+evidence+from+brain+imaging+and+behavior&amp;author=Zeithamova+D&amp;author=Maddox+WT&amp;author%5B2%5D=Schnyer+DM&amp;publication_year=2008&amp;journal=Journal+of+Neuroscience&amp;volume=28&amp;pages=pp.+13194%E2%80%9313201&amp;pmid=19052210" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib76" id="bib76">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Zeithamova+D%22" class="reference__authors_link">Zeithamova D</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Dominick+AL%22" class="reference__authors_link">Dominick AL</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Preston+AR%22" class="reference__authors_link">Preston AR</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(2012)</span>


            <a href="https://doi.org/10.1016/j.neuron.2012.05.010" class="reference__title">Hippocampal and ventral medial prefrontal activation during retrieval-mediated learning supports novel inference</a>




          <div class="reference__origin"><i>Neuron</i> <b>75</b>:168–179.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1016/j.neuron.2012.05.010" class="doi__link">
            https://doi.org/10.1016/j.neuron.2012.05.010
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://www.ncbi.nlm.nih.gov/pubmed/22794270" class="reference__abstract_link">PubMed</a>
          </li>
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Hippocampal+and+ventral+medial+prefrontal+activation+during+retrieval-mediated+learning+supports+novel+inference&amp;author=Zeithamova+D&amp;author=Dominick+AL&amp;author%5B2%5D=Preston+AR&amp;publication_year=2012&amp;journal=Neuron&amp;volume=75&amp;pages=pp.+168%E2%80%93179&amp;pmid=22794270" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
    <li class="reference-list__item">
      <div class="reference" data-popup-label="See in references" data-popup-contents="bib77" id="bib77">
          <ol class="reference__authors_list">
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Zeithamova+D%22" class="reference__authors_link">Zeithamova D</a></li>
              <li class="reference__author">
                <a href="https://scholar.google.com/scholar?q=%22author:Bowman+CR%22" class="reference__authors_link">Bowman CR</a></li>
          </ol>
          <span class="reference__authors_list_suffix">(2020)</span>


            <a href="https://doi.org/10.1016/j.nlm.2020.107317" class="reference__title">Generalization and the Hippocampus: more than one story?</a>




          <div class="reference__origin"><i>Neurobiology of Learning and Memory</i> <b>175</b>:107317.</div>

        <div class="doi__reference-spacing"></div>

          <span class="doi">
              <a href="https://doi.org/10.1016/j.nlm.2020.107317" class="doi__link">
            https://doi.org/10.1016/j.nlm.2020.107317
              </a>
          </span>


          <ul class="reference__abstracts">
          <li class="reference__abstract">
            <a href="https://www.ncbi.nlm.nih.gov/pubmed/33007461" class="reference__abstract_link">PubMed</a>
          </li>
          <li class="reference__abstract">
            <a href="https://scholar.google.com/scholar_lookup?title=Generalization+and+the+Hippocampus%3A+more+than+one+story%3F&amp;author=Zeithamova+D&amp;author=Bowman+CR&amp;publication_year=2020&amp;journal=Neurobiology+of+Learning+and+Memory&amp;volume=175&amp;pages=107317&amp;pmid=33007461" class="reference__abstract_link">Google Scholar</a>
          </li>

          </ul>
      </div>
    </li>
</ol>







  </div>

</section>



                    <section
    class="article-section "
   id="info"
  data-behaviour="ArticleSection"
  data-initial-state="closed"
>

    <header class="article-section__header">
      <h2 class="article-section__header_text">Article and author information</h2>

    </header>

  <div class="article-section__body">
      <h3 class="authors-details__heading">Author details</h3>
<ol class="authors-details__authors">
    <li class="authors-details__author"><div class="author-details" data-popup-contents="xe59294b7" id="xe59294b7">

  <h4 class="author-details__name">Caitlin R Bowman</h4>

    <section class="author-details__section">
        <ol class="author-details__list list list--bullet">
            <li class="author-details__text">Department of Psychology, University of Oregon, Eugene, United States</li>
            <li class="author-details__text">Department of Psychology, University of Wisconsin-Milwaukee, Milwaukee, United States</li>
        </ol>
    </section>
    <section class="author-details__section">
        <h5 class="author-details__heading">Contribution</h5>
        <span class="author-details__text">Conceptualization, Resources, Data curation, Software, Formal analysis, Supervision, Validation, Investigation, Visualization, Methodology, Writing - original draft, Project administration</span>
    </section>
    <section class="author-details__section">
        <h5 class="author-details__heading">For correspondence</h5>
        <span class="author-details__text"><a href="mailto:cbowman@uoregon.edu">cbowman@uoregon.edu</a></span>
    </section>
    <section class="author-details__section">
        <h5 class="author-details__heading">Competing interests</h5>
        <span class="author-details__text">No competing interests declared</span>
    </section>

  <section class="author-details__section">
    <span class="orcid">
      <a href="https://orcid.org/0000-0002-5833-3591">
        <picture>
          <source srcset="/assets/patterns/img/icons/orcid.b96370b9.svg" type="image/svg+xml">
          <img src="/assets/patterns/img/icons/orcid.e0a7f9de.png" class="orcid__icon"
               alt="ORCID icon">
        </picture> <span class="visuallyhidden">"This ORCID iD identifies the author of this article:"</span>
         0000-0002-5833-3591</a>
    </span>
  </section>


</div>

</li>
    <li class="authors-details__author"><div class="author-details" data-popup-contents="x82a302d4" id="x82a302d4">

  <h4 class="author-details__name">Takako Iwashita</h4>

    <section class="author-details__section">
        <span class="author-details__text">Department of Psychology, University of Oregon, Eugene, United States</span>
    </section>
    <section class="author-details__section">
        <h5 class="author-details__heading">Contribution</h5>
        <span class="author-details__text">Resources, Investigation, Project administration, Writing - review and editing</span>
    </section>
    <section class="author-details__section">
        <h5 class="author-details__heading">Competing interests</h5>
        <span class="author-details__text">No competing interests declared</span>
    </section>



</div>

</li>
    <li class="authors-details__author"><div class="author-details" data-popup-contents="x9ea0ce01" id="x9ea0ce01">

  <h4 class="author-details__name">Dagmar Zeithamova</h4>

    <section class="author-details__section">
        <span class="author-details__text">Department of Psychology, University of Oregon, Eugene, United States</span>
    </section>
    <section class="author-details__section">
        <h5 class="author-details__heading">Contribution</h5>
        <span class="author-details__text">Conceptualization, Resources, Supervision, Funding acquisition, Methodology, Writing - review and editing</span>
    </section>
    <section class="author-details__section">
        <h5 class="author-details__heading">For correspondence</h5>
        <span class="author-details__text"><a href="mailto:dasa@uoregon.edu">dasa@uoregon.edu</a></span>
    </section>
    <section class="author-details__section">
        <h5 class="author-details__heading">Competing interests</h5>
        <span class="author-details__text">No competing interests declared</span>
    </section>



</div>

</li>
</ol>
<section
    class="article-section "



>

    <header class="article-section__header">
      <h3 class="article-section__header_text">Funding</h3>

    </header>

  <div class="article-section__body">
      <section
    class="article-section "



>

    <header class="article-section__header">
      <h4 class="article-section__header_text">National Institute on Aging (F32-AG-054204)</h4>

    </header>

  <div class="article-section__body">


    <ul class="list list--bullet">
            <li>Caitlin R Bowman</li>
    </ul>








  </div>

</section>
<section
    class="article-section "



>

    <header class="article-section__header">
      <h4 class="article-section__header_text">National Institute of Neurological Disorders and Stroke (R01-NS112366)</h4>

    </header>

  <div class="article-section__body">


    <ul class="list list--bullet">
            <li>Dasa Zeithamova</li>
    </ul>








  </div>

</section>
<section
    class="article-section "



>

    <header class="article-section__header">
      <h4 class="article-section__header_text">University of Oregon (Robert and Beverly Lewis Center for Neuroimaging)</h4>

    </header>

  <div class="article-section__body">


    <ul class="list list--bullet">
            <li>Dagmar Zeithamova</li>
    </ul>








  </div>

</section>
<p class="paragraph">The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</p>







  </div>

</section>
<section
    class="article-section "



>

    <header class="article-section__header">
      <h3 class="article-section__header_text">Acknowledgements</h3>

    </header>

  <div class="article-section__body">
      <p class="paragraph">Funding for this work was provided in part by the National Institute of Neurological Disorders and Stroke Grant R01-NS112366 (DZ), National Institute on Aging Grant F32-AG054204 (CRB), and Lewis Family Endowment, which supports the Robert and Beverly Lewis Center for Neuroimaging at the University of Oregon (DZ).</p>







  </div>

</section>
<section
    class="article-section "



>

    <header class="article-section__header">
      <h3 class="article-section__header_text">Ethics</h3>

    </header>

  <div class="article-section__body">
      <p class="paragraph">Human subjects: All participants provided written informed consent, and Research Compliance Services at the University of Oregon approved all experimental procedures (approval code 10162014.010).</p>







  </div>

</section>
<section
    class="article-section "
   id="copyright"


>

    <header class="article-section__header">
      <h3 class="article-section__header_text">Copyright</h3>

    </header>

  <div class="article-section__body">
      <p>© 2020, Bowman et al.</p><p>This article is distributed under the terms of the <a href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</a>, which permits unrestricted use and redistribution provided that the original author and source are credited.</p>






  </div>

</section>







  </div>

</section>



                    <section
    class="article-section "
   id="metrics"
  data-behaviour="ArticleSection"
  data-initial-state="closed"
>

    <header class="article-section__header">
      <h2 class="article-section__header_text">Metrics</h2>

    </header>

  <div class="article-section__body">
      <ul class="statistic-collection clearfix" data-behaviour="StatisticCollection">
    <li class="statistic-collection__item">
      <dl class="statistic">
        <dd class="statistic__value">
          6,589
        </dd>
        <dt class="statistic__label">
                views
        </dt>
      </dl>
    </li>
    <li class="statistic-collection__item">
      <dl class="statistic">
        <dd class="statistic__value">
          606
        </dd>
        <dt class="statistic__label">
                downloads
        </dt>
      </dl>
    </li>
    <li class="statistic-collection__item">
      <dl class="statistic">
        <dd class="statistic__value">
          48
        </dd>
        <dt class="statistic__label">
                citations
        </dt>
      </dl>
    </li>
</ul>
<p class="paragraph">Views, downloads and citations are aggregated across all versions of this paper published by eLife.</p>
<h3 class="list-heading">Citations by DOI</h3>
<ul class="statistic-collection clearfix" data-behaviour="StatisticCollection">
    <li class="statistic-collection__item">
      <dl class="statistic">
        <dd class="statistic__value">
          48
        </dd>
        <dt class="statistic__label">
                citations for umbrella DOI <a href="https://doi.org/10.7554/eLife.59360">https://doi.org/10.7554/eLife.59360</a>
        </dt>
      </dl>
    </li>
</ul>
<div class='altmetric-container-with-details
            '>
    <script src='https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js'></script>
    <div class='altmetric-embed'
    data-hide-less-than="1" 
    data-badge-type='medium-donut' 
     data-badge-details='right' 
    data-doi=10.7554/eLife.59360></div>
</div><div
    data-behaviour="Metrics"
    data-id="59360"
    data-type="article"
    data-container-id="page-views"
    data-metric="page-views"
    data-period="month"
    data-api-endpoint="https://api.elifesciences.org"
    data-chevron-left-svg="/assets/patterns/img/patterns/molecules/chevron-left-ic.5a64c74f.svg"
    data-chevron-left-srcset="/assets/patterns/img/patterns/molecules/chevron-left-ic_2x.6b3ada52.png 48w, /assets/patterns/img/patterns/molecules/chevron-left-ic_1x.1e8d2a3f.png 24w"
    data-chevron-left-src="/assets/patterns/img/patterns/molecules/chevron-left-ic_1x.1e8d2a3f.png"
    data-chevron-right-svg="/assets/patterns/img/patterns/molecules/chevron-right-ic.b299caa7.svg"
    data-chevron-right-srcset="/assets/patterns/img/patterns/molecules/chevron-right-ic_2x.f14c9491.png 48w, /assets/patterns/img/patterns/molecules/chevron-right-ic_1x.d24da5fc.png 24w"
    data-chevron-right-src="/assets/patterns/img/patterns/molecules/chevron-right-ic_1x.d24da5fc.png"
    aria-hidden="true"
>
</div>
<div
    data-behaviour="Metrics"
    data-id="59360"
    data-type="article"
    data-container-id="downloads"
    data-metric="downloads"
    data-period="month"
    data-api-endpoint="https://api.elifesciences.org"
    data-chevron-left-svg="/assets/patterns/img/patterns/molecules/chevron-left-ic.5a64c74f.svg"
    data-chevron-left-srcset="/assets/patterns/img/patterns/molecules/chevron-left-ic_2x.6b3ada52.png 48w, /assets/patterns/img/patterns/molecules/chevron-left-ic_1x.1e8d2a3f.png 24w"
    data-chevron-left-src="/assets/patterns/img/patterns/molecules/chevron-left-ic_1x.1e8d2a3f.png"
    data-chevron-right-svg="/assets/patterns/img/patterns/molecules/chevron-right-ic.b299caa7.svg"
    data-chevron-right-srcset="/assets/patterns/img/patterns/molecules/chevron-right-ic_2x.f14c9491.png 48w, /assets/patterns/img/patterns/molecules/chevron-right-ic_1x.d24da5fc.png 24w"
    data-chevron-right-src="/assets/patterns/img/patterns/molecules/chevron-right-ic_1x.d24da5fc.png"
    aria-hidden="true"
>
</div>







  </div>

</section>



                    <section
    class="article-section "



>

    <header class="article-section__header">
      <h2 class="article-section__header_text">Download links</h2>

    </header>

  <div class="article-section__body">
      <div data-behaviour="ArticleDownloadLinksList" id="downloads" aria-labelledby="downloads-label">
  <div class="visuallyhidden"><span id="downloads-label">A two-part list of links to download the article, or parts of the article, in various formats.</span></div>

    <div class="article-download-links-list__group">
      <h3 class="article-download-links-list__heading" id="downloads">Downloads<span class="visuallyhidden"> (link to download the article as PDF)</span></h3>


      <ul class="article-download-list">
            <li class="article-download-links-list__item">


                  <a href="https://elifesciences.org/download/aHR0cHM6Ly9jZG4uZWxpZmVzY2llbmNlcy5vcmcvYXJ0aWNsZXMvNTkzNjAvZWxpZmUtNTkzNjAtdjIucGRmP2Nhbm9uaWNhbFVyaT1odHRwczovL2VsaWZlc2NpZW5jZXMub3JnL2FydGljbGVzLzU5MzYw/elife-59360-v2.pdf?_hash=94ApUCuaXToU5ZPDLIlVvq4kX%2FTFO6RUjjIe0w73BoY%3D"
                  class="article-download-links-list__link"

                  >Article PDF</a>


            </li>
      </ul>
    </div>
    <div class="article-download-links-list__group">
      <h3 class="article-download-links-list__heading" id="downloads">Open citations<span class="visuallyhidden"> (links to open the citations from this article in various online reference manager services)</span></h3>


      <ul class="article-download-list">
            <li class="article-download-links-list__item">


                  <a href="https://www.mendeley.com/import?doi=10.7554/eLife.59360"
                  class="article-download-links-list__link"

                  >Mendeley</a>


            </li>
            <li class="article-download-links-list__item">


                  <span data-behaviour="CheckPMC" data-check-available="https://ncbi.nlm.nih.gov/pmc/utils/idconv/v1.0/?ids=10.7554/eLife.59360&amp;format=json" data-display-on-available="<a href='' class='article-download-links-list__link'></a>"></span>


            </li>
      </ul>
    </div>
    <div class="article-download-links-list__group article-download-links-list__group--js-exclude">
      <h3 class="article-download-links-list__heading" id="cite-this-article">Cite this article<span class="visuallyhidden"> (links to download the citations from this article in formats compatible with various reference manager tools)</span></h3>

        <div class="article-download-links-list__intro"><div class="reference">
    <ol class="reference__authors_list">
        <li class="reference__author">
          Caitlin R Bowman</li>
        <li class="reference__author">
          Takako Iwashita</li>
        <li class="reference__author">
          Dagmar Zeithamova</li>
    </ol>
    <span class="reference__authors_list_suffix">(2020)</span>



      <div class="reference__title">Tracking prototype and exemplar representations in the brain across learning</div>



    <div class="reference__origin"><i>eLife</i> <b>9</b>:e59360.</div>

  <div class="doi__reference-spacing"></div>

    <span class="doi">
      https://doi.org/10.7554/eLife.59360
    </span>



</div>
</div>

      <ul class="article-download-list">
            <li class="article-download-links-list__item">


                  <a href="/articles/59360.bib"
                  class="article-download-links-list__link"

                  >Download BibTeX</a>


            </li>
            <li class="article-download-links-list__item">


                  <a href="/articles/59360.ris"
                  class="article-download-links-list__link"

                  >Download .RIS</a>


            </li>
      </ul>
    </div>

</div>







  </div>

</section>



                    <section
    class="article-section  article-section__sharers "
   id="share"


>

    <header class="article-section__header">
      <h3 class="article-section__header_text">Share this article</h3>

    </header>

  <div class="article-section__body">
      <span class="doi">
    <a href="https://doi.org/10.7554/eLife.59360" class="doi__link">
  https://doi.org/10.7554/eLife.59360
    </a>
</span>

<ul class="social-media-sharers">
  <li>
    <a class="social-media-sharer email" href="mailto:?subject=Tracking%20prototype%20and%20exemplar%20representations%20in%20the%20brain%20across%20learning&amp;body=https%3A%2F%2Fdoi.org%2F10.7554%2FeLife.59360" target="_blank" rel="noopener noreferrer" aria-label="Share by Email">
      <svg width="24px" height="24px" viewBox="0 0 24 24" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
          <g id="email" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
              <rect id="clear-button-bg" x="0" y="0" width="24" height="24"></rect>
              <path d="M20,4 L4,4 C2.9,4 2.01,4.9 2.01,6 L2,18 C2,19.1 2.9,20 4,20 L20,20 C21.1,20 22,19.1 22,18 L22,6 C22,4.9 21.1,4 20,4 Z M20,8 L12,13 L4,8 L4,6 L12,11 L20,6 L20,8 Z" id="Shape" fill="#000000" fill-rule="nonzero"></path>
          </g>
      </svg>
    </a>
  </li>
  <li>
    <a class="social-media-sharer" href="https://twitter.com/intent/tweet/?text=Tracking%20prototype%20and%20exemplar%20representations%20in%20the%20brain%20across%20learning&amp;url=https%3A%2F%2Fdoi.org%2F10.7554%2FeLife.59360" target="_blank" rel="noopener noreferrer" aria-label="Tweet a link to this page">
      <svg width="24px" height="24px" viewBox="0 0 24 24" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
        <g id="x-24-articles" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
            <rect id="Rectangle" x="0" y="0" width="24" height="24"></rect>
            <path d="M13.6604555,11.4785798 L20.954365,3 L19.225942,3 L12.8926412,10.3618317 L7.83425247,3 L2,3 L9.64927632,14.1323934 L2,23.023486 L3.72852106,23.023486 L10.4166498,15.2491415 L15.7586789,23.023486 L21.5929313,23.023486 L13.660031,11.4785798 L13.6604555,11.4785798 Z M11.293009,14.2304723 L10.5179779,13.1219369 L4.35133136,4.30120576 L7.00623887,4.30120576 L11.9827944,11.4198173 L12.7578255,12.5283527 L19.2267583,21.7814574 L16.5718508,21.7814574 L11.293009,14.2308968 L11.293009,14.2304723 Z" id="Shape" fill="#212121" fill-rule="nonzero"></path>
        </g>
      </svg>
    </a>
  </li>
  <li>
    <a class="social-media-sharer" href="https://facebook.com/sharer/sharer.php?u=https%3A%2F%2Fdoi.org%2F10.7554%2FeLife.59360" target="_blank" rel="noopener noreferrer" aria-label="Share on Facebook">
      <svg width="24px" height="24px" viewBox="0 0 24 24" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
          <g id="facebook" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
              <g>
                  <rect id="clear-button-bg" x="0" y="0" width="24" height="24"></rect>
                  <path d="M11.8969072,2 C6.43099227,2 2,6.41067993 2,11.8515383 C2,16.7687258 5.61915593,20.844338 10.3505155,21.5833958 L10.3505155,14.6992486 L7.83762887,14.6992486 L7.83762887,11.8515383 L10.3505155,11.8515383 L10.3505155,9.68112126 C10.3505155,7.21207948 11.8280541,5.84825714 14.0887242,5.84825714 C15.1715271,5.84825714 16.3041237,6.04067 16.3041237,6.04067 L16.3041237,8.465072 L15.0561469,8.465072 C13.8267075,8.465072 13.443299,9.22446783 13.443299,10.0035475 L13.443299,11.8515383 L16.1881443,11.8515383 L15.7493557,14.6992486 L13.443299,14.6992486 L13.443299,21.5833958 C18.1746585,20.844338 21.7938144,16.7687258 21.7938144,11.8515383 C21.7938144,6.41067993 17.3628222,2 11.8969072,2 Z" id="Fill-1" fill="#212121" fill-rule="nonzero"></path>
              </g>
          </g>
      </svg>
    </a>
  </li>
  <li>
    <a class="social-media-sharer" href="https://www.linkedin.com/shareArticle?title=Tracking%20prototype%20and%20exemplar%20representations%20in%20the%20brain%20across%20learning&amp;url=https%3A%2F%2Fdoi.org%2F10.7554%2FeLife.59360" target="_self" aria-label="Share this page to LinkedIn (opens up email program, if configured on this system)">
      <svg width="24px" height="24px" viewBox="0 0 24 24" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
        <g id="linkedin" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
            <g>
                <rect id="Rectangle" x="0" y="0" width="24" height="24"></rect>
                <path d="M18.9751954,18.9751435 L16.0232549,18.9751435 L16.0232549,14.3522816 C16.0232549,13.2499163 16.0035753,11.8308324 14.4879383,11.8308324 C12.9504693,11.8308324 12.7152366,13.0319032 12.7152366,14.2720257 L12.7152366,18.974836 L9.76329605,18.974836 L9.76329605,9.46835755 L12.5971589,9.46835755 L12.5971589,10.7675189 L12.6368256,10.7675189 C13.2146663,9.77952684 14.2890899,9.18943873 15.4328668,9.23189481 C18.4247815,9.23189481 18.9764254,11.1998552 18.9764254,13.7600486 L18.9751954,18.9751435 Z M6.43252317,8.16888879 C5.48643292,8.16905854 4.71933758,7.40223852 4.7191677,6.45614827 C4.71899794,5.51005802 5.48581795,4.74296266 6.4319082,4.74279278 C7.37799845,4.74262301 8.14509382,5.50944301 8.14526371,6.45553326 C8.14534522,6.90986184 7.96494211,7.34561469 7.64374096,7.66693118 C7.32253981,7.98824766 6.88685175,8.16880719 6.43252317,8.16888879 M7.90849343,18.9751435 L4.95347798,18.9751435 L4.95347798,9.46835755 L7.90849343,9.46835755 L7.90849343,18.9751435 Z M20.4468607,2.00148552 L3.47012787,2.00148552 C2.66777241,1.99243094 2.00979231,2.63513479 2,3.43748159 L2,20.4846305 C2.00945705,21.2873639 2.6673842,21.9307041 3.47012787,21.9223954 L20.4468607,21.9223954 C21.2511925,21.9322515 21.9116962,21.2889485 21.922831,20.4846305 L21.922831,3.43625161 C21.9113617,2.63231978 21.2508045,1.98965267 20.4468607,2" id="Path_2520" fill="#212121" fill-rule="nonzero"></path>
            </g>
        </g>
    </svg>
    </a>
  </li>
  <li>
    <a class="social-media-sharer" href="https://reddit.com/submit/?title=Tracking%20prototype%20and%20exemplar%20representations%20in%20the%20brain%20across%20learning&amp;url=https%3A%2F%2Fdoi.org%2F10.7554%2FeLife.59360" target="_blank" rel="noopener noreferrer" aria-label="Share this page on Reddit">
      <svg width="24px" height="24px" viewBox="0 0 24 24" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
          <g id="reddit" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
              <g>
                  <rect id="Rectangle" x="0" y="0" width="24" height="24"></rect>
                  <path d="M11.9872095,1.051163 C18.0270212,1.051163 22.923256,5.94739779 22.923256,11.9872095 C22.923256,18.0270212 18.0270212,22.923256 11.9872095,22.923256 C5.94739779,22.923256 1.051163,18.0270212 1.051163,11.9872095 C1.051163,5.94739779 5.94739779,1.051163 11.9872095,1.051163 Z M16.5534886,5.37441881 C16.1058142,5.37441881 15.7220932,5.63023277 15.5430235,6.0139537 L12.895349,5.451163 C12.8186049,5.4383723 12.7418607,5.451163 12.6779072,5.48953509 C12.6139537,5.52790719 12.5755816,5.59186067 12.5500002,5.66860486 L11.7441863,9.48023277 C10.0430235,9.53139556 8.52093044,10.0302328 7.42093044,10.8360467 C7.13953509,10.5674421 6.74302347,10.3883723 6.32093044,10.3883723 C5.4383723,10.3883723 4.72209323,11.1046514 4.72209323,11.9872095 C4.72209323,12.6395351 5.10581416,13.1895351 5.66860486,13.445349 C5.64302347,13.5988374 5.63023277,13.7651165 5.63023277,13.9313956 C5.63023277,16.3872095 8.48255835,18.3697677 12.0127909,18.3697677 C15.5430235,18.3697677 18.395349,16.3872095 18.395349,13.9313956 C18.395349,13.7651165 18.3825583,13.6116281 18.356977,13.4581397 C18.8813956,13.2023258 19.2779072,12.6395351 19.2779072,11.9872095 C19.2779072,11.1046514 18.5616281,10.3883723 17.67907,10.3883723 C17.2441863,10.3883723 16.8604653,10.5546514 16.57907,10.8360467 C15.4918607,10.0558142 13.9825583,9.54418626 12.3197677,9.48023277 L13.0488374,6.06511649 L15.4151165,6.5639537 C15.4406979,7.16511649 15.9395351,7.651163 16.5534886,7.651163 C17.1802328,7.651163 17.6918607,7.13953509 17.6918607,6.51279091 C17.6918607,5.88604672 17.1802328,5.37441881 16.5534886,5.37441881 Z M9.72325602,15.7093025 C10.2093025,16.195349 11.2581397,16.3744188 12.0127909,16.3744188 C12.7674421,16.3744188 13.8034886,16.195349 14.3023258,15.7093025 C14.4174421,15.5941863 14.6093025,15.5941863 14.7244188,15.7093025 C14.8139537,15.8372095 14.8139537,16.0162793 14.6988374,16.1313956 C13.9186049,16.9116281 12.4348839,16.9627909 12.0000002,16.9627909 C11.5651165,16.9627909 10.0686049,16.8988374 9.301163,16.1313956 C9.18604672,16.0162793 9.18604672,15.8244188 9.301163,15.7093025 C9.41627928,15.5941863 9.60813974,15.5941863 9.72325602,15.7093025 Z M9.48023277,11.9872095 C10.1069769,11.9872095 10.6186049,12.4988374 10.6186049,13.1255816 C10.6186049,13.7523258 10.1069769,14.2639537 9.48023277,14.2639537 C8.85348858,14.2639537 8.34186067,13.7523258 8.34186067,13.1255816 C8.34186067,12.4988374 8.85348858,11.9872095 9.48023277,11.9872095 Z M14.4941863,11.9872095 C15.1209304,11.9872095 15.6325583,12.4988374 15.6325583,13.1255816 C15.6325583,13.7523258 15.1209304,14.2639537 14.4941863,14.2639537 C13.8674421,14.2639537 13.3558142,13.7523258 13.3558142,13.1255816 C13.3558142,12.4988374 13.8674421,11.9872095 14.4941863,11.9872095 Z" id="Combined-Shape" fill="#212121" fill-rule="nonzero"></path>
              </g>
          </g>
      </svg>
    </a>
  </li>
  <li>
    <a class="social-media-sharer" href="https://toot.kytta.dev/?text=Tracking%20prototype%20and%20exemplar%20representations%20in%20the%20brain%20across%20learning%20https%3A%2F%2Fdoi.org%2F10.7554%2FeLife.59360" target="_blank" rel="noopener noreferrer" aria-label="Share this page on Mastodon">
      <svg width="24px" height="24px">
          <g id="mastodon" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
              <g>
                  <rect id="Rectangle" x="0" y="0" width="24" height="24"></rect>
                  <path d='M11.5412949,0 C14.6936837,0.0250851513 17.7279692,0.357269448 19.4950021,1.1471931 C19.4950021,1.1471931 22.9995752,2.67324958 22.9995752,7.87958222 L23,8.27840764 C22.9948717,9.32900538 22.9399626,12.2394095 22.5108214,14.3877566 C22.1727192,16.0807457 19.4826471,17.9335557 16.3930961,18.2926354 C14.7820287,18.4797396 13.1958041,18.6517151 11.5043627,18.5762011 C8.73816776,18.4528442 6.55544838,17.9335557 6.55544838,17.9335557 C6.55544838,18.1956567 6.57205459,18.4452152 6.605267,18.6786105 C6.96489093,21.335697 9.312211,21.4948713 11.5357152,21.5690923 C13.7799439,21.6438306 15.7782681,21.0305374 15.7782681,21.0305374 L15.8704657,23.0052819 C15.8704657,23.0052819 14.3007146,23.8257215 11.5043627,23.9766203 C9.96237703,24.0591169 8.04774824,23.9388633 5.8177344,23.3643616 C0.981210876,22.1183796 0.149439349,17.1004442 0.022169409,12.0089343 C-0.0166226796,10.4972307 0.00729025582,9.07177343 0.00729025582,7.87958222 C0.00729025582,2.67324958 3.51199627,1.1471931 3.51199627,1.1471931 C5.27916201,0.357269448 8.31145476,0.0250851513 11.4638436,0 L11.5412949,0 Z M15.8599153,5 C14.461897,5 13.4033065,5.46697151 12.7035195,6.40103745 L12.0229667,7.39238391 L11.3425554,6.40103745 C10.642627,5.46697151 9.58403654,5 8.18615964,5 C6.97793807,5 6.00462882,5.36912752 5.26142334,6.08919046 C4.54070495,6.8092534 4.18190148,7.78253068 4.18190148,9.00730143 L4.18190148,15 L6.91358821,15 L6.91358821,9.1834452 C6.91358821,7.95732233 7.50716259,7.33498047 8.69445278,7.33498047 C10.0071899,7.33498047 10.6652556,8.0732355 10.6652556,9.5330285 L10.6652556,12.7167687 L13.3808194,12.7167687 L13.3808194,9.5330285 C13.3808194,8.0732355 14.0387436,7.33498047 15.3514807,7.33498047 C16.5387709,7.33498047 17.1323453,7.95732233 17.1323453,9.1834452 L17.1323453,15 L19.864032,15 L19.864032,9.00730143 C19.864032,7.78253068 19.5052285,6.8092534 18.7846516,6.08919046 C18.0413047,5.36912752 17.0679954,5 15.8599153,5 Z' id='Combined-Shape' fill='#212121' fill-rule='nonzero'></path>
              </g>
          </g>
      </svg>
    </a>
  </li>
</ul>








  </div>

</section>




<section class="article-meta">

  <div class="article-meta__container">


    <section class="article-meta__group">
      <h4 class="article-meta__group_title">Categories and tags</h4>
      <ul class="article-meta__link_list">
          <li class="article-meta__link_list_item">
            <a href="/articles/research-article" class="article-meta__link">Research Article</a></li>
          <li class="article-meta__link_list_item">
            <a href="/subjects/neuroscience" class="article-meta__link">Neuroscience</a></li>
          <li class="article-meta__link_list_item">
            <a href="/search?for=long%20term%20memory" class="article-meta__link">long term memory</a></li>
          <li class="article-meta__link_list_item">
            <a href="/search?for=fmri" class="article-meta__link">fmri</a></li>
          <li class="article-meta__link_list_item">
            <a href="/search?for=generalization" class="article-meta__link">generalization</a></li>
          <li class="article-meta__link_list_item">
            <a href="/search?for=category%20learning" class="article-meta__link">category learning</a></li>
          <li class="article-meta__link_list_item">
            <a href="/search?for=hippocampus" class="article-meta__link">hippocampus</a></li>
      </ul>
    </section>


    <section class="article-meta__group">
      <h4 class="article-meta__group_title">Research organism</h4>
      <ul class="article-meta__link_list">
          <li class="article-meta__link_list_item">
            <a href="/search?for=Human" class="article-meta__link">Human</a></li>
      </ul>
    </section>


  </div>

</section>







        </div>


    </div>

</div>




            </div>


    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://elifesciences.org/articles/59360"
    },
    "headline": "Tracking prototype and exemplar representations in the brain across learning",
    "datePublished": "2020-11-26",
    "author": [
        {
            "@type": "Person",
            "name": "Caitlin R Bowman"
        },
        {
            "@type": "Person",
            "name": "Takako Iwashita"
        },
        {
            "@type": "Person",
            "name": "Dagmar Zeithamova"
        }
    ],
    "publisher": {
        "@type": "Organization",
        "name": "eLife Sciences Publications, Ltd",
        "logo": {
            "@type": "ImageObject",
            "url": "https://elifesciences.org/assets/patterns/img/patterns/organisms/elife-logo-symbol@2x.ff81306e.png"
        }
    },
    "keywords": [
        "long term memory",
        "fmri",
        "generalization",
        "category learning",
        "hippocampus"
    ],
    "about": [
        "Neuroscience"
    ],
    "description": "Concepts can be represented at multiple levels of specificity (individual examples, abstract category averages) within a single task across different regions of the brain.",
    "isPartOf": {
        "@type": "Periodical",
        "name": "eLife",
        "issn": "2050-084X"
    }
}
</script>


            </main>

                            <section class="email-cta">

  <div class="email-cta__container">

      <header class="email-cta__header">
        <h2 class="email-cta__header_text">Be the first to read new articles from eLife</h2>
      </header>

            <a href="/content-alerts" class="button button--default email-cta__button">Sign up for email alerts</a>

      <div class="email-cta__privacy">
        <a class="email-cta__privacy_link" href="/privacy">Privacy notice</a>
      </div>
  </div>

</section>


                              <div class="main-menu wrapper" id="mainMenu" data-behaviour="MainMenu" tabindex="0">
    <div class="main-menu__title-container">
          <div class="site-header__title clearfix" role="banner">
            <div class="site-header__skip_to_content">
              <a href="#maincontent" class="site-header__skip_to_content__link button button--default">Skip to Content</a>
            </div>
            <a href="/" class="site-header__logo_link">
              <img src="/assets/patterns/img/patterns/organisms/elife-logo-xs.fd623d00.svg" alt="eLife logo" class="site-header__logo_link_image"/>
              <span class="visuallyhidden" >eLife home page</span>
            </a>
          </div>
    </div>
    <nav class="main-menu__container" role="navigation">
        <h3 class="list-heading">Menu</h3>
        <ul class="main-menu__list">
          <li class="main-menu__list_item hidden-wide">
            <a href="/" class="main-menu__list_link">Home</a>
          </li>
          <li class="main-menu__list_item hidden-wide">
            <a href="/browse" class="main-menu__list_link">Browse</a>
          </li>
          <li class="main-menu__list_item hidden-wide">
            <a href="/magazine" class="main-menu__list_link">Magazine</a>
          </li>
          <li class="main-menu__list_item hidden-wide">
            <a href="/community" class="main-menu__list_link">Community</a>
          </li>
          <li class="main-menu__list_item hidden-wide">
            <a href="/about" class="main-menu__list_link">About</a>
          </li>
          <li class="main-menu__list_item">
            <a href="/subjects" class="main-menu__list_link">Research categories</a>
          </li>
          <li class="main-menu__list_item end-of-group">
            <a href="/inside-elife" class="main-menu__list_link">Inside eLife</a>
          </li>
          <li class="main-menu__list_item hidden-wide">
            <a href="/search" class="main-menu__list_link">Search</a>
          </li>
          <li class="main-menu__list_item hidden-wide end-of-group">
            <a href="/content-alerts" class="main-menu__list_link">Subscribe to alerts</a>
          </li>
          <li class="main-menu__list_item hidden-wide">
            <a href="https://elifesciences.org/submit-your-research" class="main-menu__list_link">Submit your research</a>
          </li>
          <li class="main-menu__list_item">
            <a href="https://reviewer.elifesciences.org/author-guide/editorial-process" class="main-menu__list_link">Author guide</a>
          </li>
          <li class="main-menu__list_item">
            <a href="https://reviewer.elifesciences.org/reviewer-guide/review-process" class="main-menu__list_link">Reviewer guide</a>
          </li>
        </ul>
      <a href="#siteHeader" class="to-top-link">Back to top</a>
    </nav>
  </div>

<ol class="investor-logos" role="list" aria-label="eLife is funded by these organisations">
    <li class="investor-logos__item" role="listitem">

      <div class="investor-logos__container">
        <picture class="investor-logos__picture">
            <source srcset="/assets/images/investors/hhmi.9d0951a2.svg"
                    type="image/svg+xml"
              >
            <source srcset="/assets/images/investors/hhmi@2x.39287b22.webp 2x, /assets/images/investors/hhmi@1x.7a33c14e.webp 1x"
                    type="image/webp"
              >
            <source srcset="/assets/images/investors/hhmi@2x.1ad6ab6c.png 2x, /assets/images/investors/hhmi@1x.739f96c8.png 1x"
                    type="image/png"
              >
            <img src="/assets/images/investors/hhmi@1x.739f96c8.png"

                 alt="Howard Hughes Medical Institute"
                 class="investor-logos__img"
            >
        </picture>
      </div>

    </li>
    <li class="investor-logos__item" role="listitem">

      <div class="investor-logos__container">
        <picture class="investor-logos__picture">
            <source srcset="/assets/images/investors/wellcome.813f8634.svg"
                    type="image/svg+xml"
              >
            <source srcset="/assets/images/investors/wellcome@2x.20f8e70c.webp 2x, /assets/images/investors/wellcome@1x.04660921.webp 1x"
                    type="image/webp"
              >
            <source srcset="/assets/images/investors/wellcome@2x.04433270.png 2x, /assets/images/investors/wellcome@1x.b38198be.png 1x"
                    type="image/png"
              >
            <img src="/assets/images/investors/wellcome@1x.b38198be.png"

                 alt="Wellcome Trust"
                 class="investor-logos__img"
            >
        </picture>
      </div>

    </li>
    <li class="investor-logos__item" role="listitem">

      <div class="investor-logos__container">
        <picture class="investor-logos__picture">
            <source srcset="/assets/images/investors/max.a0546fd2.svg"
                    type="image/svg+xml"
              >
            <source srcset="/assets/images/investors/max@2x.8503a434.webp 2x, /assets/images/investors/max@1x.893132ad.webp 1x"
                    type="image/webp"
              >
            <source srcset="/assets/images/investors/max@2x.727ba110.png 2x, /assets/images/investors/max@1x.71f18cc3.png 1x"
                    type="image/png"
              >
            <img src="/assets/images/investors/max@1x.71f18cc3.png"

                 alt="Max-Planck-Gesellschaft"
                 class="investor-logos__img"
            >
        </picture>
      </div>

    </li>
    <li class="investor-logos__item" role="listitem">

      <div class="investor-logos__container">
        <picture class="investor-logos__picture">
            <source srcset="/assets/images/investors/kaw.db745e57.svg"
                    type="image/svg+xml"
              >
            <source srcset="/assets/images/investors/kaw@2x.dfb04de2.webp 2x, /assets/images/investors/kaw@1x.aabae49d.webp 1x"
                    type="image/webp"
              >
            <source srcset="/assets/images/investors/kaw@2x.ee42bf3a.png 2x, /assets/images/investors/kaw@1x.d11d4f95.png 1x"
                    type="image/png"
              >
            <img src="/assets/images/investors/kaw@1x.d11d4f95.png"

                 alt="Knut and Alice Wallenberg Foundation"
                 class="investor-logos__img"
            >
        </picture>
      </div>

    </li>
</ol>

<footer class="site-footer">

  <div class="site-footer__container">

    <div class="grid-cell">

      <nav class="footer-navigation">
        <ul class="footer-navigation__list">
          <li class="footer-navigation__list_item">
            <a href="/about" class="footer-navigation__list_link">About</a>
          </li>
          <li class="footer-navigation__list_item">
            <a href="/jobs" class="footer-navigation__list_link">Jobs</a>
          </li>
          <li class="footer-navigation__list_item">
            <a href="/who-we-work-with" class="footer-navigation__list_link">Who we work with</a>
          </li>
          <li class="footer-navigation__list_item">
            <a href="/alerts" class="footer-navigation__list_link">Alerts</a>
          </li>
          <li class="footer-navigation__list_item">
            <a href="/contact" class="footer-navigation__list_link">Contact</a>
          </li>
          <li class="footer-navigation__list_item">
            <a href="/terms" class="footer-navigation__list_link">Terms and conditions</a>
          </li>
          <li class="footer-navigation__list_item">
            <a href="/privacy" class="footer-navigation__list_link">Privacy notice</a>
          </li>
          <li class="footer-navigation__list_item">
            <a href="/inside-elife" class="footer-navigation__list_link">Inside eLife</a>
          </li>
          <li class="footer-navigation__list_item">
            <a href="/archive/2025" class="footer-navigation__list_link">Monthly archive</a>
          </li>
          <li class="footer-navigation__list_item">
            <a href="/for-the-press" class="footer-navigation__list_link">For the press</a>
          </li>
          <li class="footer-navigation__list_item">
            <a href="/resources" class="footer-navigation__list_link">Resources</a>
          </li>
          <li class="footer-navigation__list_item">
            <a href="http://developers.elifesciences.org" class="footer-navigation__list_link">XML and Data</a>
          </li>
        </ul>
      </nav>

      <div class="social-links" aria-label="Social media links for eLife Sciences">
        <ul class="social-links__list">
            <li class="social-links__list_item">
            <a href="https://twitter.com/elife" class="social-links__list_link" aria-label="Twitter">
              <svg width="28px" height="28px" viewBox="0 0 28 28" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
              <g id="x-28-footer" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
                  <rect id="Rectangle" x="0" y="0" width="28" height="28"></rect>
                  <path d="M16.0929883,12.5202067 L24.2829833,3 L22.3422167,3 L15.230845,11.2662617 L9.55101333,3 L3,3 L11.58902,15.5000517 L3,25.48345 L4.94087667,25.48345 L12.4506683,16.7539967 L18.4489867,25.48345 L25,25.48345 L16.0925117,12.5202067 L16.0929883,12.5202067 Z M13.4346917,15.61018 L12.564445,14.3654567 L5.64020167,4.461064 L8.621275,4.461064 L14.20922,12.454225 L15.0794667,13.6989483 L22.3431333,24.0888333 L19.36206,24.0888333 L13.4346917,15.6106567 L13.4346917,15.61018 Z" id="Shape" fill="#212121" fill-rule="nonzero"></path>
              </g>
          </svg>
            </a>
          </li>
          <li class="social-links__list_item">
            <a href="https://www.facebook.com/elifesciences" class="social-links__list_link" aria-label="Facebook">
              <svg width="28" height="28">
                <path d="M26,14 C26,7.37257812 20.6274219,2 14,2 C7.37257813,2 2,7.37257812 2,14 C2,19.9895469 6.38822656,24.9539844 12.125,25.8542187 L12.125,17.46875 L9.078125,17.46875 L9.078125,14 L12.125,14 L12.125,11.35625 C12.125,8.34875 13.9165156,6.6875 16.6575781,6.6875 C17.9704766,6.6875 19.34375,6.921875 19.34375,6.921875 L19.34375,9.875 L17.8305781,9.875 C16.3398828,9.875 15.875,10.8000078 15.875,11.7489922 L15.875,14 L19.203125,14 L18.6710938,17.46875 L15.875,17.46875 L15.875,25.8542187 C21.6117734,24.9539844 26,19.9895469 26,14" id="Fill-1" fill="#0A0B09"></path>
              </svg>
            </a>
          </li>
          <li class="social-links__list_item">
            <a href="https://www.instagram.com/elifesciences/" class="social-links__list_link" aria-label="Instagram">
              <svg width="28" height="28">
                <path d="M13.9999762,4.16220594 C17.2041298,4.16220594 17.5836306,4.17440025 18.8489821,4.23213276 C20.0189703,4.28553056 20.6543612,4.48102079 21.0772565,4.64531072 C21.6373379,4.86299845 22.0371308,5.12308049 22.4570251,5.54297487 C22.8769195,5.96286924 23.1370016,6.36266203 23.3546416,6.92274348 C23.5189792,7.3456388 23.7144694,7.98102972 23.7678672,9.15097025 C23.8255997,10.4163695 23.8377941,10.7958701 23.8377941,14.0000238 C23.8377941,17.2041775 23.8255997,17.5836782 23.7678672,18.8490297 C23.7144694,20.0190179 23.5189792,20.6544088 23.3546416,21.0773041 C23.1370016,21.6373856 22.8769195,22.0371784 22.4570251,22.4570727 C22.0371308,22.8769671 21.6373379,23.1370492 21.0772565,23.3546892 C20.6543612,23.5190268 20.0189703,23.7145171 18.8489821,23.7679149 C17.5838211,23.8256473 17.2043204,23.8378417 13.9999762,23.8378417 C10.795632,23.8378417 10.4161313,23.8256473 9.15097025,23.7679149 C7.9809821,23.7145171 7.34559117,23.5190268 6.92274348,23.3546892 C6.3626144,23.1370492 5.9628216,22.8769671 5.54292723,22.4570727 C5.12303286,22.0371784 4.86295081,21.6373856 4.64531072,21.0773041 C4.48097317,20.6544088 4.28548292,20.0190179 4.23208512,18.8490774 C4.17435262,17.5836782 4.1621583,17.2041775 4.1621583,14.0000238 C4.1621583,10.7958701 4.17435262,10.4163695 4.23208512,9.15101788 C4.28548292,7.98102972 4.48097317,7.3456388 4.64531072,6.92274348 C4.86295081,6.36266203 5.12303286,5.96286924 5.54292723,5.54297487 C5.9628216,5.12308049 6.3626144,4.86299845 6.92274348,4.64531072 C7.34559117,4.48102079 7.9809821,4.28553056 9.15092262,4.23213276 C10.4163218,4.17440025 10.7958225,4.16220594 13.9999762,4.16220594 M13.9999762,2 C10.740948,2 10.3323429,2.01381388 9.05241535,2.07221326 C7.77510763,2.13051737 6.90283244,2.33334326 6.13949638,2.63000828 C5.35039031,2.93667645 4.68117918,3.34699637 4.01406396,4.01411159 C3.34694874,4.68122682 2.93662882,5.35043793 2.62996064,6.13954402 C2.33329562,6.90288008 2.13046973,7.77515526 2.07216562,9.05246298 C2.01376625,10.3323429 2,10.7409957 2,14.0000238 C2,17.259052 2.01376625,17.6677047 2.07216562,18.9475846 C2.13046973,20.2248924 2.33329562,21.0971676 2.62996064,21.8605036 C2.93662882,22.649562 3.34694874,23.3188208 4.01406396,23.985936 C4.68117918,24.6530513 5.35039031,25.0633712 6.13949638,25.3700394 C6.90283244,25.6667044 7.77510763,25.8695303 9.05241535,25.9278343 C10.3323429,25.9862338 10.740948,26 13.9999762,26 C17.2590043,26 17.667657,25.9862338 18.947537,25.9278343 C20.2248448,25.8695303 21.09712,25.6667044 21.860456,25.3700394 C22.649562,25.0633712 23.3187732,24.6530513 23.9858884,23.985936 C24.6530037,23.3188208 25.0633236,22.6496097 25.3699918,21.8605036 C25.6666568,21.0971676 25.8694827,20.2248924 25.9277867,18.9475846 C25.9861861,17.6677047 26,17.259052 26,14.0000238 C26,10.7409957 25.9861861,10.3323429 25.9277867,9.05246298 C25.8694827,7.77515526 25.6666568,6.90288008 25.3699918,6.13954402 C25.0633236,5.35043793 24.6530037,4.68122682 23.9858884,4.01411159 C23.3187732,3.34699637 22.649562,2.93667645 21.860456,2.63000828 C21.09712,2.33334326 20.2248448,2.13051737 18.947537,2.07221326 C17.667657,2.01381388 17.2590043,2 13.9999762,2 Z M13.9999762,7.8378417 C10.5967121,7.8378417 7.83779407,10.5967597 7.83779407,14.0000238 C7.83779407,17.4032879 10.5967121,20.162206 13.9999762,20.162206 C17.4032403,20.162206 20.1621583,17.4032879 20.1621583,14.0000238 C20.1621583,10.5967597 17.4032403,7.8378417 13.9999762,7.8378417 Z M13.9999762,18.0000477 C11.7908507,18.0000477 9.99995236,16.2091493 9.99995236,14.0000238 C9.99995236,11.7908983 11.7908507,10 13.9999762,10 C16.2091017,10 18,11.7908983 18,14.0000238 C18,16.2091493 16.2091017,18.0000477 13.9999762,18.0000477 Z M21.8455942,7.59438394 C21.8455942,8.38968246 21.2009145,9.03440967 20.405616,9.03440967 C19.6103175,9.03440967 18.9655903,8.38968246 18.9655903,7.59438394 C18.9655903,6.79908542 19.6103175,6.15440585 20.405616,6.15440585 C21.2009145,6.15440585 21.8455942,6.79908542 21.8455942,7.59438394 Z" id="Shape" fill="#212121" fill-rule="nonzero"></path>
              </svg>
            </a>
          </li>
          <li class="social-links__list_item">
            <a href="https://www.youtube.com/channel/UCNEHLtAc_JPI84xW8V4XWyw" class="social-links__list_link" aria-label="YouTube">
              <svg width="28" height="28">
                <path d="M14.3394034,5.00081974 C15.0650046,5.00416663 16.8547629,5.01866982 18.7408249,5.08151696 L19.3085648,5.10185861 L19.3085648,5.10185861 L19.8762758,5.12532402 C21.6678517,5.20481265 23.3716441,5.33464406 24.1578396,5.54670204 C25.2765782,5.84762249 26.157476,6.73428158 26.456476,7.86025886 C26.9532448,9.72604587 26.995825,13.4452634 26.9994747,14.0691357 L26.9994747,14.2486746 C26.995825,14.8725277 26.9532448,18.5916449 26.456476,20.457702 C26.157476,21.5835316 25.2765782,22.4701907 24.1578396,22.7712589 C23.3302654,22.9944175 21.4859633,23.126491 19.5927869,23.2046569 L19.0243355,23.2265266 C16.9417355,23.3010584 14.9171078,23.3148606 14.2386654,23.3174166 L13.7609683,23.3174166 C12.4040836,23.3123047 5.66245741,23.2622078 3.84179414,22.7712589 C2.72320323,22.4701907 1.84215778,21.5835316 1.54315778,20.457702 C1.06191297,18.6499592 1.00691357,15.1031629 1.00062792,14.316078 L1.00062792,14.0017303 C1.00691357,13.2146223 1.06191297,9.66774002 1.54315778,7.86025886 C1.84215778,6.73428158 2.72320323,5.84762249 3.84179414,5.54670204 C5.6210787,5.06678135 12.1001879,5.00801555 13.6602303,5.00081974 Z M11.1612194,10.2011473 L11.1612194,17.7986829 L17.8387126,14.0000603 L11.1612194,10.2011473 Z" id="Combined-Shape" fill="#222321"></path>
              </svg>
            </a>
          </li>
          <li class="social-links__list_item">
            <a href="https://www.linkedin.com/company/elife-sciences-publications-ltd" class="social-links__list_link" aria-label="LinkedIn">
              <svg width="28" height="28">
                <path d="M22.3830154,22.3830154 L18.8384,22.3830154 L18.8384,16.832 C18.8384,15.5083077 18.8147692,13.8043077 16.9948308,13.8043077 C15.1486769,13.8043077 14.8662154,15.2465231 14.8662154,16.7356308 L14.8662154,22.3826462 L11.3216,22.3826462 L11.3216,10.9675077 L14.7244308,10.9675077 L14.7244308,12.5275077 L14.7720615,12.5275077 C15.465918,11.3411519 16.7560586,10.632589 18.1294769,10.6835692 C21.7220923,10.6835692 22.3844923,13.0466462 22.3844923,16.1208615 L22.3830154,22.3830154 Z M7.32209231,9.4071385 C6.1860511,9.40734236 5.26494244,8.48656431 5.26473849,7.3505231 C5.26453461,6.21448189 6.18531266,5.29337322 7.32135387,5.29316926 C8.45739508,5.29296537 9.37850375,6.21374341 9.37870773,7.34978462 C9.37880563,7.89533086 9.16218215,8.41857185 8.77649194,8.80440054 C8.39080174,9.19022923 7.86763855,9.40704054 7.32209231,9.4071385 M9.0944,22.3830154 L5.54609231,22.3830154 L5.54609231,10.9675077 L9.0944,10.9675077 L9.0944,22.3830154 Z M24.1501538,2.00147692 L3.76492308,2.00147692 C2.80147497,1.99060441 2.01138912,2.76234691 1.99963077,3.72578462 L1.99963077,24.1955692 C2.01098655,25.1594712 2.80100881,25.9319778 3.76492308,25.9218636 L24.1501538,25.9218636 C25.115975,25.9338359 25.9090911,25.1613738 25.9224615,24.1955692 L25.9224615,3.72430769 C25.9086895,2.75896671 25.1155091,1.98726833 24.1501538,1.9998447" id="Path_2520" fill="#212121" fill-rule="nonzero"></path>
              </svg>
            </a>
          </li>
          <li class="social-links__list_item">
            <a rel='me' href='https://fediscience.org/@eLife' class='social-links__list_link' aria-label='Mastodon'>
              <svg width="28px" height="28px">
                  <g id="Artboard" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
                      <g id="Group-7" transform="translate(3.000000, 2.000000)" fill="#212121" fill-rule="nonzero">
                          <g id="mastodon-24px-black">
                              <path d="M11.5412949,0 C14.6936837,0.0250851513 17.7279692,0.357269448 19.4950021,1.1471931 C19.4950021,1.1471931 22.9995752,2.67324958 22.9995752,7.87958222 L23,8.27840764 C22.9948717,9.32900538 22.9399626,12.2394095 22.5108214,14.3877566 C22.1727192,16.0807457 19.4826471,17.9335557 16.3930961,18.2926354 C14.7820287,18.4797396 13.1958041,18.6517151 11.5043627,18.5762011 C8.73816776,18.4528442 6.55544838,17.9335557 6.55544838,17.9335557 C6.55544838,18.1956567 6.57205459,18.4452152 6.605267,18.6786105 C6.96489093,21.335697 9.312211,21.4948713 11.5357152,21.5690923 C13.7799439,21.6438306 15.7782681,21.0305374 15.7782681,21.0305374 L15.8704657,23.0052819 C15.8704657,23.0052819 14.3007146,23.8257215 11.5043627,23.9766203 C9.96237703,24.0591169 8.04774824,23.9388633 5.8177344,23.3643616 C0.981210876,22.1183796 0.149439349,17.1004442 0.022169409,12.0089343 C-0.0166226796,10.4972307 0.00729025582,9.07177343 0.00729025582,7.87958222 C0.00729025582,2.67324958 3.51199627,1.1471931 3.51199627,1.1471931 C5.27916201,0.357269448 8.31145476,0.0250851513 11.4638436,0 L11.5412949,0 Z M15.8599153,5 C14.461897,5 13.4033065,5.46697151 12.7035195,6.40103745 L12.0229667,7.39238391 L11.3425554,6.40103745 C10.642627,5.46697151 9.58403654,5 8.18615964,5 C6.97793807,5 6.00462882,5.36912752 5.26142334,6.08919046 C4.54070495,6.8092534 4.18190148,7.78253068 4.18190148,9.00730143 L4.18190148,15 L6.91358821,15 L6.91358821,9.1834452 C6.91358821,7.95732233 7.50716259,7.33498047 8.69445278,7.33498047 C10.0071899,7.33498047 10.6652556,8.0732355 10.6652556,9.5330285 L10.6652556,12.7167687 L13.3808194,12.7167687 L13.3808194,9.5330285 C13.3808194,8.0732355 14.0387436,7.33498047 15.3514807,7.33498047 C16.5387709,7.33498047 17.1323453,7.95732233 17.1323453,9.1834452 L17.1323453,15 L19.864032,15 L19.864032,9.00730143 C19.864032,7.78253068 19.5052285,6.8092534 18.7846516,6.08919046 C18.0413047,5.36912752 17.0679954,5 15.8599153,5 Z" id="Combined-Shape"></path>
                          </g>
                      </g>
                  </g>
              </svg>
            </a>
          </li>
        </ul>
      </div>

      <div class="github-link-wrapper">
        <a href="https://github.com/elifesciences" class="github-link">
          <svg width="20" height="20">
            <g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
              <g id="github-logo-svg" fill="#000000">
                <path d="M9.99907916,1.72084569e-15 C4.47773105,1.72084569e-15 2.22044605e-16,4.5904205 2.22044605e-16,10.2533868 C2.22044605e-16,14.7833822 2.86503576,18.6260419 6.83876117,19.9818304 C7.33908346,20.0762447 7.5214095,19.7596422 7.5214095,19.4877292 C7.5214095,19.2441405 7.512815,18.5996059 7.50790386,17.7442129 C4.72635747,18.3635703 4.13947635,16.3695415 4.13947635,16.3695415 C3.68458209,15.1849574 3.02894503,14.8696139 3.02894503,14.8696139 C2.12099819,14.2338913 3.09770097,14.2464799 3.09770097,14.2464799 C4.10141502,14.3188641 4.62936247,15.30329 4.62936247,15.30329 C5.52134811,16.869937 6.97013414,16.417378 7.53982627,16.1549064 C7.63068234,15.4927479 7.88913104,15.0408184 8.17459099,14.784641 C5.95414224,14.525946 3.6195095,13.6460053 3.6195095,9.7171139 C3.6195095,8.59799041 4.00933116,7.68217225 4.64900703,6.96588286 C4.54587311,6.7065584 4.20270727,5.66359573 4.74722981,4.25241751 C4.74722981,4.25241751 5.5864207,3.97672792 7.4968538,5.30356275 C8.29430001,5.07570971 9.15006599,4.96241262 10.0003069,4.95800662 C10.849934,4.96241262 11.7050861,5.07570971 12.5037601,5.30356275 C14.4129654,3.97672792 15.2509285,4.25241751 15.2509285,4.25241751 C15.7966788,5.66359573 15.453513,6.7065584 15.350993,6.96588286 C15.9918966,7.68217225 16.3786488,8.59799041 16.3786488,9.7171139 C16.3786488,13.6560761 14.0403327,14.5227989 11.8131312,14.7764585 C12.1716443,15.0930609 12.4914822,15.7187126 12.4914822,16.6748142 C12.4914822,18.045709 12.4792044,19.1516145 12.4792044,19.4877292 C12.4792044,19.7621599 12.6596888,20.0812801 13.1667639,19.981201 C17.1374198,18.6222653 20,14.7821233 20,10.2533868 C20,4.5904205 15.5222689,1.72084569e-15 9.99907916,1.72084569e-15" id="Fill-51"></path>
              </g>
            </g>
          </svg>
          <div class="github-link--text">Find us on GitHub</div>
        </a>
      </div>

      <div class="carbon-neutral-wrapper">
        <a href="https://www.carbonfootprint.com/" class="carbon-neutral-link">
          <img src="/assets/patterns/img/patterns/molecules/carbon-neutral-organisation.e2525acf.png" alt="Carbon Neutral Organisation carbon footprint standard [TM]" />
        </a>
      </div>

    </div>

    <div class="grid-cell">

      <div class="site-smallprint">
        <small>eLife is a non-profit organisation inspired by research funders and led by scientists. Our mission is to help scientists accelerate discovery by operating a platform for research communication that encourages and recognises the most responsible behaviours in science.</small>
        <small>eLife Sciences Publications, Ltd is a limited liability non-profit non-stock corporation incorporated in the State of Delaware, USA, with company number 5030732, and is registered in the UK with company number FC030576 and branch number BR015634 at the address:</small>

        <address>
          eLife Sciences Publications, Ltd<br>
          95 Regent Street<br>
          Cambridge CB2 1AW<br>
          UK
        </address>
      </div>

      <div class="site-smallprint">
        <small>© <time>2025</time> eLife Sciences Publications Ltd. Subject to a <a href="https://creativecommons.org/licenses/by/4.0/" rel="license" class="site-smallprint__copyright_link">Creative Commons Attribution license</a>, except where otherwise noted. ISSN:&nbsp;2050-084X</small>
      </div>

    </div>

  </div>

</footer>


        </div>

    </div>
        <script>
            window.elifeConfig = window.elifeConfig || {};

            window.elifeConfig.scriptPaths = [
                '/assets/patterns/js/main.755b79b8.js'
            ];

                        window.elifeConfig.hypothesis = {
              usernameUrl: 'https://elifesciences.org/profiles/',
              services: [{
                apiUrl: 'https://hypothes.is/api/',
                authority: 'elifesciences.org',
                grantToken: null,
                onLoginRequest: function () {
                  window.location.assign('/log-in');
                },
                onSignupRequest: function () {
                  window.location.assign('/log-in');
                }              }]
            };

            window.elifeConfig.domain = 'elifesciences.org';

            (function (window) {
  'use strict';

  try {
    var scriptPaths,
        $body;
    if (
      !!window.localStorage &&
      !!(window.document.createElement('div')).dataset &&
      typeof window.document.querySelector === 'function' &&
      typeof window.addEventListener === 'function'
    ) {
      scriptPaths = window.elifeConfig.scriptPaths;
      if (Array.isArray(scriptPaths) && scriptPaths.length) {
        $body = window.document.querySelector('body');
        scriptPaths.forEach(function (scriptPath) {
          var $script = window.document.createElement('script');
          $script.src = scriptPath;
          $body.appendChild($script);
        });
      }
    }

  } catch (e) {
      window.console.error('JavaScript loading failed with the error: "' + e +
      '". Additionally, RUM logging failed.');
  }

}(window));

        </script>

    <link href="/assets/patterns/css/all.d18120fb.css" rel="stylesheet">


</body>

</html>            </div>
        </div>

    </div>
</body>
</html>
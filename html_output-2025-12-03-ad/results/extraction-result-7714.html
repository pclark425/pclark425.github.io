<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7714 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7714</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7714</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-144.html">extraction-schema-144</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <p><strong>Paper ID:</strong> paper-264490974</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.17526v2.pdf" target="_blank">Can large language models replace humans in systematic reviews? Evaluating GPT‐4's efficacy in screening and extracting data from peer‐reviewed and grey literature in multiple languages</a></p>
                <p><strong>Paper Abstract:</strong> Systematic reviews are vital for guiding practice, research and policy, although they are often slow and labour‐intensive. Large language models (LLMs) could speed up and automate systematic reviews, but their performance in such tasks has yet to be comprehensively evaluated against humans, and no study has tested Generative Pre‐Trained Transformer (GPT)‐4, the biggest LLM so far. This pre‐registered study uses a “human‐out‐of‐the‐loop” approach to evaluate GPT‐4's capability in title/abstract screening, full‐text review and data extraction across various literature types and languages. Although GPT‐4 had accuracy on par with human performance in some tasks, results were skewed by chance agreement and dataset imbalance. Adjusting for these caused performance scores to drop across all stages: for data extraction, performance was moderate, and for screening, it ranged from none in highly balanced literature datasets (~1:1) to moderate in those datasets where the ratio of inclusion to exclusion in studies was imbalanced (~1:3). When screening full‐text literature using highly reliable prompts, GPT‐4's performance was more robust, reaching “human‐like” levels. Although our findings indicate that, currently, substantial caution should be exercised if LLMs are being used to conduct systematic reviews, they also offer preliminary evidence that, for certain review tasks delivered under specific conditions, LLMs can rival human performance.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7714.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7714.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large autoregressive transformer language model from OpenAI; in this paper it was accessed through the ChatGPT interface and used autonomously to perform title/abstract screening, full-text screening, and structured data extraction for a systematic review.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>CAN LARGE LANGUAGE MODELS REPLACE HUMANS IN THE SYSTEMATIC REVIEW PROCESS? EVALUATING GPT-4'S EFFICACY IN SCREENING AND EXTRACTING DATA FROM PEER-REVIEWED AND GREY LITERATURE IN MULTIPLE LANGUAGES</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Qusai Khraisha et al.</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2023</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Human‑out‑of‑the‑loop GPT‑4 screening and extraction pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>The authors used GPT‑4 (via ChatGPT) with engineered prompts to autonomously screen titles/abstracts and segmented full texts against inclusion/exclusion criteria and to extract structured study variables (parent/child characteristics, study design, sampling, instruments). They applied prompt batching, segmentation of long texts, per‑criterion prompting, and test–retest reliability checks of prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Titles/abstracts, segmented full-texts (methods & results sections), grey literature reports/websites</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Binary inclusion/exclusion decisions per criterion and structured extracted fields (participant counts/demographics, design, sampling, measurement instruments)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Prompt engineering with per-criterion prompts, batching (limited number of abstracts per prompt), segmentation of long texts, test–retest reliability evaluation of prompts; no chain‑of‑thought or few‑shot scheme reported</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>Documents from an in‑progress systematic review on parenting in protracted refugee situations: 300 titles/abstracts screened, 150 full texts screened, data extraction from 30 documents (mixed peer‑reviewed, grey, non‑English)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Sensitivity, specificity, accuracy, Cohen's kappa, weighted kappa, PABAK (prevalence‑adjusted bias‑adjusted kappa), dataset imbalance</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Specificity was generally high (>0.80) across stages; sensitivity and adjusted agreement often ranged from none to moderate after accounting for chance and imbalance; for a subset of 'high‑reliability' prompts GPT‑4 achieved almost‑perfect agreement in full‑text screening and very high weighted kappa when penalising false rejections.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Performance strongly dependent on prompt reliability and prompt length/complexity; hallucination risk; segmentation due to token limits; dataset imbalance and chance agreement inflated simple accuracy; small sample size and limited generalisability across topics; difficulty with open or broad concepts (e.g., 'parenting behaviour').</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Can large language models replace humans in systematic reviews? Evaluating GPT‐4's efficacy in screening and extracting data from peer‐reviewed and grey literature in multiple languages", 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7714.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7714.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Trialstreamer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TrialStreamER (living auto-updated clinical trial database)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automated system that builds and maintains a continuously updated database of clinical trial reports and supports extraction of trial information; cited as an example of automation helping data extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Trialstreamer: A living, automatically updated database of clinical trial reports</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Trialstreamer: A living, automatically updated database of clinical trial reports</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>I. J. Marshall et al.</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2020</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>TrialStreamer automated trial ingestion and extraction</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>An automated pipeline that ingests clinical trial reports and extracts structured trial metadata to populate a living database; referenced as an automation tool used for extraction tasks in literature synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Clinical trial reports / full-text articles</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured database entries (trial metadata and extracted fields)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Can large language models replace humans in systematic reviews? Evaluating GPT‐4's efficacy in screening and extracting data from peer‐reviewed and grey literature in multiple languages", 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7714.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7714.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RobotReviewer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RobotReviewer (automated risk-of-bias assessment tool)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automated tool designed to assist with assessing study quality and bias in systematic reviews; cited as an example of AI applied to systematic review tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Nivolumab for adults with hodgkin's lymphoma (a rapid review using the software robotreviewer)</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Nivolumab for adults with hodgkin's lymphoma (a rapid review using the software robotreviewer)</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>M. Goldkuhle et al.</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2018</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>RobotReviewer automated bias and quality assessment</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>An NLP-based system that extracts trial characteristics and predicts risk-of-bias judgments to assist reviewers in quality assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Full-text clinical trial reports / study reports</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Predicted risk-of-bias judgements and extracted methodological statements</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Can large language models replace humans in systematic reviews? Evaluating GPT‐4's efficacy in screening and extracting data from peer‐reviewed and grey literature in multiple languages", 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7714.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7714.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Rayyan</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rayyan (systematic review screening tool)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A widely used semi-automated web app to assist with title/abstract screening, providing organizational and some AI-supported features to speed screening.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Rayyan screening assistance</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Tool that supports human reviewers with tagging, blinding, and some recommendation/automation features for screening titles and abstracts.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Titles and abstracts / citations</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Screening labels, reviewer organization metadata</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Can large language models replace humans in systematic reviews? Evaluating GPT‐4's efficacy in screening and extracting data from peer‐reviewed and grey literature in multiple languages", 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7714.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7714.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Abstracker</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Abstracker (automatic abstract screening tool)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automated tool mentioned alongside Rayyan that assists with screening titles and abstracts in systematic reviews.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Abstracker abstract screening</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Tool that automates or assists in the abstract screening stage of systematic reviews (mentioned as example of prior AI tools).</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Titles and abstracts</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Screening recommendations/labels</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Can large language models replace humans in systematic reviews? Evaluating GPT‐4's efficacy in screening and extracting data from peer‐reviewed and grey literature in multiple languages", 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7714.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7714.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Guo et al. (2023)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automated paper screening for clinical reviews using large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced study that applied large language models for automated paper screening in clinical reviews and reported high specificity but varied sensitivity; used as related work in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automated paper screening for clinical reviews using large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Automated paper screening for clinical reviews using large language models</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>E. Guo et al.</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2023</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LLM-based automated screening</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Applied large language models to automate title/abstract screening for clinical systematic reviews; reported performance comparisons to other AI methods and humans (discussed in the present paper).</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Titles/abstracts (clinical reviews)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Inclusion/exclusion labels</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Specificity, sensitivity (reported in cited work and compared in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Reported high specificity similar to the present study; paper is cited as having a specificity around 90% in similar tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Cited by the present paper as possibly affected by dataset contamination and not accounting for imbalance/chance agreement in some evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Can large language models replace humans in systematic reviews? Evaluating GPT‐4's efficacy in screening and extracting data from peer‐reviewed and grey literature in multiple languages", 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7714.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7714.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Syriani et al. (2023)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Assessing the ability of ChatGPT to screen articles for systematic reviews</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced preprint that evaluated GPT (ChatGPT) for autonomous article screening for systematic reviews and is used as related work in the current study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Assessing the ability of chatgpt to screen articles for systematic reviews</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Assessing the ability of chatgpt to screen articles for systematic reviews</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>E. Syriani et al.</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2023</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>ChatGPT screening evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Evaluation of ChatGPT/GPT variants for automated screening tasks in systematic reviews; compared GPT to older AI methods.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Titles/abstracts</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Screening decisions</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT / earlier GPT versions</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Reported superior performance of GPT relative to some older AI methods (noted in present paper).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Cited present paper notes that some prior studies (including Syriani et al.) may have dataset contamination from materials used in model training.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Can large language models replace humans in systematic reviews? Evaluating GPT‐4's efficacy in screening and extracting data from peer‐reviewed and grey literature in multiple languages", 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7714.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7714.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Alshami et al. (2023)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Harnessing the power of ChatGPT for automating systematic review process: Methodology, case study, limitations, and future directions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced work that applied ChatGPT to automate parts of the systematic review process, though reportedly using a human-in-the-loop approach.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Harnessing the power of chatgpt for automating systematic review process: Methodology, case study, limitations, and future directions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Harnessing the power of chatgpt for automating systematic review process: Methodology, case study, limitations, and future directions</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>A. Alshami et al.</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2023</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>ChatGPT-assisted systematic review automation</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Applied ChatGPT to assist with systematic review tasks (notably screening), but relied on human-in-the-loop validation rather than fully autonomous operation.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Titles/abstracts / systematic review texts</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Screening suggestions and possibly summaries</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Reported use of human-in-the-loop; present paper cites this as a methodological difference from fully autonomous evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Can large language models replace humans in systematic reviews? Evaluating GPT‐4's efficacy in screening and extracting data from peer‐reviewed and grey literature in multiple languages", 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7714.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7714.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Moreno-Garcia et al. (2023)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero‑shot classification methods for automated abstract screening</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced study applying machine learning and zero‑shot classification methods to automate abstract screening, illustrating non-LLM and LLM-adjacent approaches to distillation of literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A novel application of machine learning and zero-shot classification methods for automated abstract screening in systematic reviews</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>A novel application of machine learning and zero-shot classification methods for automated abstract screening in systematic reviews</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>C. F. Moreno‑Garcia et al.</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2023</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Zero‑shot classification pipeline for abstract screening</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Applied zero‑shot classification (and other ML techniques) to automate abstract screening for systematic reviews; cited as prior automation work.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Titles/abstracts</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Inclusion/exclusion predictions</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Zero‑shot classification (explicit in the cited title)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Can large language models replace humans in systematic reviews? Evaluating GPT‐4's efficacy in screening and extracting data from peer‐reviewed and grey literature in multiple languages", 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7714.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e7714.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Summerscales et al. (2011)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automatic summarization of results from clinical trials</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An earlier method for automatic summarization of clinical trial results, cited as an antecedent to contemporary automated synthesis systems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automatic summarization of results from clinical trials</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Automatic summarization of results from clinical trials</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>R. L. Summerscales et al.</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2011</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Automatic clinical trial summarization</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Techniques to automatically summarize results from clinical trials into condensed outputs; used as prior work demonstrating automated distillation of scientific reports.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Clinical trial reports / full texts</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Summaries of trial results</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Older systems limited to predetermined structures and struggled with full-text complexity (noted in this paper's discussion).</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Can large language models replace humans in systematic reviews? Evaluating GPT‐4's efficacy in screening and extracting data from peer‐reviewed and grey literature in multiple languages", 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Automated paper screening for clinical reviews using large language models <em>(Rating: 2)</em></li>
                <li>Assessing the ability of chatgpt to screen articles for systematic reviews <em>(Rating: 2)</em></li>
                <li>Harnessing the power of chatgpt for automating systematic review process: Methodology, case study, limitations, and future directions <em>(Rating: 2)</em></li>
                <li>Trialstreamer: A living, automatically updated database of clinical trial reports <em>(Rating: 2)</em></li>
                <li>A novel application of machine learning and zero-shot classification methods for automated abstract screening in systematic reviews <em>(Rating: 1)</em></li>
                <li>Automatic summarization of results from clinical trials <em>(Rating: 1)</em></li>
                <li>Using artificial intelligence methods for systematic review in health sciences: A systematic review <em>(Rating: 2)</em></li>
                <li>Nivolumab for adults with hodgkin's lymphoma (a rapid review using the software robotreviewer) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7714",
    "paper_id": "paper-264490974",
    "extraction_schema_id": "extraction-schema-144",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4",
            "brief_description": "A large autoregressive transformer language model from OpenAI; in this paper it was accessed through the ChatGPT interface and used autonomously to perform title/abstract screening, full-text screening, and structured data extraction for a systematic review.",
            "citation_title": "",
            "mention_or_use": "use",
            "paper_title": "CAN LARGE LANGUAGE MODELS REPLACE HUMANS IN THE SYSTEMATIC REVIEW PROCESS? EVALUATING GPT-4'S EFFICACY IN SCREENING AND EXTRACTING DATA FROM PEER-REVIEWED AND GREY LITERATURE IN MULTIPLE LANGUAGES",
            "authors": "Qusai Khraisha et al.",
            "year": 2023,
            "method_name": "Human‑out‑of‑the‑loop GPT‑4 screening and extraction pipeline",
            "method_description": "The authors used GPT‑4 (via ChatGPT) with engineered prompts to autonomously screen titles/abstracts and segmented full texts against inclusion/exclusion criteria and to extract structured study variables (parent/child characteristics, study design, sampling, instruments). They applied prompt batching, segmentation of long texts, per‑criterion prompting, and test–retest reliability checks of prompts.",
            "input_type": "Titles/abstracts, segmented full-texts (methods & results sections), grey literature reports/websites",
            "output_type": "Binary inclusion/exclusion decisions per criterion and structured extracted fields (participant counts/demographics, design, sampling, measurement instruments)",
            "prompting_technique": "Prompt engineering with per-criterion prompts, batching (limited number of abstracts per prompt), segmentation of long texts, test–retest reliability evaluation of prompts; no chain‑of‑thought or few‑shot scheme reported",
            "model_name": "GPT-4",
            "model_size": null,
            "datasets_used": "Documents from an in‑progress systematic review on parenting in protracted refugee situations: 300 titles/abstracts screened, 150 full texts screened, data extraction from 30 documents (mixed peer‑reviewed, grey, non‑English)",
            "evaluation_metric": "Sensitivity, specificity, accuracy, Cohen's kappa, weighted kappa, PABAK (prevalence‑adjusted bias‑adjusted kappa), dataset imbalance",
            "reported_results": "Specificity was generally high (&gt;0.80) across stages; sensitivity and adjusted agreement often ranged from none to moderate after accounting for chance and imbalance; for a subset of 'high‑reliability' prompts GPT‑4 achieved almost‑perfect agreement in full‑text screening and very high weighted kappa when penalising false rejections.",
            "limitations": "Performance strongly dependent on prompt reliability and prompt length/complexity; hallucination risk; segmentation due to token limits; dataset imbalance and chance agreement inflated simple accuracy; small sample size and limited generalisability across topics; difficulty with open or broad concepts (e.g., 'parenting behaviour').",
            "counterpoint": true,
            "uuid": "e7714.0",
            "source_info": {
                "paper_title": "Can large language models replace humans in systematic reviews? Evaluating GPT‐4's efficacy in screening and extracting data from peer‐reviewed and grey literature in multiple languages",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Trialstreamer",
            "name_full": "TrialStreamER (living auto-updated clinical trial database)",
            "brief_description": "An automated system that builds and maintains a continuously updated database of clinical trial reports and supports extraction of trial information; cited as an example of automation helping data extraction.",
            "citation_title": "Trialstreamer: A living, automatically updated database of clinical trial reports",
            "mention_or_use": "mention",
            "paper_title": "Trialstreamer: A living, automatically updated database of clinical trial reports",
            "authors": "I. J. Marshall et al.",
            "year": 2020,
            "method_name": "TrialStreamer automated trial ingestion and extraction",
            "method_description": "An automated pipeline that ingests clinical trial reports and extracts structured trial metadata to populate a living database; referenced as an automation tool used for extraction tasks in literature synthesis.",
            "input_type": "Clinical trial reports / full-text articles",
            "output_type": "Structured database entries (trial metadata and extracted fields)",
            "prompting_technique": null,
            "model_name": null,
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": null,
            "limitations": null,
            "counterpoint": null,
            "uuid": "e7714.1",
            "source_info": {
                "paper_title": "Can large language models replace humans in systematic reviews? Evaluating GPT‐4's efficacy in screening and extracting data from peer‐reviewed and grey literature in multiple languages",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "RobotReviewer",
            "name_full": "RobotReviewer (automated risk-of-bias assessment tool)",
            "brief_description": "An automated tool designed to assist with assessing study quality and bias in systematic reviews; cited as an example of AI applied to systematic review tasks.",
            "citation_title": "Nivolumab for adults with hodgkin's lymphoma (a rapid review using the software robotreviewer)",
            "mention_or_use": "mention",
            "paper_title": "Nivolumab for adults with hodgkin's lymphoma (a rapid review using the software robotreviewer)",
            "authors": "M. Goldkuhle et al.",
            "year": 2018,
            "method_name": "RobotReviewer automated bias and quality assessment",
            "method_description": "An NLP-based system that extracts trial characteristics and predicts risk-of-bias judgments to assist reviewers in quality assessment.",
            "input_type": "Full-text clinical trial reports / study reports",
            "output_type": "Predicted risk-of-bias judgements and extracted methodological statements",
            "prompting_technique": null,
            "model_name": null,
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": null,
            "limitations": null,
            "counterpoint": null,
            "uuid": "e7714.2",
            "source_info": {
                "paper_title": "Can large language models replace humans in systematic reviews? Evaluating GPT‐4's efficacy in screening and extracting data from peer‐reviewed and grey literature in multiple languages",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Rayyan",
            "name_full": "Rayyan (systematic review screening tool)",
            "brief_description": "A widely used semi-automated web app to assist with title/abstract screening, providing organizational and some AI-supported features to speed screening.",
            "citation_title": "",
            "mention_or_use": "mention",
            "paper_title": null,
            "authors": null,
            "year": null,
            "method_name": "Rayyan screening assistance",
            "method_description": "Tool that supports human reviewers with tagging, blinding, and some recommendation/automation features for screening titles and abstracts.",
            "input_type": "Titles and abstracts / citations",
            "output_type": "Screening labels, reviewer organization metadata",
            "prompting_technique": null,
            "model_name": null,
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": null,
            "limitations": null,
            "counterpoint": null,
            "uuid": "e7714.3",
            "source_info": {
                "paper_title": "Can large language models replace humans in systematic reviews? Evaluating GPT‐4's efficacy in screening and extracting data from peer‐reviewed and grey literature in multiple languages",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Abstracker",
            "name_full": "Abstracker (automatic abstract screening tool)",
            "brief_description": "An automated tool mentioned alongside Rayyan that assists with screening titles and abstracts in systematic reviews.",
            "citation_title": "",
            "mention_or_use": "mention",
            "paper_title": null,
            "authors": null,
            "year": null,
            "method_name": "Abstracker abstract screening",
            "method_description": "Tool that automates or assists in the abstract screening stage of systematic reviews (mentioned as example of prior AI tools).",
            "input_type": "Titles and abstracts",
            "output_type": "Screening recommendations/labels",
            "prompting_technique": null,
            "model_name": null,
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": null,
            "limitations": null,
            "counterpoint": null,
            "uuid": "e7714.4",
            "source_info": {
                "paper_title": "Can large language models replace humans in systematic reviews? Evaluating GPT‐4's efficacy in screening and extracting data from peer‐reviewed and grey literature in multiple languages",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Guo et al. (2023)",
            "name_full": "Automated paper screening for clinical reviews using large language models",
            "brief_description": "A referenced study that applied large language models for automated paper screening in clinical reviews and reported high specificity but varied sensitivity; used as related work in this paper.",
            "citation_title": "Automated paper screening for clinical reviews using large language models",
            "mention_or_use": "mention",
            "paper_title": "Automated paper screening for clinical reviews using large language models",
            "authors": "E. Guo et al.",
            "year": 2023,
            "method_name": "LLM-based automated screening",
            "method_description": "Applied large language models to automate title/abstract screening for clinical systematic reviews; reported performance comparisons to other AI methods and humans (discussed in the present paper).",
            "input_type": "Titles/abstracts (clinical reviews)",
            "output_type": "Inclusion/exclusion labels",
            "prompting_technique": null,
            "model_name": null,
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": "Specificity, sensitivity (reported in cited work and compared in this paper)",
            "reported_results": "Reported high specificity similar to the present study; paper is cited as having a specificity around 90% in similar tasks.",
            "limitations": "Cited by the present paper as possibly affected by dataset contamination and not accounting for imbalance/chance agreement in some evaluations.",
            "counterpoint": null,
            "uuid": "e7714.5",
            "source_info": {
                "paper_title": "Can large language models replace humans in systematic reviews? Evaluating GPT‐4's efficacy in screening and extracting data from peer‐reviewed and grey literature in multiple languages",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Syriani et al. (2023)",
            "name_full": "Assessing the ability of ChatGPT to screen articles for systematic reviews",
            "brief_description": "A referenced preprint that evaluated GPT (ChatGPT) for autonomous article screening for systematic reviews and is used as related work in the current study.",
            "citation_title": "Assessing the ability of chatgpt to screen articles for systematic reviews",
            "mention_or_use": "mention",
            "paper_title": "Assessing the ability of chatgpt to screen articles for systematic reviews",
            "authors": "E. Syriani et al.",
            "year": 2023,
            "method_name": "ChatGPT screening evaluation",
            "method_description": "Evaluation of ChatGPT/GPT variants for automated screening tasks in systematic reviews; compared GPT to older AI methods.",
            "input_type": "Titles/abstracts",
            "output_type": "Screening decisions",
            "prompting_technique": null,
            "model_name": "ChatGPT / earlier GPT versions",
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": "Reported superior performance of GPT relative to some older AI methods (noted in present paper).",
            "limitations": "Cited present paper notes that some prior studies (including Syriani et al.) may have dataset contamination from materials used in model training.",
            "counterpoint": null,
            "uuid": "e7714.6",
            "source_info": {
                "paper_title": "Can large language models replace humans in systematic reviews? Evaluating GPT‐4's efficacy in screening and extracting data from peer‐reviewed and grey literature in multiple languages",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Alshami et al. (2023)",
            "name_full": "Harnessing the power of ChatGPT for automating systematic review process: Methodology, case study, limitations, and future directions",
            "brief_description": "A referenced work that applied ChatGPT to automate parts of the systematic review process, though reportedly using a human-in-the-loop approach.",
            "citation_title": "Harnessing the power of chatgpt for automating systematic review process: Methodology, case study, limitations, and future directions",
            "mention_or_use": "mention",
            "paper_title": "Harnessing the power of chatgpt for automating systematic review process: Methodology, case study, limitations, and future directions",
            "authors": "A. Alshami et al.",
            "year": 2023,
            "method_name": "ChatGPT-assisted systematic review automation",
            "method_description": "Applied ChatGPT to assist with systematic review tasks (notably screening), but relied on human-in-the-loop validation rather than fully autonomous operation.",
            "input_type": "Titles/abstracts / systematic review texts",
            "output_type": "Screening suggestions and possibly summaries",
            "prompting_technique": null,
            "model_name": "ChatGPT",
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": null,
            "limitations": "Reported use of human-in-the-loop; present paper cites this as a methodological difference from fully autonomous evaluations.",
            "counterpoint": null,
            "uuid": "e7714.7",
            "source_info": {
                "paper_title": "Can large language models replace humans in systematic reviews? Evaluating GPT‐4's efficacy in screening and extracting data from peer‐reviewed and grey literature in multiple languages",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Moreno-Garcia et al. (2023)",
            "name_full": "Zero‑shot classification methods for automated abstract screening",
            "brief_description": "A referenced study applying machine learning and zero‑shot classification methods to automate abstract screening, illustrating non-LLM and LLM-adjacent approaches to distillation of literature.",
            "citation_title": "A novel application of machine learning and zero-shot classification methods for automated abstract screening in systematic reviews",
            "mention_or_use": "mention",
            "paper_title": "A novel application of machine learning and zero-shot classification methods for automated abstract screening in systematic reviews",
            "authors": "C. F. Moreno‑Garcia et al.",
            "year": 2023,
            "method_name": "Zero‑shot classification pipeline for abstract screening",
            "method_description": "Applied zero‑shot classification (and other ML techniques) to automate abstract screening for systematic reviews; cited as prior automation work.",
            "input_type": "Titles/abstracts",
            "output_type": "Inclusion/exclusion predictions",
            "prompting_technique": "Zero‑shot classification (explicit in the cited title)",
            "model_name": null,
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": null,
            "limitations": null,
            "counterpoint": null,
            "uuid": "e7714.8",
            "source_info": {
                "paper_title": "Can large language models replace humans in systematic reviews? Evaluating GPT‐4's efficacy in screening and extracting data from peer‐reviewed and grey literature in multiple languages",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Summerscales et al. (2011)",
            "name_full": "Automatic summarization of results from clinical trials",
            "brief_description": "An earlier method for automatic summarization of clinical trial results, cited as an antecedent to contemporary automated synthesis systems.",
            "citation_title": "Automatic summarization of results from clinical trials",
            "mention_or_use": "mention",
            "paper_title": "Automatic summarization of results from clinical trials",
            "authors": "R. L. Summerscales et al.",
            "year": 2011,
            "method_name": "Automatic clinical trial summarization",
            "method_description": "Techniques to automatically summarize results from clinical trials into condensed outputs; used as prior work demonstrating automated distillation of scientific reports.",
            "input_type": "Clinical trial reports / full texts",
            "output_type": "Summaries of trial results",
            "prompting_technique": null,
            "model_name": null,
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": null,
            "limitations": "Older systems limited to predetermined structures and struggled with full-text complexity (noted in this paper's discussion).",
            "counterpoint": null,
            "uuid": "e7714.9",
            "source_info": {
                "paper_title": "Can large language models replace humans in systematic reviews? Evaluating GPT‐4's efficacy in screening and extracting data from peer‐reviewed and grey literature in multiple languages",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Automated paper screening for clinical reviews using large language models",
            "rating": 2,
            "sanitized_title": "automated_paper_screening_for_clinical_reviews_using_large_language_models"
        },
        {
            "paper_title": "Assessing the ability of chatgpt to screen articles for systematic reviews",
            "rating": 2,
            "sanitized_title": "assessing_the_ability_of_chatgpt_to_screen_articles_for_systematic_reviews"
        },
        {
            "paper_title": "Harnessing the power of chatgpt for automating systematic review process: Methodology, case study, limitations, and future directions",
            "rating": 2,
            "sanitized_title": "harnessing_the_power_of_chatgpt_for_automating_systematic_review_process_methodology_case_study_limitations_and_future_directions"
        },
        {
            "paper_title": "Trialstreamer: A living, automatically updated database of clinical trial reports",
            "rating": 2,
            "sanitized_title": "trialstreamer_a_living_automatically_updated_database_of_clinical_trial_reports"
        },
        {
            "paper_title": "A novel application of machine learning and zero-shot classification methods for automated abstract screening in systematic reviews",
            "rating": 1,
            "sanitized_title": "a_novel_application_of_machine_learning_and_zeroshot_classification_methods_for_automated_abstract_screening_in_systematic_reviews"
        },
        {
            "paper_title": "Automatic summarization of results from clinical trials",
            "rating": 1,
            "sanitized_title": "automatic_summarization_of_results_from_clinical_trials"
        },
        {
            "paper_title": "Using artificial intelligence methods for systematic review in health sciences: A systematic review",
            "rating": 2,
            "sanitized_title": "using_artificial_intelligence_methods_for_systematic_review_in_health_sciences_a_systematic_review"
        },
        {
            "paper_title": "Nivolumab for adults with hodgkin's lymphoma (a rapid review using the software robotreviewer)",
            "rating": 1,
            "sanitized_title": "nivolumab_for_adults_with_hodgkins_lymphoma_a_rapid_review_using_the_software_robotreviewer"
        }
    ],
    "cost": 0.01699375,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>CAN LARGE LANGUAGE MODELS REPLACE HUMANS IN THE SYSTEMATIC REVIEW PROCESS? EVALUATING GPT-4'S EFFICACY IN SCREENING AND EXTRACTING DATA FROM PEER-REVIEWED AND GREY LITERATURE IN MULTIPLE LANGUAGES A PREPRINT
October 30, 2023</p>
<p>Qusai Khraisha khraishq@tcd.ie 
Trinity Centre for Global Health School of Psychology Trinity College Dublin Ireland</p>
<p>Sophie Put 
Department of Education
University of York
UK</p>
<p>Johanna Kappenberg 
School of Psychology Trinity College Dublin
Ireland</p>
<p>Azza Warraitch 
Trinity Centre for Global Health School of Psychology Trinity College Dublin Ireland</p>
<p>Kristin Hadfield 
Trinity Centre for Global Health School of Psychology Trinity College Dublin Ireland</p>
<p>CAN LARGE LANGUAGE MODELS REPLACE HUMANS IN THE SYSTEMATIC REVIEW PROCESS? EVALUATING GPT-4'S EFFICACY IN SCREENING AND EXTRACTING DATA FROM PEER-REVIEWED AND GREY LITERATURE IN MULTIPLE LANGUAGES A PREPRINT
October 30, 20235F7CC9B9FA3206BE928066BAD6FBD7C8arXiv:2310.17526v2[cs.CL]Systematic reviewsLarge language modelsLLMsGPTArtificial intelligenceAINatural Language ProcessingNLPMachine learning
Systematic reviews are vital for guiding practice, research, and policy, yet they are often slow and labour-intensive.Large language models (LLMs) could offer a way to speed up and automate systematic reviews, but their performance in such tasks has not been comprehensively evaluated against humans, and no study has tested GPT-4, the biggest LLM so far.This pre-registered study evaluates GPT-4's capability in title/abstract screening, full-text review, and data extraction across various literature types and languages using a 'human-out-of-the-loop' approach.Although GPT-4 had accuracy on par with human performance in most tasks, results were skewed by chance agreement and dataset imbalance.After adjusting for these, there was a moderate level of performance for data extraction, and -barring studies that used highly reliable prompts -screening performance levelled at none to moderate for different stages and languages.When screening full-text literature using highly reliable prompts, GPT-4's performance was 'almost perfect.' Penalising GPT-4 for missing key studies using highly reliable prompts improved its performance even more.Our findings indicate that, currently, substantial caution should be used if LLMs are being used to conduct systematic reviews, but suggest that, for certain systematic review tasks delivered under reliable prompts, LLMs can rival human performance.</p>
<p>Introduction</p>
<p>Systematic reviews play a crucial role in advancing practice, research, and policy (Aromataris et al., 2015).However, the current approach to systematic reviews is laborious and can be slow to the point that the resulting synthesis of knowledge may no longer be up to date when it is completed (Borah et al., 2017;Michelson and Reuter, 2019).The explosion of scientific literature, coupled with the complexity and specificity of many research questions, further adds to these challenges (Fiorini et al., 2018).Artificial intelligence (AI) has emerged as a potential solution to these challenges, with recent studies and evaluations suggesting its capability to enhance the quality and efficiency of systematic reviews (Blaizot et al., 2022;Dijk et al., 2023;Kebede et al., 2023;Mahuli et al., 2023;Moreno-Garcia et al., 2023;Nugroho et al., 2023;Santos et al., 2023).Some examples of AI tools that have been used in systematic reviews include Rayyan and Abstracker, which help with screening titles and abstracts (Giummarra et al., 2020;Rogers et al., 2020), trialStreamer, which helps with data extraction from full-text articles (Marshall et al., 2020), and RobotReviewer, which helps with assessing study quality and bias (Goldkuhle et al., 2018).The major shortcoming of these tools is that their performance significantly deteriorates without looping in a human in the decision-making process, as shown by Blaizot and colleagues' (2022) meta-analysis.One possible reason for these limitations is that they split text into fixed segments, which has been argued to hinder their ability to understand context, especially in longer texts (Guo et al., 2023).Newer large language models (LLMs) based on the transformer technology (Vaswani et al., 2017), such as Generative Pre-Trained Transformer (GPT) and Bidirectional Encoder Representations from Transformers (BERT), may overcome this problem since they capture more contextual information.This was demonstrated in a recent study, which highlighted GPT's superior performance on systematic review tasks compared to older AI methods (Syriani et al., 2023).</p>
<p>The question of whether AI tools can match or surpass human performance in conducting systematic reviews carries profound implications for the future of scientific research.It holds the potential to radically transform knowledge synthesis, turning systematic reviews from static literature summaries into dynamic, continually updated resourcespotentially altering the very way we approach science.Given these significant implications, it is crucial to acknowledge the current uncertain state of AI in this domain, especially regarding the most substantial LLM model, GPT-4.This model has been reported to significantly surpass all other LLMs, including previous versions of GPT, in various natural language processing tasks across both English and other languages (OpenAI, 2023).Yet, as of now, nothing is documented about GPT-4's performance in conducting systematic reviews.</p>
<p>Research on using other LLMs in systematic reviews (mostly earlier versions of GPT) is not as comprehensive or systematic as it could be, with much of the work containing contaminated datasets and inadequate metrics.No study, for instance, has tested grey literature and non-English literature, which can constitute a large proportion of the evidence base for some topics (Lawrence et al., 2014).Most studies focused on narrow aspects of systematic reviews, such as Boolean queries (S.Wang et al., 2023) or only evaluating performance on titles and abstracts screening (Alshami et al., 2023;Guo et al., 2023;Syriani et al., 2023).Some have methodological shortcomings, such as Mahuli et al., (2023), who did not provide an objective evaluation of GPT's performance, or Alshami et al., (2023), who did not test GPT's autonomous performance, instead relying on a 'human-in-the-loop' approach.A few may have included contaminated data, such as Guo et al., (2023) and Syriani et al., (2023), who used datasets for systematic reviews that published their results before or in 2021, when GPT was trained, which may bias the results in favour of GPT.Other studies, such as Guo et al., (2023), did not consider imbalance nor incorporate chance agreement in their interpretation of the results, thereby potentially inflating GPT's accuracy.Syriani et al., (2023) addressed this issue but focused on investigating GPT's performance against other AI tools, not human reviewers.Our pre-registered study is the first to evaluate GPT-4's autonomous performance across several systematic review processes, including title/abstract screening, full-text screening, and data extraction.It is also the first to test an LLM model in reviewing grey literature and literature in other languages.</p>
<p>Methods</p>
<p>This study assessed GPT-4's performance in screening and extracting data from documents for an ongoing systematic review on parenting in protracted refugee situations.We used the ChatGPT interface to access the GPT-4 model between May and September 2023.We tested documents that were reviewed using four inclusion/exclusion criteria: containing empirical data, parenting behaviour, refugee status, and protracted refugee situation.Links to our GPT-4 prompts and outputs, as well as the R code used for analysis, are on the Open Science Foundation (OSF) page (link).We registered this study protocol on the OSF, while the details of the review can be found on both OSF (link) and PROSPERO (Anonymised).We screened 300 titles/abstracts and 150 full-texts, as well as extracted data from 30 documents (see Figures 1 and 2).Sample size was largely based on studies reviewed by at least two humans for screening, which was the case for English language documents at all stages and those written in other languages in the title/abstract stage.However, due to time and resource constraints, we only used one human reviewer for non-English studies at the full-text level, and for data extraction from all studies.While we ensured a mix of decisions in terms of a random selection of relevant and irrelevant studies written in English to gain deeper insight into inclusion performance, this was generally not possible for non-English studies due to the large number of excluded studies.</p>
<p>Prompt engineering approach</p>
<p>We experimented with GPT-4 prompt formats for title/abstract screening.Initially, we tested citations alone, but GPT-4 appeared to focus mainly on titles, and its accuracy decreased with an increased volume of citations.To address this, we presented complete abstracts in our prompts.Our next challenge was finding that GPT-4 struggled with complex queries and large data volumes, prompting us to present inclusion/exclusion criteria separately and reduce the number of titles/abstracts in each message.This approach seemed to reduce hallucinations, a frequently observed phenomenon where an LLM confidently produces an inaccurate output (Beutel et al., 2023), but reduced consistency upon retesting.Presenting criteria and text within the same message and lowering the number of studies based on the criterion's complexity improved consistency without increasing hallucinations.</p>
<p>We assessed test-retest reliability using 10 studies on each of our four criteria.Each criterion corresponds to a prompt.We tested the four prompts five times and generated five decisions per criterion per study.We then recorded each time GPT-4 output was inconsistent from the original answer.These scores were used to assess the impact of prompt reliability on accuracy scores.</p>
<p>Screening and extraction</p>
<p>Each abstract batch (a maximum of three abstracts for the protracted concept -given that it contained a longer promptand five abstracts for other concepts) underwent four chat tests based on inclusion/exclusion criteria.We tested multiple abstracts within the same query to increase efficiency in screening.We proceeded to the next criterion in another chat if GPT-4 responded 'yes/maybe'.A 'no' meant exclusion from further review.If an abstract was excluded for not meeting a criterion, we removed it and introduced a new one for subsequent tests, meaning that the number of abstracts remained the same every time a criterion was tested.We set a five-message limit for titles/abstracts and three for full texts to avoid GPT-4's inaccuracies arising from too much information.Lengthy abstracts in grey literature were divided for separate evaluations until GPT-4 finalised a decision.During grey literature screening, we modified the query to fit source formats, switching "titles/abstracts" to "websites/reports" and "texts."</p>
<p>For full text, we adjusted our queries, using "Include" instead of "Yes/maybe" and "Exclude" for "No".Due to character limits, we segmented texts.To reduce contamination of responses with each other, every time a text snippet was sent to GPT-4, it was accompanied by the specific inclusion/exclusion criterion it was being tested against together within the same prompt.If GPT-4 said "Include" for a segment, we viewed that criterion as met and began a new chat with the next criterion from the first segment.An "Exclude" led us to present subsequent segments.If GPT-4 never indicated "Include" by the last segment, the study was marked as excluded, and no further criteria were tested.If humans had already excluded a study, GPT-4 began screening with the humans' exclusion reason.If GPT-4 shared the decision made by humans, no further tests were needed.Otherwise, all remaining criteria were checked until we received an exclusion decision.As a result, 72% of the English peer-reviewed literature needed testing on the 'parenting' and 'protracted refugee situations' prompt, compared to 44% and 18% for grey and non-English literature, respectively.</p>
<p>Extracted information mainly revolved around characteristics of parents (e.g., number of parents, gender, and education distribution), characteristics of children (e.g., number of children, gender, and education distribution), study design (by duration, data collection method and group allocation), sampling technique, and instruments used to measure parenting (e.g., instrument type and name, target respondent and the main focus of the instrument).Only text from the methods and results sections of the chosen papers were inputted into GPT-4.Similar to the above, we segmented the text into pieces.Only the first response to a prompt was deemed final, except if the response was incomplete, then the subsequent responses were also recorded as part of the answer.</p>
<p>Analysis metrics</p>
<p>We applied four metrics: True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN).TP and TN are the cases where GPT-4 agreed with human reviewers, meaning it made correct decisions.FP and FN are the cases where GPT-4 disagreed with human reviewers, meaning it made wrong decisions.Ratios derived from these metrics provide insight into the imbalance of the dataset by dividing true cases against false cases.A meta-analysis indicated that systematic reviews' datasets have between 40% to less than 1% of relevant studies, with an average of 3%.This suggests that imbalance scores of 3% are typical, while a score of 40% or less than 1% is 'somewhat typical,' and a score of above 40% is 'atypical' (Sampson et al., 2011).
Imbalance = T P + F P T N + F N(1)
For GPT-4 performance, we calculated sensitivity, specificity and accuracy, which are commonly used metrics (e.g., Frömke et al., 2022;Patel et al., 2021).Sensitivity (also known as recall) shows how well GPT-4 identified positive cases by taking (TP) and dividing them by the sum of TP and FN.Specificity indicates how well GPT-4 identified negative cases by taking TN and dividing them by the sum of TN and FP.Accuracy, which evaluates the overall correctness of GPT-4, was calculated by adding TP and TN and dividing by the total number of cases.There is no consensus on how to interpret these scores, as they largely depend on the context (e.g., Shreffler and Huecker, 2023).</p>
<p>Previous studies have reported that human error rates in systematic review screening range from 5% to 20%, implying that a score of 100% is 'superior' to humans, while an accuracy score of 80% to 95% for GPT-4 could be regarded as 'on par' (Wang et al., 2020).In the worst possible documented prediction, human rates reached up to 40% (Wang et al., 2020).This suggests that accuracy scores between 60% and 80% could be regarded as 'near-par,' and scores below 60% could be regarded as 'subpar' to humans.</p>
<p>Sensitivity = T P T P + F N</p>
<p>(2)
Specificity = T N T N + F P (3) Accuracy = T P + T N T P + T N + F P + F N(4)
Cohen's Kappa (κ) was calculated to compare the actual and expected agreement between GPT-4 and human reviewers (Cohen, 1960).This is important for systematic reviews, where inclusion is rare, and exclusion is common, which can make humans and GPT-4 seem more agreeable than they are.The actual agreement (Po) is the proportion of cases humans and GPT-4 gave the same rating, positive or negative.The expected agreement (Pe) is the probability that humans and GPT-4 gave the same rating by chance, based on their rating frequencies.We subtracted the expected agreement from the actual agreement and divided it by the maximum possible agreement (1 minus the expected agreement).This gives a score between -1 and 1.We followed what McHugh (2012) suggested for the classification of agreement scores, which are more stringent on interpreting values than those suggested by Cohen (1960): values .0-.20 as no agreement, .21-.40 as minimal, .41-.59 as weak, .60 -.79 as moderate, and 0.80 -.90 as almost perfect agreement.</p>
<p>Cohen's kappa has been shown to produce a false agreement rate in imbalanced datasets, so we used PABAK (prevalenceadjusted bias-adjusted kappa) to account kappa for the effects of prevalence and bias in the data set (Byrt et al., 1993).PABAK corrects for this by accounting for the distribution of the categories in the denominator and by adjusting the counts of agreements and disagreements in the formula.We also used weighted kappa to better capture the severity of false rejections by GPT-4, which are the most serious errors it can make, because this would exclude a study which meets the inclusion criteria.Weighted kappa (ωκ) assigns higher weights to greater disagreements, with 0 for complete agreement.There is no standard test that can combine weights with PABAK, so we could not account for the effects of data imbalance in the weighted Cohen's kappa scores.Sampson et al. (2011) found that the median search precision for systematic reviews is around 3%; that is, there are about 30 times more excluded studies than included ones.This suggests a weight of 30 for false rejections in the calculation of weighted kappa.
κ = 1 − Pe Po − Pe (5) ωκ = 1 − 1 − Poω 1 − Peω (6) PABAK = 1 − 0.5 2Po − 1 (7)
3 Results</p>
<p>In this study, we evaluated GPT-4's performance in screening and extracting peer-reviewed, grey (non-peer reviewed), and non-English literature.We first report on the reliability of answers when using the same prompt.This means that GPT-4 gave the same answer every time, without any variation.For instance, if GPT-4 gave the same answer 10 times in a row for the same text and prompt, it scored 100%, but if it gave the same initial answer only 7 times out of 10, it scored 70%.GPT-4 performed best when assessing empirical data and refugees (100% reliability) and struggled with the concepts of parenting behaviour (50% reliability) and protracted refugee situations (70% reliability).With these findings in mind, we created a sub-literature sample that included only prompts relating to refugee status and empirical data called the 'high-reliability prompt group,' given that these prompts had the highest reliability scores.Ultimately, this sub-sample included 23 studies: a third were English peer-reviewed studies, a third were English grey literature studies, and another third were non-English studies.</p>
<p>We subsequently looked at the balance of data, which is the ratio of relevant to irrelevant studies, for each literature type, language, and stage (for the extraction stage, this means the presence or absence of data).Peer-reviewed studies in English were fully balanced, as intended by our design, except in the extraction stage (.03; that is, 1 included for every 30 excluded).Unlike non-English studies (.05), the grey literature was fully balanced at the title/abstract stage, but then both grey literature and non-English studies were skewed towards irrelevance in the full-text screening (.11 and .09, respectively) and extraction stages (.24 and .20, respectively).It is important to note that while balance aids in understanding all aspects of performance, it does not reflect the inherent imbalances in real-world datasets of systematic review.Based on Sampson et al., (2011) finding that there are typically about 30 times more excluded studies than included studies, our most imbalanced datasets were also the most consistent with other systematic reviews.</p>
<p>Across all stages and categories, the specificity was on par with human performance (&gt;.80, except for English peerreviewed full-text screening), indicating a robust ability of GPT-4 to correctly identify irrelevant studies (Table 1).This was especially true in literature containing non-English studies (&gt;.90).Sensitivity, indicating how effectively GPT-4 identified relevant studies, was highest in the extraction stage for both peer-reviewed (English: .75)and grey literature (.65), although note that for non-English studies the sensitivity was only .36 for data extraction.For non-English studies, perfect sensitivity was achieved during the full-text stage.Accuracy was somewhat higher in the data extraction stage than in the title/abstract screening phase, ranging between near-par and on par with human performance, except for the English peer-reviewed literature.</p>
<p>The balanced dataset of English peer-reviewed literature had lower accuracy (title/abstract: .67,full-text: .69)than the more unbalanced non-English literature dataset (title/abstract: .88,full-text: .96).In extraction, which was the only time the dataset of English peer-reviewed literature was imbalanced, accuracy was the highest (.84).This suggests that GPT-4's high accuracy might be due to chance.Supporting this notion, the associated adjusted kappa scores were low, ranging from 'none' to 'moderate' as categorised by McHugh (2012).An outlier in these scores was the 'almost perfect' agreement seen in the highly-reliable prompt group, which exclusively featured responses from highly reliable prompts (.91).When we weighted kappa to emphasise false rejections, in a way penalising GPT-4 for missing key studies, scores for the highly reliable prompt group improved even more (.97).</p>
<p>Discussion</p>
<p>We found mixed results on the efficacy of GPT-4 as compared to human reviewers across various systematic review tasks, languages, and literature types.GPT-4's accuracy was influenced by chance agreement and dataset imbalance, and when these factors were considered, GPT-4 often substantially underperformed humans.Yet, under specific conditionsnamely, when given entirely highly reliable prompts in full-text screening -GPT-4 demonstrated an 'almost perfect' performance on par with humans.While our findings indicate the need for caution in assuming uniform proficiency across tasks, they also suggest that, under certain conditions, LLMs have the potential to revolutionise how we synthesise knowledge.</p>
<p>Our results for sensitivity, and specificity are consistent with previous studies.For instance, Guo et al. (2023) and Alshami et al. (2023) found that specificity was the strongest metric for title/abstract screening, as we did.They achieved a specificity score close to ours (90% and 93%, respectively, vs our 92%).Their sensitivity score was slightly higher than ours (76% and 84%, respectively, vs our 67%; although note that Alshami et al., 2023 used a human-in-the-loop method).Our accuracy score for the peer-reviewed literature (67%) was lower than in previous work, possibly because our study was the only one that artificially balanced its dataset.Unlike the others (Alshami et al., 2023;Guo et al., 2023), we took this step because skewed datasets can make accuracy metrics unreliable, as later indicated by our low adjusted kappa.Such a limitation will not be easily detected by anecdotal tests (Mahuli et al., 2023;Qureshi et al., 2023) and may mislead general users who may assume that GPT is performing well when it is not.A possible reason for this misconception is that GPT typically generates text that resembles human writing in terms of quality, style, and content (Jakesch et al., 2023), thus misleading users to think that GPT has human-level abilities based on its human-like outputs.</p>
<p>This study was the first to report on the novel application of an LLM in conducting full-text screening and extraction.Automation techniques have been hailed as a way of increasing reproducibility (Ivimey-Cook et al., 2023).However, previous endeavours employing AI tools for these tasks pinpointed several challenges.These included the need for continuous human intervention in full-text screening (Beller et al., 2018), the necessity for extensive pre-screening of training datasets (Halamoda-Kenzaoui et al., 2022), and the incapability to either review full-texts (Clark et al., 2020;Nye et al., 2018) or to extract data deviating from a predetermined structure (Summerscales et al., 2011;Wallace et al., 2016) .</p>
<p>29</p>
<p>.69</p>
<p>*</p>
<p>The inter-rater reliability between human reviewers for the full-text data was a Cohen Kappa coefficient of .77.However, the review is not yet completed, so the final value may vary slightly.The inter-rater reliability between human reviewers was calculated using data mostly from the peer-reviewed literature, but it also includes a small portion of grey literature and non-English studies.</p>
<p>**</p>
<p>The human reviewers achieved an adjusted Cohen Kappa of .89for the same literature sample Cohen Kappa was calculated for above.</p>
<p>Note.We used PABAK (Prevalence-Adjusted Bias-Adjusted Kappa) to adjust Kappa for the effects of prevalence and bias in the data set (Byrt et al., 1993) and a weight of 30 for false rejections in the calculation of weighted Kappa based on previous studies which have shown that the median search precision for systematic reviews is around 3% (Sampson et al., 2011).</p>
<p>difference.However, we should note that the English peer-reviewed literature data had a very unusual balance of studies, unlike the other databases, which are closer in their compositions to other systematic reviews, which suggests caution against assuming generalisability.Lastly, GPT-4 performance was strongly influenced by prompt reliability, which could itself be affected by word count and prompt complexity.Longer prompts, like those for parenting behaviour and protracted refugee situations (around 400 words and 1600 words, respectively; the other two prompts were less than 300 words), may have lost important context due to their size.We also observe that the 'parenting behaviour' prompt might have been most challenging for GPT-4 because the prompt was more open than specific (to capture non-traditional ways of parenting), unlike all other ones, which contained exact definitions.</p>
<p>There are three main strengths and weaknesses to this study.First, despite our comprehensive approach covering various literature types and our efforts to achieve balance, the challenges in balancing non-English texts and our very attempts at balancing could have both introduced biases.Second, we used various metrics to measure GPT-4's accuracy, consistency, and agreement with human reviewers, but our relatively small sample size of evaluated papers might limit the generalisability of GPT-4's findings for other systematic reviews.Third, we registered and documented our research protocol and analysis plan to ensure its validity and replicability but might have benefited from also creating a detailed plan for prompt engineering in advance.</p>
<p>Conclusion</p>
<p>Over a hundred years ago, audiences worldwide were captivated by a horse named Hans, who, with a confident tap of his hoof, appeared to solve mathematical problems.They believed Hans possessed an extraordinary cognitive ability, almost human-like in nature.Yet, beneath this illusion lay a simpler truth: Hans was reading his handler.He picked up on the faintest of cues -a twitch of a muscle, a barely noticeable nod, or even an unconscious sigh of anticipation.This phenomenon mirrors the workings of LLMs like GPT-4.While not deciphering human cues per se, they are heavily influenced by the prompts they receive, much like Hans needed clear guidance from his handler.But there is a key distinction: whereas Hans required human guidance for his output, GPT-4 needs clear human input and generates outputs autonomously.Our study underscores this, showing that when given a reliable prompt, GPT-4's screening performance rises to be almost perfect.In harnessing this potential, LLMs might pave the way for a transformative era in systematic reviews.</p>
<p>Table 1 :
1
. Our findings indicate that GPT-4 has limited potential in full-text screening and data extraction, with moderate performance in non-English and grey literature, and a very poor ability with English peer-reviewed texts.Its training on publicly available data, which might lean more towards grey literature and non-English sources, could explain this Performance Evaluation of GPT-4 versus Human Reviewers in Screening and Extraction Balance Sensitivity Specificity Accuracy Cohen Kappa * Weighted Kappa Adjusted Kappa * *
.34.32.75.08.55.64.91.63.62.23.24.40.05.44-.11.97.63.53.34.32.21.07.24-.10.65.54.45.35.67.66.88.54.78.96.85.82.81.85.92.84.89.69.80.95.94.84.85.94.42.48.50.38.601.36.75.65.3611.05.92.11.09.05.03.24.20Title and abstract screeningEnglish peer-reviewedEnglish greyOther languagesFull text screeningEnglish peer-reviewedEnglish greyOther languagesHigh-reliability prompt groupHigh-reliability prompt groupData extractionEnglish peer-reviewedEnglish greyOther languages</p>
<p>Harnessing the power of chatgpt for automating systematic review process: Methodology, case study, limitations, and future directions. A Alshami, M Elsayed, E Ali, A E E Eltoukhy, T Zayed, Systems. 1173512023</p>
<p>Summarizing systematic reviews: Methodological development, conduct and reporting of an umbrella review approach. E Aromataris, R Fernandez, C M Godfrey, C Holly, H Khalil, P Tungpunkom, JBI Evidence Implementation. 1331322015</p>
<p>Making progress with the automation of systematic reviews: Principles of the international collaboration for the automation of systematic reviews (icasr). E Beller, J Clark, G Tsafnat, C Adams, H Diehl, H Lund, M Ouzzani, K Thayer, J Thomas, T Turner, J Xia, K Robinson, P Glasziou, C Adams, O Ahtirschi, E Beller, J Clark, R Christensen, H Diehl, Systematic Reviews. 2018777</p>
<p>G Beutel, E Geerits, J T Kielstein, Artificial hallucination: Gpt on lsd? Critical Care. 202327148</p>
<p>Using artificial intelligence methods for systematic review in health sciences: A systematic review. A Blaizot, S K Veettil, P Saidoung, C F Moreno-Garcia, N Wiratunga, M Aceves-Martins, N M Lai, N Chaiyakunapruk, Research Synthesis Methods. 1332022</p>
<p>Analysis of the time and workers needed to conduct systematic reviews of medical interventions using data from the prospero registry. R Borah, A W Brown, P L Capers, K A Kaiser, BMJ Open. 72e0125452017</p>
<p>Bias, prevalence and kappa. T Byrt, J Bishop, J B Carlin, Journal of Clinical Epidemiology. 4651993</p>
<p>A full systematic review was completed in 2 weeks using automation tools: A case study. J Clark, P Glasziou, C Del Mar, A Bannach-Brown, P Stehlik, A M Scott, Journal of Clinical Epidemiology. 1212020</p>
<p>A coefficient of agreement for nominal scales. J Cohen, Educational and Psychological Measurement. 2011960</p>
<p>Best match: New relevance search for pubmed. N Fiorini, K Canese, G Starchenko, E Kireev, W Kim, V Miller, M Osipov, M Kholodov, R Ismagilov, S Mohan, J Ostell, Z Lu, PLOS Biology. 168e20053432018</p>
<p>A semiparametric approach for meta-analysis of diagnostic accuracy studies with multiple cut-offs. C Fr "omke, M Kirstein, A Zapf, Research Synthesis Methods. 1352022</p>
<p>A systematic review of the association between fault or blame-related attributions and procedures after transport injury and health and work-related outcomes. M J Giummarra, G Lau, G Grant, B J Gabbe, Accident; Analysis and Prevention. 1351053332020</p>
<p>Nivolumab for adults with hodgkin's lymphoma (a rapid review using the software robotreviewer). M Goldkuhle, M Dimaki, G Gartlehner, I Monsef, P Dahm, J.-P Glossmann, A Engert, B Von Tresckow, N Skoetz, The Cochrane Database of Systematic Reviews. 77D0125562018</p>
<p>Automated paper screening for clinical reviews using large language models. E Guo, M Gupta, J Deng, Y.-J Park, M Paget, C Naugler, arXiv:2305.008442023</p>
<p>Toxic effects of nanomaterials for health applications: How automation can support a systematic review of the literature. B Halamoda-Kenzaoui, E Rolland, J Piovesan, Puertas, A Gallardo, S Bremer-Hoffmann, Journal of Applied Toxicology. 4212022</p>
<p>Advice for improving the reproducibility of data extraction in meta-analysis. E R Ivimey-Cook, D W A Noble, S Nakagawa, M J Lajeunesse, J L Pick, 2023Research Synthesis Methods</p>
<p>Human heuristics for ai-generated language are flawed. M Jakesch, J T Hancock, M Naaman, Proceedings of the National Academy of Sciences. 12011e22088391202023</p>
<p>In-depth evaluation of machine learning methods for semi-automating article screening in a systematic review of mechanistic literature. M M Kebede, C Le Cornet, R T Fortner, Research Synthesis Methods. 1422023</p>
<p>Where is the evidence? realising the value of grey literature for public policy and practice. A Lawrence, J Houghton, J Thomas, P Weldon, 2014a discussion paper</p>
<p>Application chatgpt in conducting systematic reviews and meta-analyses. S A Mahuli, A Rai, A V Mahuli, A Kumar, British Dental Journal. 23522023</p>
<p>Trialstreamer: A living, automatically updated database of clinical trial reports. I J Marshall, B Nye, J Kuiper, A Noel-Storr, R Marshall, R Maclean, F Soboczenski, A Nenkova, J Thomas, B C Wallace, Journal of the American Medical Informatics Association. 27122020</p>
<p>Interrater reliability: The kappa statistic. M L Mchugh, Biochemia Medica. 2232012</p>
<p>The significant cost of systematic reviews and meta-analyses: A call for greater involvement of machine learning to assess the promise of clinical trials. M Michelson, K Reuter, Contemporary Clinical Trials Communications. 161004432019</p>
<p>A novel application of machine learning and zero-shot classification methods for automated abstract screening in systematic reviews. C F Moreno-Garcia, C Jayne, E Elyan, M Aceves-Martins, Decision Analytics Journal. 61001622023</p>
<p>The shift in research trends related to artificial intelligence in library repositories during the coronavirus pandemic. P A Nugroho, N E V Anna, N Ismail, Library Hi Tech. 2023ahead-of-print</p>
<p>A corpus with multilevel annotations of patients, interventions and outcomes to support language processing for medical literature. B Nye, J J Li, R Patel, Y Yang, I Marshall, A Nenkova, B Wallace, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 56th Annual Meeting of the Association for Computational Linguistics20181</p>
<p>Openai, arXiv:2303.08774Gpt-4 technical report. 2023</p>
<p>Graphical enhancements to summary receiver operating characteristic plots to facilitate the analysis and reporting of meta-analysis of diagnostic test accuracy data. A Patel, N Cooper, S Freeman, A Sutton, Research Synthesis Methods. 1212021</p>
<p>Are chatgpt and large language models "the answer" to bringing us closer to systematic review automation?. R Qureshi, D Shaughnessy, K A R Gill, K A Robinson, T Li, E Agai, Systematic Reviews. 121722023</p>
<p>Interventions for increasing colorectal cancer screening uptake among african-american men: A systematic review and meta-analysis. C R Rogers, P Matthews, L Xu, K Boucher, C Riley, M Huntington, N Le Duc, K S Okuyemi, M J Foster, PloS One. 159e02383542020</p>
<p>The use of artificial intelligence for automating or semi-automating biomedical literature analyses: A scoping review. Á O D Santos, E S Da Silva, L M Couto, G V L Reis, V S Belo, Journal of Biomedical Informatics. 1421043892023</p>
<p>Diagnostic testing accuracy: Sensitivity, specificity, predictive values and likelihood ratios. J Shreffler, M R Huecker, StatPearls. StatPearls Publishing2023</p>
<p>Automatic summarization of results from clinical trials. R L Summerscales, S Argamon, S Bai, J Hupert, A Schwartz, 2011 IEEE International Conference on Bioinformatics and Biomedicine. 2011</p>
<p>Assessing the ability of chatgpt to screen articles for systematic reviews. E Syriani, I David, G Kumar, arXiv:2307.064642023</p>
<p>Artificial intelligence in systematic reviews: Promising when appropriately used. S H B Van Dijk, M G J Brusse-Keizer, C C Bucsán, J Van Der Palen, C J M Doggen, A Lenferink, BMJ Open. 137e0722542023</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L Kaiser, I Polosukhin, 2017</p>
<p>Extracting pico sentences from clinical trial reports using supervised distant supervision. B C Wallace, J Kuiper, A Sharma, 2016</p>
<p>Can chatgpt write a good boolean query for systematic review literature search?. S Wang, H Scells, B Koopman, G Zuccon, arXiv:2302.034952023</p>
<p>Error rates of human reviewers during abstract screening in systematic reviews. Z Wang, T Nayfeh, J Tetzlaff, P O'blenis, M H Murad, PLoS ONE. 151e02277422020</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7238 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7238</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7238</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-136.html">extraction-schema-136</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <p><strong>Paper ID:</strong> paper-273233870</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.07391v1.pdf" target="_blank">The Cognitive Capabilities of Generative AI: A Comparative Analysis with Human Benchmarks</a></p>
                <p><strong>Paper Abstract:</strong> There is increasing interest in tracking the capabilities of general intelligence foundation models. This study benchmarks leading large language models and vision language models against human performance on the Wechsler Adult Intelligence Scale (WAIS-IV), a comprehensive, population-normed assessment of underlying human cognition and intellectual abilities, with a focus on the domains of VerbalComprehension (VCI), Working Memory (WMI), and Perceptual Reasoning (PRI). Most models demonstrated exceptional capabilities in the storage, retrieval, and manipulation of tokens such as arbitrary sequences of letters and numbers, with performance on the Working Memory Index (WMI) greater or equal to the 99.5th percentile when compared to human population normative ability. Performance on the Verbal Comprehension Index (VCI) which measures retrieval of acquired information, and linguistic understanding about the meaning of words and their relationships to each other, also demonstrated consistent performance at or above the 98th percentile. Despite these broad strengths, we observed consistently poor performance on the Perceptual Reasoning Index (PRI; range 0.1-10th percentile) from multimodal models indicating profound inability to interpret and reason on visual information. Smaller and older model versions consistently performed worse, indicating that training data, parameter count and advances in tuning are resulting in significant advances in cognitive ability.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7238.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7238.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WMI_overview</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Working Memory Index (WAIS-IV) — aggregated model performance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Aggregate report of generative models' performance on the WAIS‑IV Working Memory Index (WMI), showing exceptionally high percentile ranks for most models with a small-model exception.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple models (GPT-3.5 Turbo, GPT-4 Turbo, GPT-4o, Gemini Pro, Gemini Advanced, Gemini Flash, Gemini Goldfish, Claude 3 Opus, Claude 3.5 Sonnet; exception: Gemini Nano)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>State-of-the-art large language models and multimodal vision-language models from OpenAI, Google (Gemini family), and Anthropic (Claude); architectures are transformer-based LLMs and multimodal extensions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Working Memory Index (WMI)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>working memory / short-term storage and manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>WAIS‑IV composite measuring ability to temporarily store, retrieve, and manipulate arbitrary information; composed of subtests including Digit Span, Arithmetic, and Letter-Number Sequencing, administered via text-adapted prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>WAIS‑IV standard scores and normative percentiles (age-normed, selected norms for ages 25–29).</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Human baseline: WAIS‑IV normative sample (age 25–29) used for conversion to standard scores and percentiles (paper uses WAIS‑IV norms as the baseline). Typical population median corresponds to the 50th percentile in these norms.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Most models scored in the Very Superior range on WMI (reported range ~95th -> >99.9th percentile); the paper states WMI performance >= 99.5th percentile for many models; exception Gemini Nano at 37th percentile.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Instruction-style, zero-shot prompts adapted from WAIS‑IV administration instructions (text conversion of items); prompts included the manual instructions and examples.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Wechsler, 2008 (WAIS‑IV) norms; age 25–29 norms selected by the authors.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Authors report statistically significant stronger WMI relative to other indices (noted significance levels p < .15 or p < .05 for comparisons to normative expectations and other indices).</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>WMI advantage reflects token storage/manipulation strengths (e.g., Digit Span ceiling performance). Adaptations to text prompts may advantage models; PSI subtests omitted; FSIQ not computed because PSI omitted.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Cognitive Capabilities of Generative AI: A Comparative Analysis with Human Benchmarks', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7238.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7238.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VCI_overview</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Verbal Comprehension Index (WAIS-IV) — aggregated model performance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Aggregate report of generative models' performance on the WAIS‑IV Verbal Comprehension Index (VCI), showing very high performance overall but variability by model and subtest.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple models (GPT-3.5 Turbo, GPT-4 Turbo, GPT-4o, Gemini Pro, Gemini Advanced, Gemini Flash, Gemini Goldfish, Claude 3 Opus, Claude 3.5 Sonnet; exceptions: Gemini Flash, Gemini Nano)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based LLMs and multimodal models evaluated via text-adapted WAIS‑IV verbal subtests.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Verbal Comprehension Index (VCI)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>verbal knowledge, language comprehension, reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>WAIS‑IV composite assessing verbal knowledge and reasoning through subtests such as Similarities, Vocabulary, Information, and Comprehension; administered via text prompts adapted from the WAIS manual.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>WAIS‑IV standard scores and normative percentiles (age-normed, age 25–29).</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Human baseline: WAIS‑IV normative sample (age 25–29) used; population median represents the 50th percentile.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Most models scored in the Very Superior range on VCI (reported at or above the 98th percentile); exceptions include Gemini Flash at 82nd percentile (Superior) and Gemini Nano at 23rd percentile (Borderline).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Instruction-style, zero-shot prompts adapted from WAIS‑IV subtest instructions (examples included where present).</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Wechsler, 2008 (WAIS‑IV) norms; age 25–29 norms selected by the authors.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Authors report significant VCI–PRI discrepancies (e.g., VCI–PRI differences reaching p < .05 for many models) and various VCI–WMI comparisons noted at p < .15 or p < .05.</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Within VCI, models were strongest on Information (crystallized knowledge retrieval; reported range 99.6th–99.9th percentiles) and relatively weaker on Similarities and Vocabulary for some smaller/older models, indicating retrieval > conceptual/analogical verbal reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Cognitive Capabilities of Generative AI: A Comparative Analysis with Human Benchmarks', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7238.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7238.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PRI_overview</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Perceptual Reasoning Index (WAIS-IV) — aggregated multimodal model performance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Aggregated performance of multimodal models on the WAIS‑IV Perceptual Reasoning Index (PRI), showing pervasive deficits on visual/perceptual reasoning relative to human norms.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multimodal models (GPT-4 Turbo, GPT-4o, Gemini Flash, Gemini Goldfish, Claude 3 Opus, Claude 3.5 Sonnet)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multimodal transformer-based models with image-and-text capabilities evaluated on WAIS‑IV perceptual reasoning subtests converted to text+image prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Perceptual Reasoning Index (PRI)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>perceptual reasoning, visuospatial processing, nonverbal abstract reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>WAIS‑IV composite assessing visuospatial skills and nonverbal abstract reasoning via subtests like Matrix Reasoning, Visual Puzzles, Figure Weights, and Picture Completion; administered to multimodal models only (image stimuli presented where applicable).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>WAIS‑IV standard scores and normative percentiles (age-normed).</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Human baseline: WAIS‑IV normative sample (age 25–29) used for percentile conversion; population median ~50th percentile.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Overall poor: majority of multimodal models performed in the Extremely Low range (many <1st–2nd percentile); reported PRI range across models ~0.1th–10th percentile; Claude 3.5 Sonnet was the best at ~10th percentile (Low Average/Borderline).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Text+image prompts adapted from WAIS‑IV visual subtests; visuals presented for multimodal models; administration text followed manual instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Wechsler, 2008 (WAIS‑IV) norms; age 25–29 norms selected by the authors.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>PRI performance was significantly worse than VCI and WMI (authors report PRI vs other indices significance at p < .05 across multimodal models).</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Authors substituted Figure Weights for Block Design (block manipulation unavailable). Visual tasks reveal large and consistent deficits indicating inability to interpret and reason on visual information despite multimodality; some progress observed across model generations (e.g., Claude 3.5 Sonnet improvements).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Cognitive Capabilities of Generative AI: A Comparative Analysis with Human Benchmarks', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7238.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7238.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Information_subtest</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>WAIS‑IV Information subtest — crystallized knowledge retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Performance of models on the Information subtest, indexing stored general knowledge (crystallized knowledge) with near-ceiling results reported for most models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple models (listed in paper; most models tested)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based LLMs evaluated using text prompts of WAIS‑IV Information items.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Information (WAIS‑IV subtest)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>crystallized knowledge / general knowledge retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Subjects answer broad general-knowledge questions; in study items were converted to text prompts and scored per WAIS‑IV criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>WAIS‑IV scaled scores and normative percentiles.</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Human baseline: WAIS‑IV norms used; the normative percentiles indicate population distribution with 50th percentile as median.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Near-perfect scores reported across multiple models; authors report Information subtest percentiles in the range ~99.6th–99.9th percentile for many models.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Instruction-style, zero-shot prompts following WAIS‑IV item wording.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Wechsler, 2008 (WAIS‑IV) norms.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Authors report multiple models show significantly higher Information scores compared to other VCI subtests (significance levels reported in Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Interpretation: models excel at retrieving encoded factual knowledge (crystallized knowledge) relative to human norms; this likely reflects pretraining data content.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Cognitive Capabilities of Generative AI: A Comparative Analysis with Human Benchmarks', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7238.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7238.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Digit_Span</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Digit Span (WAIS‑IV subtest) — forwards/backwards/sequencing</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Models achieved near-ceiling Digit Span performance (encoding and simple retrieval of digit sequences) whereas manipulative conditions (backwards/sequencing) showed variability, especially for small models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple models (GPT-3.5 Turbo, GPT-4 variants, Gemini family, Claude models); exception: Gemini Nano</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLMs and multimodal models tested on Digit Span subtests via text prompts representing sequences of digits to be recalled or manipulated.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Digit Span (Forwards, Backwards, Sequencing)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>working memory (verbal/auditory span and manipulation)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Subjects repeat sequences of digits forward, backward, or in sequence; here prompts provided digit sequences and responses scored per WAIS‑IV. Longest spans recorded and converted to age-normed base rates.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Raw longest span and WAIS‑IV base rates / percentiles (age 25–29 where applicable).</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Human baseline: WAIS‑IV normative base rates reported (Table 5); e.g., Longest Digit Span Forwards (LDSF) base rate (age 25–29) = 17.5% for the corresponding raw score; other base rates reported in Table 5.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Most models scored perfect or near-perfect Digit Span (e.g., many reported raw scaled scores of 19 and percentiles ~99.9); exception Gemini Nano showed substantially lower/manipulation-limited performance (e.g., Digit Span Backwards at ~2nd percentile in one reported case).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot text prompts replicating WAIS‑IV Digit Span instructions and digit sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Wechsler, 2008 (WAIS‑IV) norms; specific base rates and age-normed percentages reported in Table 5 of the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Authors report discrepancy analyses and base rates for Digit Span comparisons (scale-level comparisons shown in Table 3 with reported critical values and base rates; e.g., Digit Span total vs Arithmetic differences reached significance in some models).</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Authors note models excel at encoding/retrieval of token sequences but smaller models (Gemini Nano) struggle with manipulation tasks (backwards/sequencing), indicating limits on working-memory-like manipulation despite high forward-span performance. Adaptation to text may advantage models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Cognitive Capabilities of Generative AI: A Comparative Analysis with Human Benchmarks', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7238.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7238.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Arithmetic_subtest</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>WAIS‑IV Arithmetic subtest — mental arithmetic word problems</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Reported relative weakness on Arithmetic for several models compared to Digit Span and other WMI components, with lower percentiles for arithmetic reasoning despite strong digit storage.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple models (noted weaker for Gemini Nano; mixed across others)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLMs evaluated on WAIS‑IV Arithmetic word problems converted into text prompts and scored per WAIS criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Arithmetic (WAIS‑IV subtest)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>numerical reasoning / working memory / mental calculation</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Timed mental arithmetic word problems of increasing difficulty administered without paper; here presented as text prompts and scored according to WAIS‑IV.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>WAIS‑IV scaled scores and normative percentiles.</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Human baseline: WAIS‑IV normative percentiles used (age 25–29 norms).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported relative weakness in Arithmetic compared to Digit Span; authors report lower scaled scores and percentiles for Arithmetic in several models (examples: some models scored at percentiles much lower than their Digit Span percentiles; specific numeric examples in Table 2 show variation).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Instruction-style, zero-shot text prompts following WAIS‑IV Arithmetic items.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Wechsler, 2008 (WAIS‑IV) norms.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Authors report scale-level comparisons where Digit Span total minus Arithmetic differences reached significance for several models (see Table 3; critical values and base rates provided).</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Interpreted as models being strong at storing numeric tokens but relatively weaker at multi-step mental arithmetic reasoning without external calculation aids; smaller models (Gemini Nano) particularly weak.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Cognitive Capabilities of Generative AI: A Comparative Analysis with Human Benchmarks', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7238.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7238.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MatrixReasoning_Claude_improvement</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Matrix Reasoning (WAIS‑IV) — Claude 3 Opus → Claude 3.5 Sonnet improvement</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Reported generational improvement in Matrix Reasoning and Figure Weights for Anthropic's Claude series: substantial percentile increase from Claude 3 Opus to Claude 3.5 Sonnet.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude 3 Opus → Claude 3.5 Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Anthropic multimodal models optimized for reasoning; both evaluated on WAIS‑IV perceptual reasoning subtests with image+text prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Matrix Reasoning (WAIS‑IV subtest) and Figure Weights</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>nonverbal abstract reasoning (pattern detection) and quantitative analogical reasoning on visual stimuli</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Matrix Reasoning: select image completing a pattern; Figure Weights: balance analog-scale visual weights; items were converted to multimodal prompts and scored per WAIS‑IV.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>WAIS‑IV scaled scores and percentiles (age-normed).</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Human baseline: WAIS‑IV normative percentiles (age 25–29) used for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Claude 3 Opus: Matrix Reasoning ~0.1th percentile; Claude 3.5 Sonnet: Matrix Reasoning improved to ~25th percentile. For Figure Weights, improvement reported from ~0.1th percentile (Opus) to ~50th percentile (Sonnet) in one reported comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Multimodal prompts (image + instructions from WAIS‑IV manual) presented to models.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Wechsler, 2008 (WAIS‑IV) norms.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Authors emphasize measurable improvements across generations but do not report formal p-values for this specific pairwise generation comparison beyond percentile changes; PRI vs other indices significance reported elsewhere (p < .05).</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>These improvements indicate perceptual reasoning capabilities can be acquired/improved with model development, but large gaps remain versus language-domain performance. Exact numeric values come from comparisons reported in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Cognitive Capabilities of Generative AI: A Comparative Analysis with Human Benchmarks', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Evidence of interrelated cognitive-like capabilities in large language models: Indications of artificial general intelligence or achievement? <em>(Rating: 2)</em></li>
                <li>What is the visual cognition gap between humans and multimodal llms? <em>(Rating: 2)</em></li>
                <li>Efficiently measuring the cognitive ability of llms: An adaptive testing perspective. <em>(Rating: 1)</em></li>
                <li>WAIS-IV Scoring and Administration Manual <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7238",
    "paper_id": "paper-273233870",
    "extraction_schema_id": "extraction-schema-136",
    "extracted_data": [
        {
            "name_short": "WMI_overview",
            "name_full": "Working Memory Index (WAIS-IV) — aggregated model performance",
            "brief_description": "Aggregate report of generative models' performance on the WAIS‑IV Working Memory Index (WMI), showing exceptionally high percentile ranks for most models with a small-model exception.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple models (GPT-3.5 Turbo, GPT-4 Turbo, GPT-4o, Gemini Pro, Gemini Advanced, Gemini Flash, Gemini Goldfish, Claude 3 Opus, Claude 3.5 Sonnet; exception: Gemini Nano)",
            "model_description": "State-of-the-art large language models and multimodal vision-language models from OpenAI, Google (Gemini family), and Anthropic (Claude); architectures are transformer-based LLMs and multimodal extensions.",
            "model_size": null,
            "test_name": "Working Memory Index (WMI)",
            "test_category": "working memory / short-term storage and manipulation",
            "test_description": "WAIS‑IV composite measuring ability to temporarily store, retrieve, and manipulate arbitrary information; composed of subtests including Digit Span, Arithmetic, and Letter-Number Sequencing, administered via text-adapted prompts.",
            "evaluation_metric": "WAIS‑IV standard scores and normative percentiles (age-normed, selected norms for ages 25–29).",
            "human_performance": "Human baseline: WAIS‑IV normative sample (age 25–29) used for conversion to standard scores and percentiles (paper uses WAIS‑IV norms as the baseline). Typical population median corresponds to the 50th percentile in these norms.",
            "llm_performance": "Most models scored in the Very Superior range on WMI (reported range ~95th -&gt; &gt;99.9th percentile); the paper states WMI performance &gt;= 99.5th percentile for many models; exception Gemini Nano at 37th percentile.",
            "prompting_method": "Instruction-style, zero-shot prompts adapted from WAIS‑IV administration instructions (text conversion of items); prompts included the manual instructions and examples.",
            "fine_tuned": false,
            "human_data_source": "Wechsler, 2008 (WAIS‑IV) norms; age 25–29 norms selected by the authors.",
            "statistical_significance": "Authors report statistically significant stronger WMI relative to other indices (noted significance levels p &lt; .15 or p &lt; .05 for comparisons to normative expectations and other indices).",
            "notes": "WMI advantage reflects token storage/manipulation strengths (e.g., Digit Span ceiling performance). Adaptations to text prompts may advantage models; PSI subtests omitted; FSIQ not computed because PSI omitted.",
            "uuid": "e7238.0",
            "source_info": {
                "paper_title": "The Cognitive Capabilities of Generative AI: A Comparative Analysis with Human Benchmarks",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "VCI_overview",
            "name_full": "Verbal Comprehension Index (WAIS-IV) — aggregated model performance",
            "brief_description": "Aggregate report of generative models' performance on the WAIS‑IV Verbal Comprehension Index (VCI), showing very high performance overall but variability by model and subtest.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple models (GPT-3.5 Turbo, GPT-4 Turbo, GPT-4o, Gemini Pro, Gemini Advanced, Gemini Flash, Gemini Goldfish, Claude 3 Opus, Claude 3.5 Sonnet; exceptions: Gemini Flash, Gemini Nano)",
            "model_description": "Transformer-based LLMs and multimodal models evaluated via text-adapted WAIS‑IV verbal subtests.",
            "model_size": null,
            "test_name": "Verbal Comprehension Index (VCI)",
            "test_category": "verbal knowledge, language comprehension, reasoning",
            "test_description": "WAIS‑IV composite assessing verbal knowledge and reasoning through subtests such as Similarities, Vocabulary, Information, and Comprehension; administered via text prompts adapted from the WAIS manual.",
            "evaluation_metric": "WAIS‑IV standard scores and normative percentiles (age-normed, age 25–29).",
            "human_performance": "Human baseline: WAIS‑IV normative sample (age 25–29) used; population median represents the 50th percentile.",
            "llm_performance": "Most models scored in the Very Superior range on VCI (reported at or above the 98th percentile); exceptions include Gemini Flash at 82nd percentile (Superior) and Gemini Nano at 23rd percentile (Borderline).",
            "prompting_method": "Instruction-style, zero-shot prompts adapted from WAIS‑IV subtest instructions (examples included where present).",
            "fine_tuned": false,
            "human_data_source": "Wechsler, 2008 (WAIS‑IV) norms; age 25–29 norms selected by the authors.",
            "statistical_significance": "Authors report significant VCI–PRI discrepancies (e.g., VCI–PRI differences reaching p &lt; .05 for many models) and various VCI–WMI comparisons noted at p &lt; .15 or p &lt; .05.",
            "notes": "Within VCI, models were strongest on Information (crystallized knowledge retrieval; reported range 99.6th–99.9th percentiles) and relatively weaker on Similarities and Vocabulary for some smaller/older models, indicating retrieval &gt; conceptual/analogical verbal reasoning.",
            "uuid": "e7238.1",
            "source_info": {
                "paper_title": "The Cognitive Capabilities of Generative AI: A Comparative Analysis with Human Benchmarks",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "PRI_overview",
            "name_full": "Perceptual Reasoning Index (WAIS-IV) — aggregated multimodal model performance",
            "brief_description": "Aggregated performance of multimodal models on the WAIS‑IV Perceptual Reasoning Index (PRI), showing pervasive deficits on visual/perceptual reasoning relative to human norms.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multimodal models (GPT-4 Turbo, GPT-4o, Gemini Flash, Gemini Goldfish, Claude 3 Opus, Claude 3.5 Sonnet)",
            "model_description": "Multimodal transformer-based models with image-and-text capabilities evaluated on WAIS‑IV perceptual reasoning subtests converted to text+image prompts.",
            "model_size": null,
            "test_name": "Perceptual Reasoning Index (PRI)",
            "test_category": "perceptual reasoning, visuospatial processing, nonverbal abstract reasoning",
            "test_description": "WAIS‑IV composite assessing visuospatial skills and nonverbal abstract reasoning via subtests like Matrix Reasoning, Visual Puzzles, Figure Weights, and Picture Completion; administered to multimodal models only (image stimuli presented where applicable).",
            "evaluation_metric": "WAIS‑IV standard scores and normative percentiles (age-normed).",
            "human_performance": "Human baseline: WAIS‑IV normative sample (age 25–29) used for percentile conversion; population median ~50th percentile.",
            "llm_performance": "Overall poor: majority of multimodal models performed in the Extremely Low range (many &lt;1st–2nd percentile); reported PRI range across models ~0.1th–10th percentile; Claude 3.5 Sonnet was the best at ~10th percentile (Low Average/Borderline).",
            "prompting_method": "Text+image prompts adapted from WAIS‑IV visual subtests; visuals presented for multimodal models; administration text followed manual instructions.",
            "fine_tuned": false,
            "human_data_source": "Wechsler, 2008 (WAIS‑IV) norms; age 25–29 norms selected by the authors.",
            "statistical_significance": "PRI performance was significantly worse than VCI and WMI (authors report PRI vs other indices significance at p &lt; .05 across multimodal models).",
            "notes": "Authors substituted Figure Weights for Block Design (block manipulation unavailable). Visual tasks reveal large and consistent deficits indicating inability to interpret and reason on visual information despite multimodality; some progress observed across model generations (e.g., Claude 3.5 Sonnet improvements).",
            "uuid": "e7238.2",
            "source_info": {
                "paper_title": "The Cognitive Capabilities of Generative AI: A Comparative Analysis with Human Benchmarks",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Information_subtest",
            "name_full": "WAIS‑IV Information subtest — crystallized knowledge retrieval",
            "brief_description": "Performance of models on the Information subtest, indexing stored general knowledge (crystallized knowledge) with near-ceiling results reported for most models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple models (listed in paper; most models tested)",
            "model_description": "Transformer-based LLMs evaluated using text prompts of WAIS‑IV Information items.",
            "model_size": null,
            "test_name": "Information (WAIS‑IV subtest)",
            "test_category": "crystallized knowledge / general knowledge retrieval",
            "test_description": "Subjects answer broad general-knowledge questions; in study items were converted to text prompts and scored per WAIS‑IV criteria.",
            "evaluation_metric": "WAIS‑IV scaled scores and normative percentiles.",
            "human_performance": "Human baseline: WAIS‑IV norms used; the normative percentiles indicate population distribution with 50th percentile as median.",
            "llm_performance": "Near-perfect scores reported across multiple models; authors report Information subtest percentiles in the range ~99.6th–99.9th percentile for many models.",
            "prompting_method": "Instruction-style, zero-shot prompts following WAIS‑IV item wording.",
            "fine_tuned": false,
            "human_data_source": "Wechsler, 2008 (WAIS‑IV) norms.",
            "statistical_significance": "Authors report multiple models show significantly higher Information scores compared to other VCI subtests (significance levels reported in Table 4).",
            "notes": "Interpretation: models excel at retrieving encoded factual knowledge (crystallized knowledge) relative to human norms; this likely reflects pretraining data content.",
            "uuid": "e7238.3",
            "source_info": {
                "paper_title": "The Cognitive Capabilities of Generative AI: A Comparative Analysis with Human Benchmarks",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Digit_Span",
            "name_full": "Digit Span (WAIS‑IV subtest) — forwards/backwards/sequencing",
            "brief_description": "Models achieved near-ceiling Digit Span performance (encoding and simple retrieval of digit sequences) whereas manipulative conditions (backwards/sequencing) showed variability, especially for small models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple models (GPT-3.5 Turbo, GPT-4 variants, Gemini family, Claude models); exception: Gemini Nano",
            "model_description": "LLMs and multimodal models tested on Digit Span subtests via text prompts representing sequences of digits to be recalled or manipulated.",
            "model_size": null,
            "test_name": "Digit Span (Forwards, Backwards, Sequencing)",
            "test_category": "working memory (verbal/auditory span and manipulation)",
            "test_description": "Subjects repeat sequences of digits forward, backward, or in sequence; here prompts provided digit sequences and responses scored per WAIS‑IV. Longest spans recorded and converted to age-normed base rates.",
            "evaluation_metric": "Raw longest span and WAIS‑IV base rates / percentiles (age 25–29 where applicable).",
            "human_performance": "Human baseline: WAIS‑IV normative base rates reported (Table 5); e.g., Longest Digit Span Forwards (LDSF) base rate (age 25–29) = 17.5% for the corresponding raw score; other base rates reported in Table 5.",
            "llm_performance": "Most models scored perfect or near-perfect Digit Span (e.g., many reported raw scaled scores of 19 and percentiles ~99.9); exception Gemini Nano showed substantially lower/manipulation-limited performance (e.g., Digit Span Backwards at ~2nd percentile in one reported case).",
            "prompting_method": "Zero-shot text prompts replicating WAIS‑IV Digit Span instructions and digit sequences.",
            "fine_tuned": false,
            "human_data_source": "Wechsler, 2008 (WAIS‑IV) norms; specific base rates and age-normed percentages reported in Table 5 of the paper.",
            "statistical_significance": "Authors report discrepancy analyses and base rates for Digit Span comparisons (scale-level comparisons shown in Table 3 with reported critical values and base rates; e.g., Digit Span total vs Arithmetic differences reached significance in some models).",
            "notes": "Authors note models excel at encoding/retrieval of token sequences but smaller models (Gemini Nano) struggle with manipulation tasks (backwards/sequencing), indicating limits on working-memory-like manipulation despite high forward-span performance. Adaptation to text may advantage models.",
            "uuid": "e7238.4",
            "source_info": {
                "paper_title": "The Cognitive Capabilities of Generative AI: A Comparative Analysis with Human Benchmarks",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Arithmetic_subtest",
            "name_full": "WAIS‑IV Arithmetic subtest — mental arithmetic word problems",
            "brief_description": "Reported relative weakness on Arithmetic for several models compared to Digit Span and other WMI components, with lower percentiles for arithmetic reasoning despite strong digit storage.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple models (noted weaker for Gemini Nano; mixed across others)",
            "model_description": "LLMs evaluated on WAIS‑IV Arithmetic word problems converted into text prompts and scored per WAIS criteria.",
            "model_size": null,
            "test_name": "Arithmetic (WAIS‑IV subtest)",
            "test_category": "numerical reasoning / working memory / mental calculation",
            "test_description": "Timed mental arithmetic word problems of increasing difficulty administered without paper; here presented as text prompts and scored according to WAIS‑IV.",
            "evaluation_metric": "WAIS‑IV scaled scores and normative percentiles.",
            "human_performance": "Human baseline: WAIS‑IV normative percentiles used (age 25–29 norms).",
            "llm_performance": "Reported relative weakness in Arithmetic compared to Digit Span; authors report lower scaled scores and percentiles for Arithmetic in several models (examples: some models scored at percentiles much lower than their Digit Span percentiles; specific numeric examples in Table 2 show variation).",
            "prompting_method": "Instruction-style, zero-shot text prompts following WAIS‑IV Arithmetic items.",
            "fine_tuned": false,
            "human_data_source": "Wechsler, 2008 (WAIS‑IV) norms.",
            "statistical_significance": "Authors report scale-level comparisons where Digit Span total minus Arithmetic differences reached significance for several models (see Table 3; critical values and base rates provided).",
            "notes": "Interpreted as models being strong at storing numeric tokens but relatively weaker at multi-step mental arithmetic reasoning without external calculation aids; smaller models (Gemini Nano) particularly weak.",
            "uuid": "e7238.5",
            "source_info": {
                "paper_title": "The Cognitive Capabilities of Generative AI: A Comparative Analysis with Human Benchmarks",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "MatrixReasoning_Claude_improvement",
            "name_full": "Matrix Reasoning (WAIS‑IV) — Claude 3 Opus → Claude 3.5 Sonnet improvement",
            "brief_description": "Reported generational improvement in Matrix Reasoning and Figure Weights for Anthropic's Claude series: substantial percentile increase from Claude 3 Opus to Claude 3.5 Sonnet.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Claude 3 Opus → Claude 3.5 Sonnet",
            "model_description": "Anthropic multimodal models optimized for reasoning; both evaluated on WAIS‑IV perceptual reasoning subtests with image+text prompts.",
            "model_size": null,
            "test_name": "Matrix Reasoning (WAIS‑IV subtest) and Figure Weights",
            "test_category": "nonverbal abstract reasoning (pattern detection) and quantitative analogical reasoning on visual stimuli",
            "test_description": "Matrix Reasoning: select image completing a pattern; Figure Weights: balance analog-scale visual weights; items were converted to multimodal prompts and scored per WAIS‑IV.",
            "evaluation_metric": "WAIS‑IV scaled scores and percentiles (age-normed).",
            "human_performance": "Human baseline: WAIS‑IV normative percentiles (age 25–29) used for comparison.",
            "llm_performance": "Claude 3 Opus: Matrix Reasoning ~0.1th percentile; Claude 3.5 Sonnet: Matrix Reasoning improved to ~25th percentile. For Figure Weights, improvement reported from ~0.1th percentile (Opus) to ~50th percentile (Sonnet) in one reported comparison.",
            "prompting_method": "Multimodal prompts (image + instructions from WAIS‑IV manual) presented to models.",
            "fine_tuned": false,
            "human_data_source": "Wechsler, 2008 (WAIS‑IV) norms.",
            "statistical_significance": "Authors emphasize measurable improvements across generations but do not report formal p-values for this specific pairwise generation comparison beyond percentile changes; PRI vs other indices significance reported elsewhere (p &lt; .05).",
            "notes": "These improvements indicate perceptual reasoning capabilities can be acquired/improved with model development, but large gaps remain versus language-domain performance. Exact numeric values come from comparisons reported in the text.",
            "uuid": "e7238.6",
            "source_info": {
                "paper_title": "The Cognitive Capabilities of Generative AI: A Comparative Analysis with Human Benchmarks",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Evidence of interrelated cognitive-like capabilities in large language models: Indications of artificial general intelligence or achievement?",
            "rating": 2,
            "sanitized_title": "evidence_of_interrelated_cognitivelike_capabilities_in_large_language_models_indications_of_artificial_general_intelligence_or_achievement"
        },
        {
            "paper_title": "What is the visual cognition gap between humans and multimodal llms?",
            "rating": 2,
            "sanitized_title": "what_is_the_visual_cognition_gap_between_humans_and_multimodal_llms"
        },
        {
            "paper_title": "Efficiently measuring the cognitive ability of llms: An adaptive testing perspective.",
            "rating": 1,
            "sanitized_title": "efficiently_measuring_the_cognitive_ability_of_llms_an_adaptive_testing_perspective"
        },
        {
            "paper_title": "WAIS-IV Scoring and Administration Manual",
            "rating": 2,
            "sanitized_title": "waisiv_scoring_and_administration_manual"
        }
    ],
    "cost": 0.0163335,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>The Cognitive Capabilities of Generative AI: A Comparative Analysis with Human Benchmarks
9 Oct 2024</p>
<p>Isaac Galatzer-Levy 
University of Washington School of Medicine</p>
<p>David Munday 
University of Washington School of Medicine</p>
<p>Jed Mcgiffin 
University of Washington School of Medicine</p>
<p>Xin Liu 
University of Washington School of Medicine</p>
<p>Danny Karmon 
University of Washington School of Medicine</p>
<p>Ilia Labzovsky 
University of Washington School of Medicine</p>
<p>Rivka Moroshko 
University of Washington School of Medicine</p>
<p>Amir Zait 
University of Washington School of Medicine</p>
<p>Daniel Mcduff 
University of Washington School of Medicine</p>
<p>† Corresponding 
University of Washington School of Medicine</p>
<p>Google Research 
University of Washington School of Medicine</p>
<p>Google Deepmind 
University of Washington School of Medicine</p>
<p>The Cognitive Capabilities of Generative AI: A Comparative Analysis with Human Benchmarks
9 Oct 20240BA24A4363D2B4C8DF79860A98AC59D2arXiv:2410.07391v1[cs.AI]
There is increasing interest in tracking the capabilities of general intelligence foundation models.This study benchmarks leading large language models and vision language models against human performance on the Wechsler Adult Intelligence Scale (WAIS-IV), a comprehensive, population-normed assessment of underlying human cognition and intellectual abilities, with a focus on the domains of Verbal Comprehension (VCI), Working Memory (WMI), and Perceptual Reasoning (PRI).Most models demonstrated exceptional capabilities in the storage, retrieval, and manipulation of tokens such as arbitrary sequences of letters and numbers, with performance on the Working Memory Index (WMI) greater or equal to the 99.5 ℎ percentile when compared to human population normative ability.Performance on the Verbal Comprehension Index (VCI) which measures retrieval of acquired information, and linguistic understanding about the meaning of words and their relationships to each other, also demonstrated consistent performance at or above the 98 ℎ percentile.Despite these broad strengths, we observed consistently poor performance on the Perceptual Reasoning Index (PRI; range 0.1-10 ℎ percentile) from multimodal models indicating profound inability to interpret and reason on visual information.Smaller and older model versions consistently performed worse, indicating that training data, parameter count, and advances in tuning are resulting in significant advances in cognitive ability.</p>
<p>Introduction</p>
<p>Generative artificial intelligence (GenAI) refers to a class of models capable of creating new content, whether it is text, images, music, or even code (Gardner et al., 2023;Ouyang et al., 2023;Team et al., 2023;Wu et al., 2023).Unlike traditional AI systems designed for specific tasks, these models learn the underlying patterns and structures within vast datasets and then use this knowledge to generate novel outputs that often mimic human creativity (Nath et al., 2024;Zhao et al., 2023).The excitement surrounding GenAI stems from its potential to revolutionize numerous fields by performing human-like cognitive functions (Wang et al., 2024;Zhang et al., 2024;Zhuang et al., 2023).This ability to understand, learn, adapt, and create in a way that mirrors our own thought processes opens doors to groundbreaking applications across industries like art, design, research, and communication (Ge et al., 2024;Ko et al., 2023;Lu et al., 2024;Yang et al., 2024).However, human cognition encompasses a vast array of specialized abilities in the processing, storage, interpretation, and generation of information across auditory and visual channels to accomplish these unique human capabilities (Cao et al., 2024;Subramonyam et al., 2023).</p>
<p>Recent advances in generative AI have been driven by large parameter models (e.g., the GPT (Achiam et al., 2023), Gemini (Team et al., 2023) families) trained with a broad range of textual content.Scaling experiments have shown that testing losses often follow power law relationships with data, model size (number of parameters) and compute (Kaplan et al., 2020).This progress has extended to multimodal models that learn complex cross-modal relationships between representations such as language and visual media.The capabilities of these models has sparked debate about the Figure 1 | Benchmarking Models Against Human Performance on the Wechsler Adult Intelligence Scale.We perform a comprehensive, population-normed assessment of AI models against underlying human cognition and intellectual abilities, with a focus on the domains of Verbal Comprehension (VCI), Working Memory (WMI), andPerceptual Reasoning (PRI).potential impact of AI and proximity to artificial general intelligence (Bengio et al., 2024).The rapid progress increases the need for careful assessment of performance.</p>
<p>To assess their potential and limitations as models of human cognition, it is crucial to characterize the cognitive capabilities of generative models and understand how they compare directly to human ability (Ilić and Gignac, 2024).This will facilitate an understanding of pragmatic capabilities, limitations, and an ability to track progress just as is done with human normative cognitive tests of ability.It will also provide insight into unique streams of cognitive development of generative artificial intelligence under the assumption that models, like biological organisms, will develop unique capabilities based on their underlying neural architecture and environmental demands.</p>
<p>General cognitive capabilities have been systematically researched and benchmarked in humans for over a century (Spearman, 1961;Sternberg et al., 1981;Weiss et al., 2013).Refined standardized tests of ability and performance that are normed in large representative population-based samples allow for individual performance scores to be compared directly to normative performance.Because tests such as the Wechsler Adult Intelligence Scale-Fourth Edition (WAIS-IV; (Wechsler, 2008)) have population norms for comparison, individual scores can be used to make decisions about relative ability, and can be used to inform the allocation of special resources for education, housing, care, or competency.They can also be utilized to identify exceptional ability and gaps between areas of performance that are diagnostic of specific neurocognitive disorders (e.g., Autism Spectrum Disorder, Dyslexia; (Neisser et al., 1996)).</p>
<p>Beyond the focus on comparison to human performance, there may be value in the development of specific tests of GenAI cognition across discernible domains that are analogous to human tests akin to the development of analogous tests of cognition in animal models.Indeed, models of cognitive constructs such as working memory, semantic understanding, and visual processing, have facilitated research to understand the underlying neural architecture and mechanisms of cognition.Similar to the relative capabilities of the visual cortex of an hawk compared to a human -both display impressive but highly specialized abilities -we may find that GenAI models, by virtue of their architecture and context, develop novel patterns in cognitive functioning that prove to be quite different from humans, where, for example, consistent non-negligible positive correlations have been demonstrated between many cognitive ability test scores (Detterman and Daniel, 1989;Walker et al., 2023).This phenomenon, which is known as the Positive Manifold (Jensen, 1998), has been demonstrated to hold for LLMs using acceptable, yet non-validated, subtest proxies of the VCI and WMI cognitive ability tests (Ilić and Gignac, 2024).</p>
<p>We compared the performance of GenAI models on tests of cognitive performance that are commonly used to index human intellectual ability.We focused on tests of verbal ability including linguistic-based knowledge and conceptual understanding, capabilities to temporarily store, retrieve and manipulate arbitrary information, and capabilities to understand, reason, and manipulate visual information.Both individual test scores and composite index scores provide nuanced and interpretable information about intellectual capabilities and both were used to evaluate and better understand the capabilities of generative models.</p>
<p>Methods</p>
<p>To facilitate the assessment of GenAI models using the Wechsler Adult Intelligence Scale, Fourth Edition (WAIS-IV), we implemented a series of methodological adaptations to accommodate the unique input and output modalities of these models.This involved converting traditional verbal and visual stimuli into text-based prompts (see Appendix B) and interpreting model-generated text outputs as responses to test items.Specific adaptations for each subtest, along with validation procedures, are detailed in 2.1.For this study, we selected a representative set of state-of-the-art (SOTA) large language models (LLMs) and vision language models (VLMs), encompassing a range of model sizes, architectures, and training datasets (See section 2.2).</p>
<p>Testing materials, administration, and scoring</p>
<p>Individual items from WAIS-IV subtests (Wechsler, 2008) were converted to prompts to be individually administered to the selected LLM (See Table 1 for a full list of tests with descriptions).Specifically, tasks comprising the verbal comprehension and working memory indices (VCI and WMI) were converted to language-based prompts and administered to all models including those with languageonly capabilities as well as those with multimodal language and visual capabilities.Tests from the perceptual reasoning index (PRI) were only administered to multimodal models because they require both image recognition and language capabilities.We were unable to administer the Block Design subtest from the PRI, because it requires manual manipulation of real-world objects (cubes).However, valid PRI composite scores were still calculable substituting the alternate PRI subtest Figure Weights (Wechsler, 2008).Subtests from the Processing Speed Index (PSI) were not administered as there was no clear way to maintain fidelity to the WAIS-IV testing procedures that are required for valid comparison to human performance norms.A full-scale IQ score (FSIQ) could not be calculated for any model as PSI is a required component of FSIQ.</p>
<p>In each instance, the prompt presented to the model consisted of the instructions as outlined in the manual (Wechsler, 2008), including the example provided as part of the administration process.</p>
<p>However, certain adaptations were necessary to accommodate the specific requirements of the model.Several tests involved reading the test content, while others utilized visual and physical aids (e.g., cards and boards).These tests were converted into prompts, and in some cases, the translation provided the GenAI models with an advantage due to their ability to access the full context while generating responses.Additionally, phrases such as "You can only ask me to read the problem one more time" were omitted, as GenAI models are trained on instructions and often request repetition without responding to the actual assessment being conducted.Phrases such as "Just say what I say" or "Listen" were not removed and were left as they were originally presented, causing some models to mistakenly provide responses such as "I am a text-based chat assistant and thus I cannot hear or repeat the numbers."and "Okay, I'm listening!.".No further instructions or inquiries were provided.No additional instructions or queries were provided.</p>
<p>Each subtest consists of progressively challenging questions whose difficulty-level is normed against the population.For example, the Digit Span test, in which subjects are requested to verbally repeat a sequence of digits read aloud to them, begins with a sequence of three digits and progresses to a sequence of eight or nine digits depending on the subtest.Primary WAIS-IV composite indices and related cognitive domains are described below, along with brief descriptions of each subtest and administration procedures (See Table 1).</p>
<p>Verbal Comprehension Index</p>
<p>These tests assess verbal knowledge and reasoning capacity.Subtests include: Similarities, in which the subject must state how two words are alike (e.g., "How are a kite and an airplane alike?");Vocabulary, in which the subject provides definitions of words; Information in which the subject answers questions on a broad set of topics (e.g., "Who was the first president of the United States?"); and Comprehension in which subjects are asked to reason on common sense narrative scenarios.</p>
<p>Working Memory Index These subtests probe the capacity to store, manipulate, and reorder information, including reasoning numerically.Digit Span requires subjects to repeat back random number strings according to varying rules (i.e., Forwards, Backwards, and Sequencing conditions).Arithmetic requires subjects to solve math word problems of increasing difficulty without pen and paper.Letter-Number Sequencing requires subjects to manipulate arbitrary letter-number strings into alphabetic and numerical order.</p>
<p>Perceptual Reasoning Index Involves tests of visuospatial skills and nonverbal abstract reasoning.Matrix Reasoning (Ex: Fig. 2) requires subjects to choose the correct image from multiple options to successfully complete a visual pattern.Visual Puzzles (Ex: Fig. 3) requires subjects to select the correct tile to complete a picture, assessing mental rotation and spatial relations ability.In Figure Weights (Ex: Fig. 4), subjects are presented with an analogue scale depicting shapes of relative weight (e.g., two blue squares equals one red triangle).The subject is presented with a balanced scale demonstrating the weight relationship between objects, and an incomplete scale with shapes on only one side.They must then select from multiple choices to balance the incomplete scale.Picture Completion is a test in which subjects are presented with an image that is missing some key characteristic (e.g., an image of a woman without a shadow standing next to a sign that is casting a shadow).The subject must verbally indicate what is missing in the image.</p>
<p>All answers to prompts were scored by one of two clinical psychologists trained to administer and score the WAIS-IV.Any ambiguous results were reviewed by both psychologists to reach a consensus on scoring.Individual test scores were collated into raw index scores and then converted to age-normed scores with accompanying performance percentiles.As different norms exist for different age ranges, we selected the norms for age 25 years to 29 years and 11 months.In addition to the calculation of composite indices, we also compared individual subtests to domain-specific averages and composite scores in a series of discrepancy analyses, in order to probe the relative strengths and weaknesses of the models.The WAIS-IV provides standardized scoring for a variety of such comparisons, to determine if performance on an individual subtest might represent a statistically significant deviation from what would be expected in the population of those with similar performance profiles.</p>
<p>Models</p>
<p>Language only models included OpenAI's GPT-3.5 Turbo, Google's Gemini Nano, Gemini Pro, and Gemini Advanced.Multimodal models included OpenAI's GPT-4 Turbo, GPT-4o, Google's Gemini Flash, Gemini Goldfish and Anthropic's Claude 3 Opus, Claude 3.5 Sonnet.While model characteristics including the underlying training data, number of parameters, or internal tuning approach in the model are not publically available, key characteristics of the models are known that can be informative about factors that influence performance.First, we are able to compare progressive versioning of models to qualitatively compare performance.As an example, GPT-3.5 Turbo, Gemini Pro, and Claude 3 Opus are all earlier versions when compared to GPT-4 Turbo, GPT-4o, Gemini Advanced, and Claude 3.5 Sonnet.Further, Gemini Flash and Gemini Nano are intentionally designed as smaller models to be hosted on hardware chips rather than in cloud computing where the models can be much larger.Finally, Claude models indicate that they optimize for advanced reasoning.While all of these pieces of information are limited due to the lack of public disclosure, they do allow us to determine if 1) larger parameter versus smaller parameter models perform better; 2) models intentionally optimized for reasoning do improve reasoning capabilities.</p>
<p>Results</p>
<p>Index level results (VCI, WMI, PRI) demonstrate both exceptional capabilities and significant deficits when compared to a normative population (See Table 2 for full results).All but one model demonstrated abilities on the VCI in the Very Superior range, scoring within the top percentile of human normative ability with the exception of Gemini Flash which fell in the Superior range (82  %ile) and Gemini Nano (23  %ile) which demonstrated ability in the Borderline range.The majority of models also performed in the Very Superior range on the WMI, with the exception of Gemini Nano (37 ℎ %ile; Low Average).However, among the models with multimodal capabilities, performance on the PRI demonstrated a significant and consistent deficit with all models demonstrating capabilities in the Extremely Low range (&lt; 1  %ile) with the lone exception of Claude Sonnet which demonstrated ability in the Low Average range (10 ℎ %tile).</p>
<p>Next, we analyzed individual differences in index-level results to understand relative strengths and weaknesses within domains of intellectual ability (See Table 3 for full Results).First, we observed a consistent relative strength in WMI compared to other indexes representing a statistically significant difference compared to the normative population (p&lt;.15 or p&lt;.05 level).Overall,this indicates that models' generally evidence stronger ability to process, store and manipulate information compared to any other ability, including language.Further, all models with visual/multimodal capability to perform tests comprising the PRI demonstrated significantly worse perceptual reasoning performance p&lt;.05) when compared verbal comprehension and working memory indexes.This indicates a general relative weakness that is invariant across distinct developers and models in the ability to understand and reason on visual information.</p>
<p>Next, we examined performance on individual tests within each index (See Table 3 and Table 4).Within the WMI, a consistent relative weakness in Arithmetic, was observed in comparison to Digit Span, indicating a relative weakness in mathematical reasoning compared to the ability to encode, manipulate, and reorganize numeric values.Gemini Nano demonstrated further relative weaknesses in the ability to sequence and reverse numbers compared to the ability to encode and retrieve simple numeric lists.Analysis of the VCI demonstrated consistent relative strengths across models from independent developers, on the Information subtest compared to Similarities and Vocabulary (p&lt;.05 and p&lt;.15).This indicates that models are particularly strong in the storage and retrieval of natural language-encoded knowledge -often referred to as "crystallized knowledge" in human subjects -in comparison to the cognitive capabilities required for verbal analogic and verbal abstract reasoning and semantic understanding.This difference in relative performance persisted across generations of models indicating that models are consistently stronger in predicting the write answer than understanding and reasoning with language concepts.Finally, within the PRI, relative differences in opposite directions between best-in class models were observed with GPT-4o demonstrating stronger relative ability in decoding Visual Puzzles and Claude Sonnet demonstrating significantly stronger relative performance in Matrix Reasoning.</p>
<p>Wide variability in performance was observed on the VCI.While scores were highly skewed towards exceptional performance (99.5 ℎ %ile to &gt;99.9 ℎ %ile, models that are known to be small pa- (&gt;99.9)(&gt;99.9)(37) (99.5) (&gt;99.9)(99.5) (99.9) (&gt;99.9)(99.9)</p>
<p>Digit Span 19 (99.9) 19 (99.9) 19 (99.9) 11 ( 63) 19 (99.9) 19 (99.9) 19 (99.9) 19 (99.9) 19 (99.9) 19 (99.9)DS Forward 18 (99.6)18 (99.6)18 (99.6)18 (99.6)18 (99.6)18 (99.6)18 (99.6)18 (99.6)18 (99.6)18 (99.6)DS Backwards 18 (99.6)16 ( 98) 19 (99.9) 4 ( 2) 19 (99.9) 19 (99.9) 19 (99.9) 19 (99.9) 19 (99.9) 19 (99.9)DS Sequencing 19 (99.9) 19 (99.9) 19 (99.9) 13 ( 84) 19 (99.9) 19 (99.9) 18 (99.6)19 (99.9) 19 (99.9) 17 (99) Arithmetic 15 (95) 19 (99.9) 19 (99.9) 7 ( 16) 15 ( 95) 19 (99.9) 15 ( 95) 17 ( 99) 19 (99.9) 17 (99) LN Sequencing 19 (99.9) 19 (99.9) 19 (99.9) 1 (0.1) 19 (99.9) 19 (99.9) 16 ( 98) 19 (99.9) 14 ( 91 rameter counts demonstrated the worst relative and absolute performance (Gemini Flash = 82  %ile; Gemini Nano = 23  %ile).Similarly, most models performed exceptionally on the WMI (Range, 95 ℎ %ile -&gt;99.9 ℎ %ile) with the exception of Gemini Nano (37 ℎ %ile).In contrast, all models tested demonstrated poor performance on the PRI (Range, &lt;.01 ℎ %ile -10 ℎ %ile), with skewed performance such that the majority of models fell below the 2  %ile.The exception is Claude Sonnet which demonstrates a significant increase over its predecessor Claude Opus indicating that while performance overall is poor, there is measurable progress in the ability for generative models to reason on visual concepts.</p>
<p>Test level analyses demonstrate statistically significant relative strengths and weaknesses within individual composite indices.First, all models demonstrated perfect or near perfect scores on Information indicating that prior encoding and ability to access crystalized knowledge is exceptional compared to a normative population (Range, 99.6 ℎ %ile -99.9 ℎ %ile).Next, within the VCI, multiple models demonstrated relative weaknesses in similarities (Gemini Nano, GPT 3.5, Gemini Pro; range indicating a relative difficulty reasoning on linguistic concepts compared to correctly retrieving crystallized linguistic knowledge.Gemini Nano and Gemini Flash both demonstrated relative weaknesses in vocabulary indicating these models struggle to understand language relative to other models.Gemini Nano also demonstrates lower scores in Comprehension when compared to Information, reinforcing that while this model has encoded and can retrieve crystallized knowledge well, it struggles to demonstrate human-level reasoning with linguistic information.</p>
<p>With regard to the WMI, models again performed exceptionally well with all demonstrating perfect scores in Digit Span with the exception of Gemini Nano which demonstrated a relatively poorer performance that was consistent with average human performance.Both Arithmetic and Letter Number Sequencing represented relative weaknesses for Gemini Nano.Further, when comparing  subtests of Digit Span, we observed a relative weakness in Digit Span Backwards when compared to Digit Span Forward.Once again, Nano performed inconsistently, demonstrating abilities in digit span forward that were in the 99.6 ℎ %ile) while failing at manipulating similar numeric strings (Digit Span Backwards 2  %ile).Further, Nano was unable to correctly perform any Letter-Number sequencing tasks (see Table 5 for complete results).Taken together, results indicate that Nano is proficient in encoding and retrieving the correct information but performs poorly when tasked with manipulating information, a key attribute of working memory.
0 2 -1 14<strong> -1 -1 -1 -1 -1 -1 Base Rate - - - &lt;0.10% - - - - - - (Critical Value, Significance) 3.65</strong> DS Forward-DS Sequence -1 -1 -1 5** -1 -1 0 -1 -1 -1 Base Rate - - - 8.5% - - - - - - (Critical
Finally the PRI was assessed in the subset of multimodal models (GPT-4 Turbo, GPT-4o, Gemini Flash, Gemini Goldfish, Claude 3 Opus, Claude 3.5 Sonnet) demonstrating consistent performance in the Extremely Low range (&lt; 0.2  % ile).The best performing model was Claude 3.5 Sonnet which fell in the 10 ℎ %ile, consistent with Borderline performance in this domain.The Figure Weights test was a relative strength compared to other tests (Visual Puzzles, Matrix Reasoning) indicating a relative strength in analytical reasoning compared to pattern recognition or spatial reasoning.Of note, and despite overall poor performance, Claude 3.5 Sonnet demonstrated dramatic improvements over Claude 3 Opus in Matrix Reasoning and Figure Weights indicating that while performance in these domains lag behind language-based domains, models can be trained and optimized to acquire perceptual reasoning capabilities.</p>
<p>Discussion</p>
<p>Results demonstrate that Generative AI models are capable of exceptional performance compared to normative human ability in key domains of cognition.First, and perhaps not surprising, all models demonstrate exceptional capabilities in storage, retrieval, and manipulation of arbitrary tokenized information such as sequences of numbers and letters while being significantly weaker in mathematical reasoning.While poor performance was most pronounced in small parameter models, the discrepancy between reasoning and information management persisted across model generations and developers indicating a generally stable discrepancy in ability.</p>
<p>Generative models also demonstrated exceptional verbal abilities, with variability in performance according to the size of the model.Once again, models were strongest in retrieval of stored information with consistent relative weaknesses in tasks that require understanding of linguistic concepts or the relationships between words and concepts.However, with the exception of Gemini Nano, all models demonstrated understanding as well as crystalized knowledge well above normative ability indicating that models generally excel in language-based tasks, even those requiring reasoning and understanding beyond simple regurgitation of acquired knowledge.In the case of small parameter models, they may be best for storing and retrieving information naturalistically (e.g., in the context of hardware constraints) but may not be capable of understanding or manipulating that information to solve a problem.</p>
<p>Finally, the dramatically poorer performance on visual processing tasks indicates that generative models, as they stand today, have profound deficits in the ability to understand the meaning or There is some indication that current modeling approaches can lead to the acquisition of these abilities as Claude 3.5 Sonnet demonstrated profound increases over the previous generation (Claude 3 Opus).Claude 3.5 Sonnet showed advances over its predecessor in Matrix Reasoning (0.1 ℎ %ile vs. 25 ℎ %ile), which measures the ability to detect meaningful patterns in visual stimuli and Figure Weights which indexes the ability to understand and reason on mathematical relationships that are visually presented (0.1 ℎ %ile vs. 50 ℎ %ile).No such improvement was observed in visual puzzle solving or image completion tasks that test the ability to understand and make sense of visual information.</p>
<p>Given the relative complexity of visual information processing compared to auditory processing in both humans and other vertebrates that rely on complex visual capabilities, the gap between visual and auditory cognitive capabilities in generative AI might not be addressed through incremental improvements in architecture or model training but may require separate specialized architecture for visual and auditory processing with enhanced interaction capabilities, as is the case with vertebrates.</p>
<p>The above results clearly demonstrate that for current SoTA models the Positive Manifold, which posits positive correlations between different cognitive capabilities, holds when VCI and WMI are considered (as demonstrated in Ilić and Gignac (2024), and fails to hold for when including PRI as well.</p>
<p>The current work presents with the significant limitations that the model parameters themselves (underlying training data, parameter count, tuning approach) are all proprietary across all examples and thus not available for comparative analysis.While this does not limit the validity of the tests as these parameters are not known with human subjects either, it does limit the ability to draw definitive conclusions about factors that influence performance.This hampers scientific inquiry into GenAI models as model organisms to understand the acquisition of cognitive capabilities.The study is further limited by the inherently non-standard approach to WAIS-IV administration.While we attempted to copy key testing conditions and excluded tests that could not conform, there is an inherent limitation in the difference in testing setup from that which the scores were normed on.Despite these limitations, the current work represents the first of its kind approach to benchmark GenAI against human norms of intelligence.The results demonstrate the unique cognitive capabilities of current generative AI models as well as relative weaknesses, while providing a path to advance discrete domains of cognition that underlie wide sets of human cognitive abilities.</p>
<p>A. Broader Impact</p>
<p>The development of benchmarks help to assess the performance of foundation models.Capable models need to be developed responsibility and with attention to their strengths and flaws.Leveraging knowledge and tools from disciples such as clinical and neuro-psychology has allowed us to develop a set of grounded tasks that shed insight on the functioning of LLMs that are complementary to existing public benchmarks.However, we must acknowledge that these tasks were designed specifically for evaluating human cognitive functioning and therefore extrapolation of the results to performance on more mundane, real-world tasks and conclusions that compare language model abilities to human cognitive functioning need to be treated with care.</p>
<p>B. Prompt Examples</p>
<p>C. Visual Tasks Examples</p>
<p>Figure 2 | Example of the Matrix Reasoning test in which subjects are requested to identify a pattern between different rows and columns of the matrix.In this example, the first row is composed of green square, followed by a red triangle (from left to right).The second row also starts with a green square meaning that the correct option in the question mark is a red triangle.Answer: 2</p>
<p>Similarities</p>
<p>Figure WeightsYour goal is to place the item(s) that balance the scales Example: Look at this scale.It is not balanced.Choose the shape that will balance the scales.</p>
<p>Figure 3 |
3
Figure 3 | Example of the Visual Puzzles test in which subjects are requested to identify all the pieces that compose the provided design.In this example, the provided design is a three color rectangle.It can be composed by arranging rectangle (2), rectangle (1) and flipping rectangle (4), from left to right.Answer: 1,2,4.</p>
<p>Figure 4 |
4
Figure 4 | Example of the Figure Weights test in which subjects are requested to determine the correct way to balance the scales by finding the missing pieces.In this example, the scale on the left holds a single red circle, while the scale on the right holds two red circles.Considering that each red circle carries the same weight, attaining balance necessitates the addition of one red circle to the scale on the left.Answer: 1.</p>
<p>Table 1 |
1
Summary of Tests.Indices, subtests, and cognitive dimensions measured by the WAIS-IV.
Index / SubtestCognitive DimensionsAdministrationAssessedVerbal ComprehensionSimilarities Vocabulary Information ComprehensionVerbal abstract reasoning, con-cept formation Language comprehension, word knowledge General fund of knowledge, long-term memory recall sense, practical reasoning Social judgment, commonExaminee verbally explains how two words are alike. T M Examinee provides verbal definitions of words. T M Examinee answers questions covering a broad range of general knowledge areas. T M and common sense problem-solving. Examinee answers questions about social situations T MPerceptual ReasoningBlock Design Matrix Reasoning Nonverbal abstract reasoning, Visual construction, spatial rea-soning pattern recognition Visual Puzzles Mental Rotation, Spatial rea-soning, part-whole analysis reasoning Figure Weights Quantitative and analogicalExaminee manipulates blocks to reproduce visual de-signs. Examinee selects the missing element that completes a visual pattern. Examinee chooses pieces that fit together to create a complete visual image. that balances the scale. Examinee views scales and selects the response optionN/A M M MPicture Comple-Visual perception, attention toExaminee identifies the missing element from a pic-Mtiondetailture.Working Mem.Digit Span Arithmetic Letter-Num. Seq. Working memory, alphanu-Attention, working memory, mental manipulation Calculation Ability meric sequencingExaminee recalls a series of digits forward and back-ward. Examinee mentally solves arithmetic word problems. T M T M number strings. Examinee performs alphanumeric sequencing of letter-T MProcess. SpeedSymbol Search Coding CancellationVisual scanning, target discrim-ination, processing speed Graphomotor processing speed Examinee copies symbols paired with geometric shapes Examinee rapidly scans a search group and indicates whether a target symbol is present. as quickly as possible. ning target items according to a rule. Selective attention, visual scan-Examinee scans an array of visual stimuli and marksN/A N/A N/AT = Text-Only Models, M = Multi-modal (Text and Image) Models</p>
<p>Table 2 |
2
Model Scores on the WAIS-IV.Composite index standard scores, individual subtest scaled scores, and percentile rankings for each generative AI model tested.
OpenAIGoogleAnthropicGPT-3.5GPT-4GPT-4oGeminiGeminiGeminiGeminiGemini Claude 3 Claude 3.5TurboTurboNanoPro 1.0Adv.FlashGoldfishOpusSonnet(1.0 S)(1.0 M) (1.0 XL) (1.5 S)(1.5 M)WAIS-IV Comp. Idx.ScaledScaledScaledScaledScaledScaledScaledScaledScaledScaledWAIS-IV SubtestsScore (%) Score (%) Score (%) Score (%) Score (%) Score (%) Score (%) Score (%) Score (%) Score (%)Verbal Comp.14313614189132134114141138145Standard Score (%ile) (99.8)(99)(99.7)(23)(98)(99)(82)(99.7)(99)(99.9)Similarities13 (84)14 (91)15 (95)1 (0.1)8 (25)13 (84)14 (91)14 (91)13 (84)14 (91)Vocabulary19 (99.9) 15 (95)16 (98)4 (2)19 (99.9) 15 (95)6 (9)17 (99)17 (99) 19 (99.9)Information19 (99.9) 19 (99.9) 19 (99.9) 19 (99.9) 19 (99.9) 18 (99.6) 19 (99.9) 19 (99.9) 19 (99.9) 19 (99.9)Comprehension19 (99.9) 18 (99.6) 19 (99.9) 12 (75) 18 (99.6) 19 (99.9) 19 (99.9) 19 (99.9) 17 (99) 19 (99.9)Working Mem. Idx.13915015095139150139145150145Standard Score (%ile) (99.5)</p>
<p>Table 3 | Discrepancy Score Comparison Table for WAIS-IV Composite Indices.
3
Includes scale-level comparisons for Digit Span vs. Arithmetic and individual Digit Span component score comparisons.
OpenAIGoogleAnthropicIndex Level Comparisons GPT-3.5 GPT-4 GPT-4o Gemini Gemini Gemini Gemini Gemini Claude 3 Claude 3.5Turbo TurboNano Pro 1.0Adv.Flash Goldfish OpusSonnet(1.0 S) (1.0 M) (1.0 XL) (1.5 S) (1.5 M)VCI -PRI (Difference)-86<strong>83</strong>---64<strong>89</strong>88<strong>64</strong>VCI -PRI Base Rate-0.2% 0.2%---0.2%0.2%0.2%0.2%(Critical Value, Significance)8.32<strong> 8.32</strong>8.32<strong> 8.32</strong>8.32<strong>8.32</strong>VCI -WMI (Difference)4-14<strong> -9</strong>-6-7<em>-16</em><em>-25</em><em>-4-12</em><em>0VCI -WMI Base Rate-14.1% 25%-29.7% 10.5%3.0%-18.1%-(Critical Value, Significance)8.81</em><em> 8.81</em><em>6.48</em> 8.81<strong> 8.81</strong>8.81<strong>-PRI -WMI (Difference)--100</strong> -92<strong>----89</strong>-93<strong>-100</strong>-64<strong>PRI -WMI Base Rate-0.1% 0.1%---0.1%0.1%0.1%0.1%(Critical Value, Significance)8.81</strong> 8.81<strong>8.81</strong> 8.81<strong>8.81</strong>8.81<strong>Scale Level ComparisonsDigit Span Total -Arithmetic 4</strong>004<strong>4</strong>04<strong>2*02</strong>Base Rate9.4%--9.4%9.4%-9.4%29%-29%(Critical Value, Significance) 2.57<strong>2.57</strong> 2.57<strong>2.57</strong> 1.89<em>1.89</em>DS Forward-DS Backwards</p>
<p>Table 4 |
4
Verbal Comprehension vs Perceptual Reasoning.Relative strengths and weaknesses for WAIS-IV verbal comprehension and perceptual reasoning subtests.
OpenAIGoogleAnthropicVerbal SubtestsGPT-3.5 GPT-4o GPT-4 Gemini Gemini Gemini Gemini Gemini Claude 3 Claude 3.5Turbo TurboNano Pro 1.0Adv.Flash Goldfish OpusSonnet(1.0 S) (1.0 M) (1.0 XL) (1.5 S) (1.5 M)Mean VCI Subtests17.00 16.00 16.678.0015.3315.6712.6716.6716.3317.33Similarities (subtest -mean) -4.00<strong> -2.00</strong> -1.67<em> -7.00</em><em> -7.33</em><em> -1.67</em>0.33 -2.67<strong> -3.33</strong>-3.33<strong>Base Rate2%15%25%1%1%25%-5%2%2%(Critical Value, Significance)1.91</strong> 1.91<strong> 1.56* 1.91</strong> 1.91<strong> 1.56*1.91</strong>1.91<strong>1.91</strong>Vocabulary (subtest -mean)2.00<strong> -1.00 -0.67 -4.00</strong> 3.67<strong>-0.67 -6.67</strong> 0.330.671.67<strong>Base Rate10%--1%1%-1%--15%(Critical Value, Significance)1.58</strong>1.58<strong> 1.58</strong>1.58<strong>1.58</strong>Information (subtest -mean) 2.00<strong> 3.00</strong> 2.33<strong> 11.00</strong> 3.67<strong> 2.33</strong> 6.33<strong> 2.33</strong>2.67<strong>1.67</strong>Base Rate15%5%10%1%1%10%1%10%5%25%(Critical Value, Significance)1.64<strong> 1.64</strong> 1.64<strong> 1.64</strong> 1.64<strong> 1.64</strong> 1.64<strong> 1.64</strong>1.64<strong>1.64</strong>Perceptual ReasoningMean PRI Subtests-1.333.00---1.332.000.676.67Matrix Reas. (subtest-mean)--0.33 -2.00<strong>----0.33-1.000.331.33Base Rate--25%-------(Critical Value, Significance)1.92</strong>Visual Puzzles (subtest-mean)--0.33 -1.00----0.33-1.00-0.67-4.67<strong>Base Rate---------1%(Critical Value, Significance)1.99</strong>
Note: Base rates for Verbal and Perceptual Reasoning subtests reflect cumulative percentages of the WAIS-IV normative sample (all ages) that obtained equivalent score discrepancies.<em>p &lt; 0.15; </em>*p &lt; 0.05</p>
<p>Table 5 |
5
Longest Digit and Letter-Number Sequencing Spans with discrepancy analyses.Base rates for Longest Digit Spans and L-N Sequencing Span raw scores reflect cumulative percentages from the WAIS-IV normative sample that obtained equivalent raw scores within the selected age bracket (25-29 years).Base Rates for Digit Span and LN Sequencing Span discrepancy scores reflect cumulative percentages from the entire WAIS-IV normative sample (i.e., all ages ranges) that obtained equivalent discrepancy scores.relationship in visual representations.Across developers and versions, models could not understand the meaning of objects, reason, problem-solve, or detect abnormal patterns in visual representations.
OpenAIGoogleAnthropicGPT-3.5 GPT-4 GPT-4o Gemini Gemini Gemini Gemini Gemini Claude 3 Claude 3.5Turbo TurboNano Pro 1.0Adv.Flash Goldfish OpusSonnet(1.0 S) (1.0 M) (1.0 XL) (1.5 S) (1.5 M)Longest Digit Span Forwards (LDSF)9999999999LDSF Base Rate (25-29 yrs.)(17.5%) (17.5%) (17.5%) (17.5%) (17.5%) (17.5%) (17.5%) (17.5%) (17.5%) (17.5%)Longest Digit Span Back. (LDSB)8783888888LDSB Base Rate (25-29 yrs.)(7%) (18.5%) (7%) (97.9%) (7%)(7%)(7%)(7%)(7%)(7%)Longest Digit Span Seq. (LDSS)9999999999LDSS Base Rate (25-29 yrs.)(2.5%) (2.5%) (2.5%) (2.5%) (2.5%) (2.5%) (2.5%) (2.5%) (2.5%)(2.5%)Longest Letter-Number Seq. (LLNS)8880888888LLNS Base Rate (25-29 yrs.)(6%)(6%)(6%) (100%) (6%)(6%)(6%)(6%)(6%)(6%)Longest Digit Span ComparisonsLDSF-LDSB (difference score)1216111111Base Rate (All Ages)(86%) (64.5%) (86%)(1%)(86%) (86%) (86%) (86%)(86%)(86%)LDSF-LDSS (difference score)0000000000Base Rate (All Ages)----------LDSB-LDSS (difference score)-1-2-1-6-1-1-1-1-1-1Base Rate (All Ages)(66.5%) (40%) (66.5%) (&lt;1%) (66.5%) (66.5%) (66.5%) (66.5%) (66.5%) (66.5%)Note:</p>
<p>. J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, arXiv:2303.087742023arXiv preprint</p>
<p>Managing extreme ai risks amid rapid progress. Y Bengio, G Hinton, A Yao, D Song, P Abbeel, T Darrell, Y N Harari, Y.-Q Zhang, L Xue, S Shalev-Shwartz, arXiv:2406.10424What is the visual cognition gap between humans and multimodal llms?. X Cao, B Lai, W Ye, Y Ma, J Heintz, J Chen, J Cao, J M Rehg, 2024. 2024384arXiv preprint</p>
<p>Correlations of mental tests with each other and with cognitive variables are highest for low iq groups. D K Detterman, M H Daniel, Intelligence. 1341989</p>
<p>J Gardner, S Durand, D Stoller, R M Bittner, arXiv:2310.07160Llark: A multimodal foundation model for music. 2023arXiv preprint</p>
<p>Openagi: When llm meets domain experts. Y Ge, W Hua, K Mei, J Tan, S Xu, Z Li, Y Zhang, Advances in Neural Information Processing Systems. 202436</p>
<p>Evidence of interrelated cognitive-like capabilities in large language models: Indications of artificial general intelligence or achievement?. D Ilić, G E Gignac, 10.1016/j.intell.2024.101858.URLhttps://www.sciencedirect.com/science/article/pii/S0160289624000527Intelligence. 0160-28961061018582024</p>
<p>Human evolution, behavior, and intelligence: The g factor: The science of mental ability. A R Jensen, 1998WestportCT, USA</p>
<p>J Kaplan, S Mccandlish, T Henighan, T B Brown, B Chess, R Child, S Gray, A Radford, J Wu, D Amodei, arXiv:2001.08361Scaling laws for neural language models. 2020arXiv preprint</p>
<p>Large-scale text-to-image generation models for visual artists' creative works. H.-K Ko, G Park, H Jeon, J Jo, J Kim, J Seo, Proceedings of the 28th international conference on intelligent user interfaces. the 28th international conference on intelligent user interfaces2023</p>
<p>Rtllm: An open-source benchmark for design rtl generation with large language model. Y Lu, S Liu, Q Zhang, Z Xie, 2024 29th Asia and South Pacific Design Automation Conference (ASP-DAC). IEEE2024</p>
<p>Characterising the creative process in humans and large language models. S S Nath, P Dayan, C Stevenson, arXiv:2405.008992024arXiv preprint</p>
<p>Intelligence: knowns and unknowns. U Neisser, G Boodoo, T J BouchardJr, A W Boykin, N Brody, S J Ceci, D F Halpern, J C Loehlin, R Perloff, R J Sternberg, American psychologist. 512771996</p>
<p>Llm is like a box of chocolates: the non-determinism of chatgpt in code generation. S Ouyang, J M Zhang, M Harman, M Wang, arXiv:2308.028282023arXiv preprint</p>
<p>general intelligence" objectively determined and measured. C Spearman, 1961</p>
<p>People's conceptions of intelligence. R J Sternberg, B E Conway, J L Ketron, M Bernstein, Journal of personality and social psychology. 411371981</p>
<p>H Subramonyam, C L Pondoc, C Seifert, M Agrawala, R Pea, arXiv:2309.14459Bridging the gulf of envisioning: Cognitive design challenges in llm interfaces. 2023arXiv preprint</p>
<p>G Team, R Anil, S Borgeaud, Y Wu, J.-B Alayrac, J Yu, R Soricut, J Schalkwyk, A M Dai, A Hauth, arXiv:2312.11805family of highly capable multimodal models. 2023arXiv preprint</p>
<p>The association between intelligence and face processing abilities: A conceptual and meta-analytic review. D L Walker, R Palermo, Z Callis, G E Gignac, Intelligence. 961017182023</p>
<p>Z Wang, S Zhao, Y Wang, H Huang, J Shi, S Xie, Z Wang, Y Zhang, H Li, J Yan, arXiv:2408.06904Re-task: Revisiting llm tasks from capability, skill, and knowledge perspectives. D Wechsler, Scoring Wais-Iv Administration, Manual, Psychcorp, 2024. 2008arXiv preprint</p>
<p>Wais-iv and clinical validation of the four-and five-factor interpretative approaches. L G Weiss, T Z Keith, J Zhu, H Chen, Journal of Psychoeducational Assessment. 3122013</p>
<p>Next-gpt: Any-to-any multimodal llm. S Wu, H Fei, L Qu, W Ji, T.-S Chua, arXiv:2309.055192023arXiv preprint</p>
<p>Talk2care: An llm-based voice assistant for communication between healthcare providers and older adults. Z Yang, X Xu, B Yao, E Rogers, S Zhang, S Intille, N Shara, G G Gao, D Wang, Proceedings of the ACM on Interactive. 822024Wearable and Ubiquitous Technologies</p>
<p>Y Zhang, S Mao, T Ge, X Wang, A De Wynter, Y Xia, W Wu, T Song, M Lan, F Wei, arXiv:2404.01230Llm as a mastermind: A survey of strategic reasoning with large language models. 2024arXiv preprint</p>
<p>W X Zhao, K Zhou, J Li, T Tang, X Wang, Y Hou, Y Min, B Zhang, J Zhang, Z Dong, Y Du, C Yang, Y Chen, Z Chen, J Jiang, R Ren, Y Li, X Tang, Z Liu, P Liu, J.-Y Nie, J.-R Wen, A survey of large language models. 2023</p>
<p>Y Zhuang, Q Liu, Y Ning, W Huang, R Lv, Z Huang, G Zhao, Z Zhang, Q Mao, S Wang, arXiv:2306.10512Efficiently measuring the cognitive ability of llms: An adaptive testing perspective. 2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6699 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6699</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6699</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-128.html">extraction-schema-128</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <p><strong>Paper ID:</strong> paper-0725b276e351bba6b2a52ecb64f3c964b9acc2f9</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/0725b276e351bba6b2a52ecb64f3c964b9acc2f9" target="_blank">ADaPT: As-Needed Decomposition and Planning with Language Models</a></p>
                <p><strong>Paper Venue:</strong> NAACL-HLT</p>
                <p><strong>Paper TL;DR:</strong> This work introduces As-Needed Decomposition and Planning for complex Tasks (ADaPT), an approach that explicitly plans and decomposes complex sub-tasks as-needed, i.e., when the LLM is unable to execute them.</p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) are increasingly being used for interactive decision-making tasks requiring planning and adapting to the environment. Recent works employ LLMs-as-agents in broadly two ways: iteratively determining the next action (iterative executors) or generating plans and executing sub-tasks using LLMs (plan-and-execute). However, these methods struggle with task complexity, as the inability to execute any sub-task may lead to task failure. To address these shortcomings, we introduce As-Needed Decomposition and Planning for complex Tasks (ADaPT), an approach that explicitly plans and decomposes complex sub-tasks as-needed, i.e., when the LLM is unable to execute them. ADaPT recursively decomposes sub-tasks to adapt to both task complexity and LLM capability. Our results demonstrate that ADaPT substantially outperforms established strong baselines, achieving success rates up to 28.3% higher in ALFWorld, 27% in WebShop, and 33% in TextCraft -- a novel compositional dataset that we introduce. Through extensive analysis, we illustrate the importance of multilevel decomposition and establish that ADaPT dynamically adjusts to the capabilities of the executor LLM as well as to task complexity.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6699.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6699.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ADAPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>As-Needed Decomposition and Planning for complex Tasks (ADAPT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recursive LLM-program that combines an executor LLM (iterative ReAct-style) and a planner LLM plus a controller: it runs the executor on a (sub)task and only calls the planner to further decompose that subtask if the executor reports failure, recursively repeating until success or a max depth.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>ADAPT: As-Needed Decomposition and Planning with Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ADAPT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Modular planner-executor architecture with a recursive controller: (1) executor — an LLM prompted in a ReAct/think-act-observe style to iteratively interact with the environment and produce a binary self-assessed success/failure; (2) planner — an LLM that generates short abstract plans (3–5 steps) and logical composition (AND/OR) of steps; (3) controller — an algorithmic routine that calls executor, decides on decomposition using the success heuristic, and passes salient state between planner/executor calls.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (default in experiments); also evaluated with GPT-4, LLaMA-2-70B, Lemur-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-3.5 (size not specified by authors); GPT-4 (not specified); LLaMA-2-70B; Lemur-70B</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ALFWorld (text-based household), WebShop (web-interaction text environment), TextCraft (novel compositional text-only crafting game)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>no dedicated/global memory; controller-level short-term state propagation (task-dependent salient info passed between recursive calls)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>textual task metadata and short state: last successful action(s) from prior executions, current webpage text (WebShop), agent inventory (TextCraft); logical plan expressions emitted by planner</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>rule-based propagation by controller: after a successful subtask, the controller appends/propagates the salient output (e.g., product ID, last successful action, inventory) to subsequent planner/executor prompts; no persistent memory store or learned write network</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>prompt-based inclusion: controller injects the propagated textual state directly into the LLM prompts for planner/executor (no search or indexed retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>few-shot prompting / in‑context demonstrations (no supervised fine-tuning reported)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>success rate (%) on held-out test games/queries/tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>ADAPT does not use a separate memory module; compared to iterative executor-only (ReAct) performance (same executor) the paper reports large gains: ALFWorld (All) ADAPT 71.6% vs ReAct 43.3% (d_max=3); WebShop ADAPT 44.0% vs ReAct 32.0%; TextCraft ADAPT 52.0% vs ReAct 19.0%.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td>Key ablations analyze maximum decomposition depth (d_max): increasing d_max from 1→2→3 consistently improves performance across datasets, showing decomposition depth matters; ADAPT benefits especially when executor is weak (prompts with only atomic skills), e.g., executor-only 3.3% → ADAPT 41.7% in one setting; using different planner/executor LMs (strong planner + weaker executor) improves performance versus executor-only.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Relies on the executor's self-generated binary success heuristic which can be overconfident (notably inflated on WebShop), and ADAPT has no dedicated long-term memory or explicit natural-language reflection store; controller state propagation is task-specific and relatively simple.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td>Generate short, abstract plans (3–5 steps) and decompose further only as-needed; employ an executor self-assessment heuristic to trigger planner calls; propagate only salient successful subtask outputs (product IDs, inventory, last successful action) rather than full failure rationales; keep planner calls sparse and allow different LLMs for planner/executor to tradeoff cost and capability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ADaPT: As-Needed Decomposition and Planning with Language Models', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6699.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6699.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A language-agent method that, after executing a full trial, generates natural-language reflections/feedback and stores those reflections in a memory buffer; subsequent trials use that stored feedback to alter behavior and improve performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>ADAPT: As-Needed Decomposition and Planning with Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Iterative trial-based agent that runs full trial(s); upon failure it generates natural-language feedback ('reflections') which are stored in a memory and included in prompts for future trials so the agent can learn from prior mistakes; repeats whole-task trials up to d_max times incorporating reflections.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Implemented with GPT-3.5 in comparisons (original Reflexion paper used similar LMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-3.5 (size not specified)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ALFWorld, WebShop, TextCraft (used as a baseline in ADAPT experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>explicit textual memory buffer storing natural-language feedback/reflections across trials</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>free-form natural-language reflections / feedback produced by the LLM about failed trials (text strings)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>append new reflections to stored memory after each failed full-task trial (rule-based write of generated reflection text)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>prompt augmentation: the stored reflections are prepended/appended to the LLM prompt in subsequent trials (prompt-based retrieval, no indexing/search beyond concatenation)</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>few-shot prompting and multi-trial prompting; not supervised fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>success rate (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported Reflexion success rates (d_max=3): ALFWorld All 57.5% (Table 1), WebShop 35.0% (Table 2), TextCraft 32.0% (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Compared to iterative executor-only (ReAct) baseline: ReAct ALFWorld 43.3%, WebShop 32.0%, TextCraft 19.0%; Reflexion generally improves over ReAct but is outperformed by ADAPT (see paper comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td>ADAPT authors note Reflexion's natural-language feedback gives limited gains relative to ADAPT; Reflexion's approach of reflecting on entire-trial failures can be inefficient because successful subtasks are repeated across trials.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Repeats the entire trial even when only a single subtask fails (redundant re-execution); stored reflections can be noisy or not sufficiently targeted; limited gains in some domains (paper reports ADAPT beating Reflexion despite Reflexion's dedicated memory).</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td>If using reflection-based memory, store and use targeted, concise feedback focused on the failing subtask(s) rather than global trial-level reflections to avoid redundant re-execution; combine reflection memory with localized decomposition to reduce wasted effort.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ADaPT: As-Needed Decomposition and Planning with Language Models', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6699.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6699.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct: Synergizing reasoning and acting in language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative prompting method that interleaves 'thought' traces and actions: the LLM ingests the full action-observation history, produces an internal chain-of-thought and then outputs the next environment action — effectively an executor-only agent.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>React: Synergizing reasoning and acting in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>ADAPT: As-Needed Decomposition and Planning with Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ReAct (iterative executor)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Executor-only agent that at each step conditions on the entire textual history (actions + observations) and produces 'thought' and an environment action; no separate planner and no explicit memory beyond the prompt/history.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 used in baselines/experiments</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-3.5 (size not specified)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ALFWorld, WebShop, TextCraft</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>implicit context-window memory (the entire action-observation trajectory included in the LLM prompt)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>raw observation text and prior actions (textual history concatenated in prompt)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>append-only concatenation of actions and observations into the context (bounded by context window)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>implicit attention over the full context window (no external retrieval index)</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>few-shot in-context demonstrations; no fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>success rate (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>ReAct performance reported in paper (d_max=3 baseline): ALFWorld All 43.3%; WebShop 32.0%; TextCraft 19.0%. These serve as the 'executor-only' (no planner, no explicit memory buffer) baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td>ReAct struggles as tasks grow in compositional complexity and action-observation history length; performance improves when combined with planning/decomposition (ADAPT) or when given stronger in-context trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Must maintain the entire plan implicitly in the prompt which leads to long trajectories, higher distraction and worse compositional generalization; susceptible to hallucinating or losing track over long histories.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td>Reduce executor trajectory length via modular planning or decomposition; provide atomic skill demonstrations in-context; consider hybrid planner-executor designs for complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ADaPT: As-Needed Decomposition and Planning with Language Models', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6699.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6699.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Plan-and-Execute</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Plan-and-Execute (plan-and-solve prompting baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage approach where an LLM planner first creates a (fixed) detailed plan and then an executor LLM executes each step; the plan is produced once and not adaptively decomposed during execution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Plan-and-solve prompting: Improving zeroshot chain-of-thought reasoning by large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>ADAPT: As-Needed Decomposition and Planning with Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Plan-and-Execute</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Planner LLM emits a detailed multi-step plan up front; the executor LLM is then tasked to execute each plan step sequentially without further decomposition or adaptive replanning when a subtask is hard/unachievable.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 in the experiments</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-3.5 (size not specified)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ALFWorld, WebShop, TextCraft</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>none beyond the static plan included in prompt</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>the detailed plan text included in the prompt context</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>none (single planning step; no iterative updates)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>plan text is present in prompt for executor to consult</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>few-shot prompting/in-context examples for planner and executor</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>success rate (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Reported plan-and-execute performance in paper: ALFWorld All 43.3% (Table 1), WebShop 17.0% (Table 2), TextCraft 27.0% (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td>Paper contrasts fixed detailed plans vs short abstract plans + as-needed decomposition: fixed plans can cascade errors when initial assumptions are wrong; ADAPT's adaptive decomposition yields higher success.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Non-adaptive to execution failures; an unachievable subtask in the fixed plan can cause whole-task failure; detailed upfront plans are brittle in unexplored environments.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td>Prefer short abstract plans with the option to recursively decompose only when the executor fails (as ADAPT does) to avoid cascading planning errors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ADaPT: As-Needed Decomposition and Planning with Language Models', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6699.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6699.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Try-Again</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Try Again with ReAct (multiple retrials controller)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple controller baseline that requests the ReAct executor to retry the same task up to d_max times and keeps the best trial's result (no decomposition and no memory of past failures used to guide retries).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>ADAPT: As-Needed Decomposition and Planning with Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Try Again with ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Controller issues multiple independent full-trial attempts to the executor (ReAct) and selects the best outcome; does not store or use targeted feedback across trials and does not decompose sub-tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (used for executor in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-3.5 (size not specified)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ALFWorld, WebShop, TextCraft</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>none</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>few-shot prompting / repeated sampling</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>success rate (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Reported Try-Again success rates: ALFWorld All 47.8% (Table 1), WebShop 30.0% (Table 2), TextCraft 15.0% (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td>Using repeated full-trial retries yields moderate improvements over single-trial ReAct in some settings, but is less efficient and less effective than targeted decomposition (ADAPT).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Wastes computation by repeating successful sub-tasks; lacks targeted corrections and memory of why failures happened; inferior to approaches that decompose the failing sub-task.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td>If reties are used, combine them with targeted reflection or decomposition to avoid redoing already-correct subtasks and to focus corrective effort.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ADaPT: As-Needed Decomposition and Planning with Language Models', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>React: Synergizing reasoning and acting in language models <em>(Rating: 2)</em></li>
                <li>WebShop: Towards scalable realworld web interaction with grounded language agents <em>(Rating: 2)</em></li>
                <li>ALFWorld: Aligning Text and Embodied Environments for Interactive Learning <em>(Rating: 2)</em></li>
                <li>Plan-and-solve prompting: Improving zeroshot chain-of-thought reasoning by large language models <em>(Rating: 1)</em></li>
                <li>Language agent tree search unifies reasoning acting and planning in language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6699",
    "paper_id": "paper-0725b276e351bba6b2a52ecb64f3c964b9acc2f9",
    "extraction_schema_id": "extraction-schema-128",
    "extracted_data": [
        {
            "name_short": "ADAPT",
            "name_full": "As-Needed Decomposition and Planning for complex Tasks (ADAPT)",
            "brief_description": "A recursive LLM-program that combines an executor LLM (iterative ReAct-style) and a planner LLM plus a controller: it runs the executor on a (sub)task and only calls the planner to further decompose that subtask if the executor reports failure, recursively repeating until success or a max depth.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "ADAPT: As-Needed Decomposition and Planning with Language Models",
            "agent_name": "ADAPT",
            "agent_description": "Modular planner-executor architecture with a recursive controller: (1) executor — an LLM prompted in a ReAct/think-act-observe style to iteratively interact with the environment and produce a binary self-assessed success/failure; (2) planner — an LLM that generates short abstract plans (3–5 steps) and logical composition (AND/OR) of steps; (3) controller — an algorithmic routine that calls executor, decides on decomposition using the success heuristic, and passes salient state between planner/executor calls.",
            "model_name": "GPT-3.5 (default in experiments); also evaluated with GPT-4, LLaMA-2-70B, Lemur-70B",
            "model_size": "GPT-3.5 (size not specified by authors); GPT-4 (not specified); LLaMA-2-70B; Lemur-70B",
            "benchmark_name": "ALFWorld (text-based household), WebShop (web-interaction text environment), TextCraft (novel compositional text-only crafting game)",
            "memory_used": false,
            "memory_type": "no dedicated/global memory; controller-level short-term state propagation (task-dependent salient info passed between recursive calls)",
            "memory_representation": "textual task metadata and short state: last successful action(s) from prior executions, current webpage text (WebShop), agent inventory (TextCraft); logical plan expressions emitted by planner",
            "memory_update_mechanism": "rule-based propagation by controller: after a successful subtask, the controller appends/propagates the salient output (e.g., product ID, last successful action, inventory) to subsequent planner/executor prompts; no persistent memory store or learned write network",
            "memory_retrieval_method": "prompt-based inclusion: controller injects the propagated textual state directly into the LLM prompts for planner/executor (no search or indexed retrieval)",
            "training_method": "few-shot prompting / in‑context demonstrations (no supervised fine-tuning reported)",
            "evaluation_metric": "success rate (%) on held-out test games/queries/tasks",
            "performance_with_memory": null,
            "performance_without_memory": "ADAPT does not use a separate memory module; compared to iterative executor-only (ReAct) performance (same executor) the paper reports large gains: ALFWorld (All) ADAPT 71.6% vs ReAct 43.3% (d_max=3); WebShop ADAPT 44.0% vs ReAct 32.0%; TextCraft ADAPT 52.0% vs ReAct 19.0%.",
            "has_comparative_results": true,
            "ablation_findings": "Key ablations analyze maximum decomposition depth (d_max): increasing d_max from 1→2→3 consistently improves performance across datasets, showing decomposition depth matters; ADAPT benefits especially when executor is weak (prompts with only atomic skills), e.g., executor-only 3.3% → ADAPT 41.7% in one setting; using different planner/executor LMs (strong planner + weaker executor) improves performance versus executor-only.",
            "reported_limitations": "Relies on the executor's self-generated binary success heuristic which can be overconfident (notably inflated on WebShop), and ADAPT has no dedicated long-term memory or explicit natural-language reflection store; controller state propagation is task-specific and relatively simple.",
            "best_practices_recommendations": "Generate short, abstract plans (3–5 steps) and decompose further only as-needed; employ an executor self-assessment heuristic to trigger planner calls; propagate only salient successful subtask outputs (product IDs, inventory, last successful action) rather than full failure rationales; keep planner calls sparse and allow different LLMs for planner/executor to tradeoff cost and capability.",
            "uuid": "e6699.0",
            "source_info": {
                "paper_title": "ADaPT: As-Needed Decomposition and Planning with Language Models",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion: Language agents with verbal reinforcement learning",
            "brief_description": "A language-agent method that, after executing a full trial, generates natural-language reflections/feedback and stores those reflections in a memory buffer; subsequent trials use that stored feedback to alter behavior and improve performance.",
            "citation_title": "Reflexion: Language agents with verbal reinforcement learning",
            "mention_or_use": "use",
            "paper_title": "ADAPT: As-Needed Decomposition and Planning with Language Models",
            "agent_name": "Reflexion",
            "agent_description": "Iterative trial-based agent that runs full trial(s); upon failure it generates natural-language feedback ('reflections') which are stored in a memory and included in prompts for future trials so the agent can learn from prior mistakes; repeats whole-task trials up to d_max times incorporating reflections.",
            "model_name": "Implemented with GPT-3.5 in comparisons (original Reflexion paper used similar LMs)",
            "model_size": "GPT-3.5 (size not specified)",
            "benchmark_name": "ALFWorld, WebShop, TextCraft (used as a baseline in ADAPT experiments)",
            "memory_used": true,
            "memory_type": "explicit textual memory buffer storing natural-language feedback/reflections across trials",
            "memory_representation": "free-form natural-language reflections / feedback produced by the LLM about failed trials (text strings)",
            "memory_update_mechanism": "append new reflections to stored memory after each failed full-task trial (rule-based write of generated reflection text)",
            "memory_retrieval_method": "prompt augmentation: the stored reflections are prepended/appended to the LLM prompt in subsequent trials (prompt-based retrieval, no indexing/search beyond concatenation)",
            "training_method": "few-shot prompting and multi-trial prompting; not supervised fine-tuning",
            "evaluation_metric": "success rate (%)",
            "performance_with_memory": "Reported Reflexion success rates (d_max=3): ALFWorld All 57.5% (Table 1), WebShop 35.0% (Table 2), TextCraft 32.0% (Table 2).",
            "performance_without_memory": "Compared to iterative executor-only (ReAct) baseline: ReAct ALFWorld 43.3%, WebShop 32.0%, TextCraft 19.0%; Reflexion generally improves over ReAct but is outperformed by ADAPT (see paper comparisons).",
            "has_comparative_results": true,
            "ablation_findings": "ADAPT authors note Reflexion's natural-language feedback gives limited gains relative to ADAPT; Reflexion's approach of reflecting on entire-trial failures can be inefficient because successful subtasks are repeated across trials.",
            "reported_limitations": "Repeats the entire trial even when only a single subtask fails (redundant re-execution); stored reflections can be noisy or not sufficiently targeted; limited gains in some domains (paper reports ADAPT beating Reflexion despite Reflexion's dedicated memory).",
            "best_practices_recommendations": "If using reflection-based memory, store and use targeted, concise feedback focused on the failing subtask(s) rather than global trial-level reflections to avoid redundant re-execution; combine reflection memory with localized decomposition to reduce wasted effort.",
            "uuid": "e6699.1",
            "source_info": {
                "paper_title": "ADaPT: As-Needed Decomposition and Planning with Language Models",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "ReAct",
            "name_full": "ReAct: Synergizing reasoning and acting in language models",
            "brief_description": "An iterative prompting method that interleaves 'thought' traces and actions: the LLM ingests the full action-observation history, produces an internal chain-of-thought and then outputs the next environment action — effectively an executor-only agent.",
            "citation_title": "React: Synergizing reasoning and acting in language models",
            "mention_or_use": "use",
            "paper_title": "ADAPT: As-Needed Decomposition and Planning with Language Models",
            "agent_name": "ReAct (iterative executor)",
            "agent_description": "Executor-only agent that at each step conditions on the entire textual history (actions + observations) and produces 'thought' and an environment action; no separate planner and no explicit memory beyond the prompt/history.",
            "model_name": "GPT-3.5 used in baselines/experiments",
            "model_size": "GPT-3.5 (size not specified)",
            "benchmark_name": "ALFWorld, WebShop, TextCraft",
            "memory_used": false,
            "memory_type": "implicit context-window memory (the entire action-observation trajectory included in the LLM prompt)",
            "memory_representation": "raw observation text and prior actions (textual history concatenated in prompt)",
            "memory_update_mechanism": "append-only concatenation of actions and observations into the context (bounded by context window)",
            "memory_retrieval_method": "implicit attention over the full context window (no external retrieval index)",
            "training_method": "few-shot in-context demonstrations; no fine-tuning",
            "evaluation_metric": "success rate (%)",
            "performance_with_memory": null,
            "performance_without_memory": "ReAct performance reported in paper (d_max=3 baseline): ALFWorld All 43.3%; WebShop 32.0%; TextCraft 19.0%. These serve as the 'executor-only' (no planner, no explicit memory buffer) baseline.",
            "has_comparative_results": true,
            "ablation_findings": "ReAct struggles as tasks grow in compositional complexity and action-observation history length; performance improves when combined with planning/decomposition (ADAPT) or when given stronger in-context trajectories.",
            "reported_limitations": "Must maintain the entire plan implicitly in the prompt which leads to long trajectories, higher distraction and worse compositional generalization; susceptible to hallucinating or losing track over long histories.",
            "best_practices_recommendations": "Reduce executor trajectory length via modular planning or decomposition; provide atomic skill demonstrations in-context; consider hybrid planner-executor designs for complex tasks.",
            "uuid": "e6699.2",
            "source_info": {
                "paper_title": "ADaPT: As-Needed Decomposition and Planning with Language Models",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Plan-and-Execute",
            "name_full": "Plan-and-Execute (plan-and-solve prompting baseline)",
            "brief_description": "A two-stage approach where an LLM planner first creates a (fixed) detailed plan and then an executor LLM executes each step; the plan is produced once and not adaptively decomposed during execution.",
            "citation_title": "Plan-and-solve prompting: Improving zeroshot chain-of-thought reasoning by large language models",
            "mention_or_use": "use",
            "paper_title": "ADAPT: As-Needed Decomposition and Planning with Language Models",
            "agent_name": "Plan-and-Execute",
            "agent_description": "Planner LLM emits a detailed multi-step plan up front; the executor LLM is then tasked to execute each plan step sequentially without further decomposition or adaptive replanning when a subtask is hard/unachievable.",
            "model_name": "GPT-3.5 in the experiments",
            "model_size": "GPT-3.5 (size not specified)",
            "benchmark_name": "ALFWorld, WebShop, TextCraft",
            "memory_used": false,
            "memory_type": "none beyond the static plan included in prompt",
            "memory_representation": "the detailed plan text included in the prompt context",
            "memory_update_mechanism": "none (single planning step; no iterative updates)",
            "memory_retrieval_method": "plan text is present in prompt for executor to consult",
            "training_method": "few-shot prompting/in-context examples for planner and executor",
            "evaluation_metric": "success rate (%)",
            "performance_with_memory": null,
            "performance_without_memory": "Reported plan-and-execute performance in paper: ALFWorld All 43.3% (Table 1), WebShop 17.0% (Table 2), TextCraft 27.0% (Table 2).",
            "has_comparative_results": true,
            "ablation_findings": "Paper contrasts fixed detailed plans vs short abstract plans + as-needed decomposition: fixed plans can cascade errors when initial assumptions are wrong; ADAPT's adaptive decomposition yields higher success.",
            "reported_limitations": "Non-adaptive to execution failures; an unachievable subtask in the fixed plan can cause whole-task failure; detailed upfront plans are brittle in unexplored environments.",
            "best_practices_recommendations": "Prefer short abstract plans with the option to recursively decompose only when the executor fails (as ADAPT does) to avoid cascading planning errors.",
            "uuid": "e6699.3",
            "source_info": {
                "paper_title": "ADaPT: As-Needed Decomposition and Planning with Language Models",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Try-Again",
            "name_full": "Try Again with ReAct (multiple retrials controller)",
            "brief_description": "A simple controller baseline that requests the ReAct executor to retry the same task up to d_max times and keeps the best trial's result (no decomposition and no memory of past failures used to guide retries).",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "ADAPT: As-Needed Decomposition and Planning with Language Models",
            "agent_name": "Try Again with ReAct",
            "agent_description": "Controller issues multiple independent full-trial attempts to the executor (ReAct) and selects the best outcome; does not store or use targeted feedback across trials and does not decompose sub-tasks.",
            "model_name": "GPT-3.5 (used for executor in experiments)",
            "model_size": "GPT-3.5 (size not specified)",
            "benchmark_name": "ALFWorld, WebShop, TextCraft",
            "memory_used": false,
            "memory_type": "none",
            "memory_representation": null,
            "memory_update_mechanism": null,
            "memory_retrieval_method": null,
            "training_method": "few-shot prompting / repeated sampling",
            "evaluation_metric": "success rate (%)",
            "performance_with_memory": null,
            "performance_without_memory": "Reported Try-Again success rates: ALFWorld All 47.8% (Table 1), WebShop 30.0% (Table 2), TextCraft 15.0% (Table 2).",
            "has_comparative_results": true,
            "ablation_findings": "Using repeated full-trial retries yields moderate improvements over single-trial ReAct in some settings, but is less efficient and less effective than targeted decomposition (ADAPT).",
            "reported_limitations": "Wastes computation by repeating successful sub-tasks; lacks targeted corrections and memory of why failures happened; inferior to approaches that decompose the failing sub-task.",
            "best_practices_recommendations": "If reties are used, combine them with targeted reflection or decomposition to avoid redoing already-correct subtasks and to focus corrective effort.",
            "uuid": "e6699.4",
            "source_info": {
                "paper_title": "ADaPT: As-Needed Decomposition and Planning with Language Models",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "React: Synergizing reasoning and acting in language models",
            "rating": 2
        },
        {
            "paper_title": "WebShop: Towards scalable realworld web interaction with grounded language agents",
            "rating": 2
        },
        {
            "paper_title": "ALFWorld: Aligning Text and Embodied Environments for Interactive Learning",
            "rating": 2
        },
        {
            "paper_title": "Plan-and-solve prompting: Improving zeroshot chain-of-thought reasoning by large language models",
            "rating": 1
        },
        {
            "paper_title": "Language agent tree search unifies reasoning acting and planning in language models",
            "rating": 1
        }
    ],
    "cost": 0.021639,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>ADAPT: As-Needed Decomposition and Planning with Language Models</h1>
<p>Archiki Prasad<em> Alexander Koller</em> Mareike Hartmann<em> Peter Clark</em> Ashish Sabharwal<em> Mohit Bansal</em> Tushar Khot*<br>$\triangle$ UNC Chapel Hill $\triangle$ Allen Institute for AI $\quad$ Saarland University</p>
<h4>Abstract</h4>
<p>Large Language Models (LLMs) are increasingly being used for interactive decisionmaking tasks requiring planning and adapting to the environment. Recent works employ LLMs-as-agents in broadly two ways: iteratively determining the next action (iterative executors) or generating plans and executing sub-tasks using LLMs (plan-and-execute). However, these methods struggle with task complexity, as the inability to execute any sub-task may lead to task failure. To address these shortcomings, we introduce AsNeeded Decomposition and Planning for complex Tasks (ADAPT), an approach that explicitly plans and decomposes complex sub-tasks as-needed, i.e., when the LLM is unable to execute them. ADAPT recursively decomposes sub-tasks to adapt to both task complexity and LLM capability. Our results demonstrate that ADAPT substantially outperforms established strong baselines, achieving success rates up to $28.3 \%$ higher in ALFWorld, $27 \%$ in WebShop, and $33 \%$ in TextCraft - a novel compositional dataset that we introduce. Through extensive analysis, we illustrate the importance of multilevel decomposition and establish that ADAPT dynamically adjusts to the capabilities of the executor LLM as well as to task complexity. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Recent advances in Large Language Models (LLMs) have expanded their application beyond conventional NLP tasks to more complex tasks involving mathematical, symbolic, and commonsense reasoning (Wei et al., 2022; Huang and Chang, 2023). Recent models have even been applied to decision-making tasks, such as performing household chores, navigating a webpage, etc., that require interactions with external environments or tools (Yao et al., 2023b; Qin et al., 2023).</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Prior works on using LLMs for decision-making, such as ReAct (Yao et al., 2023b), iteratively generate the next action to be executed in the environment given the history of actions and observations (see Fig. 1; top-left). However, as the tasks become more complex, LLMs struggle due to their limited composition ability (Dziri et al., 2023) and inability to deal with the distractors (Shi et al., 2023) in a long action-observation trajectory.</p>
<p>To mitigate this, modular approaches (Khot et al., 2023; Yang et al., 2023; Sun et al., 2023) incorporate a separate planner module that utilizes an LLM to create a high-level plan. ${ }^{2}$ The planner then delegates simpler sub-tasks to an executor LLM module thereby reducing the compositional complexity and length of action trajectory required by the executor. We refer to this category broadly as plan-andexecute approaches (see Fig. 1; top-right). While the plans enable these methods to guide the execution and track progress (Wang et al., 2023b), their non-adaptive nature poses a limitation when confronting unachievable sub-tasks. These approaches inherently lack the flexibility to adapt to task complexity and manage execution failures, as shown in Fig. 1(top-right), where just one sub-task that is too complex results in overall task failure.</p>
<p>To address such failures, we propose As-Needed Decomposition and Planning for complex Tasks (ADAPT), a recursive algorithm that further decomposes sub-tasks when necessary, to dynamically accommodate to task complexity. We utilize separate planner and executor LLM modules within our framework but only decompose a task using the planner, if the executor LLM detects a failure. As shown in Fig. 1, the overall task of putting a clean mug on a desk in an unfamiliar</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Top-Left: Iterative executors such as ReAct (Yao et al., 2023b) interact directly with the environment, performing planning implicitly. Top-Right: Plan-and-Execute, e.g., Yang et al. (2023), creates a fixed plan for the task, without accounting for complexity in executing step 1. Bottom: ADAPT dynamically decomposes based on success of the executor.</p>
<p>household is too complex for the model, leading to failure of the iterative executor. While a plan-and-execute-style approach initially breaks down the task into three sub-tasks, it falls short in accounting for the complexity in finding a mug. Moreover, it is challenging to anticipate the difficulty of such a subtask in advance, as the executor could find a mug in the first attempt or in an obscure location. Therefore, ADAPT employs its recursive structure to <em>dynamically adapt</em> to execution failures (assessed by LLMs), by <em>further decomposing</em> the complex sub-task of <em>finding a mug</em> via the planner.</p>
<p>Empirically, we demonstrate the effectiveness of ADAPT on three datasets involving interactive environments: ALFWorld (Shridhar et al., 2021), WebShop (Yao et al., 2022), and a new compositional text game for crafting Minecraft recipes called <em>TextCraft</em> (Sec. 4.1). Using GPT-3.5 as the underlying LLM, ADAPT outperforms strong baselines (discussed in Sec. 4.2) such as ReAct (Yao et al., 2023b), and Plan-and-Solve (Wang et al., 2023b) by up to 28.3%, 27%, and 33% absolute points on ALFWorld, WebShop, and TextCraft respectively (Sec. 5). Compared to Reflexion (Shinn et al., 2023), an adaptive approach that addresses <em>failures in the full task trajectory</em>, ADAPT yields higher success rates by 14.1%, 9%, and 20% on ALFWorld, WebShop, and TextCraft respectively. Through extensive analysis of ADAPT, we establish the importance of recursive decomposition (Sec. 6.1) and showcase dynamic adaptation to the capabilities of the executor LLM including open-source models such LLaMA-2 (Touvron et al., 2023) and Lemur (Xu et al., 2023) in Sec. 6.2. Lastly, we demonstrate that ADAPT incorporates task complexity (Sec. 6.3), where the extent of recursive decomposition aligns with the inherent task complexity. To summarize, our contributions are:</p>
<ol>
<li>We present ADAPT, a recursive algorithm that dynamically decomposes complex sub-tasks on an as-needed basis, i.e., <em>intervening only if the task is too complex for the executor</em>.</li>
<li>On three diverse datasets, ALFWorld, WebShop, and TextCraft, ADAPT improves success rate of GPT-3.5 over previous approaches by up to 28.3%, 27%, and 33% points respectively.</li>
<li>Analysis of ADAPT underscores the significance of recursive decomposition and the ability to adapt dynamically to varying LLM execution capabilities and task complexities.</li>
</ol>
<h2>2 Related Work</h2>
<p>LLMs for Decision-Making. LLMs have been successfully used as agents to perform a wide variety of decision-making tasks such as robotic navigation (Ahn et al., 2022; Huang et al., 2023b; Singh et al., 2023), complex multi-modal games like Minecraft (Fan et al., 2022; Wang et al., 2023a), text-based environments (Shridhar et al., 2021; Liu et al., 2023). While most of these works focus on learning from trajectories, ReAct (Yao et al., 2023b) uses few-shot prompting to build an agent that reasons about the current state (thoughts) and generates the next action in the environment, given prior actions and observations. Their iterative approach (shown in Fig. 1; top-left) can handle failures, but they have to keep track of the entire plan <em>implicitly</em> while deciding every local action (c.f. ADAPT in Fig. 9 of Appendix A). By incorporating planning and execution into separate modules and enabling dynamic adaptation we are able to achieve higher success rates (refer to Sec. 5).</p>
<p>Several follow-up works improve upon the ReAct framework by incorporating feedback in future trials (Madaan et al., 2023; Shinn et al., 2023), or using LLMs to develop heuristics for search (Yao et al., 2023a; Zhou et al., 2023). In contrast to ADAPT, they do not employ task decomposition, leading to unnecessary computation as they explore multiple trajectories or trials for the whole task, even though the LLM struggles with just one subtask. Such works are complementary to ADAPT as they can be incorporated within the planner or executor modules to strengthen LLM performance (just like they are incorporated in ReAct).</p>
<p>Decomposition and Modularity. Our work follows extensive literature in NLP on decomposing tasks into neural modules (Andreas et al., 2016; Gupta et al., 2019; Jiang and Bansal, 2019) or seq2seq models (Min et al., 2019; Talmor and Berant, 2018; Khot et al., 2021; Perez et al., 2020; Saha et al., 2023b). With the advent of few-shot prompted black-box LLMs, this paradigm of programmatic decomposition into LLMs has become more popular (Yao et al., 2023b; Khot et al., 2023; Wang et al., 2023b, inter alia), referred to as LLM Programs (Schlag et al., 2023; Dohan et al., 2022). Additionally, past works in program synthesis (Murali et al., 2018; Nye et al., 2019; Zheng et al., 2023) also employ task decomposition via generating a "program sketch" prior to program generation.</p>
<p>ADAPT not only decomposes tasks via the planner module and delegates them to the executor module but also automatically adapts to executor failures by further decomposing complex tasks as-needed. This dynamic capability distinguishes ADAPT from prior works with a non-adaptive structure. ADAPT extends the recursive and hierarchical decomposition in Khot et al. (2023), enabling inter-module communications, and robust strategies for execution failures, excelling in realworld textual environments like online shopping.</p>
<p>Hierarchical Problem Solving. In AI problemsolving, there is a longstanding tradition of hierarchical task decomposition employed in planning (Ghallab et al., 2004; Georgievski and Aiello, 2014; Höller et al., 2020), reinforcement learning (Sutton et al., 1999; Barto and Mahadevan, 2003; Nachum et al., 2018; Zhang et al., 2021), and navigation (She et al., 2014; Sharma et al., 2022; Blukis et al., 2022; Min et al., 2022; Song et al., 2023). These approaches, such as Hierarchical Task Networks (Erol et al., 1994), leverage domain knowl-
edge, e.g., hand-specified library of plans, to break complex problems into simpler tasks. Our work embraces this tradition but distinguishes itself by exploring how LLMs can autonomously decompose tasks by leveraging their extensive world knowledge, without predefined plan libraries. Lastly, ADAPT performs dynamic hierarchical planning by employing its recursive structure.</p>
<h2>3 Methodology</h2>
<p>We introduce As-Needed Decomposition and Planning for complex Tasks (ADAPT), a modular approach for decision-making that integrates an LLM as an executor and a planner (Secs. 3.1 and 3.2) within an LLM program called the controller (Sec. 3.3). In Fig. 1, when ADAPT is given a complex task, it first attempts to accomplish the entire task by running the executor iteratively, and resorting to the LLM planner for further decomposition into sub-tasks if the executor fails. Subsequently, ADAPT is recursively called for each sub-task to ensure their successful completion, ultimately leading to overall task success.</p>
<h3>3.1 LLM as an Executor $\stackrel{\Delta}{\sim}$</h3>
<p>Overview. In a given environment, the executor is provided with a concise natural language task specification, as shown in Fig. 2 (left). Following Yao et al. (2023b), the executor iteratively interacts with the environment via actions generated by the LLM. This interaction continues until the task is either completed or a preset maximum iteration limit is reached. Consistent with Ahn et al. (2022), we provide the LLM with in-context demonstrations of low-level "atomic" skills specific to the environment (listed in Table 5 of Appendix A), such as knowing how to correctly heat objects in ALFWorld. This approach offers two advantages: (i) it allows us to employ the same executor with environment-specific knowledge for all baselines (Sec. 4.2); and (ii) it enables the planner (discussed in Sec. 3.2) to work at a higher level of abstraction, leveraging the LLM's general world knowledge.</p>
<p>Execution Capabilities of an LLM. At a minimum, the LLM executor should reliably execute atomic skills. While we provide demonstrations for successful execution of atomic skills, LLMs can adapt to failures by combining multiple skills to perform complex tasks, as discussed in Sec. 6.2. For instance, in Fig. 2 (left), we show the LLM successfully cleaning a mug it's carrying (an atomic</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Block diagram of the ADAPT pipeline with an example from ALFWorld. Left: Use of LLM as an executor to interact iteratively with the environment along with an example execution trajectory. Middle: Overall recursive algorithm (depth k ≤ dmax) that embeds the executor and planner, refer to Algorithm 1 for details. Right: Outline of using LLM as a planner to generate sub-tasks (steps) and logical operators combining them.</p>
<p>skill). An advanced executor could combine "finding a mug" with the "cleaning" skill to accomplish "find a clean mug" without an explicit planner.</p>
<p>Self-generated Success Heuristic. In order to decompose based on the abilities of the executor, we need to determine whether the executor is capable of finishing the given (sub-)task independently or if further decomposition is required. To this end, we employ the executor LLM to determine the completion of the (sub-)task <em>without relying on the environment</em> for obtaining gold rewards for (sub-)tasks. We include a simple instruction in the executor prompt to output "<em>task completed</em>" if it determines it has succeeded, otherwise output "<em>task failed</em>" in case it cannot proceed. Refer to example in Fig. 2 (left). Our success heuristic aligns with binary classification models employed in Shinn et al. (2023), providing a way to simulate intermediate rewards, which complements end-of-task environment rewards (Rengarajan et al., 2022). We study this LLM-generated heuristic in Appendix F and show that it closely matches the gold reward.</p>
<h3>3.2 LLM as a Planner</h3>
<p>Overview. The objective of the planner is to break down complex tasks into smaller sub-tasks. To achieve this, we instruct the LLM to generate a concise yet comprehensive plan consisting of a few steps, typically 3-5, as shown in Fig. 2 (right). We opt for shorter, more abstract plans because expecting a detailed, fine-grained plan upfront can be impractical, especially in unexplored environments. E.g., devising a 10-step plan to put a clean mug on a desk without prior knowledge of the mug's location can lead to cascading errors due to incorrect assumptions. Therefore, we task the LLM to generate short plans, with the <em>flexibility to decompose further</em> in subsequent iterations, based on the executor's capabilities.</p>
<p>Composition Logic for Sub-tasks. Along with the sub-tasks, we prompt the planner to generate logical operators to combine various sub-tasks in the plan to accomplish the task. We allow for two logical operators: "AND" and "OR". Sub-tasks are linked using AND when they must be executed sequentially for the task to succeed. However, in cases requiring exploration, such as finding an item in an unknown room, we employ the OR operator to simulate conditional checks. Here, the task succeeds if any of the sub-tasks are successful. For instance, in Fig. 1, the plan to "<em>find a mug</em>" would be to "<em>find a mug on the countertop</em>" OR "<em>find a mug in the cabinet</em>". We execute the latter only if the agent has not found the mug yet. While examples in Figs. 1 and 2 show homogeneous logic, ADAPT can handle complex logical expressions as described in Appendix B.</p>
<h3>3.3 Controller – LLM Program</h3>
<p>Overall Pipeline. Thus far, we describe two LLM-based modules that can perform the roles of low-level execution and high-level planning. We incorporate these modules into ADAPT via the controller which is a pre-determined and recursive algorithm – making the overall pipeline of ADAPT an LLM program (Schlag et al., 2023; Dohan et al., 2022), shown in Algorithm 1. The overall flow of the controller program is as follows: (i) given an input task, the controller calls the executor to check if it can succeed in performing the task directly; (ii)</p>
<p>if the executor does not succeed, the controller delegates decomposing the complex task to the planner and recursively calls ADAPT for each sub-task until we hit a termination criterion, i.e., if a maximum depth $d_{\max }(\geq 1)$ is reached.</p>
<p>Fig. 2 (mid) shows the control flow of ADAPT. A complex task such as "put a clean mug on the desk" is first assigned to the executor. If the executor does not succeed, then ADAPT calls the planner to decompose the task into sub-tasks along with a logical operator (AND or OR) indicating how to compose them. Each sub-task (referred to as 'step' in Fig. 2) is then assigned recursively to ADAPT and is combined using the logical operator. In the end, the success of sub-tasks after recursive decomposition ensures overall task success (unrolled calls to planner and executor are shown in Fig. 1).</p>
<h2>4 Experimental Setup</h2>
<p>We describe the datasets used in our experiments and baselines used for comparison with ADAPT.</p>
<h3>4.1 Datasets</h3>
<p>We employ LLMs-as-agents to perform tasks in the following three environments and use task success rate as our evaluation metric in Secs. 5 and 6.</p>
<p>ALFWorld. ALFWorld (Shridhar et al., 2021) is a text-based game version of the embodied ALFRED benchmark (Shridhar et al., 2020) implemented in the TextWorld environment (Côté et al., 2019). It encompasses 6 distinct task types, where an agent is required to accomplish high-level tasks through navigation and interaction via text-based actions in a simulated household that gives textual feedback to an agent (e.g., put a clean mug on desk discussed earlier in Fig. 2). Following Shridhar et al. (2021), we present results on 134 unseen evaluation games (test set) with a separate dev set of 10 games per task from the seen evaluation games split. Along with atomic skills, we add example gold trajectories, following Yao et al. (2023b), for two tasks: heat and look in the executor prompt. ${ }^{3}$</p>
<p>WebShop. WebShop (Yao et al., 2022) is an online shopping website environment featuring 1.18 million real-world products containing 500 user queries in the test set. It serves as a complex</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Example gold trajectory in TextCraft for a task with recipe depth of 2.
decision-making environment with practical applications wherein an agent must navigate a website through a variety of commands to purchase an item matching a user specification (e.g., grey sectional sofa priced less than $\$ 300$ with fast delivery). Following Shinn et al. (2023), we report performance on 100 user instructions and use a different subset of 40 queries as the dev set.</p>
<p>TextCraft. We create a new text-only environment for crafting Minecraft ${ }^{4}$ items similar to WordCraft (Coenen et al., 2021). Unlike existing agentbased environments, tasks in TextCraft exhibit a natural compositional structure, resembling cooking recipes with steps of varying complexity, where some sub-tasks are more intricate, such as layering a lasagna, while others are simpler, like baking it.</p>
<p>Tasks in TextCraft are inherently decomposable. In Fig. 3, crafting a beehive necessitates crafting its ingredients, like planks and honeycomb, which may require further decomposition. The agent thus needs to identify and adapt to varying task complexity, e.g., crafting a plank is easier than crafting a beehive. Moreover, some recipes allow using any item from a particular category. For instance, crafting a beehive uses planks (a category), requiring the agent to use linguistic knowledge for proper item selection (e.g., select oak planks, a specific item in the category planks). We evaluate our approach on a test set of 200 tasks where the target items have recipe trees of depth 2,3 , and 4 (example tree of depth 2 is shown in Fig. 3). We use the</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method ( $d_{\max }=3$ )</th>
<th style="text-align: center;">Pick</th>
<th style="text-align: center;">Clean</th>
<th style="text-align: center;">Heat</th>
<th style="text-align: center;">Cool</th>
<th style="text-align: center;">Look</th>
<th style="text-align: center;">Pick2</th>
<th style="text-align: center;">All</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ReAct</td>
<td style="text-align: center;">33.3</td>
<td style="text-align: center;">$\underline{67.7}$</td>
<td style="text-align: center;">43.5</td>
<td style="text-align: center;">33.3</td>
<td style="text-align: center;">55.6</td>
<td style="text-align: center;">$\underline{11.8}$</td>
<td style="text-align: center;">43.3</td>
</tr>
<tr>
<td style="text-align: left;">Plan-and-Execute</td>
<td style="text-align: center;">29.2</td>
<td style="text-align: center;">61.3</td>
<td style="text-align: center;">47.8</td>
<td style="text-align: center;">38.1</td>
<td style="text-align: center;">$\mathbf{6 1 . 1}$</td>
<td style="text-align: center;">$\underline{11.8}$</td>
<td style="text-align: center;">43.3</td>
</tr>
<tr>
<td style="text-align: left;">Try Again with ReAct</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">51.6</td>
<td style="text-align: center;">$\underline{60.8}$</td>
<td style="text-align: center;">47.6</td>
<td style="text-align: center;">$\mathbf{6 1 . 1}$</td>
<td style="text-align: center;">5.9</td>
<td style="text-align: center;">47.8</td>
</tr>
<tr>
<td style="text-align: left;">Reflexion</td>
<td style="text-align: center;">70.8</td>
<td style="text-align: center;">61.3</td>
<td style="text-align: center;">$\mathbf{6 1 . 0}$</td>
<td style="text-align: center;">$\underline{66.7}$</td>
<td style="text-align: center;">$\mathbf{6 1 . 1}$</td>
<td style="text-align: center;">5.9</td>
<td style="text-align: center;">$\underline{57.5}$</td>
</tr>
<tr>
<td style="text-align: left;">ADAPT (Ours)</td>
<td style="text-align: center;">$\mathbf{8 7 . 5}$</td>
<td style="text-align: center;">$\mathbf{8 0 . 6}$</td>
<td style="text-align: center;">$\underline{60.8}$</td>
<td style="text-align: center;">$\mathbf{7 6 . 2}$</td>
<td style="text-align: center;">$\mathbf{6 1 . 1}$</td>
<td style="text-align: center;">$\mathbf{5 2 . 9}$</td>
<td style="text-align: center;">$\underline{\mathbf{7 1 . 6}}$</td>
</tr>
</tbody>
</table>
<p>Table 1: ADAPT yields the highest the overall success rates (\%) compared to baselines from prior work (discussed in Sec. 4.2) on ALFWorld (test split). Best (highest) success rates are highlighted in bold and second-highest rates are underlined.
items with recipe tree depth of 3 (123 tasks), depth of 4 (11 tasks) and depth of 2 (77 out of 297) in our test set, and the rest of depth 2 tasks constitute the dev set. Additional details about creating the environment are present in Appendix E.</p>
<h3>4.2 Baseline Approaches</h3>
<p>We compare ADAPT with four classes of baseline approaches described below.</p>
<p>Iterative Executor-Only (ReAct). In this setting, we employ the executor to interact iteratively with the environment, adopting the think-act-observe prompting style from ReAct (Yao et al., 2023b). All methods discussed below, including ADAPT, share the same executor, ensuring a standardized impact of the executor's strength and design choices when comparing relative performance in Sec. 5. When $d_{\max }=1$, ADAPT solely relies on this executor.</p>
<p>Plan-and-Execute. As shown in Fig. 1, in this setting, we generate a plan first and then assign each sub-task to the executor. This approach only plans once and as a result has a non-adaptive structure (consistent with Wang et al. (2023b); Yang et al. (2023); Sun et al. (2023)). To ensure each plan step is executable without further decomposition, we design new prompts with more detailed plans. Note that ADAPT with $d_{\max }=2$ differs from plan-and-execute as it is adaptive, i.e., decomposes only when executor fails and generates relatively shorter plans (refer to Appendix B).</p>
<p>Try Again with ReAct. By design, ADAPT makes multiple calls to the executor module, albeit with different (sub-)tasks. Like Yang et al. (2023), we design a simple controller that requests the executor to retry the task in a total of $d_{\max }$ separate trials and then uses the trial with the best performance for each task instance.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">WebShop</th>
<th style="text-align: center;">TextCraft</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ReAct</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">19.0</td>
</tr>
<tr>
<td style="text-align: left;">Plan-and-Execute</td>
<td style="text-align: center;">17.0</td>
<td style="text-align: center;">27.0</td>
</tr>
<tr>
<td style="text-align: left;">Try Again with ReAct</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">15.0</td>
</tr>
<tr>
<td style="text-align: left;">Reflexion</td>
<td style="text-align: center;">$35.0^{\dagger}$</td>
<td style="text-align: center;">$\underline{32.0}$</td>
</tr>
<tr>
<td style="text-align: left;">LATS (Zhou et al., 2023)</td>
<td style="text-align: center;">$\underline{38.0}^{\dagger}$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">ADAPT (Ours)</td>
<td style="text-align: center;">$\mathbf{4 4 . 0}$</td>
<td style="text-align: center;">$\mathbf{5 2 . 0}$</td>
</tr>
</tbody>
</table>
<p>Table 2: ADAPT yields the highest success rate on WebShop and TextCraft (test split) with $d_{\max }=3$ and 4 respectively. ${ }^{\dagger}$ Performance reported by Zhou et al. (2023)</p>
<p>Reflexion. Shinn et al. (2023) execute the entire task first, and if unsuccessful, reflect and store feedback in memory for subsequent $d_{\max }-1$ trials. While adaptive, this approach repeats the entire trial even if a single sub-task fails, redundantly re-executing previously successful sub-tasks.</p>
<p>ADAPT and Shared Implementation Details. Following (Yao et al., 2023b; Shinn et al., 2023; Zhou et al., 2023), by default, we use the GPT3.5 (Ouyang et al., 2022) LLM for both planning and execution in ADAPT and other baselines. We use the completion-based models for ALFWorld and TextCraft and the chat-based model for WebShop. ${ }^{5}$ Further, we use ADAPT (and other baselines) with $d_{\max }=3$ for ALFWorld, and WebShop and increase to $d_{\max }=4$ for TextCraft to accommodate recipes with a depth of 4 (Sec. 4.1). For additional details, refer to Appendix A. We increase the maximum number of iterations for the ReAct baseline by a factor of $d_{\max }$ and ensure all baselines use a comparable number of LLM calls (Sec. 6.5).</p>
<h2>5 Main Results</h2>
<p>Using GPT-3.5 as the underlying LLM, in this section, we show that ADAPT yields the highest success rate compared to baselines from prior work on ALFWorld, WebShop, and TextCraft datasets.</p>
<p>ALFWorld. In Table 1, we observe that ADAPT achieves the highest overall success rate, while using ReAct alone results in the lowest overall performance. By leveraging adaptive decomposition, ADAPT improves over ReAct's performance by $28.3 \%$ points (absolute) as well as over Plan-and-Execute and Try Again by $28.3 \%$ and $23.8 \%$ points, respectively. Lastly, we find that ADAPT</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Success rate of ADAPT increases with the maximum depth dmax for all datasets (dev splits).</p>
<p>The yields 14.1% points higher overall success rate than Reflexion, despite the latter having access to dedicated memory and natural language feedback. Specifically, we find baselines yield poor results on 'pick2' tasks (&lt; 12% success rate) as they require the agent to compose two 'pick'-style tasks involving a longer action history. However, ADAPT yields significant improvements (by over a factor of 4×) for this type of tasks.</p>
<p><strong>WebShop.</strong> Table 2 shows a similar trend with ADAPT <em>surpassing all baselines</em> and achieving the highest success rate. ADAPT outperforms ReAct, Plan-and-Execute, and Try-Again baselines by up to 27% points. We corroborate the findings of Shinn et al. (2023) and observe that natural language feedback offers limited gains in performance, as compared to ADAPT (which surpasses Reflexion by 9% points). Additionally, we compare with a recent search-based baseline LATS (Zhou et al., 2023) and find that ADAPT outperforms the success rate of LATS by 6% points.</p>
<p><strong>TextCraft.</strong> Our results on TextCraft are summarized in Table 2. First, we observe that ADAPT <em>achieves an improvement of</em> 33% compared to the ReAct executor. In contrast to Plan-and-Execute, i.e., starting with a fixed plan, having the dynamic ability to adapt to complex sub-tasks (in this case, crafting complex ingredients) in ADAPT improves performance by 25% points. Lastly, ADAPT outperforms Reflexion by 20% points, highlighting the importance of adaptive and as-needed planning. We hypothesize that ADAPT consistently outperforms Reflexion across datasets as the latter relies on generating feedback based on errors in the entire trajectory. In contrast, due to its design, ADAPT often handle failures of small sub-tasks and redirects more resources in the form of calling the planner.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: ADAPT improves success rates across varying settings capturing different executor capabilities (i.e., executor-only performance) on ALFWorld (dev).</p>
<p>and decomposition to the challenging sub-tasks.</p>
<h2>6 Analysis and Discussion</h2>
<p>We analyze ADAPT in detail by addressing the following research questions on dev data splits.</p>
<h3>6.1 How does performance of ADAPT scale with the depth of decomposition?</h3>
<p><strong>Setup.</strong> To assess the impact of adaptive decomposition, we study ADAPT under three settings with increasing maximum depth dmax ∈ {1, 2, 3} for ALFWorld, WebShop, and TextCraft. Note that dmax = 1 setting corresponds to the iterative executor-only baseline (ReAct).</p>
<p><strong>Results.</strong> Fig. 4 shows that across all datasets, performance of ADAPT scales with increasing the maximum depth dmax. Consistently, we find a significant improvement in success rates as we move from dmax = 1 to dmax = 2, i.e., adding the planner to decompose a complex task when executor fails to prove to be effective. Finally, the performance increase from dmax = 2 to dmax = 3 validates our hypothesis that some sub-tasks are difficult for the LLM to directly execute successfully, and decomposing these further boosts overall performance.</p>
<h3>6.2 Does ADAPT cater to different execution capabilities of LLMs?</h3>
<p><strong>Same LLM, different execution capabilities.</strong> We run ADAPT on three different executor prompts on ALFWorld: (i) task-specific gold trajectories, (ii) atomic skills and common gold-trajectories for 2 tasks used in Sec. 5 (hybrid), and (iii) only atomic skills. Using gold trajectories aligns closely with the task at inference time and thus, should exhibit high performance. In contrast,</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: ADAPT improves (test) performance of GPT-3.5, GPT-4, LLaMA, and Lemur LLMs across datasets.</p>
<p>The executor using only atomic skills relies on the inherent composition abilities of the LLM, yielding weaker performance. Here we examine if ADAPT can improve success rates for all three settings.</p>
<p><strong>Results.</strong> In Fig. 5, we observe that ADAPT consistently improves over the executor-only baseline for <em>all diverse executor settings</em>. As expected, the executor prompted with task-specific trajectories performs the best (left), while the executor with only atomic skills performs the worst (right). Notably, ADAPT substantially improves performance of the relatively weak executor, improving success rate from 3.3% to 41.7%.</p>
<p><strong>ADAPT with different LLMs.</strong> We study the ability of ADAPT to improve performance across different LLMs (as planners and executors): (i) GPT-3.5, (ii) GPT-4 (OpenAI, 2023), (iii) LLaMA-2 70B (Touvron et al., 2023), and (iv) Lemur 70B (Xu et al., 2023) on test splits of all datasets.</p>
<p><strong>Results.</strong> Fig. 6 shows that ADAPT consistently improves downstream performance for <em>all</em> models across <em>all</em> three datasets. Consistent with Liu et al. (2023), we find that the gated GPT models outperform the open-source models based on absolute success rates. Nevertheless, ADAPT is effective across LLMs and improves performance of GPT-4, the strongest LLM, by up to 37%, as well as LLaMA, the least performant LLM, by up to 15% on the TextCraft dataset.</p>
<h3>6.3 Does ADAPT handle task complexity?</h3>
<p><strong>Setup.</strong> By the compositional design of TextCraft, complexity of each task in the dataset can be defined with respect to the depth of the crafting recipe, i.e., recipes with higher depth would be more complex to craft. We evaluate efficacy of ADAPT and the ReAct baseline on the test set of TextCraft.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Recipe Depth</th>
<th></th>
<th>Success Rate</th>
</tr>
</thead>
<tbody>
<tr>
<td>ReAct</td>
<td>2</td>
<td>1.0</td>
<td>26.9</td>
</tr>
<tr>
<td>ADAPT (dmax = 4)</td>
<td>2</td>
<td>1.9</td>
<td>78.2</td>
</tr>
<tr>
<td>ReAct</td>
<td>3</td>
<td>1.0</td>
<td>1.8</td>
</tr>
<tr>
<td>ADAPT (dmax = 4)</td>
<td>3</td>
<td>2.8</td>
<td>38.7</td>
</tr>
</tbody>
</table>
<p>Table 3: ADAPT improves TextCraft (test) performance even as recipe depth increases. The maximum decomposition depth used by ADAPT to succeed at the task (kmax) also scales with the recipe depth.</p>
<p>with increasing recipe depth.^{6} Furthermore, while we provide ADAPT with a maximum budget of dmax = 4, we study how the maximum decomposition depth utilized by ADAPT to succeed (kmax) varies with task complexity.</p>
<p><strong>Results.</strong> In Table 3 we observe that ADAPT improves success rates for games with recipe depth of 2 from 26.9% to 78.2%, and of depth 3 from 1.8% to 38.7% as compared to the ReAct baseline. As expected, the executor alone is unable to handle complex recipes with depth ≥ 3, but with the help of ADAPT the performance improves significantly. Additionally, given the same budget dmax = 4, as the recipe depth (complexity) increases from 2 to 3, ADAPT's level of decomposition (kmax) also increases from 1.9 to 2.8. This showcases that ADAPT leverages as-needed decomposition in order to handle task complexity.</p>
<h3>6.4 Can we use different planner and executor LLMs within ADAPT?</h3>
<p><strong>Setup.</strong> The planner and executor modules of ADAPT do not need to necessarily use the same underlying model. Following, Lin et al. (2023) we explore if a relatively smaller LLM can be used to perform local actions in the executor and a more</p>
<p>^{6}As we have only 11 tasks with recipe depth of 4, we exclude them from this analysis.</p>
<table>
<thead>
<tr>
<th>Executor LM</th>
<th>Planner LM</th>
<th>Success Rate</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT-3.5</td>
<td>-</td>
<td>38.4</td>
</tr>
<tr>
<td>GPT-3.5</td>
<td>GPT-3.5</td>
<td>58.3</td>
</tr>
<tr>
<td>LLaMA-2-70B</td>
<td>-</td>
<td>20.4</td>
</tr>
<tr>
<td>LLaMA-2-70B</td>
<td>GPT-3.5</td>
<td>43.3</td>
</tr>
</tbody>
</table>
<p>Table 4: ADAPT improves performance on ALFWorld (dev) when using different planner and executor LLMs.
advanced LLM be used to devise plans. To this end, we explore different combinations of planner and executor LLM, with the latter using both gated and open-source models on ALFWorld.</p>
<p>Results. Table 4 shows that ADAPT can successfully be used to generate plans from one LLM that are useful to a different, possibly smaller, executor LLM, improving success rates by up to $19.9 \%$ compared to the executor-only (ReAct) setting. Interestingly, using an open-source model, such as LLaMA-2-70B-chat (Touvron et al., 2023) can be used as an executor with a more advanced LLMs such as GPT-3.5 to improve success rates by $22.9 \%$ points. Since the planner LLM is used sparingly, open-source executors can dramatically decrease the monetary or computational costs of using ADAPT. We defer combining knowledge from stronger and weaker LMs within ADAPT to future work, as examined in the context of mathematical reasoning (Fu et al., 2023; Saha et al., 2023a).</p>
<h3>6.5 How does ADAPT compare to baselines in terms of LLM calls?</h3>
<p>Setup. Performance of decision-making agents can be enhanced by increasing the number of calls allowed to an LLM, e.g., number of retrials in Reflexion. To verify that the gains in ADAPT are not simply due to higher number of LLM calls, we compare the average of number of LLM calls made by ADAPT to the baselines.</p>
<p>Results. Fig. 7 shows that a ADAPT employs a comparable number of LLM calls w.r.t. Try-Again and Reflexion baselines in order to yield performance improvements discussed in Sec. 5 (Tables 1 and 2). Note that while all methods including ReAct and Plan-and-Execute baselines are offered a comparable computational budget, the actual number of LLM calls used by the latter is often lower due to their inability to handle intermediate execution failures. This strengthens the argument for effectiveness of ADAPT as the improvements do
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Average number of LLM calls for each approach including ADAPT and baselines discussed in Sec. 4.2 with GPT-3.5 LLM across datasets.
not simply stem from using substantially higher number of calls to the LLM.</p>
<h2>7 Conclusion</h2>
<p>We introduce ADAPT, a recursive algorithm designed to harness the planning capabilities of LLMs, dynamically decomposing complex tasks when the LLM acting as an executor encounters challenges. Our evaluation across three diverse decision-making tasks, ALFWorld, WebShop, and TextCraft, reveals impressive performance of ADAPT, surpassing existing baselines by substantial margins of up to $28.3 \%, 27 \%$, and $33 \%$ points, respectively. This not only underscores the effectiveness of ADAPT but also highlights the significance of as-needed decomposition in enhancing task performance. Moreover, our findings demonstrate that ADAPT not only adapts to the capabilities of the underlying executor LLM but also takes into account the complexity of individual task instances, showcasing its versatility and effectiveness.</p>
<h2>Acknowledgements</h2>
<p>Part of this work was done during internship at AI2 and was partially supported at UNC by NSFCAREER Award 1846185, NSF-AI Engage Institute DRL-2112635, DARPA Machine Commonsense (MCS) Grant N66001-19-2-4031,. We sincerely thank Bodhisattwa Prasad Majumder, Chris Callison-Burch, Shashank Gupta, Peter Jansen, Bill Yuchen Lin and the Aristo team for their valuable feedback. We also thank Swarnadeep Saha, Elias Stengel-Eskin, and Peter Hase for their feedback.</p>
<h2>Limitations</h2>
<p>ADAPT relies on the success heuristic generated by the executor LLM to determine if the model is capable of performing a complex task. For decision-making tasks studied in this work, we find that LLMs can reliably determine task success based on past action trajectories and textual feedback from the environment (see Appendix F). However, Huang et al. (2023a); Stechly et al. (2023) discuss the limits of LLM's ability to self-evaluate and self-refine. In such situations, future works may additionally employ external verifiers (Lightman et al., 2023; Shridhar et al., 2023), theory-ofmind strategies among multiple LMs (Saha et al., 2023a), and other calibration and self-evaluation techniques (Kadavath et al., 2022). These improved self-evaluation techniques could be useful to extend our framework to non-decision making tasks such as question answering.</p>
<h2>References</h2>
<p>Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et al. 2022. Do as i can, not as i say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691.</p>
<p>Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. 2016. Neural module networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 39-48.</p>
<p>Andrew G Barto and Sridhar Mahadevan. 2003. Recent advances in hierarchical reinforcement learning. Discrete event dynamic systems, 13(1-2):41-77.</p>
<p>Valts Blukis, Chris Paxton, Dieter Fox, Animesh Garg, and Yoav Artzi. 2022. A persistent spatial semantic representation for high-level natural language instruction execution. In Conference on Robot Learning, pages 706-717. PMLR.</p>
<p>Andy Coenen, Luke Davis, Daphne Ippolito, Emily Reif, and Ann Yuan. 2021. Wordcraft: a human-ai collaborative editor for story writing. arXiv preprint arXiv:2107.07430.</p>
<p>Marc-Alexandre Côté, Akos Kádár, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, et al. 2019. Textworld: A learning environment for text-based games. In Computer Games: 7th Workshop, CGW 2018, Held in Conjunction with the 27th International Conference on Artificial Intelligence, IJCAI 2018, Stockholm, Sweden, July 13, 2018, Revised Selected Papers 7, pages 41-75. Springer.</p>
<p>David Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael Gontijo Lopes, Yuhuai Wu, Henryk Michalewski, Rif A Saurous, Jascha Sohl-Dickstein, et al. 2022. Language model cascades. arXiv preprint arXiv:2207.10342.</p>
<p>Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jian, Bill Yuchen Lin, Peter West, Chandra Bhagavatula, Ronan Le Bras, Jena D Hwang, et al. 2023. Faith and fate: Limits of transformers on compositionality. arXiv preprint arXiv:2305.18654.</p>
<p>Kutluhan Erol, James Hendler, and Dana S Nau. 1994. Htn planning: Complexity and expressivity. In AAAI, volume 94, pages 1123-1128.</p>
<p>Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. 2022. Minedojo: Building open-ended embodied agents with internet-scale knowledge. Advances in Neural Information Processing Systems, 35:1834318362.</p>
<p>Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and Tushar Khot. 2023. Specializing smaller language models towards multi-step reasoning. arXiv preprint arXiv:2301.12726.</p>
<p>Ilche Georgievski and Marco Aiello. 2014. An overview of hierarchical task network planning. arXiv preprint arXiv:1403.7426.</p>
<p>Malik Ghallab, Dana Nau, and Paolo Traverso. 2004. Automated Planning: theory and practice. Elsevier.</p>
<p>Nitish Gupta, Kevin Lin, Dan Roth, Sameer Singh, and Matt Gardner. 2019. Neural module networks for reasoning over text. In International Conference on Learning Representations.</p>
<p>Daniel Höller, Gregor Behnke, Pascal Bercher, Susanne Biundo, Humbert Fiorino, Damien Pellier, and Ron Alford. 2020. Hddl: An extension to pddl for expressing hierarchical planning problems. In Proceedings of the AAAI conference on artificial intelligence, pages 9883-9891.</p>
<p>Jie Huang and Kevin Chen-Chuan Chang. 2023. Towards reasoning in large language models: A survey. In Findings of the Association for Computational Linguistics: ACL 2023, pages 1049-1065, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. 2023a. Large language models cannot self-correct reasoning yet. arXiv preprint arXiv:2310.01798.</p>
<p>Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al. 2023b. Inner monologue: Embodied reasoning through planning with language models. In Conference on Robot Learning, pages 1769-1782. PMLR.</p>
<p>Yichen Jiang and Mohit Bansal. 2019. Self-assembling modular networks for interpretable multi-hop reasoning. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4474-4484, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et al. 2022. Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221.</p>
<p>Tushar Khot, Daniel Khashabi, Kyle Richardson, Peter Clark, and Ashish Sabharwal. 2021. Text modular networks: Learning to decompose tasks in the language of existing models. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1264-1279, Online. Association for Computational Linguistics.</p>
<p>Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. 2023. Decomposed prompting: A modular approach for solving complex tasks. In The Eleventh International Conference on Learning Representations.</p>
<p>Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023. Let's verify step by step. arXiv preprint arXiv:2305.20050.</p>
<p>Bill Yuchen Lin, Yicheng Fu, Karina Yang, Prithviraj Ammanabrolu, Faeze Brahman, Shiyu Huang, Chandra Bhagavatula, Yejin Choi, and Xiang Ren. 2023. Swiftsage: A generative agent with fast and slow thinking for complex interactive tasks. arXiv preprint arXiv:2305.17390.</p>
<p>Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. 2023. Agentbench: Evaluating llms as agents. arXiv preprint arXiv:2308.03688.</p>
<p>Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. 2023. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651.</p>
<p>Sewon Min, Victor Zhong, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2019. Multi-hop reading comprehension through question decomposition and rescoring. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6097-6109, Florence, Italy. Association for Computational Linguistics.</p>
<p>So Yeon Min, Devendra Singh Chaplot, Pradeep Kumar Ravikumar, Yonatan Bisk, and Ruslan Salakhutdinov.
2022. Film: Following instructions in language with modular methods. In International Conference on Learning Representations.</p>
<p>Vijayaraghavan Murali, Letao Qi, Swarat Chaudhuri, and Chris Jermaine. 2018. Neural sketch learning for conditional program generation. In International Conference on Learning Representations.</p>
<p>Ofir Nachum, Shixiang Shane Gu, Honglak Lee, and Sergey Levine. 2018. Data-efficient hierarchical reinforcement learning. Advances in neural information processing systems, 31.</p>
<p>Maxwell Nye, Luke Hewitt, Joshua Tenenbaum, and Armando Solar-Lezama. 2019. Learning to infer program sketches. In International Conference on Machine Learning, pages 4861-4870. PMLR.</p>
<p>OpenAI. 2023. Gpt-4 technical report.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744.</p>
<p>Ethan Perez, Patrick Lewis, Wen-tau Yih, Kyunghyun Cho, and Douwe Kiela. 2020. Unsupervised question decomposition for question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8864-8880, Online. Association for Computational Linguistics.</p>
<p>Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. 2023. Toolllm: Facilitating large language models to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789.</p>
<p>Desik Rengarajan, Gargi Vaidya, Akshay Sarvesh, Dileep Kalathil, and Srinivas Shakkottai. 2022. Reinforcement learning with sparse rewards using guidance from offline demonstration. In International Conference on Learning Representations.</p>
<p>Swarnadeep Saha, Peter Hase, and Mohit Bansal. 2023a. Can language models teach weaker agents? teacher explanations improve students via theory of mind. arXiv preprint arXiv:2306.09299.</p>
<p>Swarnadeep Saha, Shiyue Zhang, Peter Hase, and Mohit Bansal. 2023b. Summarization programs: Interpretable abstractive summarization with neural modular trees. In The Eleventh International Conference on Learning Representations.</p>
<p>Imanol Schlag, Sainbayar Sukhbaatar, Asli Celikyilmaz, Wen-tau Yih, Jason Weston, Jürgen Schmidhuber, and Xian Li. 2023. Large language model programs. arXiv preprint arXiv:2305.05364.</p>
<p>Pratyusha Sharma, Antonio Torralba, and Jacob Andreas. 2022. Skill induction and planning with latent language. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1713-1726, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Lanbo She, Shaohua Yang, Yu Cheng, Yunyi Jia, Joyce Chai, and Ning Xi. 2014. Back to the blocks world: Learning new actions through situated human-robot dialogue. In Proceedings of the 15th annual meeting of the special interest group on discourse and dialogue (SIGDIAL), pages 89-97.</p>
<p>Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed Huai hsin Chi, Nathanael Scharli, and Denny Zhou. 2023. Large language models can be easily distracted by irrelevant context. In International Conference on Machine Learning.</p>
<p>Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: Language agents with verbal reinforcement learning. arXiv preprint arXiv:2303.11366, 14.</p>
<p>Kumar Shridhar, Koustuv Sinha, Andrew Cohen, Tianlu Wang, Ping Yu, Ram Pasunuru, Mrinmaya Sachan, Jason Weston, and Asli Celikyilmaz. 2023. The art of llm refinement: Ask, refine, and trust. arXiv preprint arXiv:2311.07961.</p>
<p>Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. 2020. Alfred: A benchmark for interpreting grounded instructions for everyday tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10740-10749.</p>
<p>Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. 2021. ALFWorld: Aligning Text and Embodied Environments for Interactive Learning. In Proceedings of the International Conference on Learning Representations (ICLR).</p>
<p>Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. 2023. Progprompt: Generating situated robot task plans using large language models. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 11523-11530. IEEE.</p>
<p>Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M Sadler, Wei-Lun Chao, and Yu Su. 2023. Llm-planner: Few-shot grounded planning for embodied agents with large language models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2998-3009.</p>
<p>Kaya Stechly, Matthew Marquez, and Subbarao Kambhampati. 2023. Gpt-4 doesn't know it's wrong: An analysis of iterative prompting for reasoning problems. arXiv preprint arXiv:2310.12397.</p>
<p>Simeng Sun, Y. Liu, Shuo Wang, Chenguang Zhu, and Mohit Iyyer. 2023. Pearl: Prompting large language models to plan and execute actions over long documents. ArXiv, abs/2305.14564.</p>
<p>Richard S Sutton, Doina Precup, and Satinder Singh. 1999. Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. Artificial intelligence, 112(1-2):181-211.</p>
<p>Alon Talmor and Jonathan Berant. 2018. The web as a knowledge-base for answering complex questions. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 641-651, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.</p>
<p>Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. 2023a. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291.</p>
<p>Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. 2023b. Plan-and-solve prompting: Improving zeroshot chain-of-thought reasoning by large language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2609-2634, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. 2019. Huggingface's transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771.</p>
<p>Yiheng Xu, Hongjin Su, Chen Xing, Boyu Mi, Qian Liu, Weijia Shi, Binyuan Hui, Fan Zhou, Yitao Liu, Tianbao Xie, Zhoujun Cheng, Siheng Zhao, Lingpeng Kong, Bailin Wang, Caiming Xiong, and Tao Yu. 2023. Lemur: Harmonizing natural language and code for language agents.</p>
<p>John Yang, Akshara Prabhakar, Karthik Narasimhan, and Shunyu Yao. 2023. Intercode: Standardizing and benchmarking interactive coding with execution feedback. arXiv preprint arXiv:2306.14898.</p>
<p>Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. 2022. Webshop: Towards scalable realworld web interaction with grounded language agents. <em>Advances in Neural Information Processing Systems</em>, 35:20744–20757.</p>
<p>Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. 2023a. Tree of thoughts: Deliberate problem solving with large language models. <em>arXiv preprint arXiv:2305.10601</em>.</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. 2023b. React: Synergizing reasoning and acting in language models. In <em>The Eleventh International Conference on Learning Representations</em>.</p>
<p>Jesse Zhang, Haonan Yu, and Wei Xu. 2021. Hierarchical reinforcement learning by discovering intrinsic options. In <em>International Conference on Learning Representations</em>.</p>
<p>Wenqing Zheng, SP Sharan, Ajay Kumar Jaiswal, Kevin Wang, Yihan Xi, Dejia Xu, and Zhangyang Wang. 2023. Outline, then details: Syntactically guided coarse-to-fine code generation. In <em>International Conference on Machine Learning</em>, pages 42403–42419. PMLR.</p>
<p>Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. 2023. Language agent tree search unifies reasoning acting and planning in language models. <em>arXiv preprint arXiv:2310.04406</em>.</p>
<h3>A ADAPT Implementation Details</h3>
<p><strong>Executor.</strong> We use a common ReAct executor for each dataset. To this end, we provide the LLM in the executor with in-context example trajectories for each atomic skill (refer to Table 5 for an exhaustive list). Atomic skills are inherently task dependent, and thus, vary with the underlying environment. For ALFWorld, in which the agent needs to navigate and perform tasks in the household, the atomic skills include: taking an object, putting it down at a location, cleaning, heating, etc. On the other hand, the goal in WebShop is to buy a product based on user queries, thus, atomic skills include: searching a specified query, shortlisting products based on search page, matching if a product satisfies a criteria, and buying a product. Lastly, the atomic skills in TextCraft are fetching objects from the environment, and crafting them given the recipe and the ingredients. Following Yao et al. (2023b), we add gold trajectories for two tasks: heat and look in the executor prompt for ALFWorld, and one full gold trajectory for TextCraft.</p>
<table>
<thead>
<tr>
<th>Atomic Skill</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>ALFWorld</td>
<td></td>
</tr>
<tr>
<td>put</td>
<td>Assuming that the robot is carrying an object, put it on a given receptacle.</td>
</tr>
<tr>
<td>take</td>
<td>Take a specified object from a specified receptacle.</td>
</tr>
<tr>
<td>clean/heat/cool</td>
<td>Assuming that the robot is carrying an object, clean/heat/cool the object.</td>
</tr>
<tr>
<td>examine</td>
<td>Assuming the robot is at a desk with a desk lamp, use it to look at an object.</td>
</tr>
<tr>
<td>WebShop</td>
<td></td>
</tr>
<tr>
<td>search</td>
<td>Put a given query in the search box, results in a page with list of products.</td>
</tr>
<tr>
<td>shortlist</td>
<td>Based on the search page and query, get list of any matching products.</td>
</tr>
<tr>
<td>match</td>
<td>Given a product ID and query, navigate to the product page and verify it matches the query.</td>
</tr>
<tr>
<td>buy</td>
<td>Given a product ID and query, buy product by selecting relevant options.</td>
</tr>
<tr>
<td>TextCraft</td>
<td></td>
</tr>
<tr>
<td>craft</td>
<td>Assuming the agent has all the ingredients in the inventory, craft a target object by picking an appropriate command from the list of crafting recipes.</td>
</tr>
<tr>
<td>fetch</td>
<td>Look for a given object in the inventory or get it directly from the game.</td>
</tr>
<tr>
<td>inventory</td>
<td>Look-up the game inventory.</td>
</tr>
</tbody>
</table>
<p>Table 5: Overview of atomic skills used in Sec. 3.1.</p>
<p><strong>Planner.</strong> We provide the LLM with a brief description of atomic skills and in-context demonstrations of few task decompositions for each dataset.</p>
<ul>
<li><strong>ALFWorld:</strong> The planner includes 6 demonstrations of task decompositions for one household configuration. Specifically, <em>"find"</em> is not an atomic skill for the executor, and therefore, needs to be handled by the planner (refer to Fig. 2).</li>
<li><strong>WebShop:</strong> The planner breaks down a given task in terms of the atomic skills described in Table 5 via 2 in-context demonstrations.</li>
<li><strong>TextCraft:</strong> The planner determines the necessary ingredients for each item and creates a plan to obtain them and then craft the item, illustrated via 2 examples with different crafting commands.</li>
</ul>
<p><strong>Controller.</strong> The controller performs two crucial roles in the overall functioning of ADAPT. First, it serves as the <em>communication bridge</em> between planner and executor, propagating salient information across the two depending on the task. Second, since ADAPT is a recursive algorithm, the controller determines the <em>termination criterion</em> using the logical expression from the planner and success heuristic from the executor or if a maximum depth <em>d</em>max (≥1) is reached. The controller propagates task-dependent salient information described below:</p>
<ul>
<li><strong>ALFWorld:</strong> In the controller, we propagate the last successful action from a previous execution</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">Pick</th>
<th style="text-align: center;">Clean</th>
<th style="text-align: center;">Heat</th>
<th style="text-align: center;">Cool</th>
<th style="text-align: center;">Look</th>
<th style="text-align: center;">Pick2</th>
<th style="text-align: center;">All</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ReAct</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">41.9</td>
<td style="text-align: center;">47.8</td>
<td style="text-align: center;">80.9</td>
<td style="text-align: center;">83.3</td>
<td style="text-align: center;">23.5</td>
<td style="text-align: center;">56.7</td>
</tr>
<tr>
<td style="text-align: left;">Plan-and-Execute</td>
<td style="text-align: center;">$\underline{87.5}$</td>
<td style="text-align: center;">58.1</td>
<td style="text-align: center;">$\underline{73.9}$</td>
<td style="text-align: center;">52.4</td>
<td style="text-align: center;">$\underline{83.3}$</td>
<td style="text-align: center;">17.6</td>
<td style="text-align: center;">63.4</td>
</tr>
<tr>
<td style="text-align: left;">Try Again with ReAct</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;">38.7</td>
<td style="text-align: center;">60.9</td>
<td style="text-align: center;">76.2</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">23.5</td>
<td style="text-align: center;">56.7</td>
</tr>
<tr>
<td style="text-align: left;">Reflexion</td>
<td style="text-align: center;">83.3</td>
<td style="text-align: center;">$\underline{61.3}$</td>
<td style="text-align: center;">$\underline{73.9}$</td>
<td style="text-align: center;">$\mathbf{8 5 . 7}$</td>
<td style="text-align: center;">61.1</td>
<td style="text-align: center;">$\underline{29.4}$</td>
<td style="text-align: center;">$\underline{67.2}$</td>
</tr>
<tr>
<td style="text-align: left;">ADAPT (Ours)</td>
<td style="text-align: center;">$\mathbf{9 1 . 7}$</td>
<td style="text-align: center;">$\mathbf{6 7 . 7}$</td>
<td style="text-align: center;">$\mathbf{7 8 . 3}$</td>
<td style="text-align: center;">$\underline{81.0}$</td>
<td style="text-align: center;">$\mathbf{1 0 0}$</td>
<td style="text-align: center;">$\mathbf{6 4 . 7}$</td>
<td style="text-align: center;">$\mathbf{7 9 . 8}$</td>
</tr>
</tbody>
</table>
<p>Table 6: Comparison of success rates (\%) achieved by ADAPT and other baselines from prior work on ALFWorld (test split) with executor used by Yao et al. (2023b)</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">Score</th>
<th style="text-align: center;">Success Rate</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Iterative Executor-Only</td>
<td style="text-align: center;">42.1</td>
<td style="text-align: center;">29.0</td>
</tr>
<tr>
<td style="text-align: left;">Static Decomposition</td>
<td style="text-align: center;">27.7</td>
<td style="text-align: center;">17.0</td>
</tr>
<tr>
<td style="text-align: left;">Retry Execution</td>
<td style="text-align: center;">45.4</td>
<td style="text-align: center;">30.0</td>
</tr>
<tr>
<td style="text-align: left;">Naive</td>
<td style="text-align: center;">58.3</td>
<td style="text-align: center;">24.0</td>
</tr>
<tr>
<td style="text-align: left;">Reflexion*</td>
<td style="text-align: center;">64.2</td>
<td style="text-align: center;">35.0</td>
</tr>
<tr>
<td style="text-align: left;">LATS (Zhou et al., 2023)*</td>
<td style="text-align: center;">75.9</td>
<td style="text-align: center;">38.0</td>
</tr>
<tr>
<td style="text-align: left;">ADAPT (Ours)</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">$\mathbf{4 4 . 0}$</td>
</tr>
</tbody>
</table>
<p>Table 7: Performance comparison of different methods on WebShop.
run to subsequent calls of the executor. Note that information is only propagated from successful sub-tasks. For sub-tasks connected via "OR", each receives the same information from the controller. Unlike Shinn et al. (2023), executor does not get text feedback from prior failures.</p>
<ul>
<li>WebShop: We propagate the current page visible to the agent along with past unsuccessful executor tasks to the planner (without any rationales). Once we find a matching product, we also propagate the product ID in future executor calls.</li>
<li>TextCraft: We propagate the current inventory of the agent to the executor. This is akin to executors starting with the inventory command as the first step to keep stock of which items are missing and need to be fetched or crafted.
For partial rolled-out trajectories with ADAPT refer to Figs. 9 to 11. Communication between planner and executor is highlighted in gray box(es) .</li>
</ul>
<p>LLM-related Hyperparameters. Following previous works (Shinn et al., 2023; Liu et al., 2023) we use text-davinci-003 from the OpenAI API for ALFWorld. For WebShop, we use the gpt-3.5-turbo models, and for TextCraft we use
the gpt-3.5-turbo-instruct models. All executors have a maximum budget of iterations to interact with the environment and execute the task. We set this budget to 20,15 , and 20 respectively for ALFWorld, WebShop, and TextCraft respectively. For try again with ReAct, we sample additional trajectories with a temperature of 0.7 . As discussed in Sec. 4.2, we run the iterative executor-only baseline for 60, 45, 60 iterations for ALFWorld, WebShop, and TextCraft respectively. In Sec. 6.2, we use publicly available checkpoints for LLaMA $70 \mathrm{~B}^{7}$ and Lemur $70 \mathrm{~B}^{8}$ available on Huggingface (Wolf et al., 2019). For both planner and executor modules, we use a fixed prompt consisting of few in-context examples (as described above) for each dataset. We show all executor and planner prompts to the LLM in Appendix G. Due to cost constraints, we report success rates for a single run of each LLM in Secs. 5 and 6.</p>
<h2>B Handling Complex Logic in Plans</h2>
<p>While the examples in Figs. 1 and 2 show homogeneous logic across sub-tasks in the plan, our controller can handle complex logical expressions including both "AND" and "OR" operators. Specifically, we provide instructions to the planner to output this logical expressing at the end of the plan with a fixed prefix: Execution Order. We then build a deterministic parser that can parse complex logical expressions that the controller can process. We do so by splitting the logical expression into a series of homogeneous expression each passed to ADAPT. Whenever the task given to ADAPT comprises of multiple sub-tasks connected via (one) logical operator, we automatically decompose this task as per the logical expression. For example, in Fig. 8, a detailed plans used by the plan-andexecute baseline (discussed in Sec. 4.2) comprised</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Adaptive Multi-level Plans in ADaPT</p>
<p>Plan: Put a clean mug on desk
@ Think: To do this task.
Step 1: Find and take the mug AND
@ Think: Now that I have found it.
Step 2: Clean the mug using sinkbasin AND
@ Think: Now that I have cleaned ...
Step 3: Put clean mug on desk</p>
<p>Plan: Find and take the mug
@ Think: To do this task.
Step 1: Find and take mug from countertop OR
@ Think: If I do not find the mug, ...
Step 2: Find and take mug from cabinet OR
@ Think: If I do not find the mug,
Step 3: Find and take mug from drawer</p>
<p>Detailed Plans in Plan-and-Execute</p>
<p>Plan: Put a clean mug on desk
@ Think: To do this task.
Step 1: Find and take mug from
countertop OR
@ Think: If I do not find the mug,
Step 2: Find and take mug from cabinet OR
@ Think: If I do not find the mug, ...
Step 3: Find and take mug from drawer AND
@ Think: Now that I have found it.
Step 4: Clean the mug using sinkbasin AND
@ Think: Now that I have cleaned ...
Step 5: Put clean mug on desk</p>
<p>Logic: {(Step 1 OR Step 2 OR Step 3) AND
Step 4 AND Step 5)</p>
<p>Figure 8: Illustration of how multiple levels of plans
from ADAPT, can be collapsed into one detailed plan
in non-adaptive settings as used in the plan-and-execute
baseline (Sec. 4.2). Our controller can handle complex
(non-homogeneous) logical expressions.</p>
<p>of logical expressions using both AND, and OR
operators. Therefore, the parser will break auto-
matically break this into multiple levels, i.e., Step
6 = Step 1 OR Step 2 OR Step 3, followed by Step
6 AND Step 4 AND Step 5. While such complex
logical expressions are mostly associated with the
plan-and-execute baseline, they can be easily used
within the ADAPT framework. Furthermore, this
allows the plan-and-execute baseline to simulate
a multi-level planning structure via detailed plans
without being adaptive to the executor.</p>
<p>C Task-specific Executors in ALFWorld</p>
<p>In Table 1, we use a standardized executor with
in-context demonstrations of atomic skills and two
gold trajectories. While this allows for a common
executor across different sub-tasks, task-specific
executors yield higher performance on the specific
sub-tasks. We now show ADAPT can also be used
on top of task-specific executors used by Yao et al.
(2023b). The results are shown in Table 6. First,
Method #Products Success Rate
ReAct 3 27.5
ADAPT (dmax = 3) 3 47.5
ReAct 10 20.0
ADAPT (dmax = 3) 10 42.5</p>
<p>Table 8: ADAPT improves WebShop (dev) performance
irrespective of how many products (3 or 10) are chosen
from the search page.</p>
<p>we observe that ADAPT yields the overall success
rate by up to 23.1% points and also surpasses base-
lines on all but 1 task types. Interestingly, we find
strong performance of the plan-and-execute base-
line when using a stronger executor (as compared
to Table 1) possibly as such an executor can handle
complex sub-tasks better. Consistent with Table 1,
ADAPT outperforms Reflexion by 12.6% points
despite lack of dedicated memory and natural lan-
guage feedback.</p>
<p>D Additional WebShop Experiments</p>
<p>Evaluation Metrics. We focus on success rate
and not the (soft) score as the primary metric for
this task because it is possible to get a non-zero
score by naively buying a product. To this effect,
we construct a naive executor that inputs the user
query in the search bar and buys the first available
product. Table 7 shows that while this baseline
yields the lowest success rate, it surprisingly yields
a high success rate of 58.3. In contrast, our ex-
ecutors often do not buy products especially when
the previous sub-goals fail which can adversely im-
pact scores even though the success rate remains
unaffected. Therefore, we argue for optimizing the
success rate instead of the score as opposed to prior
works (Zhou et al., 2023).</p>
<p>ADAPT accommodating task complexity. By
default, Yao et al. (2023b) use a search page with
only the top-3 search results displayed. Intuitively,
increasing the number of products on the search
page requires the model to choose from a wider
array of products and track all their information to
determine the best fit to the user query, making the
overall task harder. Therefore, we apply ADAPT
on Webshop in two settings with 3, and 10 products
per search page.</p>
<p>Results. From Table 8, we observe that ADAPT
effectively improves success rate by 20.0% and
22.5% for 3 and 10 products respectively over the</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 9: Comparison of iterative executors such as ReAct with ADAPT. On left, ReAct uses interleaved "thought" statements to set milestones and track their progress. However, due to a large action history, it struggles to follow the plan exactly and hallucinates the wrong object (highlighted in red). ADAPT, on the right, decomposes complex tasks into smaller sub-tasks whenever the executor fails, leading to shorter action trajectories for easy execution.</p>
<p>ReAct baseline. The difference in ReAct performance for both settings corroborates our hypothesis that increasing number of products on the search page increases task complexity, all else equal. Notably, we show that ADAPT yields higher improvement for more complex task settings.</p>
<h2>E TextCraft</h2>
<p>TextCraft: Environment Details. In TextCraft, the objective is to obtain target Minecraft items by crafting them from available items in the environment. We define an environment with three ac-
tions: craft <item> using <ingredients>, get <item>, and inventory. We utilize Minecraft's crafting recipes to specify craftable items and their ingredients, assuming that all other items are obtainable from the environment. Similar to AlfWorld, our agent can directly execute these operations in the embodied game. The game begins with a list of crafting commands provided to the agent that detail recipes that can be used to craft the final target, its ingredients along with some distractors (details in Appendix E). A reward of 1 is generated when the target item gets added to the</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 10: Partial rolled out trajectories for WebShop with ADAPT. In the gray box we communicate to the planner the current (search) page that is visible to the agent, and once a matching product is found, we propagate it to future executor runs. Note "match on search page" corresponds to shortlist skill in Table 5, and "detail match on product page" corresponds to match skill.
agent's inventory. An illustrative gold trajectory from TextCraft is shown in Fig. 3.</p>
<p>We create the TextCraft environment using Minecraft v1.16.5 recipes. We only consider the recipes craftable using a crafting table. We consider both shapeless (only count matters) and shaped (position of ingredients matters) recipes and convert them into crafting commands (e.g. craft 4 sticks using 2 planks). Items that do not have any recipe are considering obtainable via the get command, e.g. get 4 diamond.</p>
<p>Since the entire set of crafting commands would not fit in the context of modern LLMs, we create
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 11: Partial rolled out trajectories for TextCraft using ADAPT. In the gray box, we propagate the inventory of the agent to subsequent executor calls. Note that while "diorite" is not directly present in the environment, i.e., it needs to be crafted. The executor LLM is able to inherently compose skills to fetch it without further decomposition.
a set of relevant crafting commands for every task. Apart from the set of gold crafting commands (i.e, crafting commands for all the items in the recipe tree), we also add up to 10 distractor commands. To create this distractor set, we sub-sample up to 10 recipes for every ingredient in the recipes of our gold recipe tree. We finally sub-sample up to 10 distractors from this entire set to ensure a reason-</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 12: Comparison of LLM-generated success heuristic with gold environment rewards to compute success rates for all datasets.
able context size. Note that we do not provide the list of valid get commands as that can be inferred from the craft commands.</p>
<h1>F Evaluation of Success Heuristic</h1>
<p>In Sec. 3.1, we describe the executor module used in ADAPT. For tasks assigned to the executor, we prompt the LLM to generate a binary success heuristic. We use this heuristic repeatedly to evaluate if the (sub-)task needs to be decomposed further. We now study the ability of LLMs to generate this success heuristic on all our datasets. To this end, we run ADAPT and in the end compare the success rate when using the LLM's self-assessed task success with the gold reward from the environment in Fig. 12. On ALFWorld and TextCraft, we find the LLM slightly over-estimates its overall task success. This is to be expected as the underlying tasks involve minimal subjectivity (e.g., the agent either has an item on its inventory or not). However, on WebShop, where a product can match the user criteria to different degrees (partially or fully), we find that the LLM's assessment is significantly inflated compared to the environment reward ( $&gt;30$ points). This imperfect feedback affects downstream performance of ADAPT, as the algorithm terminates even though further decomposition is needed. We leave it to future work to address the shortcomings of self-evaluation with LLMs (Huang et al., 2023a; Stechly et al., 2023).</p>
<h2>G Prompts</h2>
<p>We provide all the prompts used in our planner and executor modules for ALFWorld, WebShop, and TextCraft datasets in the following pages.</p>
<h1>ALFWorld Hybrid Executor Prompt</h1>
<p>Here is a demo of actions you can perform.
You are in the middle of a room. Looking quickly around you, you see a desk 1, microwave 1, a cabinet 3, a cabinet 9, a drawer 2, a coffeemachine 1, a stoveburner 4, a drawer 5, a cabinet 11, a drawer 3, a stoveburner 1, a drawer 1, a toaster 1, a fridge 1, a stoveburner 2, a cabinet 6, a cabinet 10, a countertop 1, a cabinet 13, a cabinet 7, a garbagecan 1, a cabinet 2, a cabinet 8, a cabinet 12, a drawer 4, a cabinet 1, a sinkbasin 1, a cabinet 5, a stoveburner 3, and a cabinet 4.</p>
<p>Your task is to: Put a tomato in cabinet 1.</p>
<blockquote>
<p>think: Input assumption: I am carrying a tomato. Now I need to verify this.
OK.
$&gt;$ inventory
You are carrying: a tomato 1.
$&gt;$ examine tomato 1
There's nothing special about tomato 1.
think: Based on previous context assumption is true, now I can proceed.
$&gt;$ go to cabinet 1
The cabinet 1 is closed.
$&gt;$ open cabinet 1
You open the cabinet 1. The cabinet 1 is open. In it, you see a candle 1, and a spraybottle 2.
$&gt;$ put tomato 1 in/on cabinet 1
You put the tomato 1 in/on the cabinet 1.
think: End state: Now I put tomato (1) in cabinet (1). Task completed!
Your task is to: Take a mug from cabinet 5
$&gt;$ think: Input assumption: None
OK.
$&gt;$ go to cabinet 5
The cabinet 5 is closed.
$&gt;$ open cabinet 5
You open the cabinet 5. The cabinet 5 is open. In it, you see a mug 3.
$&gt;$ take mug 3 from cabinet 5
You pick up the mug 3 from the cabinet 5.
think: End state: Now I take a mug (3) from cabinet (5). Task completed!
Your task is to: Heat an egg.
think: Input assumption: I am carrying an egg. Now I need to verify this.
OK.
$&gt;$ inventory
You are carrying: a egg 2.
$&gt;$ examine egg 2
There's nothing special about egg 2.
think: Based on previous context assumption is true, now I can proceed.
OK.
$&gt;$ go to microwave 1
The microwave 1 is closed.
$&gt;$ heat egg 2 with microwave 1
You heat the egg 2 using the microwave 1.
think: End state: Now I heat a egg (2). Task completed!
Your task is to: Cool a mug.
think: Input assumption: I am carrying a mug. Now I need to verify this.
OK.
$&gt;$ inventory
You are carrying: a mug 3.
$&gt;$ examine mug 3
There's nothing special about mug 3.
think: Based on previous context assumption is true, now I can proceed.
OK.
$&gt;$ go to fridge 1
The fridge 1 is closed.
$&gt;$ cool mug 3 with fridge 1
You cool the mug 3 using the fridge 1.
$&gt;$ think: End state: Now I cool a mug (3). Task completed!
Your task is to: Clean a lettuce.
think: Input assumption: I am carrying a lettuce. Now I need to verify this.
OK.
$&gt;$ inventory
You are carrying: a lettuce 1.
$&gt;$ examine lettuce 1
There's nothing special about lettuce 1.
think: Based on previous context assumption is true, now I can proceed.
OK.
$&gt;$ go to sinkbasin 1
On the sinkbasin 1, you see a apple 2, a ladle 2, a spoon 1, and a tomato 3.
$&gt;$ clean lettuce 1 with sinkbasin 1
You clean the lettuce 1 using the sinkbasin 1.
$&gt;$ think: End state: Now I clean a lettuce (1). Task completed!</p>
</blockquote>
<h1>ALFMorld Hybrid Executor Prompt (cont.)</h1>
<div class="codehilite"><pre><span></span><code><span class="nv">Your</span><span class="w"> </span><span class="nv">task</span><span class="w"> </span><span class="nv">is</span><span class="w"> </span><span class="nv">to</span>:<span class="w"> </span><span class="nv">Use</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">desklamp</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">look</span><span class="w"> </span><span class="nv">at</span><span class="w"> </span><span class="nv">tomato</span><span class="w"> </span><span class="mi">1</span>.
<span class="o">&gt;</span><span class="w"> </span><span class="nv">think</span>:<span class="w"> </span><span class="nv">Input</span><span class="w"> </span><span class="nv">assumption</span>:<span class="w"> </span><span class="nv">I</span><span class="w"> </span><span class="nv">am</span><span class="w"> </span><span class="nv">at</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">location</span><span class="w"> </span><span class="nv">that</span><span class="w"> </span><span class="nv">contains</span><span class="w"> </span><span class="nv">desklamp</span><span class="w"> </span><span class="ss">(</span><span class="mi">1</span><span class="ss">)</span>.<span class="w"> </span><span class="nv">Now</span><span class="w"> </span><span class="nv">I</span><span class="w"> </span><span class="nv">need</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">verify</span><span class="w"> </span><span class="nv">this</span>.
<span class="nv">OK</span>.
<span class="w"> </span><span class="nv">inventory</span>
<span class="nv">You</span><span class="w"> </span><span class="nv">are</span><span class="w"> </span><span class="nv">carrying</span>:<span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">tomato</span><span class="w"> </span><span class="mi">1</span>.
<span class="nv">look</span>
<span class="nv">You</span><span class="w"> </span><span class="nv">are</span><span class="w"> </span><span class="nv">facing</span><span class="w"> </span><span class="nv">desk</span><span class="w"> </span><span class="mi">1</span>.<span class="w"> </span><span class="k">Next</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">it</span>,<span class="w"> </span><span class="nv">you</span><span class="w"> </span><span class="nv">see</span><span class="w"> </span><span class="nv">nothing</span>.
<span class="nv">examine</span><span class="w"> </span><span class="nv">desk</span><span class="w"> </span><span class="mi">1</span>
<span class="nv">On</span><span class="w"> </span><span class="nv">desk</span><span class="w"> </span><span class="mi">1</span>,<span class="w"> </span><span class="nv">you</span><span class="w"> </span><span class="nv">see</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">spoon</span><span class="w"> </span><span class="mi">1</span>,<span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">desklamp</span><span class="w"> </span><span class="mi">1</span>,<span class="w"> </span><span class="nv">and</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">pencil</span><span class="w"> </span><span class="mi">2</span>.
<span class="nv">think</span>:<span class="w"> </span><span class="nv">Based</span><span class="w"> </span><span class="nv">on</span><span class="w"> </span><span class="nv">previous</span><span class="w"> </span><span class="nv">context</span><span class="w"> </span><span class="nv">assumption</span><span class="w"> </span><span class="nv">is</span><span class="w"> </span><span class="nv">true</span>,<span class="w"> </span><span class="nv">now</span><span class="w"> </span><span class="nv">I</span><span class="w"> </span><span class="nv">can</span><span class="w"> </span><span class="nv">proceed</span>.
<span class="nv">OK</span>.
<span class="nv">use</span><span class="w"> </span><span class="nv">desklamp</span><span class="w"> </span><span class="mi">1</span>
<span class="nv">You</span><span class="w"> </span><span class="nv">turn</span><span class="w"> </span><span class="nv">on</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">desklamp</span><span class="w"> </span><span class="mi">1</span>.
<span class="o">&gt;</span><span class="w"> </span><span class="nv">think</span>:<span class="w"> </span><span class="nv">Now</span><span class="w"> </span><span class="nv">I</span><span class="w"> </span><span class="nv">use</span><span class="w"> </span><span class="nv">desklamp</span><span class="w"> </span><span class="ss">(</span><span class="mi">1</span><span class="ss">)</span>.<span class="w"> </span><span class="nv">Task</span><span class="w"> </span><span class="nv">completed</span><span class="o">!</span>
</code></pre></div>

<p>Here are two example tasks.
You are in the middle of a room. Looking quickly around you, you see a cabinet 13, a cabinet 12, a cabinet 11, a cabinet 10, a cabinet 9 , a cabinet 8 , a cabinet 7 , a cabinet 6 , a cabinet 5 , a cabinet 4 , a cabinet 3 , a cabinet 2 , a cabinet 1 , a coffeemachine 1 , a countertop 1 , a diningtable 1 , a drawer 1 , a fridge 1 , a garbagecan 1 , a microwave 1 , a shelf 3 , a shelf 2 , a shelf 1 , a sinkbasin 1 , a stoveburner 4 , a stoveburner 3 , a stoveburner 2 , a stoveburner 1 , and a toaster 1 .
Your task is to: put a hot apple in fridge.</p>
<blockquote>
<p>think: To solve the task, I need to find and take an apple, then heat it with microwave, then put it in fridge. OK.
think: First I need to find an apple. An apple is more likely to appear in fridge (1), diningtable (1), coffeetable (1), drawer (1), cabinet (1-13), garbagecan (1). I can check one by one, starting with fridge 1.
OK.
go to fridge 1
The fridge 1 is closed.
open fridge 1
You open the fridge 1. The fridge 1 is open. In it, you see a cup 1, and a egg 1.
go to diningtable 1
On the diningtable 1, you see a apple 1, a bread 1, a fork 3, a fork 2, a fork 1, a mug 2, a peppershaker 3, a plate 2, a pot 1, a soapbottle 3, a spatula 1, a spoon 3, a spoon 2, a spoon 1, a winebottle 3, and a winebottle 2.
think: Now I find an apple (1). Next, I need to take it.
OK.
take apple 1 from diningtable 1
You pick up the apple 1 from the diningtable 1.
think: Now I take an apple (1). Next, I need to go to a microwave (1) and heat it.
OK.
go to microwave 1
The microwave 1 is closed.
heat apple 1 with microwave 1
You heat the apple 1 using the microwave 1.
think: Now I heat an apple (1). Next, I need to put it in/on fridge 1.
OK.
go to fridge 1
The fridge 1 is open. In it, you see a cup 1, and a egg 1.
put apple 1 in/on fridge 1
You put the apple 1 in/on the fridge 1 .
think: Task completed!
You are in the middle of a room. Looking quickly around you, you see a bed 1, a desk 1, a drawer 3, a drawer 2, a drawer 1, a garbagecan 1 , a safe 1 , a shelf 5 , a shelf 4 , a shelf 3 , a shelf 2 , a shelf 1 , a sidetable 2 , and a sidetable 1. Your task is to: look at bowl under the desklamp.
think: To solve the task, I need to find and take a bowl, then find and use a desklamp.
OK.
think: First I need to find a bowl. A bowl is more likely to appear in drawer shelf (1-5), drawer (1-3). I can check one by one, starting with drawer 1.
OK.
go to shelf 1
On the shelf 1, you see nothing.
go to shelf 2
On the shelf 2, you see a bowl 1.
think: Now I find a bowl (1). Next, I need to take it.
OK.
take bowl 1 from shelf 2
You pick up the bowl 1 from the shelf 2.
think: Now I take a bowl (1). Next, I need to find a desklamp. A desklamp is more likely to appear in desk (1), sidetable $(1-2)$, and drawer (1-3). I can check one by one, starting with desk 1.
OK.
go to desk 1
On the desk 1, you see a alarmclock 2, a book 1, a cellphone 1, a keychain 1, a laptop 2, a laptop 1, and a pen 3.
go to sidetable 1
On the sidetable 1, you see a cd 1, a pen 1, and a pencil 1.
go to sidetable 2
On the sidetable 2, you see a alarmclock 1, a desklamp 1, and a pen 2.
think: Now I find a desklamp (1). Next, I need to use it.
OK.
use desklamp 1
You turn on the desklamp 1.
think: Task completed!</p>
</blockquote>
<h1>ALFWorld Planner Prompt</h1>
<p>Here are some examples.
You are in the middle of a room. Looking quickly around you, you see a desk 1, microwave 1, a cabinet 3, a cabinet 9, a drawer 2, a coffeemachine 1, a stoveburner 4, a drawer 5, a cabinet 11, a drawer 3, a stoveburner 1, a drawer 1, a toaster 1, a fridge 1, a stoveburner 2, a cabinet 6, a cabinet 10, a countertop 1, a cabinet 13, a cabinet 7, a garbagecan 1, a cabinet 2, a cabinet 8, a cabinet 12, a drawer 4, a cabinet 1, a sinkbasin 1, a cabinet 5, a stoveburner 3, and a cabinet 4.</p>
<p>Goal: Put a mug in/on desk.
Come up with an abstract plan to perform this task in a couple of steps.
# Think: To perform this task, I need to find and take mug and then put it on desk. First, I will focus on finding mug.
Step 1: Find and take mug
# Think: Now that I am carrying mug, I will focus on putting it in/on desk.
Step 2: Put mug in/on desk
Execution Order: (Step 1 AND Step 2)
Goal: Clean mug and put it in/on desk.
Come up with an abstract plan to perform this task in a couple of steps.
# Think: To perform this task, I need to find and take mug, clean it, and then put it on desk. First, I will focus on finding mug.
Step 1: Find and take mug
# Think: Now that I am carrying mug, I will focus on cleaning it.
Step 2: Clean mug with sinkbasin
# Think: Now that I have cleaned mug, I will focus on putting it in/on desk.
Step 3: Put cleaned mug in/on desk
Execution Order: (Step 1 AND Step 2 AND Step 3)
Goal: Cool mug and put it in/on desk.
Come up with an abstract plan to perform this task in a couple of steps.
# Think: To perform this task, I need to find and take mug, cool it, and then put it on desk. First, I will focus on finding mug.
Step 1: Find and take mug
# Think: Now that I am carrying mug, I will focus on cooling it.
Step 2: Cool mug with fridge
# Think: Now that I have cooled mug, I will focus on putting it in/on desk.
Step 3: Put cooled mug in/on desk
Execution Order: (Step 1 AND Step 2 AND Step 3)
Goal: Heat mug and put it in/on desk.
Come up with an abstract plan to perform this task in a couple of steps.
# Think: To perform this task, I need to find and take mug, heat it, and then put it on desk. First, I will focus on finding mug.
Step 1: Find and take mug
# Think: Now that I am carrying mug, I will focus on heating it.
Step 2: Heat mug with microwave
# Think: Now that I have heated mug, I will focus on putting it in/on desk.
Step 3: Put heated mug in/on desk
Execution Order: (Step 1 AND Step 2 AND Step 3)
Goal: Look at mug under desklamp.
Come up with an abstract plan to perform this task in a couple of steps.
# Think: To perform this task, I need to find and take mug, and then go to the desklamp and use it. First, I will focus on finding mug.
Step 1: Find and take mug
# Think: Now that I have found and taken mug, I will focus on using the desklamp.
Step 2: Use the desklamp
Execution Order: (Step 1 AND Step 2)
Goal: Find and take mug
Come up with an abstract plan to perform this task in a couple of steps.
# Think: To perform this task I need to find mug in the room. mug is likely to be in desk, cabinet, countertop, or drawer. Now I will focus on finding mug in each of these locations one by one.
Step 1: Find and take mug from desk
# Think: If mug not found so far, I will next look in the cabinet.
Step 2: Find and take mug from cabinet
# Think: If mug not found so far, I will next look in the countertop.
Step 3: Find and take mug from countertop
# Think: If mug not found so far, I will next look in the drawer.
Step 4: Find and take mug from drawer
Execution Order: (Step 1 OR Step 2 OR Step 3 OR Step 4)
Here is the goal.
<room>
Goal: <task>.
Come up with an abstract plan to perform this task in a couple of steps. Constraints: The robot can hold/take/put only one object at a time to a location.
Ensure each step can be understood independently and mentions the name of object.
When stating the execution order, ensure that 'AND'/'OR' statements are properly nested using brackets '()'.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{7}$ https://huggingface.co/meta-llama/
Llama-2-70b-hf
${ }^{8}$ https://huggingface.co/OpenLemur/
lemur-70b-chat-v1&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{4}$ https://www.minecraft.net&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
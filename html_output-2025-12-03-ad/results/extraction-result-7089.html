<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7089 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7089</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7089</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-132.html">extraction-schema-132</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <p><strong>Paper ID:</strong> paper-1e80802a2fef00f2e10dfd4ab0ecce18ee2af82b</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/1e80802a2fef00f2e10dfd4ab0ecce18ee2af82b" target="_blank">Exploring the Potential of Large Language Models in Graph Generation</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This paper proposes LLM4GraphGen to explore the ability of LLMs for graph generation with systematical task designs and extensive experiments, and demonstrates that LLMs, particularly GPT-4, exhibit preliminary abilities in graph generation tasks, including rule-based and distribution-based generation.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have achieved great success in many fields, and recent works have studied exploring LLMs for graph discriminative tasks such as node classification. However, the abilities of LLMs for graph generation remain unexplored in the literature. Graph generation requires the LLM to generate graphs with given properties, which has valuable real-world applications such as drug discovery, while tends to be more challenging. In this paper, we propose LLM4GraphGen to explore the ability of LLMs for graph generation with systematical task designs and extensive experiments. Specifically, we propose several tasks tailored with comprehensive experiments to address key questions regarding LLMs' understanding of different graph structure rules, their ability to capture structural type distributions, and their utilization of domain knowledge for property-based graph generation. Our evaluations demonstrate that LLMs, particularly GPT-4, exhibit preliminary abilities in graph generation tasks, including rule-based and distribution-based generation. We also observe that popular prompting methods, such as few-shot and chain-of-thought prompting, do not consistently enhance performance. Besides, LLMs show potential in generating molecules with specific properties. These findings may serve as foundations for designing good LLMs based models for graph generation and provide valuable insights and further research.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7089.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7089.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (molecule generation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4 (used for property-based molecular graph generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper uses GPT-4 as a prompted large language model to generate molecule graphs (represented as SMILES) conditioned on a desired molecular property (inhibiting HIV replication), using few-shot and chain-of-thought prompting; generated molecules are evaluated in-silico by a GNN classifier trained on OGBG-MolHIV.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Exploring the Potential of Large Language Models in Graph Generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only large language model used via prompting (few-shot and chain-of-thought variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified in paper (referred to as GPT-4 / large)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified in this paper (GPT-4's training corpora are not detailed here).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompted generation of SMILES strings using few-shot prompting (examples of molecules with the property) and few-shot+chain-of-thought (CoT) prompting; sampling temperature t=0.5 in property-based experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES strings (explicitly stated: SMILES format used to represent molecule graphs).</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Drug discovery — generating molecules predicted to inhibit HIV replication (property-based molecular generation using OGBG-MolHIV labels).</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>No explicit chemical constraints (e.g., synthetic accessibility, toxicity filters, or ADMET filters) reported; generation constrained only via prompt specifying desired property and example molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Generated molecules were evaluated by an external GNN classifier trained on OGBG-MolHIV (used to compute C_M(G) predictions); the classifier confusion matrix was used to compute rectified probabilities C(G). No docking, quantum chemistry, retrosynthesis, or wet‑lab tools were reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>OGBG-MolHIV (used as source of example molecules with the target property and for training/evaluating the GNN classifier).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Classifier predicted probability C_M(G) (0–1), rectified probability C(G) (adjusted for classifier confusion/prior), Novel rate (fraction not identical to input examples), Unique rate (fraction not duplicated among generated set).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Table 6 (property-based generation): Few-shot prompt: C_M(G)=26.4 ± 7.5, C(G)=34.8 ± 16.5, Novel=79.1% ± 10.9, Unique=91.8% ± 6.1. Few-shot+CoT prompt: C_M(G)=32.7 ± 4.7, C(G)=48.8 ± 10.2, Novel=65.5% ± 10.9, Unique=92.7% ± 6.0. The paper reports that both C_M and rectified C(G) increase when using CoT prompting relative to few-shot alone.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Reported limitations include: only preliminary ability to generate molecules with target properties (not strong guarantees); reliance on an imperfect in-silico GNN classifier for property evaluation (classifier confusion matrix used to 'rectify' predictions); no wet‑lab synthesis or experimental property testing reported; some generated molecules are duplicates of known molecules though many are novel; prompting methods have inconsistent effects and CoT/few‑shot do not always improve performance; the paper notes general LLM failure modes on more complex graph-generation tasks (e.g., failing to learn complex motifs), indicating limitations in capturing complex domain constraints for property-directed molecular design.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring the Potential of Large Language Models in Graph Generation', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>LLM drug discovery challenge: A contest as a feasibility study on the utilization of large language models in medicinal chemistry <em>(Rating: 2)</em></li>
                <li>Artificial intelligence enabled chatgpt and large language models in drug target discovery, drug discovery, and development <em>(Rating: 2)</em></li>
                <li>Graph convolutional policy network for goal-directed molecular graph generation <em>(Rating: 1)</em></li>
                <li>A survey on deep generative models for graph generation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7089",
    "paper_id": "paper-1e80802a2fef00f2e10dfd4ab0ecce18ee2af82b",
    "extraction_schema_id": "extraction-schema-132",
    "extracted_data": [
        {
            "name_short": "GPT-4 (molecule generation)",
            "name_full": "Generative Pre-trained Transformer 4 (used for property-based molecular graph generation)",
            "brief_description": "The paper uses GPT-4 as a prompted large language model to generate molecule graphs (represented as SMILES) conditioned on a desired molecular property (inhibiting HIV replication), using few-shot and chain-of-thought prompting; generated molecules are evaluated in-silico by a GNN classifier trained on OGBG-MolHIV.",
            "citation_title": "Exploring the Potential of Large Language Models in Graph Generation",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_type": "decoder-only large language model used via prompting (few-shot and chain-of-thought variants)",
            "model_size": "not specified in paper (referred to as GPT-4 / large)",
            "training_data_description": "Not specified in this paper (GPT-4's training corpora are not detailed here).",
            "generation_method": "Prompted generation of SMILES strings using few-shot prompting (examples of molecules with the property) and few-shot+chain-of-thought (CoT) prompting; sampling temperature t=0.5 in property-based experiments.",
            "chemical_representation": "SMILES strings (explicitly stated: SMILES format used to represent molecule graphs).",
            "target_application": "Drug discovery — generating molecules predicted to inhibit HIV replication (property-based molecular generation using OGBG-MolHIV labels).",
            "constraints_used": "No explicit chemical constraints (e.g., synthetic accessibility, toxicity filters, or ADMET filters) reported; generation constrained only via prompt specifying desired property and example molecules.",
            "integration_with_external_tools": "Generated molecules were evaluated by an external GNN classifier trained on OGBG-MolHIV (used to compute C_M(G) predictions); the classifier confusion matrix was used to compute rectified probabilities C(G). No docking, quantum chemistry, retrosynthesis, or wet‑lab tools were reported.",
            "dataset_used": "OGBG-MolHIV (used as source of example molecules with the target property and for training/evaluating the GNN classifier).",
            "evaluation_metrics": "Classifier predicted probability C_M(G) (0–1), rectified probability C(G) (adjusted for classifier confusion/prior), Novel rate (fraction not identical to input examples), Unique rate (fraction not duplicated among generated set).",
            "reported_results": "Table 6 (property-based generation): Few-shot prompt: C_M(G)=26.4 ± 7.5, C(G)=34.8 ± 16.5, Novel=79.1% ± 10.9, Unique=91.8% ± 6.1. Few-shot+CoT prompt: C_M(G)=32.7 ± 4.7, C(G)=48.8 ± 10.2, Novel=65.5% ± 10.9, Unique=92.7% ± 6.0. The paper reports that both C_M and rectified C(G) increase when using CoT prompting relative to few-shot alone.",
            "experimental_validation": false,
            "challenges_or_limitations": "Reported limitations include: only preliminary ability to generate molecules with target properties (not strong guarantees); reliance on an imperfect in-silico GNN classifier for property evaluation (classifier confusion matrix used to 'rectify' predictions); no wet‑lab synthesis or experimental property testing reported; some generated molecules are duplicates of known molecules though many are novel; prompting methods have inconsistent effects and CoT/few‑shot do not always improve performance; the paper notes general LLM failure modes on more complex graph-generation tasks (e.g., failing to learn complex motifs), indicating limitations in capturing complex domain constraints for property-directed molecular design.",
            "uuid": "e7089.0",
            "source_info": {
                "paper_title": "Exploring the Potential of Large Language Models in Graph Generation",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "LLM drug discovery challenge: A contest as a feasibility study on the utilization of large language models in medicinal chemistry",
            "rating": 2
        },
        {
            "paper_title": "Artificial intelligence enabled chatgpt and large language models in drug target discovery, drug discovery, and development",
            "rating": 2
        },
        {
            "paper_title": "Graph convolutional policy network for goal-directed molecular graph generation",
            "rating": 1
        },
        {
            "paper_title": "A survey on deep generative models for graph generation",
            "rating": 1
        }
    ],
    "cost": 0.011717499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Exploring the Potential of Large Language Models in Graph Generation</h1>
<p>A Preprint<br>Yang Yao ${ }^{1}$, Xin Wang ${ }^{1, <em>}$, Zeyang Zhang ${ }^{1}$, Yijian Qin ${ }^{1}$, Ziwei Zhang ${ }^{1}$, Xu Chu ${ }^{1}$, Yuekui Yang ${ }^{1}$, Wenwu Zhu ${ }^{1, </em>}$, and Hong Mei ${ }^{2}$<br>${ }^{1}$ Tsinghua University<br>${ }^{2}$ Peking University<br>*Corresponding authors.</p>
<h4>Abstract</h4>
<p>Large language models (LLMs) have achieved great success in many fields, and recent works have studied exploring LLMs for graph discriminative tasks such as node classification. However, the abilities of LLMs for graph generation remain unexplored in the literature. Graph generation requires the LLM to generate graphs with given properties, which has valuable real-world applications such as drug discovery, while tends to be more challenging. In this paper, we propose LLM4GraphGen to explore the ability of LLMs for graph generation with systematical task designs and extensive experiments. Specifically, we propose several tasks tailored with comprehensive experiments to address key questions regarding LLMs' understanding of different graph structure rules, their ability to capture structural type distributions, and their utilization of domain knowledge for property-based graph generation. Our evaluations demonstrate that LLMs, particularly GPT-4, exhibit preliminary abilities in graph generation tasks, including rule-based and distribution-based generation. We also observe that popular prompting methods, such as few-shot and chain-of-thought prompting, do not consistently enhance performance. Besides, LLMs show potential in generating molecules with specific properties. These findings may serve as foundations for designing good LLMs based models for graph generation and provide valuable insights and further research.</p>
<h2>1 Introduction</h2>
<p>Large language models (LLMs) have experienced remarkable success across various domains Zhao et al. [2023]. In comparison to their predecessors, LLMs possess a substantial number of parameters, enabling them to exhibit exceptional capabilities, notably being foundation models Bommasani et al. [2022], in-context learning Brown et al. [2020], Bubeck et al. [2023], chain-of-thought Wei et al. [2022], etc. The application of LLMs has yielded impressive outcomes in numerous tasks, such as code generation Ni et al. [2023], Vaithilingam et al. [2022], drug discovery Murakumo et al. [2023], Chakraborty et al. [2023], knowledge probing Meyer et al. [2023], Sun et al. [2023], etc.</p>
<p>Despite not being explicitly designed for graph-structured data, exploring the potential of LLMs in comprehending and leveraging graph structures has attracted considerable attention recently Zhang et al. [2023a]. Notably, recent research endeavors have showcased promising results, demonstrating that LLMs can effectively address various discriminative tasks associated with graph data Wang et al. [2023a], Ye et al. [2023], Huang et al. [2023], Tang et al. [2023], such as node classification, link prediction, and graph classification. These studies highlight the inherent capacity of large language models to grasp the underlying structure of explicitly provided graph data, validating their potential in graph-related problem-solving.
On the other hand, graph generation has been a prominent and extensively studied research field in recent years Zhu et al. [2022]. It involves the generation of graphs with specific properties, and its significance extends to various domains, including drug discovery You et al. [2018]. Deep generative models have shown promising results in graph generation tasks, particularly in molecular generation. However, current investigations of LLMs for graph-related tasks</p>
<p>have primarily focused on discriminative tasks. Consequently, whether LLMs possess the abilities of graph generation remains unexplored in the literature.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An overview of LLM4GraphGen. Our proposed method designs a prompt tailored to each graph generation task, which is subsequently used as the input to the LLM to generate the desired graphs. Each prompt encompasses both the task description and the required output format. In the case of rule-based generation, the prompt contains the description of the rule. For distribution-based generation, a collection of graphs is provided to facilitate the LLM's learning of the underlying distribution. For property-based generation, a collection of molecules is included to enable the LLM to understand molecular properties.</p>
<p>In this work, we propose LLM4GraphGen to explore the potential of LLMs for graph generation. Specifically, we construct a pipeline where graph generation tasks are formulated as textual prompts and LLMs are required to output graphs in a specific format. Within this pipeline, we explore three key questions of applying LLMs to graph generation tasks. 1) Whether the LLMs can understand the rules of different types of graph structures? Understanding basic graph structure types through rules, e.g., trees, cycles, regular graphs, and so on, is the foundation of graph generation. To answer this question, we benchmark LLMs for eight rule-based graph generation. Moreover, we explore the impact of important parameters and prompt construction methods for rule-based graph generation capabilities. 2) Whether LLMs can understand the distribution of different types of graph structures? We assess the ability of LLMs to understand the structural type distribution of given graphs and generate new graphs with the same distribution. 3) Whether the LLMs can understand domain knowledge of graph generation? We explore LLMs in the task of property-based graph generation, particularly focusing on generating molecule structures with specific properties. This task necessitates a comprehensive understanding of both graph structures and the incorporation of domain knowledge.
Moreover, we have conducted extensive experiments and provided detailed results in this paper. By analyzing the results of the experiment, we derive several valuable observations. Our experiments demonstrate that GPT-4 has reasonably good abilities in graph generation, including rule-based and distribution-based generation. We also observe that some popular prompting methods, such as in-context learning and chain-of-thought, do not improve graph generation performance consistently. Moreover, LLMs show preliminary abilities in generating molecules with certain properties. The conclusions drawn from our experiments provide reliable and informative insights that can guide future research and practical applications for graph generation.
The contributions of our work are summarized as:</p>
<ul>
<li>We have explored the potential of using LLMs for graph generation. To the best of our knowledge, we are the first to study this valuable problem.</li>
<li>We have designed comprehensive experiments to evaluate the graph generation ability of LLMs by proposing tasks with varying difficulty, including rule-based graph generation, distribution-based graph generation, and property-based graph generation.</li>
</ul>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: An illustration of graphs with regard to different rules.</p>
<ul>
<li>We extensively evaluate the graph generation capability of state-of-the-art LLMs, such as GPT-4, with diverse prompting methods under three metrics. Our experimental results reveal interesting and insightful observations that can inspire further research.</li>
</ul>
<h1>2 LLM4GraphGen</h1>
<p>In this section, we design three types of tasks to explore the graph generation ability of LLMs from multiple aspects, including rule-based tasks, distribution-based tasks, and property-based tasks.</p>
<h3>2.1 Rule-based graph generation</h3>
<p>In this task, we employ rules in natural language to describe the structure of various graphs (e.g., being a tree, or a cycle), and the model is evaluated by the adherence of its generated graphs to these rules. In order to assess the capacity of LLMs to comprehend and follow the instructions in graph generation, we formulate the following eight rules. Figure 2 shows the example graphs with regard to different rules.</p>
<ul>
<li>Trees: The generated graph should be a tree with the specified number of nodes, i.e., an undirected graph where there is exactly one path between any two nodes.</li>
<li>Cycles: The generated graph should contain a cycle with the specified number of nodes, and should not contain any other nodes or edges.</li>
<li>Planar graphs: The generated graph should be a planar graph with the specified number of nodes and edges, i.e., there exists a way to draw the graph on a plane without edge crossings.</li>
<li>Components: The generated graph should have a specified number of connected components, i.e., there must be a specified number of connected subgraphs with no edges among them.</li>
<li>$k$-regular graphs: Every node of the generated graph should have the same number of neighbors, i.e., the degree of every node is $k$.</li>
<li>Wheel graphs: The generated graph should be formed by connecting a single node to all nodes of a cycle.</li>
<li>Bipartite graphs: The generated graph should have nodes that are divided into two disjoint and independent sets $U$ and $V$ with specified sizes.</li>
<li>$k$-color graphs: The generated graph should be $k$-colorable, i.e., each node is assigned a color, and two adjacent nodes do not have the same color.</li>
</ul>
<p>Prompt Design We adopt the following four types of prompts for rule-based graph generation:</p>
<ul>
<li>Zero-shot: The prompt contains the relevant information about the rules, as well as a specification of the output format. The model is then asked to generate graphs using the given rules.</li>
<li>Few-shot: In addition to the zero-shot prompt, the model is given several graph examples that follow the given rules. The edges of the graphs are sorted by the node ID to facilitate the model understanding.</li>
<li>Zero-shot+CoT/Few-shot+CoT: In addition to the zero-shot prompt and the few-shot prompt, the model is asked to give the answer step by step.</li>
</ul>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: An illustration of distribution-based graph generation.</p>
<h1>2.2 Distribution-based graph generation</h1>
<p>Learning distributions from the given graphs and then generating new graphs based on the distributions is a necessary ability for graph generation Zhu et al. [2022]. To this end, we propose distribution-based graph generation tasks, which define a target graph distribution and the generated set of graphs $\left{G_{i}\right}$ from the target distribution as the input to the model. The model should infer the parameters of the target distribution from the given graphs, and generate new graphs from the same distribution. We design several subtasks with increasing difficulties, including the generation of tree-or-cycles, union of components, and motifs.</p>
<p>Trees or cycles In this subtask, the target distribution is defined as a mixture of trees and cycles, where each graph has a probability of $p$ to be a tree and $1-p$ to be a cycle,</p>
<p>$$
P\left(G_{i} \text { is a tree }\right)=p, \quad P\left(G_{i} \text { is a cycle }\right)=1-p
$$</p>
<p>Specifically, the model is given 10 graphs sampled from the target distribution and is asked to infer the value of $p$ from the graphs and then generate 10 new graphs that follow the same distribution. We evaluate the task with different settings of $p$, and all input graphs have a random number of nodes ranging from 5 to 7 .</p>
<p>Union of components In this subtask, each graph from the target distribution is the union of two connected components, with each component being either a tree or a cycle. There is a probability of $p$ that the two components belong to the same kind and $1-p$ that they belong to different kinds, i.e., $G_{i}=G_{i}^{1} \cup G_{i}^{2}$. The model is asked to generate graphs with two component types specified by rules, and similarly, the input to the model is 10 random graphs drawn from the target distribution. The model is expected to infer the value of $p$ and generate 10 new graphs from the target distribution. Each component has a random number of 5 to 7 nodes. This task is more complex and evaluates the ability of graph generation in scenarios with multiple correlated factors such as the component types.</p>
<p>Motif In this subtask, each graph from the target distribution consists of a base graph and a motif graph with an inter-connected edge, where there are three kinds of base graphs (trees, ladders, wheels), and three motif graphs (cycles,</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: An illustration of property-based graph generation.
houses, cranes). Inspired by the setting of Spurious-Motif Ying et al. [2019], Wu et al. [2022], Qin et al. [2022], we construct the distribution as follows. For each graph $G_{i}$, the base graph type $S_{i}$ is chosen from a uniform distribution over three kinds of base graphs, and the motif graph $C_{i}$ with the same label is chosen with probability $p$, while the other two motif graphs are chosen with probability $(1-p) / 2$ respectively, i.e.,</p>
<p>$$
P\left(S_{i}, C_{i}\right)= \begin{cases}\frac{1}{3} p &amp; S_{i}=C_{i}, i \in{0,1,2} \ \frac{1}{6}(1-p) &amp; S_{i} \neq C_{i}, i \in{0,1,2}\end{cases}
$$</p>
<p>We generate 10 random graphs from the target distribution and ask the model to infer the value of $p$, and generate 10 new graphs from the same distribution. The main purpose of this task is to explore the abilities of LLMs to understand graph distributions under more complex factors and generate graphs based on inferred distributions.</p>
<p>Prompt Design In zero-shot prompt, we introduce the graph generation task and the target distribution, and provide a set of graphs sampled from the target distribution as the input. Then, we ask the model to infer the value of $p$ and generate new graphs. Similarly, we add exemplars in few-shot prompt, and step-by-step thinking in CoT prompt.</p>
<h1>2.3 Property-based graph generation</h1>
<p>Generating graphs with certain properties is important in real-world applications, e.g., the properties, in the field of drug discovery, such as inhibiting HIV replication, blood-brain barrier penetration, and toxicity to the human body, are important for the development of new drugs Guo and Zhao [2023], Zhu et al. [2022]. Though LLMs have learned expert knowledge like chemistry and medicine through large-scale textual data, it remains unknown whether they can leverage this information to directly generate graphs with desired properties.
In this task, we evaluate LLMs' abilities to generate graphs with given properties by adopting the molecular property prediction dataset OGBG-MolHIV Hu et al. [2020]. We choose the molecules labeled to inhibit HIV replication as the input graphs and ask the model to generate new molecules with properties similar to these graphs. Following the common practice in the field of chemistry, we adopt the SMILES Weininger [1988] format to represent the molecule graphs.</p>
<p>Table 1: The valid rate for rule-based graph generation with GPT-4. The metric measures the fraction of generated graphs that are valid under the specified rules. Values after $\pm$ denote standard errors.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Prompt</th>
<th style="text-align: center;">Trees</th>
<th style="text-align: center;">Cycles</th>
<th style="text-align: center;">Components</th>
<th style="text-align: center;">Planar</th>
<th style="text-align: center;">$k$-regular</th>
<th style="text-align: center;">Wheel</th>
<th style="text-align: center;">Bipartite</th>
<th style="text-align: center;">$k$-color</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Zero-shot</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$91.3 \pm 3.3$</td>
<td style="text-align: center;">$30.4 \pm 5.1$</td>
<td style="text-align: center;">$47.3 \pm 4.2$</td>
<td style="text-align: center;">$64.0 \pm 6.8$</td>
<td style="text-align: center;">$13.0 \pm 5.3$</td>
<td style="text-align: center;">$60.3 \pm 7.4$</td>
<td style="text-align: center;">$50.3 \pm 5.5$</td>
</tr>
<tr>
<td style="text-align: center;">Few-shot</td>
<td style="text-align: center;">$98.0 \pm 0.9$</td>
<td style="text-align: center;">$85.0 \pm 3.3$</td>
<td style="text-align: center;">$63.2 \pm 5.3$</td>
<td style="text-align: center;">$4.3 \pm 1.3$</td>
<td style="text-align: center;">$86.1 \pm 3.1$</td>
<td style="text-align: center;">$88.8 \pm 7.4$</td>
<td style="text-align: center;">$57.1 \pm 8.6$</td>
<td style="text-align: center;">$62.3 \pm 5.1$</td>
</tr>
<tr>
<td style="text-align: center;">Zero-shot+CoT</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$86.9 \pm 3.6$</td>
<td style="text-align: center;">$38.0 \pm 5.1$</td>
<td style="text-align: center;">$53.3 \pm 6.0$</td>
<td style="text-align: center;">$82.7 \pm 8.6$</td>
<td style="text-align: center;">$92.3 \pm 4.7$</td>
<td style="text-align: center;">$92.7 \pm 4.4$</td>
<td style="text-align: center;">$43.2 \pm 4.9$</td>
</tr>
<tr>
<td style="text-align: center;">Few-shot+CoT</td>
<td style="text-align: center;">$97.6 \pm 1.7$</td>
<td style="text-align: center;">$97.0 \pm 1.9$</td>
<td style="text-align: center;">$40.0 \pm 6.7$</td>
<td style="text-align: center;">$20.0 \pm 4.3$</td>
<td style="text-align: center;">$91.5 \pm 1.6$</td>
<td style="text-align: center;">$90.7 \pm 5.1$</td>
<td style="text-align: center;">$98.2 \pm 1.8$</td>
<td style="text-align: center;">$58.5 \pm 5.9$</td>
</tr>
</tbody>
</table>
<p>Table 2: The unique rate for rule-based graph generation with GPT-4. The metric measures the fraction of valid graphs that are unique under the specified rules.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Prompt</th>
<th style="text-align: center;">Trees</th>
<th style="text-align: center;">Cycles</th>
<th style="text-align: center;">Components</th>
<th style="text-align: center;">Planar</th>
<th style="text-align: center;">$k$-regular</th>
<th style="text-align: center;">Wheel</th>
<th style="text-align: center;">Bipartite</th>
<th style="text-align: center;">$k$-color</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Zero-shot</td>
<td style="text-align: center;">$98.6 \pm 0.9$</td>
<td style="text-align: center;">$88.3 \pm 1.6$</td>
<td style="text-align: center;">$91.9 \pm 3.0$</td>
<td style="text-align: center;">$99.7 \pm 0.3$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$98.7 \pm 0.8$</td>
<td style="text-align: center;">$44.3 \pm 7.0$</td>
<td style="text-align: center;">$98.3 \pm 1.6$</td>
</tr>
<tr>
<td style="text-align: center;">Few-shot</td>
<td style="text-align: center;">$99.3 \pm 0.5$</td>
<td style="text-align: center;">$92.3 \pm 1.3$</td>
<td style="text-align: center;">$97.7 \pm 1.1$</td>
<td style="text-align: center;">$96.8 \pm 2.5$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$98.8 \pm 1.1$</td>
<td style="text-align: center;">$50.0 \pm 8.2$</td>
<td style="text-align: center;">$98.5 \pm 1.5$</td>
</tr>
<tr>
<td style="text-align: center;">Zero-shot+CoT</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$89.0 \pm 4.2$</td>
<td style="text-align: center;">$98.5 \pm 0.8$</td>
<td style="text-align: center;">$98.6 \pm 0.8$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$85.0 \pm 5.7$</td>
<td style="text-align: center;">$16.9 \pm 4.2$</td>
<td style="text-align: center;">$98.6 \pm 1.0$</td>
</tr>
<tr>
<td style="text-align: center;">Few-shot+CoT</td>
<td style="text-align: center;">$99.7 \pm 0.3$</td>
<td style="text-align: center;">$83.0 \pm 5.1$</td>
<td style="text-align: center;">$96.3 \pm 1.5$</td>
<td style="text-align: center;">$98.0 \pm 1.4$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$88.3 \pm 5.1$</td>
<td style="text-align: center;">$10.7 \pm 0.7$</td>
<td style="text-align: center;">$96.3 \pm 2.6$</td>
</tr>
</tbody>
</table>
<p>To evaluate the performance of molecule generation, we measure the fraction of generated molecules that have the same properties as the input molecules. Since it would be infeasible to check the properties manually using experiments or expert knowledge, we use a GNN classifier trained on the OGBG-MolHIV dataset to assist in the evaluation of molecule generation. Specifically, let $G_{1}, G_{2}, \ldots, G_{n}$ be the generated graphs that represent molecules, $C_{M}(G) \in[0,1]$ be the prediction given by the classifier, e.g., $C_{M}(G)=0.5$ means the molecule $G$ is predicted to have the desired property with $50 \%$ probability. Since the classifier may not be exactly accurate, we calculate the rectified predictions $C(G)$ by taking into consideration the dataset priors as well as the classifier accuracy in the original dataset.</p>
<p>Prompt Design We design a few-shot prompt that the LLM is given a description of the desired property and a collection of molecules that have the property. Then, the model is asked to generate new molecules with the same property. For the CoT prompt, the model is also asked to provide a step-by-step explanation of the answer.</p>
<h1>3 Experimental Results and Analyses</h1>
<p>In this section, we conducted extensive experiments on LLMs for our proposed graph generation tasks. Through analyzing the results, we derive valuable observations to benefit practical applications.</p>
<h3>3.1 Rule-based graph generation</h3>
<p>We explore the graph generation abilities of several representative LLMs in this part, including GPT-4, GPT-3.5, and LLama2-13B. We use the following three metrics to evaluate the graph generation quality:</p>
<ul>
<li>Valid rate: The proportion of generated graphs that match the rules.</li>
<li>Novel rate: The proportion of generated graphs that is different from the given example graph.</li>
<li>Unique rate: The proportion of generated graphs that are not identical.</li>
</ul>
<p>Since GPT-4 is the strongest LLM currently available, we perform our main experiments with GPT-4. The valid rate and unique rate for various rules and prompts are listed in Table 1 and Table 2 respectively. It is worth noting that by designing appropriate prompts, the novel rate of each experiment is $100 \%$ or close to $100 \%$ for GPT-4. More details about the setups and results can be found in the appendix.
Observation 1: GPT-4 has reasonably good abilities for rule-based graph generation.
Table 1 demonstrates that GPT-4 has the ability to generate graphs according to rules in general. Specifically, for simple rules such as Trees and Cycles, GPT-4 achieves good generation quality. But the generation quality is not good enough for other rules. A possible reason is that the topological structures of trees and cycles are relatively simple. LLMs can quickly come up with their generation algorithms according to the massive training data. For more complex rules like $k$-regular, wheel, and bipartite, LLMs cannot achieve satisfactory results with zero-shot prompt. Nevertheless, adjusting prompts such as few-shot and chain-of-thought can improve the generation quality. For components, planar graphs, and $k$-color graphs, it is difficult for LLMs to find the accurate generation method from rule descriptions, even in conditions giving few-shot examples and CoT prompt, resulting in poor generation quality.</p>
<p>Table 3: A comparison of rule-based graph generation using different LLMs. Reported values are valid rates.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Prompt</th>
<th>Cycle</th>
<th>$k$-regular</th>
<th>$k$-color</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT-4</td>
<td>Zero-shot</td>
<td>$84.7 \pm 5.9$</td>
<td>$74.7 \pm 9.9$</td>
<td>$56.0 \pm 4.5$</td>
</tr>
<tr>
<td></td>
<td>Few-shot</td>
<td>$73.3 \pm 7.1$</td>
<td>$90.0 \pm 2.9$</td>
<td>$76.4 \pm 5.3$</td>
</tr>
<tr>
<td></td>
<td>Zero-shot+CoT</td>
<td>$95.3 \pm 3.2$</td>
<td>$97.1 \pm 1.2$</td>
<td>$62.0 \pm 6.1$</td>
</tr>
<tr>
<td></td>
<td>Few-shot+CoT</td>
<td>$96.0 \pm 2.8$</td>
<td>$87.9 \pm 3.4$</td>
<td>$80.7 \pm 3.2$</td>
</tr>
<tr>
<td>GPT-3.5</td>
<td>Zero-shot</td>
<td>$84.7 \pm 5.6$</td>
<td>$6.7 \pm 2.4$</td>
<td>$0.0 \pm 0.0$</td>
</tr>
<tr>
<td></td>
<td>Few-shot</td>
<td>$22.7 \pm 7.4$</td>
<td>$8.7 \pm 3.6$</td>
<td>$29.3 \pm 7.0$</td>
</tr>
<tr>
<td></td>
<td>Zero-shot+CoT</td>
<td>$56.7 \pm 8.0$</td>
<td>$2.7 \pm 2.6$</td>
<td>$14.3 \pm 8.4$</td>
</tr>
<tr>
<td></td>
<td>Few-shot+CoT</td>
<td>$57.5 \pm 12.1$</td>
<td>$9.0 \pm 4.6$</td>
<td>$34.3 \pm 8.0$</td>
</tr>
<tr>
<td>LLama2</td>
<td>Zero-shot</td>
<td>$0.7 \pm 0.7$</td>
<td>$0.0 \pm 0.0$</td>
<td>$0.0 \pm 0.0$</td>
</tr>
<tr>
<td></td>
<td>Few-shot</td>
<td>$31.1 \pm 7.8$</td>
<td>$17.2 \pm 5.0$</td>
<td>$13.0 \pm 3.7$</td>
</tr>
<tr>
<td></td>
<td>Zero-shot+CoT</td>
<td>$1.1 \pm 1.1$</td>
<td>$0.0 \pm 0.0$</td>
<td>$0.0 \pm 0.0$</td>
</tr>
<tr>
<td></td>
<td>Few-shot+CoT</td>
<td>$16.2 \pm 5.2$</td>
<td>$8.0 \pm 3.1$</td>
<td>$13.3 \pm 3.7$</td>
</tr>
</tbody>
</table>
<p>In order to explore the graph generation effects of different LLMs, we conducted experiments on multiple LLMs and generated small-size graphs, as shown in Table 3. It can be found that GPT-4 has good generation quality for all three rules. GPT-3.5 can generate some legitimate cycles, but its performance is poor for more complex rules. LLama2 can only generate very few graphs that comply with rules given examples.</p>
<h1>3.1.1 The impact of prompt on graph generation</h1>
<p>It is known that prompts have a significant impact on the performance of LLMs. In this section, we compare the performance of different prompts for rule-based graph generation with GPT-4. The valid rate and unique rate for different prompts are listed in Table 1 and Table 2 respectively. We have the following observations.</p>
<p>Observation 2: Providing examples has an inconsistent impact on LLMs in generating different types of graphs.
Giving examples can have an impact on generation quality, but not all have a positive impact. As shown in Table 1, for trees, the valid rate of few-shot prompt is worse than zero-shot prompt, which is because adding examples may disrupt the understanding of LLM for rule descriptions. For planar graphs, giving examples greatly reduces the valid rate of the generated graph, as it is difficult to generalize the properties of the planar graph from the examples, and instead may lead to some opposite rules. For wheel graphs, the valid rate of few-shot is much better than zero-shot because the generation of a wheel can actually be divided into two steps: first, generate a cycle, and then add a new node to connect it to each previous node. LLM can learn about the existence of the cycle from the examples, which strengthens its understanding of the rules.</p>
<p>Observation 3: CoT prompt has diverse impacts on different evaluation metrics for graph generation.
CoT prompt can have an impact on generation quality, but not all have a positive impact. Making LLM think step by step can help it better decompose tasks. For example, as mentioned earlier, generating a wheel graph can be divided into two steps. Generating a bipartite graph can also be divided into two steps: firstly, the nodes are divided into two subsets, and then the nodes in the two subsets are connected. However, although the valid rate has increased, its unique rate has greatly decreased because, after step-by-step thinking, it always tends to divide nodes into the same subset.</p>
<h3>3.1.2 The impact of parameters on graph generation</h3>
<p>Graph generation using LLMs can be sensitive to the choice of parameters. In order to explore the impact of various parameters on graph generation, we construct the ablation study. We compare the performance of graph generation when using different graph sizes in Table 4. The "Medium" size in the experiment is the same as the size in the main experiment. More details of the experiment can be found in the appendix. Through the analysis of the generated results, we have reached the following conclusion:</p>
<p>Observation 4: As the graph size increases, the performance of LLM in graph generation decreases for most rules, except for simple cases such as cycles.</p>
<p>Table 4 shows the valid rate and unique rate when using LLM to generate graphs of different sizes for three rules. In experience, the larger the graph generated, the poorer the generation quality, but this is not the case for cycles. This may be because the cycle is too simple.</p>
<p>Table 4: The comparison of different graph sizes for rule-based graph generation with GPT-4. "Valid" denotes fractions of generated graphs that are valid under specific rules, and "Unique" denotes fractions of generated graphs that are not duplicates.</p>
<table>
<thead>
<tr>
<th>Size</th>
<th>Prompt</th>
<th>Cycles</th>
<th></th>
<th>$k$-regular</th>
<th></th>
<th>$k$-color</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>Valid</td>
<td>Unique</td>
<td>Valid</td>
<td>Unique</td>
<td>Valid</td>
<td>Unique</td>
</tr>
<tr>
<td>Small</td>
<td>Zero-shot</td>
<td>$84.7 \pm 5.9$</td>
<td>$90.0 \pm 3.1$</td>
<td>$74.7 \pm 9.9$</td>
<td>$100.0 \pm 0.0$</td>
<td>$56.0 \pm 4.5$</td>
<td>$100.0 \pm 0.0$</td>
</tr>
<tr>
<td></td>
<td>Few-shot</td>
<td>$73.3 \pm 7.1$</td>
<td>$96.0 \pm 1.8$</td>
<td>$90.0 \pm 2.9$</td>
<td>$100.0 \pm 0.0$</td>
<td>$76.4 \pm 5.3$</td>
<td>$100.0 \pm 0.0$</td>
</tr>
<tr>
<td></td>
<td>Zero-shot+CoT</td>
<td>$95.3 \pm 3.2$</td>
<td>$84.7 \pm 4.8$</td>
<td>$97.1 \pm 1.2$</td>
<td>$100.0 \pm 0.0$</td>
<td>$62.0 \pm 6.1$</td>
<td>$96.7 \pm 3.2$</td>
</tr>
<tr>
<td></td>
<td>Few-shot+CoT</td>
<td>$96.0 \pm 2.8$</td>
<td>$94.7 \pm 2.1$</td>
<td>$87.9 \pm 3.4$</td>
<td>$100.0 \pm 0.0$</td>
<td>$80.7 \pm 3.2$</td>
<td>$100.0 \pm 0.0$</td>
</tr>
<tr>
<td>Medium</td>
<td>Zero-shot</td>
<td>$91.3 \pm 3.3$</td>
<td>$88.3 \pm 1.6$</td>
<td>$64.0 \pm 6.8$</td>
<td>$100.0 \pm 0.0$</td>
<td>$50.3 \pm 5.5$</td>
<td>$98.3 \pm 1.6$</td>
</tr>
<tr>
<td></td>
<td>Few-shot</td>
<td>$85.0 \pm 3.3$</td>
<td>$92.3 \pm 1.3$</td>
<td>$86.1 \pm 3.1$</td>
<td>$100.0 \pm 0.0$</td>
<td>$62.3 \pm 5.1$</td>
<td>$98.5 \pm 1.5$</td>
</tr>
<tr>
<td></td>
<td>Zero-shot+CoT</td>
<td>$86.9 \pm 3.6$</td>
<td>$89.0 \pm 4.2$</td>
<td>$82.7 \pm 8.6$</td>
<td>$100.0 \pm 0.0$</td>
<td>$43.2 \pm 4.9$</td>
<td>$98.6 \pm 1.0$</td>
</tr>
<tr>
<td></td>
<td>Few-shot+CoT</td>
<td>$97.0 \pm 1.9$</td>
<td>$83.0 \pm 5.1$</td>
<td>$91.5 \pm 1.6$</td>
<td>$100.0 \pm 0.0$</td>
<td>$58.5 \pm 5.9$</td>
<td>$96.3 \pm 2.6$</td>
</tr>
<tr>
<td>Large</td>
<td>Zero-shot</td>
<td>$96.0 \pm 1.6$</td>
<td>$84.0 \pm 4.4$</td>
<td>$61.3 \pm 10.6$</td>
<td>$100.0 \pm 0.0$</td>
<td>$44.3 \pm 6.8$</td>
<td>$97.9 \pm 1.5$</td>
</tr>
<tr>
<td></td>
<td>Few-shot</td>
<td>$82.0 \pm 6.7$</td>
<td>$96.7 \pm 2.0$</td>
<td>$70.0 \pm 7.4$</td>
<td>$100.0 \pm 0.0$</td>
<td>$64.0 \pm 8.8$</td>
<td>$74.0 \pm 13.7$</td>
</tr>
<tr>
<td></td>
<td>Zero-shot+CoT</td>
<td>$100.0 \pm 0.0$</td>
<td>$85.6 \pm 9.0$</td>
<td>$70.0 \pm 21.2$</td>
<td>$100.0 \pm 0.0$</td>
<td>$58.9 \pm 6.4$</td>
<td>$95.6 \pm 4.2$</td>
</tr>
<tr>
<td></td>
<td>Few-shot+CoT</td>
<td>$95.0 \pm 3.6$</td>
<td>$85.7 \pm 6.2$</td>
<td>$82.5 \pm 7.1$</td>
<td>$100.0 \pm 0.0$</td>
<td>$53.3 \pm 10.8$</td>
<td>$89.2 \pm 5.5$</td>
</tr>
</tbody>
</table>
<p>In addition, we can also observe from Table 4 that the larger the size, the smaller the unique rate, indicating that LLM tends to generate graphs from smaller sets. This is counterintuitive because the larger the size, the larger the set of graphs that satisfy the rules. This indicates that LLM can handle a certain amount of information, and the larger the size of the graph, the smaller the set of graphs.</p>
<h1>3.2 Distribution-based graph generation</h1>
<p>We use GPT-4 for graph generation tasks, and there are two metrics for evaluating the quality of generation:</p>
<ul>
<li>$p_{\text {pred }}$ : The value of $p$ represents the distribution of a set of graphs. $p_{\text {pred }}$ is the value of $p$ predicted by the LLM using the set of input graphs.</li>
<li>$p_{\text {gen }}$ : The value of $p$ calculated by the generated graphs.</li>
</ul>
<p>We adopt three experiment settings (refer to Section 2.2 for more details) with increasing difficulty. The purpose of the experiments we designed is as follows:</p>
<ul>
<li>Trees or cycles: Exploring the distribution of graphs.</li>
<li>Union of components: Exploring the distribution of subgraph combinations within a graph.</li>
<li>Motif: Exploring the distribution of subgraph combinations within the graph for more complex situations.</li>
</ul>
<p>The previous experiments have demonstrated that GPT-4 can generate trees and cycles with high accuracy. Therefore, our designed experiments on "Trees or cycles" can eliminate the impact of generation quality as much as possible. In addition, we also constructed a set of experiments to demonstrate the ability of GPT-4 to generate two-component graphs with trees and cycles as subgraphs, as shown in Table 5. This indicates that our constructed "Union of components" experiment can eliminate the impact of generation quality as much as possible. The results can be seen in Figure 5, which provides the $p_{\text {pred }}$ and $p_{\text {gen }}$ values for the three tasks.</p>
<p>Observation 5: LLMs can understand and generate graphs with simple distributions, but perform poorly in complex situations.</p>
<p>Figure 5 shows the performance of GPT-4 on different tasks. From the figure, for "Trees or cycles" and "Union of components", we can see that although the values of $p_{\text {pred }}, p_{\text {gen }}$, and $p$ cannot be completely consistent when using CoT prompt, the curve trends are similar. Since we generate the graph based on probability rather than strictly proportional, this error is reasonable. However, for "Motif" tasks, none of the three prompts learned anything useful, and their $p_{\text {pred }}$ always fluctuated around 0.5 .</p>
<p>Observation 6: Detailed examples and CoT are helpful for distribution-based graph generation.
From Figure 5, it can be seen that for the first two tasks, the $p_{\text {pred }}$ and $p_{\text {gen }}$ generated by CoT are relatively accurate, while zero-shot and few-shot have hardly learned anything. This indicates that providing only examples is not enough,</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Performance of GPT-4 on distribution-based graph generation. $p$ represents the parameter of the distribution where the graphs are sampled. $p_{\text {pred }}$ is the value of $p$ predicted by LLM for the input graphs. $p_{\text {gen }}$ is the value of $p$ calculated by the generated graphs. In this task, the performance of LLMs is better when $p_{\text {pred }}$ and $p_{\text {gen }}$ are closer to the ground-truth parameter $p$.</p>
<p>Table 5: The valid rate of two-component graph generation.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Prompt</th>
<th style="text-align: center;">Tree+Tree</th>
<th style="text-align: center;">Cycle+Cycle</th>
<th style="text-align: center;">Tree+Cycle</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Zero-shot</td>
<td style="text-align: center;">$43.0 \pm 8.3$</td>
<td style="text-align: center;">$39.0 \pm 6.1$</td>
<td style="text-align: center;">$38.0 \pm 11.9$</td>
</tr>
<tr>
<td style="text-align: center;">Few-shot</td>
<td style="text-align: center;">$81.0 \pm 6.2$</td>
<td style="text-align: center;">$13.0 \pm 6.5$</td>
<td style="text-align: center;">$48.0 \pm 11.8$</td>
</tr>
<tr>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$89.0 \pm 4.4$</td>
</tr>
</tbody>
</table>
<p>and it is necessary to demonstrate in detail how to estimate the distribution from the given graph and generate a new set of graphs based on the given p-value for GPT-4.</p>
<h1>3.3 Property-based graph generation</h1>
<p>We use GPT-4 for graph generation, and there are four metrics for evaluating the quality of graph generation:</p>
<ul>
<li>$C_{M}(G)$ : the classifier's predicted probability of having the desired properties for the generated molecules.</li>
<li>$C(G)$ : the rectified probability of having the desired properties for the generated molecules.</li>
<li>Novel rate: the fraction of generated molecules that are distinct from the input molecules.</li>
<li>Unique rate: the fraction of generated molecules that are not duplicates.</li>
</ul>
<p>From Table 6, we have the following observation.
Observation 7: LLMs show preliminary abilities in generating molecules with certain properties.
As shown in Table 6, both $C(G)$ and $C_{M}(G)$ are greater than 0 , indicating that in the classifier's view, some of the generated molecules have specified properties. Although some molecules are duplicates of known molecules, there are still some new molecules that have this property. And it can be found that the generated molecules have relatively high levels of novel rate and unique rate.</p>
<p>Table 6: Results of property-based graph generation. $C_{M}(G)$ is the classifier’s predicted probability of having the desired properties for the generated molecules, while $C(G)$ is the rectified probability. “Novel” denotes generated molecules that are not the same as the input molecules, while “Unique” denotes molecules that are not duplicated with other generated molecules.</p>
<table>
<thead>
<tr>
<th>Prompt</th>
<th>$C_{M}(G)$</th>
<th>$C(G)$</th>
<th>Novel</th>
<th>Unique</th>
</tr>
</thead>
<tbody>
<tr>
<td>Few-shot</td>
<td>$26.4 \pm 7.5$</td>
<td>$34.8 \pm 16.5$</td>
<td>$79.1 \pm 10.9$</td>
<td>$91.8 \pm 6.1$</td>
</tr>
<tr>
<td>Few-shot+CoT</td>
<td>$32.7 \pm 4.7$</td>
<td>$48.8 \pm 10.2$</td>
<td>$65.5 \pm 10.9$</td>
<td>$92.7 \pm 6.0$</td>
</tr>
</tbody>
</table>
<p>In addition, we observe that both $C(G)$ and $C_{M}(G)$ increase when using CoT prompt, which indicating that it’s beneficial for LLM to think step by step, so that the reason abilities of LLM can be activated in these complex graph generation scenarios.</p>
<h1>4 Related Work</h1>
<p>We review related work on large language model for graphs and graph generation respectively.</p>
<h3>4.1 Large Language Model for Graphs</h3>
<p>Recently, there have been several works about utilizing large language models (LLMs) for solving problems on graphs. He et al. [2023] propose to utilize LLMs to generate textural explanations on academic networks, and the explanations are further leveraged to enhance the node features for conducting node classification. Besides enhancing the models with LLM-generated features, Chen et al. [2023] and Ye et al. [2023] further propose to directly adopt LLMs for predicting node categories on text-attributed graphs. Wang et al. [2023a] introduce a benchmark framework to evaluate the performance of LLMs with several graph algorithmic tasks, including topological sort, maximum flows, etc. Guo et al. [2023] is another benchmark to evaluate LLMs’ abilities to tackle structural information, and several factors, such as the role of graph description languages, are taken into consideration in the evaluation process. Zhang et al. [2023b] further considers evaluating the LLMs’ abilities to handle spatial-temporal information on dynamic graphs. Several other works explore the applications of LLMs in graph tool learning Zhang [2023], Jiang et al. [2023], knowledge graphs Pan et al. [2023], etc. However, most existing works mainly focus on graph discriminative tasks, and we explore the potential of LLMs on graph generation, which remains under-explored in the literature.</p>
<h3>4.2 Graph generation</h3>
<p>Graph generation is a highly anticipated research field that is crucial for areas such as code generation and new drug discovery Zhu et al. [2022]. Recently, deep graph generation methods have achieved remarkable results, which can be roughly classified into auto-regressive models Wang et al. [2023b], Bacciu and Podda [2021], variational autoencoder models Du et al., Jin et al. [2018], normalizing flow models Zang and Wang [2020], Luo et al. [2021], generative adversarial network Gamage et al. [2020], Maziarka et al. [2020], diffusion models Jo et al. [2022], Vignac et al. [2023]. However, these methods mainly focus on learning the distribution of graphs from existing graphs for generation, ignoring the complex yet rich domain knowledge in practical applications. Inspired by the recent progress of LLMs in leveraging expert knowledge, in this paper, we systematically explore the graph generation capabilities of LLMs for the first time by proposing various tasks, including rule-based generation, distribution-based generation, and property-based generation.</p>
<h2>5 Conclusion</h2>
<p>In this paper, we explore the potential of large language models (LLMs) for graph generation tasks, which remains unexplored in the literature. We propose LLM4GraphGen to systematically assess LLMs’ capabilities in understanding and applying various graph structure rules, capturing structural type distributions, and leveraging domain knowledge in property-based graph generation. Our findings indicate that while LLMs show promising preliminary abilities in rule-based and distribution-based graph generation, the effectiveness of popular prompting methods like few-shot and chain-of-thought prompting is not consistent. Additionally, the potential of LLMs to generate molecules with specific properties is a notable outcome. These insights open new avenues for future research in graph generation, highlighting the evolving capabilities as well as the limitations of LLMs in graph generation tasks.</p>
<h1>References</h1>
<p>Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. arXiv preprint arXiv:2303.18223, 2023.
Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2303.18223, 2022.
Tom Brown, Benjamin Mann, Nick Ryder, et al. Language models are few-shot learners. In Advances in Neural Information Processing Systems, pages 1877-1901, 2020.
Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 24824-24837, 2022.
Ansong Ni, Srini Iyer, Dragomir Radev, Veselin Stoyanov, Wen-Tau Yih, Sida Wang, and Xi Victoria Lin. LEVER: Learning to verify language-to-code generation with execution. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 26106-26128. PMLR, 2023. URL https://proceedings.mlr.press/v202/ni23b.html.</p>
<p>Priyan Vaithilingam, Tianyi Zhang, and Elena L. Glassman. Expectation vs. experience: Evaluating the usability of code generation tools powered by large language models. In Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems, CHI EA '22, New York, NY, USA, 2022. Association for Computing Machinery. ISBN 9781450391566. doi:10.1145/3491101.3519665. URL https://doi.org/10.1145/3491101.3519665.
Kusuri Murakumo, Naruki Yoshikawa, Kentaro Rikimaru, Shogo Nakamura, Kairi Furui, Takamasa Suzuki, Hiroyuki Yamasaki, Yuki Nishigaya, Yuzo Takagi, and Masahito Ohue. LLM drug discovery challenge: A contest as a feasibility study on the utilization of large language models in medicinal chemistry. In AI for Accelerated Materials Design - NeurIPS 2023 Workshop, 2023. URL https://openreview.net/forum?id=kjUylvko18.
Chiranjib Chakraborty, Manojit Bhattacharya, and Sang-Soo Lee. Artificial intelligence enabled chatgpt and large language models in drug target discovery, drug discovery, and development. Molecular Therapy-Nucleic Acids, 33: 866-868, 2023.
Lars-Peter Meyer, Claus Stadler, Johannes Frey, Norman Radtke, Kurt Junghanns, Roy Meissner, Gordian Dziwis, Kirill Bulert, and Michael Martin. Llm-assisted knowledge graph engineering: Experiments with chatgpt. arXiv preprint arXiv:2307.06917, 2023.
Kai Sun, Yifan Ethan Xu, Hanwen Zha, Yue Liu, and Xin Luna Dong. Head-to-tail: How knowledgeable are large language models (llm)? aka will llms replace knowledge graphs? arXiv preprint arXiv:2308.10168, 2023.
Ziwei Zhang, Haoyang Li, Zeyang Zhang, Yijian Qin, Xin Wang, and Wenwu Zhu. Graph meets llms: Towards large graph models. NeurIPS 2023 New Frontiers in Graph Learning Workshop, 2023a.
Heng Wang, Shangbin Feng, Tianxing He, Zhaoxuan Tan, Xiaochuang Han, and Yulia Tsvetkov. Can language models solve graph problems in natural language? Thirty-seventh Conference on Neural Information Processing Systems, 2023a.
Ruosong Ye, Caiqi Zhang, Runhui Wang, Shuyuan Xu, and Yongfeng Zhang. Natural language is all a graph needs. arXiv preprint arXiv:2308.07134, 2023.
Jin Huang, Xingjian Zhang, Qiaozhu Mei, and Jiaqi Ma. Can llms effectively leverage graph structural information: When and why. arXiv preprint arXiv:2309.16595, 2023.
Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Lixin Su, Suqi Cheng, Dawei Yin, and Chao Huang. Graphgpt: Graph instruction tuning for large language models. arXiv preprint arXiv:2310.13023, 2023.
Yanqiao Zhu, Yuanqi Du, Yinkai Wang, Yichen Xu, Jieyu Zhang, Qiang Liu, and Shu Wu. A survey on deep graph generation: Methods and applications. In Learning on Graphs Conference, pages 47-1, 2022.
Jiaxuan You, Bowen Liu, Zhitao Ying, Vijay Pande, and Jure Leskovec. Graph convolutional policy network for goal-directed molecular graph generation. Advances in neural information processing systems, 31, 2018.
Zhitao Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, and Jure Leskovec. Gnnexplainer: Generating explanations for graph neural networks. Advances in neural information processing systems, 32, 2019.</p>
<p>Yingxin Wu, Xiang Wang, An Zhang, Xiangnan He, and Tat-Seng Chua. Discovering invariant rationales for graph neural networks. In International Conference on Learning Representations, 2022.
Yijian Qin, Xin Wang, Ziwei Zhang, Pengtao Xie, and Wenwu Zhu. Graph neural architecture search under distribution shifts. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvári, Gang Niu, and Sivan Sabato, editors, International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 18083-18095. PMLR, 2022. URL https://proceedings. mlr.press/v162/qin22b.html.
Xiaojie Guo and Liang Zhao. A systematic survey on deep generative models for graph generation. IEEE Trans. Pattern Anal. Mach. Intell., 45(5):5370-5390, 2023. doi:10.1109/TPAMI.2022.3214832. URL https://doi.org/ 10.1109/TPAMI.2022.3214832.</p>
<p>Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. Advances in neural information processing systems, 33:22118-22133, 2020.
David Weininger. Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. Journal of chemical information and computer sciences, 28(1):31-36, 1988.
Xiaoxin He, Xavier Bresson, Thomas Laurent, and Bryan Hooi. Explanations as features: Llm-based features for text-attributed graphs. arXiv preprint arXiv:2305.19523, 2023.
Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi Wen, Xiaochi Wei, Shuaiqiang Wang, Dawei Yin, Wenqi Fan, Hui Liu, and Jiliang Tang. Exploring the potential of large language models (llms) in learning on graphs. arXiv preprint arXiv:2307.03393, 2023.
Jiayan Guo, Lun Du, and Hengyu Liu. Gpt4graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking. arXiv preprint arXiv:2305.15066, 2023.
Zeyang Zhang, Xin Wang, Ziwei Zhang, Haoyang Li, Yijian Qin, Simin Wu, and Wenwu Zhu. Llm4dyg: Can large language models solve problems on dynamic graphs? arXiv preprint arXiv:2310.17110, 2023b.
Jiawei Zhang. Graph-toolformer: To empower llms with graph reasoning ability via prompt augmented by chatgpt. arXiv preprint arXiv:2304.11116, 2023.
Jinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Wayne Xin Zhao, and Ji-Rong Wen. Structgpt: A general framework for large language model to reason over structured data. arXiv preprint arXiv:2305.09645, 2023.
Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, and Xindong Wu. Unifying large language models and knowledge graphs: A roadmap. arXiv preprint arXiv:2306.08302, 2023.
Heng Wang, Shangbin Feng, Tianxing He, Zhaoxuan Tan, Xiaochuang Han, and Yulia Tsvetkov. Can language models solve graph problems in natural language? CoRR, abs/2305.10037, 2023b. doi:10.48550/arXiv.2305.10037. URL https://doi.org/10.48550/arXiv.2305.10037.
Davide Bacciu and Marco Podda. Graphgen-redux: a fast and lightweight recurrent model for labeled graph generation. In International Joint Conference on Neural Networks, IJCNN 2021, Shenzhen, China, July 18-22, 2021, pages 1-8. IEEE, 2021. doi:10.1109/IJCNN52387.2021.9533743. URL https://doi.org/10.1109/IJCNN52387.2021. 9533743.</p>
<p>Yuanqi Du, Xiaojie Guo, Amarda Shehu, and Liang Zhao. Interpretable Molecular Graph Generation via Monotonic Constraints, pages 73-81. doi:10.1137/1.9781611977172.9. URL https://epubs.siam.org/doi/abs/10. $1137 / 1.9781611977172.9$.
Wengong Jin, Regina Barzilay, and Tommi S. Jaakkola. Junction tree variational autoencoder for molecular graph generation. In Jennifer G. Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pages 2328-2337. PMLR, 2018. URL http://proceedings.mlr.press/v80/ jin18a.html.
Chengxi Zang and Fei Wang. Moflow: An invertible flow model for generating molecular graphs. In Rajesh Gupta, Yan Liu, Jiliang Tang, and B. Aditya Prakash, editors, KDD '20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, CA, USA, August 23-27, 2020, pages 617-626. ACM, 2020. doi:10.1145/3394486.3403104. URL https://doi.org/10.1145/3394486.3403104.
Youzhi Luo, Keqiang Yan, and Shuiwang Ji. Graphdf: A discrete flow model for molecular graph generation. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 7192-7203. PMLR, 2021. URL http://proceedings.mlr.press/v139/luo21a.html.</p>
<p>Anuththari Gamage, Eli Chien, Jianhao Peng, and Olgica Milenkovic. Multi-motifgan (MMGAN): motiftargeted graph generation and prediction. In 2020 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2020, Barcelona, Spain, May 4-8, 2020, pages 4182-4186. IEEE, 2020. doi:10.1109/ICASSP40776.2020.9053451. URL https://doi.org/10.1109/ICASSP40776.2020.9053451.
Lukasz Maziarka, Agnieszka Pocha, Jan Kaczmarczyk, Krzysztof Rataj, Tomasz Danel, and Michal Warchol. Molcyclegan: a generative model for molecular optimization. J. Cheminformatics, 12(1):2, 2020. doi:10.1186/S13321-019-0404-1. URL https://doi.org/10.1186/s13321-019-0404-1.
Jaehyeong Jo, Seul Lee, and Sung Ju Hwang. Score-based generative modeling of graphs via the system of stochastic differential equations. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvári, Gang Niu, and Sivan Sabato, editors, International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 10362-10383. PMLR, 2022. URL https://proceedings.mlr.press/v162/jo22a.html.
Clément Vignac, Igor Krawczuk, Antoine Siraudin, Bohan Wang, Volkan Cevher, and Pascal Frossard. Digress: Discrete denoising diffusion for graph generation. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/pdf?id= UaAD-Nu86WX.</p>
<h1>A Experimental setup</h1>
<h2>A. 1 Rule-based generation</h2>
<p>For the rule-based graph generation experiments, we configure the tasks with the problem settings listed in Table 7 unless otherwise specified. The task settings are chosen to be challenging for LLMs. Additionally, to produce the results in the comparison of different graph sizes, the parameters for the three different sizes are listed in Table 8; note that the "Medium" size is the same as the default settings. In the comparison of different LLMs, since models other than GPT-4 are relatively weak, we use the "Small" settings from Table 8 in the experiments. Unless otherwise specified, we use the sampling temperature $t=0.8$.</p>
<p>It is worth noting that the number of edges have a significant effect on the difficulty of "Planar" and " $k$-color" tasks, as graphs with few edges are easily planar or $k$-colorable. The parameters in the experiments are chosen such that random graphs with the specified number of nodes and edges have around $20 \%$ probability to be valid.</p>
<p>Table 7: Problem settings for different rule-based tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Settings</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Trees</td>
<td style="text-align: center;">15 nodes</td>
</tr>
<tr>
<td style="text-align: center;">Cycles</td>
<td style="text-align: center;">15 nodes</td>
</tr>
<tr>
<td style="text-align: center;">Components</td>
<td style="text-align: center;">15 nodes, 5 components</td>
</tr>
<tr>
<td style="text-align: center;">Planar</td>
<td style="text-align: center;">15 nodes, 24 edges</td>
</tr>
<tr>
<td style="text-align: center;">$k$-regular</td>
<td style="text-align: center;">16 nodes, $k=3$</td>
</tr>
<tr>
<td style="text-align: center;">Wheel</td>
<td style="text-align: center;">15 nodes</td>
</tr>
<tr>
<td style="text-align: center;">Bipartite</td>
<td style="text-align: center;">5 nodes in each partition</td>
</tr>
<tr>
<td style="text-align: center;">$k$-color</td>
<td style="text-align: center;">15 nodes, 32 edges, $k=3$</td>
</tr>
</tbody>
</table>
<p>Table 8: Problem settings for rule-based tasks with different sizes.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Settings</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Cycles</td>
<td style="text-align: center;">Small</td>
<td style="text-align: center;">10 nodes</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Medium</td>
<td style="text-align: center;">15 nodes</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Large</td>
<td style="text-align: center;">20 nodes</td>
</tr>
<tr>
<td style="text-align: center;">$k$-regular</td>
<td style="text-align: center;">Small</td>
<td style="text-align: center;">12 nodes, $k=3$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Medium</td>
<td style="text-align: center;">16 nodes, $k=3$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Large</td>
<td style="text-align: center;">20 nodes, $k=3$</td>
</tr>
<tr>
<td style="text-align: center;">$k$-color</td>
<td style="text-align: center;">Small</td>
<td style="text-align: center;">10 nodes, 20 edges, $k=3$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Medium</td>
<td style="text-align: center;">15 nodes, 32 edges, $k=3$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Large</td>
<td style="text-align: center;">18 nodes, 39 edges, $k=3$</td>
</tr>
</tbody>
</table>
<h2>A. 2 Distribution-based generation</h2>
<p>For the "Trees or cycles" task, the number of nodes for each tree or cycle is randomly selected between 5 and 7. For the "Union of components" task, each of the two components for each graph has between 5 and 7 nodes. For the "Motif" task, we define the three kinds of base graphs and motif graphs in the same way as Spurious-Motif, and the description is given in Table 9. All experiments use the sampling temperature $t=0.5$.</p>
<h2>A. 3 Property-based generation</h2>
<p>For property-based graph generation, we generate molecules using GPT-4 with sampling temperature set to $t=0.5$. To evaluate the performance of molecule generation, we measure the fraction of generated molecules that have the desired property using a GNN classifier trained on the OGBG-MolHIV dataset. More specifically, let $G_{1}, G_{2}, \ldots, G_{n}$ be the generate graphs that represent molecules. Let $C_{T}(G) \in{0,1}$ be the ground-truth label of graph $G$, with $C_{T}(G)=1$ meaning the molecule $G$ has the desired property (e.g. can inhibit HIV replication), and $C_{T}(G)=0$ meaning otherwise. Similarly, let $C_{M}(G)$ be the predicted label of $G$. These values are related using the law of total probability:</p>
<p>$$
p\left(C_{M}(G)=1\right)=p\left(C_{T}(G)=0\right) p\left(C_{M}(G)=1 \mid C_{T}(G)=0\right)+p\left(C_{T}(G)=1\right) p\left(C_{M}(G)=1 \mid C_{T}(G)=1\right)
$$</p>
<p>Table 9: The definition of base graphs and motif graphs for the "Motif" task. The definition of motif graphs are given by first listing the number of nodes, then listing the endpoints of all edges, which is in the same format as the input to LLMs.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Graph</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Trees</td>
<td style="text-align: left;">Full binary or ternary tree</td>
</tr>
<tr>
<td style="text-align: left;">Ladders</td>
<td style="text-align: left;">Two paths of same length with edge connections between pairs of nodes</td>
</tr>
<tr>
<td style="text-align: left;">Wheels</td>
<td style="text-align: left;">A single node connected to all nodes of a cycle</td>
</tr>
<tr>
<td style="text-align: left;">Cycle</td>
<td style="text-align: left;">$(5,[(1,2),(1,5),(2,3),(3,4),(4,5)])$</td>
</tr>
<tr>
<td style="text-align: left;">House</td>
<td style="text-align: left;">$(5,[(1,2),(1,5),(2,3),(2,5),(3,4),(4,5)])$</td>
</tr>
<tr>
<td style="text-align: left;">Crane</td>
<td style="text-align: left;">$(5,[(1,2),(1,3),(1,4),(1,5),(2,3),(3,4),(4,5)])$</td>
</tr>
</tbody>
</table>
<p>Table 10: The confusion matrix of the molecule classifier used in the experiments of property-based graph generation. $C(G)$ denotes the ground-truth label, $C_{M}(G)$ denotes the predicted label, and the numbers represent the amount of molecules that fall into this category.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">$C_{M}(G)=0$</th>
<th style="text-align: center;">$C_{M}(G)=1$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$C(G)=0$</td>
<td style="text-align: center;">35539</td>
<td style="text-align: center;">4145</td>
</tr>
<tr>
<td style="text-align: left;">$C(G)=1$</td>
<td style="text-align: center;">633</td>
<td style="text-align: center;">810</td>
</tr>
</tbody>
</table>
<p>where the conditional probability can be computed from the confusion matrix of the classifier as given in Table 10. By rearranging the equations, we can compute rectified predictions $C(G)$ that are closer to $C_{T}(G)$ :</p>
<p>$$
p(C(G)=1)=\frac{p\left(C_{M}(G)=1\right)-p\left(C_{M}(G)=1 \mid C_{T}(G)=0\right)}{p\left(C_{M}(G)=1 \mid C_{T}(G)=1\right)-p\left(C_{M}(G)=1 \mid C_{T}(G)=0\right)}
$$</p>
<h1>B Extended experimental results</h1>
<p>We provide complete experimental results that include valid rates, novel rates, and unique rates for rule-based generation tasks in Table 14, Table 15, and Table 16. Additionally, we compare the effect of different sampling temperatures for rule-based generation in Table 17, and the effect of different amounts of generated graphs in Table 18. From the two tables, we find that it is necessary to set appropriate sampling temperature and amount of generated graphs for different tasks.</p>
<p>For distribution-based generation, we provide numerical results in Table 11, Table 12 and Table 13.</p>
<p>Table 11: Metrics for distribution-based graph generation on the "Trees or cycles" task. $p$ represents the parameter of the distribution where the graphs are sampled. $p_{\text {pred }}$ is the value of $p$ predicted by LLM for the input graphs. $p_{\text {gen }}$ is the value of $p$ calculated by the generated graphs. "Valid" denotes fractions of generated graphs that are valid under specific rules.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">$p$</th>
<th style="text-align: center;">Prompt</th>
<th style="text-align: center;">$p_{\text {pred }}$</th>
<th style="text-align: center;">$p_{\text {gen }}$</th>
<th style="text-align: center;">Valid</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$p=0.2$</td>
<td style="text-align: center;">Zero-shot</td>
<td style="text-align: center;">$62.0 \pm 1.8$</td>
<td style="text-align: center;">$54.3 \pm 17.8$</td>
<td style="text-align: center;">$72.0 \pm 7.2$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Few-shot</td>
<td style="text-align: center;">$60.0 \pm 0.0$</td>
<td style="text-align: center;">$32.2 \pm 14.9$</td>
<td style="text-align: center;">$86.0 \pm 6.1$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">$45.0 \pm 3.5$</td>
<td style="text-align: center;">$50.4 \pm 6.7$</td>
<td style="text-align: center;">$83.3 \pm 9.8$</td>
</tr>
<tr>
<td style="text-align: center;">$p=0.4$</td>
<td style="text-align: center;">Zero-shot</td>
<td style="text-align: center;">$60.0 \pm 2.8$</td>
<td style="text-align: center;">$74.3 \pm 12.7$</td>
<td style="text-align: center;">$84.0 \pm 5.4$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Few-shot</td>
<td style="text-align: center;">$62.0 \pm 1.8$</td>
<td style="text-align: center;">$68.9 \pm 16.1$</td>
<td style="text-align: center;">$98.0 \pm 1.8$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">$43.3 \pm 2.7$</td>
<td style="text-align: center;">$59.0 \pm 4.8$</td>
<td style="text-align: center;">$83.3 \pm 7.2$</td>
</tr>
<tr>
<td style="text-align: center;">$p=0.6$</td>
<td style="text-align: center;">Zero-shot</td>
<td style="text-align: center;">$58.0 \pm 1.8$</td>
<td style="text-align: center;">$48.1 \pm 15.1$</td>
<td style="text-align: center;">$88.0 \pm 5.2$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Few-shot</td>
<td style="text-align: center;">$62.0 \pm 1.8$</td>
<td style="text-align: center;">$16.0 \pm 10.4$</td>
<td style="text-align: center;">$92.0 \pm 5.2$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">$53.3 \pm 5.4$</td>
<td style="text-align: center;">$86.3 \pm 7.1$</td>
<td style="text-align: center;">$96.7 \pm 2.7$</td>
</tr>
<tr>
<td style="text-align: center;">$p=0.8$</td>
<td style="text-align: center;">Zero-shot</td>
<td style="text-align: center;">$60.0 \pm 0.0$</td>
<td style="text-align: center;">$90.0 \pm 8.9$</td>
<td style="text-align: center;">$92.0 \pm 5.2$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Few-shot</td>
<td style="text-align: center;">$64.0 \pm 2.2$</td>
<td style="text-align: center;">$62.1 \pm 12.1$</td>
<td style="text-align: center;">$88.0 \pm 5.2$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">$76.7 \pm 5.4$</td>
<td style="text-align: center;">$92.6 \pm 6.0$</td>
<td style="text-align: center;">$93.3 \pm 2.7$</td>
</tr>
</tbody>
</table>
<p>Table 12: Metrics for distribution-based graph generation on the "Union of components" task. $p$ represents the parameter of the distribution where the graphs are sampled. $p_{\text {pred }}$ is the value of $p$ predicted by LLM for the input graphs. $p_{\text {gen }}$ is the value of $p$ calculated by the generated graphs. "Valid" denotes fractions of generated graphs that are valid under specific rules.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">$p$</th>
<th style="text-align: center;">Prompt</th>
<th style="text-align: center;">$p_{\text {pred }}$</th>
<th style="text-align: center;">$p_{\text {gen }}$</th>
<th style="text-align: center;">Valid</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Zero-shot</td>
<td style="text-align: center;">$62.0 \pm 1.8$</td>
<td style="text-align: center;">$58.2 \pm 16.2$</td>
<td style="text-align: center;">$82.0 \pm 4.4$</td>
</tr>
<tr>
<td style="text-align: center;">$p=0.2$</td>
<td style="text-align: center;">Few-shot</td>
<td style="text-align: center;">$60.0 \pm 0.0$</td>
<td style="text-align: center;">$40.3 \pm 14.9$</td>
<td style="text-align: center;">$70.0 \pm 11.7$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">$30.0 \pm 8.5$</td>
<td style="text-align: center;">$25.0 \pm 10.0$</td>
<td style="text-align: center;">$46.0 \pm 8.8$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Zero-shot</td>
<td style="text-align: center;">$58.0 \pm 1.8$</td>
<td style="text-align: center;">$57.2 \pm 16.3$</td>
<td style="text-align: center;">$70.0 \pm 13.3$</td>
</tr>
<tr>
<td style="text-align: center;">$p=0.4$</td>
<td style="text-align: center;">Few-shot</td>
<td style="text-align: center;">$60.0 \pm 0.0$</td>
<td style="text-align: center;">$62.2 \pm 12.8$</td>
<td style="text-align: center;">$84.0 \pm 6.1$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">$36.0 \pm 3.6$</td>
<td style="text-align: center;">$42.0 \pm 7.9$</td>
<td style="text-align: center;">$68.0 \pm 4.4$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Zero-shot</td>
<td style="text-align: center;">$62.0 \pm 1.8$</td>
<td style="text-align: center;">$49.2 \pm 16.1$</td>
<td style="text-align: center;">$72.0 \pm 14.8$</td>
</tr>
<tr>
<td style="text-align: center;">$p=0.6$</td>
<td style="text-align: center;">Few-shot</td>
<td style="text-align: center;">$60.0 \pm 0.0$</td>
<td style="text-align: center;">$52.8 \pm 14.8$</td>
<td style="text-align: center;">$86.0 \pm 4.6$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">$53.4 \pm 7.0$</td>
<td style="text-align: center;">$70.6 \pm 8.6$</td>
<td style="text-align: center;">$72.0 \pm 9.5$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Zero-shot</td>
<td style="text-align: center;">$58.0 \pm 1.8$</td>
<td style="text-align: center;">$72.0 \pm 11.1$</td>
<td style="text-align: center;">$54.0 \pm 16.9$</td>
</tr>
<tr>
<td style="text-align: center;">$p=0.8$</td>
<td style="text-align: center;">Few-shot</td>
<td style="text-align: center;">$60.0 \pm 0.0$</td>
<td style="text-align: center;">$45.0 \pm 15.2$</td>
<td style="text-align: center;">$86.0 \pm 3.6$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">$59.2 \pm 5.0$</td>
<td style="text-align: center;">$62.9 \pm 9.7$</td>
<td style="text-align: center;">$78.0 \pm 5.2$</td>
</tr>
</tbody>
</table>
<p>Table 13: Metrics for distribution-based graph generation on the "Motif" task. $p$ represents the parameter of the distribution where the graphs are sampled. $p_{\text {pred }}$ is the value of $p$ predicted by LLM for the input graphs. $p_{\text {gen }}$ is the value of $p$ calculated by the generated graphs. "Valid" denotes fractions of generated graphs that are valid under specific rules.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">$p$</th>
<th style="text-align: center;">Prompt</th>
<th style="text-align: center;">$p_{\text {pred }}$</th>
<th style="text-align: center;">$p_{\text {gen }}$</th>
<th style="text-align: center;">Valid</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Zero-shot</td>
<td style="text-align: center;">$60.0 \pm 0.0$</td>
<td style="text-align: center;">$18.7 \pm 7.6$</td>
<td style="text-align: center;">$56.0 \pm 18.9$</td>
</tr>
<tr>
<td style="text-align: center;">$p=0.2$</td>
<td style="text-align: center;">Few-shot</td>
<td style="text-align: center;">$48.0 \pm 5.0$</td>
<td style="text-align: center;">$42.0 \pm 14.8$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">$42.0 \pm 3.3$</td>
<td style="text-align: center;">$42.1 \pm 2.4$</td>
<td style="text-align: center;">$90.0 \pm 4.9$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Zero-shot</td>
<td style="text-align: center;">$58.0 \pm 1.8$</td>
<td style="text-align: center;">$22.9 \pm 5.9$</td>
<td style="text-align: center;">$98.0 \pm 1.8$</td>
</tr>
<tr>
<td style="text-align: center;">$p=0.4$</td>
<td style="text-align: center;">Few-shot</td>
<td style="text-align: center;">$42.0 \pm 4.4$</td>
<td style="text-align: center;">$33.7 \pm 10.5$</td>
<td style="text-align: center;">$94.0 \pm 5.4$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">$30.0 \pm 2.8$</td>
<td style="text-align: center;">$55.6 \pm 9.0$</td>
<td style="text-align: center;">$98.0 \pm 1.8$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Zero-shot</td>
<td style="text-align: center;">$52.0 \pm 1.8$</td>
<td style="text-align: center;">$24.2 \pm 7.3$</td>
<td style="text-align: center;">$90.0 \pm 6.9$</td>
</tr>
<tr>
<td style="text-align: center;">$p=0.6$</td>
<td style="text-align: center;">Few-shot</td>
<td style="text-align: center;">$48.0 \pm 8.2$</td>
<td style="text-align: center;">$38.9 \pm 10.0$</td>
<td style="text-align: center;">$98.0 \pm 1.8$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">$40.0 \pm 2.4$</td>
<td style="text-align: center;">$52.2 \pm 6.1$</td>
<td style="text-align: center;">$95.0 \pm 3.1$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Zero-shot</td>
<td style="text-align: center;">$56.0 \pm 2.2$</td>
<td style="text-align: center;">$35.1 \pm 11.2$</td>
<td style="text-align: center;">$98.0 \pm 1.8$</td>
</tr>
<tr>
<td style="text-align: center;">$p=0.8$</td>
<td style="text-align: center;">Few-shot</td>
<td style="text-align: center;">$56.0 \pm 6.7$</td>
<td style="text-align: center;">$38.0 \pm 8.2$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">$41.4 \pm 3.7$</td>
<td style="text-align: center;">$63.3 \pm 7.7$</td>
<td style="text-align: center;">$94.0 \pm 3.6$</td>
</tr>
</tbody>
</table>
<p>Table 14: The novel rate for rule-based graph generation with GPT-4. The metric measures the fraction of generated graphs that are different from the given example graphs. Values after $\pm$ denote standard errors.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Metric</th>
<th style="text-align: center;">Prompt</th>
<th style="text-align: center;">Trees</th>
<th style="text-align: center;">Cycles</th>
<th style="text-align: center;">Components</th>
<th style="text-align: center;">Planar</th>
<th style="text-align: center;">$k$-regular</th>
<th style="text-align: center;">Wheel</th>
<th style="text-align: center;">Bipartite</th>
<th style="text-align: center;">$k$-color</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Valid</td>
<td style="text-align: center;">Zero-shot</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$91.3 \pm 3.3$</td>
<td style="text-align: center;">$30.4 \pm 5.1$</td>
<td style="text-align: center;">$47.3 \pm 4.2$</td>
<td style="text-align: center;">$64.0 \pm 6.8$</td>
<td style="text-align: center;">$13.0 \pm 5.3$</td>
<td style="text-align: center;">$60.3 \pm 7.4$</td>
<td style="text-align: center;">$50.3 \pm 5.5$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Few-shot</td>
<td style="text-align: center;">$98.0 \pm 0.9$</td>
<td style="text-align: center;">$85.0 \pm 3.3$</td>
<td style="text-align: center;">$63.2 \pm 5.3$</td>
<td style="text-align: center;">$4.3 \pm 1.3$</td>
<td style="text-align: center;">$86.1 \pm 3.1$</td>
<td style="text-align: center;">$88.8 \pm 7.4$</td>
<td style="text-align: center;">$57.1 \pm 8.6$</td>
<td style="text-align: center;">$62.3 \pm 5.1$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Zero-shot+CoT</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$86.9 \pm 3.6$</td>
<td style="text-align: center;">$38.0 \pm 5.1$</td>
<td style="text-align: center;">$53.3 \pm 6.0$</td>
<td style="text-align: center;">$82.7 \pm 8.6$</td>
<td style="text-align: center;">$92.3 \pm 4.7$</td>
<td style="text-align: center;">$92.7 \pm 4.4$</td>
<td style="text-align: center;">$43.2 \pm 4.9$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Few-shot+CoT</td>
<td style="text-align: center;">$97.6 \pm 1.7$</td>
<td style="text-align: center;">$97.0 \pm 1.9$</td>
<td style="text-align: center;">$40.0 \pm 6.7$</td>
<td style="text-align: center;">$20.0 \pm 4.3$</td>
<td style="text-align: center;">$91.5 \pm 1.6$</td>
<td style="text-align: center;">$90.7 \pm 5.1$</td>
<td style="text-align: center;">$98.2 \pm 1.8$</td>
<td style="text-align: center;">$58.5 \pm 5.9$</td>
</tr>
<tr>
<td style="text-align: center;">Unique</td>
<td style="text-align: center;">Zero-shot</td>
<td style="text-align: center;">$98.6 \pm 0.9$</td>
<td style="text-align: center;">$88.3 \pm 1.6$</td>
<td style="text-align: center;">$91.9 \pm 3.0$</td>
<td style="text-align: center;">$99.7 \pm 0.3$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$98.7 \pm 0.8$</td>
<td style="text-align: center;">$44.3 \pm 7.0$</td>
<td style="text-align: center;">$98.3 \pm 1.6$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Few-shot</td>
<td style="text-align: center;">$99.3 \pm 0.5$</td>
<td style="text-align: center;">$92.3 \pm 1.3$</td>
<td style="text-align: center;">$97.7 \pm 1.1$</td>
<td style="text-align: center;">$96.8 \pm 2.5$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$98.8 \pm 1.1$</td>
<td style="text-align: center;">$50.0 \pm 8.2$</td>
<td style="text-align: center;">$98.5 \pm 1.5$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Zero-shot+CoT</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$89.0 \pm 4.2$</td>
<td style="text-align: center;">$98.5 \pm 0.8$</td>
<td style="text-align: center;">$98.6 \pm 0.8$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$85.0 \pm 5.7$</td>
<td style="text-align: center;">$16.9 \pm 4.2$</td>
<td style="text-align: center;">$98.6 \pm 1.0$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Few-shot+CoT</td>
<td style="text-align: center;">$99.7 \pm 0.3$</td>
<td style="text-align: center;">$83.0 \pm 5.1$</td>
<td style="text-align: center;">$96.3 \pm 1.5$</td>
<td style="text-align: center;">$98.0 \pm 1.4$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$88.3 \pm 5.1$</td>
<td style="text-align: center;">$10.7 \pm 0.7$</td>
<td style="text-align: center;">$96.3 \pm 2.6$</td>
</tr>
<tr>
<td style="text-align: center;">Novel</td>
<td style="text-align: center;">Zero-shot</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Few-shot</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$95.8 \pm 4.1$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Zero-shot+CoT</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Few-shot+CoT</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$96.4 \pm 3.5$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
</tr>
</tbody>
</table>
<p>Table 15: Metrics about the comparison of different graph sizes for rule-based graph generation with GPT-4. "Valid" denotes fractions of generated graphs that are valid under specific rules, "Unique" denotes fractions of generated graphs that are not duplicates, and "Novel" denotes fractions of generated graphs that are different from the given example graphs.</p>
<table>
<thead>
<tr>
<th>Size</th>
<th>Prompt</th>
<th>Cycles</th>
<th></th>
<th></th>
<th>$k$-regular</th>
<th></th>
<th></th>
<th>$k$-color</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>Valid</td>
<td>Unique</td>
<td>Novel</td>
<td>Valid</td>
<td>Unique</td>
<td>Novel</td>
<td>Valid</td>
<td>Unique</td>
<td>Novel</td>
</tr>
<tr>
<td>Small</td>
<td>Zero-shot</td>
<td>84.7 $\pm 5.9$</td>
<td>90.0 $\pm 3.1$</td>
<td>100.0 $\pm 0.0$</td>
<td>74.7 $\pm 9.9$</td>
<td>100.0 $\pm 0.0$</td>
<td>100.0 $\pm 0.0$</td>
<td>56.0 $\pm 4.5$</td>
<td>100.0 $\pm 0.0$</td>
<td>100.0 $\pm 0.0$</td>
</tr>
<tr>
<td></td>
<td>Few-shot</td>
<td>73.3 $\pm 7.1$</td>
<td>96.0 $\pm 1.8$</td>
<td>100.0 $\pm 0.0$</td>
<td>90.0 $\pm 2.9$</td>
<td>100.0 $\pm 0.0$</td>
<td>100.0 $\pm 0.0$</td>
<td>76.4 $\pm 5.3$</td>
<td>100.0 $\pm 0.0$</td>
<td>100.0 $\pm 0.0$</td>
</tr>
<tr>
<td></td>
<td>Zero-shot+CoT</td>
<td>95.3 $\pm 3.2$</td>
<td>84.7 $\pm 4.8$</td>
<td>100.0 $\pm 0.0$</td>
<td>97.1 $\pm 1.2$</td>
<td>100.0 $\pm 0.0$</td>
<td>100.0 $\pm 0.0$</td>
<td>62.0 $\pm 6.1$</td>
<td>96.7 $\pm 3.2$</td>
<td>100.0 $\pm 0.0$</td>
</tr>
<tr>
<td></td>
<td>Few-shot+CoT</td>
<td>96.0 $\pm 2.8$</td>
<td>94.7 $\pm 2.1$</td>
<td>100.0 $\pm 0.0$</td>
<td>87.9 $\pm 3.4$</td>
<td>100.0 $\pm 0.0$</td>
<td>100.0 $\pm 0.0$</td>
<td>80.7 $\pm 3.2$</td>
<td>100.0 $\pm 0.0$</td>
<td>100.0 $\pm 0.0$</td>
</tr>
<tr>
<td>Medium</td>
<td>Zero-shot</td>
<td>91.3 $\pm 3.3$</td>
<td>88.3 $\pm 1.6$</td>
<td>100.0 $\pm 0.0$</td>
<td>64.0 $\pm 6.8$</td>
<td>100.0 $\pm 0.0$</td>
<td>100.0 $\pm 0.0$</td>
<td>50.3 $\pm 5.5$</td>
<td>98.3 $\pm 1.6$</td>
<td>100.0 $\pm 0.0$</td>
</tr>
<tr>
<td></td>
<td>Few-shot</td>
<td>85.0 $\pm 3.3$</td>
<td>92.3 $\pm 1.3$</td>
<td>100.0 $\pm 0.0$</td>
<td>86.1 $\pm 3.1$</td>
<td>100.0 $\pm 0.0$</td>
<td>100.0 $\pm 0.0$</td>
<td>62.3 $\pm 5.1$</td>
<td>98.5 $\pm 1.5$</td>
<td>100.0 $\pm 0.0$</td>
</tr>
<tr>
<td></td>
<td>Zero-shot+CoT</td>
<td>86.9 $\pm 3.6$</td>
<td>89.0 $\pm 4.2$</td>
<td>100.0 $\pm 0.0$</td>
<td>82.7 $\pm 8.6$</td>
<td>100.0 $\pm 0.0$</td>
<td>100.0 $\pm 0.0$</td>
<td>43.2 $\pm 4.9$</td>
<td>98.6 $\pm 1.0$</td>
<td>100.0 $\pm 0.0$</td>
</tr>
<tr>
<td></td>
<td>Few-shot+CoT</td>
<td>97.0 $\pm 1.9$</td>
<td>83.0 $\pm 5.1$</td>
<td>100.0 $\pm 0.0$</td>
<td>91.5 $\pm 1.6$</td>
<td>100.0 $\pm 0.0$</td>
<td>100.0 $\pm 0.0$</td>
<td>58.5 $\pm 5.9$</td>
<td>96.3 $\pm 2.6$</td>
<td>100.0 $\pm 0.0$</td>
</tr>
<tr>
<td>Large</td>
<td>Zero-shot</td>
<td>96.0 $\pm 1.6$</td>
<td>84.0 $\pm 4.4$</td>
<td>100.0 $\pm 0.0$</td>
<td>61.3 $\pm 10.6$</td>
<td>100.0 $\pm 0.0$</td>
<td>100.0 $\pm 0.0$</td>
<td>44.3 $\pm 6.8$</td>
<td>97.9 $\pm 1.5$</td>
<td>100.0 $\pm 0.0$</td>
</tr>
<tr>
<td></td>
<td>Few-shot</td>
<td>82.0 $\pm 6.7$</td>
<td>96.7 $\pm 2.0$</td>
<td>100.0 $\pm 0.0$</td>
<td>70.0 $\pm 7.4$</td>
<td>100.0 $\pm 0.0$</td>
<td>100.0 $\pm 0.0$</td>
<td>64.0 $\pm 8.8$</td>
<td>74.0 $\pm 13.7$</td>
<td>100.0 $\pm 0.0$</td>
</tr>
<tr>
<td></td>
<td>Zero-shot+CoT</td>
<td>100.0 $\pm 0.0$</td>
<td>85.6 $\pm 9.0$</td>
<td>100.0 $\pm 0.0$</td>
<td>70.0 $\pm 21.2$</td>
<td>100.0 $\pm 0.0$</td>
<td>100.0 $\pm 0.0$</td>
<td>58.9 $\pm 6.4$</td>
<td>95.6 $\pm 4.2$</td>
<td>100.0 $\pm 0.0$</td>
</tr>
<tr>
<td></td>
<td>Few-shot+CoT</td>
<td>95.0 $\pm 3.6$</td>
<td>85.7 $\pm 6.2$</td>
<td>100.0 $\pm 0.0$</td>
<td>82.5 $\pm 7.1$</td>
<td>100.0 $\pm 0.0$</td>
<td>100.0 $\pm 0.0$</td>
<td>53.3 $\pm 10.8$</td>
<td>89.2 $\pm 5.5$</td>
<td>100.0 $\pm 0.0$</td>
</tr>
</tbody>
</table>
<p>Table 16: Metrics about the comparison of rule-based graph generation using different LLMs. "Valid" denotes fractions of generated graphs that are valid under specific rules, "Unique" denotes fractions of generated graphs that are not duplicates, and "Novel" denotes fractions of generated graphs that are different from the given example graphs.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Prompt</th>
<th>Cycles</th>
<th></th>
<th></th>
<th>$k$-regular</th>
<th></th>
<th></th>
<th>$k$-color</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>Valid</td>
<td>Unique</td>
<td>Novel</td>
<td>Valid</td>
<td>Unique</td>
<td>Novel</td>
<td>Valid</td>
<td>Unique</td>
<td>Novel</td>
</tr>
<tr>
<td>GPT-4</td>
<td>Zero-shot</td>
<td>84.7 $\pm 5.9$</td>
<td>90.0 $\pm 3.1$</td>
<td>100.0 $\pm 0.0$</td>
<td>74.7 $\pm 9.9$</td>
<td>100.0 $\pm 0.0$</td>
<td>100.0 $\pm 0.0$</td>
<td>56.0 $\pm 4.5$</td>
<td>100.0 $\pm 0.0$</td>
<td>100.0 $\pm 0.0$</td>
</tr>
<tr>
<td></td>
<td>Few-shot</td>
<td>73.3 $\pm 7.1$</td>
<td>96.0 $\pm 1.8$</td>
<td>100.0 $\pm 0.0$</td>
<td>90.0 $\pm 2.9$</td>
<td>100.0 $\pm 0.0$</td>
<td>100.0 $\pm 0.0$</td>
<td>76.4 $\pm 5.3$</td>
<td>100.0 $\pm 0.0$</td>
<td>100.0 $\pm 0.0$</td>
</tr>
<tr>
<td></td>
<td>Zero-shot+CoT</td>
<td>95.3 $\pm 3.2$</td>
<td>84.7 $\pm 4.8$</td>
<td>100.0 $\pm 0.0$</td>
<td>97.1 $\pm 1.2$</td>
<td>100.0 $\pm 0.0$</td>
<td>100.0 $\pm 0.0$</td>
<td>62.0 $\pm 6.1$</td>
<td>96.7 $\pm 3.2$</td>
<td>100.0 $\pm 0.0$</td>
</tr>
<tr>
<td></td>
<td>Few-shot+CoT</td>
<td>96.0 $\pm 2.8$</td>
<td>94.7 $\pm 2.1$</td>
<td>100.0 $\pm 0.0$</td>
<td>87.9 $\pm 3.4$</td>
<td>100.0 $\pm 0.0$</td>
<td>100.0 $\pm 0.0$</td>
<td>80.7 $\pm 3.2$</td>
<td>100.0 $\pm 0.0$</td>
<td>100.0 $\pm 0.0$</td>
</tr>
<tr>
<td>GPT-3.5</td>
<td>Zero-shot</td>
<td>84.7 $\pm 5.6$</td>
<td>84.0 $\pm 2.8$</td>
<td>100.0 $\pm 0.0$</td>
<td>6.7 $\pm 2.4$</td>
<td>64.0 $\pm 9.9$</td>
<td>100.0 $\pm 0.0$</td>
<td>0.0 $\pm 0.0$</td>
<td>72.0 $\pm 8.1$</td>
<td>100.0 $\pm 0.0$</td>
</tr>
<tr>
<td></td>
<td>Few-shot</td>
<td>22.7 $\pm 7.4$</td>
<td>86.0 $\pm 5.6$</td>
<td>100.0 $\pm 0.0$</td>
<td>8.7 $\pm 3.6$</td>
<td>98.7 $\pm 1.3$</td>
<td>96.7 $\pm 2.6$</td>
<td>29.3 $\pm 7.0$</td>
<td>91.3 $\pm 6.0$</td>
<td>100.0 $\pm 0.0$</td>
</tr>
<tr>
<td></td>
<td>Zero-shot+CoT</td>
<td>56.7 $\pm 8.0$</td>
<td>88.0 $\pm 3.3$</td>
<td>100.0 $\pm 0.0$</td>
<td>2.7 $\pm 2.6$</td>
<td>78.2 $\pm 9.9$</td>
<td>100.0 $\pm 0.0$</td>
<td>14.3 $\pm 8.4$</td>
<td>81.4 $\pm 6.7$</td>
<td>100.0 $\pm 0.0$</td>
</tr>
<tr>
<td></td>
<td>Few-shot+CoT</td>
<td>57.5 $\pm 12.1$</td>
<td>75.8 $\pm 9.0$</td>
<td>100.0 $\pm 0.0$</td>
<td>9.0 $\pm 4.6$</td>
<td>96.0 $\pm 3.8$</td>
<td>97.0 $\pm 2.8$</td>
<td>34.3 $\pm 8.0$</td>
<td>98.6 $\pm 1.4$</td>
<td>100.0 $\pm 0.0$</td>
</tr>
<tr>
<td>LLama2</td>
<td>Zero-shot</td>
<td>0.7 $\pm 0.7$</td>
<td>90.7 $\pm 2.6$</td>
<td>100.0 $\pm 0.0$</td>
<td>0.0 $\pm 0.0$</td>
<td>89.2 $\pm 3.8$</td>
<td>100.0 $\pm 0.0$</td>
<td>0.0 $\pm 0.0$</td>
<td>78.0 $\pm 5.2$</td>
<td>100.0 $\pm 0.0$</td>
</tr>
<tr>
<td></td>
<td>Few-shot</td>
<td>31.1 $\pm 7.8$</td>
<td>100.0 $\pm 0.0$</td>
<td>70.4 $\pm 8.0$</td>
<td>17.2 $\pm 5.0$</td>
<td>100.0 $\pm 0.0$</td>
<td>82.8 $\pm 5.0$</td>
<td>13.0 $\pm 3.7$</td>
<td>100.0 $\pm 0.0$</td>
<td>92.6 $\pm 2.6$</td>
</tr>
<tr>
<td></td>
<td>Zero-shot+CoT</td>
<td>1.1 $\pm 1.1$</td>
<td>90.4 $\pm 1.9$</td>
<td>100.0 $\pm 0.0$</td>
<td>0.0 $\pm 0.0$</td>
<td>87.0 $\pm 4.9$</td>
<td>100.0 $\pm 0.0$</td>
<td>0.0 $\pm 0.0$</td>
<td>70.9 $\pm 5.0$</td>
<td>100.0 $\pm 0.0$</td>
</tr>
<tr>
<td></td>
<td>Few-shot+CoT</td>
<td>16.2 $\pm 5.2$</td>
<td>100.0 $\pm 0.0$</td>
<td>87.2 $\pm 5.2$</td>
<td>8.0 $\pm 3.1$</td>
<td>100.0 $\pm 0.0$</td>
<td>92.3 $\pm 3.1$</td>
<td>13.3 $\pm 3.7$</td>
<td>98.3 $\pm 1.1$</td>
<td>91.7 $\pm 3.3$</td>
</tr>
</tbody>
</table>
<p>Table 17: The effect of different sampling temperatures on rule-based graph generation with GPT-4. "Valid" denotes fractions of generated graphs that are valid under specific rules, "Unique" denotes fractions of generated graphs that are not duplicates, and "Novel" denotes fractions of generated graphs that are different from the given example graphs.</p>
<table>
<thead>
<tr>
<th>Temperature</th>
<th>Prompt</th>
<th>Cycles</th>
<th></th>
<th></th>
<th>$k$-regular</th>
<th></th>
<th></th>
<th>$k$-color</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>Valid</td>
<td>Unique</td>
<td>Novel</td>
<td>Valid</td>
<td>Unique</td>
<td>Novel</td>
<td>Valid</td>
<td>Unique</td>
<td>Novel</td>
</tr>
<tr>
<td>$t=0$</td>
<td>Zero-shot</td>
<td>100.0 $\pm 0.0$</td>
<td>90.0 $\pm 0.0$</td>
<td>100.0 $\pm 0.0$</td>
<td>53.3 $\pm 11.1$</td>
<td>98.7 $\pm 0.9$</td>
<td>100.0 $\pm 0.0$</td>
<td>33.3 $\pm 6.3$</td>
<td>98.0 $\pm 1.9$</td>
<td>100.0 $\pm 0.0$</td>
</tr>
<tr>
<td></td>
<td>Few-shot</td>
<td>76.0 $\pm 4.0$</td>
<td>89.3 $\pm 1.8$</td>
<td>100.0 $\pm 0.0$</td>
<td>90.8 $\pm 3.0$</td>
<td>96.9 $\pm 3.0$</td>
<td>100.0 $\pm 0.0$</td>
<td>72.1 $\pm 9.0$</td>
<td>90.7 $\pm 4.0$</td>
<td>100.0 $\pm 0.0$</td>
</tr>
<tr>
<td></td>
<td>Zero-shot+CoT</td>
<td>91.3 $\pm 4.6$</td>
<td>88.7 $\pm 5.6$</td>
<td>100.0 $\pm 0.0$</td>
<td>80.0 $\pm 10.1$</td>
<td>100.0 $\pm 0.0$</td>
<td>100.0 $\pm 0.0$</td>
<td>49.3 $\pm 7.9$</td>
<td>96.4 $\pm 2.8$</td>
<td>100.0 $\pm 0.0$</td>
</tr>
<tr>
<td></td>
<td>Few-shot+CoT</td>
<td>96.7 $\pm 3.2$</td>
<td>62.0 $\pm 9.8$</td>
<td>100.0 $\pm 0.0$</td>
<td>85.0 $\pm 6.9$</td>
<td>100.0 $\pm 0.0$</td>
<td>100.0 $\pm 0.0$</td>
<td>52.7 $\pm 8.9$</td>
<td>96.0 $\pm 3.9$</td>
<td>100.0 $\pm 0.0$</td>
</tr>
<tr>
<td>$t=0.2$</td>
<td>Zero-shot</td>
<td>100.0 $\pm 0.0$</td>
<td>90.0 $\pm 0.0$</td>
<td>100.0 $\pm 0.0$</td>
<td>34.7 $\pm 9.9$</td>
<td>100.0 $\pm 0.0$</td>
<td>100.0 $\pm 0.0$</td>
<td>49.3 $\pm 5.4$</td>
<td>100.0 $\pm 0.0$</td>
<td>100.0 $\pm 0.0$</td>
</tr>
<tr>
<td></td>
<td>Few-shot</td>
<td>79.3 $\pm 4.5$</td>
<td>88.7 $\pm 2.3$</td>
<td>100.0 $\pm 0.0$</td>
<td>91.5 $\pm 3.7$</td>
<td>100.0 $\pm 0.0$</td>
<td>100.0 $\pm 0.0$</td>
<td>70.0 $\pm 8.1$</td>
<td>91.3 $\pm 5.1$</td>
<td>100.0 $\pm 0.0$</td>
</tr>
<tr>
<td></td>
<td>Zero-shot+CoT</td>
<td>87.3 $\pm 4.8$</td>
<td>92.7 $\pm 1.8$</td>
<td>100.0 $\pm 0.0$</td>
<td>83.0 $\pm 9.2$</td>
<td>98.0 $\pm 1.9$</td>
<td>100.0 $\pm 0.0$</td>
<td>50.7 $\pm 9.1$</td>
<td>98.0 $\pm 1.9$</td>
<td>100.0 $\pm 0.0$</td>
</tr>
<tr>
<td></td>
<td>Few-shot+CoT</td>
<td>100.0 $\pm 0.0$</td>
<td>77.3 $\pm 8.8$</td>
<td>100.0 $\pm 0.0$</td>
<td>91.3 $\pm 2.8$</td>
<td>100.0 $\pm 0.0$</td>
<td>100.0 $\pm 0.0$</td>
<td>62.0 $\pm 7.8$</td>
<td>88.7 $\pm 6.5$</td>
<td>100.0 $\pm 0.0$</td>
</tr>
<tr>
<td>$t=0.4$</td>
<td>Zero-shot</td>
<td>100.0 $\pm 0.0$</td>
<td>90.0 $\pm 0.0$</td>
<td>100.0 $\pm 0.0$</td>
<td>56.0 $\pm 10.9$</td>
<td>99.3 $\pm 0.6$</td>
<td>100.0 $\pm 0.0$</td>
<td>52.0 $\pm 7.0$</td>
<td>99.3 $\pm 0.6$</td>
<td>100.0 $\pm 0.0$</td>
</tr>
<tr>
<td></td>
<td>Few-shot</td>
<td>70.7 $\pm 4.8$</td>
<td>84.0 $\pm 2.8$</td>
<td>100.0 $\pm 0.0$</td>
<td>89.2 $\pm 4.1$</td>
<td>95.4 $\pm 4.4$</td>
<td>100.0 $\pm 0.0$</td>
<td>56.9 $\pm 8.7$</td>
<td>86.2 $\pm 7.6$</td>
<td>100.0 $\pm 0.0$</td>
</tr>
<tr>
<td></td>
<td>Zero-shot+CoT</td>
<td>98.7 $\pm 1.3$</td>
<td>84.7 $\pm 5.2$</td>
<td>100.0 $\pm 0.0$</td>
<td>76.0 $\pm 9.1$</td>
<td>98.0 $\pm 1.3$</td>
<td>100.0 $\pm 0.0$</td>
<td>44.6 $\pm 7.0$</td>
<td>94.6 $\pm 3.7$</td>
<td>100.0 $\pm 0.0$</td>
</tr>
<tr>
<td></td>
<td>Few-shot+CoT</td>
<td>97.9 $\pm 2.1$</td>
<td>91.4 $\pm 1.7$</td>
<td>100.0 $\pm 0.0$</td>
<td>90.0 $\pm 3.8$</td>
<td>100.0 $\pm 0.0$</td>
<td>100.0 $\pm 0.0$</td>
<td>58.7 $\pm 10.6$</td>
<td>90.0 $\pm 6.1$</td>
<td>100.0 $\pm 0.0$</td>
</tr>
<tr>
<td>$t=0.6$</td>
<td>Zero-shot</td>
<td>100.0 $\pm 0.0$</td>
<td>90.7 $\pm 1.1$</td>
<td>100.0 $\pm 0.0$</td>
<td>50.7 $\pm 10.9$</td>
<td>100.0 $\pm 0.0$</td>
<td>100.0 $\pm 0.0$</td>
<td>59.3 $\pm 6.1$</td>
<td>90.7 $\pm 5.8$</td>
<td>100.0 $\pm 0.0$</td>
</tr>
<tr>
<td></td>
<td>Few-shot</td>
<td>77.3 $\pm 6.4$</td>
<td>90.7 $\pm 2.9$</td>
<td>100.0 $\pm 0.0$</td>
<td>91.7 $\pm 3.1$</td>
<td>100.0 $\pm 0.0$</td>
<td>100.0 $\pm 0.0$</td>
<td>64.0 $\pm 8.2$</td>
<td>94.7 $\pm 5.2$</td>
<td>100.0 $\pm 0.0$</td>
</tr>
<tr>
<td></td>
<td>Zero-shot+CoT</td>
<td>90.0 $\pm 5.7$</td>
<td>86.0 $\pm 5.5$</td>
<td>100.0 $\pm 0.0$</td>
<td>92.5 $\pm 3.9$</td>
<td>100.0 $\pm 0.0$</td>
<td>100.0 $\pm 0.0$</td>
<td>45.7 $\pm 6.8$</td>
<td>99.3 $\pm 0.7$</td>
<td>100.0 $\pm 0.0$</td>
</tr>
<tr>
<td></td>
<td>Few-shot+CoT</td>
<td>96.0 $\pm 3.2$</td>
<td>75.3 $\pm 8.6$</td>
<td>100.0 $\pm 0.0$</td>
<td>82.9 $\pm 6.6$</td>
<td>100.0 $\pm 0.0$</td>
<td>100.0 $\pm 0.0$</td>
<td>63.3 $\pm 8.1$</td>
<td>100.0 $\pm 0.0$</td>
<td>100.0 $\pm 0.0$</td>
</tr>
<tr>
<td>$t=0.8$</td>
<td>Zero-shot</td>
<td>91.3 $\pm 3.3$</td>
<td>88.3 $\pm 1.6$</td>
<td>100.0 $\pm 0.0$</td>
<td>64.0 $\pm 6.8$</td>
<td>100.0 $\pm 0.0$</td>
<td>100.0 $\pm 0.0$</td>
<td>50.3 $\pm 5.5$</td>
<td>98.3 $\pm 1.6$</td>
<td>100.0 $\pm 0.0$</td>
</tr>
<tr>
<td></td>
<td>Few-shot</td>
<td>85.0 $\pm 3.3$</td>
<td>92.3 $\pm 1.3$</td>
<td>100.0 $\pm 0.0$</td>
<td>86.1 $\pm 3.1$</td>
<td>100.0 $\pm 0.0$</td>
<td>100.0 $\pm 0.0$</td>
<td>62.3 $\pm 5.1$</td>
<td>98.5 $\pm 1.5$</td>
<td>100.0 $\pm 0.0$</td>
</tr>
<tr>
<td></td>
<td>Zero-shot+CoT</td>
<td>86.9 $\pm 3.6$</td>
<td>89.0 $\pm 4.2$</td>
<td>100.0 $\pm 0.0$</td>
<td>82.7 $\pm 8.6$</td>
<td>100.0 $\pm 0.0$</td>
<td>100.0 $\pm 0.0$</td>
<td>43.2 $\pm 4.9$</td>
<td>98.6 $\pm 1.0$</td>
<td>100.0 $\pm 0.0$</td>
</tr>
<tr>
<td></td>
<td>Few-shot+CoT</td>
<td>97.0 $\pm 1.9$</td>
<td>83.0 $\pm 5.1$</td>
<td>100.0 $\pm 0.0$</td>
<td>91.5 $\pm 1.6$</td>
<td>100.0 $\pm 0.0$</td>
<td>100.0 $\pm 0.0$</td>
<td>58.5 $\pm 5.9$</td>
<td>96.3 $\pm 2.6$</td>
<td>100.0 $\pm 0.0$</td>
</tr>
<tr>
<td>$t=1$</td>
<td>Zero-shot</td>
<td>94.0 $\pm 3.9$</td>
<td>88.0 $\pm 3.0$</td>
<td>100.0 $\pm 0.0$</td>
<td>60.0 $\pm 10.5$</td>
<td>100.0 $\pm 0.0$</td>
<td>100.0 $\pm 0.0$</td>
<td>40.7 $\pm 7.5$</td>
<td>100.0 $\pm 0.0$</td>
<td>100.0 $\pm 0.0$</td>
</tr>
<tr>
<td></td>
<td>Few-shot</td>
<td>64.0 $\pm 6.2$</td>
<td>96.0 $\pm 1.6$</td>
<td>100.0 $\pm 0.0$</td>
<td>86.7 $\pm 4.1$</td>
<td>100.0 $\pm 0.0$</td>
<td>100.0 $\pm 0.0$</td>
<td>49.2 $\pm 8.9$</td>
<td>95.8 $\pm 4.0$</td>
<td>100.0 $\pm 0.0$</td>
</tr>
<tr>
<td></td>
<td>Zero-shot+CoT</td>
<td>95.0 $\pm 2.2$</td>
<td>86.4 $\pm 5.9$</td>
<td>100.0 $\pm 0.0$</td>
<td>78.6 $\pm 11.1$</td>
<td>98.6 $\pm 1.3$</td>
<td>100.0 $\pm 0.0$</td>
<td>57.1 $\pm 6.9$</td>
<td>100.0 $\pm 0.0$</td>
<td>100.0 $\pm 0.0$</td>
</tr>
<tr>
<td></td>
<td>Few-shot+CoT</td>
<td>80.7 $\pm 6.6$</td>
<td>91.3 $\pm 2.5$</td>
<td>100.0 $\pm 0.0$</td>
<td>81.5 $\pm 4.2$</td>
<td>100.0 $\pm 0.0$</td>
<td>100.0 $\pm 0.0$</td>
<td>45.0 $\pm 9.4$</td>
<td>100.0 $\pm 0.0$</td>
<td>100.0 $\pm 0.0$</td>
</tr>
</tbody>
</table>
<p>Table 18: The effect of different amounts of generated graphs on rule-based graph generation with GPT-4. "Valid" denotes fractions of generated graphs that are valid under specific rules, "Unique" denotes fractions of generated graphs that are not duplicates, and "Novel" denotes fractions of generated graphs that are different from the given example graphs. Items marked with "—" denote experiments where the model is unable to generate enough graphs with the required format.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Amount</th>
<th style="text-align: center;">Prompt</th>
<th style="text-align: center;">Cycles</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">$k$-regular</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">$k$-color</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Valid</td>
<td style="text-align: center;">Unique</td>
<td style="text-align: center;">Novel</td>
<td style="text-align: center;">Valid</td>
<td style="text-align: center;">Unique</td>
<td style="text-align: center;">Novel</td>
<td style="text-align: center;">Valid</td>
<td style="text-align: center;">Unique</td>
<td style="text-align: center;">Novel</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">Zero-shot</td>
<td style="text-align: center;">$90.7 \pm 4.2$</td>
<td style="text-align: center;">$81.3 \pm 2.3$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$57.3 \pm 11.1$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$50.7 \pm 7.0$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Few-shot</td>
<td style="text-align: center;">$72.0 \pm 4.1$</td>
<td style="text-align: center;">$97.3 \pm 1.8$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$92.9 \pm 2.6$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$69.3 \pm 5.3$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Zero-shot+CoT</td>
<td style="text-align: center;">$86.7 \pm 4.5$</td>
<td style="text-align: center;">$85.3 \pm 5.2$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$85.0 \pm 7.5$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$44.0 \pm 4.7$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Few-shot+CoT</td>
<td style="text-align: center;">$92.9 \pm 3.8$</td>
<td style="text-align: center;">$82.9 \pm 7.2$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$93.3 \pm 3.1$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$44.0 \pm 9.3$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
</tr>
<tr>
<td style="text-align: center;">10</td>
<td style="text-align: center;">Zero-shot</td>
<td style="text-align: center;">$91.3 \pm 3.3$</td>
<td style="text-align: center;">$88.3 \pm 1.6$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$64.0 \pm 6.8$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$50.3 \pm 5.5$</td>
<td style="text-align: center;">$98.3 \pm 1.6$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Few-shot</td>
<td style="text-align: center;">$85.0 \pm 3.3$</td>
<td style="text-align: center;">$92.3 \pm 1.3$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$86.1 \pm 3.1$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$62.3 \pm 5.1$</td>
<td style="text-align: center;">$98.5 \pm 1.5$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Zero-shot+CoT</td>
<td style="text-align: center;">$86.9 \pm 3.6$</td>
<td style="text-align: center;">$89.0 \pm 4.2$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$82.7 \pm 8.6$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$43.2 \pm 4.9$</td>
<td style="text-align: center;">$98.6 \pm 1.0$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Few-shot+CoT</td>
<td style="text-align: center;">$97.0 \pm 1.9$</td>
<td style="text-align: center;">$83.0 \pm 5.1$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$91.5 \pm 1.6$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$58.5 \pm 5.9$</td>
<td style="text-align: center;">$96.3 \pm 2.6$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">Zero-shot</td>
<td style="text-align: center;">$98.2 \pm 1.1$</td>
<td style="text-align: center;">$92.1 \pm 1.6$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$53.7 \pm 10.1$</td>
<td style="text-align: center;">$99.0 \pm 0.5$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$56.7 \pm 5.9$</td>
<td style="text-align: center;">$92.3 \pm 4.9$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Few-shot</td>
<td style="text-align: center;">$83.7 \pm 4.7$</td>
<td style="text-align: center;">$88.0 \pm 2.9$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$93.0 \pm 1.6$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$65.0 \pm 8.3$</td>
<td style="text-align: center;">$96.8 \pm 3.1$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Zero-shot+CoT</td>
<td style="text-align: center;">$95.0 \pm 3.2$</td>
<td style="text-align: center;">$85.4 \pm 6.9$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Few-shot+CoT</td>
<td style="text-align: center;">$96.3 \pm 2.0$</td>
<td style="text-align: center;">$93.7 \pm 2.2$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$95.0 \pm 1.4$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
<td style="text-align: center;">$80.0 \pm 6.2$</td>
<td style="text-align: center;">$82.9 \pm 8.6$</td>
<td style="text-align: center;">$100.0 \pm 0.0$</td>
</tr>
</tbody>
</table>            </div>
        </div>

    </div>
</body>
</html>
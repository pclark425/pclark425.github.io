<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3023 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3023</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3023</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-75.html">extraction-schema-75</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games, with a focus on how memory is used, what types of memory are implemented, and how memory affects performance on text game tasks.</div>
                <p><strong>Paper ID:</strong> paper-260333931</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2307.15833v1.pdf" target="_blank">Dialogue Shaping: Empowering Agents through NPC Interaction</a></p>
                <p><strong>Paper Abstract:</strong> One major challenge in reinforcement learning (RL) is the large amount of steps for the RL agent needs to converge in the training process and learn the optimal policy, especially in text-based game environments where the action space is extensive. However, non-player characters (NPCs) sometimes hold some key information about the game, which can potentially help to train RL agents faster. Thus, this paper explores how to interact and converse with NPC agents to get the key information using large language models (LLMs), as well as incorporate this information to speed up RL agent's training using knowledge graphs (KGs) and Story Shaping.</p>
                <p><strong>Cost:</strong> 0.008</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3023.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3023.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games, with a focus on how memory is used, what types of memory are implemented, and how memory affects performance on text game tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT (agent & NPC)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT used as both the player-agent and non-player-character</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The authors use OpenAI's ChatGPT in two roles: as an NPC serving as a reliable in-game database of room/object/hint information, and as a conversational agent whose dialogue with the NPC is used to elicit game-winning hints and to generate a target knowledge graph.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dialogue Shaping: Empowering Agents through NPC Interaction</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Large language model (ChatGPT) accessed via conversational sessions and prompted to act either as the NPC (given full game layout, objects, and hints) or as the player-agent (given goal and dialog rules). No fine-tuning is reported; ChatGPT is used in-session with prompt engineering to constrain behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>Custom LIGHT-based text-adventure games (three games built in the LIGHT framework)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>As the NPC: answer agent questions reliably about layout, objects, and hints. As the ChatGPT agent: ask efficient grounded questions to discover prerequisites and locations of objects necessary to reach the goal (e.g., how to kill the dragon, where the sword is). Also prompted to output a textual target knowledge graph summarizing the retrieved hints.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>conversational session context (chat history) and external generated knowledge graph (target KG)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_implementation_details</strong></td>
                            <td>A persistent ChatGPT session is opened for each role so the model can use prior dialog turns as context; after dialogue ends the ChatGPT agent is prompted to convert the conversation's information into a textual target knowledge graph (edges format). The paper does not modify ChatGPT's internal architecture or provide additional external memory stores beyond the session history and the generated KG artifact.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Not quantified for ChatGPT alone; performance effect reported indirectly via downstream RL agent: target KG produced by ChatGPT leads to Story Shaping agent convergence (~10,000 training steps to reach maximum game score of 15 in Game 1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_benefits</strong></td>
                            <td>Using session context plus explicit instruction prompts enables extraction of concise, useful game knowledge and creation of a target KG that substantially speeds downstream RL learning; grounding instructions reduce hallucination in dialog.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations_or_failures</strong></td>
                            <td>ChatGPT agent is prone to hallucination and asking ungrounded questions unless explicitly constrained by prompt rules; quality of the generated target KG depends on dialog quality and prompt engineering. The paper notes examples of inefficient (ungrounded) dialogues when instructions are insufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Keep separate persistent sessions for NPC and agent roles, provide explicit instructions on how to follow-up (to avoid hallucinations), and prompt the LLM to output a structured textual knowledge graph to serve as an external artifact for downstream use.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dialogue Shaping: Empowering Agents through NPC Interaction', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3023.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3023.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games, with a focus on how memory is used, what types of memory are implemented, and how memory affects performance on text game tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KGA2C + Story Shaping (KG memory)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge-Graph-Augmented Advantage Actor-Critic with Story Shaping using internal and target knowledge graphs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An RL agent (KGA2C) that encodes an internal knowledge graph representing its current state and receives an additional shaping reward based on similarity between that internal KG and a target KG (generated from LLM dialogue) to accelerate learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dialogue Shaping: Empowering Agents through NPC Interaction</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>KGA2C (Knowledge Graph A2C) with Story Shaping</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>KGA2C is an Advantage Actor-Critic RL agent augmented with knowledge-graph guidance: inputs include an encoding of the agent's internal KG plus observation tensors (current room description, inventory, last environment feedback, last action). Story Shaping supplies an auxiliary reward based on internal KG vs target KG similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>Custom LIGHT-based text-adventure games (three games built in the LIGHT framework)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Learn action policies (text commands) to reach a sparse game reward (win the game, e.g., kill the dragon). The environment gives a single win reward (15) on success; episodes have a step limit (75); optimal paths are ~10 steps.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external structured memory: internal knowledge graph (agent state) and target knowledge graph (goal specification)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_implementation_details</strong></td>
                            <td>Internal KG: a dynamic graph of triples ⟨entity, relation, entity/attribute⟩ updated every step to reflect current room, contained objects, and possessions. Target KG: a static graph produced by ChatGPT agent after dialog (filtered to edges with 'you' as subject) representing required prerequisites/goal state. The internal KG is encoded and concatenated to the agent's observation embedding; Story Shaping computes an extra reward proportional to similarity/overlap between internal KG and target KG to guide policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Story Shaping KGA2C (using target KG from dialogue) converged to optimal policy (max score 15) in Game 1 after ~10,000 training steps (trained up to 100,000 steps; evaluated every 450 steps over 50 episodes). The agent also showed consistently higher average test scores and disjoint standard deviation ranges versus baseline across training.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Baseline KGA2C (no Story Shaping target KG) required ~90,000 training steps to learn optimal policy in Game 1 and achieved lower average test scores throughout training (max score 15 eventually but much slower convergence).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_benefits</strong></td>
                            <td>Dramatically faster convergence to optimal policy (order-of-magnitude fewer training steps in reported Game 1), higher average scores during training, and more consistent performance (lower overlap in score distributions vs baseline). The target KG reduces exploration search space by encoding prerequisites (short-term goals).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations_or_failures</strong></td>
                            <td>Performance depends on correctness and completeness of the target KG generated from dialogue; Story Shaping agent exhibited a temporary drop in average score around step 30,000 in Game 1 (noted but not deeply analyzed). No ablation isolating which KG edges or encoding choices are most important is provided. The target KG is static during training, so it cannot correct errors introduced by a faulty dialogue-derived KG.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Generate a concise target KG focused on agent-centric edges (filter to 'you' subject) to shape rewards; use Story Shaping to provide auxiliary reward signals based on KG overlap; ensure dialogue prompts are constrained to reduce hallucination and produce reliable KGs; incorporate the internal KG into the agent's observation embedding for stepwise updating.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dialogue Shaping: Empowering Agents through NPC Interaction', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Story shaping: Teaching agents human-like behavior with stories. <em>(Rating: 2)</em></li>
                <li>Keep calm and explore: Language models for action generation in text-based games. <em>(Rating: 2)</em></li>
                <li>Playing textadventure games with graph-based deep reinforcement learning. <em>(Rating: 2)</em></li>
                <li>Learning to speak and act in a fantasy text adventure game. <em>(Rating: 2)</em></li>
                <li>Graph constrained reinforcement learning for natural language action spaces. <em>(Rating: 1)</em></li>
                <li>How to avoid being eaten by a grue: Structured exploration strategies for textual worlds. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3023",
    "paper_id": "paper-260333931",
    "extraction_schema_id": "extraction-schema-75",
    "extracted_data": [
        {
            "name_short": "ChatGPT (agent & NPC)",
            "name_full": "ChatGPT used as both the player-agent and non-player-character",
            "brief_description": "The authors use OpenAI's ChatGPT in two roles: as an NPC serving as a reliable in-game database of room/object/hint information, and as a conversational agent whose dialogue with the NPC is used to elicit game-winning hints and to generate a target knowledge graph.",
            "citation_title": "Dialogue Shaping: Empowering Agents through NPC Interaction",
            "mention_or_use": "use",
            "agent_name": "ChatGPT",
            "agent_description": "Large language model (ChatGPT) accessed via conversational sessions and prompted to act either as the NPC (given full game layout, objects, and hints) or as the player-agent (given goal and dialog rules). No fine-tuning is reported; ChatGPT is used in-session with prompt engineering to constrain behavior.",
            "game_or_benchmark_name": "Custom LIGHT-based text-adventure games (three games built in the LIGHT framework)",
            "task_description": "As the NPC: answer agent questions reliably about layout, objects, and hints. As the ChatGPT agent: ask efficient grounded questions to discover prerequisites and locations of objects necessary to reach the goal (e.g., how to kill the dragon, where the sword is). Also prompted to output a textual target knowledge graph summarizing the retrieved hints.",
            "uses_memory": true,
            "memory_type": "conversational session context (chat history) and external generated knowledge graph (target KG)",
            "memory_implementation_details": "A persistent ChatGPT session is opened for each role so the model can use prior dialog turns as context; after dialogue ends the ChatGPT agent is prompted to convert the conversation's information into a textual target knowledge graph (edges format). The paper does not modify ChatGPT's internal architecture or provide additional external memory stores beyond the session history and the generated KG artifact.",
            "performance_with_memory": "Not quantified for ChatGPT alone; performance effect reported indirectly via downstream RL agent: target KG produced by ChatGPT leads to Story Shaping agent convergence (~10,000 training steps to reach maximum game score of 15 in Game 1).",
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "memory_benefits": "Using session context plus explicit instruction prompts enables extraction of concise, useful game knowledge and creation of a target KG that substantially speeds downstream RL learning; grounding instructions reduce hallucination in dialog.",
            "memory_limitations_or_failures": "ChatGPT agent is prone to hallucination and asking ungrounded questions unless explicitly constrained by prompt rules; quality of the generated target KG depends on dialog quality and prompt engineering. The paper notes examples of inefficient (ungrounded) dialogues when instructions are insufficient.",
            "best_practices_or_recommendations": "Keep separate persistent sessions for NPC and agent roles, provide explicit instructions on how to follow-up (to avoid hallucinations), and prompt the LLM to output a structured textual knowledge graph to serve as an external artifact for downstream use.",
            "uuid": "e3023.0",
            "source_info": {
                "paper_title": "Dialogue Shaping: Empowering Agents through NPC Interaction",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "KGA2C + Story Shaping (KG memory)",
            "name_full": "Knowledge-Graph-Augmented Advantage Actor-Critic with Story Shaping using internal and target knowledge graphs",
            "brief_description": "An RL agent (KGA2C) that encodes an internal knowledge graph representing its current state and receives an additional shaping reward based on similarity between that internal KG and a target KG (generated from LLM dialogue) to accelerate learning.",
            "citation_title": "Dialogue Shaping: Empowering Agents through NPC Interaction",
            "mention_or_use": "use",
            "agent_name": "KGA2C (Knowledge Graph A2C) with Story Shaping",
            "agent_description": "KGA2C is an Advantage Actor-Critic RL agent augmented with knowledge-graph guidance: inputs include an encoding of the agent's internal KG plus observation tensors (current room description, inventory, last environment feedback, last action). Story Shaping supplies an auxiliary reward based on internal KG vs target KG similarity.",
            "game_or_benchmark_name": "Custom LIGHT-based text-adventure games (three games built in the LIGHT framework)",
            "task_description": "Learn action policies (text commands) to reach a sparse game reward (win the game, e.g., kill the dragon). The environment gives a single win reward (15) on success; episodes have a step limit (75); optimal paths are ~10 steps.",
            "uses_memory": true,
            "memory_type": "external structured memory: internal knowledge graph (agent state) and target knowledge graph (goal specification)",
            "memory_implementation_details": "Internal KG: a dynamic graph of triples ⟨entity, relation, entity/attribute⟩ updated every step to reflect current room, contained objects, and possessions. Target KG: a static graph produced by ChatGPT agent after dialog (filtered to edges with 'you' as subject) representing required prerequisites/goal state. The internal KG is encoded and concatenated to the agent's observation embedding; Story Shaping computes an extra reward proportional to similarity/overlap between internal KG and target KG to guide policy learning.",
            "performance_with_memory": "Story Shaping KGA2C (using target KG from dialogue) converged to optimal policy (max score 15) in Game 1 after ~10,000 training steps (trained up to 100,000 steps; evaluated every 450 steps over 50 episodes). The agent also showed consistently higher average test scores and disjoint standard deviation ranges versus baseline across training.",
            "performance_without_memory": "Baseline KGA2C (no Story Shaping target KG) required ~90,000 training steps to learn optimal policy in Game 1 and achieved lower average test scores throughout training (max score 15 eventually but much slower convergence).",
            "has_performance_comparison": true,
            "memory_benefits": "Dramatically faster convergence to optimal policy (order-of-magnitude fewer training steps in reported Game 1), higher average scores during training, and more consistent performance (lower overlap in score distributions vs baseline). The target KG reduces exploration search space by encoding prerequisites (short-term goals).",
            "memory_limitations_or_failures": "Performance depends on correctness and completeness of the target KG generated from dialogue; Story Shaping agent exhibited a temporary drop in average score around step 30,000 in Game 1 (noted but not deeply analyzed). No ablation isolating which KG edges or encoding choices are most important is provided. The target KG is static during training, so it cannot correct errors introduced by a faulty dialogue-derived KG.",
            "best_practices_or_recommendations": "Generate a concise target KG focused on agent-centric edges (filter to 'you' subject) to shape rewards; use Story Shaping to provide auxiliary reward signals based on KG overlap; ensure dialogue prompts are constrained to reduce hallucination and produce reliable KGs; incorporate the internal KG into the agent's observation embedding for stepwise updating.",
            "uuid": "e3023.1",
            "source_info": {
                "paper_title": "Dialogue Shaping: Empowering Agents through NPC Interaction",
                "publication_date_yy_mm": "2023-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Story shaping: Teaching agents human-like behavior with stories.",
            "rating": 2,
            "sanitized_title": "story_shaping_teaching_agents_humanlike_behavior_with_stories"
        },
        {
            "paper_title": "Keep calm and explore: Language models for action generation in text-based games.",
            "rating": 2,
            "sanitized_title": "keep_calm_and_explore_language_models_for_action_generation_in_textbased_games"
        },
        {
            "paper_title": "Playing textadventure games with graph-based deep reinforcement learning.",
            "rating": 2,
            "sanitized_title": "playing_textadventure_games_with_graphbased_deep_reinforcement_learning"
        },
        {
            "paper_title": "Learning to speak and act in a fantasy text adventure game.",
            "rating": 2,
            "sanitized_title": "learning_to_speak_and_act_in_a_fantasy_text_adventure_game"
        },
        {
            "paper_title": "Graph constrained reinforcement learning for natural language action spaces.",
            "rating": 1,
            "sanitized_title": "graph_constrained_reinforcement_learning_for_natural_language_action_spaces"
        },
        {
            "paper_title": "How to avoid being eaten by a grue: Structured exploration strategies for textual worlds.",
            "rating": 1,
            "sanitized_title": "how_to_avoid_being_eaten_by_a_grue_structured_exploration_strategies_for_textual_worlds"
        }
    ],
    "cost": 0.0081215,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Dialogue Shaping: Empowering Agents through NPC Interaction
28 Jul 2023</p>
<p>Wei Zhou 
Georgia Institute of Technology
30332AtlantaGAUSA</p>
<p>Xiangyu Peng 
Georgia Institute of Technology
30332AtlantaGAUSA</p>
<p>Mark Riedl 
Georgia Institute of Technology
30332AtlantaGAUSA</p>
<p>Dialogue Shaping: Empowering Agents through NPC Interaction
1613-007328 Jul 202319CDFB73B9901D6949A0BC22FD9FC008arXiv:2307.15833v1[cs.CL]Large Language ModelChatGPTReinforcement LearningKnowledge GraphText adventure game
One major challenge in reinforcement learning (RL) is the large amount of steps for the RL agent needs to converge in the training process and learn the optimal policy, especially in text-based game environments where the action space is extensive.However, non-player characters (NPCs) sometimes hold some key information about the game, which can potentially help to train RL agents faster.Thus, this paper explores how to interact and converse with NPC agents to get the key information using large language models (LLMs), as well as incorporate this information to speed up RL agent's training using knowledge graphs (KGs) and Story Shaping.</p>
<p>Introduction</p>
<p>Reinforcement learning (RL) has demonstrated remarkable effectiveness in solving intricate decision-making tasks, but its trial-and-error approach often leads to slow convergence to the optimal policy.In text-adventure games, NPCs possess crucial information that could spare the agent from extensive trial-and-error.Utilizing this prior knowledge could significantly reduce the agent's policy search space, making it more efficient by breaking down complex tasks into smaller, focused objectives.For instance, knowing that "killing the dragon" requires a sword allows the agent to concentrate on finding the sword directly, rather than wasting steps exploring how to defeat the dragon.</p>
<p>Large Language Models (LLMs) are incredibly capable of conversational tasks and are highly configurable using prompting techniques.Thus, we chose to use them as the dialogue module responsible for talking to the NPC.Meanwhile, they are not as efficient as RL agent in terms of searching for the optimal chain of actions.Therefore, we chose to keep the RL agent as the main component responsible for searching for the optimal policy while speeding its search using dialogue module that is comprised of LLMs.</p>
<p>The RL agent acts as an action module and the LLMs act as a dialogue module.Yet, we still need to find a way to bridge these two modules, i.e. incorporating the information that the dialogue module retrieves into the action module.For this purpose, we turn to the technique</p>
<p>AIIDE-23: The 19th AAAI Conference on Artificial Intelligence and</p>
<p>Interactive Digital Entertainment, October 08-12, 2023, Salt Lake City, UT wzhou322@gatech.edu(W.Zhou); xpeng62@gatech.edu(X.Peng); riedl@cc.gatech.edu(M.R. ) of Story Shaping [1], which is able to guide the action module to imitate the optimal trajectory.</p>
<p>In this paper, we propose Dialogue Shaping, a framework that is able to extract useful information through conversation with NPCs, and then convert the information into knowledge graphs which are then used to speed up RL agent's convergence to optimal policy by using the Story Shaping technique [1].</p>
<p>Background and Related Work</p>
<p>Reinforcement Learning in Text Games Text games involve turn-based interactions where players read descriptions of the game's environment in natural language and respond with short text-based actions.These games can be described using partially-observable Markov Decision Processes, denoted as ⟨, , , , Ω, , ⟩, representing possible states, transition probabilities, vocabulary for commands, observation probabilities, reward function, and discount factor.The RL agent's goal is to learn a policy () →  to maximize expected future rewards.</p>
<p>Large Language Models in RL The power of Large Language Models (LLMs) has gained significant attention in recent years due to their advanced ability to adapt to numerous downstream tasks.ChatGPT, an LLM chatbot created by OpenAI, offers diverse interaction modes, and users can engage with it by providing prompts for acting as the NPC and the agent in text games [2].Recent studies also explored the integration of large language models into reinforcement learning frameworks to enhance the capabilities of agents.Contextual Action Language Model (CALM) [3] used LLM to generate a set of concise candidate actions at each step of the game for the reinforcement learning agent, thereby greatly reducing the</p>
<p>Preliminaries</p>
<p>Text Games</p>
<p>We create three text games in the LIGHT environment [4], which is a large-scale crowdsourced text adventure game framework, in which agents can both perceive, emote and act.The LIGHT environment also provides a database of rooms, characters, and objects, from which we can build our custom games.The visualization of one of the games we created and used in the experiments can be found in Figure 1.</p>
<p>Knowledge Graph</p>
<p>A knowledge graph consists of triples ⟨, , ⟩, capturing information about entities, their attributes, and relationships.Our method uses two types of KGs: internal KG and target KG.</p>
<p>During RL exploration in the text game, the internal KG represents the agent's current state, including the room it's in and the objects it possesses [5,6,7,8,9,10].We update this KG at each step based on changes in the game environment (e.g., moving to a new room) or the agent's actions (e.g., acquiring objects).</p>
<p>The target KG describes the final state the agent must achieve to win the game, specifying the last room the agent should be in and the required objects.This KG is generated before training and stays unchanged.</p>
<p>KGA2C agent</p>
<p>KGA2C [7] is used for our game-playing agent for both baseline and Story Shaping [1].It is an RL agent that combines both Advantage Actor Critic methods [11] and KG guidance to enhance its learning and decision-making capabilities.The input embedding to the KGA2C agent is a concatenation of encoding of the agent's current internal KG and four observation tensors, including the description of the current room the agent is located in, the agent's current inventory, feedback of the environment from the agent's last action, and agent's last action.</p>
<p>Story Shaping</p>
<p>Story Shaping, proposed by Peng et al., is a technique that helps the RL agent infers tacit knowledge on how to accomplish a task.For each training step in the game, Story Shaping gives the RL agent an extra reward signal (in addition to the game environment's reward signal) based on the similarity between agent's current internal KG and target KG, and therefore encourage the agent to perform actions that will make its internal KG similar to the target KG.The target KG in this paper is generated by prompting the ChatGPT agent and it represents a summary of the hints the ChatGPT agent learns through talking to the ChatGPT NPC.</p>
<p>Information Retrieval from Dialogue</p>
<p>In order to retrieve correct and important information about the game from NPC, it is expected to know the game setting and it should be able to provide the correct response every time it is asked by the agent.In other words, the NPC should act as a "database" of the game.</p>
<p>NPC Prompting</p>
<p>We open one session of ChatGPT, as shown in Figure 2, and prompted it to be the non-player character.The NPC is provided with general information about the game, including the layout and the available objects, as well as the hints to win the game.One example of hints is getting a sword in the Artillery room is a prerequisite to kill the dragon.</p>
<p>Agent Prompting</p>
<p>ChatGPT is prompted to be the player agent in the game.The ChatGPT agent is provided with its goal in the game (e.g.kill the dragon) and general instructions on how to converse with the NPC (e.g.ask questions based on previous given answers).We did not reveal any game details in the prompts for the ChatGPT agent, because it is expected to gain those information by asking questions to the ChatGPT NPC.</p>
<p>Story Shaping from Dialogue</p>
<p>After the dialogue with NPC, we train a KGA2C agent to play the game.In order to incorporate the information learned by the ChatGPT agent during conversation with NPC into KGA2C agent's training, we prompt ChatGPT agent to generate a knowledge graph and use it as a target knowledge graph for the Story Shaping KGA2C agent.</p>
<p>The pipeline for our method is shown in Figure 3.</p>
<p>Experiments</p>
<p>We conducted experiments to test our assumption that the dialogue is able to accelerate KGA2C agent's convergence by generating the target knowledge graph.</p>
<p>ChatGPT Prompting for Agent vs NPC Dialogue</p>
<p>Table 1 An example prompt used to trigger ChatGPT to act as NPC in game 1. Goal and prerequisites are different for each game.</p>
<p>You are an NPC in a text-adventure game.You and the agent are both in the game.For each step, waits for the agent to ask questions, then you should provide a correct answer based on the information about the game given as follow:
Layout: [room1]-east-[room2], [room2]-east-[room3], etc. (A-east-B means A is to the east of B)
Goal and prerequisite: A dragon is in the dungeon.</p>
<p>The only way to kill the dragon is to use a sword and there is no other way.</p>
<p>Object information:
[object1], [object2] is in [room1]. [object3], [object4], [object5] is in [room2]. [room3] has no objects...
We open one session of ChatGPT and prompt it to be the NPC of the game.Table 1 shows the NPC prompt</p>
<p>Table 2</p>
<p>An example prompt used to trigger ChatGPT to act as the agent in game 1. Goal is different for each game.</p>
<p>You are an agent in a text-adventure game.You and the NPC are both in the game.Your goal is to kill the dragon.For each step, you should ask questions to the NPC in order to get the information on how to kill the dragon.</p>
<p>Ask a new set of questions based on the current observation and answers given to the previous set of questions according to the following rule: 1. ask similar and follow-up questions to previous questions that have a "yes" answer.2. Avoid asking similar and follow-up questions to previous questions that have a "no" answer.</p>
<p>Table 3</p>
<p>Examples of an efficient dialogue and an inefficient dialogue between ChatGPT agent and ChatGPT NPC.</p>
<p>Desired Dialogue</p>
<p>Agent: Do I need an object to kill the dragon?Can the dragon be lured out?Can the dragon be defeated without using a weapon?NPC: Yes.No. No. Agent: What object I should get to kill the dragon?NPC: A sword.Agent: Where can I find the sword?NPC: The Artillery room.Dialogue ends.Enough information is gathered.</p>
<p>Undesired Dialogue</p>
<p>Agent: Do I need an object to kill the dragon?Can the dragon be lured out?Can the dragon be defeated without using a weapon?NPC: Yes.No. No. Agent: Can the dragon be trapped or immobilized temporarily?Is there any way to communicate with the dragon or understand its intentions?NPC: No. No. Agent: Is there any information about the dragon's origin or history?Can the dragon be reasoned with or persuaded to leave peacefully?NPC: No. No.</p>
<p>• • •</p>
<p>for one game.We also open another separate session of ChatGPT and prompt it to be the agent of the game with a goal in mind.Table 2 shows the agent prompt for one game.</p>
<p>Then, the dialogue begins as the agent comes up with a set of questions and the NPC provides answers to them back and forth.ChatGPT NPC proves to be a reliable game database, correctly responding to queries about room and object locations.Moreover, when the ChatGPT agent makes ungrounded assumptions about the game (like "Does the barkeeper possess any knowledge about dragon's weaknesses" while there is no barkeeper) in its questions, the ChatGPT NPC is able to recognize (finding</p>
<p>Table 4</p>
<p>The prompt used to trigger the ChatGPT agent to generate the target knowledge graph, which will later be used in the training of KGA2C agent with Story Shaping.</p>
<p>Output a textual knowledge graph that contains the game information required to reach the goal.Output it in the format of edges (entity1 −−direction or verbs→ entity2).For example, you−−have→rugs, town center −−west→ the bar out that the game does not have a barkeeper) and negate them.</p>
<p>In evaluating the performance of ChatGPT agent, we aim to minimize the number of exchanges with the Chat-GPT NPC while retrieving hints on winning the game.We found out that ChatGPT agent is much more likely to hallucinate by coming up with ungrounded questions without explicit instructions on how to ask the optimal questions in our prompt.As shown in the desired dialogue in Table 3, when we include those explicit instructions in the prompt, it is able to ground its inquiries.Otherwise, it will fail to follow up on the previous questions that have a "yes" answer and endlessly ask ungrounded questions as shown in the undesired dialogue in Table 3.</p>
<p>KGA2C Agent Training with Dialogue Shaping</p>
<p>After the dialogue ends and the ChatGPT agent retrieved information on how to reach the goal, we prompt it to convert that information into a textual knowledge graph representation as shown in Table 4.We then filter the edges in the knowledge graph by only including ones that have "you" as a subject, because we are only interested in what actions the agent has to perform to reach to goal.Finally, we use this filtered knowledge graph as the target knowledge graph to "shape" the Story Shaping KGA2C agent behaviors.We generate each game using the LIGHT framework [4].We design each game such that the RL agent will only get one reward signal of 15 when it wins the game.For every game, the KGA2C agent is trained for 100,000 steps.After every 450 steps, the agent is evaluated for 50 episodes with 10 random seeds.We gather metrics like average and standard deviation of the test scores achieved for those 50 episodes, like in Figure 4.The maximum step limit for a single episode is 75 steps, while the optimal path for all games usually takes around 10 steps.</p>
<p>We trained the baseline KGA2C agent and the one with Story Shaping assistance for each game.Baseline KGA2C agent only receives reward signals that are built into the game mechanism (i.e.reaching the final goal), whereas the Story Shaping KGA2C agent receives additional reward signals when its internal knowledge graph overlaps with the target knowledge graph which is generated by the dialogue module (i.e.complete the prerequisite of the goal).</p>
<p>Results</p>
<p>Figure 4 showed the average test score and its standard deviation of the baseline KGA2C agent and Story Shaping KGA2C agent equipped with target knowledge graph generated from the dialogue during training for game 1.The Story Shaping KGA2C agent outperformed the baseline in all games.In all games, the Story Shaping agent converged to the optimal policy (gaining maximum score of 15) much faster than the baseline.In game 1, the Story Shaping KGA2C agent converged to the optimal policy after trained for around 10000 steps despite a temporary drop in average scores around step 30000, while the baseline agent took around 90000 training steps to learn the optimal policy, according to figure 4.Moreover, almost at all the training steps, the standard deviation score range of the Story Shaping agent is disjoint from that of the baseline, meaning that the Story Shaping agent can consistently achieve higher score than the baseline.</p>
<p>Conclusions</p>
<p>Through evaluation of our technique across a range of text games, we have shown that the dialogue module is able to extract key game information which might take a traditional action based RL agent tens of thousands of steps to learn.Moreover, we show that the dialogue module is able to pass along those key information and guide the action agent through knowledge graph and Story Shaping technique effectively and reliably.Thus, we have proven the substantial potential of the dialogue component to greatly speed up RL agent's convergence to the optimal policy.Future work might further exploit this potential by exploring approaches like few-shot prompting or finetuning LLMs to more effectively retrieve useful information from the NPC.</p>
<p>Figure 1 :
1
Figure 1: LIGHT Game Map for Game 1</p>
<p>Figure 2 :
2
Figure 2: Screenshot of the starting conversation between the user and the ChatGPT NPC.The question asked is generated by the ChatGPT agent and copied by the user.</p>
<p>Figure 3 :
3
Figure 3: Dialogue Shaping pipeline containing ChatGPT NPC dialogue with ChatGPT agent and target knowledge graph extraction.This target knowledge graph is used by the KGA2C agent with Story Shaping assistance to get additional reward signals.</p>
<p>Figure 4 :
4
Figure 4: Average and standard deviation of the test scores throughout 100000 training steps for Game 1.The standard deviation is represented as the shaded area around the lines.</p>
<p>X Peng, C Cui, W Zhou, R Jia, M Riedl, arXiv:2301.10107Story shaping: Teaching agents human-like behavior with stories. 2023arXiv preprint</p>
<p>Chatgpt Openai, A large-scale open-domain chatbot. 2022</p>
<p>Keep calm and explore: Language models for action generation in text-based games. S Yao, R Rao, M Hausknecht, K Narasimhan, arXiv:2010.029032020</p>
<p>J Urbanek, A Fan, S Karamcheti, S Jain, S Humeau, E Dinan, T Rocktäschel, D Kiela, A Szlam, J Weston, arXiv:1903.03094Learning to speak and act in a fantasy text adventure game. 2019arXiv preprint</p>
<p>Playing textadventure games with graph-based deep reinforcement learning. P Ammanabrolu, M O Riedl, arXiv:1812.016282018arXiv preprint</p>
<p>Bringing stories alive: Generating interactive fiction worlds. P Ammanabrolu, W Cheung, D Tu, W Broniec, M Riedl, Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment. the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment202016</p>
<p>Graph constrained reinforcement learning for natural language action spaces. P Ammanabrolu, M Hausknecht, arXiv:2001.088372020arXiv preprint</p>
<p>P Ammanabrolu, E Tien, M Hausknecht, M O Riedl, arXiv:2006.07409How to avoid being eaten by a grue: Structured exploration strategies for textual worlds. 2020arXiv preprint</p>
<p>Deep reinforcement learning with stacked hierarchical attention for text-based games. Y Xu, M Fang, L Chen, Y Du, J T Zhou, C Zhang, Advances in Neural Information Processing Systems. 332020</p>
<p>Inherently explainable reinforcement learning in natural language. X Peng, M Riedl, P Ammanabrolu, Advances in Neural Information Processing Systems. 352022</p>
<p>Asynchronous methods for deep reinforcement learning. V Mnih, A P Badia, M Mirza, A Graves, T Lillicrap, T Harley, D Silver, K Kavukcuoglu, International conference on machine learning. PMLR2016</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-304 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-304</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-304</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-14.html">extraction-schema-14</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <p><strong>Paper ID:</strong> paper-6f6e2e0311589a9af045f6acd00b7dee6d19fce4</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/6f6e2e0311589a9af045f6acd00b7dee6d19fce4" target="_blank">The Impact of Positional Encoding on Length Generalization in Transformers</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> This work suggests that explicit position embeddings are not essential for decoder-only Transformers to generalize well to longer sequences, and NoPE outperforms other explicit positional encoding methods while requiring no additional computation.</p>
                <p><strong>Paper Abstract:</strong> Length generalization, the ability to generalize from small training context sizes to larger ones, is a critical challenge in the development of Transformer-based language models. Positional encoding (PE) has been identified as a major factor influencing length generalization, but the exact impact of different PE schemes on extrapolation in downstream tasks remains unclear. In this paper, we conduct a systematic empirical study comparing the length generalization performance of decoder-only Transformers with five different position encoding approaches including Absolute Position Embedding (APE), T5's Relative PE, ALiBi, and Rotary, in addition to Transformers without positional encoding (NoPE). Our evaluation encompasses a battery of reasoning and mathematical tasks. Our findings reveal that the most commonly used positional encoding methods, such as ALiBi, Rotary, and APE, are not well suited for length generalization in downstream tasks. More importantly, NoPE outperforms other explicit positional encoding methods while requiring no additional computation. We theoretically demonstrate that NoPE can represent both absolute and relative PEs, but when trained with SGD, it mostly resembles T5's relative PE attention patterns. Finally, we find that scratchpad is not always helpful to solve length generalization and its format highly impacts the model's performance. Overall, our work suggests that explicit position embeddings are not essential for decoder-only Transformers to generalize well to longer sequences.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e304.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e304.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Addition (base experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Addition task (digitwise addition) evaluated in decoder-only Transformer experiments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Digitwise addition (presented as sequence-to-sequence problems) was used to evaluate length generalization across positional encodings; models were trained from scratch on bounded-length examples and tested on longer instances (extrapolation).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>decoder-only Transformer (task-trained)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>107M</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>decoder-only transformer (causal attention)</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>multi-digit addition (digitwise addition presented as sequences; examples show 5-digit + 4-digit producing 5-digit result)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>examples include multi-digit additions (e.g., 5 3 7 2 6 + 1 9 1 7 producing 5 5 6 4 3); training lengths typically up to L=20 tokens (digits/positions); exact numeric ranges not further enumerated</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>trained-from-scratch autoregressive LM on synthetic addition dataset; evaluated with and without scratchpad (chain-of-thought / intermediate computation logging); standard autoregressive decoding</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Qualitative: all PEs achieve perfect or near-perfect accuracy on IID (seen-length) test examples; on length extrapolation NoPE and T5's Relative Bias outperform other positional encodings (ALiBi middle, APE and Rotary poor). Exact numeric accuracies are not reported in the main text, only aggregate rankings and per-length curves (figures referenced).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Attention and representational findings: NoPE models learn position-sensitive behaviors similar to T5's Relative PE (empirically shown via attention-distribution similarity). Scratchpad can help for addition in some formats, but its benefit depends strongly on format. NoPE and T5 encourage both short- and long-range attention (bimodal); ALiBi exhibits strong recency (short-range) bias; Rotary and APE show no strong positional attention preference. The paper also presents constructive proofs that a NoPE decoder-only Transformer can recover absolute positions in the first layer (using BOS anchoring and averaged attention) and implement relative positional terms in later layers.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Trained with L=20 and evaluated up to 2L (test lengths sampled from U(1,2L)). Behaviour: strong performance at IID; extrapolation degrades for many explicit PEs (APE/Rotary), while NoPE and T5-relative generalize better. No numeric scaling curves provided in text for addition beyond the qualitative plots.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>APE and Rotary show poor length generalization on addition; ALiBi tends to focus on recent tokens which can limit long-range extrapolation; scratchpad is not universally helpful and can hurt if format is suboptimal.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared positional encoding variants: NoPE, APE (sinusoidal), T5's Relative Bias, ALiBi, Rotary; also compared with/without scratchpad (several scratchpad formats).</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>For digitwise addition in length-generalization tests, decoder-only Transformers without explicit positional encoding (NoPE) or with T5-style relative bias generalize best; scratchpad helps only in some formats and does not eliminate the effect of positional encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Impact of Positional Encoding on Length Generalization in Transformers', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e304.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e304.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Other mathematical tasks</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Summation, Polynomial Evaluation, Sorting, Parity, and related algorithmic tasks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A battery of mathematical and algorithmic tasks (summation modulo, polynomial evaluation, sorting, parity, LEGO, etc.) were used to probe length generalization across positional encodings in decoder-only Transformers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>decoder-only Transformer (task-trained)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>107M</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>decoder-only transformer (causal attention)</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>summation (modular summation), polynomial evaluation (evaluate polynomial at a value modulo), sorting (numbers), parity (counting 1s), plus small-program semantics (LEGO)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>task-specific generative processes; typical instance lengths sampled up to L=20 during training and up to 2L at test; specific digit ranges not enumerated in main text</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>trained-from-scratch on synthetic datasets per task; some tasks also evaluated with scratchpad formats (chain-of-thought style intermediate steps)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Qualitative: near-perfect train/IID performance for most PEs; on extrapolation NoPE and T5's Relative Bias generally outperform other PEs across tasks. ALiBi is intermediate; APE and Rotary typically underperform on extrapolation. Detailed per-task numeric breakdowns are referenced in appendices (not reproduced in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Same mechanistic patterns as addition: NoPE tends to learn relative-like encodings; attention analyses show NoPE and T5 attend both near and far tokens (bimodal), ALiBi focuses on recent tokens, Rotary/APE show flatter/uniform attention. Theoretical constructions show NoPE can internally encode absolute positions (first layer) and implement relative biases (later layers), enabling algorithmic computation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Extrapolation tested by training up to L=20 and testing up to 2L; explicit PEs often fail to extrapolate as length grows; NoPE and T5 relatively robust. No precise numerical scaling curves in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Failures concentrated on explicit PEs when extrapolating beyond training lengths; methods with recency bias (ALiBi) can overly prefer short-range dependencies and fail on tasks requiring long-range reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Comparisons across PEs (NoPE, APE, Rotary, ALiBi, T5 Relative) and scratchpad vs no-scratchpad.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Across a range of arithmetic and algorithmic tasks, NoPE and T5's relative bias generalize best to longer sequences; commonly used PEs (APE, Rotary, ALiBi) often underperform on length extrapolation in downstream mathematical tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Impact of Positional Encoding on Length Generalization in Transformers', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e304.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e304.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NoPE mechanistic theory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mechanistic explanation and theoretical construction for how NoPE encodes positions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper proves that a decoder-only Transformer without explicit positional encoding can (1) recover absolute positions in its first layer (via a constructed attention head using BOS anchoring and uniform keys) and (2) implement relative positional bias in subsequent layers (attention dot product decomposes into content + function of distance). Empirically, NoPEs trained with SGD tend to learn relative-like encodings similar to T5's Relative PE.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>NoPE decoder-only Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>107M (main experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>decoder-only transformer (causal attention) without explicit positional encodings</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>general arithmetic/algorithmic tasks (addition, summation, polynomial eval, sorting, parity, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>training setup typically uses L=20; theoretical results apply generally (constructive proofs not tied to a particular numeric range)</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>theoretical constructive proofs plus empirical attention-distribution comparisons (Jensen-Shannon divergence between heads), and layerwise attention analysis</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Theorem 1 constructs a first-layer head that, using a BOS token and uniform-key construction, produces an attention output proportional to 1/t (where t is the current position length) which a feedforward MLP can map to explicit absolute positions — establishing that absolute positions are representable without explicit PE. Theorem 2 shows that if absolute positions persist in hidden states, later-layer attention dot-products can be parameterized as content-dependent term plus a function of relative distance: <q_n,k_m> = f_cnt(q,k) + f_rel(n-m). Empirically, attention-pattern similarity measures show NoPE trained with SGD tends to produce attention maps closest to T5 Relative PE, indicating that learned NoPE behavior in practice is relative-like. Attention-distribution analyses on scratchpad tasks reveal NoPE/T5 produce bimodal short+long-range attention patterns useful for arithmetic procedures that require both local stepwise processing (scratchpad steps) and access to the original input.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>The theoretical constructions are compared conceptually to explicit PEs (APE, ALiBi, Rotary, T5 Relative); empirical similarity measured between NoPE and each explicit PE using JSD of attention distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>No explicit positional encoding is necessary for a decoder-only Transformer to represent absolute or relative positions; when trained, NoPEs empirically adopt relative-like encodings (similar to T5 Relative Bias), which helps length generalization on arithmetic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Impact of Positional Encoding on Length Generalization in Transformers', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e304.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e304.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Scratchpad / CoT effects</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scratchpad (chain-of-thought) interventions and their impact on arithmetic length generalization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The study investigates whether emitting intermediate computation steps (scratchpad/CoT) makes positional encoding choice irrelevant and how scratchpad format affects arithmetic performance and attention.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>decoder-only Transformer (task-trained)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>107M</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>decoder-only transformer (causal attention)</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition and other mathematical/reasoning tasks when using scratchpad formatting</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>scratchpad experiments use reduced length threshold L=8 to avoid OOM due to long generated sequences; specific numeric ranges not enumerated</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>scratchpad / chain-of-thought style outputs with multiple scratchpad formats (including a 'full Format' with step input, computation, output, variables, and remaining input); models trained from scratch on datasets with scratchpad outputs</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Qualitative: scratchpad helps in some tasks and formats (e.g., the 'full Format' on addition is reported as helpful across PEs), but is not universally beneficial; effectiveness is highly task- and format-dependent. No single PE + scratchpad combination universally solves length-generalization failures.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Attention analysis shows which parts of scratchpad models attend to: NoPE and T5 Relative often exhibit a bimodal attention distribution (attending both the current scratchpad step and distant tokens such as original input), ALiBi strongly prefers short-range (recent) tokens, while Rotary and APE show more uniform attention. Redundant repeated info in scratchpad (e.g., remaining input) may or may not be leveraged depending on PE and format.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Scratchpad can be ineffective or harmful if the format is suboptimal; long scratchpad sequences require reduced training length thresholds and can exacerbate length-generalization issues if positional encoding is mismatched.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared models trained with and without scratchpad across different PEs and scratchpad formats.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Scratchpad/CoT does not eliminate the dependence on positional encoding for length generalization; its benefit is highly format-dependent and interacts with the model's positional encoding (NoPE and T5 benefit most via attention patterns that mix short- and long-range dependencies).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Impact of Positional Encoding on Length Generalization in Transformers', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e304.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e304.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Scaling probe (1B)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Preliminary 1B-parameter scale experiments probing perplexity-based length generalization across PEs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Post-submission experiments trained three ~1B-parameter models (ALiBi, Rotary, NoPE) on a subset of StarCoder training data to inspect scaling effects on context-length extrapolation (reported in terms of language modeling perplexity).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>decoder-only Transformer (pretrained subset)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈1B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>decoder-only transformer (causal attention)</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>not task-specific; this is a language-modeling perplexity probe relevant to context-length extrapolation (implications for downstream arithmetic tasks discussed qualitatively)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>pretraining on StarCoder subset with context length 1024; evaluation of perplexity at longer contexts to probe length generalization</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Perplexity (LM) findings: at IID (within training context) all PEs have similar perplexity; at longer contexts Rotary fails to generalize (perplexity explodes), while NoPE and ALiBi generalize similarly up to almost twice training context; beyond that ALiBi is more stable than NoPE. Downstream fine-tuning on the small synthetic tasks showed identical performance across PEs when the pretraining context is much larger than task instance lengths.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Scaling behavior indicates that some PEs (Rotary) can break catastrophically on longer contexts at large scale (perplexity explosion), while NoPE and ALiBi are more robust up to moderate extrapolation; this complements downstream-task findings and highlights that perplexity and downstream extrapolation can diverge.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>At 1B scale: similar IID perplexities; Rotary fails at longer contexts; NoPE and ALiBi generalize up to ~2x training context; beyond that ALiBi more stable.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Rotary shows catastrophic perplexity growth at longer contexts; NoPE becomes less stable than ALiBi at very large extrapolations in LM perplexity (though downstream-task behavior may differ).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>ALiBi vs Rotary vs NoPE at ~1B parameters, evaluated by LM perplexity across increasing context windows.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>At larger scale (1B), positional encoding choice still matters: Rotary may fail catastrophically on longer contexts, while NoPE and ALiBi show more moderate extrapolation up to ~2x the trained context; LM perplexity behavior does not always match downstream arithmetic-task extrapolation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Impact of Positional Encoding on Length Generalization in Transformers', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Show your work: Scratchpads for intermediate computation with language models. <em>(Rating: 2)</em></li>
                <li>Analyzing mathematical reasoning abilities of neural models. <em>(Rating: 2)</em></li>
                <li>Train short, test long: Attention with linear biases enables input length extrapolation. <em>(Rating: 2)</em></li>
                <li>Transformer language models without positional encodings still learn positional information. <em>(Rating: 2)</em></li>
                <li>Roformer: Enhanced transformer with rotary position embedding. <em>(Rating: 2)</em></li>
                <li>Exploring length generalization in large language models. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-304",
    "paper_id": "paper-6f6e2e0311589a9af045f6acd00b7dee6d19fce4",
    "extraction_schema_id": "extraction-schema-14",
    "extracted_data": [
        {
            "name_short": "Addition (base experiments)",
            "name_full": "Addition task (digitwise addition) evaluated in decoder-only Transformer experiments",
            "brief_description": "Digitwise addition (presented as sequence-to-sequence problems) was used to evaluate length generalization across positional encodings; models were trained from scratch on bounded-length examples and tested on longer instances (extrapolation).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "decoder-only Transformer (task-trained)",
            "model_size": "107M",
            "model_architecture": "decoder-only transformer (causal attention)",
            "arithmetic_operation_type": "multi-digit addition (digitwise addition presented as sequences; examples show 5-digit + 4-digit producing 5-digit result)",
            "number_range_or_complexity": "examples include multi-digit additions (e.g., 5 3 7 2 6 + 1 9 1 7 producing 5 5 6 4 3); training lengths typically up to L=20 tokens (digits/positions); exact numeric ranges not further enumerated",
            "method_or_intervention": "trained-from-scratch autoregressive LM on synthetic addition dataset; evaluated with and without scratchpad (chain-of-thought / intermediate computation logging); standard autoregressive decoding",
            "performance_result": "Qualitative: all PEs achieve perfect or near-perfect accuracy on IID (seen-length) test examples; on length extrapolation NoPE and T5's Relative Bias outperform other positional encodings (ALiBi middle, APE and Rotary poor). Exact numeric accuracies are not reported in the main text, only aggregate rankings and per-length curves (figures referenced).",
            "mechanistic_insight": "Attention and representational findings: NoPE models learn position-sensitive behaviors similar to T5's Relative PE (empirically shown via attention-distribution similarity). Scratchpad can help for addition in some formats, but its benefit depends strongly on format. NoPE and T5 encourage both short- and long-range attention (bimodal); ALiBi exhibits strong recency (short-range) bias; Rotary and APE show no strong positional attention preference. The paper also presents constructive proofs that a NoPE decoder-only Transformer can recover absolute positions in the first layer (using BOS anchoring and averaged attention) and implement relative positional terms in later layers.",
            "performance_scaling": "Trained with L=20 and evaluated up to 2L (test lengths sampled from U(1,2L)). Behaviour: strong performance at IID; extrapolation degrades for many explicit PEs (APE/Rotary), while NoPE and T5-relative generalize better. No numeric scaling curves provided in text for addition beyond the qualitative plots.",
            "failure_modes": "APE and Rotary show poor length generalization on addition; ALiBi tends to focus on recent tokens which can limit long-range extrapolation; scratchpad is not universally helpful and can hurt if format is suboptimal.",
            "comparison_baseline": "Compared positional encoding variants: NoPE, APE (sinusoidal), T5's Relative Bias, ALiBi, Rotary; also compared with/without scratchpad (several scratchpad formats).",
            "key_finding": "For digitwise addition in length-generalization tests, decoder-only Transformers without explicit positional encoding (NoPE) or with T5-style relative bias generalize best; scratchpad helps only in some formats and does not eliminate the effect of positional encoding.",
            "uuid": "e304.0",
            "source_info": {
                "paper_title": "The Impact of Positional Encoding on Length Generalization in Transformers",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Other mathematical tasks",
            "name_full": "Summation, Polynomial Evaluation, Sorting, Parity, and related algorithmic tasks",
            "brief_description": "A battery of mathematical and algorithmic tasks (summation modulo, polynomial evaluation, sorting, parity, LEGO, etc.) were used to probe length generalization across positional encodings in decoder-only Transformers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "decoder-only Transformer (task-trained)",
            "model_size": "107M",
            "model_architecture": "decoder-only transformer (causal attention)",
            "arithmetic_operation_type": "summation (modular summation), polynomial evaluation (evaluate polynomial at a value modulo), sorting (numbers), parity (counting 1s), plus small-program semantics (LEGO)",
            "number_range_or_complexity": "task-specific generative processes; typical instance lengths sampled up to L=20 during training and up to 2L at test; specific digit ranges not enumerated in main text",
            "method_or_intervention": "trained-from-scratch on synthetic datasets per task; some tasks also evaluated with scratchpad formats (chain-of-thought style intermediate steps)",
            "performance_result": "Qualitative: near-perfect train/IID performance for most PEs; on extrapolation NoPE and T5's Relative Bias generally outperform other PEs across tasks. ALiBi is intermediate; APE and Rotary typically underperform on extrapolation. Detailed per-task numeric breakdowns are referenced in appendices (not reproduced in main text).",
            "mechanistic_insight": "Same mechanistic patterns as addition: NoPE tends to learn relative-like encodings; attention analyses show NoPE and T5 attend both near and far tokens (bimodal), ALiBi focuses on recent tokens, Rotary/APE show flatter/uniform attention. Theoretical constructions show NoPE can internally encode absolute positions (first layer) and implement relative biases (later layers), enabling algorithmic computation.",
            "performance_scaling": "Extrapolation tested by training up to L=20 and testing up to 2L; explicit PEs often fail to extrapolate as length grows; NoPE and T5 relatively robust. No precise numerical scaling curves in main text.",
            "failure_modes": "Failures concentrated on explicit PEs when extrapolating beyond training lengths; methods with recency bias (ALiBi) can overly prefer short-range dependencies and fail on tasks requiring long-range reasoning.",
            "comparison_baseline": "Comparisons across PEs (NoPE, APE, Rotary, ALiBi, T5 Relative) and scratchpad vs no-scratchpad.",
            "key_finding": "Across a range of arithmetic and algorithmic tasks, NoPE and T5's relative bias generalize best to longer sequences; commonly used PEs (APE, Rotary, ALiBi) often underperform on length extrapolation in downstream mathematical tasks.",
            "uuid": "e304.1",
            "source_info": {
                "paper_title": "The Impact of Positional Encoding on Length Generalization in Transformers",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "NoPE mechanistic theory",
            "name_full": "Mechanistic explanation and theoretical construction for how NoPE encodes positions",
            "brief_description": "The paper proves that a decoder-only Transformer without explicit positional encoding can (1) recover absolute positions in its first layer (via a constructed attention head using BOS anchoring and uniform keys) and (2) implement relative positional bias in subsequent layers (attention dot product decomposes into content + function of distance). Empirically, NoPEs trained with SGD tend to learn relative-like encodings similar to T5's Relative PE.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "NoPE decoder-only Transformer",
            "model_size": "107M (main experiments)",
            "model_architecture": "decoder-only transformer (causal attention) without explicit positional encodings",
            "arithmetic_operation_type": "general arithmetic/algorithmic tasks (addition, summation, polynomial eval, sorting, parity, etc.)",
            "number_range_or_complexity": "training setup typically uses L=20; theoretical results apply generally (constructive proofs not tied to a particular numeric range)",
            "method_or_intervention": "theoretical constructive proofs plus empirical attention-distribution comparisons (Jensen-Shannon divergence between heads), and layerwise attention analysis",
            "performance_result": null,
            "mechanistic_insight": "Theorem 1 constructs a first-layer head that, using a BOS token and uniform-key construction, produces an attention output proportional to 1/t (where t is the current position length) which a feedforward MLP can map to explicit absolute positions — establishing that absolute positions are representable without explicit PE. Theorem 2 shows that if absolute positions persist in hidden states, later-layer attention dot-products can be parameterized as content-dependent term plus a function of relative distance: &lt;q_n,k_m&gt; = f_cnt(q,k) + f_rel(n-m). Empirically, attention-pattern similarity measures show NoPE trained with SGD tends to produce attention maps closest to T5 Relative PE, indicating that learned NoPE behavior in practice is relative-like. Attention-distribution analyses on scratchpad tasks reveal NoPE/T5 produce bimodal short+long-range attention patterns useful for arithmetic procedures that require both local stepwise processing (scratchpad steps) and access to the original input.",
            "performance_scaling": null,
            "failure_modes": null,
            "comparison_baseline": "The theoretical constructions are compared conceptually to explicit PEs (APE, ALiBi, Rotary, T5 Relative); empirical similarity measured between NoPE and each explicit PE using JSD of attention distributions.",
            "key_finding": "No explicit positional encoding is necessary for a decoder-only Transformer to represent absolute or relative positions; when trained, NoPEs empirically adopt relative-like encodings (similar to T5 Relative Bias), which helps length generalization on arithmetic tasks.",
            "uuid": "e304.2",
            "source_info": {
                "paper_title": "The Impact of Positional Encoding on Length Generalization in Transformers",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Scratchpad / CoT effects",
            "name_full": "Scratchpad (chain-of-thought) interventions and their impact on arithmetic length generalization",
            "brief_description": "The study investigates whether emitting intermediate computation steps (scratchpad/CoT) makes positional encoding choice irrelevant and how scratchpad format affects arithmetic performance and attention.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "decoder-only Transformer (task-trained)",
            "model_size": "107M",
            "model_architecture": "decoder-only transformer (causal attention)",
            "arithmetic_operation_type": "addition and other mathematical/reasoning tasks when using scratchpad formatting",
            "number_range_or_complexity": "scratchpad experiments use reduced length threshold L=8 to avoid OOM due to long generated sequences; specific numeric ranges not enumerated",
            "method_or_intervention": "scratchpad / chain-of-thought style outputs with multiple scratchpad formats (including a 'full Format' with step input, computation, output, variables, and remaining input); models trained from scratch on datasets with scratchpad outputs",
            "performance_result": "Qualitative: scratchpad helps in some tasks and formats (e.g., the 'full Format' on addition is reported as helpful across PEs), but is not universally beneficial; effectiveness is highly task- and format-dependent. No single PE + scratchpad combination universally solves length-generalization failures.",
            "mechanistic_insight": "Attention analysis shows which parts of scratchpad models attend to: NoPE and T5 Relative often exhibit a bimodal attention distribution (attending both the current scratchpad step and distant tokens such as original input), ALiBi strongly prefers short-range (recent) tokens, while Rotary and APE show more uniform attention. Redundant repeated info in scratchpad (e.g., remaining input) may or may not be leveraged depending on PE and format.",
            "performance_scaling": null,
            "failure_modes": "Scratchpad can be ineffective or harmful if the format is suboptimal; long scratchpad sequences require reduced training length thresholds and can exacerbate length-generalization issues if positional encoding is mismatched.",
            "comparison_baseline": "Compared models trained with and without scratchpad across different PEs and scratchpad formats.",
            "key_finding": "Scratchpad/CoT does not eliminate the dependence on positional encoding for length generalization; its benefit is highly format-dependent and interacts with the model's positional encoding (NoPE and T5 benefit most via attention patterns that mix short- and long-range dependencies).",
            "uuid": "e304.3",
            "source_info": {
                "paper_title": "The Impact of Positional Encoding on Length Generalization in Transformers",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Scaling probe (1B)",
            "name_full": "Preliminary 1B-parameter scale experiments probing perplexity-based length generalization across PEs",
            "brief_description": "Post-submission experiments trained three ~1B-parameter models (ALiBi, Rotary, NoPE) on a subset of StarCoder training data to inspect scaling effects on context-length extrapolation (reported in terms of language modeling perplexity).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "decoder-only Transformer (pretrained subset)",
            "model_size": "≈1B",
            "model_architecture": "decoder-only transformer (causal attention)",
            "arithmetic_operation_type": "not task-specific; this is a language-modeling perplexity probe relevant to context-length extrapolation (implications for downstream arithmetic tasks discussed qualitatively)",
            "number_range_or_complexity": null,
            "method_or_intervention": "pretraining on StarCoder subset with context length 1024; evaluation of perplexity at longer contexts to probe length generalization",
            "performance_result": "Perplexity (LM) findings: at IID (within training context) all PEs have similar perplexity; at longer contexts Rotary fails to generalize (perplexity explodes), while NoPE and ALiBi generalize similarly up to almost twice training context; beyond that ALiBi is more stable than NoPE. Downstream fine-tuning on the small synthetic tasks showed identical performance across PEs when the pretraining context is much larger than task instance lengths.",
            "mechanistic_insight": "Scaling behavior indicates that some PEs (Rotary) can break catastrophically on longer contexts at large scale (perplexity explosion), while NoPE and ALiBi are more robust up to moderate extrapolation; this complements downstream-task findings and highlights that perplexity and downstream extrapolation can diverge.",
            "performance_scaling": "At 1B scale: similar IID perplexities; Rotary fails at longer contexts; NoPE and ALiBi generalize up to ~2x training context; beyond that ALiBi more stable.",
            "failure_modes": "Rotary shows catastrophic perplexity growth at longer contexts; NoPE becomes less stable than ALiBi at very large extrapolations in LM perplexity (though downstream-task behavior may differ).",
            "comparison_baseline": "ALiBi vs Rotary vs NoPE at ~1B parameters, evaluated by LM perplexity across increasing context windows.",
            "key_finding": "At larger scale (1B), positional encoding choice still matters: Rotary may fail catastrophically on longer contexts, while NoPE and ALiBi show more moderate extrapolation up to ~2x the trained context; LM perplexity behavior does not always match downstream arithmetic-task extrapolation.",
            "uuid": "e304.4",
            "source_info": {
                "paper_title": "The Impact of Positional Encoding on Length Generalization in Transformers",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Show your work: Scratchpads for intermediate computation with language models.",
            "rating": 2
        },
        {
            "paper_title": "Analyzing mathematical reasoning abilities of neural models.",
            "rating": 2
        },
        {
            "paper_title": "Train short, test long: Attention with linear biases enables input length extrapolation.",
            "rating": 2
        },
        {
            "paper_title": "Transformer language models without positional encodings still learn positional information.",
            "rating": 2
        },
        {
            "paper_title": "Roformer: Enhanced transformer with rotary position embedding.",
            "rating": 2
        },
        {
            "paper_title": "Exploring length generalization in large language models.",
            "rating": 2
        }
    ],
    "cost": 0.016093999999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>The Impact of Positional Encoding on Length Generalization in Transformers</h1>
<p>Amirhossein Kazemnejad ${ }^{1}$, Inkit Padhi ${ }^{2}$<br>Karthikeyan Natesan Ramamurthy ${ }^{2}$, Payel Das ${ }^{2}$, Siva Reddy ${ }^{1,3,4}$<br>${ }^{1}$ Mila, McGill University; ${ }^{2}$ IBM Research;<br>${ }^{3}$ Facebook CIFAR AI Chair; ${ }^{3}$ ServiceNow Research<br>{amirhossein.kazemnejad,siva.reddy}@mila.quebec<br>inkpad@ibm.com, {knatesa, daspa}@us.ibm.com</p>
<h4>Abstract</h4>
<p>Length generalization, the ability to generalize from small training context sizes to larger ones, is a critical challenge in the development of Transformer-based language models. Positional encoding (PE) has been identified as a major factor influencing length generalization, but the exact impact of different PE schemes on extrapolation in downstream tasks remains unclear. In this paper, we conduct a systematic empirical study comparing the length generalization performance of decoder-only Transformers with five different position encoding approaches including Absolute Position Embedding (APE), T5's Relative PE, ALiBi, and Rotary, in addition to Transformers without positional encoding (NoPE). Our evaluation encompasses a battery of reasoning and mathematical tasks. Our findings reveal that the most commonly used positional encoding methods, such as ALiBi, Rotary, and APE, are not well suited for length generalization in downstream tasks. More importantly, NoPE outperforms other explicit positional encoding methods while requiring no additional computation. We theoretically demonstrate that NoPE can represent both absolute and relative PEs, but when trained with SGD, it mostly resembles T5's Relative PE attention patterns. Finally, we find that scratchpad is not always helpful to solve length generalization and its format highly impacts the model's performance. Overall, our work suggests that explicit position encodings are not essential for decoder-only Transformers to generalize well to longer sequences.</p>
<h2>1 Introduction</h2>
<p>The ability to generalize from smaller training context sizes to larger ones, commonly known as length generalization, is a major challenge for Transformer-based language models (Vaswani et al., 2017; Deletang et al., 2023; Zhang et al., 2023). Even with larger Transformers, this issue persists (Brown et al., 2020; Furrer et al., 2020). With larger context sizes, a model can benefit from more in-context-learning examples, higher numbers of reasoning steps, or longer text generation. However, training a Transformer with a larger context size can be excessively slow and memory-intensive. This is even more pronounced in the recent paradigm of model finetuning on instruction-following datasets (Wei et al., 2022a; Chung et al., 2022; Ouyang et al., 2022). It is not only infeasible to train the model on all possible context lengths, but also the number of training examples drops dramatically as the sequence length increases requiring the model to generalize from finite and shorter-length training examples. In this work, we focus on the effect of positional encoding on length generalization in the "decoder-only" Transformers on various tasks trained from scratch. Figure 1 summarizes our finding that using no positional encoding is better than using explicit positional encodings.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: No positional encoding (NoPE) outperforms all other positional encodings at length generalization of decoder-only Transformers (GPT-style) trained from scratch and evaluated on a battery of reasoning-like downstream tasks. This figure shows aggregate ranking of positional encoding methods across 10 tasks.</p>
<p>Positional encoding (PE) seems to be a major factor in the length generalization of Transformers as the model has to systematically encode tokens in all possible positions. The original Transformer architecture (Vaswani et al., 2017) used non-parametric periodic functions to represent absolute position embeddings (APE) in a systematic manner, but further studies have shown that these functions are inadequate for length generalization (Ontanon et al., 2022). The prevailing belief is that relative PEs (Shaw et al., 2018; Raffel et al., 2020) are more effective in length generalization than APE variants (Ontanon et al., 2022; Csordás et al., 2021). However, Press et al. (2022) has shown that even Transformers with relative PEs, such as Rotary (Su et al., 2021), can be poor at length generalization. But the evaluation of PEs often relies on language modeling perplexity as a key metric (Haviv et al., 2022; Press et al., 2022) which does not always align with the performance on downstream tasks (Tay et al., 2022). This raises important questions: what exactly is the influence of positional encoding on length generalization at downstream tasks? Moreover, early empirical evidence shows that decoder-only Transformers without explicit position information (Tsai et al., 2019; Haviv et al., 2022) can perform as well as existing PEs in in-distribution settings, but its effects on length generalization and downstream performance are unclear.</p>
<p>Recently, asking models to emit intermediate computation steps into a scratchpad, also referred to as chain-of-thought, has been adopted to improve the length extrapolation in Transformers (Nye et al., 2021; Wei et al., 2022b). These techniques are architecture-independent and can be used with any PE method. However, it remains an open question whether these techniques, at least in regard to length generalization, render the choice of PE irrelevant, especially given that model performance is highly sensitive to the scratchpad format (Bueno et al., 2022; Akyurek and Akyurek, 2022).</p>
<p>To answer these questions, we conduct a systematic study on the length generalization of decoder-only Transformers, popularized by the GPT-family of models (Radford et al., 2019), with the most commonly used positional encoding schemes, both with and without scratchpad. Specifically, we evaluate APE (Vaswani et al., 2017), T5's Relative PE (Raffel et al., 2020), ALiBi (Press et al., 2022), Rotary (Su et al., 2021) and no positional encoding (NoPE) on a battery of reasoning and mathematical tasks. Our results show that:</p>
<ul>
<li>Most commonly used positional encoding methods, including ALiBi, Rotary, and APE, are ill-suited for length generalization in downstream tasks and are outperformed by T5's Relative PE.</li>
<li>Transformers without positional encoding (NoPE) outperform all explicit positional encoding schemes. They achieve this without computing additional terms in the attention mechanism (in contrast to explicit PEs).</li>
<li>We show that NoPE is theoretically capable of representing both absolute and relative PEs. But empirically, it is closer to the relative encoding scheme similar to T5's Relative PE.</li>
<li>Scratchpad is not always helpful for length generalization and its format highly impacts the performance. The attention distributions reveal that NoPE and T5's Relative PE encourage attending to both long and short-range positions, ALiBi to recent positions, and Rotary and APE to no particular positions.</li>
</ul>
<p>2 Background: Positional Encoding in Transformers</p>
<p>Transformers, in contrast to sequential models such as RNNs, are parallel architectures that employ positional encoding to help encode word order. The most common choices for positional encoding are either absolute, where each absolute position (e.g. 1, 2, 3, …) is directly represented, or relative, where the distance between tokens is used as positional information. In this section, we briefly review the popular encoding methods used in Transformers (Refer to Appendix B for more formal details).</p>
<p>Absolute Position Embedding (APE), embeds each absolute position $i$ into position vector $\boldsymbol{p}<em i="i">{i}$ and adds word embeddings to their corresponding $\boldsymbol{p}</em>$ before feeding them to the model. The non-parametric variant of APE uses periodic functions such as sine and cosine to generate embeddings for any position $i$ <em>Vaswani et al. (2017)</em>. On the other hand, a learned version of APE, used in GPT3 <em>Brown et al. (2020)</em> and OPT <em>Zhang et al. (2022)</em>, trains the position embeddings along with the model parameters, and it cannot generate a position embedding for unseen positions, so the context window is set to a fixed length.</p>
<p>T5’s Relative bias, first maps the relative distance $(i-j)$ between tokens at positions $i$ and $j$ to a scalar bias value $b=f(i-j)$, where $f$ is a lookup table. The relative bias $b$ (learned during training) then is added to the dot product of the query and key in the self-attention mechanism. The lookup table maps distances larger than a threshold to the same parameter to enable generalization to unseen distances.</p>
<p>Rotary, used in PaLM <em>Chowdhery et al. (2022)</em> and LLaMA <em>Touvron et al. (2023)</em>, rotates the query and key representations with an angle proportional to their absolute positions before applying the dot product attention. As a result of this rotation, the attention dot product will only depend on the relative distance between tokens, effectively making it a relative positional encoding <em>Su et al. (2021)</em>.</p>
<p>ALiBi, used in BLOOM <em>Scao et al. (2022a)</em>, is similar to T5’s Relative Bias but instead subtracts a scalar bias from the attention score. This bias grows linearly with the distance between the query and key tokens. This, in effect, creates a preference toward recent tokens (recency bias).</p>
<p>Note that encoder-only Transformers, such as BERT, become bag-of-words models in the absence of positional encoding. However, decoder-only Transformers with causal attention mask are not permutation invariant and can model sequences even without explicit position information <em>Tsai et al. (2019)</em>. But it is unclear if these models encode position information implicitly or generalize to unseen lengths. We demystify this in Section 5.</p>
<h2>3 Model Evaluation</h2>
<p>Length Generalization Setup Following <em>Anil et al. (2022)</em>, we focus on algorithmic tasks such as copying, addition, etc. For each task, we train on a finite number of examples of up to a certain length and test them on both seen and unseen lengths at inference. We present these problems as sequence-to-sequence tasks, where the input sequence is the problem instance and the output sequence is the solution. Formally, let $\mathcal{D}={(\boldsymbol{x}<em i="i">{i},\boldsymbol{y}</em>})}$ denote a dataset of such task where $\boldsymbol{x<em i="i">{i}$ is the input and $\boldsymbol{y}</em>$. This can be the number of tokens or any general notion of length/depth of reasoning. Using this function and a threshold $L$, we employ samples where $\lambda \leq L$ for learning the task and samples where $\lambda&gt;L$ for evaluating generalization. The performance on each instance is reported as the exact-match accuracy of its answer with the ground truth.}$ is the output sequence. For each task a function $\lambda: \mathcal{D} \rightarrow \mathbb{N}$ can be defined that returns the length bucket of a task instance $d \in \mathcal{D</p>
<p>Architecture We use a conventional decoder-only Transformer architecture as a base for all experiments and consider different approaches for encoding positions: Absolute Position Embedding (APE), ALiBi, Rotary and T5’s Relative Bias. We also consider removing the positional encoding (NoPE) to better understand its role in length generalization. Note that we use APE with sinusoidal functions <em>Vaswani et al. (2017)</em> as the learnable variant cannot produce embeddings for unseen positions. Given the absence of publicly available Transformer-based LM trained with aforementioned PEs on the same pretraining data, we opt to train our models from scratch for each task on its training data with the autoregressive language modeling objective $\log p_{\theta}(\boldsymbol{y} \mid \boldsymbol{x})=\sum_{t=1}^{T} \log p_{\theta}\left(y_{t} \mid \boldsymbol{x}, \boldsymbol{y}_{1: t-1}\right)$. We use the same hyperparameters for all PEs and employ the “base" model size configuration, popular in HuggingFace library <em>Wolf et al. (2020)</em>, resulting in $\sim 107$ M trainable weights (List of all hyperparameters in Appendix D.2).</p>
<p>Table 1: Examples of the input and output of the tasks.</p>
<table>
<thead>
<tr>
<th>Task</th>
<th>Input Example</th>
<th>Output Example</th>
</tr>
</thead>
<tbody>
<tr>
<td>Primitive Tasks</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Copy</td>
<td>Copy the following words: <w1> <w2> <w3> <w4> <w5></td>
<td><w1> <w2> <w3> <w4> <w5></td>
</tr>
<tr>
<td>Reverse</td>
<td>Reverse the following words: <w1> <w2> <w3> <w4> <w5></td>
<td><w5> <w4> <w3> <w2> <w1></td>
</tr>
<tr>
<td>Mathematical and Algorithmic Tasks</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Addition</td>
<td>Compute: 5 3 7 2 6 + 1 9 1 7 ?</td>
<td>The answer is 5 5 6 4 3.</td>
</tr>
<tr>
<td>Polynomial Eval.</td>
<td>Evaluate x = 3 in ( 3 x ** 0 + 1 x ** 1 + 1 x ** 2 ) % 10 ?</td>
<td>The answer is 5.</td>
</tr>
<tr>
<td>Sorting</td>
<td>Sort the following numbers: 3 1 4 1 5 ?</td>
<td>The answer is 1 1 3 4 5.</td>
</tr>
<tr>
<td>Summation</td>
<td>Compute: ( 1 + 2 + 3 + 4 + 7 ) % 10 ?</td>
<td>The answer is 7.</td>
</tr>
<tr>
<td>Parity</td>
<td>Is the number of 1's even in [ 1 0 0 1 1] ?</td>
<td>The answer is No.</td>
</tr>
<tr>
<td>LEGO</td>
<td>If a = -1; b = -a; c = +b; d = +c. Then what is c?</td>
<td>The answer is +1.</td>
</tr>
<tr>
<td>Classical Length Generalization Datasets</td>
<td></td>
<td></td>
</tr>
<tr>
<td>SCAN</td>
<td>jump twice and run left</td>
<td>JUMP JUMP TURN_LEFT RUN</td>
</tr>
<tr>
<td>PCFG</td>
<td>shift prepend K10 R1 K12 , E12 F16</td>
<td>F16 K10 R1 K12 E12</td>
</tr>
</tbody>
</table>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Aggregate ranking of positional encoding methods on length extrapolation across three different groups of tasks. No PE and T5's Relative Bias outperform other encoding methods in these categories.</p>
<p>Tasks Our study of length generalization is concentrated on downstream tasks. Particularly, we evaluate the models on three categories (Table 1) of synthetic tasks that have been widely used in the literature to investigate length generalization: (1) Primitive tasks such as Copying and Reversing (Ontanon et al., 2022), (2) Mathematical and reasoning tasks such as Addition (Nye et al., 2021), Polynomial Evaluation, Sorting, Summation (Saxton et al., 2019), Parity (Anil et al., 2022), LEGO (Zhang et al., 2023) and (3) Classical length generalization datasets such as SCAN (Lake and Baroni, 2018) and PCFG (Hupkes et al., 2020). These tasks provide us with complete control over the train-test distribution, while also requiring reasoning and compositionality skills, which serve as fundamental building blocks for more complex tasks. For the first two categories, we generate the corresponding datasets. Specifically, we first sample the length of the task instance from the uniform distribution $\mathcal{U}(1, L)$, and then, according to the task's generative process, we sample the input and output sequences. For the test set, we follow the same procedure but sample length from $\mathcal{U}(1,2 L)$ to include both seen and unseen lengths. Throughout the paper, unless otherwise stated, we use $L=20$. For the third category of tasks, we use length generalization splits from the corresponding datasets. Table 1 provides an example of each task (More examples in Appendix D.1).</p>
<p>We report the results of our empirical evaluation over ten tasks and three seeds per dataset-PE pair.</p>
<h2>4 What Is The Effect of Positional Encoding?</h2>
<p>In this section we provide comparative results of positional encodings at length generalization. To provide a holistic view, following Liang et al. (2022), we report the mean ranking of various models in Figures 1 and 2 when compared against each other for all tasks and scenarios. Furthermore, we showcase the accuracy of models evaluated on examples of various lengths in Figure 3. (Detailed results for each task and scenario can be found in Appendix E).</p>
<p>First, we observe that in most tasks, models achieve a perfect or near-perfect accuracy (Figure 3) on the I.I.D. lengths, which indicates that models have no problem fitting to the training data. However, the differences among positional encoding methods become more apparent when we evaluate on lengths that are larger than seen during training. In most extrapolation scenarios, T5's Relative Bias</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Showcasing the generalization behavior of different positional encodings on 6 datasets. The shaded area represents evaluation examples with I.I.D. lengths (i.e. seen during training). Since all models perform perfectly, or close to it, on the I.I.D. lengths (measured on unseen examples), for improved readability, we only show a subset of them in the figure. Refer to Appendix E for more detailed plots.</p>
<p>outperforms other explicit positional encodings. ALiBi positions itself in the middle of the pack, while APE and Rotary show poor generalization performance.</p>
<p>Although Rotary is often considered a relative encoding method (Ontanon et al., 2022), our results show that it performs more similarly to APE than to other relative schemes. Moreover, ALiBi, despite its promise for length generalization, underperforms with respect to T5's Relative Bias in most cases. This result aligns with Taylor et al. (2022) who found no significant improvement from ALiBi.</p>
<p>Surprisingly, the NoPE model, which is just a decoder-only Transformer without any positional encoding, performs on par with or even better than the best-performing explicit PE, T5's Relative Bias. NoPE achieves the same level of generalization without <em>any computational overhead</em> since it does not compute any additional term in the attention mechanism. This property has a direct impact on the runtime and memory footprint of the model. For instance, Press et al. (2022) reported that the additional computation incurred by T5's Relative Bias can make the training and inference time of the model almost two times slower than the Transformer with APE.</p>
<h2>5 How Does NoPE Represent Positions?</h2>
<p>The surprising performance of NoPE model suggests that it captures useful positional information that can also generalize. But, how it does so is the primary question. In the next two sections, we provide theoretical and empirical analysis towards answering this question.</p>
<h3>5.1 NoPE can theoretically represent both absolute and relative PEs</h3>
<p>Let $f_{\theta}$ be a NoPE decoder-only Transformer model, where $\theta$ denotes the model parameters. $f_{\theta}$ processes the input sequence $\boldsymbol{x}=[\langle\mathbf{bos}\rangle, x_{1}, \ldots, x_{T}]$ by applying a series of layers. Note that since $f_{\theta}$ does not have any PE, the input $\boldsymbol{x}$ is not augmented with positional information (e.g. $[1,2, \ldots, T]$). Each layer $l$, consisting of self-attention heads and a feed-forward sub-layer, reads the previous hidden state $\boldsymbol{H}^{(l-1)}$ and produces the hidden state at layer $l$: $\boldsymbol{H}^{l}$. Each head is parameterized by a query $\boldsymbol{W}<em K="K">{Q}$, key $\boldsymbol{W}</em>}$, value $\boldsymbol{W<em O="O">{V}$, and output $\boldsymbol{W}</em>}$ matrices, where $\boldsymbol{W<em K="K">{Q}, \boldsymbol{W}</em>}, \boldsymbol{W<em O="O">{V} \in \mathbb{R}^{h \times d}$ and $\boldsymbol{W}</em>} \in \mathbb{R}^{d \times h}$. $d$ and $h$ are the model's hidden state size and attention dimension, respectively. $\boldsymbol{W<em 2="2">{1}, \boldsymbol{W}</em>$ are the weight matrices of the feed-forward sub-layer.} \in \mathbb{R}^{d \times k \cdot d</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Distance of NoPE attention patterns with other positional encoding schemes measured across instances of SCAN dataset. The left figure shows the distance per layer, and the right figure shows the average distance across all layers. NoPE' denotes NoPE trained with a different seed.</p>
<p>Theorem 1 (Absolute Encoding). Let $\boldsymbol{x}$ be an input sequence of length $T+1$ to the model. Then, the first layer of $f_{\theta}$ can recover absolute positions $[1, \ldots, T+1]$ in the hidden state $\boldsymbol{H}^{(1)}$. That is, there exist $\boldsymbol{W}<em K="K">{Q}, \boldsymbol{W}</em>}, \boldsymbol{W<em O="O">{V}, \boldsymbol{W}</em>}, \boldsymbol{W<em 2="2">{1}$, and $\boldsymbol{W}</em>$ such that the self-attention and feedforward operations in the first layer compute absolute positions and write it to the next hidden state.</p>
<p>We refer to Appendix C. 1 for the complete proof of Theorem 1. This theorem shows that stochastic gradient descent (SGD) can potentially learn to recover absolute positions in NoPE Transformers. Next, we demonstrate how relative PE can be implemented in subsequent layers:</p>
<p>Theorem 2 (Relative Encoding). Suppose that the hidden state $\boldsymbol{H}^{(1)}$ contains absolute positional information, as stated in Theorem 1, and assume that it is not overwritten by any subsequent layers. Then, the self-attention in all subsequent layers can implement a relative positional encoding: there exists a parameterization of $f_{\theta}$ such that, for $l \geq 2$, the attention dot product between query $\boldsymbol{q}<em m="m">{n}$ and key $\boldsymbol{k}</em>$ at positions $n$ and $m$ can be expressed as:</p>
<p>$$
\left\langle\boldsymbol{q}<em m="m">{n}, \boldsymbol{k}</em>(n-m)
$$}\right\rangle=f_{\mathrm{cnt}}(\boldsymbol{q}, \boldsymbol{k})+f_{\mathrm{rel}</p>
<p>where $f_{\mathrm{cnt}}$ is a function of their content, and $f_{\mathrm{rel}}$ is a function of their relative distance.</p>
<p>Appendix C. 2 provides the complete proof of Theorem 2. Our theoretical results suggest that SGD can choose between relative and absolute encoding in NoPE Transformers. But, what mechanism SGD learns in practice is not clear. We next investigate this question empirically.</p>
<h1>5.2 NoPE learns to use relative PE in practice</h1>
<p>In order to explore the mechanisms that NoPE employs in practice, we conduct a quantitative analysis by comparing its attention pattern to models trained with different positional encoding techniques. The hypothesis is that if NoPE utilizes a similar algorithm to other PEs, then the attention patterns of these models should be quite similar.</p>
<p>To this end, we feed the same input to both models and, at layer $l$, we compute the minimum distance between the attention distribution of any heads in the first model and any head in the second model. Formally, let $\mathrm{P}<em t="t">{t}=p\left(\boldsymbol{k} \mid \boldsymbol{q}</em>}\right)$ be a probability distribution produced by a causal selfattention head for query at position $t$, over the keys $\boldsymbol{k} \in\left[\boldsymbol{k<em t="t">{1}, \ldots \boldsymbol{k}</em>}\right]$ in a given transformer layer. Over a sequence of length $T$, we define the similarity between two heads P and Q as $D_{\mathrm{AT}}(\mathrm{P}, \mathrm{Q})=$ $\frac{1}{T} \sum_{t=1}^{T} D_{\mathrm{JSD}}\left(\mathrm{P<em t="t">{t} \mid \mathrm{Q}</em>\right)$ which averages the Jensen-Shannon divergence (JSD) between the two heads over all positions. For the distance of two models $A$ and $B$ at layer $l$, we take the minimum distance</p>
<p>between all pairs of attention heads in the corresponding layer:</p>
<p>$$
D^{(l)}(A, B)=\min <em l="l">{(\mathrm{P}, \mathrm{Q}) \in A</em>)
$$} \times B_{l}} D_{\mathrm{AT}}(\mathrm{P}, \mathrm{Q</p>
<p>where $A_{l}$ and $B_{l}$ are the attention heads in layer $l$ of models $A$ and $B$ respectively. We empirically measure the distance between NoPE and other positional encoding schemes after training. Specifically, we sample examples from each length bucket and feed them (the concatenation gold input and output) to compute the attention maps and the distance using Equation (2). We also consider the distance between different seeds of NoPE as a baseline. Figure 4 shows the distance per layer for the first four layers. (later layers show similar trends Figure F.7). We find that NoPE's attention patterns are most similar to that of T5's Relative PE, and least similar to APE and Rotary. The same trend can be observed across all layers and length buckets, and even when averaged across all layers. These results potentially suggest that a Transformer model without positional encoding, trained with stochastic gradient descent learns to represent positions in a way similar to T5's Relative PE, which is a relative positional encoding scheme.</p>
<h1>6 Does Scratchpad Render The Choice of Positional Encoding Irrelevant?</h1>
<p>In scratchpad/CoT prompting, the model generates intermediate computations required to reach the final answer as explicit parts of the output. Such mechanisms, in effect, provide a direct decomposition and storage for intermediate values, which has been shown to improve the length generalization of Transformers even at small scale (Nye et al., 2021). Since scratchpad only modifies the model's input and output (not the architecture), it is unclear and unexplored how architectural choices such as positional encoding affect the length generalization in the presence of scratchpad. To answer this question, we train all PEs with and without scratchpad on the mathematical and reasoning group of tasks, and compare their performance.</p>
<p>Moreover, the decision of how to represent the intermediate computations in the scratchpad, i.e. the scratchpad format, is an important design choice that has a non-trivial impact on the model's performance (Bueno et al., 2022).</p>
<p>Input (a)
Compute 53726 + 1917 =
Output ( $x, y$ )
<scratch>
$\mathcal{I}$ For digits 6 and 7 ,
$\mathcal{C}$ We have $(6+7+$ carry $)$
$\% 10=13 \% 10=13 \% 10$
$\mathcal{O}$ Which is equal to 3 .
$\mathcal{V}$ We update carry to $13 / /$
$10=1$.
$\mathcal{R}$ So, the remaining input is
5372 + 191
$\cdots$
</scratch>
The answer is 55643 .
Figure 5: Example of an addition task depicted with its first scratchpad step. Each step consists of five components: Step Input $\mathcal{I}$, Step Computation $\mathcal{C}$, Step Output $\mathcal{O}$, Intermediate Variable Updates $\mathcal{V}$, and Remaining Input $\mathcal{R}$.
addition task. Additionally, our findings indicate that having a positional encoding with robust length generalization is crucial since scratchpad/CoT alone may not enhance the generalization.</p>
<h3>6.1 Which part of the sequence is attended to?</h3>
<p>The scratchpad format that is often used (Nye et al., 2021), similar to Figure 5, contains redundant information. One such example is the repetition of the remaining portion of an input $(\mathcal{R})$ in each step of the scratchpad. But, the attention can attend to this information directly from the main input. So, it remains unclear which specific part of the scratchpad different PEs rely on to solve the task.</p>
<p>To address this question, we take the models trained with full Format on addition, the case in which scratchpad is helpful across all PEs, and examine their attentions. Specifically, for tokens in</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Mean ranks of scratchpad format aggregated across all models per each dataset. The effectiveness of scratchpad is task dependent.
the output sequence, we calculate the distance $d$ between the current query $\boldsymbol{q}<em n="n">{t}$ and the attended key $\boldsymbol{k}</em>$.
Figure 7 depicts the distribution of $\bar{d}$. Values of $\bar{d}$ close to 0 indicate attention to tokens near the current position (e.g. current scratchpad step), while values close to 1 signify attention to distant tokens (e.g. the input). NoPE and T5's Relative PE resemble each other and exhibit a bimodal distribution, reflecting both short-range and long-range attention. Conversely, ALiBi (due to its recency bias) strongly favors short-range attention. Rotary, on the other hand, produces a distribution resembling APE, which is more uniformly distributed. Notably, NoPE and T5's RPE are the top-performing PEs in this setup, which suggest the bimodal distribution to be more optimal.}$ as $(t-n+1)$ and subsequently normalize it based on the length of the sequence at the present step. The normalized value is denoted as $\bar{d</p>
<h1>7 Discussion</h1>
<p>Practitioners have to make important choices about the nuances of the Transformer architecture like positional encoding before undertaking the costly pretraining process. In the I.I.D evaluation of PEs, we demonstrate similar performance across different PEs, in line with observations of Haviv et al. (2022) and Scao et al. (2022b), which makes the choice of optimal positional encoding challenging.
In our paper, we utilize length generalization in downstream tasks as a mean to assess the expressivity of positional encodings. Our setup, in contrast to the I.I.D. evaluation, reveals a clear distinction among approaches of encoding positions. We find that NoPE outperforms explicit PEs, and within explicit PEs, commonly used methods lag behind T5's Relative PE. In fact, the recent release of LLMs (Touvron et al., 2023; Chowdhery et al., 2022) suggests a shift towards adopting Rotary as a replacement for APE in the Transformer architecture. However, our result in Section 4 clearly demonstrates that Rotary marginally outperforms APE at length generalization. Furthermore, it exhibits similar behavior to APE, as shown in Section 6.1, indicating potential susceptibility to the same limitations.
The disadvantages of explicit PEs over NoPE in length extrapolation contribute to the growing evidence that positional encodings pose challenges for Transformers (Sinha et al., 2022; Luo et al., 2021). Our empirical results and theoretical analysis suggest that removing positional encoding holds promise as a modification to the widely used decoder-only Transformer architecture.</p>
<p>Scaling up to 1B models In order to study the behavior of position embeddings at scale, we trained three 1B variants post-submission - ALiBi, Rotary and NoPE - with context length of 1024 tokens on a subset of StarCoder training set (Li et al., 2023). For more details, refer to Appendix F. Our results on language modeling show that at I.I.D all variants have similar perplexity, but at length generalization, Rotary fails to generalize as its perplexity explodes. NoPE and ALiBi generalize similarly to larger context sizes up to almost twice their training context size, and for larger contexts ALiBi is relatively more stable than NoPE (see the discussion on perplexity vs. downstream performance). Preliminary exploration of fine-tuning the pretrained models, on datasets in Section 3, yielded to identical performance among PE variants as the training context size of the 1.3B models is much larger than instance lengths in our datasets. A comprehensive downstream evaluation of these models remains a subject for future research.</p>
<p>Perplexity vs. downstream Performance Due to human cognitive constraints Gibson, 1998 Kiyono et al., 2021), language modeling data might encompasses short-range dependencies. The combination of this naturally occurring structure (which can be abundant in internet-based corpora) with the Recency Bias inherent in positional encodings like ALiBi could portray an unrealistic representation of models' length generalization performance. In fact, Chi et al. (2023) recently demonstrated that ALiBi's length generalization performance could be replicated using a window attention mask, where tokens beyond a window size $w$ are masked out. Interestingly, we also observe that T5's Relative PE, which can be regarded as trainable version of ALiBi, learns to attend both large and short range dependencies (Figure F.3). This is in line with Tay et al. (2022) observation and underscores the importance of evaluation setups on downstream tasks as compared to solely relying on perplexity.</p>
<h1>8 Related Work</h1>
<p>Length Generalization Failure In Transformers The length generalization problem has been a topic of interest in the study of neural sequence models for a long time Graves et al., 2016 Kaiser and Sutskever, 2016 Lake and Baroni, 2018; Hupkes et al., 2020; Yehudai et al., 2021). Transformers, being state-of-the-art sequence models, have been no exception. A group of studies showed the generalization failure of conventional Transformers with APE on specific datasets such as PCFG (Hupkes et al., 2020), LEGO Zhang et al., 2023), or CLUTRR (Sinha et al., 2019; Gontier et al., 2020). The length generalization problem has been reported even in pretrained Transformers such as T5 (Furrer et al., 2020) and LaMDA (Anil et al., 2022). Csordás et al. (2021) and Ontanon et al. (2022) study the effect of positional encoding on length generalization but mainly focus on showing relative PE outperforms APEs. Press et al. (2022), on the other hand, propose a new encoding method, ALiBi , and demonstrate that it outperforms popular PEs on extrapolation but only in the context of human language modeling. Most relevant is Deletang et al. (2023)'s recent study on length generalization in various neural sequence models (including RNNs and Stacked-RNNs) for tasks from Chomsky hierarchy. However, they do not focus on the difference among positional encoding or on autoregressive models. Unlike these studies, our work extensively compares length generalization in popular PEs for a wide range of tasks, specifically focusing on autoregressive models, which represent many contemporary LLMs.</p>
<p>Positional Encoding A core component of Transformers is the positional encoding mechanism, which helps the model represent the order of the input sequence. Self-attention mechanism in the encoder of Transformers is order-invariant and requires PE to avoid becoming a bag-of-word model. Many methods have been proposed for this purpose. Originally, Vaswani et al. (2017) introduced absolute positional encoding sinusoidal functions (a learned variant popularized by Devlin et al. (2019)). Relative approach for encoding positional information was further introduced by Shaw et al. (2018), which gave rise to a number of pre-trained LM with relative encodings such as TransformerXL (Dai et al., 2019) and T5 (Raffel et al., 2020) that perform well in length generalization. More recently, Su et al. (2021) takes the concept of sinusoidal functions and suggests a new way of encoding positional information by rotating the hidden representations before applying self-attention. This method, referred to as Rotary, has become a popular choice in the recent LLMs. Press et al. (2022) simplify the T5's Relative encoding and introduced a more efficient variant called ALiBi, while keeping the same or improving extrapolation performance. Decoder-only Transformers, due to their causal attention mask, are not order-agnostic and can operate without explicit positional information.</p>
<p>This was observed early on by Shen et al. (2018) and later explained by Tsai et al. (2019). The observation that Transformers without positional encoding can perform on par with explicit PE has been made in various domains such as machine translation (Yang et al., 2019), language modelling (Irie et al., 2019; Haviv et al., 2022), and even other domains like vision or speech (Likhomanenko et al., 2021). In our work, not only we demonstrate that Transformers can operate without explicit position information, but also we present an important setup where they outperform explicit PEs. Furthermore, we theoretically show how they are capable of learning both absolute and relative encodings.</p>
<h1>9 Conclusion</h1>
<p>We studied the robustness of different positional encodings, in decoder-only Transformers, at length generalization on various downstream mathematical and reasoning tasks. Our extensive empirical study shows the effectiveness of NoPE, and further demonstrates that widely used explicit PEs are not suited for length generalization. We also prove that NoPE can implicitly learn both absolute and relative positions, but uses the latter in practice. Finally, we find the effectiveness of scratchpad is task-dependent, and is not a reliable solution for length generalization.</p>
<h2>Limitations</h2>
<p>Our work primarily focuses on positional encodings as a design choice in the Transformers decoder architecture. We could not study how large-scale pretraining affects different PEs because there are no publicly available large language models trained with various PEs under similar conditions. We leave this for future work due to our limited compute budget.</p>
<h2>Acknowledgements</h2>
<p>We are grateful to our anonymous reviewers, Nicolas Chapados, and Omer Levy for their invaluable suggestions and discussions The Mila-IBM grant program provided the funding for this project. SR acknowledges the support provided by the NSERC Discovery Grant program and the Facebook CIFAR AI Chair program. This research was enabled in part by compute resources provided by Mila, the Digital Research Alliance of Canada and ServiceNow.</p>
<h2>References</h2>
<p>Ekin Akyurek and Afra Feyza Akyurek. 2022. Notes on teaching gpt-3 adding numbers.
Ekin Akyurek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. 2023. What learning algorithm is in-context learning? investigations with linear models. In The Eleventh International Conference on Learning Representations.</p>
<p>Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, Logesh Kumar Umapathi, et al. 2023. Santacoder: don't reach for the stars! ArXiv, abs/2301.03988.</p>
<p>Cem Anil, Yuhuai Wu, Anders Johan Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Venkatesh Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur. 2022. Exploring length generalization in large language models. In Advances in Neural Information Processing Systems.</p>
<p>Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. 2016. Layer normalization. ArXiv, abs/1607.06450.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya</p>
<p>Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.</p>
<p>Mirelle Candida Bueno, Carlos Gemmell, Jeff Dalton, Roberto Lotufo, and Rodrigo Nogueira. 2022. Induced natural language rationales and interleaved markup tokens enable extrapolation in large language models. In Proceedings of the 1st Workshop on Mathematical Natural Language Processing (MathNLP), pages 17-24, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.</p>
<p>Ta-Chung Chi, Ting-Han Fan, Alexander Rudnicky, and Peter Ramadge. 2023. Dissecting transformer length extrapolation via the lens of receptive field analysis. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1352213537, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. ArXiv, abs/2204.02311.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. ArXiv, abs/2210.11416.</p>
<p>Róbert Csordás, Kazuki Irie, and Juergen Schmidhuber. 2021. The devil is in the detail: Simple tricks improve systematic generalization of transformers. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 619-634, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov. 2019. Transformer-XL: Attentive language models beyond a fixed-length context. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2978-2988, Florence, Italy. Association for Computational Linguistics.</p>
<p>Gregoire Deletang, Anian Ruoss, Jordi Grau-Moya, Tim Genewein, Li Kevin Wenliang, Elliot Catt, Chris Cundy, Marcus Hutter, Shane Legg, Joel Veness, and Pedro A Ortega. 2023. Neural networks and the chomsky hierarchy. In International Conference on Learning Representations.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, et al. 2021. A mathematical framework for transformer circuits. Transformer Circuits Thread.</p>
<p>Daniel Furrer, Marc van Zee, Nathan Scales, and Nathanael Schärli. 2020. Compositional generalization in semantic parsing: Pre-training vs. specialized architectures. ArXiv, abs/2007.08970.</p>
<p>Edward Gibson. 1998. Linguistic complexity: locality of syntactic dependencies. Cognition, 68(1):176 .</p>
<p>Nicolas Gontier, Koustuv Sinha, Siva Reddy, and Christopher Pal. 2020. Measuring systematic generalization in neural proof generation with transformers. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.</p>
<p>Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka GrabskaBarwińska, Sergio Gómez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, et al. 2016. Hybrid computing using a neural network with dynamic external memory. Nature, $538(7626): 471-476$.</p>
<p>Adi Haviv, Ori Ram, Ofir Press, Peter Izsak, and Omer Levy. 2022. Transformer language models without positional encodings still learn positional information. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 1382-1390, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Dan Hendrycks and Kevin Gimpel. 2020. Gaussian error linear units (gelus). ArXiv, abs/1606.08415.
Dieuwke Hupkes, Verna Dankers, Mathijs Mul, and Elia Bruni. 2020. Compositionality decomposed: How do neural networks generalise? Journal of Artificial Intelligence Research, 67:757-795.</p>
<p>Kazuki Irie, Albert Zeyer, Ralf Schlüter, and Hermann Ney. 2019. Language modeling with deep transformers. In Interspeech 2019, 20th Annual Conference of the International Speech Communication Association, Graz, Austria, 15-19 September 2019, pages 3905-3909.</p>
<p>Lukasz Kaiser and Ilya Sutskever. 2016. Neural gpus learn algorithms. In 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings.</p>
<p>Shun Kiyono, Sosuke Kobayashi, Jun Suzuki, and Kentaro Inui. 2021. SHAPE: Shifted absolute position embedding for transformers. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3309-3321, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Brenden M. Lake and Marco Baroni. 2018. Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pages 2879-2888. PMLR.</p>
<p>Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, et al. 2023. Starcoder: may the source be with you! ArXiv, abs/2305.06161.</p>
<p>Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. 2022. Holistic evaluation of language models. ArXiv, abs/2211.09110.</p>
<p>Tatiana Likhomanenko, Qiantong Xu, Gabriel Synnaeve, Ronan Collobert, and Alex Rogozhnikov. 2021. CAPE: Encoding relative positions with continuous augmented positional embeddings. In Advances in Neural Information Processing Systems.</p>
<p>David Lindner, János Kramár, Matthew Rahtz, Thomas McGrath, and Vladimir Mikulik. 2023. Tracr: Compiled transformers as a laboratory for interpretability. ArXiv, abs/2301.05062.</p>
<p>Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V. Le, Barret Zoph, Jason Wei, and Adam Roberts. 2023. The flan collection: Designing data and methods for effective instruction tuning. ArXiv, abs/2301.13688.</p>
<p>Ziyang Luo, Artur Kulmizev, and Xiaoxi Mao. 2021. Positional artefacts propagate through masked language model embeddings. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 5312-5327, Online. Association for Computational Linguistics.</p>
<p>Maxwell Nye, Anders Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. 2021. Show your work: Scratchpads for intermediate computation with language models. ArXiv, abs/2112.00114.</p>
<p>Santiago Ontanon, Joshua Ainslie, Zachary Fisher, and Vaclav Cvicek. 2022. Making transformers solve compositional tasks. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3591-3607, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. ArXiv, abs/2203.02155.</p>
<p>Sejun Park, Chulhee Yun, Jaeho Lee, and Jinwoo Shin. 2020. Minimum width for universal approximation. ArXiv.</p>
<p>Ofir Press, Noah A. Smith, and Mike Lewis. 2022. Train short, test long: Attention with linear biases enables input length extrapolation. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.</p>
<p>Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI Blog, 1(8):9.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21:140:1-140:67.</p>
<p>David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. 2019. Analysing mathematical reasoning abilities of neural models. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.</p>
<p>Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. 2022a. Bloom: A 176b-parameter open-access multilingual language model. ArXiv, abs/2211.05100.</p>
<p>Teven Le Scao, Thomas Wang, Daniel Hesslow, Lucile Saulnier, Stas Bekman, M Saiful Bari, Stella Biderman, Hady Elsahar, Jason Phang, Ofir Press, Colin Raffel, Victor Sanh, Sheng Shen, Lintang Sutawika, Jaesung Tae, Zheng Xin Yong, Julien Launay, and Iz Beltagy. 2022b. What language model to train if you have one million GPU hours? In Challenges \&amp; Perspectives in Creating Large Language Models.</p>
<p>Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-attention with relative position representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 464-468, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang, Shirui Pan, and Chengqi Zhang. 2018. Disan: Directional self-attention network for rnn/cnn-free language understanding. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, pages 5446-5455. AAAI Press.</p>
<p>Koustuv Sinha, Amirhossein Kazemnejad, Siva Reddy, Joelle Pineau, Dieuwke Hupkes, and Adina Williams. 2022. The curious case of absolute position embeddings. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 4449-4472, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Koustuv Sinha, Shagun Sodhani, Jin Dong, Joelle Pineau, and William L. Hamilton. 2019. CLUTRR: A diagnostic benchmark for inductive reasoning from text. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4506-4515, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. 2021. Roformer: Enhanced transformer with rotary position embedding. ArXiv, abs/2104.09864.</p>
<p>Yi Tay, Mostafa Dehghani, Jinfeng Rao, William Fedus, Samira Abnar, Hyung Won Chung, Sharan Narang, Dani Yogatama, Ashish Vaswani, and Donald Metzler. 2022. Scale efficiently: Insights from pretraining and finetuning transformers. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.</p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. 2022. Galactica: A large language model for science. ArXiv, abs/2211.09085.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. LLaMA: Open and efficient foundation language models. ArXiv, abs/2302.13971.</p>
<p>Yao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, and Ruslan Salakhutdinov. 2019. Transformer dissection: An unified understanding for transformer's attention via the lens of kernel. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4344-4353, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 5998-6008.</p>
<p>Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, et al. 2022. Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks. In Proc. of EMNLP, pages 5085-5109, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. 2022a. Finetuned language models are zero-shot learners. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. 2022b. Chain of thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems.</p>
<p>Gail Weiss, Yoav Goldberg, and Eran Yahav. 2021. Thinking like transformers. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 11080-11090. PMLR.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online. Association for Computational Linguistics.</p>
<p>Baosong Yang, Longyue Wang, Derek F. Wong, Lidia S. Chao, and Zhaopeng Tu. 2019. Assessing the ability of self-attention networks to learn word order. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3635-3644, Florence, Italy. Association for Computational Linguistics.</p>
<p>Gilad Yehudai, Ethan Fetaya, Eli A. Meirom, Gal Chechik, and Haggai Maron. 2021. From local structures to size generalization in graph neural networks. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 11975-11986. PMLR.</p>
<p>Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. Opt: Open pre-trained transformer language models. ArXiv, abs/2205.01068.</p>
<p>Yi Zhang, Arturs Backurs, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar, and Tal Wagner. 2023. Unveiling transformers with lego: a synthetic reasoning task. ArXiv, abs/2206.04301.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure A.1: Histogram of instruction lengths in two instruction finetuning datasets: FLAN (CoT subset) (Longpre et al., 2023) and Super Natural Instructions (Wang et al., 2022). The dotted line indicates the median length of the instructions in each dataset.</p>
<h1>A Number of instances decreases rapidly as sequence length grows</h1>
<p>The recent trend of SFT-RLHF pipeline (Ouyang et al., 2022) relies on finetuning LLMs on the instruction following tasks. However, the training data of these datasets is often skewed towards shorter sequences. Figure A. 1 shows the distribution of instruction lengths in two instruction finetuning datasets: FLAN (CoT subset) (Longpre et al., 2023) and Super Natural Instructions (Wang et al., 2022). The median length of instructions in these datasets is quite short compared to the maximum length that exists. Such distribution shape highlights the importance of length generalization in these tasks. In fact, the models are supposed to learn from short instructions and generalize to ones during inference that might be much longer.</p>
<h2>B Background</h2>
<h2>B. 1 Preliminaries</h2>
<p>In this section, we lay the groundwork and introduce the notation we use throughout the paper. We will refer to this in Appendices C. 1 and C.2.
Let $f_{\theta}$ be a decoder-only Transformer model, where $\theta$ denotes the full set of model parameters. $f_{\theta}$ processes the input sequence $\boldsymbol{x}=\left[x_{0}, x_{1}, \ldots, x_{T}\right]$ and maps it to the output sequence $\boldsymbol{y}=$ $\left[y_{0}, y_{1}, \ldots, y_{T}\right]$ by applying a sequence of Transformer layers. Note that being decoder-only means the attention mechanism in each layer is causal, i.e. the attention weights are computed based on the previous positions only.
The layer $\operatorname{TLayer}^{(l)}\left(\boldsymbol{H}^{(l-1)} ; \theta_{l}\right)$, consisting of self-attention heads and a feed-forward sub-layer, reads the previous hidden state $\boldsymbol{H}^{(l-1)}$ and produces the hidden state at layer $l: \boldsymbol{H}^{l}$, where $l$ is the layer index, and $\theta_{l}$ is the set of parameters of the $l$-th layer. Each hidden state $\boldsymbol{H}^{(l)} \in \mathbb{R}^{d \times(T+1)}$ is matrix where column $t$, denoted as $\boldsymbol{h}<em l="l">{t}^{(l)}$, is the hidden state at position $t$.
A layer $l$ is parameterized by a set of parameters $\theta</em>}=\left{\left(\boldsymbol{W<em K="K">{Q}^{m}, \boldsymbol{W}</em>}^{m}, \boldsymbol{W<em O="O">{V}^{m}, \boldsymbol{W}</em>\right)}^{m<em 1="1">{m}, \boldsymbol{W}</em>}, \boldsymbol{W<em Q="Q">{2}\right}$, where $\boldsymbol{W}</em>}^{m}, \boldsymbol{W<em V="V">{K}^{m}, \boldsymbol{W}</em>}^{m} \in \mathbb{R}^{h \times d}$ and $\boldsymbol{W<em 1="1">{O}^{m} \in \mathbb{R}^{d \times h}$ are the query, key, value, and output matrices of the $m$-th head, respectively. $\boldsymbol{W}</em>$ ), and $k$ is a multiplier of the hidden state size in the feed-forward sub-layer (it is usually set to 4 in common implementations of the Transformer). Note that we drop the layer index $l$ and the attention head index $m$ where it is clear from the context.
The Transformer layer $\operatorname{TLayer}^{(l)}$ processes each column of $\boldsymbol{H}^{(l-1)}$ independently and in parallel to produce the output. The computation of the $t$-th column of $\boldsymbol{H}^{(l)}$ is as follows:}, \boldsymbol{W}_{2} \in \mathbb{R}^{d \times k . d}$ are the weight matrices of the feed-forward sub-layer. $d$ denotes the model's hidden state size, $h$ is the attention dimension (where $h=\frac{d}{#\text { heads }</p>
<p>$$
\boldsymbol{h}<em t="t">{t}^{(l)}=\operatorname{FF}\left(\lambda\left(\boldsymbol{a}</em>}+\boldsymbol{h<em t="t">{t}^{(l-1)}\right)\right)+\boldsymbol{a}</em>
$$}+\boldsymbol{h}_{t}^{(l-1)</p>
<p>where FF is the feed-forward sub-layer, $\lambda$ is layer normalization, and $\boldsymbol{a}<em t="t">{t} \in \mathbb{R}^{d}$ is the output of the multi-head self-attention sub-layer at position $t$. Specifically, $\boldsymbol{a}</em>$ is computed as:</p>
<p>$$
\boldsymbol{a}<em m="m">{t}=\sum</em>\right)
$$} \operatorname{Attn}^{(m)}\left(\boldsymbol{h}_{t}^{(l-1)}, \boldsymbol{H}^{(l-1)</p>
<p>where $\operatorname{Attn}^{(m)}$ is the $m$-th attention head. Let $\boldsymbol{o}<em t="t">{t} \in \mathbb{R}^{d}$ denote the output of an attention head at position $t$. Then, $\boldsymbol{o}</em>$ is computed as:</p>
<p>$$
\boldsymbol{o}<em O="O">{t}=\boldsymbol{W}</em>}\left(\sum_{i \leq t} \hat{\boldsymbol{\alpha}<em i="i">{i} \boldsymbol{v}</em>\right)
$$</p>
<p>where $\hat{\boldsymbol{\alpha}}=\operatorname{softmax}(\boldsymbol{\alpha}) \in \mathbb{R}^{(t+1)}$, and $\boldsymbol{\alpha}$ is the attention weight vector such that:</p>
<p>$$
\boldsymbol{\alpha}=\left[\left\langle\boldsymbol{q}<em 0="0">{t}, \boldsymbol{k}</em>}\right\rangle,\left\langle\boldsymbol{q<em 1="1">{t}, \boldsymbol{k}</em>}\right\rangle, \ldots,\left\langle\boldsymbol{q<em t="t">{t}, \boldsymbol{k}</em>
$$}\right\rangle\right]^{\top</p>
<p>where $\boldsymbol{q}<em Q="Q">{t}=\boldsymbol{W}</em>} \boldsymbol{h<em i="i">{t}^{(l-1)} \in \mathbb{R}^{h}, \boldsymbol{k}</em>}=\boldsymbol{W<em i="i">{K} \boldsymbol{h}</em>}^{(l-1)} \in \mathbb{R}^{h}$, and $\boldsymbol{v<em V="V">{i}=\boldsymbol{W}</em> .\langle\cdot, \cdot\rangle$ denotes the dot product operation.
The feed-forward sub-layer $\mathrm{FF}(\cdot) \in \mathbb{R}^{d}$ is a two-layer MLP:} \boldsymbol{h}_{i}^{(l-1)} \in \mathbb{R}^{h</p>
<p>$$
\mathrm{FF}(\boldsymbol{x})=\boldsymbol{W}<em 1="1">{2} \sigma\left(\boldsymbol{W}</em>\right)
$$}^{\top} \boldsymbol{x</p>
<p>where $\sigma$ is a non-linear activation function (usually ReLU or GeLU (Hendrycks and Gimpel, 2020)). Additionally, $\lambda(\cdot) \in \mathbb{R}^{d}$ is layer normalization (Ba et al., 2016). Note that we take the additive (Elhage et al., 2021) view of attention heads in Equation (4) instead of concatenate and multiple view (Vaswani et al., 2017) as it is easier to understand and analyze. But, they are mathematically equivalent (Elhage et al., 2021).
The hidden state is initialized with a learned embedding of the input sequence $\boldsymbol{H}^{(0)}=\boldsymbol{W}<em E="E">{E} \boldsymbol{X}$, where $\boldsymbol{W}</em>$ is the one-hot encoded input sequence. $V$ is the vocabulary size.} \in \mathbb{R}^{d \times V}$ is the embedding matrix and $\boldsymbol{X} \in \mathbb{R}^{V \times(T+1)</p>
<h1>B. 2 Positional Encoding</h1>
<p>Almost all positional encoding methods can be explained and formulated as how they implement the dot product operation in Equation (6). So, in this section, we explain how the dot product $\left\langle\boldsymbol{q}<em i="i">{t}, \boldsymbol{k}</em>\right\rangle$ is implemented in different positional encoding schemes.</p>
<p>Absolute Positional Encoding (APE) The process of Absolute Positional Encoding (APE) involves assigning a position vector $\boldsymbol{p}_{i}$ to each absolute position $i$ and combining them with word embeddings before inputting them into the model. So, APE first modifies how the hidden state is initialized:</p>
<p>$$
\boldsymbol{H}^{(0)}=\boldsymbol{W}<em P="P">{E} \boldsymbol{X}+\boldsymbol{W}</em>
$$} \boldsymbol{P</p>
<p>where $\boldsymbol{W}<em p="p">{P} \in \mathbb{R}^{d \times T}$ is the positional embedding matrix and $\boldsymbol{P} \in \mathbb{R}^{V</em>$ is the maximum absolute position. Therefore, the hidden state at column $j$ is:} \times(T+1)}$ is the one-hot encoded absolute position sequence. $V_{p</p>
<p>$$
\boldsymbol{h}<em j="j">{j}^{(0)}=\boldsymbol{e}</em>
$$}+\boldsymbol{p}_{j</p>
<p>where $\boldsymbol{e}<em j="j">{j} \in \mathbb{R}^{d}$ is the word embedding of token $x</em>$ is the positional embedding for position $j$. Then, the dot product for the first layer in Equation (6) is computed as:}$ and $\boldsymbol{p}_{j} \in \mathbb{R}^{d</p>
<p>$$
\begin{aligned}
\left\langle\boldsymbol{q}<em i="i">{t}, \boldsymbol{k}</em>}\right\rangle= &amp; \left\langle\boldsymbol{W<em t="t">{Q} \boldsymbol{h}</em>}^{(0)}, \boldsymbol{W<em i="i">{K} \boldsymbol{h}</em>\right\rangle \
= &amp; \left\langle\boldsymbol{W}}^{(0)<em t="t">{Q}\left(\boldsymbol{e}</em>}+\boldsymbol{p<em K="K">{t}\right), \boldsymbol{W}</em>}\left(\boldsymbol{e<em i="i">{i}+\boldsymbol{p}</em>\right)\right\rangle \
= &amp; \left(\boldsymbol{W}<em t="t">{Q}\left(\boldsymbol{e}</em>}+\boldsymbol{p<em K="K">{t}\right)\right)^{\top}\left(\boldsymbol{W}</em>}\left(\boldsymbol{e<em i="i">{i}+\boldsymbol{p}</em>\right)\right) \
= &amp; \boldsymbol{e}<em Q="Q">{t}^{\top} \boldsymbol{W}</em>}^{\top} \boldsymbol{W<em i="i">{K} \boldsymbol{e}</em>}+\boldsymbol{e<em Q="Q">{t}^{\top} \boldsymbol{W}</em>}^{\top} \boldsymbol{W<em i="i">{K} \boldsymbol{p}</em> \
&amp; +\boldsymbol{p}<em Q="Q">{t}^{\top} \boldsymbol{W}</em>}^{\top} \boldsymbol{W<em i="i">{K} \boldsymbol{e}</em>}+\boldsymbol{p<em Q="Q">{t}^{\top} \boldsymbol{W}</em>}^{\top} \boldsymbol{W<em i="i">{K} \boldsymbol{p}</em>
\end{aligned}
$$</p>
<p>In the learned variant of APE, $\boldsymbol{p}<em j="j">{j} \in \mathbb{R}^{d}$ is learned during training. In the sinusoidal variant, $\boldsymbol{p}</em>$ is computed as:}$ is calculated using a non-parametric function. Specifically, $\boldsymbol{p}_{j</p>
<p>$$
\boldsymbol{p}<em 1="1">{j}=\left[\sin \left(\omega</em>
$$} \cdot j\right), \cos \left(\omega_{1} \cdot j\right), \sin \left(\omega_{2} \cdot j\right), \cos \left(\omega_{2} \cdot j\right), \ldots, \sin \left(\omega_{d / 2} \cdot j\right), \cos \left(\omega_{d / 2} \cdot j\right)\right]^{\top</p>
<p>where $\omega_{i}=\frac{1}{10000^{d / / d}}$.
T5's Relative PE The Relative bias in T5 is a type of relative positional encoding that initially calculates the relative distance $(t-i)$ between tokens at positions $t$ and $i$. This distance is then transformed into a scalar bias value $b$ and is incorporated into the dot product between the query and key. $b$ is learned during training. Thus, the dot product in every layer can be written as:</p>
<p>$$
\left\langle\boldsymbol{q}<em i="i">{t}, \boldsymbol{k}</em>}\right\rangle=\boldsymbol{q<em i="i">{t}^{\top} \boldsymbol{k}</em>
$$}+b_{\operatorname{bucket}(n-m)</p>
<p>where</p>
<p>$$
\operatorname{bucket}(n)= \begin{cases}n &amp; \text { if } n&lt;\frac{\mathcal{B}}{2} \ \frac{\mathcal{B}}{2}+\left\lfloor\frac{\log \left(g_{i / 2}^{b_{i}}\right)}{\log \left(\frac{g_{i}}{g_{i / 2}}\right)} \times \frac{\mathcal{B}}{2}\right\rfloor &amp; \text { if } \frac{\mathcal{B}}{2} \leq n&lt;\mathcal{D} \ \mathcal{B}-1 &amp; \text { if } n \geq \mathcal{D}\end{cases}
$$</p>
<p>This function maps the relative distance $d$ to a bucket index, which will be used to look up the weight corresponding to that bucket. $\mathcal{B}$ is the number of buckets, $\mathcal{D}$ is the maximum distance. It assigns half of the buckets to distances smaller than $\frac{\mathcal{D}}{2}$ with linear spacing and the other half to distances larger than $\frac{\mathcal{D}}{2}$ with logarithmic spacing. The weight for distances larger than $\mathcal{D}$ is the same. This is to facilitate generalization to unseen distances. In the original implementation of T5, $\mathcal{B}=32$ and $\mathcal{D}=128$. Following shows an example of the bucket function with $\mathcal{B}=5$ and $\mathcal{D}=6$ :</p>
<p>$$
\operatorname{bucket}\left(\left[\begin{array}{llllllllll}
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \
1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \
2 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \
3 &amp; 2 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \
4 &amp; 3 &amp; 2 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \
5 &amp; 4 &amp; 3 &amp; 2 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \
6 &amp; 5 &amp; 4 &amp; 3 &amp; 2 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \
7 &amp; 6 &amp; 5 &amp; 4 &amp; 3 &amp; 2 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \
8 &amp; 7 &amp; 6 &amp; 5 &amp; 4 &amp; 3 &amp; 2 &amp; 1 &amp; 0 &amp; 0 \
9 &amp; 8 &amp; 7 &amp; 6 &amp; 5 &amp; 4 &amp; 3 &amp; 2 &amp; 1 &amp; 0
\end{array}\right]\right)=\left[\begin{array}{llllllllll}
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \
1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \
2 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \
3 &amp; 2 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \
3 &amp; 3 &amp; 2 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \
4 &amp; 3 &amp; 3 &amp; 2 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \
4 &amp; 4 &amp; 3 &amp; 3 &amp; 2 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \
4 &amp; 4 &amp; 4 &amp; 3 &amp; 3 &amp; 2 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \
4 &amp; 4 &amp; 4 &amp; 4 &amp; 3 &amp; 3 &amp; 2 &amp; 1 &amp; 0 &amp; 0 \
4 &amp; 4 &amp; 4 &amp; 4 &amp; 4 &amp; 3 &amp; 3 &amp; 2 &amp; 1 &amp; 0
\end{array}\right]\right]
$$</p>
<p>ALiBi Similar to T5's Relative PE, ALiBi subtracts a scalar bias from the attention score. As the distance between the query and key tokens increases, the bias grows linearly. Specifically, the dot product in every layer can be written as:</p>
<p>$$
\left\langle\boldsymbol{q}<em i="i">{t}, \boldsymbol{k}</em>}\right\rangle=\boldsymbol{q<em i="i">{t}^{\top} \boldsymbol{k}</em>
$$}-(t-i) \cdot C^{(m+1)</p>
<p>where $m$ is head index and $C$ is a constant defined as:</p>
<p>$$
C=2^{-2^{-\log _{2}(\text { * head }+3)}}
$$</p>
<p>For example, if the number of heads is 8 , then we have $\frac{1}{2}, \frac{1}{2^{2}}, \ldots, \frac{1}{2^{8}}$ (Press et al., 2022).
Rotary The Rotary is a relative PE that applies a rotation to the query and key representations based on their absolute positions before dot product attention. Due to this rotation, the attention dot product relies solely on the relative distance between tokens.
First, we formulate Rotary for model dimension $d=2$. Rotary positional encoding defines the dot product as:</p>
<p>$$
\begin{aligned}
\left\langle\boldsymbol{q}<em i="i">{t}, \boldsymbol{k}</em>}\right\rangle &amp; =\left\langle\operatorname{Rot}\left(\boldsymbol{q<em i="i">{t}, t\right), \operatorname{Rot}\left(\boldsymbol{k}</em>, i\right)\right\rangle \
&amp; =\left\langle\boldsymbol{R}^{t \theta} \boldsymbol{q}<em i="i">{t}, \boldsymbol{R}^{i \theta} \boldsymbol{k}</em>\right\rangle \
&amp; =\left(\boldsymbol{R}^{t \theta} \boldsymbol{q}<em i="i">{t}\right)^{\intercal}\left(\boldsymbol{R}^{i \theta} \boldsymbol{k}</em>\right) \
&amp; =\boldsymbol{q}<em i="i">{t}^{\intercal}\left(\boldsymbol{R}^{t \theta}\right)^{\intercal} \boldsymbol{R}^{i \theta} \boldsymbol{k}</em> \
&amp; =\boldsymbol{q}<em i="i">{t}^{\intercal} \boldsymbol{R}^{(i-t) \theta} \boldsymbol{k}</em>
\end{aligned}
$$</p>
<p>where $\boldsymbol{R}^{t \theta}$ is a rotation matrix that rotates $\boldsymbol{x}$ by $t \theta$ radians:</p>
<p>$$
\boldsymbol{R}^{n \theta}=\left[\begin{array}{cc}
\cos (n \theta) &amp; -\sin (n \theta) \
\sin (n \theta) &amp; \cos (n \theta)
\end{array}\right]
$$</p>
<p>for $d&gt;2$, Rotary applies the same approach on every two consecutive dimensions of $\boldsymbol{q}<em i="i">{t}$ and $\boldsymbol{k}</em>$, but with different $\theta$ angles. Refer to Su et al. (2021) for the exact formulation.</p>
<p>NoPE NoPE does not explicitly encode positional encodings. So, the dot product in every layer can be written as:</p>
<p>$$
\left\langle\boldsymbol{q}<em i="i">{t}, \boldsymbol{k}</em>}\right\rangle=\boldsymbol{q<em i="i">{t}^{\intercal} \boldsymbol{k}</em>
$$</p>
<h1>C Proofs</h1>
<p>In this section, we provide proof of why NoPE can implicitly learn both absolute and relative positions. We refer the readers to Appendix B. 1 for the notation and definitions used in this section.</p>
<h2>C. 1 Absolute Positional Encoding in NoPE</h2>
<p>This section discusses how NoPE can recover absolute positions in the hidden state. Our proof is inspired by Weiss et al. (2021); Lindner et al. (2023) and relies on the causal attention mask in the decoder-only Transformer and the softmax function to recover absolute positions.</p>
<p>Theorem 1 (Absolute Encoding). Let $\boldsymbol{x}=\left[\langle b o s\rangle, x_{1}, \ldots, x_{T}\right]$ be an input sequence of length $T+1$ to the model. Then, the first layer of $f_{\theta}$ can recover absolute positions $[1, \ldots, T+1]$ in the hidden state $\boldsymbol{H}^{(1)}$. That is, there exist $\boldsymbol{W}<em K="K">{Q}, \boldsymbol{W}</em>}, \boldsymbol{W<em O="O">{V}, \boldsymbol{W}</em>}, \boldsymbol{W<em 2="2">{1}$, and $\boldsymbol{W}</em>$ such that the self-attention and feedforward operations in the first layer compute absolute positions and write it to the next hidden state.</p>
<h2>Proof.</h2>
<p>Our proof only specifies the weights of a single attention head in the first layer (and additionally the parameterization of feedforward sub-layer). In this parameterization, we only require the first three dimensions of the hidden states. The rest of the heads, as long as they do not override the first three dimensions, can be arbitrary. This does not impose any challenges as Transformers used in practice usually have a very large model dimension $d$. In the rest, we provide the construction of the weights and then verify that they can recover absolute positions.
First, we construct the word embedding matrix $\boldsymbol{W}<em E="E">{E} \in \mathbb{R}^{d \times V}$, where each column is the embedding of a token in the vocabulary. We construct $\boldsymbol{W}</em>$ such that it always sets the first dimension of every embedding vector to be 1 . Additionally, it sets the second dimension to 1 if and only if the token is <bos>. Otherwise, it sets it to zero. The third dimension of all embedding vectors is set to zero. Other dimensions can take any arbitrary values. Without loss of generality, assume <bos> is the first token in the vocabulary, i.e. The first column. Then, we have:</p>
<p>$$
\boldsymbol{W}<em 4_1="4,1">{E}=\left[\begin{array}{ccccc}
1 &amp; 1 &amp; 1 &amp; \ldots &amp; 1 \
1 &amp; 0 &amp; 0 &amp; \ldots &amp; 0 \
0 &amp; 0 &amp; 0 &amp; \ldots &amp; 0 \
e</em> \
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \
e_{d, 1} &amp; e_{d, 2} &amp; e_{d, 2} &amp; \ldots &amp; e_{d, V}
\end{array}\right]_{d \times V}
$$} &amp; e_{4,2} &amp; e_{4,3} &amp; \ldots &amp; e_{4, V</p>
<p>where $e_{d, i} \in \mathbb{R}$.
Secondly, for head dimensions $h \geq 1$, we construct the weights $\boldsymbol{W}<em K="K">{Q}, \boldsymbol{W}</em>}, \boldsymbol{W<em O="O">{V}, \boldsymbol{W}</em>$ of the first attention head in the first layer. Specifically,</p>
<p>$$
\boldsymbol{W}<em _times="\times" d="d" h="h">{K}=\left[\begin{array}{cccc}
1 &amp; 0 &amp; \ldots &amp; 0 \
1 &amp; 0 &amp; \ldots &amp; 0 \
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \
1 &amp; 0 &amp; \ldots &amp; 0
\end{array}\right]</em>} \quad \boldsymbol{W<em _times="\times" d="d" h="h">{V}=\left[\begin{array}{cccc}
0 &amp; 1 &amp; 0 &amp; \ldots &amp; 0 \
0 &amp; 0 &amp; 0 &amp; \ldots &amp; 0 \
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \
0 &amp; 0 &amp; 0 &amp; \ldots &amp; 0
\end{array}\right]</em>
$$</p>
<p>$\boldsymbol{W}<em V="V">{K}$ reads from the first dimension of the hidden state, which is initialized with 1 using the embedding matrix. Since all word embeddings have one in their first dimension, this parameterization will result all key vectors to be the same. Moreover, $\boldsymbol{W}</em>$ reads from the second dimension of the hidden state, which is initialized with 1 if the token is <bos>. So, the value vector will have 1 in its first dimension only if the corresponding token is <bos>.
$\boldsymbol{W}<em O="O">{Q}$ can be any arbitrary matrix. $\boldsymbol{W}</em>$ will write the result of the attention to the third dimension of the hidden state and can be constructed as:</p>
<p>$$
\boldsymbol{W}<em _times="\times" d="d" h="h">{O}=\left[\begin{array}{cccccc}
0 &amp; 0 &amp; 0 &amp; 0 &amp; \ldots &amp; 0 \
0 &amp; 0 &amp; 0 &amp; 0 &amp; \ldots &amp; 0 \
1 &amp; 0 &amp; 0 &amp; 0 &amp; \ldots &amp; 0 \
0 &amp; 0 &amp; 0 &amp; 0 &amp; \ldots &amp; 0 \
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \
0 &amp; 0 &amp; 0 &amp; 0 &amp; \ldots &amp; 0
\end{array}\right]</em>
$$</p>
<p>Now, we verify that for any input sequence $\boldsymbol{x}=\left[\begin{array}{llll}\langle\operatorname{bos}\rangle, x_{1}, \ldots, x_{T}\end{array}\right]$, the first layer can recover absolute positions $[1, \ldots, T+1]$ in the hidden state $\boldsymbol{H}^{(1)}$. We verify this for column $t$ of $\boldsymbol{H}^{(1)}$. That is, we show that absolute position information is available in the third dimension of $\boldsymbol{h}<em E="E">{t}^{(1)}$.
First, we use the word embedding matrix $\boldsymbol{W}</em>$ :}$ to compute the embedding $\boldsymbol{H}^{(0)</p>
<p>$$
\boldsymbol{H}^{(0)}=\boldsymbol{W}<em 4_1="4,1">{E} \boldsymbol{X}=\left[\begin{array}{ccccc}
1 &amp; 1 &amp; 1 &amp; \ldots &amp; 1 \
1 &amp; 0 &amp; 0 &amp; \ldots &amp; 0 \
0 &amp; 0 &amp; 0 &amp; \ldots &amp; 0 \
e</em> \
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \
e_{d, 1} &amp; e_{d, 2} &amp; e_{d, 2} &amp; \ldots &amp; e_{d, V}
\end{array}\right]_{d \times(T+1)}
$$} &amp; e_{4,2} &amp; e_{4,3} &amp; \ldots &amp; e_{4, V</p>
<p>We now provide the attention computation at position $1 \leq t \leq T+1$. First, we use $\boldsymbol{W}<em t="t">{Q}$ to compute the query vector $\boldsymbol{q}</em>}$ by applying $\boldsymbol{q<em Q="Q">{t}=\boldsymbol{W}</em>$ :} \boldsymbol{h}_{t}^{(0)</p>
<p>$$
\boldsymbol{q}<em 1="1">{t}=\left[q</em>
$$}, q_{2}, q_{3}, \ldots, q_{h}\right]^{\boldsymbol{\top}</p>
<p>Recall that $\boldsymbol{W}<em j="j">{Q}$ can be any arbitrary matrix. So, $q</em>} \in \mathbb{R}$ can take any arbitrary value. Next, we compute the key vectors by applying $\boldsymbol{k<em K="K">{i}=\boldsymbol{W}</em>$ :} \boldsymbol{h}_{i}^{(0)</p>
<p>$$
\boldsymbol{k}<em 2="2">{1}=\left(\begin{array}{l}
1 \
1 \
\vdots \
1
\end{array}\right) \quad \boldsymbol{k}</em>
1 \
1 \
\vdots \
1
\end{array}\right) \quad \ldots \quad \boldsymbol{k}_{t}=\left(\begin{array}{l}
1 \
1 \
\vdots \
1
\end{array}\right)
$$}=\left(\begin{array}{l</p>
<p>Note that all key vectors are the same and we only need to compute them up to position $t$ as the attention mask is causal, i.e query can only look at positions $\leq t$. Next, we compute the attention weight vectors $\boldsymbol{\alpha}$ :</p>
<p>$$
\begin{aligned}
\boldsymbol{\alpha} &amp; =\left[\left\langle\boldsymbol{q}<em 1="1">{t}, \boldsymbol{k}</em>}\right\rangle,\left\langle\boldsymbol{q<em 2="2">{t}, \boldsymbol{k}</em>}\right\rangle, \ldots,\left\langle\boldsymbol{q<em t="t">{t}, \boldsymbol{k}</em> \
&amp; =\left[\alpha^{}\right\rangle\right]^{\boldsymbol{\top}<em>}, \alpha^{</em>}, \ldots, \alpha^{*}\right]^{\boldsymbol{\top}}
\end{aligned}
$$</p>
<p>where $\alpha^{*}=q_{1}+q_{2}+\ldots+q_{h}$. Next, we apply softmax to compute the attention probabilities. Since all $\boldsymbol{\alpha}^{i}$ 's are the same, we have:</p>
<p>$$
\hat{\boldsymbol{\alpha}}=\operatorname{softmax}(\boldsymbol{\alpha})=\left[\frac{1}{t}, \frac{1}{t}, \ldots, \frac{1}{t}\right]^{\top}
$$</p>
<p>Now, we compute the value vectors by applying $\boldsymbol{v}<em V="V">{i}=\boldsymbol{W}</em>$ :} \boldsymbol{h}_{i}^{(0)</p>
<p>$$
\boldsymbol{v}<em 2="2">{1}=\left(\begin{array}{c}
1 \
0 \
\vdots \
0
\end{array}\right) \quad \boldsymbol{v}</em>
0 \
0 \
\vdots \
0
\end{array}\right) \quad \ldots \quad \boldsymbol{v}_{t}=\left(\begin{array}{c}
0 \
0 \
\vdots \
0
\end{array}\right)
$$}=\left(\begin{array}{c</p>
<p>Finally, we compute the output of the attention head by applying $\boldsymbol{W}_{O}$ :</p>
<p>$$
\boldsymbol{o}<em O="O">{t}=\boldsymbol{W}</em>}\left(\sum_{i \leq t} \hat{\boldsymbol{\alpha}<em i="i">{i} \boldsymbol{v}</em>}\right)=\boldsymbol{W<em _leq="\leq" i="i" t="t">{O}\left(\frac{1}{t} \sum</em>} \boldsymbol{v<em O="O">{i}\right)=\boldsymbol{W}</em>
1 / t \
0 \
\vdots \
0
\end{array}\right)}\left(\begin{array}{c<em d="d">{h}=\left(\begin{array}{c}
0 \
0 \
1 / t \
0 \
\vdots \
0
\end{array}\right)</em>
$$</p>
<p>Thus, the output of our constructed attention head recovers the absolute position information and writes it to the third dimension of output.
We used the decoder-only property of Transformer implicitly in Equation (23), which helped us to only attend to position $\leq t$. So, the lengths of the attended sequence are always $t$. Moreover, the presence of <bos> token in the input sequence helped us to anchor the absolute position information. This is not a problem as in practice models are often prompted with some instructions which can act as <bos> token.
With this information available to the rest of the network, the feedforward sub-layer, with sufficient hidden width, can recover the absolute positions $[1,2, \ldots, T+1]$ from the third dimension of attention output. This is because the feedforward sub-layer is MLP with ReLU activation. So, it can learn any arbitrary function (Park et al., 2020). Note that the layer-norm operation can be bypassed as explained by Akyurek et al. (2023).</p>
<h1>C. 2 Relative Positional Encoding in NoPE</h1>
<p>In this section, we show if the hidden state contains absolute positional information as explained in the previous section, then the attention mechanism in all subsequent layers can implement a relative positional encoding. We refer the readers to Appendices B. 1 and C. 1 for the notation and definitions used in this section.</p>
<p>Theorem 2 (Relative Encoding). Suppose that the hidden state $\boldsymbol{H}^{(1)}$ contains absolute positional information, as stated in Theorem 1, and assume that it is not overwritten by any subsequent layers. Then, the self-attention in all subsequent layers can implement a relative positional encoding: there exists a parameterization of $f_{\theta}$ such that, for $l \geq 2$, the attention dot product between query $\boldsymbol{q}<em i="i">{t}$ and key $\boldsymbol{k}</em>$ at positions $t$ and $i(t \geq i)$ can be expressed as:</p>
<p>$$
\left\langle\boldsymbol{q}<em i="i">{t}, \boldsymbol{k}</em>}\right\rangle=f_{\mathrm{ent}}\left(\boldsymbol{q<em i="i">{t}, \boldsymbol{k}</em>(t-i)
$$}\right)+f_{\mathrm{rel}</p>
<p>where $f_{\text {ent }}$ is a function of their content, and $f_{\text {rel }}$ is a function of their relative distance.</p>
<h2>Proof.</h2>
<p>Our proof only specifies a few entries of weight matrices for attention heads in layers $l \geq 2$, which does not impose any challenges for Transformers used in practice as they usually have a very large model dimension $d$. Moreover, we require to have absolute positions in the third dimension of the hidden state as explained in Theorem 1. To show NoPE can implement relative encoding, we only need to prove that its attention dot product depends on the relative distance between tokens (See</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Since using scratchpad creates very long sequences, we follow Nye et al. (2021) and set the length threshold $L=8$ for tasks that use it to avoid out-of-memory errors.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-519 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-519</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-519</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-15.html">extraction-schema-15</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <p><strong>Paper ID:</strong> paper-c305ab1bdba79442bec72ec7f5c5ee7c49c2a566</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c305ab1bdba79442bec72ec7f5c5ee7c49c2a566" target="_blank">Visual Language Maps for Robot Navigation</a></p>
                <p><strong>Paper Venue:</strong> IEEE International Conference on Robotics and Automation</p>
                <p><strong>Paper TL;DR:</strong> VLMaps is a spatial map representation that directly fuses pretrained visual-language features with a 3D reconstruction of the physical world and can be shared among multiple robots with different embodiments to generate new obstacle maps on-the-fly.</p>
                <p><strong>Paper Abstract:</strong> Grounding language to the visual observations of a navigating agent can be performed using off-the-shelf visual-language models pretrained on Internet-scale data (e.g., image captions). While this is useful for matching images to natural language descriptions of object goals, it remains disjoint from the process of mapping the environment, so that it lacks the spatial precision of classic geometric maps. To address this problem, we propose VLMaps, a spatial map representation that directly fuses pretrained visual-language features with a 3D reconstruction of the physical world. VLMaps can be autonomously built from video feed on robots using standard exploration approaches and enables natural language indexing of the map without additional labeled data. Specifically, when combined with large language models (LLMs), VLMaps can be used to (i) translate natural language commands into a sequence of open-vocabulary navigation goals (which, beyond prior work, can be spatial by construction, e.g., “in between the sofa and the TV” or “three meters to the right of the chair”) directly localized in the map, and (ii) can be shared among multiple robots with different embodiments to generate new obstacle maps on-the-fly (by using a list of obstacle categories). Extensive experiments carried out in simulated and real-world environments show that VLMaps enable navigation according to more complex language instructions than existing methods. Videos are available at https://vlmaps.github.io.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e519.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e519.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VLMaps</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Visual-Language Maps</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A spatial top-down grid map representation that fuses pretrained visual-language (VLM) pixel embeddings into a 3D reconstruction so that free-form natural language can be localized to precise map coordinates for navigation and planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>VLMaps (system)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not a single neural model but a pipeline: LSeg/CLIP visual-language pixel embeddings are extracted per RGB frame, back-projected into world coordinates from RGB-D + odometry, and averaged into a top-down H×W×C grid (scale s). This produces an explicit spatial map where each cell contains a CLIP-compatible embedding enabling open-vocabulary language indexing.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Open-vocabulary spatial and multi-object navigation (zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Build a persistent top-down spatial map from RGB-D video and odometry; localize open-vocabulary landmarks (text queries) by matching CLIP-text embeddings to map-cell embeddings; generate obstacle maps per embodiment; use LLMs (code-writing) to decompose natural-language commands into parameterized navigation primitives that query the VLMap for coordinates and then plan with an off-the-shelf navigation stack.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>navigation (spatial goal navigation, multi-object sequential goals, cross-embodiment obstacle-aware planning)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial + object-relational (spatial locations/layouts fused with object-level semantic embeddings); enables procedural chaining when combined with an LLM</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>visual-language pretraining (LSeg / CLIP embeddings inherited from internet-scale image-text pretraining) fused with explicit 3D reconstruction and odometry; plus user-provided language lists at runtime</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>explicit retrieval / nearest-neighbor similarity between map embeddings (Q) and CLIP text embeddings (E); user text lists provided at runtime</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>explicit spatial map: top-down grid M ∈ R^{H×W×C} where each cell stores averaged CLIP-space VLM pixel embeddings; segmentation masks derived by Q·E^T similarity; obstacle maps by height filtering plus mask unions</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate (SR) for sequential subgoals, independent subgoal SR, success-rate-weighted-by-path-length (SPL), pixel accuracy / mIoU for top-down segmentation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Multi-object navigation (Table I) with VLMaps: 1 subgoal SR=59%, 2-in-a-row SR=34%, 3-in-a-row SR=22%, 4-in-a-row SR=15%; independent subgoals SR=59%. Zero-shot spatial goal navigation (Table II) with VLMaps+code-LLM: 1 subgoal SR=62%, 2-in-a-row=33%, 3-in-a-row=14%, 4-in-a-row=10%. Top-down semantic segmentation (Table V) VLMaps: pixel accuracy=92.3%, mean accuracy=27.7%, mIoU=19.0%, freq-weighted mIoU=85.9%.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Accurately localizes object landmarks and spatial references when the VLM embeddings are consistent across views; can represent spatial relations like 'left of', 'in between', and metric offsets by applying scripted offsets to localized coordinates; supports multi-step procedural plans when combined with code-generating LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Sensitive to 3D reconstruction noise and odometry drift which degrades map quality and landmark indexing; averaging embeddings across views can blur distinctions (causes noisy segmentation); confusion in cluttered scenes with many similar objects (object ambiguity), leading to mislocalization and wrong planning targets.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Outperforms LM-Nav, CoW, and CLIP Map baselines on multi-object and spatial goal tasks (see Table I & II). Example: CoW 1-subgoal SR=42% vs VLMaps 59%; CLIP Map 1-subgoal SR=33% vs VLMaps 59%. Ground-truth semantic map upper bound much higher (e.g., GT multi-object 1-subgoal SR=91%).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Replacing LSeg features with raw CLIP visual features (CLIP Map baseline) substantially degrades performance (CLIP Map multi-object 1-subgoal SR=33% vs VLMaps 59%); top-down segmentation ablation (Table V) shows VLMaps yields much higher pixel accuracy and freq-weighted mIoU than CoW. The authors also report that averaging embeddings is a simple fusion with observed noise—suggested area for improved fusion.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Fusing VLM pixel embeddings into an explicit spatial map preserves both open-vocabulary semantic generalization (from VLM pretraining) and spatial precision (from geometry/odometry). This allows language-only planners (LLMs) to ground abstract spatial and object-relational references to concrete map coordinates and to support zero-shot spatial navigation without additional fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Visual Language Maps for Robot Navigation', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e519.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e519.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Code-writing LLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Code-writing Large Language Model (Codex-like / GPT code models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pre-trained code-generating LLM is few-shot prompted to translate natural-language navigation commands into executable Python robot code which calls parameterized navigation primitives that query the VLMap.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evaluating large language models trained on code</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Code-writing LLM (Codex-style / large code LM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large transformer language models trained on large corpora of code (GitHub) and natural language; repurposed with few-shot examples to synthesize short Python programs (sequences of API calls and control flow) that implement navigation plans. The model itself receives only a text prompt (docstring/comment examples) and outputs code.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Zero-shot translation of natural-language spatial commands into executable robot code</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Input: free-form natural language instructions (may include spatial relations and metric offsets). Output: Python code composed of provided navigation primitives (robot.move_to_left('counter'), robot.move_in_between('sink','oven'), etc.) which the robot then executes by querying the VLMap and navigation stack.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>instruction following / multi-step procedural planning (navigation)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural + spatial + object-relational (procedural: generating sequences/loops/conditionals; spatial/object-relational: referencing landmarks by name and relative offsets)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pretraining on large code corpora and natural language; few-shot in-context examples in the prompt at test time</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>few-shot prompting with examples (code-generation), i.e., prompt contains paired comment (NL) and desired API code snippets; the LLM synthesizes new code for unseen instructions</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>procedural programs / action sequences expressed as Python code calling parameterized navigation primitives; spatial references are kept symbolic (strings referring to landmarks) and later grounded by querying VLMap embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>system-level navigation success rates (SR, SPL) when the generated code is executed with VLMap grounding and navigation stack</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>When combined with VLMaps and executed on the robot stack, code-writing LLMs enabled the system to achieve VLMaps results: zero-shot spatial goal navigation SRs (Table II) 1-subgoal=62%, 2-in-a-row=33%, 3-in-a-row=14%, 4-in-a-row=10%; multi-object sequential navigation as in Table I (VLMaps numbers). Real robot: 10/20 language-based spatial goals completed in test scene.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Reliable at decomposing natural-language into sequences of parameterized API calls for many seen prompt patterns (e.g., directional moves, 'in between', repetition loops), can perform arithmetic and control flow (loops) when required, enabling procedural execution without direct sensory input.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>LLM-generated code depends on correct grounding provided by VLMap; if map localization is noisy or object indices ambiguous, code still executes but leads to incorrect behavior. LLMs can produce incorrect API sequences if prompts are out-of-distribution or if instructions involve ambiguous references not resolvable via the map.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to LM-Nav's parsing approach (which uses GPT-3 for parsing), the code-generating approach here permits generation of precise, parameterized calls (robot.move_north('laptop'), robot.move_forward(3)) that combine better with VLMaps to achieve higher spatial navigation SRs (LM-Nav 1-subgoal SR=26% vs VLMaps+code-LLM 62% on spatial tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Not explicitly ablated between different code LLMs in paper; authors emphasize few-shot prompting with several examples is crucial for reliable generation. They contrasted map-based methods with and without this code-generation pathway (LM-Nav uses LLM parsing differently and performs worse).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Code-writing LLMs can compensate for lacking direct sensory input by emitting symbolic procedural plans that refer to an external spatial grounding (the VLMap); thus the LLM's procedural knowledge (how to sequence API calls) and object-relational reasoning (referring to landmark names) are effectively combined with an external perceptual memory to achieve embodied behaviors zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Visual Language Maps for Robot Navigation', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e519.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e519.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LSeg</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Languages-driven Semantic Segmentation (LSeg)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A visual-language segmentation model whose visual encoder maps pixels into the CLIP feature space enabling language-driven segmentation with free-form categories; used to produce per-pixel embeddings that are fused into the VLMap.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Languagesdriven semantic segmentation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LSeg (VLM visual encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A segmentation model that uses a visual encoder producing pixel embeddings in CLIP space and applies a CLIP text encoder to produce category embeddings, enabling segmentation for arbitrary text labels without retraining; used here to obtain dense pixel-level CLIP-space embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Dense open-vocabulary semantic segmentation and embedding extraction for map fusion</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Per RGB frame, LSeg computes H×W×C pixel embeddings; these are back-projected into 3D using depth and odometry and averaged into corresponding top-down map cells, providing open-vocabulary, language-aligned semantic features per map cell.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>perception / semantic mapping (supporting downstream navigation)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>object-relational (object category semantics, open-vocabulary), contributes to spatial via spatially indexed embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pretraining (CLIP image-text supervision) and segmentation training used by LSeg authors; no additional finetuning in this work</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>direct visual encoding at inference (per-pixel embeddings), followed by similarity lookup with CLIP text embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>dense per-pixel CLIP-space embeddings that are geometrically projected and averaged into a top-down grid, forming semantic/embedding content inside VLMap cells</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>used indirectly to measure downstream segmentation/mapping quality: top-down pixel accuracy, mean accuracy, mIoU (Table V) and per-class IoU (Table VI); influences navigation SR</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>When LSeg features are used to build VLMaps (vs projecting raw CLIP visual features as CLIP Map baseline), VLMaps achieve higher segmentation/map accuracy: pixel accuracy 92.3% (vs lower for CLIP Map) and improved navigation performance (see VLMaps numbers). Per-class IoU improvements are large for classes such as 'chair' (VLMaps 77.04% IOU vs CoW 6.99%).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Provides consistent, view-aggregated semantic embeddings enabling better landmark indexing and lower false positives compared to Grad-CAM saliency (CoW) or raw CLIP features; helps resolve correspondences across views by averaging embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>When query categories are out-of-distribution for LSeg training (e.g., 'seating'), LSeg embeddings may map them to similar seen categories causing misalignment; averaging across views can also dilute fine-grained distinctions leading to noisy masks.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>CLIP Map (project CLIP visual features) underperforms compared to LSeg-based VLMaps; CoW (GradCAM) produces noisy saliency maps with many false positives (see Fig. 4 and Table VI).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Using CLIP visual features instead of LSeg (CLIP Map) degrades map segmentation and navigation; authors report qualitative and quantitative degradation in object mask quality and navigation SR.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A language-driven segmentation encoder whose pixel embeddings lie in CLIP space provides an effective semantic prior that, when geometrically anchored into a map, improves open-vocabulary landmark localization and downstream embodied planning compared to saliency-based or raw CLIP projection baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Visual Language Maps for Robot Navigation', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e519.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e519.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LM-Nav</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LM-Nav: Robotic navigation with large pre-trained models of language, vision, and action</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior system that combines pretrained models (GPT-3 for language, CLIP for vision) to parse language into landmarks and navigate on a topological graph of stored image nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Lm-nav: Robotic navigation with large pre-trained models of language, vision, and action</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LM-Nav (GPT-3 + CLIP + graph-based planner)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Creates a topological graph of images as nodes; uses GPT-3 to parse instructions into landmarks and CLIP to match language to image nodes; navigation is planned by moving to matched nodes on the graph.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Zero-shot language-based navigation using image-graph localization</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given natural language instructions, identify target nodes (images) in a stored graph representing the environment and plan topological navigation to those nodes; limited to locations represented by stored images.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>navigation (language-guided)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>object-relational + procedural (parsing to landmark sequence) but limited spatial precision (topological image nodes rather than metric spatial maps)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>GPT-3 pretraining for parsing; CLIP pretraining for image-language matching; stored image observations for spatial memory</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>language parsing via LLM (GPT-3) + CLIP matching to images (retrieval). No code generation; planning limited to graph traversal.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>topological image graph where each node is an image observation; semantic mapping is implicit via retrieval to nodes rather than an explicit metric map</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>success rate (SR) on multi-object and spatial goal navigation tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Table I multi-object navigation: 1-subgoal SR=26%, 2-in-a-row=4%, 3-in-a-row=1%, 4-in-a-row=1% (independent subgoal SR=26%). Table II spatial goal navigation: 1-subgoal SR=5%, 2-in-a-row=5%, 3/4-in-a-row=0%.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Works for instructions that map closely to image nodes in the stored graph (seen viewpoints); uses language parsing to produce sequences of landmark names.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Fails when required spatial precision exceeds what image-node localization provides (cannot express 'in between' or metric offsets reliably); poor long-horizon chaining due to limited graph coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Underperforms VLMaps (e.g., VLMaps 1-subgoal SR 59% vs LM-Nav 26% in multi-object navigation) and other map-based methods which provide metric localization.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Paper does not provide ablations of LM-Nav itself, but compares performance as a baseline demonstrating the value of explicit spatial maps and LSeg-VLMap fusion over image-graph approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Language models (GPT-3) can parse instructions into landmark lists, but without an explicit metric spatial representation they struggle to execute spatially precise goals; coupling LLM parsing with an explicit spatial map (VLMaps) yields substantially better spatial grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Visual Language Maps for Robot Navigation', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e519.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e519.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoW</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CLIP on Wheels (CoW)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A zero-shot object navigation approach that combines CLIP-based saliency (via Grad-CAM) with classical exploration to find object goals, producing saliency masks that guide navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Clip on wheels: Zero-shot object navigation as object localization and exploration</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CoW (CLIP + Grad-CAM saliency)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses CLIP to compute saliency heatmaps (with Grad-CAM) for a text-specified object category on image observations; thresholds saliency to derive segmentation masks and integrates over exploration to guide object search.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Zero-shot object navigation (object-centric)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Explore environment using saliency-guided observations until a target object category is found; returns when robot stops within a threshold distance of detected object.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>navigation (object search / object-centric goal)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>object-relational (object detection/recognition), limited spatial reasoning (saliency is image-centric, not a metric spatial model)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pretrained CLIP (image-text) and Grad-CAM for saliency; no additional embodied fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>zero-shot prompting (text label input) to CLIP; Grad-CAM saliency computed on visual encoder activations; thresholding to form masks</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>image-plane saliency maps aggregated during exploration to propose target regions; not a metric spatial embedding fused into a map</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>success rate (SR) on multi-object and spatial navigation; top-down segmentation IoU</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Table I multi-object navigation: 1-subgoal SR=42%, 2-in-a-row=15%, 3-in-a-row=7%, 4-in-a-row=3%; independent subgoals SR=36%. Table II spatial goal navigation: 1-subgoal SR=33%, 2-in-a-row=5%, 3/4-in-a-row=0%. Top-down segmentation (Table VI) shows low IoU for many classes relative to VLMaps (e.g., chair IOU 6.99% vs VLMaps 77.04%).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Effective at locating single object instances relatively quickly when saliency maps are well-localized and not noisy.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Grad-CAM saliency maps are noisy with many false positives and over-segmentation; poor at fine-grained spatial relations like 'left of' or 'in between'; struggles with long-horizon multi-object sequences due to noisy localization.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Worse than VLMaps on both multi-object and spatial tasks (see Tables I & II); substantially worse top-down segmentation IoUs for many classes (Table VI).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Compared directly to VLMaps and CLIP Map; results indicate CoW's saliency approach is more error-prone than embedding-fusion into spatial maps.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Image-plane language grounding (CLIP+GradCAM) can retrieve objects zero-shot but lacks spatial precision and persistent, shareable spatial representation; anchoring language-aligned embeddings into metric maps (VLMaps) remedies many of these shortcomings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Visual Language Maps for Robot Navigation', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e519.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e519.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Open-vocab Obstacle Maps</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Open-vocabulary Obstacle Map Generation (cross-embodiment)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A VLMaps-enabled method to generate embodiment-specific binary obstacle maps at runtime by localizing a user-provided list of obstacle category names and combining these masks with geometric height filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>VLMaps-based obstacle map generator</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Given a VLMap and a user-specified list of obstacle category names, compute segmentation masks for those categories via CLIP similarity on map embeddings; apply height filtering on point cloud projections and union/select subset of classes for each embodiment to produce binary obstacle maps.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Cross-embodiment obstacle-aware navigation (LoCoBot vs Drone)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Create different obstacle maps for different robot embodiments (e.g., ground robot considers 'table' an obstacle whereas drone may not) by selecting subsets of localized category masks; use these maps in the navigation stack to plan obstacle-aware paths.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>navigation (embodiment-conditioned obstacle planning)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial + object-relational (which objects constitute obstacles for a given embodiment; spatial occupancy and vertical extent via height filtering)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>VLMap embeddings (LSeg/CLIP) fused with 3D reconstruction and user-provided obstacle-lists at runtime</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>explicit runtime query to VLMap with text category lists; geometric filtering by point height to exclude floor/ceiling</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>binary obstacle map O ∈ {0,1}^{H×W} formed by intersecting height-filtered geometric occupancy with unioned segmentation masks for selected obstacle categories</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success Rate (SR) and SPL (success weighted by path-length efficiency) on multi-object navigation for different embodiments and maps</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Table III (AI2THOR): LoCoBot (ground map) example: 1-subgoal SR=53% SPL=49.0; Drone (ground map) 1-subgoal SR=53% SPL=41.8; Drone (drone map) 1-subgoal SR=56% SPL=45.4. Independent subgoals SR: LoCoBot 62.5%, Drone (drone map) 72.5%. Using embodiment-specific obstacle maps improved SPL for drone relative to using a ground-only map.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Enables embodiments to exploit different affordances (drone can fly over objects if 'table' omitted from obstacle list) and improves path efficiency (SPL) without re-building the map or retraining models.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Sensitive to semantic misclassification (wrong objects labeled as obstacles) and to noisy geometry (height errors), which can either over-constrain or under-constrain planning; requires good VLMap segmentation and depth accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared drone using ground-map vs drone-map: drone with drone-map shows higher SPL (e.g., 1-subgoal SPL 45.4% vs 41.8%), indicating practical benefit over using a single static obstacle map for different embodiments.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Authors varied obstacle class lists per embodiment and observed the path-efficiency differences; no explicit numeric ablation beyond listed setups, but results show selecting appropriate obstacle categories is critical.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>An explicit, language-indexable spatial map enables rapid reconfiguration of obstacle semantics per embodiment at runtime, letting purely language-specified object-relational knowledge (what counts as obstacle) drive metric planning without sensory retraining.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Visual Language Maps for Robot Navigation', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Socratic models: Composing zero-shot multimodal reasoning with language <em>(Rating: 2)</em></li>
                <li>Lm-nav: Robotic navigation with large pre-trained models of language, vision, and action <em>(Rating: 2)</em></li>
                <li>Clip on wheels: Zero-shot object navigation as object localization and exploration <em>(Rating: 2)</em></li>
                <li>Languagesdriven semantic segmentation <em>(Rating: 2)</em></li>
                <li>Learning transferable visual models from natural language supervision <em>(Rating: 2)</em></li>
                <li>Code as policies: Language model programs for embodied control <em>(Rating: 2)</em></li>
                <li>Open-vocabulary queryable scene representations for real world planning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-519",
    "paper_id": "paper-c305ab1bdba79442bec72ec7f5c5ee7c49c2a566",
    "extraction_schema_id": "extraction-schema-15",
    "extracted_data": [
        {
            "name_short": "VLMaps",
            "name_full": "Visual-Language Maps",
            "brief_description": "A spatial top-down grid map representation that fuses pretrained visual-language (VLM) pixel embeddings into a 3D reconstruction so that free-form natural language can be localized to precise map coordinates for navigation and planning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "VLMaps (system)",
            "model_size": null,
            "model_description": "Not a single neural model but a pipeline: LSeg/CLIP visual-language pixel embeddings are extracted per RGB frame, back-projected into world coordinates from RGB-D + odometry, and averaged into a top-down H×W×C grid (scale s). This produces an explicit spatial map where each cell contains a CLIP-compatible embedding enabling open-vocabulary language indexing.",
            "task_name": "Open-vocabulary spatial and multi-object navigation (zero-shot)",
            "task_description": "Build a persistent top-down spatial map from RGB-D video and odometry; localize open-vocabulary landmarks (text queries) by matching CLIP-text embeddings to map-cell embeddings; generate obstacle maps per embodiment; use LLMs (code-writing) to decompose natural-language commands into parameterized navigation primitives that query the VLMap for coordinates and then plan with an off-the-shelf navigation stack.",
            "task_type": "navigation (spatial goal navigation, multi-object sequential goals, cross-embodiment obstacle-aware planning)",
            "knowledge_type": "spatial + object-relational (spatial locations/layouts fused with object-level semantic embeddings); enables procedural chaining when combined with an LLM",
            "knowledge_source": "visual-language pretraining (LSeg / CLIP embeddings inherited from internet-scale image-text pretraining) fused with explicit 3D reconstruction and odometry; plus user-provided language lists at runtime",
            "has_direct_sensory_input": true,
            "elicitation_method": "explicit retrieval / nearest-neighbor similarity between map embeddings (Q) and CLIP text embeddings (E); user text lists provided at runtime",
            "knowledge_representation": "explicit spatial map: top-down grid M ∈ R^{H×W×C} where each cell stores averaged CLIP-space VLM pixel embeddings; segmentation masks derived by Q·E^T similarity; obstacle maps by height filtering plus mask unions",
            "performance_metric": "Success rate (SR) for sequential subgoals, independent subgoal SR, success-rate-weighted-by-path-length (SPL), pixel accuracy / mIoU for top-down segmentation",
            "performance_result": "Multi-object navigation (Table I) with VLMaps: 1 subgoal SR=59%, 2-in-a-row SR=34%, 3-in-a-row SR=22%, 4-in-a-row SR=15%; independent subgoals SR=59%. Zero-shot spatial goal navigation (Table II) with VLMaps+code-LLM: 1 subgoal SR=62%, 2-in-a-row=33%, 3-in-a-row=14%, 4-in-a-row=10%. Top-down semantic segmentation (Table V) VLMaps: pixel accuracy=92.3%, mean accuracy=27.7%, mIoU=19.0%, freq-weighted mIoU=85.9%.",
            "success_patterns": "Accurately localizes object landmarks and spatial references when the VLM embeddings are consistent across views; can represent spatial relations like 'left of', 'in between', and metric offsets by applying scripted offsets to localized coordinates; supports multi-step procedural plans when combined with code-generating LLMs.",
            "failure_patterns": "Sensitive to 3D reconstruction noise and odometry drift which degrades map quality and landmark indexing; averaging embeddings across views can blur distinctions (causes noisy segmentation); confusion in cluttered scenes with many similar objects (object ambiguity), leading to mislocalization and wrong planning targets.",
            "baseline_comparison": "Outperforms LM-Nav, CoW, and CLIP Map baselines on multi-object and spatial goal tasks (see Table I & II). Example: CoW 1-subgoal SR=42% vs VLMaps 59%; CLIP Map 1-subgoal SR=33% vs VLMaps 59%. Ground-truth semantic map upper bound much higher (e.g., GT multi-object 1-subgoal SR=91%).",
            "ablation_results": "Replacing LSeg features with raw CLIP visual features (CLIP Map baseline) substantially degrades performance (CLIP Map multi-object 1-subgoal SR=33% vs VLMaps 59%); top-down segmentation ablation (Table V) shows VLMaps yields much higher pixel accuracy and freq-weighted mIoU than CoW. The authors also report that averaging embeddings is a simple fusion with observed noise—suggested area for improved fusion.",
            "key_findings": "Fusing VLM pixel embeddings into an explicit spatial map preserves both open-vocabulary semantic generalization (from VLM pretraining) and spatial precision (from geometry/odometry). This allows language-only planners (LLMs) to ground abstract spatial and object-relational references to concrete map coordinates and to support zero-shot spatial navigation without additional fine-tuning.",
            "uuid": "e519.0",
            "source_info": {
                "paper_title": "Visual Language Maps for Robot Navigation",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Code-writing LLM",
            "name_full": "Code-writing Large Language Model (Codex-like / GPT code models)",
            "brief_description": "A pre-trained code-generating LLM is few-shot prompted to translate natural-language navigation commands into executable Python robot code which calls parameterized navigation primitives that query the VLMap.",
            "citation_title": "Evaluating large language models trained on code",
            "mention_or_use": "use",
            "model_name": "Code-writing LLM (Codex-style / large code LM)",
            "model_size": null,
            "model_description": "Large transformer language models trained on large corpora of code (GitHub) and natural language; repurposed with few-shot examples to synthesize short Python programs (sequences of API calls and control flow) that implement navigation plans. The model itself receives only a text prompt (docstring/comment examples) and outputs code.",
            "task_name": "Zero-shot translation of natural-language spatial commands into executable robot code",
            "task_description": "Input: free-form natural language instructions (may include spatial relations and metric offsets). Output: Python code composed of provided navigation primitives (robot.move_to_left('counter'), robot.move_in_between('sink','oven'), etc.) which the robot then executes by querying the VLMap and navigation stack.",
            "task_type": "instruction following / multi-step procedural planning (navigation)",
            "knowledge_type": "procedural + spatial + object-relational (procedural: generating sequences/loops/conditionals; spatial/object-relational: referencing landmarks by name and relative offsets)",
            "knowledge_source": "pretraining on large code corpora and natural language; few-shot in-context examples in the prompt at test time",
            "has_direct_sensory_input": false,
            "elicitation_method": "few-shot prompting with examples (code-generation), i.e., prompt contains paired comment (NL) and desired API code snippets; the LLM synthesizes new code for unseen instructions",
            "knowledge_representation": "procedural programs / action sequences expressed as Python code calling parameterized navigation primitives; spatial references are kept symbolic (strings referring to landmarks) and later grounded by querying VLMap embeddings",
            "performance_metric": "system-level navigation success rates (SR, SPL) when the generated code is executed with VLMap grounding and navigation stack",
            "performance_result": "When combined with VLMaps and executed on the robot stack, code-writing LLMs enabled the system to achieve VLMaps results: zero-shot spatial goal navigation SRs (Table II) 1-subgoal=62%, 2-in-a-row=33%, 3-in-a-row=14%, 4-in-a-row=10%; multi-object sequential navigation as in Table I (VLMaps numbers). Real robot: 10/20 language-based spatial goals completed in test scene.",
            "success_patterns": "Reliable at decomposing natural-language into sequences of parameterized API calls for many seen prompt patterns (e.g., directional moves, 'in between', repetition loops), can perform arithmetic and control flow (loops) when required, enabling procedural execution without direct sensory input.",
            "failure_patterns": "LLM-generated code depends on correct grounding provided by VLMap; if map localization is noisy or object indices ambiguous, code still executes but leads to incorrect behavior. LLMs can produce incorrect API sequences if prompts are out-of-distribution or if instructions involve ambiguous references not resolvable via the map.",
            "baseline_comparison": "Compared to LM-Nav's parsing approach (which uses GPT-3 for parsing), the code-generating approach here permits generation of precise, parameterized calls (robot.move_north('laptop'), robot.move_forward(3)) that combine better with VLMaps to achieve higher spatial navigation SRs (LM-Nav 1-subgoal SR=26% vs VLMaps+code-LLM 62% on spatial tasks).",
            "ablation_results": "Not explicitly ablated between different code LLMs in paper; authors emphasize few-shot prompting with several examples is crucial for reliable generation. They contrasted map-based methods with and without this code-generation pathway (LM-Nav uses LLM parsing differently and performs worse).",
            "key_findings": "Code-writing LLMs can compensate for lacking direct sensory input by emitting symbolic procedural plans that refer to an external spatial grounding (the VLMap); thus the LLM's procedural knowledge (how to sequence API calls) and object-relational reasoning (referring to landmark names) are effectively combined with an external perceptual memory to achieve embodied behaviors zero-shot.",
            "uuid": "e519.1",
            "source_info": {
                "paper_title": "Visual Language Maps for Robot Navigation",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "LSeg",
            "name_full": "Languages-driven Semantic Segmentation (LSeg)",
            "brief_description": "A visual-language segmentation model whose visual encoder maps pixels into the CLIP feature space enabling language-driven segmentation with free-form categories; used to produce per-pixel embeddings that are fused into the VLMap.",
            "citation_title": "Languagesdriven semantic segmentation",
            "mention_or_use": "use",
            "model_name": "LSeg (VLM visual encoder)",
            "model_size": null,
            "model_description": "A segmentation model that uses a visual encoder producing pixel embeddings in CLIP space and applies a CLIP text encoder to produce category embeddings, enabling segmentation for arbitrary text labels without retraining; used here to obtain dense pixel-level CLIP-space embeddings.",
            "task_name": "Dense open-vocabulary semantic segmentation and embedding extraction for map fusion",
            "task_description": "Per RGB frame, LSeg computes H×W×C pixel embeddings; these are back-projected into 3D using depth and odometry and averaged into corresponding top-down map cells, providing open-vocabulary, language-aligned semantic features per map cell.",
            "task_type": "perception / semantic mapping (supporting downstream navigation)",
            "knowledge_type": "object-relational (object category semantics, open-vocabulary), contributes to spatial via spatially indexed embeddings",
            "knowledge_source": "pretraining (CLIP image-text supervision) and segmentation training used by LSeg authors; no additional finetuning in this work",
            "has_direct_sensory_input": true,
            "elicitation_method": "direct visual encoding at inference (per-pixel embeddings), followed by similarity lookup with CLIP text embeddings",
            "knowledge_representation": "dense per-pixel CLIP-space embeddings that are geometrically projected and averaged into a top-down grid, forming semantic/embedding content inside VLMap cells",
            "performance_metric": "used indirectly to measure downstream segmentation/mapping quality: top-down pixel accuracy, mean accuracy, mIoU (Table V) and per-class IoU (Table VI); influences navigation SR",
            "performance_result": "When LSeg features are used to build VLMaps (vs projecting raw CLIP visual features as CLIP Map baseline), VLMaps achieve higher segmentation/map accuracy: pixel accuracy 92.3% (vs lower for CLIP Map) and improved navigation performance (see VLMaps numbers). Per-class IoU improvements are large for classes such as 'chair' (VLMaps 77.04% IOU vs CoW 6.99%).",
            "success_patterns": "Provides consistent, view-aggregated semantic embeddings enabling better landmark indexing and lower false positives compared to Grad-CAM saliency (CoW) or raw CLIP features; helps resolve correspondences across views by averaging embeddings.",
            "failure_patterns": "When query categories are out-of-distribution for LSeg training (e.g., 'seating'), LSeg embeddings may map them to similar seen categories causing misalignment; averaging across views can also dilute fine-grained distinctions leading to noisy masks.",
            "baseline_comparison": "CLIP Map (project CLIP visual features) underperforms compared to LSeg-based VLMaps; CoW (GradCAM) produces noisy saliency maps with many false positives (see Fig. 4 and Table VI).",
            "ablation_results": "Using CLIP visual features instead of LSeg (CLIP Map) degrades map segmentation and navigation; authors report qualitative and quantitative degradation in object mask quality and navigation SR.",
            "key_findings": "A language-driven segmentation encoder whose pixel embeddings lie in CLIP space provides an effective semantic prior that, when geometrically anchored into a map, improves open-vocabulary landmark localization and downstream embodied planning compared to saliency-based or raw CLIP projection baselines.",
            "uuid": "e519.2",
            "source_info": {
                "paper_title": "Visual Language Maps for Robot Navigation",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "LM-Nav",
            "name_full": "LM-Nav: Robotic navigation with large pre-trained models of language, vision, and action",
            "brief_description": "A prior system that combines pretrained models (GPT-3 for language, CLIP for vision) to parse language into landmarks and navigate on a topological graph of stored image nodes.",
            "citation_title": "Lm-nav: Robotic navigation with large pre-trained models of language, vision, and action",
            "mention_or_use": "use",
            "model_name": "LM-Nav (GPT-3 + CLIP + graph-based planner)",
            "model_size": null,
            "model_description": "Creates a topological graph of images as nodes; uses GPT-3 to parse instructions into landmarks and CLIP to match language to image nodes; navigation is planned by moving to matched nodes on the graph.",
            "task_name": "Zero-shot language-based navigation using image-graph localization",
            "task_description": "Given natural language instructions, identify target nodes (images) in a stored graph representing the environment and plan topological navigation to those nodes; limited to locations represented by stored images.",
            "task_type": "navigation (language-guided)",
            "knowledge_type": "object-relational + procedural (parsing to landmark sequence) but limited spatial precision (topological image nodes rather than metric spatial maps)",
            "knowledge_source": "GPT-3 pretraining for parsing; CLIP pretraining for image-language matching; stored image observations for spatial memory",
            "has_direct_sensory_input": false,
            "elicitation_method": "language parsing via LLM (GPT-3) + CLIP matching to images (retrieval). No code generation; planning limited to graph traversal.",
            "knowledge_representation": "topological image graph where each node is an image observation; semantic mapping is implicit via retrieval to nodes rather than an explicit metric map",
            "performance_metric": "success rate (SR) on multi-object and spatial goal navigation tasks",
            "performance_result": "Table I multi-object navigation: 1-subgoal SR=26%, 2-in-a-row=4%, 3-in-a-row=1%, 4-in-a-row=1% (independent subgoal SR=26%). Table II spatial goal navigation: 1-subgoal SR=5%, 2-in-a-row=5%, 3/4-in-a-row=0%.",
            "success_patterns": "Works for instructions that map closely to image nodes in the stored graph (seen viewpoints); uses language parsing to produce sequences of landmark names.",
            "failure_patterns": "Fails when required spatial precision exceeds what image-node localization provides (cannot express 'in between' or metric offsets reliably); poor long-horizon chaining due to limited graph coverage.",
            "baseline_comparison": "Underperforms VLMaps (e.g., VLMaps 1-subgoal SR 59% vs LM-Nav 26% in multi-object navigation) and other map-based methods which provide metric localization.",
            "ablation_results": "Paper does not provide ablations of LM-Nav itself, but compares performance as a baseline demonstrating the value of explicit spatial maps and LSeg-VLMap fusion over image-graph approaches.",
            "key_findings": "Language models (GPT-3) can parse instructions into landmark lists, but without an explicit metric spatial representation they struggle to execute spatially precise goals; coupling LLM parsing with an explicit spatial map (VLMaps) yields substantially better spatial grounding.",
            "uuid": "e519.3",
            "source_info": {
                "paper_title": "Visual Language Maps for Robot Navigation",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "CoW",
            "name_full": "CLIP on Wheels (CoW)",
            "brief_description": "A zero-shot object navigation approach that combines CLIP-based saliency (via Grad-CAM) with classical exploration to find object goals, producing saliency masks that guide navigation.",
            "citation_title": "Clip on wheels: Zero-shot object navigation as object localization and exploration",
            "mention_or_use": "use",
            "model_name": "CoW (CLIP + Grad-CAM saliency)",
            "model_size": null,
            "model_description": "Uses CLIP to compute saliency heatmaps (with Grad-CAM) for a text-specified object category on image observations; thresholds saliency to derive segmentation masks and integrates over exploration to guide object search.",
            "task_name": "Zero-shot object navigation (object-centric)",
            "task_description": "Explore environment using saliency-guided observations until a target object category is found; returns when robot stops within a threshold distance of detected object.",
            "task_type": "navigation (object search / object-centric goal)",
            "knowledge_type": "object-relational (object detection/recognition), limited spatial reasoning (saliency is image-centric, not a metric spatial model)",
            "knowledge_source": "pretrained CLIP (image-text) and Grad-CAM for saliency; no additional embodied fine-tuning",
            "has_direct_sensory_input": true,
            "elicitation_method": "zero-shot prompting (text label input) to CLIP; Grad-CAM saliency computed on visual encoder activations; thresholding to form masks",
            "knowledge_representation": "image-plane saliency maps aggregated during exploration to propose target regions; not a metric spatial embedding fused into a map",
            "performance_metric": "success rate (SR) on multi-object and spatial navigation; top-down segmentation IoU",
            "performance_result": "Table I multi-object navigation: 1-subgoal SR=42%, 2-in-a-row=15%, 3-in-a-row=7%, 4-in-a-row=3%; independent subgoals SR=36%. Table II spatial goal navigation: 1-subgoal SR=33%, 2-in-a-row=5%, 3/4-in-a-row=0%. Top-down segmentation (Table VI) shows low IoU for many classes relative to VLMaps (e.g., chair IOU 6.99% vs VLMaps 77.04%).",
            "success_patterns": "Effective at locating single object instances relatively quickly when saliency maps are well-localized and not noisy.",
            "failure_patterns": "Grad-CAM saliency maps are noisy with many false positives and over-segmentation; poor at fine-grained spatial relations like 'left of' or 'in between'; struggles with long-horizon multi-object sequences due to noisy localization.",
            "baseline_comparison": "Worse than VLMaps on both multi-object and spatial tasks (see Tables I & II); substantially worse top-down segmentation IoUs for many classes (Table VI).",
            "ablation_results": "Compared directly to VLMaps and CLIP Map; results indicate CoW's saliency approach is more error-prone than embedding-fusion into spatial maps.",
            "key_findings": "Image-plane language grounding (CLIP+GradCAM) can retrieve objects zero-shot but lacks spatial precision and persistent, shareable spatial representation; anchoring language-aligned embeddings into metric maps (VLMaps) remedies many of these shortcomings.",
            "uuid": "e519.4",
            "source_info": {
                "paper_title": "Visual Language Maps for Robot Navigation",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Open-vocab Obstacle Maps",
            "name_full": "Open-vocabulary Obstacle Map Generation (cross-embodiment)",
            "brief_description": "A VLMaps-enabled method to generate embodiment-specific binary obstacle maps at runtime by localizing a user-provided list of obstacle category names and combining these masks with geometric height filtering.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "VLMaps-based obstacle map generator",
            "model_size": null,
            "model_description": "Given a VLMap and a user-specified list of obstacle category names, compute segmentation masks for those categories via CLIP similarity on map embeddings; apply height filtering on point cloud projections and union/select subset of classes for each embodiment to produce binary obstacle maps.",
            "task_name": "Cross-embodiment obstacle-aware navigation (LoCoBot vs Drone)",
            "task_description": "Create different obstacle maps for different robot embodiments (e.g., ground robot considers 'table' an obstacle whereas drone may not) by selecting subsets of localized category masks; use these maps in the navigation stack to plan obstacle-aware paths.",
            "task_type": "navigation (embodiment-conditioned obstacle planning)",
            "knowledge_type": "spatial + object-relational (which objects constitute obstacles for a given embodiment; spatial occupancy and vertical extent via height filtering)",
            "knowledge_source": "VLMap embeddings (LSeg/CLIP) fused with 3D reconstruction and user-provided obstacle-lists at runtime",
            "has_direct_sensory_input": true,
            "elicitation_method": "explicit runtime query to VLMap with text category lists; geometric filtering by point height to exclude floor/ceiling",
            "knowledge_representation": "binary obstacle map O ∈ {0,1}^{H×W} formed by intersecting height-filtered geometric occupancy with unioned segmentation masks for selected obstacle categories",
            "performance_metric": "Success Rate (SR) and SPL (success weighted by path-length efficiency) on multi-object navigation for different embodiments and maps",
            "performance_result": "Table III (AI2THOR): LoCoBot (ground map) example: 1-subgoal SR=53% SPL=49.0; Drone (ground map) 1-subgoal SR=53% SPL=41.8; Drone (drone map) 1-subgoal SR=56% SPL=45.4. Independent subgoals SR: LoCoBot 62.5%, Drone (drone map) 72.5%. Using embodiment-specific obstacle maps improved SPL for drone relative to using a ground-only map.",
            "success_patterns": "Enables embodiments to exploit different affordances (drone can fly over objects if 'table' omitted from obstacle list) and improves path efficiency (SPL) without re-building the map or retraining models.",
            "failure_patterns": "Sensitive to semantic misclassification (wrong objects labeled as obstacles) and to noisy geometry (height errors), which can either over-constrain or under-constrain planning; requires good VLMap segmentation and depth accuracy.",
            "baseline_comparison": "Compared drone using ground-map vs drone-map: drone with drone-map shows higher SPL (e.g., 1-subgoal SPL 45.4% vs 41.8%), indicating practical benefit over using a single static obstacle map for different embodiments.",
            "ablation_results": "Authors varied obstacle class lists per embodiment and observed the path-efficiency differences; no explicit numeric ablation beyond listed setups, but results show selecting appropriate obstacle categories is critical.",
            "key_findings": "An explicit, language-indexable spatial map enables rapid reconfiguration of obstacle semantics per embodiment at runtime, letting purely language-specified object-relational knowledge (what counts as obstacle) drive metric planning without sensory retraining.",
            "uuid": "e519.5",
            "source_info": {
                "paper_title": "Visual Language Maps for Robot Navigation",
                "publication_date_yy_mm": "2022-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Socratic models: Composing zero-shot multimodal reasoning with language",
            "rating": 2
        },
        {
            "paper_title": "Lm-nav: Robotic navigation with large pre-trained models of language, vision, and action",
            "rating": 2
        },
        {
            "paper_title": "Clip on wheels: Zero-shot object navigation as object localization and exploration",
            "rating": 2
        },
        {
            "paper_title": "Languagesdriven semantic segmentation",
            "rating": 2
        },
        {
            "paper_title": "Learning transferable visual models from natural language supervision",
            "rating": 2
        },
        {
            "paper_title": "Code as policies: Language model programs for embodied control",
            "rating": 2
        },
        {
            "paper_title": "Open-vocabulary queryable scene representations for real world planning",
            "rating": 1
        }
    ],
    "cost": 0.019143,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Visual Language Maps for Robot Navigation</h1>
<p>Chenguang Huang ${ }^{1}$, Oier Mees ${ }^{1}$, Andy Zeng ${ }^{2}$, Wolfram Burgard ${ }^{3}$</p>
<h4>Abstract</h4>
<p>Grounding language to the visual observations of a navigating agent can be performed using off-the-shelf visuallanguage models pretrained on Internet-scale data (e.g., image captions). While this is useful for matching images to natural language descriptions of object goals, it remains disjoint from the process of mapping the environment, so that it lacks the spatial precision of classic geometric maps. To address this problem, we propose VLMaps, a spatial map representation that directly fuses pretrained visual-language features with a 3D reconstruction of the physical world. VLMaps can be autonomously built from video feed on robots using standard exploration approaches and enables natural language indexing of the map without additional labeled data. Specifically, when combined with large language models (LLMs), VLMaps can be used to (i) translate natural language commands into a sequence of open-vocabulary navigation goals (which, beyond prior work, can be spatial by construction, e.g., "in between the sofa and the TV" or "three meters to the right of the chair") directly localized in the map, and (ii) can be shared among multiple robots with different embodiments to generate new obstacle maps on-the-fly (by using a list of obstacle categories). Extensive experiments carried out in simulated and real-world environments show that VLMaps enable navigation according to more complex language instructions than existing methods. Videos are available at https://vlmaps.github.io.</p>
<h2>I. INTRODUCTION</h2>
<p>People are excellent navigators of the physical world - due in part to their remarkable ability to build cognitive maps [1] that form the basis of spatial memory [2], [3] to (i) localize landmarks at varying ontological levels, such as a book; on the shelf; in the living room, or to (ii) determine whether the layout permits navigation between two points. Classic methods for robot navigation [4], [5] build geometric maps for path planning and can parse goals from natural language commands [6], [7], but struggle to generalize to unseen instructions. Learning methods directly optimize for navigation policies grounded in language end-to-end (commands to actions) [8], [9], but require copious amounts of data.</p>
<p>Meanwhile, recent works show that visual-language models (VLMs) [10], [11] pretrained on Internet-scale data (e.g., image captions) can be used out-of-the-box to ground language to the visual observations of a navigating agent, without additional data collection or model fine-tuning. These models enable mobile robots to handle new instructions that specify unseen object goals and can be combined with exploration algorithms to search for the first instance of any object (CoW) [12] or traverse objectcentric landmarks in graphs (LM-Nav) [13]. While promising, these methods predominantly use VLMs as critics to match image observations to object goal descriptions, but do so in ways that remain disjoint from the mapping of the environment. Without grounding language onto a spatial representation, these systems</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1: VLMaps is a spatial map representation in which pretrained visuallanguage model features are fused into a 3D reconstruction of the physical world. Spatially anchoring visual language features enables natural language indexing in the map, which can be used to, e.g., localize landmarks or spatial references with respect to landmarks - enabling zero-shot spatial goal navigation without additional data collection or model finetuning.
may struggle to (i) recognize correspondences that associate independent observations of the same object, to (ii) localize spatial goals e.g., "in between the sofa and the TV", or to (iii) build persistent representations that can be shared across different embodiments, e.g., mobile robots, drones. Existing VLM-based solutions generalize to new object goals, but lose the spatial precision of classic geometric maps - is it possible to get the best of both?</p>
<p>In this work, we investigate the utility of a spatial visuallanguage map representation VLMaps, which fuses pretrained visual-language features from image observations directly with a 3D reconstruction of the physical world. VLMaps can be effectively built from video feed on robots using standard exploration algorithms. When paired with large language models (LLMs) in Socratic fashion [14], VLMaps can translate natural language instructions into a sequence of open-vocabulary goals, directly localized in the map. A key aspect of VLMaps is that they are spatial, which enables them to:</p>
<ul>
<li>Localize spatial goals beyond object-centric ones, e.g., "in between the TV and sofa" or "to the right of the chair" or "kitchen area" using code-writing LLMs, expanding beyond capabilities of CoW or LM-Nav.</li>
<li>Generate new obstacle maps for new embodiments given natural language descriptions of landmark categories that they can or cannot traverse, e.g., "tables" are obstacles for a large mobile robot, but traversable for a drone.</li>
</ul>
<p>Extensive experiments show that using VLMaps enables more effective long-horizon multi-object goal navigation than baseline alternatives, e.g., CoW [12] and LM-Nav [13], and, in particular, excels at enabling spatial open-vocabulary navigation tasks. We also provide ablations on different ways of constructing</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2: VLMaps enables a robot to perform complex zero-shot spatial goal navigation tasks given natural language commands, without additional data collection or model finetuning.</p>
<p>VLMaps with different language models as well as a discussion on limitations, which point to areas for future work. Code and videos are available at https://vlmaps.github.io.</p>
<h2>II. Related Work</h2>
<p><strong>Semantic Mapping.</strong> The maturity of traditional SLAM techniques together with the advancements in semantic understanding capabilities of convolutional neural networks has recently spurred considerable interest around augmenting 3D maps with semantic information [15], [16]. The literature has focused on either densely annotating 3D volumetric maps with 2D semantic segmentation CNNs [16] or object-oriented approaches [17], [18], [19], which build 3D maps around detected objects to enable object-level pose-graph optimization. Although progress has been made at generating more abstract maps, such as scene graphs [20], [21], current approaches are limited to a predefined set of semantic classes. In contrast to this, VLMaps are open-vocabulary semantic maps that, unlike prior work, enable <em>natural language indexing in the map</em>.</p>
<p><strong>Vision and Language Navigation.</strong> Recently, also Vision-and-Language Navigation (VLN) has received increased attention [8], [22]. Further work has focused on learning end-to-end policies that can follow route-based instructions on topological graphs of simulated environments [8], [23], [24]. However, agents trained in this setting do not have low-level planning capabilities and rely heavily on the topological graph, limiting their real-world applicability [9]. Moreover, despite extensions to continuous state spaces [22], [25], [26], most of these learning-based methods are data-intensive.</p>
<p><strong>Zero-shot Models.</strong> The recent success of large pretrained vision and language models [10], [27] has spurred a flurry of interest in applying their zero-sot capabilities to different domains including object detection and segmentation [28], [29], [11], robot manipulation [30], [31], [32], [33], and navigation [13], [12], [34]. Most related to our work is the approach denoted LM-Nav [13], which combines three pre-trained models to navigate via a topological graph in the real world. CoW [12] performs zero-shot language-based object navigation by combining CLIP-based [10] saliency maps and traditional exploration methods. However, both LM-Nav [13] and CoW [12] are limited to navigating to object landmarks and are less capable of understanding finer-grained queries, such as "to the left of the chair" and "in between the TV and the sofa". In contrast, our method enables spatial language indexing beyond object-centric goals and can generate open-vocabulary obstacle maps. A concurrent work is NLMap [34], which demonstrates that VLMs can be used to build queryable scene representations to allow LLM robot planning [35] with new objects and locations.</p>
<h2>III. Method</h2>
<p>Our goal is to build a <em>spatial</em> visual-language map representation, in which landmarks ("the sofa") or spatial references ("between the sofa and the TV") can be directly localized using natural language. We propose VLMaps as one such representation, which can be constructed using off-the-shelf visual-language models (VLMs) and standard 3D reconstruction libraries. In the following subsections, we describe (i) how to build a VLMap (Sec. III-A), (ii) how to use these maps to localize open-vocabulary landmarks (Sec. III-B), (iii) how to build open-vocabulary obstacle maps from a list of obstacle categories for different robot embodiments (Sec. III-C), and (iv) how VLMaps can be used together with large language models (LLMs) for zero-shot spatial goal navigation on real robots from natural language commands (Sec. III-D), without additional data collection or model fine-tuning. Our pipeline is visualized in Fig. 3.</p>
<h3>A. Building a Visual-Language Map</h3>
<p>The key idea behind VLMaps is to fuse pretrained visual-language features with a 3D reconstruction. We achieve this by computing dense pixel-level embeddings from an existing visual-language model (over the video feed of the robot) and by back-projecting them onto the 3D surface of the environment (captured from depth data used for reconstruction with visual odometry).</p>
<p>In our work, we utilize LSeg [11] as the visual-language model, a language-driven semantic segmentation model that segments the RGB images based on a set of free-form language categories. The LSeg visual encoder maps an image such that the embedding of each pixel lies in the CLIP feature space. In our approach, we fuse the LSeg pixel embeddings with their corresponding 3D map locations. In this way, without explicit manual segmentation labels, we incorporate a powerful language-driven semantic prior that inherits the generalization capabilities of VLMs. The only assumption we make is access to odometry, which is readily available from RGB-D SLAM systems and enables us to build a map from sequences of RGB-D images.</p>
<p>Formally, we define VLMap as $$M \in \mathbb{R}^{H \times W \times C}$$, where $$H$$ and $$W$$ represent the size of the top-down grid map, and $$C$$ represents the length of the VLM embedding vector for each grid cell. Together with the scale parameter $$s$$, a VLMap $$M$$ represents an area with size $$s H \times s W$$ meters. To build the map, we, for each RGB-D frame, back-project all the depth pixels $$u = (u, v)$$ to form a local depth point cloud that we transform to the world frame, $$P_k = D(u)K^{-1} \tilde{u}$$ and $$P_{W} = T_{Wk} P_k$$ where $$\tilde{u} = (u, v, 1)$$, $$K$$ is the intrinsic matrix of the depth camera, $$D(u) \in \mathbb{R}$$ is the depth value of the pixel $$u$$, $$T_{Wk}$$ is the transformation from the world coordinate frame to the k-th camera frame, $$P_k \in \mathbb{R}^3$$ is the 3D point position in</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3: System overview. A VLMap is created by fusing pretrained visual-language features into the reconstruction of the environment to enable visual-spatial-language-based reasoning. By providing a list of open-vocabulary labels, we retrieve segmentation masks for semantic classes required by downstream applications.</p>
<p>The k-th frame, and <strong>P</strong><sup><em>W</em></sup> ∈ ℝ<sup>3</sup> is the 3D point position in the world coordinate frame. We then project the point <strong>P</strong><sup><em>W</em></sup> to the ground plane and get the pixel <strong>u</strong>'s corresponding position on the grid map,</p>
<p>$$p_{\text{map}}^e = \left\lfloor \frac{\hat{H}}{2} + \frac{P_{\hat{W}}}{s} + 0.5 \right\rfloor, p_{\text{map}}^g = \left\lfloor \frac{\hat{W}}{2} - \frac{P_{\hat{W}}}{s} + 0.5 \right\rfloor \qquad(1)$$</p>
<p>where <em>p</em><sub><em>z</em></sub><sup><em>map</em></sup> and <em>p</em><sub><em>z</em></sub><sup><em>map</em></sup> represent the coordinates of the projected point in the map <em>M</em>.</p>
<p>Once we build the grid map, we apply LSeg's visual encoder <em>f</em>(<strong>I</strong>) : ℝ<sup><em>H</em>×<em>W</em>×3</sup> → ℝ<sup><em>H</em>×<em>W</em>×<em>C</em></sup> to the RGB image <strong>I</strong><sub><em>k</em></sub> and generate the pixel-level embedding <strong>F</strong><sub><em>k</em></sub> ∈ ℝ<sup><em>H</em>×<em>W</em>×<em>C</em></sup>. Given the RGB-D registration, we project each image pixel <strong>u</strong>'s embedding <strong>q</strong> = <strong>F</strong><sub><em>k</em></sub>(<strong>u</strong>) ∈ ℝ<sup><em>C</em></sup> to its corresponding grid cell location (<em>p</em><sub><em>z</em></sub><sup><em>map</em></sup>, <em>p</em><sub><em>z</em></sub><sup><em>map</em></sup>) in the top-down grid map. Intuitively, there exist multiple 3D points projecting to the same grid location in the map. Thus, we average their embeddings, <strong>M</strong>(<em>p</em><sub><em>z</em></sub><sup><em>map</em></sup>, <em>p</em><sub><em>z</em></sub><sup><em>map</em></sup>) = 1/<em>n</em> ∑<sub><em>i</em>=1</sub><sup><em>n</em></sup> <strong>q</strong><sub><em>i</em></sub> where <strong>M</strong>(<em>p</em><sub><em>z</em></sub><sup><em>map</em></sup>, <em>p</em><sub><em>z</em></sub><sup><em>map</em></sup>) ∈ ℝ<sup><em>C</em></sup> represents the map features at the grid position (<em>p</em><sub><em>z</em></sub><sup><em>map</em></sup>, <em>p</em><sub><em>z</em></sub><sup><em>map</em></sup>), <em>n</em> represents the total number of points projecting to the grid location (<em>p</em><sub><em>z</em></sub><sup><em>map</em></sup>, <em>p</em><sub><em>z</em></sub><sup><em>map</em></sup>), and <strong>q</strong><sub><em>i</em></sub> ∈ ℝ<sup><em>C</em></sup> denotes the corresponding pixel embedding of each point. We note that these <em>n</em> points might not only come from a single frame, but also from points from multiple frames. Therefore, the resulting features contain the averaged embeddings from multiple views of the same object.</p>
<h4><em>B. Localizing Open-Vocabulary Landmarks</em></h4>
<p>We now describe how to localize landmarks in VLMaps with free-form natural language. Formally, we define the input language list as <strong>L</strong> = [<strong>I</strong><sub>0</sub>, <strong>I</strong><sub>1</sub>,..., <strong>I</strong><sub><em>M</em></sub>] where <strong>I</strong><sub><em>i</em></sub> represents the i-th category in text form, and <em>M</em> represents the number of categories defined by the user. Some examples of the input language list are ["chair", "sofa", "table", "other"] or ["furniture", "floor", "other"]. As Li <em>et al.</em> [11], we apply the pre-trained CLIP text encoder [10] to convert such list of texts into a list of vector embeddings [<strong>e</strong><sub>0</sub>, <strong>e</strong><sub>1</sub>,..., <strong>e</strong><sub><em>M</em></sub>], <strong>e</strong> ∈ ℝ<sup><em>C</em></sup>, which are organized into an embedding matrix <em>E</em> ∈ ℝ<sup><em>M</em>×<em>C</em></sup>, where each row of the matrix represents the embedding of a category. The map embeddings <em>M</em> are also flattened into a matrix <em>Q</em> ∈ ℝ<sup><em>H</em>W×<em>C</em></sup>, where each row represents the embedding of a pixel in the top-down grid map. We then compute the pixel-to-category similarity matrix <em>S</em> = <em>Q</em> · <em>E</em><sup><em>T</em></sup>, where <em>S</em> ∈ ℝ<sup><em>H</em>W×<em>M</em></sup>. Each element <em>S</em><sub><em>ij</em></sub> in the matrix stores the similarity value between a pixel and a text category, indicating how likely this pixel belongs to the class. By applying the argmax operator along the row direction to <em>S</em> and reshaping the resulting vector to shape <em>H</em> × <em>W</em>, we get the final segmentation result <em>R</em> ∈ ℝ<sup><em>H</em>×<em>W</em></sup>. Each element <em>R</em><sub><em>ij</em></sub> represents the label index of the input language list <strong>L</strong> at the grid map location (<em>i, j</em>). With the final resulting matrix <em>R</em>, we compute the most related language-based category for every pixel in the grid map.</p>
<h4><em>C. Generating Open-Vocabulary Obstacle Maps</em></h4>
<p>Building a VLMap enables us to generate obstacle maps that inherit the open-vocabulary nature of the VLMs used (LSeg and CLIP). Specifically, given a list of obstacle categories described with natural language, we can localize those obstacles at runtime to generate a binary map for collision avoidance and/or shortest path planning. A prominent use case for this is sharing a VLMap of the same environment between different robots with different embodiments (i.e., cross-embodiment problem [36], [37]), which may be useful for multi-agent coordination [38]. For example, a large mobile robot may need to navigate around a table (or other large furniture), while a drone can directly fly over it. By simply providing two different lists of obstacle categories – one for the large mobile robot (that contains "table"), and another for the drone (that does not), we can generate two distinct obstacles maps for the two robots to use respectively, sourced on-the-fly from the same VLMap.</p>
<p>To do so, we first extract an obstacle map <strong>O</strong> ∈ {0,1}<sup><em>H</em>×<em>W</em></sup> where each projected position of the depth point cloud in the top-down map is assigned 1, and otherwise 0. To avoid points from the floor or the ceiling, points <em>P</em><sub><em>W</em></sub> are filtered out depending on their height,</p>
<p>$$\mathcal{O}<em _mathcal_W="\mathcal{W">{ij} = \begin{cases} 1, &amp; t_1 \le P</em>$$}}^g \le t_2 \text{ and } p_{\text{map}}^e = i \text{ and } p_{\text{map}}^g = j \ 0, &amp; \text{otherwise} \end{cases} \tag{2</p>
<p>where <em>t</em><sub>1</sub>, <em>t</em><sub>2</sub> ∈ ℝ are the lower and upper thresholds for the <em>y</em>-component of the point <em>P</em><sub><em>W</em></sub>. Second, to obtain obstacle maps tailored to a certain embodiment, we define a list of potential obstacle categories <strong>L</strong><sub><em>obs</em></sub> = [<strong>I</strong><sub><em>obst</em></sub>, <strong>I</strong><sub><em>obs</em>1</sub>, ..., <strong>I</strong><sub><em>obs</em>M<em></sub>], where </em><em>I</em><em><sub></em>obs<em>i</sub> represents the i-th obstacle category in language, and </em>M* represents the total number of obstacle categories defined by the user. We</p>
<p>then apply the open-vocabulary landmark indexing introduced in Sec. III-B and obtain segmentation masks for all defined obstacles. For a specific embodiment $k$, we choose a subset of classes out of the whole potential obstacle list $\mathcal{L}<em e="e" m__k="m_{k">{o b s}$ and take the union of their segmentation masks to get the obstacles mask $\widetilde{\mathcal{O}}</em>}}$. We ignore false predictions of obstacles on floor region in $\widetilde{\mathcal{O}<em k="k">{e m</em>}}$ by taking the intersection with $\mathcal{O}$ to get the final obstacle map $\widetilde{\mathcal{O}<em k="k">{e m</em>$.}</p>
<h2>D. Zero-Shot Spatial Goal Navigation from Language</h2>
<p>In this section, we describe our approach to long-horizon (spatial) goal navigation, given a set of landmark descriptions specified by natural language instructions such as</p>
<div class="codehilite"><pre><span></span><code><span class="nv">move</span><span class="w"> </span><span class="nv">first</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">left</span><span class="w"> </span><span class="nv">side</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">counter</span>,<span class="w"> </span><span class="k">then</span>
<span class="nv">move</span><span class="w"> </span><span class="nv">between</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">sink</span><span class="w"> </span><span class="nv">and</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">oven</span>,<span class="w"> </span><span class="k">then</span><span class="w"> </span><span class="nv">move</span><span class="w"> </span><span class="nv">back</span>
<span class="nv">and</span><span class="w"> </span><span class="nv">forth</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">sofa</span><span class="w"> </span><span class="nv">and</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">table</span><span class="w"> </span><span class="nv">twice</span>
</code></pre></div>

<p>Notably different from prior work [12], [13], VLMaps allow us to reference precise spatial goals such as: "in between the sofa at the TV" or "three meters to the east of the chair." Specifically, we use a large language model (LLM) to interpret the input natural language commands and break them down into subgoals [35], [13], [14]. In contrast to prior work, which may reference these subgoals with language and map to low-level policies with semantic translation [39] or affordances [35], [40], [41], [42], we leverage the code-writing capabilities of LLMs to generate executable Python robot code [43], [33], [44], [27] that can (i) make precise calls to parameterized navigation primitives, and (ii) perform arithmetic when needed. The generated code can directly be executed on the robot with the built-in Python exec function.</p>
<p>Note that recent works [43], [33], [44], [27] have shown that code-writing language models (e.g., Codex [44]) trained on billions of lines of code from Github can be used to synthesize new simple Python programs from docstrings. In this work, we re-purpose these models for mobile robot planning, by priming them with several input examples of natural language commands (formatted as comments) paired with corresponding robot code (via few-shot prompting). The robot code can express functions or logic structures (if-then-else statements or for/while loops) and parameterize API calls (e.g., robot.move_to(target_name) or robot.turn(degrees). The full list is available in the Appendix, Sec. A) that map to spatial behaviors specified by the language commands. At test time, the models can subsequently take in new commands and autonomously re-compose API calls to generate new robot code respectively (prompt in gray, input task commands in green, and generated outputs are highlighted):</p>
<div class="codehilite"><pre><span></span><code><span class="gh">#</span> move a bit to the right of the fridge
robot.move_to_right(&#39;refrigerator&#39;)
<span class="gh">#</span> face the toilet
robot.face(&#39;toilet&#39;)
<span class="gh">#</span> move to the west of the chair
robot.move_west(&#39;chair&#39;)
<span class="gh">#</span> turn right 20 degrees
robot.turn(20)
<span class="gh">#</span> move back and forth to the chair and table 3 times
pos1 = robot.get_pos(&#39;chair&#39;)
...
<span class="gh">#</span> move forward for 3 meters
robot.move_forward(3)
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="gh">#</span> move first to the left side of the counter, then
move between the sink and the oven, then move back and
forth to the sofa and the table twice
robot.move_to_left(&#39;counter&#39;)
robot.move_in_between(&#39;sink&#39;, &#39;oven&#39;)
pos1 = robot.get_pos(&#39;sofa&#39;)
pos2 = robot.get_pos(&#39;table&#39;)
for i in range(2):
    robot.move_to(pos1)
    robot.move_to(pos2)
<span class="gh">#</span> move 2 meters north of the laptop, then move 3
meters rightward
robot.move_north(&#39;laptop&#39;)
robot.face(&#39;laptop&#39;)
robot. turn(180)
robot.move_forward(2)
robot. turn(90)
robot.move_forward(3)
</code></pre></div>

<p>The code-writing LLM generates code that not only references the new landmarks mentioned in the language commands (as comments), but also can chain together new sequences of API calls to follow unseen instructions accordingly. The prompt has been truncated for brevity here. Please see the full prompt in the Appendix (Sec. B).</p>
<p>The navigation primitive functions being called by the language model (e.g., robot.move_to_left('counter')) use a pre-generated VLMap to localize the coordinates of the open-vocabulary landmarks ("counter") in the maps (described in Sec. III-B) modified with predefined scripted offsets (to define "left"). We then navigate to these coordinates using an off-the-shelf navigation stack that takes as input the embodiment-specific obstacle map (generated using the same VLMap, with the process described in Sec. III-C).</p>
<h2>IV. EXPERIMENTS</h2>
<p>The goals of our experiments are four-fold: (i) to quantitatively evaluate our VLMaps approach against recent open-vocabulary navigation baselines on the standard task of multi-object goal navigation (Sec. IV-B), (ii) to investigate whether our method can better navigate to spatial goals specified by language commands versus alternative approaches (Sec. IV-C), (iii) to study whether VLMaps with their capacity to specify open-vocabulary obstacle maps can provide utility in improving the navigation efficiency of different robots with different embodiments (Sec. IV-D), and (iv) to demonstrate on real robots that VLMaps can enable zero-shot spatial goal navigation given unseen language instructions (Sec. IV-E).</p>
<h2>A. Simulation Setup</h2>
<p>Experimental setup. We use the Habitat simulator [45] with the Matterport3D dataset [46] for the evaluation of multi-object and spatial goal navigation tasks. The dataset contains a large set of realistic indoor scenes that help evaluate the generalization capabilities of navigating agents. To evaluate the creation of open-vocabulary multi-embodiment obstacle maps, we adopt the AI2THOR simulator due to its support of multiple agent types, such as LoCoBot and drone. In these two environments, the robot is required to navigate in a continuous environment with actions: move forward 0.05 meters, turn left 1 degree, turn right 1 degree and stop. For map creation in Habitat, we collect 12,096 RGB-D frames across ten different scenes and record the camera</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4: Object mask for object type "chair". 4a shows the top-down map of the scene and the red circles specify the locations of type "chair". 4b shows the ground truth mask for type "chair" and 4c, 4d, 4e show the predicted masks by CLIP Map, CoW and VLMaps.</p>
<p>pose of each frame. Similarly, we collect 1,826 RGB-D frames across ten rooms in AI2THOR.</p>
<p><strong>Baselines.</strong> We evaluate VLMaps against three baseline methods, all of which utilize visual-language models and are capable of zero-shot language-based navigation:</p>
<ul>
<li>LM-Nav [13] creates a graph where image observations of an environment are stored as nodes while the proximity between images are represented as edges. By combining GPT-3 and CLIP, it parses language instructions into a list of landmarks and plans on the graph towards corresponding nodes.</li>
<li>CLIP on Wheels (CoW) [12] achieves language-based object navigation by building a saliency map for the target category with CLIP and GradCAM [47]. By thresholding the saliency values, it retrieves a segmentation mask for the target object category and then plans the path on the map.</li>
<li>CLIP-features-based map (CLIP Map) is an ablative baseline that generates a feature map for the environment in a similar way as ours. Instead of using LSeg visual features, it projects the CLIP visual features onto the map averaged across views. Object category masks are generated by thresholding the similarity between map features and the object category features.</li>
</ul>
<p>For additional context and analysis, we also report results from a system that has access to a ground truth semantic map for navigation, to provide a systems-level upper bound on performance.</p>
<h3><em>B. Multi-Object Navigation</em></h3>
<p>We collect 91 sequences of tasks for the evaluation of object navigation. In each sequence, we randomly specify a starting position of the robot in one scene and then pick four among 30 object categories as subgoal object types. The robot is required to navigate to these four subgoals sequentially. In each sequence of subgoals, when the robot reaches one subgoal category, it should call the <strong>stop</strong> action to indicate its progress. We consider the navigation to one subgoal as success when the distance of stop position from the correct object is within one meter. To evaluate the long-horizon navigation capabilities of the agents, we compute the success rate (SR) of continuously reaching one to four subgoals in a sequence, shown in Tab. I. We also report the independent subgoal success rate, which indicates the total successful subgoals number divided by the total subgoals number (364 subgoals).</p>
<table>
<thead>
<tr>
<th>Tasks</th>
<th>No. Subgoals in a Row</th>
<th></th>
<th></th>
<th></th>
<th>Independent Subgoals</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>1</td>
<td>2</td>
<td>3</td>
<td>4</td>
<td></td>
</tr>
<tr>
<td>LM-Nav [13]</td>
<td>26</td>
<td>4</td>
<td>1</td>
<td>1</td>
<td>26</td>
</tr>
<tr>
<td>CoW [12]</td>
<td>42</td>
<td>15</td>
<td>7</td>
<td>3</td>
<td>36</td>
</tr>
<tr>
<td>CLIP Map</td>
<td>33</td>
<td>8</td>
<td>2</td>
<td>0</td>
<td>30</td>
</tr>
<tr>
<td>VLMaps (ours)</td>
<td>59</td>
<td>34</td>
<td>22</td>
<td>15</td>
<td>59</td>
</tr>
<tr>
<td>GT Map</td>
<td>91</td>
<td>78</td>
<td>71</td>
<td>67</td>
<td>85</td>
</tr>
</tbody>
</table>
<p>TABLE I: The VLMaps-approach performs favorably over alternative open-vocabulary baselines on multi-object navigation (success rate [%]) and specifically excels on longer-horizon tasks with multiple sub-goals.</p>
<p>We observe that VLMaps performs consistently better compared to all baselines. LM-Nav has a weak performance as it is only able to navigate to locations represented by images stored in graph nodes. To obtain more insights into the map-based methods, we visualize the object masks generated by VLMaps, CoW, and CLIP Map, in comparison to GT, in Fig. 4. The masks generated by CoW (Fig. 4d) and CLIP (Fig. 4c) both contain considerable false positive predictions. Since the planning generates the path to the nearest masked target area, these predictions lead to planning towards wrong goals. In contrast, the predictions obtained with VLMaps shown in Fig. 4e are less noisy, which leads to higher success rates in object navigation.</p>
<h3><em>C. Zero-Shot Spatial Goal Navigation from Language</em></h3>
<p>In these experiments, we investigate the performance of VLMaps versus other baselines for zero-shot <em>spatial</em> goal navigation from language. Our benchmark consists of 21 trajectories in seven scenes, with manually specified corresponding language instructions for evaluation. Each trajectory contains four different spatial locations as subgoals. Examples of subgoals are "east of the table", "in between the chair and the sofa", or "move forward 3 meters". There are also instructions for the robot to realign itself in reference to nearby objects such as "with the counter on your right". We only consider a subgoal as having been achieved, when the robot reaches the subgoal location within a range of one meter. We compute the in-a-row success rate in the same way as in Sec. IV-B. For all map-based methods, including CoW, CLIP Map, ground truth semantic map and our method, we apply the code generation techniques introduced in Sec. III-D. For LM-Nav, we simply use the same parsing method in the original paper [13] to break down the language instruction into subgoals.</p>
<table>
<thead>
<tr>
<th>Tasks</th>
<th>No. Subgoals in a Row</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>1</td>
<td>2</td>
<td>3</td>
<td>4</td>
</tr>
<tr>
<td>LM-Nav [13]</td>
<td>5</td>
<td>5</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>CoW [12]</td>
<td>33</td>
<td>5</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>CLIP Map</td>
<td>19</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>VLMaps (ours)</td>
<td>62</td>
<td>33</td>
<td>14</td>
<td>10</td>
</tr>
<tr>
<td>GT Map</td>
<td>76</td>
<td>48</td>
<td>33</td>
<td>29</td>
</tr>
</tbody>
</table>
<p>TABLE II: The VLMaps approach can navigate to spatial goals specified by natural language and outperforms other open-vocabulary zero-shot navigation baseline alternatives (success rate [%]) in this setting.</p>
<p>Tab. II summarizes the zero-shot spatial goal navigation success rates. Our method outperforms other baselines in this task.</p>
<p>Different from object navigation tasks where agents only need to approach a certain object type within a range disregarding the relative spatial shift to the object, the language-based spatial goal navigation tasks require the robot to accurately arrive at the described location in reference to the object. This poses a bigger challenge to the landmark localization ability of the method. The low localization ability of CoW and CLIP Map analyzed in the previous section (Sec. IV-B) leads to their high failure rates in this task.</p>
<h3>IV-D Cross-Embodiment Navigation</h3>
<p>We study the ability of VLMaps to improve navigation efficiency by retrieving different obstacle maps for navigation with different embodiments (given the same VLMap). We evaluate more than 100 sequences of subgoals as in Sec. IV-B in the AI2THOR simulator. We evaluate VLMaps on both a LoCoBot and a drone to test its capability of generating obstacle maps at runtime for multi-embodiment navigation. We apply the open-vocabulary obstacle map generation method in Sec. III-C to create an obstacle map for the drone (drone map) and one for the LoCoBot (ground map) by defining obstacles for them differently (see the prompts in Appendix Sec. E). We test the navigation ability of these embodiments with three setups: a LoCoBot with a ground map, a drone with a ground map, and a drone with a drone map.</p>
<p>We evaluate the Success Rate (SR) and the Success rate weighted by the (normalized inverse) Path Length (SPL) [48] defined as: $S P L=\frac{1}{N} \sum_{i=1}^{N} S_{i} \frac{l_{i}}{max\left(p_{i}, l_{i}\right)}$ where $N$ is the total number of evaluated tasks, $S_{i} \in{0,1}$ is the binary indicator of success, $l_{i}$ denotes the ground truth shortest path length, and $p_{i}$ denotes the actual path length of the agent in navigation. This metric indicates how efficient the actual path is compared to the ground truth shortest path when the navigation task is achieved. In our three setups, the ground truth trajectories for the LoCoBot and the drone are planned on floor-level and on height level of 1.7 meters respectively.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Tasks</th>
<th style="text-align: center;">No. Subgoals in a Row</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Independent</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">4</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Subgoals</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SR SPL</td>
<td style="text-align: center;">SR SPL</td>
<td style="text-align: center;">SR SPL</td>
<td style="text-align: center;">SR SPL</td>
<td style="text-align: center;">SR</td>
<td style="text-align: center;">SPL</td>
<td style="text-align: center;">SR</td>
</tr>
<tr>
<td style="text-align: center;">LoCoBot (ground map)</td>
<td style="text-align: center;">53</td>
<td style="text-align: center;">49.0</td>
<td style="text-align: center;">28</td>
<td style="text-align: center;">17.8</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">6.7</td>
<td style="text-align: center;">62.5</td>
</tr>
<tr>
<td style="text-align: center;">Drone (ground map)</td>
<td style="text-align: center;">53</td>
<td style="text-align: center;">41.8</td>
<td style="text-align: center;">28</td>
<td style="text-align: center;">15.5</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">5.3</td>
<td style="text-align: center;">62.0</td>
</tr>
<tr>
<td style="text-align: center;">Drone (drone map)</td>
<td style="text-align: center;">56</td>
<td style="text-align: center;">45.4</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">16.3</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">7.0</td>
<td style="text-align: center;">72.5</td>
</tr>
</tbody>
</table>
<p>TABLE III: VLMaps generate different obstacle maps for different robot embodiments, conditioned on a list of obstacle categories. This improves object navigation efficiency (Success [\%] weighted by Path Length, SPL).</p>
<p>The results provided in Tab. III show that the average navigation success rates of the ground-map version of the LoCoBot and the drone are similar because the same obstacles map is used for planning. However, there is an obvious gap between their SPL values. This is because when the drone does not have access to a customized obstacle map, it fails to benefit from flying over ground objects to improve the navigation efficiency. In contrast, while achieving similar success rate compared to the drone with a ground map, the drone with a drone map manages to navigate with higher path efficiency, reflected by the increased SPL values. The comparable SPL values for the drone with the drone map and the LoCoBot with the ground map shows that VLMaps help to general-</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 5: VLMaps enable different embodiments to define their own obstacle maps for navigation. The left image shows the top-down view of an environment. The middle columns show the observations of agents during navigation. The images on the right demonstrate the obstacles maps generated for different embodiments and the corresponding navigation paths.
ize the navigation efficiency among different embodiments. An example of the multi-embodiment object navigation task is shown in Fig. 5, where by defining a more efficient obstacles map, the drone flies over the sofa and reaches the laptop target directly, while the LoCoBot has to move aside first to avoid colliding with the sofa.</p>
<h2>E. Real Robot Experiments</h2>
<p>We also perform real-world experiments using the HSR mobile robot for indoor navigation given natural language commands. For map creation, we record 374 frames for the evaluated scene and use an off-the-shelf RGB-D SLAM solution, RTAB-Map [49] to estimate the camera poses. During inference, we also use the global localization module of RTAB-Map to initialize the robot pose. We test our VLMaps in a semantically rich indoor scene with more than ten different classes of objects. We define 20 different language-based spatial goals for testing purposes. Across different test runs, we initialize the robot at different locations.</p>
<p>The robot finishes ten navigation goals out of the 20. Among the successful trials, six of them are spatial goals like "move between the chair and the wooden box" or "move to the south of the table". three of them are goals relative to the current position of the robot like "move 3 meters right and then move 2 meters left". Another one is an instruction with repetition: "move between the keyboard and the laptop twice". We observe that failure cases are caused by: 1) inaccurate depth, which introduces noise during the map creation and decreases the landmark indexing accuracy and 2) action noise, which can negatively influence the navigation performance at test time. Overall, these results demonstrate the ability of VLMaps to index landmarks with natural language in the real world and, more importantly, its applicability to achieve a wide variety of open-vocabulary language-based spatial navigation goals.</p>
<h2>V. DISCUSSION AND LIMITATIONS</h2>
<p>In this work, we propose VLMaps, a spatial map representation enriched with pretrained visual-language features, which enables natural language indexing in the map. When combined with large language models, VLMaps can be applied in zero-shot spatial goal navigation and can be shared among multiple robots with different embodiments to generate new obstacles map in runtime. VLMaps are not without limitations. Notably, they remain sensitive to 3D reconstruction noise and odometry drift during navigation. They also cannot resolve object ambiguities during landmark indexing when the scene is cluttered with similar objects. In future work, we plan to improve VLMaps with better visual language models and to extend it to scenes with dynamic objects and moving humans.</p>
<h2>REFERENCES</h2>
<p>[1] T. P. McNamara, J. K. Hardy, and S. C. Hirtle, "Subjective hierarchies in spatial memory." Journal of Experimental Psychology: Learning, Memory, and Cognition, vol. 15, no. 2, p. 211, 1989.
[2] M. M. Chun and Y. Jiang, "Contextual cueing: Implicit learning and memory of visual context guides spatial attention," Cognitive psychology, vol. 36, no. 1, pp. 28-71, 1998.
[3] E. L. Newman, J. B. Caplan, M. P. Kirschen, I. O. Korolev, R. Sekuler, and M. J. Kahana, "Learning your way around town: How virtual taxicab drivers learn to use both layout and landmark information," Cognition, vol. 104, no. 2, pp. 231-253, 2007.
[4] S. Thrun, W. Burgard, and D. Fox, "A probabilistic approach to concurrent mapping and localization for mobile robots," Autonomous Robots, vol. 5, no. 3, pp. 253-271, 1998.
[5] F. Endres, J. Hess, N. Engelhard, J. Sturm, D. Cremers, and W. Burgard, "An evaluation of the rgb-d slam system," in 2012 IEEE international conference on robotics and automation. IEEE, 2012, pp. 1691-1696.
[6] S. Tellex, T. Kollar, S. Dickerson, M. Walter, A. Banerjee, S. Teller, and N. Roy, "Understanding natural language commands for robotic navigation and mobile manipulation," in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 25, no. 1, 2011, pp. 1507-1514.
[7] M. MacMahon, B. Stankiewicz, and B. Kuipers, "Walk the talk: Connecting language, knowledge, and action in route instructions," Def, vol. 2, no. 6, p. 4, 2006.
[8] P. Anderson, Q. Wu, D. Teney, J. Bruce, M. Johnson, N. Sünderhauf, I. Reid, S. Gould, and A. Van Den Hengel, "Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 3674-3683.
[9] P. Anderson, A. Shrivastava, J. Truong, A. Majumdar, D. Parikh, D. Batra, and S. Lee, "Sim-to-real transfer for vision-and-language navigation," in Conference on Robot Learning. PMLR, 2021, pp. 671-681.
[10] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark et al., "Learning transferable visual models from natural language supervision," in International Conference on Machine Learning. PMLR, 2021, pp. 8748-8763.
[11] B. Li, K. Q. Weinberger, S. Belongie, V. Koltun, and R. Ranftl, "Languagesdriven semantic segmentation," in International Conference on Learning Representations, 2021.
[12] S. Y. Gadro, M. Wortsman, G. Bharco, L. Schmidt, and S. Song, "Clip on wheels: Zero-shot object navigation as object localization and exploration," arXiv preprint arXiv:2203.10421, 2022.
[13] D. Shah, B. Osinski, B. lchter, and S. Levine, "Lm-nav: Robotic navigation with large pre-trained models of language, vision, and action," arXiv preprint arXiv:2207.04429, 2022.
[14] A. Zeng, A. Wong, S. Welker, K. Choromanski, F. Tombari, A. Purohit, M. Ryoo, V. Sindhwani, J. Lee, V. Vanhoucke et al., "Socratic models: Composing zero-shot multimodal reasoning with language," arXiv preprint arXiv:2204.00598, 2022.
[15] R. F. Salas-Moreno, R. A. Newcombe, H. Strasdat, P. H. Kelly, and A. J. Davison, "Slams+: Simultaneous localisation and mapping at the level of objects," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2013, pp. 1352-1359.
[16] J. McCormac, A. Handa, A. Davison, and S. Leutenegger, "Semanticfusion: Dense 3d semantic mapping with convolutional neural networks," in 2017 IEEE International Conference on Robotics and automation (ICRA). IEEE, 2017, pp. 4628-4635.
[17] M. Runz, M. Buffier, and L. Agapito, "Maskfusion: Real-time recognition, tracking and reconstruction of multiple moving objects," in 2018 IEEE International Symposium on Mixed and Augmented Reality (ISMAR). IEEE, 2018, pp. 10-20.
[18] J. McCormac, R. Clark, M. Bloesch, A. Davison, and S. Leutenegger, "Fusion++: Volumetric object-level slam," in 2018 international conference on 3D vision (3DV). IEEE, 2018, pp. 32-41.
[19] B. Xu, W. Li, D. Tzoumanikas, M. Bloesch, A. Davison, and S. Leutenegger, "Mid-fusion: Octree-based object-level multi-instance dynamic slam," in 2019 International Conference on Robotics and Automation (ICRA). IEEE, 2019, pp. 5231-5237.
[20] N. Hughes, Y. Chang, and L. Carlone, "Hydra: a real-time spatial perception system for 3d scene graph construction and optimization," Proceedings of Robotics: Science and Systems. New York City, NY, USA, http://dx. doi. org/10.15607/RSS, 2022.
[21] S.-C. Wu, J. Wald, K. Tateno, N. Navab, and F. Tombari, "Scenegraphfusion: Incremental 3d scene graph prediction from rgb-d sequences," in Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 7515-7525.
[22] J. Krantz, E. Wijmans, A. Majumdar, D. Batra, and S. Lee, "Beyond the nav-graph: Vision-and-language navigation in continuous environments," in European Conference on Computer Vision. Springer, 2020, pp. 104-120.
[23] D. Fried, R. Hu, V. Cirik, A. Rohrbach, J. Andreas, L.-P. Morency, T. Berg-Kirkpatrick, K. Saenko, D. Klein, and T. Darrell, "Speaker-follower models for vision-and-language navigation," Advances in Neural Information Processing Systems, vol. 31, 2018.
[24] P.-L. Guhur, M. Tapaswi, S. Chen, I. Laptev, and C. Schmid, "Airbert: In-domain pretraining for vision-and-language navigation," in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 1634-1643.
[25] J. Krantz, A. Gokaslan, D. Batra, S. Lee, and O. Maksymets, "Waypoint models for instruction-guided navigation in continuous environments," in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 15 162-15 171.
[26] Y. Hong, Z. Wang, Q. Wu, and S. Gould, "Bridging the gap between learning in discrete and continuous environments for vision-and-language navigation," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 15 439-15 449.
[27] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., "Language models are few-shot learners," Advances in neural information processing systems, vol. 33, pp. 1877-1901, 2020.
[28] A. Kamath, M. Singh, Y. LeCun, G. Synnaeve, I. Misra, and N. Carion, "Mdetr-modulated detection for end-to-end multi-modal understanding," in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 1780-1790.
[29] X. Gu, T.-Y. Lin, W. Kuo, and Y. Cui, "Open-vocabulary object detection via vision and language knowledge distillation," in International Conference on Learning Representations, 2021.
[30] M. Shridhar, L. Manuelli, and D. Fox, "Cliport: What and where pathways for robotic manipulation," in Conference on Robot Learning. PMLR, 2022, pp. 894-906.
[31] O. Mees, L. Hermann, E. Rosete-Beas, and W. Burgard, "Calvin: A benchmark for language-conditioned policy learning for long-horizon robot manipulation tasks," IEEE Robotics and Automation Letters (RA-L), vol. 7, no. 3, pp. 7327-7334, 2022.
[32] O. Mees, L. Hermann, and W. Burgard, "What matters in language conditioned robotic imitation learning over unstructured data," IEEE Robotics and Automation Letters (RA-L), vol. 7, no. 4, pp. 11 205-11 212, 2022.
[33] O. Mees, J. Borja-Diaz, and W. Burgard, "Grounding language with visual affordances over unstructured data," arXiv preprint arXiv:2210.01911, 2022.
[34] B. Chen, F. Xia, B. lchter, K. Rao, K. Gopalakrishnan, M. S. Ryoo, A. Stone, and D. Kappler, "Open-vocabulary queryable scene representations for real world planning," arXiv preprint arXiv:2209.09874, 2022.
[35] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog et al., "Do as i can, not as i say: Grounding language in robotic affordances," arXiv preprint arXiv:2204.01691, 2022.
[36] K. Zakka, A. Zeng, P. Florence, J. Tompson, J. Bohg, and D. Dwibedi, "Xirl: Cross-embodiment inverse reinforcement learning," in Conference on Robot Learning. PMLR, 2022, pp. 537-546.
[37] A. Ganapathi, P. Florence, J. Varley, K. Burns, K. Goldberg, and A. Zeng, "Implicit kinematic policies: Unifying joint and cartesian action spaces in end-to-end robot learning," arXiv preprint arXiv:2203.01983, 2022.
[38] J. Wu, X. Sun, A. Zeng, S. Song, S. Rusinkiewicz, and T. Funkhouser, "Spatial intention maps for multi-agent mobile manipulation," in 2021 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2021, pp. 8749-8756.
[39] W. Huang, P. Abbeel, D. Pathak, and I. Mordatch, "Language models as zero-shot planners: Extracting actionable knowledge for embodied agents," arXiv:2201.07207, 2022.
[40] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mordatch, Y. Chebotar et al., "Inner monologue: Embodied reasoning through planning with language models," arXiv preprint arXiv:2207.05608, 2022.
[41] A. Zeng, "Learning visual affordances for robotic manipulation," Ph.D. dissertation, Princeton University, 2019.
[42] J. Borja-Diaz, O. Mees, G. Kalweit, L. Hermann, J. Boedecker, and W. Burgard, "Affordance learning from play for sample-efficient policy learning," in Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), Philadelphia, USA, 2022.</p>
<p>[43] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence, and A. Zeng, "Code as policies: Language model programs for embodied control," arXiv preprint arXiv:2209.07753, 2022.
[44] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman et al., "Evaluating large language models trained on code," arXiv:2107.03374, 2021.
[45] M. Savva, A. Kadian, O. Maksymets, Y. Zhao, E. Wijmans, B. Jain, J. Straub, J. Liu, V. Koltun, J. Malik, D. Parikh, and D. Batra, "Habitat: A Platform for Embodied AI Research," in Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2019.
[46] A. Chang, A. Dai, T. Funkhouser, M. Halber, M. Niessner, M. Savva, S. Song, A. Zeng, and Y. Zhang, "Matterport3D: Learning from RGB-D data in indoor environments," International Conference on 3D Vision (3DV), 2017.
[47] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra, "Grad-cam: Visual explanations from deep networks via gradient-based localization," International Journal of Computer Vision, vol. 128, no. 2, pp. 336-359, 2020.
[48] P. Anderson, A. Chang, D. S. Chaplot, A. Dosovitskiy, S. Gupta, V. Koltun, J. Kosecka, J. Malik, R. Mottaghi, M. Savva et al., "On evaluation of embodied navigation agents," arXiv preprint arXiv:1807.06757, 2018.
[49] M. Labbé and F. Michaud, "Rtab-map as an open-source lidar and visual simultaneous localization and mapping library for large-scale and long-term online operation," Journal of Field Robotics, vol. 36, no. 2, pp. 416-446, 2019.
[50] J. Long, E. Shelhamer, and T. Darrell, "Fully convolutional networks for semantic segmentation," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2015, pp. 3431-3440.</p>
<h2>APPENDIX</h2>
<h2>A. Full List of Navigation Primitives</h2>
<p>Our full list of navigation primitives are listed in Table IV.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">primitives</th>
<th style="text-align: center;">functions</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">move_to(pos)</td>
<td style="text-align: center;">move to a position on the map.</td>
</tr>
<tr>
<td style="text-align: center;">move_to_left(object_name)</td>
<td style="text-align: center;">move to the left side of the nearest front object.</td>
</tr>
<tr>
<td style="text-align: center;">move_to_right(object_name)</td>
<td style="text-align: center;">move to the right side of the nearest front object.</td>
</tr>
<tr>
<td style="text-align: center;">get_pos(object_name)</td>
<td style="text-align: center;">get the map position of the nearest front object.</td>
</tr>
<tr>
<td style="text-align: center;">get_contour(object_name)</td>
<td style="text-align: center;">get the contour turning points of the nearest front object on the map.</td>
</tr>
<tr>
<td style="text-align: center;">with_object_on_left(object_name)</td>
<td style="text-align: center;">turn until the nearest object is on the robot's left side.</td>
</tr>
<tr>
<td style="text-align: center;">with_object_on_right(object_name)</td>
<td style="text-align: center;">turn until the nearest object is on the robot's right side.</td>
</tr>
<tr>
<td style="text-align: center;">move_in_between(object_a, object_b)</td>
<td style="text-align: center;">move in between two objects.</td>
</tr>
<tr>
<td style="text-align: center;">turn(angle)</td>
<td style="text-align: center;">turn right a certain angle. If the angle value is negative, turn left.</td>
</tr>
<tr>
<td style="text-align: center;">face(object_name)</td>
<td style="text-align: center;">turn until the nearest object is in front of the robot.</td>
</tr>
<tr>
<td style="text-align: center;">turn_absolute(angle)</td>
<td style="text-align: center;">turn to absolute angle. 0 is north, 90 is east, -90 is west, 180 is south.</td>
</tr>
<tr>
<td style="text-align: center;">move_north(object_name)</td>
<td style="text-align: center;">move to the north side of the nearest front object.</td>
</tr>
<tr>
<td style="text-align: center;">move_south(object_name)</td>
<td style="text-align: center;">move to the south side of the nearest front object.</td>
</tr>
<tr>
<td style="text-align: center;">move_east(object_name)</td>
<td style="text-align: center;">move to the east side of the nearest front object.</td>
</tr>
<tr>
<td style="text-align: center;">move_west(object_name)</td>
<td style="text-align: center;">move to the west side of the nearest front object.</td>
</tr>
<tr>
<td style="text-align: center;">move_to_object(object_name)</td>
<td style="text-align: center;">move to the nearest object.</td>
</tr>
<tr>
<td style="text-align: center;">move_forward(dist)</td>
<td style="text-align: center;">move forward "dist" meters.</td>
</tr>
</tbody>
</table>
<p>TABLE IV: List of the navigation primitives used.</p>
<h2>B. Full Prompts</h2>
<p>Our full prompts used for getting the navigation results are listed below.</p>
<div class="codehilite"><pre><span></span><code><span class="gh">#</span> move a bit to the right of the fridge
robot.move_to_right(&#39;refrigerator&#39;)
<span class="gh">#</span> move in between the couch and bookshelf
robot.move_in_between(&#39;couch&#39;, &#39;bookshelf&#39;)
<span class="gh">#</span> face the toilet
robot.face(&#39;toilet&#39;)
<span class="gh">#</span> move to the west of the chair
robot.move_west(&#39;chair&#39;)
<span class="gh">#</span> turn right 20 degrees
robot.turn(20)
<span class="gh">#</span> find any chairs in the environment
robot.move_to_object(&#39;chair&#39;)
<span class="gh">#</span> with the television on your left
robot.with_object_on_left(&#39;television&#39;)
<span class="gh">#</span> move forward for 3 meters
robot.move_forward(3)
<span class="gh">#</span> move right 2 meters
robot.turn(90)
robot.move_forward(2)
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="gh">#</span> move back and forth to the chair and table 3 times
pos1 = robot.get_pos(&#39;chair&#39;)
pos2 = robot.get_pos(&#39;table&#39;)
for i in range(3):
    robot.move_to(pos1)
    robot.move_to(pos2)
<span class="gh">#</span> move 3 meters south of the chair
robot.move_south(&#39;chair&#39;)
robot.face(&#39;chair&#39;)
robot.turn(180)
robot.move_forward(3)
<span class="gh">#</span> turn west
robot.turn_absolute(-90)
<span class="gh">#</span> turn east
robot.turn_absolute(90)
<span class="gh">#</span> turn south
robot.turn_absolute(180)
<span class="gh">#</span> turn north
robot.turn_absolute(0)
<span class="gh">#</span> turn east and then turn left 90 degrees
robot.turn_absolute(90)
robot.turn(-90)
<span class="gh">#</span> navigate to 3 meters right of the table
robot.move_to_right(&#39;table&#39;)
robot.face(&#39;table&#39;)
robot.turn(180)
robot.move_forward(3)
</code></pre></div>

<h2>C. Prompt engineering.</h2>
<p>For all methods in this work (including baselines), when using CLIP text encoding, instead of simply prompting the label of the object categories, we use the ensemble of prompt templates like "A photo of label", "A picture of label" mentioned in [10] to improve the retrieval performance.</p>
<h2>D. Top-Down Map Semantic Segmentation</h2>
<p>For ablation purposes, we compute the semantic segmentation masks for the top-down maps in the Habitat simulator with the Matterport3D dataset. We use the collected RGB-D frames mentioned in Sec. IV-A to create the VLMaps and the CLIP on Wheels saliency maps. We evaluate all the semantic categories (the full list can be found in the link ${ }^{1}$ ) supported in the Matterport3D dataset except "void", "floor", "ceiling", "objects", "misc". To get the ground truth semantic masks, we use the RGB-D frames and the ground truth image semantic masks to create a semantic top-down map. We back-project the depth pixels to the 3D space and project them to the top-down map. We assign the associated semantic values to the top-down map pixels. If multiple points are projected to the same location, we overwrite the old value if the new point's height is larger than the previous points. To compute semantic masks for VLMaps, we apply the open-vocabulary landmark indexing technique described in Sec. III-B to the whole list of categories. To compute semantic masks for the CLIP on Wheels, we compute the saliency values and apply the same thresholding process as in [12] to get a binary mask for each category. We evaluate the semantic segmentation metrics used in [50]. The segmentation results is shown in Table V.</p>
<p>We also show the IOU values of the top-10 frequent categories in Table VI. The table shows that VLMaps performs better than</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>CoW Map in most of the top-10 frequent categories. This is mainly because the GradCam used in CoW introduces a lot of noise in the saliency map, causing over-segmentation in the results. We also note that in the class "seating", VLMaps gets 0 IOU score. Since the LSeg model we used is pre-trained on segmentation datasets where some query classes might not be in the pre-defined training categories, LSeg's visual encoder will encode visually unseen objects ("seating") to a similar seen object's embedding space (like "chair" or "sofa" here). As a result, visual-text misalignment could happen.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Metric</th>
<th style="text-align: center;">CoW Map</th>
<th style="text-align: center;">VLMaps (ours)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">pixel accuracy</td>
<td style="text-align: center;">66.1</td>
<td style="text-align: center;">$\mathbf{9 2 . 3}$</td>
</tr>
<tr>
<td style="text-align: left;">mean accuracy</td>
<td style="text-align: center;">9.6</td>
<td style="text-align: center;">$\mathbf{2 7 . 7}$</td>
</tr>
<tr>
<td style="text-align: left;">mIOU</td>
<td style="text-align: center;">5.7</td>
<td style="text-align: center;">$\mathbf{1 9 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;">frequency weighted mIOU</td>
<td style="text-align: center;">42.9</td>
<td style="text-align: center;">$\mathbf{8 5 . 9}$</td>
</tr>
</tbody>
</table>
<p>TABLE V: Top-Down Map Semantic Segmentation Results.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Class</th>
<th style="text-align: center;">Class Portion</th>
<th style="text-align: center;">IOU</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">CoW</td>
<td style="text-align: center;">VLMaps (ours)</td>
</tr>
<tr>
<td style="text-align: left;">wall</td>
<td style="text-align: center;">39.70</td>
<td style="text-align: center;">63.80</td>
<td style="text-align: center;">$\mathbf{9 8 . 5 7}$</td>
</tr>
<tr>
<td style="text-align: left;">chair</td>
<td style="text-align: center;">9.66</td>
<td style="text-align: center;">6.99</td>
<td style="text-align: center;">$\mathbf{7 7 . 0 4}$</td>
</tr>
<tr>
<td style="text-align: left;">table</td>
<td style="text-align: center;">8.14</td>
<td style="text-align: center;">1.20</td>
<td style="text-align: center;">$\mathbf{1 3 . 8 2}$</td>
</tr>
<tr>
<td style="text-align: left;">door</td>
<td style="text-align: center;">4.94</td>
<td style="text-align: center;">12.50</td>
<td style="text-align: center;">$\mathbf{2 8 . 2 8}$</td>
</tr>
<tr>
<td style="text-align: left;">seating</td>
<td style="text-align: center;">4.54</td>
<td style="text-align: center;">$\mathbf{8 . 7 7}$</td>
<td style="text-align: center;">0.00</td>
</tr>
<tr>
<td style="text-align: left;">stairs</td>
<td style="text-align: center;">3.75</td>
<td style="text-align: center;">12.35</td>
<td style="text-align: center;">$\mathbf{2 2 . 0 2}$</td>
</tr>
<tr>
<td style="text-align: left;">cabinet</td>
<td style="text-align: center;">3.38</td>
<td style="text-align: center;">1.13</td>
<td style="text-align: center;">$\mathbf{1 . 8 7}$</td>
</tr>
<tr>
<td style="text-align: left;">sofa</td>
<td style="text-align: center;">2.7</td>
<td style="text-align: center;">1.66</td>
<td style="text-align: center;">$\mathbf{2 4 . 4 6}$</td>
</tr>
<tr>
<td style="text-align: left;">bed</td>
<td style="text-align: center;">2.64</td>
<td style="text-align: center;">2.47</td>
<td style="text-align: center;">$\mathbf{3 8 . 3}$</td>
</tr>
<tr>
<td style="text-align: left;">shelving</td>
<td style="text-align: center;">2.60</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">$\mathbf{1 . 5 9}$</td>
</tr>
</tbody>
</table>
<p>TABLE VI: Top-10 frequent per-class IOU</p>
<p>We visualize qualitative segmentation results in Figure 6. We observe that for categories "wall", "chair", "counter", "table", and "bed", the segmentation results are mostly correct. Sometimes, when the "sofa" and "chair" are in similar material and shape (in Figure 6a and 6b), VLMaps might fail to differentiate them, leading to wrong planning behaviors. We also observe from Figure 6c and Figure 6e that the segmentation of some objects are noisy. This could be caused by the features fusion strategy we adopt. For example, in the top left corner of Figure 6e, there are some chairs and tables predictions with noise compared to the ground truth in Figure 6f. When we generate VLMaps for the scenes, we average the visual embeddings of points projecting to the same location on the top-down map. The averaging operation might introduce noise in the fused features, leading to noisy segmentation predictions (predicting "sink" on the table). In the future, more advanced fusion techniques can be explored to improve the segmentation results.</p>
<h2>E. Prompts for Obstacle Maps Generation</h2>
<p>In Sec. IV-D, we generate open-vocabulary obstacle maps for a drone and a LoCoBot with the method introduced in Sec. III-C. For the LoCoBot (ground robot), we first define a potential obstacle list as ["chair", "wall", "wall above the door", "table", "window", "floor", "stairs", "other"] and perform open-vocabulary landmark indexing. Later, we only select the union of the masks for the objects "wall", "chair", "table", "window", "stairs", "other"
as the obstacle map. For the drone (flying robot), we perform landmark indexing with the potential obstacle list: ["chair", "sofa", "wall", "table", "counter", "window", "floor", "stairs", "ceiling lights", "cabinet", "counter support", "other"]. Afterwards, we take union of the masks for ["wall", "window", "stairs", "ceiling lights", "cabinet", "other"] to generate the obstacle map.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Fig. 6: Qualitative semantic segmentation results</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://github.com/niessner/Matterport/blob/master/metadata/mpcat40.tsv&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-934 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-934</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-934</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-22.html">extraction-schema-22</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <p><strong>Paper ID:</strong> paper-cc0f0cb09a73f82ed44d900f5ca710bec784acc1</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/cc0f0cb09a73f82ed44d900f5ca710bec784acc1" target="_blank">DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> It is shown that SQL queries, despite their declarative structure, can be broken down into sub-Problems and the solutions of those sub-problems can be fed into LLMs to significantly improve their performance.</p>
                <p><strong>Paper Abstract:</strong> We study the problem of decomposing a complex text-to-sql task into smaller sub-tasks and how such a decomposition can significantly improve the performance of Large Language Models (LLMs) in the reasoning process. There is currently a significant gap between the performance of fine-tuned models and prompting approaches using LLMs on challenging text-to-sql datasets such as Spider. We show that SQL queries, despite their declarative structure, can be broken down into sub-problems and the solutions of those sub-problems can be fed into LLMs to significantly improve their performance. Our experiments with three LLMs show that this approach consistently improves their performance by roughly 10%, pushing the accuracy of LLMs towards state-of-the-art, and even beating large fine-tuned models on the holdout Spider dataset.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e934.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e934.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large, proprietary transformer-based LLM from OpenAI used in the paper as a backbone LLM for in-context (few-shot) prompting and for the DIN-SQL decomposition pipeline; shows the strongest text-to-SQL performance reported in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large pretrained transformer-based language model accessed via API; used with zero-shot, few-shot, chain-of-thought style and the DIN-SQL prompting pipeline. No architectural modifications were made in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Text-to-SQL (Spider, BIRD)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>multi-step reasoning / program generation (text-to-SQL)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Spider holdout test execution accuracy: 85.3% (DIN-SQL, reported in abstract); Spider dev EX with DIN-SQL: 74.2% (dev table); BIRD holdout test EX with DIN-SQL: 55.9%</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting only (few-shot / in-context learning); no fine-tuning in this work</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Paper discusses a general gap between few-shot prompting LLMs and fine-tuned models on complex text-to-SQL tasks, attributing failures to schema-linking errors, join detection, grouping/nesting identification, mismatch between natural language and SQL syntax, and lack of decomposition tailored to SQL's declarative structure.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e934.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e934.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CodeX (Davinci)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CodeX Davinci</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A code-tuned variant of the Codex/CodeX family used in experiments; evaluated as part of the baseline and with the DIN-SQL prompting pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>CodeX Davinci</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A large code-specialized transformer LLM accessed via API; used with greedy decoding and prompting in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Text-to-SQL (Spider, BIRD)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>multi-step reasoning / program generation (text-to-SQL)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Spider holdout test execution accuracy with DIN-SQL: 78.2% (abstract); Spider dev EX with DIN-SQL: 69.9% (dev table).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting only (few-shot / in-context learning)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Same factors as GPT-4: schema linking mistakes, incorrect join detection, group-by/nesting recognition, and the mismatch between natural language descriptions and SQL structure.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e934.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e934.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CodeX (Cushman)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CodeX Cushman</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A smaller CodeX variant evaluated in the paper; performs worse than larger variants in in-context text-to-SQL prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>CodeX Cushman</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Smaller code-focused transformer LLM variant with a smaller context window; used with fewer demonstrations due to context limits.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Text-to-SQL (Spider)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>multi-step reasoning / program generation (text-to-SQL)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Spider dev EX with DIN-SQL: 47.6% (dev table)</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting only (few-shot / in-context learning)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Smaller model capacity and smaller context window reduce effectiveness of in-context decomposition; same error modes (schema-linking, joins, nesting).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e934.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e934.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Few-shot prompting (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Few-shot in-context prompting (basic few-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard few-shot in-context learning baseline where LLMs are given a small set of demonstration question-SQL pairs in the prompt and asked to generate SQL directly without decomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>few-shot prompting (in-context learning)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Method: provide a handful of exemplars (question, schema links optional, SQL) in prompt; no modular decomposition or self-correction pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Text-to-SQL (Spider, BIRD)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>multi-step reasoning / program generation (text-to-SQL)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Examples from development set: GPT-4 few-shot EX 67.4% (dev), CodeX Davinci few-shot EX 61.5% (dev).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>in-context demonstration examples</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>few-shot in-context prompting (no fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Baseline few-shot prompting struggles on complex/declarative generation tasks (SQL) because natural-language-to-SQL mapping is non-procedural and composition boundaries are unclear; leads to schema-linking and join errors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e934.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e934.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DIN-SQL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper's proposed modular prompting pipeline that decomposes text-to-SQL into schema linking, classification & decomposition, SQL generation (with NatSQL intermediate rep), and self-correction, applied via few-shot in-context prompting to substantially improve text-to-SQL execution accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>DIN-SQL (prompting pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A modular prompting architecture (not a new pretrained LLM): (1) prompt-based schema linking; (2) query classification into easy/non-nested/nested; (3) class-specific SQL generation using NatSQL intermediate representation and tailored prompts; (4) zero-shot self-correction (gentle or generic prompts). All modules are implemented via in-context prompting to LLMs (GPT-4, CodeX).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Text-to-SQL (Spider, BIRD)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>multi-step reasoning / program generation (text-to-SQL)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Reported effects: on Spider holdout test DIN-SQL + GPT-4 EX = 85.3% (abstract, new SOTA at time); DIN-SQL + CodeX Davinci EX = 78.2% (abstract). On Spider dev: GPT-4 DIN-SQL EX = 74.2% vs few-shot 67.4% (Δ +6.8 pp); CodeX Davinci DIN-SQL EX = 69.9% vs few-shot 61.5% (Δ +8.4 pp). On BIRD dev/test and VES metrics DIN-SQL improved VES by ~9% over a GPT-4 baseline (dev).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>modular pipeline implemented via prompts (schema-linking module, classifier/decomposer, NatSQL intermediate rep, chain-of-thought style for nested, self-correction verifier); uses intermediate representation rather than altering LLM internals</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting only / few-shot in-context learning (no fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting strategy / modular decomposition / intermediate representation / self-correction (hybrid prompting intervention)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>DIN-SQL decomposes text-to-SQL into four prompt-implemented modules: (1) prompt-based schema linking to extract column/table/entity references and candidate values; (2) classification & decomposition to label queries as easy/non-nested/nested and extract join tables/subqueries; (3) SQL generation specialized per class using NatSQL as an intermediate representation and different prompt templates (no intermediate steps for easy, NatSQL bridging for non-nested, explicitly solving sub-queries then composing for nested); (4) zero-shot self-correction (generic or gentle prompts) to fix minor SQL bugs. The prompts include few-shot exemplars per class and optional example rows/external knowledge for BIRD.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Substantial improvement in interactive (text-to-SQL) performance vs plain few-shot prompting: roughly +6.8 to +8.4 percentage points EX on dev (GPT-4 and CodeX Davinci), reported as 'roughly 10%' improvement in text. Ablation: removing any module reduces EX (CodeX Davinci 'All' EX 69.9% -> w/o schema-linking 65.9% -> w/o classification (simple few-shot) 63.1% -> w/o self-corr 67.3%); removing classification+using decomposed COT for all questions gives different tradeoffs across difficulties. On Spider holdout DIN-SQL + GPT-4 outperformed prior fine-tuned SOTA 79.9% EX, reaching 85.3% (Δ +5.4 pp). On BIRD dev, DIN-SQL improved VES by ~9% vs GPT-4 baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>The paper hypothesizes that the gap arises because LLM prompting without decomposition fails at schema linking, join detection, and recognizing declarative SQL structure; the lack of an intermediate explicit decomposition and schema grounding leads to multi-table/join and nesting errors. DIN-SQL mitigates these by decomposing and adding schema linking, class-specific prompts, intermediate NatSQL, and self-correction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e934.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e934.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Schema linking (prompt module)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompted Schema Linking Module</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A DIN-SQL module implemented by prompting that extracts schema entities (tables, columns) and candidate condition values from the NL question and links them to the provided database schema; intended to reduce errors from ambiguous references.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Schema linking module (prompted)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompt template with 10 few-shot examples and a 'Let's think step by step' chain-of-thought preface that outputs schema_links: column/table references, foreign keys to use, and candidate cell values.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Text-to-SQL (Spider, BIRD) - subtask: schema grounding</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>schema grounding / multi-step reasoning subtask</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Ablation effect: for CodeX Davinci removing schema-linking reduced overall EX from 69.9% to 65.9% (dev table); paper reports schema linking was the largest failure category for baseline few-shot prompting and schema-linking module helps across query classes (least improvement for hard class).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>prompt-based extraction / chain-of-thought style reasoning in prompt</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>in-context few-shot prompting (module implemented via prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting strategy (explicit grounding / preprocessing module)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Given schema and question, the prompt asks the LLM to identify mentions of columns/tables/entities, possible cell values, and relevant foreign keys; outputs a structured set of schema_links that feed downstream generation prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Improves execution accuracy across query classes; largest single failure mode for basic few-shot was schema linking, and adding this module reduces such errors (CodeX Davinci all EX 69.9% vs 65.9% without module).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Ambiguity between natural language mentions and schema entities (column/table names overlapping with NL words, multiple candidate mappings) leads to incorrect column/table selections and thus erroneous joins or aggregations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e934.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e934.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NatSQL (intermediate)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NatSQL (intermediate representation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An intermediate representation used in DIN-SQL (from prior work) that simplifies mapping from natural language to SQL by removing or abstracting difficult SQL operators and merging certain clauses; used to bridge NL and SQL generation for non-nested and nested complex classes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>NatSQL intermediate representation</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Structured, more NL-friendly intermediate form of SQL (semantically closer to NL than raw SQL) used as an intermediate target in prompts to ease generation of final SQL.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Text-to-SQL (Spider) - subtask: representation bridging</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>intermediate representation / multi-step reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>used within prompting pipeline (no additional training reported)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>representation/intermediate representation used in prompting</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>DIN-SQL uses NatSQL as the intermediate representation for non-nested complex queries (and as part of nested class prompts), providing an intermediate I_j in demonstrations so the LLM maps NL->NatSQL->SQL rather than NL->SQL directly.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Reported to help bridge NL-to-SQL mismatch and improve join-related performance; contributes to overall DIN-SQL gains (ablation suggests intermediate representations are part of the effective pipeline, though separate numeric ablation for NatSQL alone is not tabulated).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Direct NL->SQL mapping is hard because SQL operators and clause boundaries do not map cleanly to NL; intermediate representations reduce this mismatch and make generation easier for LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e934.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e934.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-correction (prompted)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot Self-correction Module (prompted)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A zero-shot post-processing prompting step that asks the LLM to identify and fix minor SQL errors (syntax, redundant/missing keywords) in generated SQL; two prompt variants (generic vs gentle) are used and tuned per model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Self-correction (prompted)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Zero-shot prompts: 'generic' expects buggy SQL and asks to fix it; 'gentle' asks to check for potential issues without assuming bugs. Used after SQL generation to repair mistakes.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Text-to-SQL (Spider, BIRD) - post-processing</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>self-verification / repair</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Ablation: for CodeX Davinci, generic self-corr pipeline achieves All EX 69.9% vs w/o self-corr 67.3% (dev table). For GPT-4, 'gentle' self-corr works better and produces small but consistent gains on hard/nested classes; removing self-corr reduces some metrics (GPT-4 w/o self-corr All EX 73.3% vs with gentle 74.2%).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>post-hoc verification/repair prompt; uses LLM to act as a fixer/verifier</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>zero-shot prompting (no extra training or fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting strategy (post-hoc repair / verifier)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>After generating SQL, the model is prompted (zero-shot) to inspect and fix potential syntax or logical issues. Two variants: 'generic' (assumes buggy SQL and asks for fixes) and 'gentle' (asks to check and suggests fixes only if needed); choice tuned per LLM (generic better for CodeX, gentle for GPT-4).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Provides measurable improvements in execution accuracy, particularly for smaller LLMs; e.g., CodeX Davinci 'generic' self-corr all EX 69.9% vs 67.3% without self-corr. For GPT-4 gentle self-corr gave small gains on most classes.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Minor syntactic and small logical errors (missing DISTINCT/DESC, redundant aggregations) can be fixed cheaply by an LLM acting as a verifier; these residual errors partly explain lower execution accuracy for prompting-only pipelines without a verification stage.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e934.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e934.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that elicits intermediate reasoning steps from LLMs to improve performance on complex multi-step tasks; referenced as related work and as a conceptual precedent for decomposition techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>chain-of-thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A few-shot prompting template that encourages the model to 'think step-by-step' and produce intermediate reasoning traces.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>multi-step reasoning (general)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>chain-of-thought elicitation via prompt</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>few-shot prompting</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting strategy (reasoning trace elicitation)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Referenced as prior technique that helps arithmetic and other stepwise problems; authors note chain-of-thought helps some tasks but listing intermediate steps can degrade simpler task performance and that SQL is declarative so step boundaries are less clear.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e934.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e934.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Least-to-most / Decomposed prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Least-to-most and Decomposed prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompting strategies from prior work (least-to-most, decomposed prompting) that break complex tasks into sequences of simpler subproblems; cited as inspiration for DIN-SQL decomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>least-to-most / decomposed prompting</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompting methods that iteratively decompose problems into subproblems and solve them sequentially, sometimes feeding intermediate solutions forward.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>multi-step reasoning / procedural decomposition</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>sequential subproblem decomposition induced by prompt templates</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting (few-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting strategy (decomposition)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Cited prior work (e.g., least-to-most, decomposed prompting) as evidence that decomposition improves LLM reasoning on some tasks; DIN-SQL adapts decomposition to SQL by designing modules aligned to SQL's declarative nature.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Least-to-most prompting enables complex reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Decomposed prompting: A modular approach for solving complex tasks <em>(Rating: 2)</em></li>
                <li>Natural SQL: Making SQL easier to infer from natural language specifications <em>(Rating: 2)</em></li>
                <li>Evaluating the text-to-SQL capabilities of large language models <em>(Rating: 2)</em></li>
                <li>Can LLM already serve as a database interface? A big bench for large-scale database grounded text-to-SQLs <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-934",
    "paper_id": "paper-cc0f0cb09a73f82ed44d900f5ca710bec784acc1",
    "extraction_schema_id": "extraction-schema-22",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4",
            "brief_description": "A large, proprietary transformer-based LLM from OpenAI used in the paper as a backbone LLM for in-context (few-shot) prompting and for the DIN-SQL decomposition pipeline; shows the strongest text-to-SQL performance reported in this work.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "GPT-4",
            "model_description": "Large pretrained transformer-based language model accessed via API; used with zero-shot, few-shot, chain-of-thought style and the DIN-SQL prompting pipeline. No architectural modifications were made in this work.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "Text-to-SQL (Spider, BIRD)",
            "interactive_task_type": "multi-step reasoning / program generation (text-to-SQL)",
            "interactive_performance": "Spider holdout test execution accuracy: 85.3% (DIN-SQL, reported in abstract); Spider dev EX with DIN-SQL: 74.2% (dev table); BIRD holdout test EX with DIN-SQL: 55.9%",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": null,
            "training_method": "prompting only (few-shot / in-context learning); no fine-tuning in this work",
            "intervention_type": null,
            "intervention_description": null,
            "intervention_effect": null,
            "hypothesized_cause_of_gap": "Paper discusses a general gap between few-shot prompting LLMs and fine-tuned models on complex text-to-SQL tasks, attributing failures to schema-linking errors, join detection, grouping/nesting identification, mismatch between natural language and SQL syntax, and lack of decomposition tailored to SQL's declarative structure.",
            "uuid": "e934.0",
            "source_info": {
                "paper_title": "DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "CodeX (Davinci)",
            "name_full": "CodeX Davinci",
            "brief_description": "A code-tuned variant of the Codex/CodeX family used in experiments; evaluated as part of the baseline and with the DIN-SQL prompting pipeline.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "CodeX Davinci",
            "model_description": "A large code-specialized transformer LLM accessed via API; used with greedy decoding and prompting in this paper.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "Text-to-SQL (Spider, BIRD)",
            "interactive_task_type": "multi-step reasoning / program generation (text-to-SQL)",
            "interactive_performance": "Spider holdout test execution accuracy with DIN-SQL: 78.2% (abstract); Spider dev EX with DIN-SQL: 69.9% (dev table).",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": null,
            "training_method": "prompting only (few-shot / in-context learning)",
            "intervention_type": null,
            "intervention_description": null,
            "intervention_effect": null,
            "hypothesized_cause_of_gap": "Same factors as GPT-4: schema linking mistakes, incorrect join detection, group-by/nesting recognition, and the mismatch between natural language descriptions and SQL structure.",
            "uuid": "e934.1",
            "source_info": {
                "paper_title": "DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "CodeX (Cushman)",
            "name_full": "CodeX Cushman",
            "brief_description": "A smaller CodeX variant evaluated in the paper; performs worse than larger variants in in-context text-to-SQL prompting.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "CodeX Cushman",
            "model_description": "Smaller code-focused transformer LLM variant with a smaller context window; used with fewer demonstrations due to context limits.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "Text-to-SQL (Spider)",
            "interactive_task_type": "multi-step reasoning / program generation (text-to-SQL)",
            "interactive_performance": "Spider dev EX with DIN-SQL: 47.6% (dev table)",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": null,
            "training_method": "prompting only (few-shot / in-context learning)",
            "intervention_type": null,
            "intervention_description": null,
            "intervention_effect": null,
            "hypothesized_cause_of_gap": "Smaller model capacity and smaller context window reduce effectiveness of in-context decomposition; same error modes (schema-linking, joins, nesting).",
            "uuid": "e934.2",
            "source_info": {
                "paper_title": "DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "Few-shot prompting (baseline)",
            "name_full": "Few-shot in-context prompting (basic few-shot)",
            "brief_description": "Standard few-shot in-context learning baseline where LLMs are given a small set of demonstration question-SQL pairs in the prompt and asked to generate SQL directly without decomposition.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "few-shot prompting (in-context learning)",
            "model_description": "Method: provide a handful of exemplars (question, schema links optional, SQL) in prompt; no modular decomposition or self-correction pipeline.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "Text-to-SQL (Spider, BIRD)",
            "interactive_task_type": "multi-step reasoning / program generation (text-to-SQL)",
            "interactive_performance": "Examples from development set: GPT-4 few-shot EX 67.4% (dev), CodeX Davinci few-shot EX 61.5% (dev).",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "in-context demonstration examples",
            "training_method": "few-shot in-context prompting (no fine-tuning)",
            "intervention_type": null,
            "intervention_description": null,
            "intervention_effect": null,
            "hypothesized_cause_of_gap": "Baseline few-shot prompting struggles on complex/declarative generation tasks (SQL) because natural-language-to-SQL mapping is non-procedural and composition boundaries are unclear; leads to schema-linking and join errors.",
            "uuid": "e934.3",
            "source_info": {
                "paper_title": "DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "DIN-SQL",
            "name_full": "DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction",
            "brief_description": "The paper's proposed modular prompting pipeline that decomposes text-to-SQL into schema linking, classification & decomposition, SQL generation (with NatSQL intermediate rep), and self-correction, applied via few-shot in-context prompting to substantially improve text-to-SQL execution accuracy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": "DIN-SQL (prompting pipeline)",
            "model_description": "A modular prompting architecture (not a new pretrained LLM): (1) prompt-based schema linking; (2) query classification into easy/non-nested/nested; (3) class-specific SQL generation using NatSQL intermediate representation and tailored prompts; (4) zero-shot self-correction (gentle or generic prompts). All modules are implemented via in-context prompting to LLMs (GPT-4, CodeX).",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "Text-to-SQL (Spider, BIRD)",
            "interactive_task_type": "multi-step reasoning / program generation (text-to-SQL)",
            "interactive_performance": "Reported effects: on Spider holdout test DIN-SQL + GPT-4 EX = 85.3% (abstract, new SOTA at time); DIN-SQL + CodeX Davinci EX = 78.2% (abstract). On Spider dev: GPT-4 DIN-SQL EX = 74.2% vs few-shot 67.4% (Δ +6.8 pp); CodeX Davinci DIN-SQL EX = 69.9% vs few-shot 61.5% (Δ +8.4 pp). On BIRD dev/test and VES metrics DIN-SQL improved VES by ~9% over a GPT-4 baseline (dev).",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "modular pipeline implemented via prompts (schema-linking module, classifier/decomposer, NatSQL intermediate rep, chain-of-thought style for nested, self-correction verifier); uses intermediate representation rather than altering LLM internals",
            "training_method": "prompting only / few-shot in-context learning (no fine-tuning)",
            "intervention_type": "prompting strategy / modular decomposition / intermediate representation / self-correction (hybrid prompting intervention)",
            "intervention_description": "DIN-SQL decomposes text-to-SQL into four prompt-implemented modules: (1) prompt-based schema linking to extract column/table/entity references and candidate values; (2) classification & decomposition to label queries as easy/non-nested/nested and extract join tables/subqueries; (3) SQL generation specialized per class using NatSQL as an intermediate representation and different prompt templates (no intermediate steps for easy, NatSQL bridging for non-nested, explicitly solving sub-queries then composing for nested); (4) zero-shot self-correction (generic or gentle prompts) to fix minor SQL bugs. The prompts include few-shot exemplars per class and optional example rows/external knowledge for BIRD.",
            "intervention_effect": "Substantial improvement in interactive (text-to-SQL) performance vs plain few-shot prompting: roughly +6.8 to +8.4 percentage points EX on dev (GPT-4 and CodeX Davinci), reported as 'roughly 10%' improvement in text. Ablation: removing any module reduces EX (CodeX Davinci 'All' EX 69.9% -&gt; w/o schema-linking 65.9% -&gt; w/o classification (simple few-shot) 63.1% -&gt; w/o self-corr 67.3%); removing classification+using decomposed COT for all questions gives different tradeoffs across difficulties. On Spider holdout DIN-SQL + GPT-4 outperformed prior fine-tuned SOTA 79.9% EX, reaching 85.3% (Δ +5.4 pp). On BIRD dev, DIN-SQL improved VES by ~9% vs GPT-4 baseline.",
            "hypothesized_cause_of_gap": "The paper hypothesizes that the gap arises because LLM prompting without decomposition fails at schema linking, join detection, and recognizing declarative SQL structure; the lack of an intermediate explicit decomposition and schema grounding leads to multi-table/join and nesting errors. DIN-SQL mitigates these by decomposing and adding schema linking, class-specific prompts, intermediate NatSQL, and self-correction.",
            "uuid": "e934.4",
            "source_info": {
                "paper_title": "DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "Schema linking (prompt module)",
            "name_full": "Prompted Schema Linking Module",
            "brief_description": "A DIN-SQL module implemented by prompting that extracts schema entities (tables, columns) and candidate condition values from the NL question and links them to the provided database schema; intended to reduce errors from ambiguous references.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": "Schema linking module (prompted)",
            "model_description": "Prompt template with 10 few-shot examples and a 'Let's think step by step' chain-of-thought preface that outputs schema_links: column/table references, foreign keys to use, and candidate cell values.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "Text-to-SQL (Spider, BIRD) - subtask: schema grounding",
            "interactive_task_type": "schema grounding / multi-step reasoning subtask",
            "interactive_performance": "Ablation effect: for CodeX Davinci removing schema-linking reduced overall EX from 69.9% to 65.9% (dev table); paper reports schema linking was the largest failure category for baseline few-shot prompting and schema-linking module helps across query classes (least improvement for hard class).",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "prompt-based extraction / chain-of-thought style reasoning in prompt",
            "training_method": "in-context few-shot prompting (module implemented via prompting)",
            "intervention_type": "prompting strategy (explicit grounding / preprocessing module)",
            "intervention_description": "Given schema and question, the prompt asks the LLM to identify mentions of columns/tables/entities, possible cell values, and relevant foreign keys; outputs a structured set of schema_links that feed downstream generation prompts.",
            "intervention_effect": "Improves execution accuracy across query classes; largest single failure mode for basic few-shot was schema linking, and adding this module reduces such errors (CodeX Davinci all EX 69.9% vs 65.9% without module).",
            "hypothesized_cause_of_gap": "Ambiguity between natural language mentions and schema entities (column/table names overlapping with NL words, multiple candidate mappings) leads to incorrect column/table selections and thus erroneous joins or aggregations.",
            "uuid": "e934.5",
            "source_info": {
                "paper_title": "DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "NatSQL (intermediate)",
            "name_full": "NatSQL (intermediate representation)",
            "brief_description": "An intermediate representation used in DIN-SQL (from prior work) that simplifies mapping from natural language to SQL by removing or abstracting difficult SQL operators and merging certain clauses; used to bridge NL and SQL generation for non-nested and nested complex classes.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "NatSQL intermediate representation",
            "model_description": "Structured, more NL-friendly intermediate form of SQL (semantically closer to NL than raw SQL) used as an intermediate target in prompts to ease generation of final SQL.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "Text-to-SQL (Spider) - subtask: representation bridging",
            "interactive_task_type": "intermediate representation / multi-step reasoning",
            "interactive_performance": null,
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": null,
            "architectural_features": null,
            "training_method": "used within prompting pipeline (no additional training reported)",
            "intervention_type": "representation/intermediate representation used in prompting",
            "intervention_description": "DIN-SQL uses NatSQL as the intermediate representation for non-nested complex queries (and as part of nested class prompts), providing an intermediate I_j in demonstrations so the LLM maps NL-&gt;NatSQL-&gt;SQL rather than NL-&gt;SQL directly.",
            "intervention_effect": "Reported to help bridge NL-to-SQL mismatch and improve join-related performance; contributes to overall DIN-SQL gains (ablation suggests intermediate representations are part of the effective pipeline, though separate numeric ablation for NatSQL alone is not tabulated).",
            "hypothesized_cause_of_gap": "Direct NL-&gt;SQL mapping is hard because SQL operators and clause boundaries do not map cleanly to NL; intermediate representations reduce this mismatch and make generation easier for LLMs.",
            "uuid": "e934.6",
            "source_info": {
                "paper_title": "DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "Self-correction (prompted)",
            "name_full": "Zero-shot Self-correction Module (prompted)",
            "brief_description": "A zero-shot post-processing prompting step that asks the LLM to identify and fix minor SQL errors (syntax, redundant/missing keywords) in generated SQL; two prompt variants (generic vs gentle) are used and tuned per model.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": "Self-correction (prompted)",
            "model_description": "Zero-shot prompts: 'generic' expects buggy SQL and asks to fix it; 'gentle' asks to check for potential issues without assuming bugs. Used after SQL generation to repair mistakes.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "Text-to-SQL (Spider, BIRD) - post-processing",
            "interactive_task_type": "self-verification / repair",
            "interactive_performance": "Ablation: for CodeX Davinci, generic self-corr pipeline achieves All EX 69.9% vs w/o self-corr 67.3% (dev table). For GPT-4, 'gentle' self-corr works better and produces small but consistent gains on hard/nested classes; removing self-corr reduces some metrics (GPT-4 w/o self-corr All EX 73.3% vs with gentle 74.2%).",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "post-hoc verification/repair prompt; uses LLM to act as a fixer/verifier",
            "training_method": "zero-shot prompting (no extra training or fine-tuning)",
            "intervention_type": "prompting strategy (post-hoc repair / verifier)",
            "intervention_description": "After generating SQL, the model is prompted (zero-shot) to inspect and fix potential syntax or logical issues. Two variants: 'generic' (assumes buggy SQL and asks for fixes) and 'gentle' (asks to check and suggests fixes only if needed); choice tuned per LLM (generic better for CodeX, gentle for GPT-4).",
            "intervention_effect": "Provides measurable improvements in execution accuracy, particularly for smaller LLMs; e.g., CodeX Davinci 'generic' self-corr all EX 69.9% vs 67.3% without self-corr. For GPT-4 gentle self-corr gave small gains on most classes.",
            "hypothesized_cause_of_gap": "Minor syntactic and small logical errors (missing DISTINCT/DESC, redundant aggregations) can be fixed cheaply by an LLM acting as a verifier; these residual errors partly explain lower execution accuracy for prompting-only pipelines without a verification stage.",
            "uuid": "e934.7",
            "source_info": {
                "paper_title": "DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "Chain-of-Thought (CoT)",
            "name_full": "Chain-of-Thought prompting",
            "brief_description": "A prompting technique that elicits intermediate reasoning steps from LLMs to improve performance on complex multi-step tasks; referenced as related work and as a conceptual precedent for decomposition techniques.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_or_agent_name": "chain-of-thought prompting",
            "model_description": "A few-shot prompting template that encourages the model to 'think step-by-step' and produce intermediate reasoning traces.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": null,
            "interactive_task_type": "multi-step reasoning (general)",
            "interactive_performance": null,
            "reports_both_qa_and_interactive": null,
            "performance_gap_observed": null,
            "architectural_features": "chain-of-thought elicitation via prompt",
            "training_method": "few-shot prompting",
            "intervention_type": "prompting strategy (reasoning trace elicitation)",
            "intervention_description": "Referenced as prior technique that helps arithmetic and other stepwise problems; authors note chain-of-thought helps some tasks but listing intermediate steps can degrade simpler task performance and that SQL is declarative so step boundaries are less clear.",
            "intervention_effect": null,
            "hypothesized_cause_of_gap": null,
            "uuid": "e934.8",
            "source_info": {
                "paper_title": "DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "Least-to-most / Decomposed prompting",
            "name_full": "Least-to-most and Decomposed prompting",
            "brief_description": "Prompting strategies from prior work (least-to-most, decomposed prompting) that break complex tasks into sequences of simpler subproblems; cited as inspiration for DIN-SQL decomposition.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_or_agent_name": "least-to-most / decomposed prompting",
            "model_description": "Prompting methods that iteratively decompose problems into subproblems and solve them sequentially, sometimes feeding intermediate solutions forward.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": null,
            "interactive_task_type": "multi-step reasoning / procedural decomposition",
            "interactive_performance": null,
            "reports_both_qa_and_interactive": null,
            "performance_gap_observed": null,
            "architectural_features": "sequential subproblem decomposition induced by prompt templates",
            "training_method": "prompting (few-shot)",
            "intervention_type": "prompting strategy (decomposition)",
            "intervention_description": "Cited prior work (e.g., least-to-most, decomposed prompting) as evidence that decomposition improves LLM reasoning on some tasks; DIN-SQL adapts decomposition to SQL by designing modules aligned to SQL's declarative nature.",
            "intervention_effect": null,
            "hypothesized_cause_of_gap": null,
            "uuid": "e934.9",
            "source_info": {
                "paper_title": "DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction",
                "publication_date_yy_mm": "2023-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Least-to-most prompting enables complex reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Decomposed prompting: A modular approach for solving complex tasks",
            "rating": 2
        },
        {
            "paper_title": "Natural SQL: Making SQL easier to infer from natural language specifications",
            "rating": 2
        },
        {
            "paper_title": "Evaluating the text-to-SQL capabilities of large language models",
            "rating": 2
        },
        {
            "paper_title": "Can LLM already serve as a database interface? A big bench for large-scale database grounded text-to-SQLs",
            "rating": 1
        }
    ],
    "cost": 0.0199705,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction</h1>
<p>Mohammadreza Pourreza<br>Department of Computer Science<br>University of Alberta<br>Edmonton, CA<br>pourreza@ualberta.ca</p>
<p>Davood Rafiei<br>Department of Computer Science<br>University of Alberta<br>Edmonton, CA<br>drafiei@ualberta.ca</p>
<h4>Abstract</h4>
<p>There is currently a significant gap between the performance of fine-tuned models and prompting approaches using Large Language Models (LLMs) on the challenging task of text-to-SQL, as evaluated on datasets such as Spider. To improve the performance of LLMs in the reasoning process, we study how decomposing the task into smaller sub-tasks can be effective. In particular, we show that breaking down the generation problem into sub-problems and feeding the solutions of those sub-problems into LLMs can be an effective approach for significantly improving their performance. Our experiments with three LLMs show that this approach consistently improves their simple few-shot performance by roughly $10 \%$, pushing the accuracy of LLMs towards SOTA or surpassing it. On the holdout test set of Spider, the SOTA, in terms of execution accuracy, was 79.9 and the new SOTA at the time of this writing using our approach is 85.3 . Our approach with in-context learning beats many heavily fine-tuned models by at least $5 \%$. Additionally, when evaluated on the BIRD benchmark, our approach achieved an execution accuracy of $55.9 \%$, setting a new SOTA on its holdout test set.</p>
<h2>1 Introduction</h2>
<p>Natural language interfaces to databases aim at making it easier for end users to access data in a relational database. For example, given the utterance "find employees who make more than their managers" and the schema of tables employees and manages, one may want to generate a query in SQL that retrieves those employees from a database. Over the past two decades, research in this field has progressed through several phases, with early systems being domain-specific, supporting controlled natural language [Popescu et al., 2003, 2004, Li et al., 2007, Li and Jagadish, 2014] or relying on rule-based approaches [Stratica et al., 2005] while more recent systems offering greater domain-independence using supervised models trained on diverse domains and datasets [Zhong et al., 2017, Yu et al., 2018] and more recently deep neural models trained on large text and code repositories [Dong and Lapata, 2016, Devlin et al., 2018].
The latest development in this progression is the use of Large Language Models (LLMs) under zero-shot and few-shot prompting [Rajkumar et al., 2022, Liu et al., 2023a]. It has been shown that LLMs provide strong baselines using only a few demonstrations and no fine-tuning [Chen et al., 2021, Brown et al., 2020, Liu et al., 2023b]. However, these models fall behind on commonly used benchmarks (e.g., Spider) compared to well-designed and fine-tuned models. Table 1 shows the performance of two latest LLMs, CodeX and GPT-4, on the development set of the Spider dataset. Despite a strong performance, LLMs fall behind, compared to existing methods [Scholak et al., 2021, Li et al., 2023a], especially on medium and complex queries. The question investigated in this paper</p>
<p>is where these LLMs fail and if some of the problems that they are facing can be mitigated to push the performance to reach or surpass fine-tuned SOTA models.</p>
<p>Prompting has several advantages over traditional approaches using pretraining or fine-tuning. The main benefit is that LLMs can perform prediction tasks without requiring large task-specific training data. Training models from scratch or fine-tuning them is a resource-intensive process, often requiring a large number of training samples and machine resources, which may not be available. Additionally, few-shot prompting has been shown to outperform previous state-of-the-art methods on several benchmark datasets and can achieve high accuracy even with limited training examples [Brown et al., 2020, Wei et al., 2022b].</p>
<p>It has been recently shown that the performance of LLMs can be improved on more complex tasks (e.g., math word problems, compositional navigation steps) using approaches such as chain-of-thought [Wei et al., 2022b], least-to-most [Zhou et al., 2022], and decomposed [Khot et al., 2022] prompting techniques where a task is broken down into multiple steps and the intermediate results are used to generate a final answer. Unlike algebraic expressions, which consist of clear steps or operations, breaking a complex SQL query can be a more daunting task because of the declarative structure of the language and the complex relationships between query clauses.</p>
<p>In this paper, we propose a novel method based on fewshot prompting that decomposes the task of natural language text to SQL (referred to as text-to-SQL) into multiple sub-tasks. Previous works on text-to-SQL prompting using LLMs are only evaluated in a zero-shot setting [Rajkumar et al., 2022, Liu et al., 2023a]. However, zero-shot prompting only provides a lower bound on the potential power of LLMs for most tasks [Zhang et al., 2022, Kojima et al., 2022, Wei et al., 2022b, 2021, Brown et al., 2020]. We show that our proposed method outperforms the few-shot prompting method by a large margin. We also compare our method with previous approaches on two cross-domain challenging benchmarks, Spider and BIRD. For Spider dataset, we use the two official evaluation metrics of execution accuracy and exact set match accuracy [Zhong et al., 2020]. We utilize two variants of the CodeX family, namely Davinci and Cushman [Chen et al., 2021], and the GPT-4 model for prompting. On the holdout test set of Spider, our method achieves an execution accuracy of $85.3 \%$ and $78.2 \%$ respectively using GPT-4 and CodeX Davinci models and an exact set match accuracy of $60 \%$ and $57 \%$ respectively using the same models. The large gap between the exact match and execution accuracies is due to the few-shot in-context nature of our method. Pretrained and fine-tuned approaches are more likely to generate SQL queries with a higher exact set match accuracy simply because these models have seen many examples during training that follow the composition style of the queries in the test set (queries in both sets are often written by the same people). Before our work, the SOTA on the test set had an execution accuracy of $79.9 \%$ [Li et al., 2023a] and an exact set match accuracy of $74 \%$ [Li et al., 2023b], and our method sets a new ground in terms of the execution accuracy. On the BIRD benchmark, our approach achieves a new SOTA result, attaining an execution accuracy of $55.9 \%$ on the holdout test set and $50.72 \%$ on the development set when employing GPT-4. Moreover, using the valid efficiency score introduced in this benchmark, our approach outperformed a GPT-4 baseline, demonstrating a $9 \%$ improvement on the development set. This highlights the effectiveness of our method.</p>
<p>Our contributions can be summarized as follows: (1) improving the performance of LLM-based text-to-SQL models through task decomposition, (2) introducing adaptive prompting strategies tailored to task complexity, (3) addressing schema linking challenges in the context of prompting, and (4) using LLMs for self correction. To replicate the reported results, visit our GitHub repository ${ }^{1}$ for access to the prompts, results, and the code.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>2 Related Work</h1>
<p>Sequence-to-sequence models [Sutskever et al., 2014] have shown great potential in code generation tasks including text-to-SQL. The key idea is to jointly encode a given natural language question and the database schema and leverage a decoder to predict the target SQL.</p>
<p>On the encoder side, learning a representation for the question and the database schema is carried out using bidirectional LSTM in IRNet [Graves and Graves, 2012], convolutional neural networks in RYANSQL [Choi et al., 2021], pretrained language models such as BERT in SQLova [Hwang et al., 2019] and graph neural networks in RATSQL [Wang et al., 2019], SADGA [Cai et al., 2021], and LGESQL [Cao et al., 2021]. Gan et al. [2021] propose an intermediate representation to bridge the gap between the natural language question and SQL statements. There has been also work on tabular language models that encode both tables and text such as TaBERT [Yin et al., 2020], TaPas [Herzig et al., 2020], and Grappa [Yu et al., 2020].</p>
<p>The methods on the decoder side can be categorized into sketch-based slot-filling and generationbased methods [Qin et al., 2022]. Sketch-based methods break the problem into several slot prediction sub-problems and aggregate the predictions for the slots of the SQL query to be generated [Hwang et al., 2019, Xu et al., 2017, Hui et al., 2021]. A drawback of these methods is that they cannot generalize to queries that do not follow the predefined templates. The generation-based methods [Guo et al., 2019, Wang et al., 2019, Cao et al., 2021, Huang et al., 2021] decode the SQL query as an abstract syntax tree.</p>
<p>In contrast to pretrained and fine-tuned models, Rajkumar et al. [2022] and Liu et al. [2023a] conduct an evaluation of the zero-shot prompting capability of LLMs on text-to-SQL using different prompts on the Spider dataset. Prompting techniques have been also used for tasks such as table understanding, table reasoning, and table-to-text generation [Guo et al., 2023, Chen, 2022], and some remarkable results have been reported using LLMs with just a small number of examples given in the prompt.</p>
<h2>3 Few-shot Error Analysis</h2>
<p>To better understand where LLMs fail under a few-shot setting, we randomly sampled 500 queries from different databases in the training set of the Spider dataset, excluding all databases used in our prompts. We searched for the queries that produced results different than those of gold queries, hence failing the execution accuracy. We manually examined these failures and classified them into six categories as shown in Figure 1 and discussed next.</p>
<p>Schema linking This category contained the largest number of failed queries and included instances where the model failed to identify column names, table names, or entities mentioned in questions. In some cases, the query required an aggregation function, but a matching column name was chosen instead. For instance, the database schema for question "What are the average and maximum capacities for all stadiums?" included a column named "average", which was selected by the model instead of taking the average of the capacity column.</p>
<p>JOIN This was the second largest category and included queries that needed a JOIN but the model was unable to identify all the tables required or the correct foreign keys to join the tables.</p>
<p>GROUP BY This category included cases where the SQL statement required a GROUP BY clause, but the model either did not recognize the need for grouping or wrong columns were used for grouping the results.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Statistics of simple few-shot failures using CodeX Davinci (Op refers to operators, Cond refers to conditions, and cols refers to columns)</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: An overview of the proposed methodology including all four modules</p>
<h1>Queries with nesting and set operations</h1>
<p>For this category, the gold query used nesting or set operations but the model did not recognize the nested structure or was unable to detect the correct nesting or set operation.</p>
<p>Invalid SQL A small set of the generated SQL statements had syntax errors and could not be executed.</p>
<p>Miscellaneous This category included cases that did not fit under any of the previously mentioned categories. Examples included SQL queries that contained extra predicates, missed a predicate, or had missing or redundant DISTINCT or DESC keywords. This category also included cases where the WHERE clause was missing or the query had redundant aggregation functions.</p>
<h2>4 Methodology</h2>
<p>Despite improvements over zero-shot, few-shot models struggle on more complex queries including those where schema linking is less trivial and the queries that use multiple joins or have a nested structure, as discussed in Section 3.</p>
<p>Our approach to address these challenges is to break down the problem into smaller sub-problems, solve each sub-problem, and use those solutions to construct a solution for the original problem. Similar approaches (e.g., chain-of-thought prompting [Wei et al., 2022b] and least-to-most prompting [Zhou et al., 2022]) have been taken to improve the performance of LLMs on tasks that can be broken down into multiple steps such as math word problems and compositional generalization [Cobbe et al., 2021, Lake and Baroni, 2018]. Unlike these domains where the tasks have a procedural structure with one step directly feeding into the next step, SQL queries in most parts are declarative and the possible steps and their boundaries are less clear. However, the thought process for writing SQL queries may be broken down to (1) detecting database tables and columns that are relevant to the query, (2) identifying the general query structure for more complex queries (e.g., group by, nesting, multiple joins, set operations, etc.), (3) formulating any procedural sub-components if they can be identified, and (4) writing the final query based on the solutions of the sub-problems.</p>
<p>Based on this thought process, our proposed method for decomposing a text-to-SQL task consists of four modules (as depicted in Figure 2): (1) schema linking, (2) query classification and decomposition, (3) SQL generation, and (4) self-correction, which are explained in detail in the following sub-sections. While these modules may be implemented using techniques from the literature, we implement them all using prompting techniques to show that LLMs are capable of solving them all if the problems are simply broken down to the right level of granularity. The few-shot examples used in the prompts are obtained from the training set of the respective benchmarks.</p>
<h3>4.1 Schema Linking Module</h3>
<p>Schema linking is responsible for identifying references to database schema and condition values in natural language queries. It is shown to help with the generalizability across domains and the synthesis of complex queries [Lei et al., 2020], making it a critical preliminary step in almost all</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Examples showing the input and output of schema linking (left) and classification and decomposition (right)
existing text-to-SQL methods [Cao et al., 2021, Wang et al., 2019, Guo et al., 2019, Xuan et al., 2021]. This was also a single category with the largest number of failures made by the LLM in our case (Figure 2).</p>
<p>We designed a prompt-based module for schema linking. The prompt includes ten randomly selected samples from the training set of the Spider dataset. Following the chain-of-thought template [Wei et al., 2022b], the prompt begins with "Let's think step by step," as suggested by Kojima et al. [2022]. For each mention of a column name in the question, the corresponding columns and their tables are selected from the given database schema. Possible entities and cell values are also extracted from the question. Figure 3a illustrates an example and the full prompt can be found in Appendix A.3.</p>
<h1>4.2 Classification \&amp; Decomposition Module</h1>
<p>For each join, there is some chance that a correct table or join condition is not detected. As the number of joins in a query increases, the chance that at least one join fails to generate correctly increases. One way to alleviate the problem is introduce a module that detects the tables to be joined. Also some queries have procedural components such as uncorrelated sub-queries, which may be generated independently and be merged with the main query.</p>
<p>To address these issues, we introduce a query classification and decomposition module. The module classifies each query into one of the three classes: easy, non-nested complex and nested complex. The easy class includes single-table queries that can be answered without join or nesting. The non-nested class includes queries that require join but no sub-queries, and the queries in the nested class can contain joins, sub-queries and set operations. The class labels are important for our query generation module, which uses different prompts for each query class. In addition to class labels, query classification and decomposition also detects the set of tables to be joined for both non-nested and nested queries as well as any sub-queries that may be detected for nested queries. Figure 3b shows an example input given to the model and the output that the model generates.</p>
<h3>4.3 SQL Generation Module</h3>
<p>As the queries become more complex, additional intermediate steps must be incorporated to bridge the gap between the natural language question and the SQL statement. This gap, known as the mismatch problem in the literature [Guo et al., 2019], poses a significant challenge to SQL generation, which stems from the fact that SQL is primarily designed for querying relational databases and not</p>
<p>representing the meaning in natural language [Kate, 2008]. While more complex queries can benefit from listing the intermediate steps in a chain-of-thought style prompting, such listings can degrade the performance for simpler tasks [Wei et al., 2022b]. On the same basis, our query generation comprises of three modules, each geared toward different classes.</p>
<p>For questions in our easy class, a simple few-shot prompting with no intermediate steps is adequate. The demonstration for an example $E_{j}$ of this class follows the format $<Q_{j}, S_{j}, A_{j}>$, where $Q_{j}$ and $A_{j}$ give the query text in English and SQL respectively and $S_{j}$ indicates the schema links.</p>
<p>Our non-nested complex class includes queries that require join. Our error analysis (§ 3) revealed that finding the right columns and foreign keys to join two tables can be challenging for LLMs under simple few-shot prompting, especially when the query requires joining multiple tables. To address this issue, we resort to an intermediate representation to bridge the gap between queries and SQL statements. Various intermediate representations have been introduced in the literature. In particular, SemQL [Guo et al., 2019] removes operators JOIN ON, FROM, and GROUP BY, which have no clear counterparts in natural language queries, and merges the HAVING and WHERE clauses. NatSQL [Gan et al., 2021] builds upon SemQL and removes the set operators. Expressions in natural language queries may not clearly map to a unique SQL clause or they may map to multiple clauses, so removing operators makes the transition from natural language to SQL easier. As our intermediate representation, we use NatSQL, which is shown to have a state-of-the-art performance when combined with other models [Li et al., 2023a]. The demonstration for an example $E_{j}$ of the non-nested complex class follows the format $<Q_{j}, S_{j}, I_{j}, A_{j}>$, where $S_{j}$ and $I_{j}$ respectively denote the schema links and the intermediate representation for the jth example.</p>
<p>Lastly, the nested complex class is the most sophisticated type and requires several intermediate steps before generating the final answer. This class can contain queries that not only require sub-queries using nesting and set operations such as EXCEPT, UNION, and INTERSECT but also multiple table joins, same as the previous class. To break down the problem further into multiple steps, our prompt for this class is designed in a way that the LLM should first solve the sub-queries, generated from the previous module, and then use them to generate the final answer. The prompt for this class follows the format $<Q_{j}, S_{j},\left\langle Q_{j_{1}}, A_{j_{1}}, \ldots, Q_{j_{k}}, A_{j_{k}}>, I_{j}, A_{j}\right\rangle$, where $k$ denotes the number of sub-questions, and $Q_{j_{i}}$ and $A_{j_{i}}$ respectively denote the $i$-th sub-question and the $i$-th sub-query. As before, $Q_{j}$ and $A_{j}$ denote the query in English and SQL respectively, $S_{j}$ gives the schema links and $I_{j}$ is a NatSQL intermediate representation.</p>
<p>Full prompts for all three query classes are provided in Appendix A.4, and all examples for the three classes are obtained from the exact same training set database chosen for the classification prompt.</p>
<h1>4.4 Self-correction Module</h1>
<p>The generated SQL queries can sometimes have missing or redundant keywords such as DESC, DISTINCT and aggregation functions. Our experience with multiple LLMs indicates that these issues are less common in larger LLMs (e.g., queries generated by GPT-4 have less bugs than those from CodeX) but are still present. To address this, we propose a self-correction module where the model is instructed to correct those minor mistakes. This is achieved in a zero-shot setting, where only the buggy code is provided to the model and it is asked to fix the bugs. We propose two different prompts for the self-correction module: generic and gentle. With a generic prompt, we request the model to identify and correct the errors in the "BUGGY SQL". The gentle prompt, on the other hand, does not assume the SQL query is buggy, and instead asks the model to check for any potential issues and provides some hints on the clauses to be checked. Our evaluation indicates that a generic prompt can yield a better result with the CodeX model, while a gentle prompt is more effective for the GPT-4 model. Unless explicitly stated otherwise, the default self-correction prompt in DIN-SQL is set to gentle for GPT-4 and generic for CodeX. Examples of both generic and gentle self-correction prompts can be found in Appendix A.6.</p>
<h2>5 Experiments</h2>
<h3>5.1 Models</h3>
<p>We evaluated the proposed method using two variants of the CodeX family (Davinci and Cushman variants) and the GPT-4 model. These are the largest open-access LLMs at the time of writing this</p>
<p>paper. Smaller models are less applicable since prompting is believed to be an emergent ability of the LLMs with the number of parameters in the scale of billions [Wei et al., 2022a].</p>
<h1>5.2 Hyperparameter</h1>
<p>All models were accessed via the OpenAI API. Greedy decoding was used to generate the output by setting the temperature at zero. The max tokens was set to 350 for the self-correction module and 600 for all other modules. The stopping token sequence was set to "# in 'in" for the self-correction module and "Q:" for all other modules.</p>
<h3>5.3 Dataset</h3>
<p>Our evaluation was conducted on two crossdomain challenging datasets, Spider and BIRD. Spider consists of 10,181 questions and 5,693 unique complex SQL queries across 200 databases, covering 138 domains, each containing multiple tables. The standard protocol for this dataset divides it into 8,659 training examples across 146 databases, 1,034 development examples across 20 databases, and a holdout of 2,147 test examples across 34 databases. The databases used in each of these sets are nonoverlapping. SQL queries are categorized into four difficulty levels, based on the number of SQL keywords used, the presence of nested subqueries, and the usage of column selections and aggregations. BIRD comprises an extensive dataset with 12,751 unique question-SQL pairs, encompassing 95 large databases totaling 33.4 GB in size. It spans a wide array of more than 37 professional domains, including blockchain, hockey, healthcare, and education. BIRD also introduces external knowledge as an additional resource to assist models in generating accurate SQL queries. Specifically four sources of external knowledge were introduced: numeric reasoning knowledge, domain knowledge, synonym knowledge, and value illustration. Notably, the SQL queries in the BIRD dataset tend to be more intricate than those in the Spider dataset. Language models without access to database content often encounter challenges with schema linking. Therefore, our prompts for the BIRD dataset include sample rows from each table to aid the model in schema linking. Furthermore, we have concatenated the provided external knowledge for each question as a hint, placed immediately after each question. However, due to constraints such as limited context window size, the presence of external knowledge, and the inclusion of sample rows, we have had to reduce the number of demonstrations within the prompts for the BIRD dataset.</p>
<h3>5.4 Metrics</h3>
<p>The performance of our models are evaluated using the official metrics of each dataset: exact-setmatch accuracy (EM) and execution accuracy (EX) for Spider and valid efficiency score (VES) and execution accuracy (EX) for BIRD.</p>
<p>The exact-set-match accuracy (EM) treats each clause as a set and compares the prediction for each clause to its corresponding clause in the reference query. A predicted SQL query is considered correct only if all of its components match the ground truth. This metric does not take values into account. The execution accuracy (EX) compares the execution output of the predicted SQL query with that of the ground truth SQL query on some database instances. Execution accuracy provides a more precise estimate of the model's performance since there may be multiple valid SQL queries</p>
<p>for a given question, and exact set match accuracy only evaluates the predicted SQL against one of them. The Valid Efficiency Score (VES) is a metric designed to measure the efficiency of running the generated SQL queries. This metric is meaningful if the generated queries are correct, meaning their result matches that of the reference query. Therefore, the VES metric takes into account both the accuracy of the generated queries and their efficiency in terms of the execution time.</p>
<h1>5.5 Results</h1>
<h3>5.5.1 Test set results</h3>
<p>As shown in Table 2 for the holdout test set of Spider, our method achieves the highest execution accuracy using GPT-4 and the third-highest execution accuracy using CodeX Davinci among all officially published results at the time of this writing. This is achieved without even utilizing the database content. In terms of exact set match accuracy, our method achieves comparable results to previous works that do not utilize database content. As demonstrated in Table 3a, in the case of the BIRD dataset, our method using GPT-4 achieved a test set execution accuracy of $55.9 \%$, setting a new SOTA.</p>
<h3>5.5.2 Development set results</h3>
<p>Most of our evaluation during development was conducted on the development set of Spider which was easily accessible unlike the test set that was only accessible through an evaluation server provided by Yu et al. [2018]. Table 4a shows the performance of our method using different LLMs, compared to zero-shot prompting of Rajkumar et al. [2022] and Liu et al. [2023a] and our own few-shot prompting. To ensure a fair comparison for the few-shot prompting, we incorporate all the examples utilized for our three classes (easy, non-nested complex, and nested complex) inside the prompt. Given that the CodeX Cushman model has a smaller input context size than the CodeX Davinci and the GPT-4 models, we only use 2 examples from each class (for a total of 6 examples).
Our method significantly outperforms both simple few-shot prompting and zero-shot prompt-
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: The break-down of failure cases for DINSQL (green) and the basic few-shot prompting (blue) across different categories
ing, in terms of both exact set match and execution accuracies, and the improvement is consistent across all models despite their sizes. For example, compared to few-shot prompting, our method improves the execution accuracy for all models by at least $10 \%$.</p>
<p>On the development set of BIRD, our approach demonstrates a substantial improvement, achieving a $4 \%$ gain in execution accuracy and a remarkable $9 \%$ improvement in valid efficiency score over a GPT-4 baseline Li et al. [2023c], establishing a new SOTA. These and other results are reported in able 3 b .</p>
<p>The performance of our method on the test set (as reported in Tables 2 and 3a) is higher than that on the development set for both Spider and BIRD. It is hard to pinpoint the exact reason when the test set is hidden, but we speculate that fewer questions in the test set may require the knowledge of the database content, making it easier for our method to predict a correct SQL query. Furthermore, the development set has schema ambiguity (e.g., a query entity can be mapped to multiple database entities but only one is considered correct), and it is possible that the test set has less ambiguity.
We further analyzed the performance of our proposed method on queries with different levels of difficulty. Table 4b presents the performance of our proposed method compared to a basic few-shot prompting on the development set of Spider. Our proposed method outperforms the basic few-shot prompting across all difficulty levels, with the greatest improvement in performance observed for the extra hard and hard classes where the few-shot prompting performed poorly. Our improvement</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">VES</th>
<th style="text-align: center;">EX</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">DIN-SQL + GPT-4 (Ours)</td>
<td style="text-align: center;">59.44</td>
<td style="text-align: center;">$\mathbf{5 5 . 9}$</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">$\mathbf{6 0 . 7 7}$</td>
<td style="text-align: center;">54.89</td>
</tr>
<tr>
<td style="text-align: center;">Claude-2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">49.02</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT + CoT</td>
<td style="text-align: center;">56.56</td>
<td style="text-align: center;">40.08</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">51.40</td>
<td style="text-align: center;">39.30</td>
</tr>
<tr>
<td style="text-align: center;">Codex</td>
<td style="text-align: center;">41.60</td>
<td style="text-align: center;">36.47</td>
</tr>
<tr>
<td style="text-align: center;">Palm-2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">33.04</td>
</tr>
<tr>
<td style="text-align: center;">T5-3B</td>
<td style="text-align: center;">27.80</td>
<td style="text-align: center;">24.05</td>
</tr>
<tr>
<td style="text-align: center;">T5-Large</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">20.94</td>
</tr>
<tr>
<td style="text-align: center;">T5-Base</td>
<td style="text-align: center;">14.7</td>
<td style="text-align: center;">12.89</td>
</tr>
</tbody>
</table>
<p>(a) Execution accuracy (EX) and Valid Efficiency Score (VES) on the holdout test set of BIRD</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">VES</th>
<th style="text-align: center;">EX</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">DIN-SQL + GPT-4 (Ours)</td>
<td style="text-align: center;">$\mathbf{5 8 . 7 9}$</td>
<td style="text-align: center;">$\mathbf{5 0 . 7 2}$</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">49.77</td>
<td style="text-align: center;">46.35</td>
</tr>
<tr>
<td style="text-align: center;">Claude-2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">42.70</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT + CoT</td>
<td style="text-align: center;">42.30</td>
<td style="text-align: center;">36.64</td>
</tr>
<tr>
<td style="text-align: center;">[Li et al., 2023c]</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">43.81</td>
<td style="text-align: center;">37.22</td>
</tr>
<tr>
<td style="text-align: center;">Codex</td>
<td style="text-align: center;">43.41</td>
<td style="text-align: center;">34.35</td>
</tr>
<tr>
<td style="text-align: center;">Palm-2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">27.38</td>
</tr>
<tr>
<td style="text-align: center;">T5-3B</td>
<td style="text-align: center;">25.57</td>
<td style="text-align: center;">23.34</td>
</tr>
<tr>
<td style="text-align: center;">T5-Large</td>
<td style="text-align: center;">22.74</td>
<td style="text-align: center;">19.75</td>
</tr>
<tr>
<td style="text-align: center;">T5-Base</td>
<td style="text-align: center;">12.90</td>
<td style="text-align: center;">11.54</td>
</tr>
</tbody>
</table>
<p>(b) Execution accuracy (EX) and Valid Efficiency Score (VES) on the development set of BIRD</p>
<p>Table 3: Performance of DIN-SQL on BIRD development set and test set.
on the easy class (compared to basic few-shot) is due to incorporating schema links in the prompt, highlighting the importance of our schema-linking module.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Prompting</th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">EX</th>
<th style="text-align: center;">EM</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">DIN-SQL (Ours)</td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">74.2</td>
<td style="text-align: center;">60.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CodeX Davinci</td>
<td style="text-align: center;">69.9</td>
<td style="text-align: center;">57.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CodeX Cushman</td>
<td style="text-align: center;">47.6</td>
<td style="text-align: center;">35.7</td>
</tr>
<tr>
<td style="text-align: center;">Few-shot (Ours)</td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">67.4</td>
<td style="text-align: center;">54.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CodeX Davinci</td>
<td style="text-align: center;">61.5</td>
<td style="text-align: center;">50.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CodeX Cushman</td>
<td style="text-align: center;">43.1</td>
<td style="text-align: center;">30.9</td>
</tr>
<tr>
<td style="text-align: center;">Zero-shot (Ours)</td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">64.9</td>
<td style="text-align: center;">40.4</td>
</tr>
<tr>
<td style="text-align: center;">$\begin{gathered} \hline \text { Zero-shot } \ \text { [Liu et al., 2023a] } \ \hline \end{gathered}$</td>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">60.1</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">$\begin{gathered} \hline \text { Zero-shot } \ \text { [Rajkumar et al., 2022] } \end{gathered}$</td>
<td style="text-align: center;">CodeX Davinci</td>
<td style="text-align: center;">47.5</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Zero-shot (DB content used)</td>
<td style="text-align: center;">CodeX Davinci</td>
<td style="text-align: center;">55.1</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">[Rajkumar et al., 2022]</td>
<td style="text-align: center;">CodeX Cushman</td>
<td style="text-align: center;">53</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT3</td>
<td style="text-align: center;">21.7</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>(a) Performance compared to zero-shot and few-shot prompting using different LLMs on the dev set of Spider</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Execution accuracy (EX)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Prompting</td>
<td style="text-align: center;">Model</td>
<td style="text-align: center;">Easy</td>
<td style="text-align: center;">Medium</td>
<td style="text-align: center;">Hard</td>
<td style="text-align: center;">Extra</td>
<td style="text-align: center;">All</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">DIN-SQL</td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">91.1</td>
<td style="text-align: center;">79.8</td>
<td style="text-align: center;">64.9</td>
<td style="text-align: center;">43.4</td>
<td style="text-align: center;">74.2</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">DIN-SQL</td>
<td style="text-align: center;">CodeX Davinci</td>
<td style="text-align: center;">89.1</td>
<td style="text-align: center;">75.6</td>
<td style="text-align: center;">58</td>
<td style="text-align: center;">38.6</td>
<td style="text-align: center;">69.9</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Few-shot</td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">86.7</td>
<td style="text-align: center;">73.1</td>
<td style="text-align: center;">59.2</td>
<td style="text-align: center;">31.9</td>
<td style="text-align: center;">67.4</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Few-shot</td>
<td style="text-align: center;">CodeX Davinci</td>
<td style="text-align: center;">84.7</td>
<td style="text-align: center;">67.3</td>
<td style="text-align: center;">47.1</td>
<td style="text-align: center;">26.5</td>
<td style="text-align: center;">61.5</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Exact set match accuracy (EM)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Prompting</td>
<td style="text-align: center;">Model</td>
<td style="text-align: center;">Easy</td>
<td style="text-align: center;">Medium</td>
<td style="text-align: center;">Hard</td>
<td style="text-align: center;">Extra</td>
<td style="text-align: center;">All</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">DIN-SQL</td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">82.7</td>
<td style="text-align: center;">65.5</td>
<td style="text-align: center;">42</td>
<td style="text-align: center;">30.7</td>
<td style="text-align: center;">60.1</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">DIN-SQL</td>
<td style="text-align: center;">CodeX Davinci</td>
<td style="text-align: center;">78.6</td>
<td style="text-align: center;">67.3</td>
<td style="text-align: center;">38.5</td>
<td style="text-align: center;">17.5</td>
<td style="text-align: center;">57.2</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Few-shot</td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">87.9</td>
<td style="text-align: center;">54</td>
<td style="text-align: center;">47.1</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">54.3</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Few-shot</td>
<td style="text-align: center;">CodeX Davinci</td>
<td style="text-align: center;">77</td>
<td style="text-align: center;">53.8</td>
<td style="text-align: center;">38.5</td>
<td style="text-align: center;">12.7</td>
<td style="text-align: center;">50.2</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>(b) Performance compared to our basic few-shot prompting across different query difficulty levels</p>
<p>Table 4: Performance of DIN-SQL against other in-context learning approaches</p>
<h1>5.5.3 Error improvements</h1>
<p>In Section 3, we did an error analysis of basic few-shot prompting on 500 queries randomly chosen from the training set. To understand the degree those errors are resolved, we ran DIN-SQL on the same 500 queries. As shown in Figure 4, our proposed approach improves the performance for all categories with the largest improvement seen for the JOIN and Nested categories. Despite having an explicit module for schema-linking, the largest portion of failure cases still belong to this category.</p>
<h3>5.6 Ablation study</h3>
<p>In an ablation study, we evaluated our approach with and without each of the four modules. As shown in Table 5 for the CodeX Davinci model, excluding any of the modules leads to an overall decrease in performance, in terms of the execution accuracy.
More details emerge as we study the effectiveness of each module across different query classes. Schema linking helps all query classes with the least improvement for the hard class. Our inspection of a sample of the failed cases reveals that schema linking sometimes finds redundant links due to an ambiguity in the question or schema, and this can introduce redundant joins or output columns.
Without a classification, we had to use either a simple few-shot prompting or a decomposed chain-of-thought (COT) prompting for all queries. The reported performance without a classification</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Prompting</th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Easy</th>
<th style="text-align: center;">Medium</th>
<th style="text-align: center;">Hard</th>
<th style="text-align: center;">Extra</th>
<th style="text-align: center;">All</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">DIN-SQL (generic self-corr)</td>
<td style="text-align: center;">CodeX Davinci</td>
<td style="text-align: center;">$\mathbf{8 9 . 1}$</td>
<td style="text-align: center;">75.6</td>
<td style="text-align: center;">$\mathbf{5 8}$</td>
<td style="text-align: center;">$\mathbf{3 8 . 6}$</td>
<td style="text-align: center;">$\mathbf{6 9 . 9}$</td>
</tr>
<tr>
<td style="text-align: center;">DIN-SQL (gentle self-corr)</td>
<td style="text-align: center;">CodeX Davinci</td>
<td style="text-align: center;">87.5</td>
<td style="text-align: center;">$\mathbf{7 6 . 9}$</td>
<td style="text-align: center;">51.7</td>
<td style="text-align: center;">36.1</td>
<td style="text-align: center;">68.7</td>
</tr>
<tr>
<td style="text-align: center;">DIN-SQL w/o self-corr</td>
<td style="text-align: center;">CodeX Davinci</td>
<td style="text-align: center;">83.9</td>
<td style="text-align: center;">75.4</td>
<td style="text-align: center;">52.3</td>
<td style="text-align: center;">36.1</td>
<td style="text-align: center;">67.3</td>
</tr>
<tr>
<td style="text-align: center;">DIN-SQL w/o schema-linking</td>
<td style="text-align: center;">CodeX Davinci</td>
<td style="text-align: center;">87.3</td>
<td style="text-align: center;">70.6</td>
<td style="text-align: center;">57.6</td>
<td style="text-align: center;">27.1</td>
<td style="text-align: center;">65.9</td>
</tr>
<tr>
<td style="text-align: center;">DIN-SQL w/o classification <br> (simple few-shot prompting)</td>
<td style="text-align: center;">CodeX Davinci</td>
<td style="text-align: center;">87.9</td>
<td style="text-align: center;">68.2</td>
<td style="text-align: center;">51.7</td>
<td style="text-align: center;">27.1</td>
<td style="text-align: center;">63.1</td>
</tr>
<tr>
<td style="text-align: center;">DIN-SQL w/o classification <br> (decomposed COT prompting)</td>
<td style="text-align: center;">CodeX Davinci</td>
<td style="text-align: center;">84.2</td>
<td style="text-align: center;">71.2</td>
<td style="text-align: center;">54.3</td>
<td style="text-align: center;">38.6</td>
<td style="text-align: center;">68.2</td>
</tr>
<tr>
<td style="text-align: center;">DIN-SQL (gentle self-corr)</td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">$\mathbf{9 1 . 1}$</td>
<td style="text-align: center;">$\mathbf{7 9 . 8}$</td>
<td style="text-align: center;">$\mathbf{6 4 . 9}$</td>
<td style="text-align: center;">$\mathbf{4 3 . 4}$</td>
<td style="text-align: center;">$\mathbf{7 4 . 2}$</td>
</tr>
<tr>
<td style="text-align: center;">DIN-SQL (generic self-corr)</td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">89.9</td>
<td style="text-align: center;">76.5</td>
<td style="text-align: center;">59.2</td>
<td style="text-align: center;">34.3</td>
<td style="text-align: center;">70.0</td>
</tr>
<tr>
<td style="text-align: center;">DIN-SQL w/o self-correc</td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">$\mathbf{9 1 . 1}$</td>
<td style="text-align: center;">79.1</td>
<td style="text-align: center;">63.2</td>
<td style="text-align: center;">41.6</td>
<td style="text-align: center;">73.3</td>
</tr>
</tbody>
</table>
<p>Table 5: Performance of our method, in terms of execution accuracy, on the dev set with and without each module
module in Table 5 is for our comprehensive framework that includes all our components except classification. This means that the approach contains not only COT prompting but also Schema Linking, Self-Correction, and NatSQL Intermediate Representation, all of which are significant contributions of our work. The decomposed chain-of-thought result presented in this table refers to employing the most complex prompt, developed for the nested complex class, for all questions instead of adopting a classification-based approach to determine prompt complexity based on the question's level of difficulty. In contrast, the result for the DIN-SQL with simple few-shot prompting refers to using the simplest prompting class, easy class, for all questions across different level's of difficulty. As expected, a decomposed chain-of-thought prompting works better for hard and extra hard queries whereas a simple few-shot works better for the easy class.</p>
<p>For self-correction, we ran our study using both CodeX Davinci and GPT-4. For CodeX Davinci, a generic self-correction prompt helps the model across all query classes. A gentle self-correction prompt is also helpful but the gain is smaller than generic one for CodeX Davinci. However, there is less chance that GPT-4 generates a buggy code, and giving a generic prompt of "Buggy SQL:... Fixed SQL:..." can hurt the performance. A gentle prompt work better for GPT-4 and improves the perfromance across all of the classes except the easy class.</p>
<h1>6 Conclusions</h1>
<p>Prompting has enabled large language models to achieve impressive performance on numerous NLP tasks across different domains, without requiring a large training set. Prior to our research, the effectiveness of prompting methods utilizing LLMs for the text-to-SQL task was inferior to that of models fine-tuned for the task. To bridge this gap, we have devised a decomposition technique to tackle some of the challenges that caused this disparity. Our extensive experiments on two challenging datasets of Spider and BIRD show that our method significantly improves the performance of prompting across all query classes, producing comparable or even superior results to state-of-the-art fine-tuned approaches.</p>
<h2>7 Limitations</h2>
<p>There are some limitations or areas of improvement to this work. Our manually constructed demonstrations are fixed for each query class. Future research may explore adaptive and automated methods for generating demonstrations at finer granularities, which can further enhance the performance of our approach. Additionally, as of the time of writing this paper, our proposed approach, characterized by its decomposed and step-by-step structure, incurs a cost of approximately $\$ 0.5$ and exhibits a latency of approximately 60 seconds when responding to a natural language question from the Spider dataset using GPT-4. We anticipate that as LLMs continue to advance, these costs and latencies should decrease, but reducing the cost is another possible direction.</p>
<h1>Acknowledgement</h1>
<p>This research was funded by Natural Sciences and Engineering Research Council of Canada. We wish to thank Tao Yu and Hongjin Su for running our code on the hold out test set of Spider and Jinyang Li, Binyuan Hui, Reynold Cheng, Ge Qu and the other authors of BIRD for running our code on the holdout test set of BIRD. We also wish to thank Csaba Czepesvari, Dale Schuurmans and the anonymous reviewers of NeurIPS for their constructive comments to improve this work.</p>
<h2>References</h2>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.</p>
<p>Ruichu Cai, Jinjie Yuan, Boyan Xu, and Zhifeng Hao. Sadga: Structure-aware dual graph aggregation network for text-to-sql. Advances in Neural Information Processing Systems, 34:7664-7676, 2021.</p>
<p>Ruisheng Cao, Lu Chen, Zhi Chen, Yanbin Zhao, Su Zhu, and Kai Yu. Lgesql: line graph enhanced text-to-sql model with mixed local and non-local relations. arXiv preprint arXiv:2106.01093, 2021.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.</p>
<p>Wenhu Chen. Large language models are few (1)-shot table reasoners. arXiv preprint arXiv:2210.06710, 2022.</p>
<p>DongHyun Choi, Myeong Cheol Shin, EungGyun Kim, and Dong Ryeol Shin. Ryansql: Recursively applying sketch-based slot fillings for complex text-to-sql in cross-domain databases. Computational Linguistics, 47(2):309-332, 2021.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.</p>
<p>Li Dong and Mirella Lapata. Language to logical form with neural attention. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 33-43, 2016.</p>
<p>Yujian Gan, Xinyun Chen, Jinxia Xie, Matthew Purver, John R Woodward, John Drake, and Qiaofu Zhang. Natural sql: Making sql easier to infer from natural language specifications. arXiv preprint arXiv:2109.05153, 2021.</p>
<p>Alex Graves and Alex Graves. Long short-term memory. Supervised sequence labelling with recurrent neural networks, pages 37-45, 2012.</p>
<p>Jiaqi Guo, Zecheng Zhan, Yan Gao, Yan Xiao, Jian-Guang Lou, Ting Liu, and Dongmei Zhang. Towards complex text-to-sql in cross-domain database with intermediate representation. arXiv preprint arXiv:1905.08205, 2019.</p>
<p>Zhixin Guo, Minyxuan Yan, Jiexing Qi, Jianping Zhou, Ziwei He, Zhouhan Lin, Guanjie Zheng, and Xinbing Wang. Few-shot table-to-text generation with prompt planning and knowledge memorization. arXiv preprint arXiv:2302.04415, 2023.</p>
<p>Jonathan Herzig, Paweł Krzysztof Nowak, Thomas Müller, Francesco Piccinno, and Julian Martin Eisenschlos. Tapas: Weakly supervised table parsing via pre-training. arXiv preprint arXiv:2004.02349, 2020.</p>
<p>Junyang Huang, Yongbo Wang, Yongliang Wang, Yang Dong, and Yanghua Xiao. Relation aware semi-autoregressive semantic parsing for nl2sql. arXiv preprint arXiv:2108.00804, 2021.</p>
<p>Binyuan Hui, Xiang Shi, Ruiying Geng, Binhua Li, Yongbin Li, Jian Sun, and Xiaodan Zhu. Improving text-to-sql with schema dependency learning. arXiv preprint arXiv:2103.04399, 2021.</p>
<p>Wonseok Hwang, Jinyeong Yim, Seunghyun Park, and Minjoon Seo. A comprehensive exploration on wikisql with table-aware word contextualization. arXiv preprint arXiv:1902.01069, 2019.</p>
<p>Rohit Kate. Transforming meaning representation grammars to improve semantic parsing. In CoNLL 2008: Proceedings of the Twelfth Conference on Computational Natural Language Learning, pages 33-40, 2008.</p>
<p>Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. Decomposed prompting: A modular approach for solving complex tasks. arXiv preprint arXiv:2210.02406, 2022.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022.</p>
<p>Brenden Lake and Marco Baroni. Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks. In International conference on machine learning, pages 2873-2882. PMLR, 2018.</p>
<p>Wenqiang Lei, Weixin Wang, Zhixin Ma, Tian Gan, Wei Lu, Min-Yen Kan, and Tat-Seng Chua. Re-examining the role of schema linking in text-to-sql. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6943-6954, 2020.</p>
<p>Fei Li and Hosagrahar V Jagadish. Constructing an interactive natural language interface for relational databases. Proceedings of the VLDB Endowment, 8(1):73-84, 2014.</p>
<p>Haoyang Li, Jing Zhang, Cuiping Li, and Hong Chen. Decoupling the skeleton parsing and schema linking for text-to-sql. arXiv preprint arXiv:2302.05965, 2023a.</p>
<p>Jinyang Li, Binyuan Hui, Reynold Cheng, Bowen Qin, Chenhao Ma, Nan Huo, Fei Huang, Wenyu Du, Luo Si, and Yongbin Li. Graphix-t5: Mixing pre-trained transformers with graph-aware layers for text-to-sql parsing. arXiv preprint arXiv:2301.07507, 2023b.</p>
<p>Jinyang Li, Binyuan Hui, Ge Qu, Binhua Li, Jiaxi Yang, Bowen Li, Bailin Wang, Bowen Qin, Rongyu Cao, Ruiying Geng, Nan Huo, Chenhao Ma, Kevin C. C. Chang, Fei Huang, Reynold Cheng, and Yongbin Li. Can llm already serve as a database interface? a big bench for large-scale database grounded text-to-sqls, 2023c.</p>
<p>Yunyao Li, Huahai Yang, and HV Jagadish. Nalix: A generic natural language search environment for xml data. ACM Transactions on database systems (TODS), 32(4):30-es, 2007.</p>
<p>Aiwei Liu, Xuming Hu, Lijie Wen, and Philip S Yu. A comprehensive evaluation of chatgpt's zero-shot text-to-sql capability. arXiv preprint arXiv:2303.13547, 2023a.</p>
<p>Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 55(9):1-35, 2023b.</p>
<p>Ana-Maria Popescu, Oren Etzioni, and Henry Kautz. Towards a theory of natural language interfaces to databases. In Proceedings of the 8th international conference on Intelligent user interfaces, pages 149-157, 2003.</p>
<p>Ana-Maria Popescu, Alex Armanasu, Oren Etzioni, David Ko, and Alexander Yates. Modern natural language interfaces to databases: Composing statistical parsing with semantic tractability. In COLING 2004: Proceedings of the 20th International Conference on Computational Linguistics, pages 141-147, 2004.</p>
<p>Jiexing Qi, Jingyao Tang, Ziwei He, Xiangpeng Wan, Chenghu Zhou, Xinbing Wang, Quanshi Zhang, and Zhouhan Lin. Rasat: Integrating relational structures into pretrained seq2seq model for text-to-sql. arXiv preprint arXiv:2205.06983, 2022.</p>
<p>Bowen Qin, Binyuan Hui, Lihan Wang, Min Yang, Jinyang Li, Binhua Li, Ruiying Geng, Rongyu Cao, Jian Sun, Luo Si, et al. A survey on text-to-sql parsing: Concepts, methods, and future directions. arXiv preprint arXiv:2208.13629, 2022.</p>
<p>Nitarshan Rajkumar, Raymond Li, and Dzmitry Bahdanau. Evaluating the text-to-sql capabilities of large language models. arXiv preprint arXiv:2204.00498, 2022.</p>
<p>Ohad Rubin and Jonathan Berant. Smbop: Semi-autoregressive bottom-up semantic parsing. arXiv preprint arXiv:2010.12412, 2020.</p>
<p>Torsten Scholak, Nathan Schucher, and Dzmitry Bahdanau. Picard: Parsing incrementally for constrained auto-regressive decoding from language models. arXiv preprint arXiv:2109.05093, 2021.</p>
<p>Niculae Stratica, Leila Kosseim, and Bipin C Desai. Using semantic templates for a natural language interface to the cindi virtual library. Data \&amp; Knowledge Engineering, 55(1):4-19, 2005.</p>
<p>Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. Advances in neural information processing systems, 27, 2014.</p>
<p>Bailin Wang, Richard Shin, Xiaodong Liu, Oleksandr Polozov, and Matthew Richardson. Ratsql: Relation-aware schema encoding and linking for text-to-sql parsers. arXiv preprint arXiv:1911.04942, 2019.</p>
<p>Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022a.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022b.</p>
<p>Xiaojun Xu, Chang Liu, and Dawn Song. Sqlnet: Generating structured queries from natural language without reinforcement learning. arXiv preprint arXiv:1711.04436, 2017.</p>
<p>Kuan Xuan, Yongbo Wang, Yongliang Wang, Zujie Wen, and Yang Dong. Sead: end-to-end text-to-sql generation with schema-aware denoising. arXiv preprint arXiv:2105.07911, 2021.</p>
<p>Pengcheng Yin, Graham Neubig, Wen-tau Yih, and Sebastian Riedel. Tabert: Pretraining for joint understanding of textual and tabular data. arXiv preprint arXiv:2005.08314, 2020.</p>
<p>Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, et al. Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task. arXiv preprint arXiv:1809.08887, 2018.</p>
<p>Tao Yu, Chien-Sheng Wu, Xi Victoria Lin, Bailin Wang, Yi Chern Tan, Xinyi Yang, Dragomir Radev, Richard Socher, and Caiming Xiong. Grappa: Grammar-augmented pre-training for table semantic parsing. arXiv preprint arXiv:2009.13845, 2020.</p>
<p>Lu Zeng, Sree Hari Krishnan Parthasarathi, and Dilek Hakkani-Tur. N-best hypotheses reranking for text-to-sql systems. arXiv preprint arXiv:2210.10668, 2022.</p>
<p>Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in large language models. arXiv preprint arXiv:2210.03493, 2022.</p>
<p>Yiyun Zhao, Jiarong Jiang, Yiqun Hu, Wuwei Lan, Henry Zhu, Anuj Chauhan, Alexander Li, Lin Pan, Jun Wang, Chung-Wei Hang, et al. Importance of synthesizing high-quality data for text-to-sql parsing. arXiv preprint arXiv:2212.08785, 2022.</p>
<p>Ruiqi Zhong, Tao Yu, and Dan Klein. Semantic evaluation for text-to-sql with distilled test suite. In The 2020 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 2020.</p>
<p>Victor Zhong, Caiming Xiong, and Richard Socher. Seq2sql: Generating structured queries from natural language using reinforcement learning. arXiv preprint arXiv:1709.00103, 2017.</p>
<p>Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2022.</p>
<h1>A Prompts</h1>
<p>This section presents a comprehensive list of all the prompts utilized in the four modules of our proposed methodology on both the GPT-4 and CodeX models. The prompts used for each module are provided in detail to allow for easy replication and understanding of the approach. Additionally, we have also included the prompt we used for the few-shot and zero-shot implementations of our method.</p>
<p>For our few-shot examples used for the Non-Nested Complex and Nested Complex classes of queries, we used the NatSQL intermediate representations from the NatSQL Github repository ${ }^{2}$. The repository gives the intermediate representation for all queries in the training set of Spider.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>A. 1 Zero-shot prompting</h1>
<p>The prompt utilized for the zero-shot prompting scenario draws its inspiration from the work of Liu et al. [2023a], proposed for the ChatGPT. In figure 5, we demonstrate one example for the Zero-shot prompting used in our work.</p>
<div class="codehilite"><pre><span></span><code><span class="gu">##</span># Complete SQLite SQLI QUERY only and with no explanation
<span class="gu">##</span># SQLite SQLI tables, with their properties:
<span class="gh">#</span>
# concert(*, concert_ID, concert_Name, Theme, Stadium_ID, Year)
<span class="gh">#</span> singer(*, Singer_ID, Name, Country, Song_Name, Song_release_Year, Age, Is_Male)
<span class="gh">#</span> singer_in_concert(*, Concert_ID, Singer_ID)
<span class="gh">#</span> stadium(*, Stadium_ID, Location, Name, Capacity, Highest, Lowest, Average)
<span class="gh">#</span>
### How many singers do we have?
</code></pre></div>

<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: An example of Zero-shot prompting.</p>
<h2>A. 2 Few-shot prompting</h2>
<p># Create SQL queries for the given questions.
Table advisor, columns = [<em>,s_ID,i_ID]
Table classroom, columns = [</em>,building,room_number,capacity]
Table course, columns = [<em>,course_id,title,dept_name,credits]
Table department, columns = [</em>,dept_name,building,budget]
Table instructor, columns = [<em>,ID,name,dept_name,salary]
Table prereq, columns = [</em>,course_id,prereq_id]
Table section, columns = [<em>,course_id,sec_id,semester,year,building,room_number,time_slot_id]
Table student, columns = [</em>,ID,name,dept_name,tot_cred]
Table takes, columns = [<em>,ID,course_id,sec_id,semester,year,grade]
Table teaches, columns = [</em>,ID,course_id,sec_id,semester,year]
Table time_slot, columns = [*,time_slot_id,day,start_hr,start_min,end_hr,end_min]
Q: "Find the buildings which have rooms with capacity more than 50."
SQL: SELECT DISTINCT building FROM classroom WHERE capacity $&gt;50$
Q: "Find the room number of the rooms which can sit 50 to 100 students and their buildings."
SQL: SELECT building, room_number FROM classroom WHERE capacity BETWEEN 50 AND 100</p>
<p>Q: "Give the name of the student in the History department with the most credits."
SQL: SELECT name FROM student WHERE dept_name = 'History' ORDER BY tot_cred DESC LIMIT 1</p>
<p>Q: "Find the total budgets of the Marketing or Finance department."
SQL: SELECT sum(budget) FROM department WHERE dept_name = 'Marketing' OR dept_name =</p>
<p>'Finance'
Q: "Find the department name of the instructor whose name contains 'Soisalon'."
SQL: SELECT dept_name FROM instructor WHERE name LIKE '\%Soisalon\%'
Q: "What is the name of the department with the most credits?"
SQL: SELECT dept_name FROM course GROUP BY dept_name ORDER BY sum(credits) DESC LIMIT 1</p>
<p>Q: "How many instructors teach a course in the Spring of 2010?"
SQL: SELECT COUNT (DISTINCT ID) FROM teaches WHERE semester = 'Spring' AND YEAR $=2010$</p>
<p>Q: "Find the name of the students and their department names sorted by their total credits in ascending order."
SQL: SELECT name, dept_name FROM student ORDER BY tot_cred
Q: "Find the year which offers the largest number of courses."
SQL: SELECT YEAR FROM SECTION GROUP BY YEAR ORDER BY count(*) DESC LIMIT 1
Q: "What are the names and average salaries for departments with average salary higher than 42000 ?"
SQL: SELECT dept_name, AVG (salary) FROM instructor GROUP BY dept_name HAVING AVG (salary) $&gt;42000$</p>
<p>Q: "How many rooms in each building have a capacity of over 50?"
SQL: SELECT count(<em>) , building FROM classroom WHERE capacity &gt; 50 GROUP BY building
Q: "Find the names of the top 3 departments that provide the largest amount of courses?"
SQL: SELECT dept_name FROM course GROUP BY dept_name ORDER BY count(</em>) DESC LIMIT 3</p>
<p>Q: "Find the maximum and average capacity among rooms in each building."
SQL: SELECT max (capacity), avg(capacity), building FROM classroom GROUP BY building
Q: "Find the title of the course that is offered by more than one department."
SQL: SELECT title FROM course GROUP BY title HAVING count(*) &gt; 1
Q: "Find the total budgets of the Marketing or Finance department."
SQL: SELECT sum(budget) FROM department WHERE dept_name = 'Marketing' OR dept_name = 'Finance'</p>
<p>Q: "Find the name and building of the department with the highest budget."
SQL: SELECT dept_name, building FROM department ORDER BY budget DESC LIMIT 1
Q: "What is the name and building of the departments whose budget is more than the average budget?"
SQL: SELECT dept_name, building FROM department WHERE budget &gt; (SELECT avg(budget) FROM department)</p>
<p>Q: "Find the total number of students and total number of instructors for each department."
SQL: SELECT count(DISTINCT T2.id), count(DISTINCT T3.id), T3.dept_name FROM department AS T1 JOIN student AS T2 ON T1.dept_name = T2.dept_name JOIN instructor AS T3 ON T1.dept_name = T3.dept_name GROUP BY T3.dept_name</p>
<p>Q: "Find the title of courses that have two prerequisites?"
SQL: SELECT T1.title FROM course AS T1 JOIN prereq AS T2 ON T1.course_id = T2.course_id GROUP BY T2.course_id HAVING count(*) $=2$</p>
<p>Q: "Find the name of students who took any class in the years of 2009 and 2010."
SQL: SELECT DISTINCT T1.name FROM student AS T1 JOIN takes AS T2 ON T1.id = T2.id WHERE T2.YEAR $=2009$ OR T2.YEAR $=2010$</p>
<p>Q: "list in alphabetic order all course names and their instructors' names in year 2008."
SQL: SELECT T1.title , T3.name FROM course AS T1 JOIN teaches AS T2 ON T1.course_id = T2.course_id JOIN instructor AS T3 ON T2.id = T3.id WHERE T2.YEAR = 2008 ORDER BY T1.title</p>
<p>Q: "Find the title of courses that have two prerequisites?"
SQL: SELECT T1.title FROM course AS T1 JOIN prereq AS T2 ON T1.course_id = T2.course_id GROUP BY T2.course_id HAVING count(*) $=2$</p>
<p>Q: "Find the name and building of the department with the highest budget."
SQL: SELECT dept_name, building FROM department ORDER BY budget DESC LIMIT 1
Q: "Find the title, credit, and department name of courses that have more than one prerequisites?"
SQL: SELECT T1.title , T1.credits , T1.dept_name FROM course AS T1 JOIN prereq AS T2 ON T1.course_id = T2.course_id GROUP BY T2.course_id HAVING count(*) $&gt;1$</p>
<p>Q: "Give the name and building of the departments with greater than average budget."
SQL: SELECT dept_name, building FROM department WHERE budget &gt; (SELECT avg(budget) FROM department)</p>
<p>Q: "Find the id of instructors who taught a class in Fall 2009 but not in Spring 2010."
SQL: SELECT id FROM teaches WHERE semester = 'Fall' AND YEAR = 2009 EXCEPT SELECT id FROM teaches WHERE semester = 'Spring' AND YEAR = 2010</p>
<p>Q: "Find the name of the courses that do not have any prerequisite?"
SQL: SELECT title FROM course WHERE course_id NOT IN (SELECT course_id FROM prereq)
Q: "Find the salaries of all distinct instructors that are less than the largest salary."
SQL: SELECT DISTINCT salary FROM instructor WHERE salary &lt; (SELECT max(salary) FROM instructor)</p>
<p>Q: "Find the names of students who have taken any course in the fall semester of year 2003."</p>
<p>SQL: SELECT name FROM student WHERE id IN (SELECT id FROM takes WHERE semester = 'Fall' AND YEAR = 2003)</p>
<p>Q: "Find the minimum salary for the departments whose average salary is above the average payment of all instructors."
SQL: SELECT min(salary), dept_name FROM instructor GROUP BY dept_name HAVING avg(salary) $&gt;$ (SELECT avg(salary) FROM instructor)</p>
<p>Q: "What is the course title of the prerequisite of course Mobile Computing?"
SQL: SELECT title FROM course WHERE course_id IN (SELECT T1.prereq_id FROM prereq AS T1 JOIN course AS T2 ON T1.course_id = T2.course_id WHERE T2.title = 'Mobile Computing')</p>
<p>Q: "Give the title and credits for the course that is taught in the classroom with the greatest capacity."
SQL: SELECT T3.title , T3.credits FROM classroom AS T1 JOIN SECTION AS T2 ON T1.building = T2.building AND T1.room_number = T2.room_number JOIN course AS T3 ON T2.course_id = T3.course_id WHERE T1.capacity $=($ SELECT max(capacity) FROM classroom $)$</p>
<h1>A. 3 Schema linking prompt</h1>
<p># Find the schema_links for generating SQL queries for each question based on the database schema and Foreign keys.</p>
<p>Table city, columns = [<em>,City_ID,Official_Name,Status,Area_km_2,Population,Census_Ranking] Table competition_record, columns = [</em>,Competition_ID,Farm_ID,Rank]
Table farm, columns = [<em>,Farm_ID,Year,Total_Horses,Working_Horses,
Total_Cattle,Oxen,Bulls,Cows,Pigs,Sheep_and_Goats]
Table farm_competition, columns = [</em>,Competition_ID,Year,Theme,Host_city_ID,Hosts]
Foreign_keys = [farm_competition.Host_city_ID = city.City_ID,competition_record.Farm_ID = farm.Farm_ID,competition_record.Competition_ID = farm_competition.Competition_ID]
Q: "Show the status of the city that has hosted the greatest number of competitions."
A: Let's think step by step. In the question "Show the status of the city that has hosted the greatest number of competitions.", we are asked:
"the status of the city" so we need column = [city.Status]
"greatest number of competitions" so we need column = [farm_competition.<em>]
Based on the columns and tables, we need these Foreign_keys = [farm_competition.Host_city_ID = city.City_ID].
Based on the tables, columns, and Foreign_keys, The set of possible cell values are = []. So the Schema_links are:
Schema_links: [city.Status,farm_competition.Host_city_ID = city.City_ID,farm_competition.</em>]
Table department, columns = [<em>,Department_ID,Name,Creation,Ranking,Budget_in_Billions,Num_Employees]
Table head, columns = [</em>,head_ID,name,born_state,age]
Table management, columns = [<em>,department_ID,head_ID,temporary_acting]
Foreign_keys = [management.head_ID = head.head_ID,management.department_ID = department.Department_ID]
Q: "How many heads of the departments are older than 56 ?"
A: Let's think step by step. In the question "How many heads of the departments are older than 56 ?", we are asked:
"How many heads of the departments" so we need column = [head.</em>]
"older" so we need column = [head.age]
Based on the columns and tables, we need these Foreign_keys = [].
Based on the tables, columns, and Foreign_keys, The set of possible cell values are = [56]. So the Schema_links are:
Schema_links: [head.<em>,head.age,56]
Table department, columns = [</em>,Department_ID,Name,Creation,Ranking,Budget_in_Billions,Num_Employees]
Table head, columns = [<em>,head_ID,name,born_state,age]
Table management, columns = [</em>,department_ID,head_ID,temporary_acting]
Foreign_keys = [management.head_ID = head.head_ID,management.department_ID = department.Department_ID]
Q: "what are the distinct creation years of the departments managed by a secretary born in state 'Alabama'?"
A: Let's think step by step. In the question "what are the distinct creation years of the departments managed by a secretary born in state 'Alabama'?", we are asked:
"distinct creation years of the departments" so we need column = [department.Creation]
"departments managed by" so we need column = [management.department_ID]
"born in" so we need column = [head.born_state]
Based on the columns and tables, we need these Foreign_keys = [department.Department_ID = management.department_ID,management.head_ID = head.head_ID].
Based on the tables, columns, and Foreign_keys, The set of possible cell values are = ['Alabama']. So the Schema_links are:
Schema_links: [department.Creation,department.Department_ID = management.department_ID, head.head_ID = management.head_ID,head.born_state,'Alabama']</p>
<p>Table Addresses, columns = [<em>,address_id,line_1,line_2,city,zip_postcode,state_province_county,country]
Table Candidate_Assessments, columns = [</em>,candidate_id,qualification,assessment_date,asessment_outcome_code]</p>
<p>Table Candidates, columns = [<em>,candidate_id,candidate_details]
Table Courses, columns = [</em>,course_id,course_name,course_description,other_details]
Table People, columns = [<em>,person_id,first_name,middle_name,
last_name,cell_mobile_number,email_address,login_name,password]
Table People_Addresses, columns = [</em>,person_address_id,person_id,address_id,date_from,date_to]
Table Student_Course_Attendance, columns = [<em>,student_id,course_id,date_of_attendance]
Table Student_Course_Registrations, columns = [</em>,student_id,course_id,registration_date]
Table Students, columns = [<em>,student_id,student_details]
Foreign_keys = [Students.student_id = People.person_id,People_Addresses.address_id = Addresses.address_id,People_Addresses.person_id =
People.person_id,Student_Course_Registrations.course_id =
Courses.course_id,Student_Course_Registrations.student_id =
Students.student_id,Student_Course_Attendance.student_id =
Student_Course_Registrations.student_id,Student_Course_Attendance.course_id = Student_Course_Registrations.course_id,Candidates.candidate_id =
People.person_id,Candidate_Assessments.candidate_id = Candidates.candidate_id]
Q: "List the id of students who never attends courses?"
A: Let's think step by step. In the question "List the id of students who never attends courses?", we are asked:
"id of students" so we need column = [Students.student_id]
"never attends courses" so we need column = [Student_Course_Attendance.student_id]
Based on the columns and tables, we need these Foreign_keys = [Students.student_id = Student_Course_Attendance.student_id].
Based on the tables, columns, and Foreign_keys, The set of possible cell values are = []. So the Schema_links are:
Schema_links: [Students.student_id = Student_Course_Attendance.student_id]
Table Country, columns = [</em>,id,name]
Table League, columns = [<em>,id,country_id,name]
Table Player, columns = [</em>,id,player_api_id,player_name,player_fifa_api_id,birthday,height,height,height]
Table Player_Attributes, columns = [<em>,id,player_fifa_api_id,player_api_id,date,overall_rating,potential ,preferred_foot,attacking_work_rate,defensive_work_rate,crossing,finishing ,heading_accuracy,short_passing,volleys,dribbling,curve,free_kick_accuracy ,long_passing,ball_control,acceleration,sprint_speed,agility,reactions,balance ,shot_power,jumping,stamina,strength,long_shots,aggression,interceptions ,positioning,vision,penalties,marking,standing_tackle,sliding_tackle,gk_diving ,gk_handling,gk_kicking,gk_positioning,gk_reflexes]
Table Team, columns = [</em>,id,team_api_id,team_fifa_api_id,team_long_name,team_short_name]
Table Team_Attributes, columns = [<em>,id,team_fifa_api_id,team_api_id,date,buildUpPlaySpeed ,buildUpPlaySpeedClass,buildUpPlayDribbling,buildUpPlayDribblingClass ,buildUpPlayPassing,buildUpPlayPassingClass,buildUpPlayPositioningClass,chanceCreationPassing ,chanceCreationPassingClass,chanceCreationCrossing,chanceCreationCrossingClass ,chanceCreationShooting,chanceCreationShootingClass,chanceCreationPositioningClass ,defencePressure,defencePressureClass,defenceAggression,defenceAggressionClass ,defenceTeamWidth,defenceTeamWidthClass,defenceDefenderLineClass]
Table sqlite_sequence, columns = [</em>,name,seq]
Foreign_keys = [Player_Attributes.player_api_id = Player.player_api_id,
Player_Attributes.player_fifa_api_id = Player.player_fifa_api_id,
League.country_id = Country.id,Team_Attributes.team_api_id = Team.team_api_id,
Team_Attributes.team_fifa_api_id = Team.team_fifa_api_id]
Q: "List the names of all left-footed players who have overall rating between 85 and 90 ."
A: Let's think step by step. In the question "List the names of all left-footed players who have overall rating between 85 and 90.", we are asked:
"names of all left-footed players" so we need column = [Player.player_name,Player_Attributes.preferred_foot]
"players who have overall rating" so we need column = [Player_Attributes.overall_rating]
Based on the columns and tables, we need these Foreign_keys = [Player_Attributes.player_api_id = Player.player_api_id].
Based on the tables, columns, and Foreign_keys, The set of possible cell values are = [left,85,90]. So</p>
<p>the Schema_links are:
Schema_links: [Player.player_name,Player_Attributes.preferred_foot,Player_Attributes.overall_rating, Player_Attributes.player_api_id = Player.player_api_id,left,85,90]</p>
<p>Table advisor, columns = [<em>,s_ID,i_ID]
Table classroom, columns = [</em>,building,room_number,capacity]
Table course, columns = [<em>,course_id,title,dept_name,credits]
Table department, columns = [</em>,dept_name,building,budget]
Table instructor, columns = [<em>,ID,name,dept_name,salary]
Table prereq, columns = [</em>,course_id,prereq_id]
Table section, columns = [<em>,course_id,sec_id,semester,year,building,room_number,time_slot_id]
Table student, columns = [</em>,ID,name,dept_name,tot_cred]
Table takes, columns = [<em>,ID,course_id,sec_id,semester,year,grade]
Table teaches, columns = [</em>,ID,course_id,sec_id,semester,year]
Table time_slot, columns = [*,time_slot_id,day,start_hr,start_min,end_hr,end_min]
Foreign_keys $=$ [course.dept_name $=$ department.dept_name,instructor.dept_name $=$ department.dept_name,section.building = classroom.building
,section.room_number = classroom.room_number
,section.course_id = course.course_id,teaches.ID = instructor.ID,teaches.course_id = section.course_id,teaches.sec_id = section.sec_id,
teaches.semester = section.semester,teaches.year = section.year,student.dept_name = department.dept_name,
takes.ID = student.ID,takes.course_id = section.course_id,
takes.sec_id = section.sec_id,takes.semester = section.semester,
takes.year = section.year,advisor.s_ID = student.ID,
advisor.i_ID = instructor.ID,prereq.prereq_id = course.course_id,prereq.course_id = course.course_id]
Q: "Give the title of the course offered in Chandler during the Fall of 2010."
A: Let's think step by step. In the question "Give the title of the course offered in Chandler during the Fall of 2010.", we are asked:
"title of the course" so we need column = [course.title]
"course offered in Chandler" so we need column = [SECTION.building]
"during the Fall" so we need column = [SECTION.semester]
"of 2010" so we need column = [SECTION.year]
Based on the columns and tables, we need these Foreign_keys $=$ [course.course_id $=$ SECTION.course_id].
Based on the tables, columns, and Foreign_keys, The set of possible cell values are $=[$ Chandler,Fall,2010]. So the Schema_links are:
Schema_links: [course.title,course.course_id = SECTION.course_id,SECTION.building,SECTION.year ,SECTION.semester,Chandler,Fall,2010]</p>
<p>Table advisor, columns = [<em>,s_ID,i_ID]
Table classroom, columns = [</em>,building,room_number,capacity]
Table course, columns = [<em>,course_id,title,dept_name,credits]
Table department, columns = [</em>,dept_name,building,budget]
Table instructor, columns = [<em>,ID,name,dept_name,salary]
Table prereq, columns = [</em>,course_id,prereq_id]
Table section, columns = [<em>,course_id,sec_id,semester,year,building,room_number,time_slot_id]
Table student, columns = [</em>,ID,name,dept_name,tot_cred]
Table takes, columns = [<em>,ID,course_id,sec_id,semester,year,grade]
Table teaches, columns = [</em>,ID,course_id,sec_id,semester,year]
Table time_slot, columns = [*,time_slot_id,day,start_hr,start_min,end_hr,end_min]
Foreign_keys $=$ [course.dept_name $=$ department.dept_name,instructor.dept_name $=$ department.dept_name,
section.building = classroom.building,section.room_number = classroom.room_number,
section.course_id = course.course_id,teaches.ID = instructor.ID,teaches.course_id = section.course_id,
teaches.sec_id = section.sec_id,teaches.semester = section.semester,teaches.year = section.year,
student.dept_name = department.dept_name,takes.ID = student.ID,takes.course_id = sec-</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ https://github.com/ygan/NatSQL&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
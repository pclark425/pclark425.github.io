<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4949 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4949</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4949</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-105.html">extraction-schema-105</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <p><strong>Paper ID:</strong> paper-70ca99ac3c21f353b3db948004510a09fdebc4f2</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/70ca99ac3c21f353b3db948004510a09fdebc4f2" target="_blank">Towards a Mechanistic Interpretation of Multi-Step Reasoning Capabilities of Language Models</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> It is shown that MechanisticProbe is able to detect the information of the reasoning tree from the model's attentions for most examples, suggesting that the LM indeed is going through a process of multi-step reasoning within its architecture in many cases.</p>
                <p><strong>Paper Abstract:</strong> Recent work has shown that language models (LMs) have strong multi-step (i.e., procedural) reasoning capabilities. However, it is unclear whether LMs perform these tasks by cheating with answers memorized from pretraining corpus, or, via a multi-step reasoning mechanism. In this paper, we try to answer this question by exploring a mechanistic interpretation of LMs for multi-step reasoning tasks. Concretely, we hypothesize that the LM implicitly embeds a reasoning tree resembling the correct reasoning process within it. We test this hypothesis by introducing a new probing approach (called MechanisticProbe) that recovers the reasoning tree from the model's attention patterns. We use our probe to analyze two LMs: GPT-2 on a synthetic task (k-th smallest element), and LLaMA on two simple language-based reasoning tasks (ProofWriter&AI2 Reasoning Challenge). We show that MechanisticProbe is able to detect the information of the reasoning tree from the model's attentions for most examples, suggesting that the LM indeed is going through a process of multi-step reasoning within its architecture in many cases.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4949.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4949.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>4-shot LLaMA (in-context)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA (7B) with 4-shot in-context prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A few-shot in-context prompting setup where LLaMA is given 4 demonstration examples in the prompt and then asked to predict a single-token answer; used to evaluate whether attention encodes oracle reasoning trees via MechanisticProbe.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models are few-shot learners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA (7B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source causal transformer (7B parameters) used in 4-shot in-context prompting and partially fine-tuned settings in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>4-shot in-context prompting (standard single-prompt inference)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>The model is given 4 exemplar input–answer pairs in the prompt (few-shot), then asked to predict the answer for a new example; a single deterministic prompting/decision procedure is used (no ensembling or multiple diverse chains).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ProofWriter; ARC (subset annotated by Ribeiro et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>ProofWriter: theorem-proving style problems asking True/False given rules/facts (reasoning-tree annotations available). ARC: multiple-choice middle-school science questions (subset annotated with reasoning depth).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>ProofWriter (4-shot): overall acc. 81.72% on depth=0; depth=1: ~78.33%; depth=2: 75.58%; depth=3: 74.64% (Table 5). ARC (4-shot): depth=1 acc. 56.32%; depth=2 acc. 53.40% (Table 5/2).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Finetuned LLaMA (partial finetuning on attention parameters) achieved 100% on ProofWriter across depths (Table 5); probe-derived scores (S_P1, S_P2) are consistently higher for finetuned model than for 4-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The paper finds that 4-shot LLaMA encodes some information about reasoning trees in attention (nonzero probing scores), but (a) its probing scores (S_P1, S_P2) are lower than for finetuned LLaMA, (b) it is more affected by the presence of many irrelevant statements (less robust selection of useful statements), and (c) layer-wise analysis shows identification of useful statements happens early but subsequent reasoning steps emerge later — overall, finetuning yields clearer mechanistic (attention-based) reasoning signals and higher robustness/accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>4-shot LLaMA is less robust to many useless statements (probe S_P1 degrades with larger |S|) and is more sensitive to noise; correlations show lower probing scores correspond to lower accuracy and higher sensitivity to input corruption (noise decreases accuracy ~10% when S_P2 small).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards a Mechanistic Interpretation of Multi-Step Reasoning Capabilities of Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4949.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4949.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Finetuned LLaMA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA (7B) partially finetuned on attention parameters (LLaMA_FT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLaMA 7B partially fine-tuned (attention parameters) on supervised training data for the tasks; used as a comparison to in-context 4-shot LLaMA to evaluate whether fine-tuning produces clearer mechanistic reasoning encodings in attention.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA (7B) - finetuned</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same architecture as above but partially fine-tuned on task-specific supervised data (the paper reports partial finetuning on attention parameters and reports resulting accuracy/probe measures).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Supervised finetuning (partial, attention parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>The model is fine-tuned with supervised labels on the target datasets (e.g., ProofWriter), adjusting parameters (in particular attention parameters) so the model learns a direct mapping from inputs to single-token outputs; again a single deterministic model/policy is used.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ProofWriter (primary); ARC (limited analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>ProofWriter: natural-language theorem-proof style classification (True/False) with provided reasoning-tree annotations; ARC: subset with annotated reasoning depth.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Finetuned LLaMA: achieves 100% accuracy on ProofWriter across depths (Table 5). Probe scores (S_P1, S_P2) are substantially higher than 4-shot LLaMA (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Compared to 4-shot LLaMA which attains e.g. 81.72% on ProofWriter depth=0 and lower probe scores, finetuning yields higher accuracy and clearer attention-encoded reasoning trees.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Finetuning makes the model follow the oracle reasoning tree more clearly (higher S_P1 and S_P2), increases robustness to irrelevant inputs, and produces attention patterns from which the reasoning tree can be more reliably recovered; layer-wise probing shows useful-statement selection occurs earlier and reasoning steps are clearer in middle/top layers for the finetuned model.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>The paper does not report a case where finetuning performs worse than 4-shot for these tasks; however, large numbers of irrelevant statements still reduce probing performance for both settings, and layer pruning was needed to focus analysis (some layers are redundant).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards a Mechanistic Interpretation of Multi-Step Reasoning Capabilities of Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4949.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4949.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-2_FT (k-th smallest)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-2 (finetuned) on k-th smallest element synthetic task</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-2 models were finetuned on a synthetic numerical reasoning task (find k-th smallest element in a list) and analyzed with MechanisticProbe; finetuned GPT-2 achieves high accuracy and attention patterns reveal a two-step procedural solution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 (various sizes as used: e.g., GPT-2 small)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer (GPT-2) variants; models were finetuned per-k on large synthetic datasets to reach >90% test accuracy for many k values.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Procedural multi-step reasoning instantiated as a reasoning tree (select top-k then pick k-th)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>The model learns a deterministic multi-step procedure encoded in its layers: bottom layers attend to/select the top-k elements, and higher layers select the k-th smallest among those — represented as a single reasoning tree per example.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>k-th smallest element (synthetic)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given a list of m numbers (m=16 default), predict the k-th smallest number; inputs tokenized such that each number is one token. Trees depth 1 (leaf nodes = top-k numbers, root = k-th smallest).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Finetuned GPT-2 achieves >90% test accuracy across many k values (e.g., Table 1 shows GPT-2_FT test acc ~99.4% for k=1, 99.42% for k=2, 98.23% for k=3, down to ~91% for larger k).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>MechanisticProbe recovers the reasoning tree from attention for finetuned GPT-2; layer-wise probing and attention visualization indicate bottom layers select top-k and top layers compute the k-th selection; causal pruning (removing heads with low 'size' entropy) degrades task accuracy, supporting that attentions used by the model implement this procedural reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Pretrained GPT-2 without finetuning cannot solve the task (very low accuracy) though it shows weak attention signals; when the task is made much harder (m=64, k up to 32), GPT-2_FT accuracy degrades and S_P2 (ability to pick k-th among top-k) collapses earlier than S_P1 — i.e., the model can often still find top-k elements (S_P1), but fails the selection step (S_P2) when beyond capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards a Mechanistic Interpretation of Multi-Step Reasoning Capabilities of Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4949.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4949.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (CoT) prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-thought prompting elicits reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique (Chain-of-Thought) that elicits multi-step intermediate reasoning steps from language models by providing exemplars that include step-by-step chains of reasoning; mentioned in this paper as relevant but not analyzed by MechanisticProbe.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-thought prompting elicits reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Chain-of-thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>other</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Prompting method that produces explicit intermediate reasoning steps (chains) in the model's generated output by including chain-of-thought exemplars in the prompt; often used for hard multi-step problems and sometimes combined with sampling/ensembling methods.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General hard multi-step reasoning tasks (not directly evaluated in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Problems for which single-token classification is insufficient and explicit intermediate reasoning (textual chain-of-thought) is used to improve model performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Not evaluated in this paper; CoT is cited as relevant prior work and as an approach used in other work for tasks that are difficult to solve without eliciting intermediate reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The paper notes that some recent reasoning tasks require chain-of-thought prompting and that such auto-regressive chain-of-thought settings are left for future mechanistic analysis; MechanisticProbe in this paper focuses on single-token prediction formulations rather than explicit CoT outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>No empirical CoT results in this paper; authors acknowledge as limitation that analysis does not cover chain-of-thought style reasoning and leave it as future work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards a Mechanistic Interpretation of Multi-Step Reasoning Capabilities of Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4949.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4949.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Selection-Inference</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Selection-inference (selection + inference decomposition)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage reasoning framework (selection of relevant facts, then inference) for interpretable logical reasoning with LMs; cited as related work motivating the two-stage probe decomposition (select useful statements then predict reasoning tree).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Selection-inference: Exploiting large language models for interpretable logical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Selection-inference (decomposed reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>other</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Decomposes reasoning into a selection step (identify relevant premises) and an inference step (derive conclusion), which can be implemented with separate prompting or models; this decomposition motivates the paper's two-probe design (P1 select useful statements; P2 predict heights/structure).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General logical reasoning / theorem-proving style problems</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks requiring selection of relevant statements from a context and then applying inference rules to answer a query.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Not directly evaluated in this paper (cited as prior work); the paper implements a related decomposition in its mechanistic probing and shows attention contains signals for both selection and subsequent inference steps.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The paper adopts a similar two-stage decomposition and finds that attention patterns often reflect a two-stage process (early layers select useful statements, later layers perform inference), giving mechanistic credence to selection-inference style decompositions.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>No direct experimental comparison to selection-inference implementations; the paper only cites it as related conceptual work and uses an analogous probe decomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards a Mechanistic Interpretation of Multi-Step Reasoning Capabilities of Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Selection-inference: Exploiting large language models for interpretable logical reasoning <em>(Rating: 2)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>Large language models are in-context semantic reasoners rather than symbolic reasoners <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4949",
    "paper_id": "paper-70ca99ac3c21f353b3db948004510a09fdebc4f2",
    "extraction_schema_id": "extraction-schema-105",
    "extracted_data": [
        {
            "name_short": "4-shot LLaMA (in-context)",
            "name_full": "LLaMA (7B) with 4-shot in-context prompting",
            "brief_description": "A few-shot in-context prompting setup where LLaMA is given 4 demonstration examples in the prompt and then asked to predict a single-token answer; used to evaluate whether attention encodes oracle reasoning trees via MechanisticProbe.",
            "citation_title": "Language models are few-shot learners",
            "mention_or_use": "use",
            "model_name": "LLaMA (7B)",
            "model_description": "Open-source causal transformer (7B parameters) used in 4-shot in-context prompting and partially fine-tuned settings in this paper.",
            "reasoning_method_name": "4-shot in-context prompting (standard single-prompt inference)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "The model is given 4 exemplar input–answer pairs in the prompt (few-shot), then asked to predict the answer for a new example; a single deterministic prompting/decision procedure is used (no ensembling or multiple diverse chains).",
            "task_name": "ProofWriter; ARC (subset annotated by Ribeiro et al.)",
            "task_description": "ProofWriter: theorem-proving style problems asking True/False given rules/facts (reasoning-tree annotations available). ARC: multiple-choice middle-school science questions (subset annotated with reasoning depth).",
            "performance": "ProofWriter (4-shot): overall acc. 81.72% on depth=0; depth=1: ~78.33%; depth=2: 75.58%; depth=3: 74.64% (Table 5). ARC (4-shot): depth=1 acc. 56.32%; depth=2 acc. 53.40% (Table 5/2).",
            "comparison_with_other_method": true,
            "performance_other_method": "Finetuned LLaMA (partial finetuning on attention parameters) achieved 100% on ProofWriter across depths (Table 5); probe-derived scores (S_P1, S_P2) are consistently higher for finetuned model than for 4-shot.",
            "key_findings": "The paper finds that 4-shot LLaMA encodes some information about reasoning trees in attention (nonzero probing scores), but (a) its probing scores (S_P1, S_P2) are lower than for finetuned LLaMA, (b) it is more affected by the presence of many irrelevant statements (less robust selection of useful statements), and (c) layer-wise analysis shows identification of useful statements happens early but subsequent reasoning steps emerge later — overall, finetuning yields clearer mechanistic (attention-based) reasoning signals and higher robustness/accuracy.",
            "counter_examples_or_negative_results": "4-shot LLaMA is less robust to many useless statements (probe S_P1 degrades with larger |S|) and is more sensitive to noise; correlations show lower probing scores correspond to lower accuracy and higher sensitivity to input corruption (noise decreases accuracy ~10% when S_P2 small).",
            "uuid": "e4949.0",
            "source_info": {
                "paper_title": "Towards a Mechanistic Interpretation of Multi-Step Reasoning Capabilities of Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Finetuned LLaMA",
            "name_full": "LLaMA (7B) partially finetuned on attention parameters (LLaMA_FT)",
            "brief_description": "LLaMA 7B partially fine-tuned (attention parameters) on supervised training data for the tasks; used as a comparison to in-context 4-shot LLaMA to evaluate whether fine-tuning produces clearer mechanistic reasoning encodings in attention.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA (7B) - finetuned",
            "model_description": "Same architecture as above but partially fine-tuned on task-specific supervised data (the paper reports partial finetuning on attention parameters and reports resulting accuracy/probe measures).",
            "reasoning_method_name": "Supervised finetuning (partial, attention parameters)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "The model is fine-tuned with supervised labels on the target datasets (e.g., ProofWriter), adjusting parameters (in particular attention parameters) so the model learns a direct mapping from inputs to single-token outputs; again a single deterministic model/policy is used.",
            "task_name": "ProofWriter (primary); ARC (limited analysis)",
            "task_description": "ProofWriter: natural-language theorem-proof style classification (True/False) with provided reasoning-tree annotations; ARC: subset with annotated reasoning depth.",
            "performance": "Finetuned LLaMA: achieves 100% accuracy on ProofWriter across depths (Table 5). Probe scores (S_P1, S_P2) are substantially higher than 4-shot LLaMA (Table 2).",
            "comparison_with_other_method": true,
            "performance_other_method": "Compared to 4-shot LLaMA which attains e.g. 81.72% on ProofWriter depth=0 and lower probe scores, finetuning yields higher accuracy and clearer attention-encoded reasoning trees.",
            "key_findings": "Finetuning makes the model follow the oracle reasoning tree more clearly (higher S_P1 and S_P2), increases robustness to irrelevant inputs, and produces attention patterns from which the reasoning tree can be more reliably recovered; layer-wise probing shows useful-statement selection occurs earlier and reasoning steps are clearer in middle/top layers for the finetuned model.",
            "counter_examples_or_negative_results": "The paper does not report a case where finetuning performs worse than 4-shot for these tasks; however, large numbers of irrelevant statements still reduce probing performance for both settings, and layer pruning was needed to focus analysis (some layers are redundant).",
            "uuid": "e4949.1",
            "source_info": {
                "paper_title": "Towards a Mechanistic Interpretation of Multi-Step Reasoning Capabilities of Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-2_FT (k-th smallest)",
            "name_full": "GPT-2 (finetuned) on k-th smallest element synthetic task",
            "brief_description": "GPT-2 models were finetuned on a synthetic numerical reasoning task (find k-th smallest element in a list) and analyzed with MechanisticProbe; finetuned GPT-2 achieves high accuracy and attention patterns reveal a two-step procedural solution.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 (various sizes as used: e.g., GPT-2 small)",
            "model_description": "Autoregressive transformer (GPT-2) variants; models were finetuned per-k on large synthetic datasets to reach &gt;90% test accuracy for many k values.",
            "reasoning_method_name": "Procedural multi-step reasoning instantiated as a reasoning tree (select top-k then pick k-th)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "The model learns a deterministic multi-step procedure encoded in its layers: bottom layers attend to/select the top-k elements, and higher layers select the k-th smallest among those — represented as a single reasoning tree per example.",
            "task_name": "k-th smallest element (synthetic)",
            "task_description": "Given a list of m numbers (m=16 default), predict the k-th smallest number; inputs tokenized such that each number is one token. Trees depth 1 (leaf nodes = top-k numbers, root = k-th smallest).",
            "performance": "Finetuned GPT-2 achieves &gt;90% test accuracy across many k values (e.g., Table 1 shows GPT-2_FT test acc ~99.4% for k=1, 99.42% for k=2, 98.23% for k=3, down to ~91% for larger k).",
            "comparison_with_other_method": false,
            "performance_other_method": null,
            "key_findings": "MechanisticProbe recovers the reasoning tree from attention for finetuned GPT-2; layer-wise probing and attention visualization indicate bottom layers select top-k and top layers compute the k-th selection; causal pruning (removing heads with low 'size' entropy) degrades task accuracy, supporting that attentions used by the model implement this procedural reasoning.",
            "counter_examples_or_negative_results": "Pretrained GPT-2 without finetuning cannot solve the task (very low accuracy) though it shows weak attention signals; when the task is made much harder (m=64, k up to 32), GPT-2_FT accuracy degrades and S_P2 (ability to pick k-th among top-k) collapses earlier than S_P1 — i.e., the model can often still find top-k elements (S_P1), but fails the selection step (S_P2) when beyond capacity.",
            "uuid": "e4949.2",
            "source_info": {
                "paper_title": "Towards a Mechanistic Interpretation of Multi-Step Reasoning Capabilities of Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Chain-of-Thought (CoT) prompting",
            "name_full": "Chain-of-thought prompting elicits reasoning in large language models",
            "brief_description": "A prompting technique (Chain-of-Thought) that elicits multi-step intermediate reasoning steps from language models by providing exemplars that include step-by-step chains of reasoning; mentioned in this paper as relevant but not analyzed by MechanisticProbe.",
            "citation_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "",
            "reasoning_method_name": "Chain-of-thought prompting",
            "reasoning_method_type": "other",
            "reasoning_method_description": "Prompting method that produces explicit intermediate reasoning steps (chains) in the model's generated output by including chain-of-thought exemplars in the prompt; often used for hard multi-step problems and sometimes combined with sampling/ensembling methods.",
            "task_name": "General hard multi-step reasoning tasks (not directly evaluated in this paper)",
            "task_description": "Problems for which single-token classification is insufficient and explicit intermediate reasoning (textual chain-of-thought) is used to improve model performance.",
            "performance": "Not evaluated in this paper; CoT is cited as relevant prior work and as an approach used in other work for tasks that are difficult to solve without eliciting intermediate reasoning.",
            "comparison_with_other_method": false,
            "performance_other_method": null,
            "key_findings": "The paper notes that some recent reasoning tasks require chain-of-thought prompting and that such auto-regressive chain-of-thought settings are left for future mechanistic analysis; MechanisticProbe in this paper focuses on single-token prediction formulations rather than explicit CoT outputs.",
            "counter_examples_or_negative_results": "No empirical CoT results in this paper; authors acknowledge as limitation that analysis does not cover chain-of-thought style reasoning and leave it as future work.",
            "uuid": "e4949.3",
            "source_info": {
                "paper_title": "Towards a Mechanistic Interpretation of Multi-Step Reasoning Capabilities of Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Selection-Inference",
            "name_full": "Selection-inference (selection + inference decomposition)",
            "brief_description": "A two-stage reasoning framework (selection of relevant facts, then inference) for interpretable logical reasoning with LMs; cited as related work motivating the two-stage probe decomposition (select useful statements then predict reasoning tree).",
            "citation_title": "Selection-inference: Exploiting large language models for interpretable logical reasoning",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "",
            "reasoning_method_name": "Selection-inference (decomposed reasoning)",
            "reasoning_method_type": "other",
            "reasoning_method_description": "Decomposes reasoning into a selection step (identify relevant premises) and an inference step (derive conclusion), which can be implemented with separate prompting or models; this decomposition motivates the paper's two-probe design (P1 select useful statements; P2 predict heights/structure).",
            "task_name": "General logical reasoning / theorem-proving style problems",
            "task_description": "Tasks requiring selection of relevant statements from a context and then applying inference rules to answer a query.",
            "performance": "Not directly evaluated in this paper (cited as prior work); the paper implements a related decomposition in its mechanistic probing and shows attention contains signals for both selection and subsequent inference steps.",
            "comparison_with_other_method": false,
            "performance_other_method": null,
            "key_findings": "The paper adopts a similar two-stage decomposition and finds that attention patterns often reflect a two-stage process (early layers select useful statements, later layers perform inference), giving mechanistic credence to selection-inference style decompositions.",
            "counter_examples_or_negative_results": "No direct experimental comparison to selection-inference implementations; the paper only cites it as related conceptual work and uses an analogous probe decomposition.",
            "uuid": "e4949.4",
            "source_info": {
                "paper_title": "Towards a Mechanistic Interpretation of Multi-Step Reasoning Capabilities of Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Selection-inference: Exploiting large language models for interpretable logical reasoning",
            "rating": 2
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2
        },
        {
            "paper_title": "Large language models are in-context semantic reasoners rather than symbolic reasoners",
            "rating": 1
        }
    ],
    "cost": 0.01724425,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Towards a Mechanistic Interpretation of Multi-Step Reasoning Capabilities of Language Models</h1>
<p>Yifan Hou ${ }^{1}$, Jiaoda $\mathbf{L i}^{1}$, Yu Fei ${ }^{2}$, Alessandro Stolfo ${ }^{1}$, Wangchunshu Zhou ${ }^{3}$, Guangtao Zeng ${ }^{4}$, Antoine Bosselut ${ }^{5}$, Mrinmaya Sachan ${ }^{1}$<br>${ }^{1}$ ETH Zürich, ${ }^{2}$ UC Irvine, ${ }^{3}$ AIWaves, ${ }^{4}$ SUTD, ${ }^{5}$ EPFL<br>${ }^{1}$ {yifan.hou, jiaoda.li, alessandro.stolfo, mrinmaya.sachan}@inf.ethz.ch, ${ }^{2}$ yu.fei@uci.edu, ${ }^{3}$ chunshu@aiwaves.cn, ${ }^{4}$ guangtao_zeng@mymail.sutd.edu.sg, ${ }^{5}$ antoine.bosselut@epfl.ch</p>
<h4>Abstract</h4>
<p>Recent work has shown that language models (LMs) have strong multi-step (i.e., procedural) reasoning capabilities. However, it is unclear whether LMs perform these tasks by cheating with answers memorized from pretraining corpus, or, via a multi-step reasoning mechanism. In this paper, we try to answer this question by exploring a mechanistic interpretation of LMs for multi-step reasoning tasks. Concretely, we hypothesize that the LM implicitly embeds a reasoning tree resembling the correct reasoning process within it. We test this hypothesis by introducing a new probing approach (called MechanisticProbe) that recovers the reasoning tree from the model's attention patterns. We use our probe to analyze two LMs: GPT2 on a synthetic task ( $k$-th smallest element), and LLaMA on two simple language-based reasoning tasks (ProofWriter \&amp; AI2 Reasoning Challenge). We show that MechanisticProbe is able to detect the information of the reasoning tree from the model's attentions for most examples, suggesting that the LM indeed is going through a process of multi-step reasoning within its architecture in many cases. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Large language models (LMs) have shown impressive capabilities of solving complex reasoning problems (Brown et al., 2020; Touvron et al., 2023). Yet, what is the underlying "thought process" of these models is still unclear (Figure 1). Do they cheat with shortcuts memorized from pretraining corpus (Carlini et al., 2022; Razeghi et al., 2022; Tang et al., 2023)? Or, do they follow a rigorous reasoning process and solve the problem procedurally (Wei et al., 2022; Kojima et al., 2022)? Answering this question is not only critical to our understanding of these models but is also critical for the development of next-generation faithful</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An example showing how LMs solve reasoning tasks. It is unclear if LMs produce the answer by reasoning or by recalling memorized information.
language-based reasoners (Creswell and Shanahan, 2022; Creswell et al., 2022; Chen et al., 2023).</p>
<p>A recent line of work tests the behavior of LMs by designing input-output reasoning examples (Zhang et al., 2023; Dziri et al., 2023). However, it is expensive and challenging to construct such high-quality examples, making it hard to generalize these analyses to other tasks/models. Another line of work, mechanistic interpretability (Merullo et al., 2023; Wu et al., 2023; Nanda et al., 2023; Stolfo et al., 2023; Bayazit et al., 2023), directly analyzes the parameters of LMs, which can be easily extended to different tasks. Inspired by recent work (Abnar and Zuidema, 2020; Voita et al., 2019; Manning et al., 2020; Murty et al., 2023) that uses attention patterns for linguistic phenomena prediction, we propose an attention-based mechanistic interpretation to expose how LMs perform multi-step reasoning tasks (Dong et al., 2021).</p>
<p>We assume that the reasoning process for answering a multi-step reasoning question can be represented as a reasoning tree (Figure 2). Then, we investigate if the LM implicitly infers such a tree when answering the question. To achieve this, we designed a probe model, MechanisticProbe, that recovers the reasoning tree from the LM's attention patterns. To simplify the probing problem and gain a more fine-grained understanding, we decompose the problem of discovering reasoning trees into two</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" />Figure 2: Illustration of our MechanisticProbe with one example from each of the three reasoning tasks considered in this work. We are given a number of input statements: $\mathcal{S}={S_{1},S_{2},...,}$ and a question: $Q$. MechanisticProbe recovers the reasoning tree describing the ground-truth reasoning process to answer the question. MechanisticProbe works in two stages: in the first stage, MechanisticProbe detects if the LM can select the set of useful statements required in reasoning, and then in the second stage, MechanisticProbe detects if the LM can predict the reasoning tree given the useful statements.</p>
<p>subproblems: 1) identifying the necessary nodes in the reasoning tree; 2) inferring the heights of identified nodes. We design our probe using two simple non-parametric classifiers for the two subproblems. Achieving high probing scores indicates that the LM captures the reasoning tree well.</p>
<p>We conduct experiments with GPT-2 <em>Radford et al. (2019)</em> on a synthetic task (finding the $k$-th smallest number in a sequence of numbers) and with LLaMA <em>Touvron et al. (2023)</em> on two natural language reasoning tasks: ProofWriter <em>Tafjord et al. (2021)</em> and AI2 Reasoning Challenge (i.e., ARC: <em>Clark et al. (2018)</em>). For most examples, we successfully detect reasoning trees from attentions of (finetuned) GPT-2 and (few-shot &amp; finetuned) LLaMA using MechanisticProbe. We also observe that LMs find useful statements immediately at the bottom layers and then do the subsequent reasoning step by step (§4).</p>
<p>To validate the influence on the LM’s predictions of the attention mechanisms that we identify, we conduct additional analyses. First, we prune the attention heads identified by MechanisticProbe and observe a significant accuracy degradation (§5). Then, we investigate the correlation between our probing scores and the LM’s performance and robustness (§6). Our findings suggest that LMs exhibit better prediction accuracy and tolerance to noise on examples with higher probing scores. Such observations highlight the significance of accurately capturing the reasoning process for the efficacy and robustness of LMs.</p>
<h2>2 Reasoning with LM</h2>
<p>In this section, we formalize the reasoning task and introduce the three tasks used in our analysis: $k$-th smallest element, ProofWriter, and ARC.</p>
<h3>2.1 Reasoning Formulation</h3>
<p>In our work, the LM is asked to answer a question $Q$ given a set of statements denoted by $\mathcal{S}=$ ${S_{1},S_{2},...}$. Some of these statements may not be useful for answering $Q$. To obtain the answer, the LM should perform reasoning using the statements in multiple steps. We assume that this process can be represented by a reasoning tree $G$. We provide specific examples in Figure 2 for the three reasoning tasks used in our analysis. This is a very broad formulation and includes settings such as theorem proving <em>Loveland (1980)</em> where the statements could be facts or rules. In our analyses, we study the tasks described below.</p>
<h3>2.2 Reasoning Tasks</h3>
<p>$k$-th smallest element. In this task, given a list of $m$ numbers ($m=16$ by default) in any order, the LM is asked to predict (i.e., generate) the $k$-th smallest number in the list. For simplicity, we only consider numbers that can be encoded as one token with GPT-2’s tokenizer. We select $m$ numbers randomly among them to construct the input number list. The reasoning trees have a depth of 1 (Figure 2 left). The root node is the $k$-th smallest number and the leaf nodes are top-$k$ numbers.</p>
<p>For this task, we select GPT-2 <em>Radford et al. (2019)</em> as the LM for the analysis. We randomly generate training data to finetune GPT-2 and ensure that the test accuracy on the reasoning task is larger than $90\%$. For each $k$, we finetune an independent GPT-2 model. More details about finetuning (e.g., hyperparameters) are in Appendix B.1.</p>
<p>ProofWriter. The ProofWriter dataset <em>Tafjord et al. (2021)</em> contains theorem-proving problems.</p>
<h4></h4>
<p>In this task, given a set of statements (verbalized rules and facts) and a question, the LM is asked to determine if the question statement is true or false. Annotations of reasoning trees $G$ are also provided in the dataset. Again, each tree has only one node at any height larger than 0. Thus, knowing the node height is sufficient to recover $G$. To simplify our analysis, we remove examples annotated with multiple reasoning trees. Details are in Appendix C.3. Furthermore, to avoid tree ambiguity (Appendix C.4), we only keep examples with reasoning trees of depth upto 1, which account for $70\%$ of the data in ProofWriter.</p>
<p>AI2 Reasoning Challenge (ARC). The ARC dataset contains multiple-choice questions from middle-school science exams <em>Clark et al. (2018)</em>. However, the original dataset does not have reasoning tree annotations. Thus, we consider a subset of the dataset provided by <em>Ribeiro et al. (2023)</em>, which annotates around 1000 examples. Considering the limited number of examples, we do not include analysis of finetuned LMs and mainly focus on the in-context learning setting. More details about the dataset are in Appendix C.3.</p>
<p>For both ProofWriter and ARC tasks, we select LLaMA (7B) <em>Touvron et al. (2023)</em> as the LM for analysis. The tasks are formalized as classifications: predicting the answer token (e.g., true or false for ProofWriter). We compare two settings: LLaMA with 4-shot in-context learning setting, and LLaMA finetuned with supervised signal (i.e., LLaMA_{FT}). We partially finetune LLaMA on attention parameters. Implementation details about in-context learning and finetuning on LLaMA can be found in Appendix C.1.</p>
<h2>3 MechanisticProbe</h2>
<p>In this section, we introduce MechanisticProbe: a probing approach that analyzes how LMs solve multi-step reasoning tasks by recovering reasoning trees from the attentions of LMs.</p>
<h3>3.1 Problem Formulation</h3>
<p>Our goal is to shed light on how LMs solve procedural reasoning tasks. Formally, given an LM with $L$ layers and $H$ attention heads, we assume that the LM can handle the multi-step reasoning task with sufficiently high accuracy.</p>
<p>[table]10] Note that most reasoning tasks with LMs can be formalized as the single-token prediction format (e.g., multi-choice question answering).</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: An illustration of the attention mechanism in a LM. The example sentence here is: Zurich is located in Switzerland. Token $t_{|T|}$ is the last token used for the prediction of the next token $t_{|T|+1}$. The arrows here show the attentions, where the red arrows denote the attentions to the last token.</p>
<p>Let us consider the simplest of the three tasks ($k$-th smallest element). Each statement $S_{i}$ here is encoded as one token $t_{i}$. The input text (containing all statements in $\mathcal{S}$) to the LM comprises a set of tokens as $T=(t_{1}, t_{2},...)$. We denote the hidden representations of the token $t_{i}$ at layer $l$ by $\boldsymbol{z}<em i="i">{i}^{l}$. We further denote the attention of head $h$ between $\boldsymbol{z}</em>(l,h)|1\leq l\leq L;1\leq h\leq H}$.}^{l+1}$ and $\boldsymbol{z}_{j}^{l}$ as $\boldsymbol{A}(l,h)[i,j]$. As shown in Figure 3, the attention matrix at layer $l$ of head $h$ is denoted as $\boldsymbol{A}(l,h)$, the lower triangular matrix. The overall attention matrix then can be denoted by $\boldsymbol{A}={\boldsymbol{A</p>
<p>Our probing task is to detect $G$ from $\boldsymbol{A}$, i.e. modeling $P(G|\boldsymbol{A})$. However, note that the size of $\boldsymbol{A}$ is very large $-L \times H \times|T|^{2}$, and it could contain many redundant features. For example, if the number of tokens in the input is 100, the attention matrix $\boldsymbol{A}$ for LLaMA contains millions of attention weights. It is impossible to probe the information directly from $\boldsymbol{A}$. In addition, the tree prediction task is difficult <em>Hou and Sachan (2021)</em> and we want our probe to be simple <em>Belinkov (2022)</em> as we want it to provide reliable interpretation about the LM rather than learn the task itself. Therefore, we introduce two ways to simplify attentions and the probing task design.</p>
<h3>3.2 Simplification of $\boldsymbol{A}$</h3>
<p>In general, we propose two ways to simplify $\boldsymbol{A}$. Considering that LLaMA is a large LM, we propose extra two ways to further reduce the number of considered attention weights in $\boldsymbol{A}$ for it.</p>
<p>Focusing on the last token. For causal LMs, the representation of the last token in the last layer $\boldsymbol{z}_{|T|}^{L}$</p>
<p>[table]10] Note that in this work we only consider causal LMs.</p>
<p>is used to predict the next token $t_{|T|+1}$ (Figure 3). Thus, we simplify $\boldsymbol{A}$ by focusing on the attentions on the last input token, denoted as $\boldsymbol{A}<em _simp="{simp" _text="\text">{\text {simp }}$. This reduces the size of attentions to $L \times H \times|T|$. Findings in previous works support that $\boldsymbol{A}</em>$ (Appendix B.4).}}$ is sufficient to reveal the focus of the prediction token [brunner2020towards; Geva2023]. In our experimental setup, we also find that analysis on $\boldsymbol{A}_{\text {simp }}$ gives similar results to that on $\boldsymbol{A</p>
<p>Attention head pooling. Many existing attention-based analysis methods use pooling (e.g., mean pooling or max pooling) on attention heads for simplicity [abnar2020attention; Manning2020; Murty2023]. We follow this idea and take the mean value across all attention heads for our analysis. Then, the size of $\boldsymbol{A}_{\text {simp }}$ is further reduced to $L \times|T|$.</p>
<p>Ignoring attention weights within the statement (LLaMA). For LLaMA on the two natural language reasoning tasks, a statement $S_{i}$ could contain multiple tokens, i.e., $|T|&gt;&gt;|\mathcal{S}|$. Thus, the size of $\boldsymbol{A}<em _simp="{simp" _text="\text">{\text {simp }}$ can still be large. To further simplify $\boldsymbol{A}</em>$}}$ under this setting, we regard all tokens of a statement as a hypernode. That is, we ignore attentions within hypernodes and focus on attentions across hypernodes. As shown in Figure 4, we can get $\boldsymbol{A}_{\text {simp }}^{\text {cross }}$ via mean pooling on all tokens of a statement and max pooling on all tokens of $Q$ as: ${ }^{5</p>
<p>$$
\boldsymbol{A}<em t__j_prime="t_{j^{\prime">{\text {simp }}^{\text {cross }}(l, h)[i]=\max </em>\left(\max }} \in Q<em i_prime="i^{\prime">{t</em>\right]\right)\right)
$$}} \in S_{i}}\left(\boldsymbol{A}(l, h)\left[i^{\prime}, j^{\prime</p>
<p>The size of simplified cross-hypernode attention matrix $\boldsymbol{A}_{\text {simp }}^{\text {cross }}$ is further reduced to $L \times(|\mathcal{S}|+1)$.</p>
<p>Pruning layers (LLaMA). Large LMs (e.g., LLaMA) are very deep (i.e., $L$ is large), and are pretrained for a large number of tasks. Thus, they have many redundant parameters for performing the reasoning task. Inspired by rogers2020learning, we prune the useless layers of LLaMA for the reasoning task and probe attentions of the remaining layers. Specifically, we keep a minimum number of layers that maintain the LM's performance on a held-out development set, and deploy our analysis on attentions of these layers. For 4 -shot LLaMA, 13/15 (out of 32) layers are removed for</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: An illustration of the way to extend our attention simplification method. In this example, $S_{1}$ and $Q$ are both composed of 2 tokens. Thus, there are $2 \times 2$ attentions. We mean pool all the tokens within the statements (remaining $1 \times 2$ attentions), and max pool all the tokens in the question (remaining $1 \times 1$ attention).</p>
<p>ProofWriter and ARC respectively. For finetuned LLaMA, 18 (out of 32) layers are removed for ProofWriter. More details about the attention pruning can be found in Appendix C.2.</p>
<h3>3.3 Simplification of the Probing Task</h3>
<p>We simplify the problem of predicting $G$ by breaking it down into two classification problems: classifying if the statement is useful or not, and classifying the height of the statement in the tree.</p>
<p>$$
P\left(G \mid \boldsymbol{A}<em _simp="{simp" _text="\text">{\text {simp }}\right)=P\left(V \mid \boldsymbol{A}</em>\right)
$$}}\right) \cdot P\left(G \mid V, \boldsymbol{A}_{\text {simp }</p>
<p>Here, $V$ is the set of nodes in $G . P\left(V \mid \boldsymbol{A}<em _simp="{simp" _text="\text">{\text {simp }}\right)$ (binary classification) measures if LMs can select useful statements from the input based on attentions, revealing if LMs correctly focus on useful statements. Given the set of nodes in $G$, the second probing task is to decide the reasoning tree. We model $P\left(G \mid V, \boldsymbol{A}</em>$}}\right)$ as the multiclass classification for predicting the height of each node. For example, when the reasoning tree depth is 2 , the height label set is ${0,1,2}$. Note that for the three reasoning tasks considered by us, $G$ is always a simple tree that has multiple leaf nodes but one intermediate node at each height. ${ }^{6</p>
<h3>3.4 Probing Score</h3>
<p>To limit the amount of information the probe learns about the probing task, we use a non-parametric classifier: k-nearest neighbors ( kNN ) to perform the two classification tasks. We use $S_{\mathrm{FI}}\left(V \mid \boldsymbol{A}<em _mathrm_FI="\mathrm{FI">{\text {simp }}\right)$ and $S</em>\right)$ to denote their F1-Macro scores. To better interpret the probing results, we}}\left(G \mid V, \boldsymbol{A}_{\text {simp }</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>introduce a random probe baseline as the control task <em>Hewitt and Liang (2019)</em> and instead of looking at absolute probing values, we interpret the probing scores compared to the score of the random baseline. The probe scores are defined as:</p>
<p>$S_{\mathrm{P}1}$ $=\frac{S_{\mathrm{F}1}(V|\boldsymbol{A}<em _mathrm_F="\mathrm{F">{\text{simp}})-S</em>}1}(V|\boldsymbol{A<em _mathrm_F="\mathrm{F">{\text{rand}})}{1-S</em>}1}(V|\boldsymbol{A<em _mathrm_P="\mathrm{P">{\text{rand}})},$ (1)
$S</em>}2}$ $=\frac{S_{\mathrm{F}1}(G|V,\boldsymbol{A<em _mathrm_F="\mathrm{F">{\text{simp}})-S</em>}1}(G|V,\boldsymbol{A<em _mathrm_F="\mathrm{F">{\text{rand}})}{1-S</em>,$ (2)}1}(G|V,\boldsymbol{A}_{\text{rand}})</p>
<p>where $\boldsymbol{A}<em _mathrm_P="\mathrm{P">{\text{rand}}$ is the simplified attention matrix given by a randomly initialized LM. After normalization, we have the range of our probing scores: $S</em> \in[0,1]$. Small values mean that there is no useful information about $G$ in attention, and large values mean that the attention patterns indeed contain much information about $G$.}1}, S_{\mathrm{P}2</p>
<h2>4 Mechanistic Probing of LMs</h2>
<p>We use the probe to analyze how LMs perform the reasoning tasks. We first verify the usefulness of attentions in understanding the reasoning process of LMs by visualizing $\boldsymbol{A}<em _simp="{simp" _text="\text">{\text {simp }}(\S 4.1)$. Then, we use the probe to quantify the information of $G$ contained in $\boldsymbol{A}</em>(\S 4.2)$. Finally, we report layer-wise probing results to understand if LMs are reasoning procedurally across their architecture (§4.3).}</p>
<h3>4.1 Attention Visualization</h3>
<p>We first analyze $\boldsymbol{A}<em _simp="{simp" _text="\text">{\text {simp }}$ on the $k$-th smallest element task via visualizations. We permute $\boldsymbol{A}</em>}}$ arranging the numbers in ascending order and denote this permulation as $\pi\left(\boldsymbol{A<em _simp="{simp" _text="\text">{\text {simp }}\right)$. We show visualizations of $\mathbb{E}\left[\pi\left(\boldsymbol{A}</em>$.}}\right)\right]$ on the test data in Figure 5. We observe that when GPT-2 tries to find the $k$-th smallest number, the prediction token first focuses on top- $k$ numbers in the list with bottom layers. Then, the correct answer is found in the top layers. These findings suggest that GPT-2 solves the reasoning task in two steps following the reasoning tree $G$. We further provide empirical evidence in Appendix B. 4 to show that analysis on $\boldsymbol{A}_{\text {simp }}$ gives similar conclusions compared to that on $\boldsymbol{A</p>
<h3>4.2 Probing Scores</h3>
<p>Next, we use our MechanisticProbe to quantify the information of $G$ contained in $\boldsymbol{A}_{\text {simp }}$ (GPT-2)</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>or $\boldsymbol{A}<em _simp="{simp" _text="\text">{\text {simp }}^{\text {cross }}$ (LLaMA).
GPT-2 on $k$-th smallest element $\left(\boldsymbol{A}</em>}}\right)$. We consider two versions of GPT-2: a pretrained version and a finetuned version (GPT-2 ${ <em _mathrm_F="\mathrm{F">{\text {FT }}$ ). We report our two probing scores (Eq. 1 and Eq. 2) with different $k$ in Table 1. The unnormalized F1-macro scores (i.e., $S</em>} 1}\left(V \mid \boldsymbol{A<em _mathrm_F="\mathrm{F">{\text {simp }}\right)$ and $S</em>\right)$ ) can be found in Appendix B.2.} 1}\left(G \mid V, \boldsymbol{A}_{\text {simp }</p>
<p>Table 1: Probing scores for GPT-2 models on synthetic reasoning tasks with different $k$. We also provide the test accuracy of these finetuned models for reference. Note that when $k=1$, the depth of $G$ is 0 , and $S_{\mathrm{F} 1}\left(G \mid V, \boldsymbol{A}<em _FT="{FT" _text="\text">{\text {simp }}\right)$ is always equal to 1 . Thus, we leave these results blank. Results show that we can clearly detect $G$ from attentions of GPT-2 ${ }</em>$.}</p>
<table>
<thead>
<tr>
<th style="text-align: center;">$k$</th>
<th style="text-align: center;">Test Acc.</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">$S_{\mathrm{F} 1}$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">$S_{\mathrm{P} 2}$</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-2</td>
<td style="text-align: center;">GPT-2 ${ }_{\text {FT }}$</td>
<td style="text-align: center;">GPT-2</td>
<td style="text-align: center;">GPT-2 ${ }_{\text {FT }}$</td>
<td style="text-align: center;">GPT-2</td>
<td style="text-align: center;">GPT-2 ${ }_{\text {FT }}$</td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">99.63</td>
<td style="text-align: center;">7.09</td>
<td style="text-align: center;">92.94</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">99.42</td>
<td style="text-align: center;">5.88</td>
<td style="text-align: center;">93.71</td>
<td style="text-align: center;">20.65</td>
<td style="text-align: center;">98.05</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">98.23</td>
<td style="text-align: center;">13.48</td>
<td style="text-align: center;">91.62</td>
<td style="text-align: center;">13.59</td>
<td style="text-align: center;">95.76</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">94.89</td>
<td style="text-align: center;">20.73</td>
<td style="text-align: center;">88.34</td>
<td style="text-align: center;">8.04</td>
<td style="text-align: center;">92.52</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">93.38</td>
<td style="text-align: center;">24.15</td>
<td style="text-align: center;">87.54</td>
<td style="text-align: center;">13.81</td>
<td style="text-align: center;">88.17</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">92.62</td>
<td style="text-align: center;">24.82</td>
<td style="text-align: center;">86.89</td>
<td style="text-align: center;">11.18</td>
<td style="text-align: center;">95.06</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">91.37</td>
<td style="text-align: center;">25.87</td>
<td style="text-align: center;">76.61</td>
<td style="text-align: center;">$&lt;1$</td>
<td style="text-align: center;">91.56</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">91.29</td>
<td style="text-align: center;">22.30</td>
<td style="text-align: center;">78.73</td>
<td style="text-align: center;">1.06</td>
<td style="text-align: center;">93.93</td>
</tr>
</tbody>
</table>
<p>These results show that without finetuning, GPT2 is incapable of solving the reasoning task, and we can detect little information about $G$ from GPT2's attentions. ${ }^{8}$ However, for GPT-2 ${ }<em _simp="{simp" _text="\text">{\text {FT }}$, which has high test accuracy on the reasoning task, MechanisticProbe can easily recover the reasoning tree $G$ from $\boldsymbol{A}</em>$ solves this synthetic reasoning task following $G$ in Figure 2 (left).}}$. This further confirms that GPT-2 ${ }_{\text {FT }</p>
<p>LLaMA on ProofWriter and ARC $\left(\boldsymbol{A}<em _mathrm_P="\mathrm{P">{\text {simp }}^{\text {cross }}\right)$. Similarly, we use MechanisticProbe to probe LLaMA on the two natural language reasoning tasks. For efficiency, we randomly sampled 1024 examples from the test sets for our analysis. When depth $=0$, LLaMA only needs to find out the useful statements for reasoning $\left(S</em>\right)$. When depth $=1$, LLaMA needs to determine the next reasoning step.} 1</p>
<p>Probing results with different numbers of input statements (i.e., $|\mathcal{S}|$ ) are in Table 2. Unnormalized classification scores can be found in Appendix C.5. It can be observed that all the probing scores are much larger than 0 , meaning that the attentions indeed contain information about $G$.</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Visualization of E[π(<em>A</em>simp)] for <em>k</em> ranging from 1 to 8. Note that <em>m</em> = 16 here, and when <em>k</em> &gt; 8, the reasoning task becomes finding <em>k</em>-th largest number. The visualizations are roughly the same without loss of generality. The <em>x</em>-axis represents the layer index and the <em>y</em>-axis represents the size (rank) of the number. The value of each cube is the attention weight (better view in color). Results show that layers of finetuned GPT-2 have different functions: bottom layers focusing on top-<em>k</em> numbers and top layers focusing on <em>k</em>-smallest numbers.</p>
<p>Table 2: Probing results for LLaMA on natural language reasoning. Note that when depth = 0, <em>S</em>F1(<em>G</em>|<em>V</em>, <em>A</em>simp) is always equal to 1. When depth = 1 and |<em>S</em>F1 = 2, all input statements are useful. <em>S</em>F1(<em>V</em>|<em>A</em>simp) is always equal to 1. We leave these results blank. Regarding ARC, we only report the scores under the 4-shot setting. Results show that attentions of LLaMA contain some information about selecting useful statements for ProofWriter but much information for ARC. Regarding determining reasoning steps, attentions of LLaMA contain much information for both ProofWriter and ARC.</p>
<table>
<thead>
<tr>
<th>ProofWriter</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Depth</td>
<td></td>
<td><em>S</em>F1</td>
<td>Test Acc.</td>
<td><em>S</em>F1</td>
<td><em>S</em>F2</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>LLaMA</td>
<td>LLaMAFT</td>
<td>LLaMA</td>
<td>LLaMAFT</td>
<td>LLaMA</td>
<td>LLaMAFT</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>0</td>
<td>All</td>
<td>81.72</td>
<td></td>
<td></td>
<td>57.21</td>
<td>49.08</td>
<td>-</td>
<td>-</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>2</td>
<td>94.81</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>100</td>
<td>100</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>4</td>
<td>95.12</td>
<td></td>
<td></td>
<td>44.83</td>
<td>48.14</td>
<td>93.34</td>
<td>96.22</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>1</td>
<td>8</td>
<td>92.19</td>
<td></td>
<td></td>
<td>27.39</td>
<td>40.09</td>
<td>83.75</td>
<td>96.44</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>12</td>
<td>90.53</td>
<td></td>
<td></td>
<td>26.23</td>
<td>32.70</td>
<td>77.58</td>
<td>93.45</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>16</td>
<td>89.55</td>
<td></td>
<td></td>
<td>17.18</td>
<td>21.07</td>
<td>77.85</td>
<td>89.31</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>20</td>
<td>88.38</td>
<td></td>
<td></td>
<td>11.10</td>
<td>15.84</td>
<td>79.99</td>
<td>94.11</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>24</td>
<td>86.13</td>
<td></td>
<td></td>
<td>9.39</td>
<td>17.33</td>
<td>80.32</td>
<td>94.42</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>ARC</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>1</td>
<td>-</td>
<td>56.32</td>
<td></td>
<td></td>
<td>97.49</td>
<td>-</td>
<td>61.73</td>
<td>-</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2</td>
<td>-</td>
<td>55.41</td>
<td></td>
<td></td>
<td>96.49</td>
<td>-</td>
<td>53.40</td>
<td>-</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Looking at <em>S</em>P1 on ProofWriter, when the number of input statements is small, MechanisticProbe can clearly decide the useful statements based on attentions. However, it becomes harder when there are more useless statements (i.e., |<em>S</em>| is large). However, for ARC, our probe can always detect useful statements from attentions easily.</p>
<p>Looking at <em>S</em>P2, we notice that our probe can easily determine the height of useful statements based on attentions on both ProofWriter and ARC datasets. By comparing the probing scores on ProofWriter, we find that LLaMAFT always has higher probing scores than 4-shot LLaMA, implying that finetuning with supervised signals makes the LM to follow the reasoning tree <em>G</em> more clearly. We also notice that 4-shot LLaMA is affected more by the number of useless statements than LLaMAFT, indicating a lack of robustness of reasoning in the few-shot setting.</p>
<h3>4.3 Layer-wise Probing</h3>
<p>After showing that LMs perform reasoning following oracle reasoning trees, we investigate how this reasoning happens inside the LM layer-by-layer.</p>
<p><strong>GPT-2 on <em>k</em>-th smallest element.</strong> In order to use our probe layer-by-layer, we define the set of simplified attentions before layer <em>l</em> as <em>A</em>simp(: <em>l</em>) = {<em>A</em>simp(<em>l</em>′)|<em>l</em>′ ≤ <em>l</em>}. Then, we report our probing scores <em>S</em>F1(<em>l</em>) and <em>S</em>F2(<em>l</em>) on these partial attentions from layer 1 to layer 12. We denote these layer-wise probing scores as <em>S</em>F1(<em>V</em>|<em>A</em>simp(: <em>l</em>)) and <em>S</em>F1(<em>G</em>|<em>V</em>, <em>A</em>simp(: <em>l</em>)).</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" />
(a) $k=1$
(b) $k=2$
(c) $k=3$
<img alt="img-6.jpeg" src="img-6.jpeg" />
(d) $k=4$
(e) $k=5$
(f) $k=6$
<img alt="img-7.jpeg" src="img-7.jpeg" />
(g) $k=7$
(h) $k=8$</p>
<p>Figure 6: Two probing scores on partial attentions across 12 layers. The $x$-axis represents the layer index of GPT$2_{\mathrm{FT}}$, and the $y$-axis represents the probing scores. Similarly, we test different $k$ from 1 to 8 ( $m=16$ by default). Results show that each layer of GPT-2 ${ }_{\mathrm{FT}}$ focuses on different steps of the synthetic reasoning task.</p>
<p>Figure 6 shows the layer-wise probing scores for each $k$ for GPT-2 ${ }<em _mathrm_P="\mathrm{P">{\text {FT }}$ models. Observing $S</em>} 1}$, i.e., selecting top- $k$ numbers, we notice that GPT$2_{\mathrm{FT}}$ quickly achieves high scores in initial layers and then, $S_{\mathrm{P} 1}$ increases gradually. Observing $S_{\mathrm{P} 2}$, i.e., selecting the $k$-th smallest number from top- $k$ numbers, we notice that GPT-2 ${ <em _mathrm_FT="\mathrm{FT">{\mathrm{FT}}$ does not achieve high scores until layer 10. This reveals how GPT$2</em>$ solves the task internally. The bottom layers find out the top- $k$ numbers, and the top layers select the $k$-th smallest number among them. Results in Figure 5 also support the above findings.}</p>
<p>LLaMA on ProofWriter and ARC ( $\boldsymbol{A}<em _mathrm_P="\mathrm{P">{\text {simp }}^{\text {cross }}$ ). Similarly, we report layer-wise probing scores $S</em>(: l)\right)$ for nodes with height $=0$ and height $=1$ to show if the statement at height 0 is processed in LLaMA before the statement at height 1 .} 1}(l)$ and $S_{\mathrm{P} 2}(l)$ for LLaMA under the 4 -shot setting. We further report $S_{\mathrm{F} 1}\left(V_{\text {height }} \mid \boldsymbol{A}_{\text {simp }}^{\text {cross }</p>
<p>The layer-wise probing results for ProofWriter are shown in Figure 7(a-e). We find that similar to GPT-2, probing results for 4 -shot LLaMA reach a plateau at an early layer (layer 2 for $S_{\mathrm{P} 1}(l)$ ), and at middle layers for $S_{\mathrm{P} 2}(l)$. This obserrvation holds as we vary $|\mathcal{S}|$ from 4 to 20 . This shows that similar to GPT-2, LLaMA first tries to identify useful statements in the bottom layers.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 7: Layer-wise probing results on ProofWriter. $x$-axis represents the number of layers used for probing. We can find that $S_{\mathrm{P} 1}(l)$ scores reach the plateau quickly and $S_{\mathrm{P} 2}(l)$ scores increase smoothly till the middle layers. This indicates that useful statement selection is mainly finished in the bottom layers and the reasoning step is decided in the middle layers. Results of $S_{\mathrm{F} 1}\left(V_{\text {height }} \mid \boldsymbol{A}_{\text {simp }}^{\text {cross }}(: l)\right)$ shows that statements at height 0 are identified by LLaMA in bottom layers, and statement at height 1 are identified later in middle layers.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 8: Layer-wise probing results on ARC (depth=1). Similar to that of ProofWriter, we can find that $S_{\mathrm{P} 1}(l)$ scores reach the plateau quickly and $S_{\mathrm{P} 2}(l)$ scores increase smoothly till the middle layers.</p>
<p>Then, it focuses on predicting the reasoning steps given the useful statements. Figure 7(f) shows how $S_{\mathrm{F} 1}\left(V_{\text {height }} \mid \boldsymbol{A}_{\text {simp }}^{\text {cross }}(: l)\right)$ varies across layers. LLaMA identifies the useful statements from all the input statements (height 0 ) immediately in layer 2. Then, LLaMA gradually focuses on these statements and builds the next layer of the reasoning tree (height 1) in the middle layers.</p>
<p>The layer-wise probing results for ARC are in Figure 8. Similar to ProofWriter, the $S_{\mathrm{P} 1}(l)$ scores on ARC for 4 -shot LLaMA reach a plateau at an early layer (layer 2), and at middle layers for $S_{\mathrm{P} 2}(l)$. This also shows that LLaMA tries to identify useful statements in the bottom layers and focuses on the next reasoning steps in the higher layers.</p>
<p>5 Do LMs Reason Using $\boldsymbol{A}_{\text{simp}}$?</p>
<p>Our analysis so far shows that LMs encode the reasoning trees in their attentions. However, as argued by <em>Ravichander et al. (2021); Lasri et al. (2022); Elazar et al. (2021)</em>, this information might be accidentally encoded but not actually used by the LM for inference. Thus, we design a causal analysis for GPT-2 on the $k$-th smallest element task to show that LMs indeed perform reasoning following the reasoning tree. The key idea is to prove that the attention heads that contribute to $\boldsymbol{A}<em _text_simp="\text{simp">{\text{simp}}$ are useful to solve the reasoning task, while those heads that are irrelevant (i.e., independent) to $\boldsymbol{A}</em>$ are not useful in the reasoning task.}</p>
<p>Intuitively, for the $k$-th smallest element task, attention heads that are sensitive to the number size (rank) are useful, while heads that are sensitive to the input position are not useful. Therefore, for each head, we calculate the attention distribution on the test data to see if the head specially focuses on numbers with a particular size or position. We use the entropy of the attention distribution to measure this: small entropy means that the head focuses particularly on some numbers. We call the entropy with respect to number size as size entropy and that with respect to input position as position entropy. The entropy of all the heads in terms of number size and position can be found in Appendix B.5.</p>
<p>We prune different kinds of attention heads in order of their corresponding entropy values and report the test accuracy on the pruned GPT-2. Results with different $k$ are shown in Figure 9. We find that the head with a small size entropy is essential for solving the reasoning task. Dropping $10\%$ of this kind of head, leads to a significant drop in performance on the reasoning task. The heads with small position entropy are highly redundant. Dropping $40\%$ of the heads with small position entropy does not affect the test accuracy much. Especially when $k=1$, dropping $90\%$ position heads could still promise a high test accuracy.</p>
<p>These results show that heads with small size entropy are fairly important for GPT-2 to find $k$-th smallest number while those with small position entropy are useless for solving the task. Note that the reasoning tree $G$ is defined on the input number</p>
<p>[table] <img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 9: Test accuracy on the reasoning task for different pruning rates. The $x$-axis represents pruning rates: ranging from $0\%$ to $90\%$, and the $y$-axis represents the test accuracy. We prune heads in the ascending order of their size/position entropy. Results show that heads with small size entropy are essential to the test accuracy while those with small position entropy are useless.</p>
<p>size and it is independent of the number position. MechanisticProbe detects the information of $G$ from attentions. Thus, our probing scores would be affected by the heads with small size entropy but would not be affected by heads with small position entropy. Then, we can say that changing our probing scores (via pruning heads in terms of size entropy) would cause the test accuracy change. Therefore, we say that there is a causal relationship between our probing scores and LM performance, and LMs perform reasoning following the reasoning tree in $\boldsymbol{A}_{\text{simp}}$.</p>
<h2>6 Correlating Probe Scores with Model Accuracy and Robustness</h2>
<p>Our results show that LMs indeed reason mechanistically. But, is mechanistic reasoning necessary for LM performance or robustness? We attempt to answer this question by associating the probing scores with the performance and robustness of LMs. Given that finetuned GPT-2 has a very high test accuracy on the synthetic task and LLaMA does not perform as well on ARC, we conduct our analysis mainly with LLaMA on the ProofWriter task.</p>
<p>Accuracy. We randomly sample 64 to 128 examples from the dataset and test 4-shot LLaMA on these examples. We calculate their test accuracy</p>
<p>[table] <img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>[table] <img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>[table] <img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>[table] <img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p>Table 3: Pearson correlation coefficient between our probing scores and test accuracy for 4-shot LLaMA.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Pearson correlation coefficient</th>
<th style="text-align: center;">$\rho$ ( $\times 100\%$ )</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\rho\left(S_{\mathrm{P}1}, S_{\mathrm{P}2}\right)$</td>
<td style="text-align: center;">0.01</td>
</tr>
<tr>
<td style="text-align: center;">$\rho$ (Test accuracy, $S_{\mathrm{P}1}$ )</td>
<td style="text-align: center;">27.42</td>
</tr>
<tr>
<td style="text-align: center;">$\rho$ (Test accuracy, $S_{\mathrm{P}2}$ )</td>
<td style="text-align: center;">71.13</td>
</tr>
</tbody>
</table>
<p>and the two probing scores $S_{\mathrm{P} 1}$ and $S_{\mathrm{P} 2}$. We repeat this experiment 2048 times. Then, we calculate the correlation between the probing scores and test accuracy. From Table 3, we find that test accuracy is closely correlated with $S_{\mathrm{P} 2}$. This implies that when we can successfully detect reasoning steps of useful statements from LM's attentions, the model is more likely to produce a correct prediction.</p>
<p>Robustness. Following the same setting, we also associate the probing scores with LM robustness. In order to quantify model robustness, we randomly corrupt one useless input statement for each example, such that the prediction would remain unchanged. ${ }^{11}$ We measure robustness by the decrease in test accuracy after the corruption.
<img alt="img-15.jpeg" src="img-15.jpeg" /></p>
<p>Figure 10: Histogram for LM prediction robustness and $S_{\mathrm{P} 2}$. We measure robustness by the change in accuracy on the corrupted dataset. We present results with 8 bins. Results show that LLaMA is more robust to noise on examples with higher probing scores $S_{\mathrm{P} 2}$.</p>
<p>Figure 10 shows that if $S_{\mathrm{P} 2}$ is small (less than 0.7), the prediction of LLaMA could be easily influenced by the noise (test accuracy decreases around $10 \%$ ). However, if the probing $S_{\mathrm{P} 2}$ is high, LLaMA is more robust, i.e., more confident in its correct prediction (test accuracy increases around $4 \%$ ). This provides evidence that if the LM encodes the gold reasoning trees, its predictions are more reliable (i.e., robust to noise in the input).</p>
<h2>7 Related Work</h2>
<p>Attention-based analysis of LMs. Attention has been popularly used to interpret LMs (Vig and</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Belinkov, 2019; DeRose et al., 2021; Bibal et al., 2022). A direct way for interpreting an attentionbased model is to visualize attentions (Samaran et al., 2021; Chefer et al., 2021). But the irrelevant, redundant, and noisy information captured by attentions makes it hard to find meaningful patterns. Alternatively, accumulation attentions that quantify how information flows across tokens can be used for interpretation (Abnar and Zuidema, 2020; Eberle et al., 2022). However, for casual LMs, information flows in one direction, and it causes an over-smoothing problem when the model is deep. ${ }^{12}$ To tackle this, other works propose new metrics and analyze attentions using them (Ethayarajh and Jurafsky, 2021; Liu et al., 2022). However, these metrics are proposed under specific scenarios, and these are not useful for detecting the reasoning process in LMs. We address this challenge by designing a more structured probe that predicts the reasoning tree in LMs.</p>
<p>Mechanistic Interpretability. Mechanistic interpretation explains how LMs work by reverse engineering, i.e., reconstructing LMs with different components (Räuker et al., 2022). A recent line of work provides interpretation focusing on the LM's weights and intermediate representations (Olah et al., 2017, 2018, 2020). Another line of work interprets LMs focusing on how the information is processed inside LMs (Olsson et al., 2022; Nanda et al., 2023). Inspired by them, Qiu et al. (2023) attempts to interpret how LMs perform reasoning. However, existing explorations do not cover the research problem we discussed.</p>
<h2>8 Conclusion</h2>
<p>In this work, we raised the question of whether LMs solve procedural reasoning tasks step-by-step within their architecture. In order to answer this question, we designed a new probe that detects the oracle reasoning tree encoded in the LM architecture. We used the probe to analyze GPT-2 on a synthetic reasoning task and the LLaMA model on two natural language reasoning tasks. Our empirical results show that we can often detect the information in the reasoning tree from the LM's attention patterns, lending support to the claim that LMs may indeed be reasoning "mechanistically".</p>
<p><sup id="fnref3:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<h2>Limitations</h2>
<p>One key limitation of this work is that we considered fairly simple reasoning tasks. We invite future work to understand the mechanism behind LM-based reasoning by exploring more challenging tasks. We list few other limitations of our work below:</p>
<p>Mutli-head attention. In this work, most of our analysis takes the mean value of attentions across all heads. However, we should notice that attention heads could have different functions, especially when the LM is shallow but wide (e.g., with many attention heads, and very high-dimensional hidden states). Shallow models might still be able to solve procedural reasoning tasks within a few layers, but the functions of the head could not be ignored.</p>
<p>Auto-regressive reasoning tasks. In our analysis, we formalize the reasoning task as classification, i.e., single-token prediction. Thus, the analysis could only be deployed on selected reasoning tasks. Some recent reasoning tasks are difficult and can only be solved by LMs via chain-of-thought prompting. We leave the analysis of reasoning under this setting for future work.</p>
<h2>Acknowledgements</h2>
<p>We are grateful to anonymous reviewers for their insightful comments and suggestions. Yifan Hou is supported by the Swiss Data Science Center PhD Grant (P22-05) and Alessandro Stolfo is supported by armasuisse Science and Technology through a CYD Doctoral Fellowship. Antoine Bosselut gratefully acknowledges the support of the Swiss National Science Foundation (No. 215390), Innosuisse (PFFS-21-29), the EPFL Science Seed Fund, the EPFL Center for Imaging, Sony Group Corporation, and the Allen Institute for AI. Mrinmaya Sachan acknowledges support from the Swiss National Science Foundation (Project No. 197155), a Responsible AI grant by the Haslerstiftung; and an ETH Grant (ETH-19 21-1).</p>
<h2>References</h2>
<p>Samira Abnar and Willem Zuidema. 2020. Quantifying attention flow in transformers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4190-4197, Online. Association for Computational Linguistics.</p>
<p>Deniz Bayazit, Negar Foroutan, Zeming Chen, Gail Weiss, and Antoine Bosselut. 2023. Discovering
knowledge-critical subnetworks in pretrained language models.</p>
<p>Yonatan Belinkov. 2022. Probing classifiers: Promises, shortcomings, and advances. Computational Linguistics, 48(1):207-219.</p>
<p>Adrien Bibal, Rémi Cardon, David Alfter, Rodrigo Wilkens, Xiaoou Wang, Thomas François, and Patrick Watrin. 2022. Is attention explanation? an introduction to the debate. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3889-3900, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. CoRR, abs/2005.14165.</p>
<p>Gino Brunner, Yang Liu, Damian Pascual, Oliver Richter, Massimiliano Ciaramita, and Roger Wattenhofer. 2020. On identifiability in transformers. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.</p>
<p>Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramèr, and Chiyuan Zhang. 2022. Quantifying memorization across neural language models. CoRR, abs/2202.07646.</p>
<p>Hila Chefer, Shir Gur, and Lior Wolf. 2021. Generic attention-model explainability for interpreting bimodal and encoder-decoder transformers. In 2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021, pages 387-396. IEEE.</p>
<p>Zeming Chen, Gail Weiss, Eric Mitchell, Asli Celikyilmaz, and Antoine Bosselut. 2023. RECKONING: reasoning through dynamic knowledge encoding. CoRR, abs/2305.06349.</p>
<p>Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the AI2 reasoning challenge. CoRR, abs/1803.05457.</p>
<p>Antonia Creswell and Murray Shanahan. 2022. Faithful reasoning using large language models. CoRR, abs/2208.14271.</p>
<p>Antonia Creswell, Murray Shanahan, and Irina Higgins. 2022. Selection-inference: Exploiting large language models for interpretable logical reasoning. CoRR, abs/2205.09712.</p>
<p>Joseph F. DeRose, Jiayao Wang, and Matthew Berger. 2021. Attention flows: Analyzing and comparing attention mechanisms in language models. IEEE Trans. Vis. Comput. Graph., 27(2):1160-1170.</p>
<p>Yue Dong, Chandra Bhagavatula, Ximing Lu, Jena D. Hwang, Antoine Bosselut, Jackie Chi Kit Cheung, and Yejin Choi. 2021. On-the-fly attention modulation for neural generation. In Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021, volume ACL/IJCNLP 2021 of Findings of ACL, pages 12611274. Association for Computational Linguistics.</p>
<p>Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jiang, Bill Yuchen Lin, Peter West, Chandra Bhagavatula, Ronan Le Bras, Jena D. Hwang, Soumya Sanyal, Sean Welleck, Xiang Ren, Allyson Ettinger, Zaïd Harchaoui, and Yejin Choi. 2023. Faith and fate: Limits of transformers on compositionality. CoRR, abs/2305.18654.</p>
<p>Oliver Eberle, Stephanie Brandl, Jonas Pilot, and Anders Søgaard. 2022. Do transformer models show similar attention patterns to task-specific human gaze? In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4295-4309, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Yanai Elazar, Shauli Ravfogel, Alon Jacovi, and Yoav Goldberg. 2021. Amnesic probing: Behavioral explanation with amnesic counterfactuals. Transactions of the Association for Computational Linguistics, 9:160175 .</p>
<p>Kawin Ethayarajh and Dan Jurafsky. 2021. Attention flows are shapley value explanations. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 49-54, Online. Association for Computational Linguistics.</p>
<p>Mor Geva, Jasmijn Bastings, Katja Filippova, and Amir Globerson. 2023. Dissecting recall of factual associations in auto-regressive language models. CoRR, abs/2304.14767.</p>
<p>John Hewitt and Percy Liang. 2019. Designing and interpreting probes with control tasks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2733-2743, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Yifan Hou and Mrinmaya Sachan. 2021. Bird's eye: Probing for linguistic graph structures with a simple information-theoretic approach. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1844-1859, Online. Association for Computational Linguistics.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. In NeurIPS.</p>
<p>Karim Lasri, Tiago Pimentel, Alessandro Lenci, Thierry Poibeau, and Ryan Cotterell. 2022. Probing for the usage of grammatical number. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8818-8831, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Yibing Liu, Haoliang Li, Yangyang Guo, Chenqi Kong, Jing Li, and Shiqi Wang. 2022. Rethinking attentionmodel explainability through faithfulness violation test. In International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 13807-13824. PMLR.</p>
<p>Ilya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.</p>
<p>Donald W. Loveland. 1980. Automated theorem proving. a logical basis. Journal of Symbolic Logic, 45(3):629-630.</p>
<p>Christopher D. Manning, Kevin Clark, John Hewitt, Urvashi Khandelwal, and Omer Levy. 2020. Emergent linguistic structure in artificial neural networks trained by self-supervision. Proc. Natl. Acad. Sci. USA, 117(48):30046-30054.</p>
<p>Jack Merullo, Carsten Eickhoff, and Ellie Pavlick. 2023. Language models implement simple word2vec-style vector arithmetic.</p>
<p>Shikhar Murty, Pratyusha Sharma, Jacob Andreas, and Christopher Manning. 2023. Grokking of hierarchical structure in vanilla transformers. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 439-448, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt. 2023. Progress measures for grokking via mechanistic interpretability. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net.</p>
<p>Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan Carter. 2020. Zoom in: An introduction to circuits. Distill, 5(3):e00024001.</p>
<p>Chris Olah, Alexander Mordvintsev, and Ludwig Schubert. 2017. Feature visualization. Distill, 2(11):e7.</p>
<p>Chris Olah, Arvind Satyanarayan, Ian Johnson, Shan Carter, Ludwig Schubert, Katherine Ye, and Alexander Mordvintsev. 2018. The building blocks of interpretability. Distill, 3(3):e10.</p>
<p>Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. 2022. In-context learning and induction heads. arXiv preprint arXiv:2209.11895.</p>
<p>Linlu Qiu, Liwei Jiang, Ximing Lu, Melanie Sclar, Valentina Pyatkin, Chandra Bhagavatula, Bailin Wang, Yoon Kim, Yejin Choi, Nouha Dziri, et al. 2023. Phenomenal yet puzzling: Testing inductive reasoning capabilities of language models with hypothesis refinement. arXiv preprint arXiv:2310.08559.</p>
<p>Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI blog.</p>
<p>Tilman Räuker, Anson Ho, Stephen Casper, and Dylan Hadfield-Menell. 2022. Toward transparent AI: A survey on interpreting the inner structures of deep neural networks. CoRR, abs/2207.13243.</p>
<p>Abhilasha Ravichander, Yonatan Belinkov, and Eduard Hovy. 2021. Probing the probing paradigm: Does probing accuracy entail task relevance? In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 3363-3377, Online. Association for Computational Linguistics.</p>
<p>Yasaman Razeghi, Robert L Logan IV, Matt Gardner, and Sameer Singh. 2022. Impact of pretraining term frequencies on few-shot numerical reasoning. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 840-854, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Danilo Neves Ribeiro, Shen Wang, Xiaofei Ma, Henghui Zhu, Rui Dong, Deguang Kong, Juliette Burger, Anjelica Ramos, Zhiheng Huang, William Yang Wang, George Karypis, Bing Xiang, and Dan Roth. 2023. STREET: A multi-task structured reasoning and explanation benchmark. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net.</p>
<p>Anna Rogers, Olga Kovaleva, and Anna Rumshisky. 2020. A primer in BERTology: What we know about how BERT works. Transactions of the Association for Computational Linguistics, 8:842-866.</p>
<p>Sebastian Ruder, Jonas Pfeiffer, and Ivan Vulić. 2022. Modular and parameter-efficient fine-tuning for NLP models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts, pages 23-29, Abu Dubai, UAE. Association for Computational Linguistics.</p>
<p>Jules Samaran, Noa Garcia, Mayu Otani, Chenhui Chu, and Yuta Nakashima. 2021. Attending self-attention: A case study of visually grounded supervision in
vision-and-language transformers. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop, pages 81-86, Online. Association for Computational Linguistics.</p>
<p>Alessandro Stolfo, Yonatan Belinkov, and Mrinmaya Sachan. 2023. Understanding arithmetic reasoning in language models using causal mediation analysis. arXiv preprint arXiv:2305.15054.</p>
<p>Oyvind Tafjord, Bhavana Dalvi, and Peter Clark. 2021. ProofWriter: Generating implications, proofs, and abductive statements over natural language. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3621-3634, Online. Association for Computational Linguistics.</p>
<p>Xiaojuan Tang, Zilong Zheng, Jiaqi Li, Fanxu Meng, Song-Chun Zhu, Yitao Liang, and Muhan Zhang. 2023. Large language models are in-context semantic reasoners rather than symbolic reasoners. CoRR, abs/2305.14825.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971.</p>
<p>Jesse Vig and Yonatan Belinkov. 2019. Analyzing the structure of attention in a transformer language model. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 63-76, Florence, Italy. Association for Computational Linguistics.</p>
<p>Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. 2019. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5797-5808, Florence, Italy. Association for Computational Linguistics.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS.</p>
<p>Zhengxuan Wu, Atticus Geiger, Christopher Potts, and Noah D. Goodman. 2023. Interpretability at scale: Identifying causal mechanisms in alpaca. CoRR, abs/2305.08809.</p>
<p>Shizhuo Dylan Zhang, Curt Tigges, Stella Biderman, Maxim Raginsky, and Talia Ringer. 2023. Can transformers learn to solve problems recursively? CoRR, abs/2305.14699.</p>
<p>A Proof of The First-Token Domination</p>
<p>Proof. Without loss of generality, we assume the LM have $L$ layers with only 1 attention head ( $h=$ $H=1$ ), and the attention weight matrix in layer $l$ is $\boldsymbol{A}(l, h)$. We consider the input that have more than 1 token. We model attention as flows following the setting of Abnar and Zuidema (2020). Then, the attention accumulation $\operatorname{Accum}()$ of the token $\boldsymbol{z}_{i}^{l+1}$ in the layer $l+1$ can be written as</p>
<p>$$
\operatorname{Accum}\left(\boldsymbol{z}<em 1="1" _leq="\leq" _leq_T_="\leq|T|" j="j">{i}^{l+1}\right)=\sum</em>} \boldsymbol{a<em j="j">{i, j}(l, h) \cdot \operatorname{Accum}\left(\boldsymbol{z}</em>\right)
$$}^{l</p>
<p>where we have</p>
<p>$$
\operatorname{Accum}\left(\boldsymbol{z}<em 1="1" _leq="\leq" _leq_T_="\leq|T|" j="j">{i}^{1}\right)=\sum</em>} \boldsymbol{a<em j="j">{i, j}(l, h) \cdot t</em>
$$</p>
<p>Since $\boldsymbol{a}_{i, j}(l, h)$ is the attention weight for casual LM, we have</p>
<p>$$
\left{\begin{array}{rll}
\boldsymbol{a}<em i_="i," j="j">{i, j}(l, h)=0 &amp; \text { if } &amp; i&lt;j \
0&lt;\boldsymbol{a}</em> &amp; i \geq j \
\sum_{j} \boldsymbol{a}_{i, j}(l, h)=1 &amp;
\end{array}\right.
$$}(l, h)&lt;1 &amp; \text { if </p>
<p>Note that attention is normalized by Softmax function in LMs. The minimum attention weight is non-zero, and we assume there exist a constant $\epsilon&gt;0$ such as $\epsilon \leq \boldsymbol{a}<em _boldsymbol_z="\boldsymbol{z">{i, j}(l, h)&lt;1$ if $i \geq j$. Now we define the information ratio $\operatorname{IR}</em><em j="j">{i}^{1}}\left(t</em>}\right)$ as the information of token $t_{j}$ stored in the hidden representation $\boldsymbol{z<em 1="1">{i}^{l}$. Consider that in each layer, token $t</em>$ would propagate its information to all tokens in the next layer with at least $\epsilon$ amount. Then, by tracing the information flow from other tokens $j&gt;1$, we have</p>
<p>$$
1-\operatorname{IR}<em i="i">{\boldsymbol{z}</em>}^{l+1}}\left(t_{1}\right) \leq(1-\epsilon)\left(1-\operatorname{IR<em i="i">{\boldsymbol{z}</em>\right)\right)
$$}^{l}}\left(t_{1</p>
<p>Using the chain rule, we have</p>
<p>$$
1-\operatorname{IR}<em i="i">{\boldsymbol{z}</em>}^{L}}\left(t_{1}\right) \leq(1-\epsilon)^{L}\left(1-\operatorname{IR<em i="i">{\boldsymbol{z}</em>\right)\right)
$$}^{1}}\left(t_{1</p>
<p>which means</p>
<p>$$
\begin{aligned}
&amp; \operatorname{IR}<em i="i">{\boldsymbol{z}</em>}^{L}}\left(t_{1}\right) \geq 1-(1-\epsilon)^{L-1}\left(1-\operatorname{IR<em i="i">{\boldsymbol{z}</em>\right)\right) \
&amp; \operatorname{IR}}^{1}}\left(t_{1<em i="i">{\boldsymbol{z}</em>
\end{aligned}
$$}^{L}}\left(t_{1}\right) \geq 1-(1-\epsilon)^{L</p>
<p>With the inequality above, we know that if the LM is deep, i.e., $L$ is large, we have $\operatorname{IR}<em i="i">{\boldsymbol{z}</em>}^{L}}\left(t_{1}\right)$ increase exponentially in terms of layer $L$, which means that $\operatorname{IR<em i="i">{\boldsymbol{z}</em>\right) \approx 1$ with large $L$ in general.}^{L}}\left(t_{1</p>
<h2>B Supplementary about GPT-2</h2>
<p>We provide some more details on our experiments on GPT-2 as well as LLaMA to help in reproducibility. First of all, we fix the random seed (42) and use the same random seed for all experiments, including LM finetuning and interpretations. In addition, to make sure the random seed is unbiased, we further re-run the same experiment with different random seeds. All of our experiments have roughly the same results as those of using other seeds.</p>
<p>Second, we design our analysis as simply as possible to ensure that there is as little random influence (i.e., confounder) as possible. For MechanisticProbe, we select the kNN classifier. For LLaMA, we run analysis experiments on a 4 shot in-context learning setting.</p>
<p>Third, we report as many intermediate and supplement results as possible. In the Appendix, there are many other interesting findings. However, due to the space limit, we cannot present them in the main paper. We hope our findings are helpful to the community to better understand LMs.</p>
<h2>B. 1 GPT-2 Finetuning</h2>
<p>The finetuning settings for all GPT-2 models are roughly identical. We generate 0.98 million sequences of numbers as the training data and 10,000 in data for validation and testing. Note that the collision probability is extremely small, thus we can assume that there is no data leakage. The epoch number is set as 2 , and the batch size here is 256 . We use the AdamW (Loshchilov and Hutter, 2019) optimizer with weight decay $1 e-3$ from Huggingface ${ }^{13}$ for finetuning. The learning rate is $1 e-6$.</p>
<h2>B. 2 Original Probing Scores</h2>
<p>The original classification scores (F1-Macro) for two probing tasks can be found in Table 4. Here, random means we randomly initialize GPT-2 and use its attentions for probing as the random baseline. The other two pretrained and finetuned are the pretrained GPT-2 model and GPT-2 model after finetuning with supervised signals.</p>
<h2>B. 3 Visualization of $\mathbb{E}\left[\pi\left(\boldsymbol{A}_{\text {simp }}\right)\right]$ for GPT-2</h2>
<p>We visualize the $\boldsymbol{A}_{\text {simp }}$ for pretrained GPT-2 without finetuning in Figure 11. We can find that even if pretrained GPT-2 cannot solve the synthetic reasoning task (finding $k$-th smallest number from a list). It can still somehow differentiate the size of</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 4: The original classification F1-Macro scores of GPT-2 for two probing tasks.</p>
<p>| Probing task | $S_{\text{FT}}(V|\boldsymbol{A}<em _text_FT="\text{FT">{\text{simp}})$ | | | $S</em>)$ | | |
| GPT-2 | random | pretrained | finetuned | random | pretrained | finetuned |
| --- | --- | --- | --- | --- | --- | --- | --- |
| $k=1$ | 48.38 | 52.04 | 96.36 | 100 | 100 | 100 |
| $k=2$ | 48.60 | 51.61 | 96.77 | 73.18 | 78.72 | 99.47 |
| $k=3$ | 47.62 | 54.68 | 95.61 | 61.45 | 66.69 | 98.36 |
| $k=4$ | 47.42 | 58.32 | 93.87 | 55.71 | 59.28 | 96.69 |
| $k=5$ | 48.50 | 60.94 | 93.58 | 54.86 | 55.48 | 94.66 |
| $k=6$ | 48.66 | 61.40 | 93.27 | 50.44 | 55.98 | 97.55 |
| $k=7$ | 49.28 | 62.40 | 88.14 | 51.75 | 51.11 | 97.55 |
| $k=8$ | 49.74 | 60.95 | 89.31 | 50.54 | 51.07 | 97.00 |}}(G|V,\boldsymbol{A}_{\text{simp}</p>
<p>numbers. The largest number of the list often has slightly larger attentions in layer 11.
<img alt="img-16.jpeg" src="img-16.jpeg" /></p>
<p>Figure 11: Visualization of $\mathbb{E}\left[\pi\left(\boldsymbol{A}_{\text {simp }}\right)\right]$ for pretrained GPT-2 (w/o any finetuning). Similarly, we take mean pooling for different attention heads.</p>
<h2>B. 4 Visulization of $\boldsymbol{A}$</h2>
<p>To avoid disturbance, we remove $40 \%$ position heads (heads with small position entropy) and take mean pooling on all left heads. We visualize $\boldsymbol{A}$ to directly show that $\boldsymbol{A}_{\text {simp }}$ contains sufficient essential information of $\boldsymbol{A}$. We consider a special case when $k=2$, the largest number is at position 8 and the second largest number is at position $12 .{ }^{14}$</p>
<p>The attention $\mathbb{E}[\boldsymbol{A}]$ is visualized as in Figure 12. From Figure 12, we can get similar conclusion as on the visualization of $\mathbb{E}\left[\boldsymbol{A}<em _simp="{simp" _text="\text">{\text {simp }}\right]$. In the bottom layers, most hidden representations focus on top-2 numbers. In the top layers, most hidden representations focus on the second smallest number. This result proves that our analysis on $\boldsymbol{A}</em>$.}}$ is as reasonable as that on $\boldsymbol{A</p>
<p>We also provide the visualization of $\mathbb{E}[\boldsymbol{A}]$ on normal test data (i.e., the input position is independent of the size) in Figure 13. We can find that the attention distribution is even, and there is no</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-17.jpeg" src="img-17.jpeg" /></p>
<p>Figure 12: Visualization of $\mathbb{E}[\boldsymbol{A}]$. The $x$-axis represents layers and the $y$-axis represents input token positions. We take the mean pooling of $60 \%$ attention heads with large position entropy. The leaf node (i.e., the largest number) is always at position 8 , and the root node (i.e., the second largest number) is always at position 12 . We track their attentions and visualize them by blue and red lines.
typical tendency.
<img alt="img-18.jpeg" src="img-18.jpeg" /></p>
<p>Figure 13: Visualization of $\mathbb{E}[\boldsymbol{A}]$ on normal test data. The $x$-axis represents layers and the $y$-axis represents input token positions. We take mean pooling of $60 \%$ attention heads with large position entropy.</p>
<h2>B. 5 Attention Head Entropy</h2>
<p>From Figure 14, we can find that most heads belong to either position head or size head. Note that we generate input data randomly, thus, the number size and input position are independent of each other. Thus, one head cannot be both position head and size head.</p>
<h2>B. 6 Do Finetuning Methods Matter?</h2>
<p>To show that our analysis is robust, we explore the attention of GPT-2 with different ways of param-</p>
<p><img alt="img-19.jpeg" src="img-19.jpeg" />
(a) Entropy in terms of number size (rank)
(b) Entropy in terms of number input position</p>
<p>Figure 14: Entropy for all attention heads of finetuned GPT-2. $x$-axis represents layers and $y$-axis represents the head index. The cube (better view in color) shows the entropy value.
eter efficient finetuning [Ruder2022]. We report our two probing scores $S_{\mathrm{P} 1}$ and $S_{\mathrm{P} 2}$ with various ways of finetuning in Figure 15 (We consider the condition when $k=2$ and $m=16$ ). We find that probing scores of finetuning the full GPT-2 model are similar to that of partially finetuning on attention parameters and MLP (multilayer perceptron) parameters. These consistent results ensure the general usage of our MechanisticProbe on current large LMs with partial finetuning.
<img alt="img-20.jpeg" src="img-20.jpeg" /></p>
<p>Figure 15: Probing scores of GPT-2 with different finetuning methods. Here, w/o denotes the baseline when GPT-2 model is not finetuned. Full denotes finetuning with all parameters. Other models are partial finetuned with corresponding parameters. With the exception of probing scores of the Bias tuned model, other partial finetuning methods have roughly the same probing scores compared to that full finetuning. This indicates that MechanisticProbe can provide consistent analysis for LMs finetuned in different ways.</p>
<p>The direct visualization of $\mathbb{E}\left[\pi\left(\boldsymbol{A}_{\text {simp }}\right)\right]$ for different finetuning methods can be found Figure 16(a-d). And the test accuracy of these 4 finetuned models are: 99.42, 99.31, 94.11, and 76.84. We can find that finetuning attention parameters or MLP parameters can obtain quite similar attention pat-
<img alt="img-21.jpeg" src="img-21.jpeg" /></p>
<p>Figure 16: Visualization of $\mathbb{E}\left[\pi\left(\boldsymbol{A}_{\text {simp }}\right)\right]$ with different ways of finetuning. The $x$-axis represents layers and the $y$-axis represents size ranking. We take mean pooling attention heads. From Figures (a-d) are GPT-2 with full finetuning, GPT-2 with partial finetuning on attention parameters, GPT-2 with partial finetuning on MLP parameters, and GPT-2 with partial finetuning on bias parameters. We can find that different finetuning methods would not affect the attention patterns much.
terns to that of full model finetuning. Regarding the bias tuning, it is slightly different. We speculate that this is because the LM does not learn the $k$-th smallest element task well (with much lower test accuracy). Generally, without significant performance drops, these results can support the general usage of our probing method MechanisticProbe under different finetuning settings.</p>
<h1>B. 7 Task Difficulty \&amp; Model Capacity</h1>
<p>This subsection discusses which factors can let GPT-2 handle reasoning tasks with more leaf nodes in $G$, i.e., large $k$. We explore the model capacity and reasoning task difficulty. Specifically, we extend the list length $m$ from 16 to 64 , and maximum $k$ from 8 to 32 . For each $k$, we finetune LMs with the same finetuning settings and evaluate them on test data with accuracy. For model capacity, we compare three versions of GPT-2 with different sizes: Distilled GPT-2, GPT-2 (small), and GPT-2 Medium. For task difficulty, we construct synthetic data by selecting $m=64$ numbers from 256, 384,</p>
<p><img alt="img-22.jpeg" src="img-22.jpeg" /></p>
<p>Figure 17: Test accuracies for LMs under different conditions. The <em>x</em>-axis represents <em>k</em> (finding the <em>k</em>-th smallest number) and the <em>y</em>-axis represents the test accuracy. The left figure explores the performance of LMs with different capacities, and the right figure explores the performance of GPT-2 with different task difficulties.</p>
<p>and 512 distinct numbers.</p>
<p>We report their test accuracies in Figure 17. Note that here the setting remains the same: for each reasoning task (i.e., <em>k</em>), we finetune an individual model. From Figure 17(a), we find that LMs with large model capacities can better solve procedural tasks with more complex <em>G</em> (i.e., more leaf nodes in <em>G</em>). But it does not mean that small LMs fail in this case. If we can reasonably reduce the task difficulty (e.g., decompose the procedural task), small LMs are still able to handle that task with complex <em>G</em> (Figure 17(b)).</p>
<h3>B.8 What if Reasoning Tasks Become Harder?</h3>
<p><img alt="img-23.jpeg" src="img-23.jpeg" /></p>
<p>Figure 18: Probing scores and test accuracy of GPT-2FT on more difficult reasoning tasks. We finetune GPT-2 models to find the <em>k</em>-th smallest number from the long number list (<em>m</em> = 64, and <em>k</em> ranges from 1 to 32). Results show that when the accuracy is low, GPT-2FT would still know how to select top-<em>k</em> numbers more or less. But they are unable to find the <em>k</em>-th smallest number from top-<em>k</em> numbers anymore.</p>
<p>Till now, we have experimented on a relatively easy task (<em>m</em> = 16). In this subsection, we increase the difficulty of the task by extending the input number list from <em>m</em> = 16 numbers to <em>m</em> = 64 numbers. We report the test accuracy as well as the two probing scores in Figure 18, varying <em>k</em> from 1 to 32. As expected, the test accuracy decreases smoothly from near 100% to around 15%. Interestingly, <em>S</em>P1 and <em>S</em>P2 do not decrease with the same speed. Even when the model has a very low accuracy, <em>S</em>P1 still maintains at a high score (above 30%), while <em>S</em>P2 quickly jumps to 0 when <em>k</em> is around 20. This suggests that GPT-2FT solves the two steps sequentially, and step 2 fails first when the task goes beyond the capacity of the model.</p>
<h3>C Supplementary about LLaMA</h3>
<h4>C.1 Settings for 4-shot and Finetuned LLaMA</h4>
<p>For the in-context learning of LLaMA, we construct the input prompt as simple as possible. Given set of statements [<em>S</em>1, <em>S</em>2, ...], the question statement <em>Q</em>, and the answer label A (e.g., "True" or "False"), the prompt templates for ProofWriter and ARC are:</p>
<p>$$
\begin{aligned}
&amp; \left[ S_1, S_2, \dots \right] + [\mathrm{Q}] + \text{True or False}? + [\mathrm{A}], \
&amp; \left[ S_1, S_2, \dots \right] + [\mathrm{Q}] + \text{The answer is:} + [\mathrm{A}]
\end{aligned}
$$</p>
<p>The test accuracy of 0-shot, 2-shot, 4-shot, and 8-shot prompting of LLaMA can be found in Table 5. We select 4-shot in-context learning setting in our analysis due to its best performance.</p>
<p>Table 5: Test accuracy of LLaMA.</p>
<table>
<thead>
<tr>
<th>ProofWriter</th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Acc. (%)</td>
<td>depth=0</td>
<td>depth=1</td>
<td>depth=2</td>
<td>depth=3</td>
</tr>
<tr>
<td>0-shot LLaMA</td>
<td>50.07</td>
<td>49.61</td>
<td>49.74</td>
<td>49.83</td>
</tr>
<tr>
<td>2-shot LLaMA</td>
<td>75.58</td>
<td>75.83</td>
<td>74.31</td>
<td>73.74</td>
</tr>
<tr>
<td>4-shot LLaMA</td>
<td>81.72</td>
<td>78.33</td>
<td>75.58</td>
<td>74.64</td>
</tr>
<tr>
<td>8-shot LLaMA</td>
<td>54.26</td>
<td>52.96</td>
<td>52.82</td>
<td>52.42</td>
</tr>
<tr>
<td>Finetuned LLaMA</td>
<td>100</td>
<td>100</td>
<td>100</td>
<td>100</td>
</tr>
<tr>
<td>ARC</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>4-shot LLaMA</td>
<td>-</td>
<td>56.32</td>
<td>53.40</td>
<td>-</td>
</tr>
</tbody>
</table>
<p>Regarding the finetuning of LLaMA (i.e., partially finetuning on attention parameters), most settings are similar to that of GPT-2. The epoch number is set as 2, and the batch size is 256. We use the AdamW optimizer with weight decay 1<em>e</em> − 5 for finetuning, and the warmup number is 500. The</p>
<p>learning rate is $1 e-6$. Test accuracy of finetuned models can be found in Table 5 as well.</p>
<h3>C. 2 Layer (Attention) Pruning</h3>
<p>For the layer pruning, we use the greedy search strategy. Specifically, we remove all attentions in layers from top to bottom. ${ }^{15}$ If the performance decrease on test data is small (less than $5 \%$ in total), the attention in that layer is dropped. For 4 -shot LLaMA on ProofWriter, 13 (out of 32) top layers are removed, and 15 (out of 32) top layers are removed for ARC. For finetuned LLaMA on ProofWriter, 2 middle layers (layer 9 and layer 13) and 16 top layers are removed. After removing all attentions in these layers, the performance decreases are around $2 \%$ for both in-context learning and finetuning settings.</p>
<h3>C. 3 Statistics of Cleaned ProofWriter and Annotated ARC</h3>
<p>Table 6: Data statistics of cleaned ProofWriter in terms of different depth.</p>
<table>
<thead>
<tr>
<th>$#$ of examples</th>
<th>Training</th>
<th>Development</th>
<th>Test</th>
</tr>
</thead>
<tbody>
<tr>
<td>depth $=0$</td>
<td>84,568</td>
<td>12,227</td>
<td>24,270</td>
</tr>
<tr>
<td>depth $=1$</td>
<td>41,718</td>
<td>6,101</td>
<td>12,044</td>
</tr>
<tr>
<td>depth $=2$</td>
<td>25,021</td>
<td>3,712</td>
<td>7,215</td>
</tr>
<tr>
<td>depth $=3$</td>
<td>14,042</td>
<td>2,079</td>
<td>4,132</td>
</tr>
<tr>
<td>depth $=4$</td>
<td>6,078</td>
<td>891</td>
<td>1765</td>
</tr>
<tr>
<td>depth $=5$</td>
<td>5,998</td>
<td>874</td>
<td>1756</td>
</tr>
</tbody>
</table>
<p>ProofWriter. We follow the original data split for training, development, and test sets. However, the depth split of ProofWriter is not suitable in our case. The original dataset only considers the largest depth of a set of examples (with similar templates) for the split. It means for example in depth 5 , there would be many of them with depth smaller than 5. In our case, we classify examples into 6 types from depth 0 to 5 only based on the example's reasoning tree depth. After the depth split, we also remove examples whose reasoning trees have loops or multiple annotations. Besides, we remove few examples whose depth annotations are wrong (e.g., annotated as depth 5 but only with 4 nodes in $G$ ). Statistics of the cleaned and re-split ProofWriter</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup>can be found in Table 6. We can find that there are less than 2000 4/5-depth examples in test data.</p>
<p>Table 7: Data statistics of annotated ARC in terms of different depth.</p>
<table>
<thead>
<tr>
<th>$#$ of examples</th>
<th>Training</th>
<th>Development</th>
<th>Test</th>
</tr>
</thead>
<tbody>
<tr>
<td>depth $=1$</td>
<td>331</td>
<td>51</td>
<td>87</td>
</tr>
<tr>
<td>depth $=2$</td>
<td>380</td>
<td>64</td>
<td>95</td>
</tr>
<tr>
<td>depth $=3$</td>
<td>277</td>
<td>28</td>
<td>67</td>
</tr>
<tr>
<td>depth $=4$</td>
<td>175</td>
<td>18</td>
<td>51</td>
</tr>
<tr>
<td>depth $&gt;4$</td>
<td>150</td>
<td>26</td>
<td>40</td>
</tr>
</tbody>
</table>
<p>ARC. We follow the original data split for training, development, and test sets (Ribeiro et al., 2023). Note that the number of examples in ARC is quite small. Thus, in our analysis, we do not run experiments only on test data. We simply merge all data for the analysis.</p>
<h3>C. 4 Reasoning Tree Ambiguity Example</h3>
<p>We consider a simple case to explore if the reasoning tree ambiguity issues happens in LLaMA. We sample 1024 examples whose annotations of 2 -depth reasoning trees are</p>
<p>$$ S_{1}-&gt;S_{2}-&gt;S 3 \rightarrow Q $$</p>
<p>We give a real example from the dataset randomly to illustrate the issue of reasoning tree ambiguity. Consider the three statements of $G$ (from 17 input statements) as $S_{1}:$ Erin is cold; $S_{2}$ : If someone is cold then they are rough; $S_{3}$ : If someone is rough then they are white; $Q:$ Erin is white (True). It is intuitive that following the annotated reasoning tree could obtain correct answer. However, there are other ways to answer the question. We can first combine $S_{2}$ and $S_{3}$ to get a new statement $S_{4}$ as $S_{4}$ : If someone is cold then they are white. Then, the reasoning tree becomes</p>
<p>$$ S_{2}-&gt;S_{3}-&gt;S_{1} \rightarrow Q $$</p>
<p>and we can rewrite it with brackets as</p>
<p>$$ S_{1}-&gt;\left(S_{2}-&gt;S_{3}\right) \rightarrow Q $$</p>
<p>There are multiple ways to do reasoning for this example, and we do not know which one the LM uses. Thus, in this work, we ignore these kinds of examples with reasoning tree depth larger than 1. ${ }^{16}$</p>
<h1>C. 5 Original Probing Scores</h1>
<p>Table 8: The original classification F1-Macro scores of LLaMA for two probing tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">ProofWriter</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Probing task</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$S_{\mathrm{F} 1}\left(V \mid \boldsymbol{A}_{\text {emp }}\right)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$S_{\mathrm{F} 1}(G) V, \boldsymbol{A}_{\text {emp }}$ )</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">LLaMA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">random</td>
<td style="text-align: center;">4-shot</td>
<td style="text-align: center;">finetuned</td>
<td style="text-align: center;">random</td>
<td style="text-align: center;">4-shot</td>
<td style="text-align: center;">finetuned</td>
</tr>
<tr>
<td style="text-align: center;">depth $=0$</td>
<td style="text-align: center;">$|\mathcal{E}|=1$</td>
<td style="text-align: center;">48.10</td>
<td style="text-align: center;">77.79</td>
<td style="text-align: center;">73.57</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">depth $=1$</td>
<td style="text-align: center;">$|\mathcal{E}|=2$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100100</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$|\mathcal{E}|=4$</td>
<td style="text-align: center;">62.38</td>
<td style="text-align: center;">79.24</td>
<td style="text-align: center;">80.49</td>
<td style="text-align: center;">47.39</td>
<td style="text-align: center;">96.50</td>
<td style="text-align: center;">98.01</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$|\mathcal{E}|=8$</td>
<td style="text-align: center;">63.31</td>
<td style="text-align: center;">73.36</td>
<td style="text-align: center;">78.02</td>
<td style="text-align: center;">54.67</td>
<td style="text-align: center;">92.63</td>
<td style="text-align: center;">98.39</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$|\mathcal{E}|=12$</td>
<td style="text-align: center;">51.64</td>
<td style="text-align: center;">64.33</td>
<td style="text-align: center;">67.45</td>
<td style="text-align: center;">49.75</td>
<td style="text-align: center;">88.73</td>
<td style="text-align: center;">96.71</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$|\mathcal{E}|=16$</td>
<td style="text-align: center;">49.79</td>
<td style="text-align: center;">58.42</td>
<td style="text-align: center;">60.37</td>
<td style="text-align: center;">44.43</td>
<td style="text-align: center;">87.65</td>
<td style="text-align: center;">94.06</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$|\mathcal{E}|=20$</td>
<td style="text-align: center;">47.40</td>
<td style="text-align: center;">53.24</td>
<td style="text-align: center;">55.73</td>
<td style="text-align: center;">48.74</td>
<td style="text-align: center;">89.74</td>
<td style="text-align: center;">96.98</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$|\mathcal{E}|=24$</td>
<td style="text-align: center;">48.40</td>
<td style="text-align: center;">53.25</td>
<td style="text-align: center;">57.34</td>
<td style="text-align: center;">51.77</td>
<td style="text-align: center;">90.51</td>
<td style="text-align: center;">97.31</td>
</tr>
<tr>
<td style="text-align: center;">ABC</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">depth=1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">51.21</td>
<td style="text-align: center;">98.77</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">48.73</td>
<td style="text-align: center;">80.38</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">depth=2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">50.86</td>
<td style="text-align: center;">98.28</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">50.38</td>
<td style="text-align: center;">76.88</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>The original classification scores (F1-Macro) for two probing tasks on LLaMA can be found in Table 8. Here, random means we randomly initialize LLaMA and use its attentions for probing as the random baseline.</p>
<p><sup id="fnref8:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{16}$ In ProofWriter, there are only $30 \%$ examples that have reasoning trees with depth larger than 1 .&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref8:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{12}$ The issue is that all token representations are dominated by the first token. Detailed discussion is in Appendix A.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
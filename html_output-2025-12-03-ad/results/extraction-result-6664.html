<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6664 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6664</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6664</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-126.html">extraction-schema-126</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <p><strong>Paper ID:</strong> paper-7ba98b00a224094c09676090f5d6d69498f5b299</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/7ba98b00a224094c09676090f5d6d69498f5b299" target="_blank">MiniF2F: a cross-system benchmark for formal Olympiad-level mathematics</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> The miniF2F benchmark currently targets Metamath, Lean, Isabelle, and HOL Light and consists of 488 problem statements drawn from the AIME, AMC, and the International Mathematical Olympiad, as well as material from high-school and undergraduate mathematics courses.</p>
                <p><strong>Paper Abstract:</strong> We present miniF2F, a dataset of formal Olympiad-level mathematics problems statements intended to provide a unified cross-system benchmark for neural theorem proving. The miniF2F benchmark currently targets Metamath, Lean, Isabelle (partially) and HOL Light (partially) and consists of 488 problem statements drawn from the AIME, AMC, and the International Mathematical Olympiad (IMO), as well as material from high-school and undergraduate mathematics courses. We report baseline results using GPT-f, a neural theorem prover based on GPT-3 and provide an analysis of its performance. We intend for miniF2F to be a community-driven effort and hope that our benchmark will help spur advances in neural theorem proving.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6664.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6664.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-f (Metamath)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-f applied to Metamath</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoder-only transformer language model fine-tuned to predict Metamath proof steps and used with log-probability based proof search; evaluated on formal Olympiad-level problems in the Metamath formal system (miniF2F).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generative language modeling for automated theorem proving</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-f (Metamath)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>700M learnable parameters</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pretrained on WebMath and then trained on an updated dump of the Metamath set.mm library (and similar synthetic datasets) to model low-level Metamath proof steps.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>miniF2F (Metamath partition)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>formal theorem proving including low-level arithmetic manipulations (e.g., n-digit addition, ring arithmetic rewrites) and Olympiad-style algebra/number-theory problems</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>formal symbolic statements in Metamath (machine-checkable proofs of goals via sequences of low-level proofsteps)</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Olympiad-level and MATH levels 1–5 (wide range; many problems require long low-level sequences)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Finetuned next-token prediction on proof-step sequences; log-probability-based proof search (sampling many proof attempts and ranking by model log-probability); model attempts e=16 tactics per expansion (per paper note)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Pass@N (Pass@1, Pass@8) and average proof length (number of proofsteps)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>miniF2F-test: Pass@1 = 1.3%, Pass@8 = 1.6%; average proof length = 20.3 steps (for proofs found)</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>No mechanistic probes reported; qualitative analysis notes that Metamath's low-level proof format makes arithmetic and simple algebra proofs extremely long, and the model must learn many low-level rewrite patterns. Authors hypothesize that lack of high-level tactics forces the model to produce very long sequences and limits success.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Struggles when proofs require many low-level rewrite steps (e.g., n-digit addition or simple ring arithmetic become tedious); fails to efficiently rewrite goals into forms amenable to existing theorems; cannot rely on high-level automation, so search is bottlenecked by long action sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>No explicit model-size scaling analysis reported for arithmetic; authors note compute deployed at training comparable to Lean setup, but performance is much lower due to absence of high-level tactics rather than model scale differences.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MiniF2F: a cross-system benchmark for formal Olympiad-level mathematics', 'publication_date_yy_mm': '2021-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6664.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6664.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-f/PACT (Lean)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-f finetuned with the PACT methodology applied to Lean</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 700M-parameter decoder-only transformer trained with a proof-artifact co-training (PACT) methodology on Lean's mathlib, used to predict tactics and guide best-first search; evaluated on miniF2F Lean problems and shown to leverage high-level Lean tactics for much higher pass rates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Proof artifact co-training for theorem proving</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-f/PACT (Lean)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>700M learnable parameters</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pretrained on WebMath and then trained on datasets extracted from the Lean mathlib library using the PACT methodology (mix2 > mix1 + tactic as described in Han et al.); trained to predict tactic applications and proof artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>miniF2F (Lean partition)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>formal theorem proving in Lean (algebra and number theory; supplying tactic arguments for high-level solvers like nlinarith, ring, norm_num)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>formal Lean statements; model outputs tactic applications and arguments (tactic script fragments)</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Olympiad-level and MATH levels 1–5 (varied); many solved problems are short due to tactic automation</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Finetuned next-token/tactic prediction with PACT; integrated with a best-first search over tactic states (GPT-f suggests tactic actions, search uses model probabilities to guide exploration); attempts per expansion e=16 noted</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Pass@N (Pass@1, Pass@8) and average tactic sequence length</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>miniF2F-test: Pass@1 = 24.6%, Pass@8 = 29.2%; average proof length = 2.5 tactics (for proofs found)</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Authors observe that the model learns to generate tactic calls and the often-missing arguments required by high-level tactics (e.g., providing premises to nlinarith). This behavior is central: the model leverages Lean's automation to compress many reasoning steps into single tactic calls, and so internal operation is effectively 'generate premises/arguments for automation' rather than performing all low-level algebra itself.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Tends to fail on problems that require multiple (>~2) nontrivial mathematical reasoning steps outside the scope of a single high-level tactic, and on goals requiring cut-introduction / witness generation (e.g., producing a nontrivial existential witness). Some tactic calls require extraneous premises that are hard to generate and when not provided the tactic fails.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>No explicit model-size scaling curve reported; paper notes the large performance gap between Lean and Metamath applications is primarily due to access to high-level tactics, despite comparable compute and model sizes between setups.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MiniF2F: a cross-system benchmark for formal Olympiad-level mathematics', 'publication_date_yy_mm': '2021-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6664.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6664.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Lean tidy baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deterministic tidy baseline (Lean best-first tactic search)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deterministic, non-neural best-first search over a curated list of Lean tactics (including added high-level tactics) that applies tactics by priority and explores tactic-state space until finding a proof or budget exhaustion.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Tidy baseline (Lean)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>heuristic best-first search (non-neural)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>n/a</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>No ML training; uses a curated ordered list of tactics L augmented with high-level tactics HL = [nlinarith, linarith, ring_nf, norm_num]; priorities guide expansion.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>miniF2F (Lean partition)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>formal theorem proving in Lean (algebra, number theory)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>formal Lean statements; baseline applies tactics deterministically according to priority queue</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Olympiad-level and MATH levels 1–5 (varied)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Deterministic application of a prioritized tactic list with bounds on queue size, depth, and iteration budget (i_max); no learned prompting</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Pass rate (number solved / total) under iteration budgets; average proof length</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>miniF2F-test (example): Pass@1 = 18.0% (Lean tidy tactic), average proof length = 1.8 tactics; with the implemented best-first configuration, results vary with i_max (e.g., up to 44/244 solved at i_max=8 or 128 in some configurations shown in paper table).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Performs well on problems solvable by existing high-level tactics; short proof lengths indicate reliance on tactic automation rather than long search. Deterministic nature removes variance but is constrained by tactic coverage and timeouts.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Fails when no single available tactic (or short sequence) solves the goal, or when problems require generation of nontrivial intermediate lemmas/witnesses; limited by fixed tactic list and inability to invent new tactic arguments beyond deterministic exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Not applicable (non-neural); increasing iteration budget i_max yields diminishing returns once tactic coverage limits are reached.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MiniF2F: a cross-system benchmark for formal Olympiad-level mathematics', 'publication_date_yy_mm': '2021-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6664.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6664.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 (informal MATH comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 evaluated on the informal MATH benchmark (reported comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3 (a much larger decoder-only transformer) performance on the informal MATH benchmark is cited for comparison, showing low accuracy on algebra and number-theory problems when evaluated in the informal (natural-language) setting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models are few-shot learners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈175B parameters (noted in original GPT-3 paper; referred to as larger than GPT-f in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pretrained on large-scale web text corpora (including code and math-adjacent web content in general GPT-3 training), as reported in original GPT-3 work; miniF2F paper only notes GPT-3 is larger than GPT-f.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MATH (informal benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>informal mathematics problem-solving (natural-language algebra and number theory problems)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language problems and stepwise informal solutions (MATH dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>MATH difficulty levels 1–5; tasks often require multi-step reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Reported results from Hendrycks et al. (2021); prompting/evaluation details are in that paper (not fully specified in miniF2F), typically few-shot and formatting as per MATH evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy by category (percentage correct)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported in Hendrycks et al. (2021) and cited here: Algebra = 6.0% accuracy, Number Theory = 3.9% accuracy (informal setting, GPT-3)</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>No internal mechanistic analysis provided in miniF2F; comparison highlights that formal-system + tactic automation yields much higher effective performance than informal-generation alone.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Low performance on multi-step symbolic reasoning and algebra/number-theory problems in the informal setting; specific error modes not analyzed here.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Not analyzed in this paper; the cited comparison only notes GPT-3 is larger than GPT-f but achieves much lower informal MATH performance than GPT-f operating in a formal system with tactic automation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MiniF2F: a cross-system benchmark for formal Olympiad-level mathematics', 'publication_date_yy_mm': '2021-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6664.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6664.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>High-level Lean tactics (nlinarith, ring_nf, norm_num)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Lean high-level automated solvers/tactics (examples: nlinarith, ring_nf, norm_num)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Powerful automated tactics in Lean that solve broad classes of arithmetic/inequality/ring goals when provided with suitable premises; models can learn to provide the necessary premises and thus leverage them to solve problems compactly.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>nlinarith / ring_nf / norm_num (Lean tactics)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>Lean tactic automation (non-ML procedural solvers)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>n/a</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not ML-trained; built-in Lean automation tactics that encode decision procedures or normalizers for arithmetic, inequalities, and ring manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>miniF2F (Lean partition)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>solving arithmetic/inequality/ring goals within formal proofs</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>formal Lean goal states passed as inputs to tactics, often requiring additional premises or lemmas as arguments</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Varies; tactics handle many routine algebraic and arithmetic subgoals, compressing many low-level steps into single tactic calls</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Invoked by model-generated tactic calls and explicitly supplied premises/arguments (the model must generate these arguments); authors also added these tactics to the tidy baseline's high-priority list.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Indirect: enabling usage of these tactics contributes to large increases in pass rates (observationally measured by comparing Metamath vs Lean results and by ablation via tidy baseline configurations)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Quantitative effect not isolated to a single number, but Lean GPT-f achieves ~24.6% Pass@1 vs 1.3% for Metamath GPT-f; tidy baseline also benefits when high-level tactics are included (reported tidy pass rates).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Authors highlight that the ML model's role often becomes 'generate appropriate premises/arguments for the high-level tactic' rather than deriving every low-level algebraic step; this demonstrates a distinct mode of leveraging external symbolic procedures.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Tactics may fail if the model does not produce necessary extraneous premises/lemmas; model must invent these arguments 'ex nihilo' which is a frequent source of failure when not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Not reported; qualitative claim: access to high-level tactics yields outsized gains irrespective of model size when compared across formal systems with similar model/train compute.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MiniF2F: a cross-system benchmark for formal Olympiad-level mathematics', 'publication_date_yy_mm': '2021-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Generative language modeling for automated theorem proving <em>(Rating: 2)</em></li>
                <li>Proof artifact co-training for theorem proving <em>(Rating: 2)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 1)</em></li>
                <li>Measuring mathematical problem solving with the math dataset <em>(Rating: 2)</em></li>
                <li>Analysing mathematical reasoning abilities of neural models <em>(Rating: 2)</em></li>
                <li>Holist: An environment for machine learning of higher order logic theorem proving <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6664",
    "paper_id": "paper-7ba98b00a224094c09676090f5d6d69498f5b299",
    "extraction_schema_id": "extraction-schema-126",
    "extracted_data": [
        {
            "name_short": "GPT-f (Metamath)",
            "name_full": "GPT-f applied to Metamath",
            "brief_description": "A decoder-only transformer language model fine-tuned to predict Metamath proof steps and used with log-probability based proof search; evaluated on formal Olympiad-level problems in the Metamath formal system (miniF2F).",
            "citation_title": "Generative language modeling for automated theorem proving",
            "mention_or_use": "use",
            "model_name": "GPT-f (Metamath)",
            "model_family": "decoder-only transformer",
            "model_size": "700M learnable parameters",
            "training_data_description": "Pretrained on WebMath and then trained on an updated dump of the Metamath set.mm library (and similar synthetic datasets) to model low-level Metamath proof steps.",
            "benchmark_name": "miniF2F (Metamath partition)",
            "task_type": "formal theorem proving including low-level arithmetic manipulations (e.g., n-digit addition, ring arithmetic rewrites) and Olympiad-style algebra/number-theory problems",
            "problem_format": "formal symbolic statements in Metamath (machine-checkable proofs of goals via sequences of low-level proofsteps)",
            "difficulty_level": "Olympiad-level and MATH levels 1–5 (wide range; many problems require long low-level sequences)",
            "prompting_method": "Finetuned next-token prediction on proof-step sequences; log-probability-based proof search (sampling many proof attempts and ranking by model log-probability); model attempts e=16 tactics per expansion (per paper note)",
            "performance_metric": "Pass@N (Pass@1, Pass@8) and average proof length (number of proofsteps)",
            "performance_value": "miniF2F-test: Pass@1 = 1.3%, Pass@8 = 1.6%; average proof length = 20.3 steps (for proofs found)",
            "internal_analysis": "No mechanistic probes reported; qualitative analysis notes that Metamath's low-level proof format makes arithmetic and simple algebra proofs extremely long, and the model must learn many low-level rewrite patterns. Authors hypothesize that lack of high-level tactics forces the model to produce very long sequences and limits success.",
            "failure_modes": "Struggles when proofs require many low-level rewrite steps (e.g., n-digit addition or simple ring arithmetic become tedious); fails to efficiently rewrite goals into forms amenable to existing theorems; cannot rely on high-level automation, so search is bottlenecked by long action sequences.",
            "scaling_trend": "No explicit model-size scaling analysis reported for arithmetic; authors note compute deployed at training comparable to Lean setup, but performance is much lower due to absence of high-level tactics rather than model scale differences.",
            "uuid": "e6664.0",
            "source_info": {
                "paper_title": "MiniF2F: a cross-system benchmark for formal Olympiad-level mathematics",
                "publication_date_yy_mm": "2021-08"
            }
        },
        {
            "name_short": "GPT-f/PACT (Lean)",
            "name_full": "GPT-f finetuned with the PACT methodology applied to Lean",
            "brief_description": "A 700M-parameter decoder-only transformer trained with a proof-artifact co-training (PACT) methodology on Lean's mathlib, used to predict tactics and guide best-first search; evaluated on miniF2F Lean problems and shown to leverage high-level Lean tactics for much higher pass rates.",
            "citation_title": "Proof artifact co-training for theorem proving",
            "mention_or_use": "use",
            "model_name": "GPT-f/PACT (Lean)",
            "model_family": "decoder-only transformer",
            "model_size": "700M learnable parameters",
            "training_data_description": "Pretrained on WebMath and then trained on datasets extracted from the Lean mathlib library using the PACT methodology (mix2 &gt; mix1 + tactic as described in Han et al.); trained to predict tactic applications and proof artifacts.",
            "benchmark_name": "miniF2F (Lean partition)",
            "task_type": "formal theorem proving in Lean (algebra and number theory; supplying tactic arguments for high-level solvers like nlinarith, ring, norm_num)",
            "problem_format": "formal Lean statements; model outputs tactic applications and arguments (tactic script fragments)",
            "difficulty_level": "Olympiad-level and MATH levels 1–5 (varied); many solved problems are short due to tactic automation",
            "prompting_method": "Finetuned next-token/tactic prediction with PACT; integrated with a best-first search over tactic states (GPT-f suggests tactic actions, search uses model probabilities to guide exploration); attempts per expansion e=16 noted",
            "performance_metric": "Pass@N (Pass@1, Pass@8) and average tactic sequence length",
            "performance_value": "miniF2F-test: Pass@1 = 24.6%, Pass@8 = 29.2%; average proof length = 2.5 tactics (for proofs found)",
            "internal_analysis": "Authors observe that the model learns to generate tactic calls and the often-missing arguments required by high-level tactics (e.g., providing premises to nlinarith). This behavior is central: the model leverages Lean's automation to compress many reasoning steps into single tactic calls, and so internal operation is effectively 'generate premises/arguments for automation' rather than performing all low-level algebra itself.",
            "failure_modes": "Tends to fail on problems that require multiple (&gt;~2) nontrivial mathematical reasoning steps outside the scope of a single high-level tactic, and on goals requiring cut-introduction / witness generation (e.g., producing a nontrivial existential witness). Some tactic calls require extraneous premises that are hard to generate and when not provided the tactic fails.",
            "scaling_trend": "No explicit model-size scaling curve reported; paper notes the large performance gap between Lean and Metamath applications is primarily due to access to high-level tactics, despite comparable compute and model sizes between setups.",
            "uuid": "e6664.1",
            "source_info": {
                "paper_title": "MiniF2F: a cross-system benchmark for formal Olympiad-level mathematics",
                "publication_date_yy_mm": "2021-08"
            }
        },
        {
            "name_short": "Lean tidy baseline",
            "name_full": "Deterministic tidy baseline (Lean best-first tactic search)",
            "brief_description": "A deterministic, non-neural best-first search over a curated list of Lean tactics (including added high-level tactics) that applies tactics by priority and explores tactic-state space until finding a proof or budget exhaustion.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Tidy baseline (Lean)",
            "model_family": "heuristic best-first search (non-neural)",
            "model_size": "n/a",
            "training_data_description": "No ML training; uses a curated ordered list of tactics L augmented with high-level tactics HL = [nlinarith, linarith, ring_nf, norm_num]; priorities guide expansion.",
            "benchmark_name": "miniF2F (Lean partition)",
            "task_type": "formal theorem proving in Lean (algebra, number theory)",
            "problem_format": "formal Lean statements; baseline applies tactics deterministically according to priority queue",
            "difficulty_level": "Olympiad-level and MATH levels 1–5 (varied)",
            "prompting_method": "Deterministic application of a prioritized tactic list with bounds on queue size, depth, and iteration budget (i_max); no learned prompting",
            "performance_metric": "Pass rate (number solved / total) under iteration budgets; average proof length",
            "performance_value": "miniF2F-test (example): Pass@1 = 18.0% (Lean tidy tactic), average proof length = 1.8 tactics; with the implemented best-first configuration, results vary with i_max (e.g., up to 44/244 solved at i_max=8 or 128 in some configurations shown in paper table).",
            "internal_analysis": "Performs well on problems solvable by existing high-level tactics; short proof lengths indicate reliance on tactic automation rather than long search. Deterministic nature removes variance but is constrained by tactic coverage and timeouts.",
            "failure_modes": "Fails when no single available tactic (or short sequence) solves the goal, or when problems require generation of nontrivial intermediate lemmas/witnesses; limited by fixed tactic list and inability to invent new tactic arguments beyond deterministic exploration.",
            "scaling_trend": "Not applicable (non-neural); increasing iteration budget i_max yields diminishing returns once tactic coverage limits are reached.",
            "uuid": "e6664.2",
            "source_info": {
                "paper_title": "MiniF2F: a cross-system benchmark for formal Olympiad-level mathematics",
                "publication_date_yy_mm": "2021-08"
            }
        },
        {
            "name_short": "GPT-3 (informal MATH comparison)",
            "name_full": "GPT-3 evaluated on the informal MATH benchmark (reported comparison)",
            "brief_description": "GPT-3 (a much larger decoder-only transformer) performance on the informal MATH benchmark is cited for comparison, showing low accuracy on algebra and number-theory problems when evaluated in the informal (natural-language) setting.",
            "citation_title": "Language models are few-shot learners",
            "mention_or_use": "mention",
            "model_name": "GPT-3",
            "model_family": "decoder-only transformer",
            "model_size": "≈175B parameters (noted in original GPT-3 paper; referred to as larger than GPT-f in this paper)",
            "training_data_description": "Pretrained on large-scale web text corpora (including code and math-adjacent web content in general GPT-3 training), as reported in original GPT-3 work; miniF2F paper only notes GPT-3 is larger than GPT-f.",
            "benchmark_name": "MATH (informal benchmark)",
            "task_type": "informal mathematics problem-solving (natural-language algebra and number theory problems)",
            "problem_format": "natural-language problems and stepwise informal solutions (MATH dataset)",
            "difficulty_level": "MATH difficulty levels 1–5; tasks often require multi-step reasoning",
            "prompting_method": "Reported results from Hendrycks et al. (2021); prompting/evaluation details are in that paper (not fully specified in miniF2F), typically few-shot and formatting as per MATH evaluation",
            "performance_metric": "Accuracy by category (percentage correct)",
            "performance_value": "Reported in Hendrycks et al. (2021) and cited here: Algebra = 6.0% accuracy, Number Theory = 3.9% accuracy (informal setting, GPT-3)",
            "internal_analysis": "No internal mechanistic analysis provided in miniF2F; comparison highlights that formal-system + tactic automation yields much higher effective performance than informal-generation alone.",
            "failure_modes": "Low performance on multi-step symbolic reasoning and algebra/number-theory problems in the informal setting; specific error modes not analyzed here.",
            "scaling_trend": "Not analyzed in this paper; the cited comparison only notes GPT-3 is larger than GPT-f but achieves much lower informal MATH performance than GPT-f operating in a formal system with tactic automation.",
            "uuid": "e6664.3",
            "source_info": {
                "paper_title": "MiniF2F: a cross-system benchmark for formal Olympiad-level mathematics",
                "publication_date_yy_mm": "2021-08"
            }
        },
        {
            "name_short": "High-level Lean tactics (nlinarith, ring_nf, norm_num)",
            "name_full": "Lean high-level automated solvers/tactics (examples: nlinarith, ring_nf, norm_num)",
            "brief_description": "Powerful automated tactics in Lean that solve broad classes of arithmetic/inequality/ring goals when provided with suitable premises; models can learn to provide the necessary premises and thus leverage them to solve problems compactly.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "nlinarith / ring_nf / norm_num (Lean tactics)",
            "model_family": "Lean tactic automation (non-ML procedural solvers)",
            "model_size": "n/a",
            "training_data_description": "Not ML-trained; built-in Lean automation tactics that encode decision procedures or normalizers for arithmetic, inequalities, and ring manipulation.",
            "benchmark_name": "miniF2F (Lean partition)",
            "task_type": "solving arithmetic/inequality/ring goals within formal proofs",
            "problem_format": "formal Lean goal states passed as inputs to tactics, often requiring additional premises or lemmas as arguments",
            "difficulty_level": "Varies; tactics handle many routine algebraic and arithmetic subgoals, compressing many low-level steps into single tactic calls",
            "prompting_method": "Invoked by model-generated tactic calls and explicitly supplied premises/arguments (the model must generate these arguments); authors also added these tactics to the tidy baseline's high-priority list.",
            "performance_metric": "Indirect: enabling usage of these tactics contributes to large increases in pass rates (observationally measured by comparing Metamath vs Lean results and by ablation via tidy baseline configurations)",
            "performance_value": "Quantitative effect not isolated to a single number, but Lean GPT-f achieves ~24.6% Pass@1 vs 1.3% for Metamath GPT-f; tidy baseline also benefits when high-level tactics are included (reported tidy pass rates).",
            "internal_analysis": "Authors highlight that the ML model's role often becomes 'generate appropriate premises/arguments for the high-level tactic' rather than deriving every low-level algebraic step; this demonstrates a distinct mode of leveraging external symbolic procedures.",
            "failure_modes": "Tactics may fail if the model does not produce necessary extraneous premises/lemmas; model must invent these arguments 'ex nihilo' which is a frequent source of failure when not provided.",
            "scaling_trend": "Not reported; qualitative claim: access to high-level tactics yields outsized gains irrespective of model size when compared across formal systems with similar model/train compute.",
            "uuid": "e6664.4",
            "source_info": {
                "paper_title": "MiniF2F: a cross-system benchmark for formal Olympiad-level mathematics",
                "publication_date_yy_mm": "2021-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Generative language modeling for automated theorem proving",
            "rating": 2
        },
        {
            "paper_title": "Proof artifact co-training for theorem proving",
            "rating": 2
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 1
        },
        {
            "paper_title": "Measuring mathematical problem solving with the math dataset",
            "rating": 2
        },
        {
            "paper_title": "Analysing mathematical reasoning abilities of neural models",
            "rating": 2
        },
        {
            "paper_title": "Holist: An environment for machine learning of higher order logic theorem proving",
            "rating": 1
        }
    ],
    "cost": 0.014734249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>MINIF2F: A CROSS-SYSTEM BENCHMARK FOR FORMAL OLYMPIAD-LEVEL MATHEMATICS</h1>
<p>Kunhao Zheng<br>École Polytechnique<br>kunhao.zheng@polytechnique.edu</p>
<p>Jesse Michael Han<br>OpenAI<br>University of Pittsburgh<br>jessemichaelhan@openai.com</p>
<h2>Stanislas Polu</h2>
<p>OpenAI
spolu@openai.com</p>
<h4>Abstract</h4>
<p>We present miniF2F, a dataset of formal Olympiad-level mathematics problems statements intended to provide a unified cross-system benchmark for neural theorem proving. The miniF2F benchmark currently targets Metamath, Lean, Isabelle (partially) and HOL Light (partially) and consists of 488 problem statements drawn from the AIME, AMC, and the International Mathematical Olympiad (IMO), as well as material from high-school and undergraduate mathematics courses. We report baseline results using GPT- $f$ (Polu \&amp; Sutskever, 2020), a neural theorem prover based on GPT-3 (Brown et al., 2020) and provide an analysis of its performance. We intend for miniF2F to be a community-driven effort and hope that our benchmark will help spur advances in neural theorem proving.</p>
<h2>1 INTRODUCTION</h2>
<p>Shared benchmarks and datasets have historically played a crucial role in driving advances in largescale applications of deep learning, e.g. in computer vision (Deng et al., 2009) and natural language processing (Wang et al., 2019; Rajpurkar et al., 2016; Paperno et al., 2016). Neural theorem proving is a rapidly developing area which aims to apply techniques from deep learning to interactive theorem proving. To date, most contributions in this area have focused on individual theorem proving systems, each with a separately-implemented mathematics library and with results reported on a dataset-specific test split; examples include the HOList (Bansal et al., 2019a), CoqGym (Yang \&amp; Deng, 2019) and LeanStep (Han et al., 2021) theorem proving environments and benchmarks. However, benchmarks from this paradigm are not ideal for measuring the mathematical reasoning ability of neural theorem provers for several reasons. Library-specific train/test splits are siloed by construction, dependent on how theorems and lemmas are split in these libraries, and as such are not directly comparable across systems. Moreover, formal mathematics libraries are closer to software repositories than informal mathematical exposition, and many lemmas are implementation-specific artifacts without precise informal mathematical or cross-system translations.</p>
<p>To date, the neural theorem proving community has not organized its efforts around a cross-system benchmark. To address this need and to provide a common resource to research groups working on formal theorem proving, we present miniF2F, a unified cross-system benchmark of formal mathematics of progressively increasing difficulty, centering around Olympiad-level problem statements (AMC, AIME, IMO) as well as high-school and undergraduate maths classes. Both the content and name of miniF2F are inspired by the IMO Grand Challenge (Selsam et al., 2019): to build an AI that can win a gold medal in the International Mathematical Olympiad in a formal-to-formal (F2F) format. More precisely, the agent must receive IMO problems written in a formal mathematical format, and must produce a formal (i.e. machine-checkable) proof for that problem.</p>
<p>We intend for miniF2F to serve as a stepping stone for different formal systems towards the IMO Grand Challenge (Selsam et al., 2019), as it is end-to-end verifiable, cross-platform and spans a wide range of difficulty. While we report baseline results on miniF2F using GPT- $f$, a language model</p>
<p>based on GPT-3 which has been finetuned for theorem proving, language models are not a mandatory approach for Olympiad problems and this assumption is not reflected in miniF2F, preserving the generality and widespread applicability of the benchmark to systems similar to DeepHOL (Bansal et al., 2019a) or Holophrasm (Whalen, 2016).</p>
<h1>2 BACKGROUND AND RELATED WORK</h1>
<h2>BENCHMARKS</h2>
<p>In the closely related field of (first-order) automated theorem proving (ATP), the TPTP (Sutcliffe, 2017) benchmark is a library of test problems in a unified format for ATP systems. In interactive theorem proving, the "Freek 100" (Wiedijk, 2008) tracks progress across various interactive theorem provers on a list of 100 mathematical theorems. Wu et al. (2021) built a simplified formal proof environment INT with an associated synthetic inequality benchmark. Competitions and communal challenges have also spurred development in formal theorem proving. The CADE ATP System Competition (CASC) (Sutcliffe, 2016) is a competition that evaluates the performance of first-order automated theorem proving systems. Proof Ground (Haslbeck et al., 2019), part of the ITP conference, is an interactive proving contest (for humans) that supports Coq, Isabelle, and Lean, which focuses on evaluating the formalization effort of proof to given problems within limited time. Finally, the IMO Grand Challenge (Selsam et al., 2019), a proposal from researchers working on the interactive proof assistant Lean, aims to build a system capable of solving IMO problems in the formal-to-formal format.</p>
<p>Due to its convenient framing as a natural language processing task, the domain of informal mathematical reasoning has received more attention than the formal one. MATH (Hendrycks et al., 2021) is a mathematics benchmark comprising 12,500 statements in natural language where exercises are classified into 5 levels of difficulty across various domains. Each exercise is combined with a detailed step-by-step proof in natural language. Scaling state-of-the-art models shows little amelioration on MATH, which requires advanced mathematical reasoning capabilities. miniF2F includes a number of formalized statements from MATH. NaturalProofs (Welleck et al., 2021) is another benchmark of natural proof in mathematics, containing 32 k theorem statements and proofs. It essentially contains the proofs in ProofWiki and other resources. While MATH is more oriented towards mathematics exercises, NaturalProofs is focused on proofs of general mathematics theorems. Saxton et al. (2019) built a mathematics dataset with $2 \times 10^{6}$ training data and $10^{4}$ test data, presented in a question-answering format where each statement is paired with a question written in natural language and a direct answer without proof.</p>
<h2>NEURAL THEOREM PROVING</h2>
<p>HOList (Bansal et al., 2019a;b; Paliwal et al., 2020) provides an environment as well as a benchmark for HOL Light. They also proposes various deep reinforcement learning approaches for theorem proving and report a pass rate of $59.91 \%$ on their benchmark. Yang \&amp; Deng (2019) built CoqGym, a large-scale dataset, which comes also with a learning environment, of 71 k human-written proofs in Coq proof assistant. They report a $30.0 \%$ pass rate on the held-out test theorems in CoqGym. Polu \&amp; Sutskever (2020) applied a decoder-only transformer similar to GPT-3 (Brown et al., 2020) to proof steps prediction in Metamath combined with a log-probability based proof search. They also proposed a methodology to train a value function to further guide proof search, achieving a $56.22 \%$ pass rate on the held-out test set. Large language models were applied to Lean by Han et al. (2021). They created an environment around the Lean prover targeted to machine learning and propose a dataset extracted from low level proof artifacts that is shown to boost performance when used as a self-supervised co-training objective. They report a $48.4 \%$ pass rate on held-out test statements from mathlib, Lean's mathematical library (mathlib Community, 2020).</p>
<h2>3 MINIF2F BENCHMARK</h2>
<p>miniF2F is a dataset of manually formalized statements of Olympiad type problems, aligned in Lean, Metamath, and Isabelle (partial at the time of writing), providing a cross-platform benchmark for formal mathematical reasoning. Olympiad type problems are of particular interest to compare</p>
<p>Table 1: Number of statements and their provenance in miniF2F v1</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Test Set</th>
<th style="text-align: center;">Validation Set</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">TOTAL</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">244</td>
<td style="text-align: center;">244</td>
</tr>
<tr>
<td style="text-align: center;">IMO</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">20</td>
</tr>
<tr>
<td style="text-align: center;">AIME</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">15</td>
</tr>
<tr>
<td style="text-align: center;">AMC</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">45</td>
<td style="text-align: center;">45</td>
</tr>
<tr>
<td style="text-align: center;">MATH</td>
<td style="text-align: center;">Algebra</td>
<td style="text-align: center;">Level 5</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">14</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Level 4</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">14</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Level 3</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">14</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Level 2</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">14</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Level 1</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">14</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Number Theory</td>
<td style="text-align: center;">Level 5</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Level 4</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">11</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Level 3</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">11</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Level 2</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">11</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Level 1</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">11</td>
</tr>
<tr>
<td style="text-align: center;">CUSTOM</td>
<td style="text-align: center;">Algebra</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">18</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Number Theory</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Induction</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">8</td>
</tr>
</tbody>
</table>
<p>automated provers across different formal systems as the theories required to solve them are well identified and they generally do not require the definition of new mathematical concepts (a capability that remains beyond the current neural theorem proving state of the art).</p>
<p>The formalized statements in miniF2F are drawn from multiple sources, ranging from high school and undergraduate level exercises to Olympiad problems. miniF2F also covers different subsubjects in mathematics as well as proof strategies, focusing on the types of exercises whose statements are expressible in most formal systems. This leads to a systemic focus on algebra, number theory and inequalities because, for example, geometry and combinatorial problems are generally challenging to formalize due to only nascent efforts in these areas in most formal systems. The statements in miniF2F are all manually formalized and selected to cover a variety of difficulty levels for both humans and machines. Formal proofs for these statements are optionally attached.
miniF2F draws from AIME, AMC, IMO problems as well as problems from the MATH (Hendrycks et al., 2021) informal dataset. Formalizing problems from the MATH dataset serves two purposes. First, problems in MATH are segmented by difficulty level (from 1 to 5), randomly selecting a subset from each of these difficulty levels allows miniF2F to cover a wider range of difficulty. Second, it provides the community an opportunity to compare capabilities of formal automated prover to their informal counter-parts as discussed in later sections.
miniF2F comprises a test set and a validation set, which are a stratified random split from the statements we formalized such that each set equally covers each problem type and difficulty (when available). Table 1 shows a detailed distribution of these statements.</p>
<p>Versioning miniF2F is an evolving effort and new statements will continuously be added. Periodically, we will freeze versions of the benchmark. The current version of the benchmark is v1 ${ }^{1}$ and results in this paper are reported using this version. v1 comprises 244 test and 244 valid statements. The set of statements of each version is guaranteed to remain stable, only allowing fixes in case errors are later discovered.</p>
<p>Rules of engagement and License miniF2F is meant to serve as a shared resource for research groups working on applying deep learning to formal theorem proving. There is no formal process to submit evaluation results and researchers are simply invited to cite miniF2F indicating the version used in their evaluations. We also encourage them to contribute proofs found by their approaches back to the benchmark. The parts of the benchmark associated with each theorem prover (Metamath,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Lean, Isabelle) are meant to be licensed in a way that is aligned with the licensing usage associated with the theorem prover's main library. As a result, the Metamath version of the benchmark is released under the MIT License, while the Lean and Isabelle versions are released under the Apache License.</p>
<p>Formalization effort and challenges We found that, for trained practitioners (but not necessarily experts, including students recently introduced to formal systems), formalizing a statement takes about 15 minutes on average, and reviewing a formalized statement, about half of that on average. Note that not all exercises are directly or naturally formalizable. In particular, multi-choice questions, word problems, and exercises that require to explicit a witness or a set as part of the answer present interesting challenges:
multi-choice questions ${ }^{2}$ these problems are generally straightforwardly formalizable by reformulating the statement using the right answer only, and could be made "fair" in a competitive setup by formalizing all possible choices and running automated provers on all of them, attributing points only if a proof of the correct answer is provided.
word problems ${ }^{3}$ where significant information is presented in natural language generally require non-trivial efforts to be formalized. We generally formalized them by explicitly modeling the mathematics concepts and expression presented in natural language while attempting as best as possible to preserve the mathematical difficulty of the original problem. Sometime the formalization work is most of the difficulty associated with the original question; in such cases we would discard the problem entirely.
problems that require to explicit a set or witness ${ }^{4}$ (e.g. find all ... such that ...) are not directly formalizable. The best approximation we relied on for these was to formalize the statement with the witness or answer provided, turning such exercises into the generation of a proof that the answer is correct, and if needed, that it is the unique one-which is, at times, a much easier exercise. A non negligible portion of IMO problems are as such, which we foresee could become a challenge in the future, to fairly compare humans to automated proving systems in a competitive setup.</p>
<p>Porting effort In addition to Metamath, Lean, Isabelle (work in progress) and HOL Light (work in progress), we are eager to extend the coverage of miniF2F to Coq, and will welcome any effort in that direction or to extend miniF2F to further systems.</p>
<h1>4 EXPERIMENTS</h1>
<p>In this section, in order to study baseline performances associated with existing systems, we report pass rates achieved by GPT- $f$ (Polu \&amp; Sutskever, 2020) applied to Metamath, GPT- $f /$ PACT (Polu \&amp; Sutskever, 2020; Han et al., 2021) applied to Lean as well as a baseline prover implemented in Lean denoted as the tidy baseline. Pass rates are reported as Pass@ $N$ where $N$ is the number of proof search attempts per statement. Pass@ $N$ is computed by running more attempts per statement, averaged to get an unbiased, low-variance estimate.</p>
<h3>4.1 METAMATH</h3>
<p>Metamath is powered by a meta logic system based on a single substitution rule. It's characterized by its simplicity which makes it convenient to study machine learning. Proofs in Metamath are, as a consequence of the low-level proofsteps, much longer than in other systems as there is no assistance from high-level tactics. Proofs which are trivial in other systems (e.g. n-digit addition or simple ring arithmetic transformations) can be quite tedious in Metamath. The absence of tactics is both</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Counts of successfully proved statements in miniF2F. Green bar: results from Lean GPT-f. Red bar: best result from the tidy baseline. Blue bar: results from Metamath GPT-f.
a benefit, as the models sees and learns on everything, and a challenge, as proofs of even simple exercises require hundreds of proofsteps.</p>
<h1>4.1.1 GPT-F</h1>
<p>We report the pass rate of GPT- $f$ applied to Metamath as described in Polu \&amp; Sutskever (2020). We use a model with 700 m learnable parameters. The model is trained on an updated dump of the set.mm library (but similar synthetic datasets), using the log-probability based search as reported in Table 8 of the GPT- $f$ paper (Polu \&amp; Sutskever, 2020).</p>
<p>The model achieves a Pass@1 of $1.3 \%$ and a Pass@8 of $1.6 \%$ on miniF2F-test. As expected, these numbers are quite low due to the length of typical proofs for even simple math exercises. The average proof length is also reported in Table 3.</p>
<h3>4.2 LEAN</h3>
<p>In comparison to Metamath, Lean benefits from a large number of powerful tactics to assist formalization efforts. Typical Lean proofs are much shorter than Metamath's. This is also a formal system of interest as it has received a lot of attention from the mathematical community as recent theories have successfully been formalized in Lean (Perfectoid Spaces (Buzzard et al., 2019), Liquid Tensor experiment (Scholze, 2020)).</p>
<p>Lean is also associated with the IMO Grand Challenge (Selsam et al., 2019) which aims to organize a formal-to-formal challenge during the upcoming IMO competitions.</p>
<h3>4.2.1 TIDY BASELINE</h3>
<p>We use the generic best-first search algorithm presented in PACT (Han et al., 2021). The algorithm works as follows: Given a list of tactics $L$ with priority, we maintain a priority queue $Q$ of tactic states whose priority is given by the priority of the last applied tactic in $L$ that led to it. While $Q$ is not empty, we pop the top tactic state $t$ from $Q$. We iterate through $L$ and apply each tactic to $t$. If no error is raised, we capture the returned tactic states from Lean and insert them back into $Q$.</p>
<p>We use the same terminology as in PACT (Han et al., 2021): maximum queue size $\omega_{\max }$, depth limit $d_{\max }$. We also enforce a budget of $i_{\max }$ iterations of the outer loop. When $Q$ 's size reach $q_{\max }$, all the tactic states to be inserted are discarded. We do not expand the next tactic state when the depth is beyond $d_{\max }$. This loop is run until a proof is found or the iterations budget is exhausted.</p>
<p>For consistency checking, we run the tidy baseline under the same settings and on the same test set as in PACT (Han et al., 2021) except that we don't set a global timeout. Our implementation</p>
<p>achieved a 10.5\% pass rate on mathlib's test split. This result is comparable to the reported $9.9 \%$ in PACT given the waived global timeout.</p>
<p>In addition to the curated list of tactics $L$ used in PACT (Han et al., 2021), we added 4 high-level tactics $H L=[n l i n a r i t h$, linarith, ring_nf, norm_num] to $L$ with higher priorities than the others. We report our pass rate on miniF2F in Table 2.</p>
<p>Table 2: The table shows the number of solved statement in miniF2F when running the tidy baseline with different values of $i_{\max }$ as well Lean's built-in tidy tactic. All tidy baseline experiments are run with $\omega_{\max }=128, d_{\max }=8$ using $L+H L$. Despite the tidy baseline being deterministic, it is still subject to per-tactic application timeouts, explaining the number 43 reported on miniF2F-test for $i_{\max }=32$.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">parameters</th>
<th style="text-align: center;">miniF2F-valid</th>
<th style="text-align: center;">miniF2F-test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Lean's tidy tactic</td>
<td style="text-align: center;">$12 / 244$</td>
<td style="text-align: center;">$13 / 244$</td>
</tr>
<tr>
<td style="text-align: left;">$i_{\max }=1$</td>
<td style="text-align: center;">$21 / 244$</td>
<td style="text-align: center;">$23 / 244$</td>
</tr>
<tr>
<td style="text-align: left;">$i_{\max }=2$</td>
<td style="text-align: center;">$31 / 244$</td>
<td style="text-align: center;">$29 / 244$</td>
</tr>
<tr>
<td style="text-align: left;">$i_{\max }=4$</td>
<td style="text-align: center;">$38 / 244$</td>
<td style="text-align: center;">$41 / 244$</td>
</tr>
<tr>
<td style="text-align: left;">$i_{\max }=8$</td>
<td style="text-align: center;">$41 / 244$</td>
<td style="text-align: center;">$44 / 244$</td>
</tr>
<tr>
<td style="text-align: left;">$i_{\max }=16$</td>
<td style="text-align: center;">$41 / 244$</td>
<td style="text-align: center;">$44 / 244$</td>
</tr>
<tr>
<td style="text-align: left;">$i_{\max }=32$</td>
<td style="text-align: center;">$41 / 244$</td>
<td style="text-align: center;">$43 / 244$</td>
</tr>
<tr>
<td style="text-align: left;">$i_{\max }=64$</td>
<td style="text-align: center;">$41 / 244$</td>
<td style="text-align: center;">$44 / 244$</td>
</tr>
<tr>
<td style="text-align: left;">$i_{\max }=128$</td>
<td style="text-align: center;">$41 / 244$</td>
<td style="text-align: center;">$44 / 244$</td>
</tr>
</tbody>
</table>
<h1>4.2.2 GPT-F/PACT</h1>
<p>We report the pass rate of GPT- $f /$ PACT as described in Han et al. (2021). We use a model with 700M learnable parameters. The model is trained on an updated dump ${ }^{76}$ of the mathlib library using the PACT methodology denoted in the paper as mix2 &gt; mix1 + tactic in Figure 6.</p>
<p>The model achieves a Pass@1 of $24.6 \%$ and a Pass@8 of $29.2 \%$ on miniF2F-test. The average proof length is also reported in Table 3.</p>
<p>Table 3: Baseline performance on Metamath and Lean. All proof searches are provided with a 128 expansions budget. GPT- $f$ attempts $e=16$ tactics per expansion while the tidy baseline attempts $e=17$ tactics per expansion ( $L+H L$, see section 4.2.1). Reported proof lengths are averages over all the proofs found in each run. Note that the tidy baseline being deterministic, there is no point attempting a proof search more than once.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">miniF2F-valid</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">miniF2F-test</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Formal</td>
<td style="text-align: center;">Model</td>
<td style="text-align: center;">Proof</td>
<td style="text-align: center;">Pass rate</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Proof</td>
<td style="text-align: center;">Pass rate</td>
</tr>
<tr>
<td style="text-align: center;">System</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Length</td>
<td style="text-align: center;">Pass@1</td>
<td style="text-align: center;">Pass@8</td>
<td style="text-align: center;">Length</td>
<td style="text-align: center;">Pass@1</td>
</tr>
<tr>
<td style="text-align: center;">Metamath</td>
<td style="text-align: center;">GPT- $f$</td>
<td style="text-align: center;">16.2</td>
<td style="text-align: center;">$1.0 \%$</td>
<td style="text-align: center;">2.0\%</td>
<td style="text-align: center;">20.3</td>
<td style="text-align: center;">$1.3 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Lean</td>
<td style="text-align: center;">tidy</td>
<td style="text-align: center;">1.7</td>
<td style="text-align: center;">16.8\%</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">1.8</td>
<td style="text-align: center;">18.0\%</td>
</tr>
<tr>
<td style="text-align: center;">Lean</td>
<td style="text-align: center;">GPT- $f$</td>
<td style="text-align: center;">2.6</td>
<td style="text-align: center;">23.9\%</td>
<td style="text-align: center;">29.3\%</td>
<td style="text-align: center;">2.5</td>
<td style="text-align: center;">24.6\%</td>
</tr>
</tbody>
</table>
<h3>4.3 DISCUSSION</h3>
<h3>4.3.1 ACCESS TO HIGH-LEVEL TACTICS</h3>
<p>One goal of miniF2F is to study the comparison of performance across formal systems. In this section we reported the performance of the same methodology (GPT- $f$ (Polu \&amp; Sutskever, 2020))</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>applied to both Lean and Metamath. Both models are pre-trained on WebMath (Polu \&amp; Sutskever, 2020) and respectively trained on datasets extracted from Lean (Han et al., 2021) and Metamath (Polu \&amp; Sutskever, 2020). The overall compute deployed at training is comparable in both setup and exactly equivalent at test-time, yet the achieved performance appears drastically superior when applied to Lean. We hypothesize that this is mainly explained by the model's access to highlevel tactics when applied to Lean, enabling the model to learn how to guide Lean's automation in an effective way.</p>
<p>An example of this high-level guidance behavior is well exemplified by the following proof of the statement algebra_sqineq_2unitcircatblt1 where the model heavily relies on Lean's nlinarith solver but provides it with essential premises to successfully guide the search.</p>
<div class="codehilite"><pre><span></span><code><span class="n">theorem</span><span class="w"> </span><span class="n">algebra_sqineq_2unitcircatblt1</span>
<span class="w">    </span><span class="p">(</span><span class="n">a</span><span class="w"> </span><span class="n">b</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="p">\</span><span class="n">mathbb</span><span class="p">{</span><span class="n">R</span><span class="p">})</span>
<span class="w">    </span><span class="p">(</span><span class="n">h</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">a</span><span class="o">^</span><span class="mi">2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">b</span><span class="o">^</span><span class="mi">2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="w"> </span><span class="o">:</span>
<span class="w">    </span><span class="n">a</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">b</span><span class="w"> </span><span class="p">\</span><span class="n">leq</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="o">:=</span>
<span class="n">begin</span>
<span class="w">    </span><span class="n">nlinarith</span><span class="w"> </span><span class="p">[</span><span class="n">sq_nonneg</span><span class="w"> </span><span class="n">a</span><span class="o">,</span><span class="n">sq_nonneg</span><span class="w"> </span><span class="n">b</span><span class="o">,</span><span class="n">sq_nonneg</span><span class="w"> </span><span class="p">(</span><span class="n">a</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">b</span><span class="p">)]</span>
<span class="k">end</span>
</code></pre></div>

<p>(The statement above (algebra_sqineq_2unitcircatblt1) requires to prove the assertion $\forall a, b \in \mathbb{R}, a^{2}+b^{2}=2 \rightarrow a \cdot b \leq 1$ ).</p>
<p>In Metamath, GPT- $f$ fails to find a proof as it requires a very large number of steps to appropriately rewrite the goal in a way that is amenable to the use of set.mm's existing theorems. The tidy baseline also fails to find a proof of that statement as nlinarith is not capable of solving the goal without being passed extraneous premises.</p>
<p>These results motivate the use of neural theorem proving with formal systems that expose powerful high level tactics and also suggest the potential of a closer collaboration between formal systems and machine learning practitioners. It also motivates the use of generative models in that setup as the arguments required by high-level tactics to succeed on non trivial problems generally do not exist in the context of the statement and therefore have to be generated ex-nihilo.</p>
<h1>4.3.2 COMPARISON OF INFORMAL AND FORMAL SETUPS</h1>
<p>The use of formal systems for neural theorem proving is often motivated by the role of the formal system as a verifier, enabling more advanced neural search strategies than possible in a fully informal setup where the generation of a model can't be verified automatically, as well as the access to powerful tactics. Our formalization of a subset of the MATH (Hendrycks et al., 2021) informal dataset provides an interesting approximate quantification of the benefit of having access to a formal system in the context of neural theorem proving. Approximate, because we only formalized a small subset of the MATH statements, but nonetheless useful since we drew uniformly from the 5 difficulty levels.</p>
<p>In Hendrycks et al. (2021), the performance of GPT-3 (which is a larger model than the GPT-f model studied here) is reported to be $6.0 \%$ in the algebra category and $3.9 \%$ in the number theory category. GPT- $f$ applied to Lean by comparison achieves $51.4 \%$ in the algebra category and $41.7 \%$ in the number theory category. It is also worthwhile to note that the tidy baseline also highly outperforms ( $31.4 \%$ in algebra and $30.0 \%$ in number theory) GPT-3 in an informal setup demonstrating the benefit of proof automation alone.</p>
<h3>4.3.3 Limitation</h3>
<p>With miniF2F being cross-system as the goal, types of problems that are less expressible in certain systems such as geometry and combinatorial problems are less covered. The shift of distribution of problem types may result in skewing the research direction of models when benchmarking on miniF2F. Directionally we aim to fix it and extend the coverage of miniF2F as we grow the benchmark. However, works and efforts on the corresponding library of other systems are required as well.</p>
<h1>5 CONCLUSION</h1>
<p>We presented miniF2F, a dataset of formal Olympiad-level mathematics problem statements, meant to serve as an initial effort towards cross-system benchmarking of neural mathematical reasoning capabilities in formal environments. We reported the performance of the neural theorem prover GPT- $f$ (Polu \&amp; Sutskever, 2020) on both the Lean and Metamath parts of miniF2F as well as the performance of our non-neural tidy baseline applied to Lean. Then, we discussed these baselines and put them in perspective with previously reported comparable results in informal environments (Hendrycks et al., 2021).</p>
<p>Finally, we hope that miniF2F will prove to be useful to the scientific community working on neural theorem proving and spur advances in this domain.</p>
<h2>ACKNOWLEDGMENTS</h2>
<p>We are grateful to Wenda Li and Xavier Martinet for contributing the Isabelle and HOL Light statements currently available in miniF2F, paving the way towards a full support of Isabelle and HOL Light, as well as their feedback and encouragement in the process. We thank Harri Edwards for his comments that greatly improved the manuscript.</p>
<h2>REFERENCES</h2>
<p>Kshitij Bansal, Sarah Loos, Markus Rabe, Christian Szegedy, and Stewart Wilcox. Holist: An environment for machine learning of higher order logic theorem proving. In International Conference on Machine Learning, pp. 454-463. PMLR, 2019a.</p>
<p>Kshitij Bansal, Christian Szegedy, Markus N Rabe, Sarah M Loos, and Viktor Toman. Learning to reason in large theories without imitation. arXiv preprint arXiv:1905.10501, 2019b.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1877-1901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/ 1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.</p>
<p>Kevin Buzzard, Johan Commelin, and Patrick Massot. Lean perfectoid spaces. https:// leanprover-community.github.io/lean-perfectoid-spaces/, 2019.</p>
<p>Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248-255. Ieee, 2009.</p>
<p>Jesse Michael Han, Jason Rute, Yuhuai Wu, Edward W Ayers, and Stanislas Polu. Proof artifact co-training for theorem proving with language models. arXiv preprint arXiv:2102.06203, 2021.</p>
<p>Maximilian P. L. Haslbeck, Tobias Nipkow, and Simon Wimmer. Proof ground. https: //www21.in.tum.de/ wimmers/proofground/, 2019.</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.</p>
<p>The mathlib Community. The lean mathematical library. In Jasmin Blanchette and Catalin Hritcu (eds.), Proceedings of the 9th ACM SIGPLAN International Conference on Certified Programs and Proofs, CPP 2020, New Orleans, LA, USA, January 20-21, 2020, pp. 367-381. ACM, 2020. doi: 10.1145/3372885.3373824. URL https://doi.org/10.1145/3372885. 3373824 .</p>
<p>Aditya Paliwal, Sarah Loos, Markus Rabe, Kshitij Bansal, and Christian Szegedy. Graph representations for higher-order logic and theorem proving. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 2967-2974, 2020.</p>
<p>Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. The LAMBADA dataset: Word prediction requiring a broad discourse context. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers. The Association for Computer Linguistics, 2016. doi: 10.18653/v1/ p16-1144. URL https://doi.org/10.18653/v1/p16-1144.</p>
<p>Stanislas Polu and Ilya Sutskever. Generative language modeling for automated theorem proving. arXiv preprint arXiv:2009.03393, 2020.</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100, 000+ questions for machine comprehension of text. In Jian Su, Xavier Carreras, and Kevin Duh (eds.), Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016, pp. 2383-2392. The Association for Computational Linguistics, 2016. doi: 10.18653/v1/d16-1264. URL https://doi.org/10.18653/v1/ d16-1264.</p>
<p>David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical reasoning abilities of neural models. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id=H1gR5iR5FX.</p>
<p>Peter Scholze. Liquid tensor experiment. https://xenaproject.wordpress.com/2020/ 12/05/liquid-tensor-experiment/, 2020.</p>
<p>Daniel Selsam, Kevin Buzzard, Reid Barton, Percey Liang, Sarah Loss, and Freek Wiedijk. Imo grand challenge. https://imo-grand-challenge.github.io/, 2019.
G. Sutcliffe. The CADE ATP System Competition - CASC. AI Magazine, 37(2):99-101, 2016.
G. Sutcliffe. The TPTP Problem Library and Associated Infrastructure. From CNF to TH0, TPTP v6.4.0. Journal of Automated Reasoning, 59(4):483-502, 2017.</p>
<p>Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id= rJ4km2R5t7.</p>
<p>Sean Welleck, Jiacheng Liu, Ronan Le Bras, Hannaneh Hajishirzi, Yejin Choi, and Kyunghyun Cho. Naturalproofs: Mathematical theorem proving in natural language. arXiv preprint arXiv:2104.01112, 2021.</p>
<p>Daniel Whalen. Holophrasm: a neural automated theorem prover for higher-order logic. CoRR, abs/1608.02644, 2016. URL http://arxiv.org/abs/1608.02644.</p>
<p>Freek Wiedijk. Formalizing 100 theorems. https://www.cs.ru.nl/ freek/100/, 2008.
Yuhuai Wu, Albert Jiang, Jimmy Ba, and Roger Baker Grosse. INT: an inequality benchmark for evaluating generalization in theorem proving. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id=O6LPudowNQm.</p>
<p>Kaiyu Yang and Jia Deng. Learning to prove theorems via interacting with proof assistants. In International Conference on Machine Learning, pp. 6984-6994. PMLR, 2019.</p>
<h1>A EXAMPLE OF STATEMENT IN MINIF2F</h1>
<p>Table 4: Problem 11 of 2000 AMC 12 is formalized with proof in different languages in miniF2F. The proof is optionally attached thus not part of the benchmark. The proof in Metamath is too long to be fully displayed.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Natural <br> Language</th>
<th style="text-align: center;">Two non-zero real numbers, $a$ and $b$, satisfy $a b=a-b$. Which of the following is a possible value of $\frac{a}{b}+\frac{b}{a}-a b$ ? (A) -2 (B) $\frac{-1}{2}$ (C) $\frac{1}{3}$ (D) $\frac{1}{2}$ (E) 2</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Metamath</td>
<td style="text-align: center;">```${</td>
</tr>
<tr>
<td style="text-align: center;">amc12-2000-p11.0 $e</td>
<td style="text-align: center;">- ( ph -&gt; A e. RR ) $. amc12-2000-p11.1 $e</td>
</tr>
<tr>
<td style="text-align: center;">Lean</td>
<td style="text-align: center;"><code>theorem amc12_2000_p11 (a b : \mathbb{R}) (h0 : a \neq 0 \wedge b \neq 0) (h1 : a * b = a - b) : a / b + b / a - a * b = 2 := begin field_simp [h0.1, h0.2], simp only [h1, mul_comm, mul_sub], ring, end</code></td>
</tr>
<tr>
<td style="text-align: center;">Isabelle</td>
<td style="text-align: center;"><code>theorem amc12_2000_p11: fixes a b::real assumes "a \&lt;noteq&gt; 0" "b \&lt;noteq&gt; 0" and "a * b = a - b" shows "a / b + b / a - a * b = 2" using assms by (smt (verit, ccfv_threshold) diff_divide_distrib div_self divide_divide_times_eq eq_divide_imp nonzero_mult_div_cancel_left) end</code></td>
</tr>
</tbody>
</table>
<h1>B PERFORMANCE BY DIFFICULTY ON STATEMENTS FORMALIZED FROM MATH DATASET</h1>
<p>The MATH dataset assigns a difficulty ranging from 1 to 5 to each of its problem. Tables 5 and 6 report the number of proved statement split by difficulty level on the algebra and number theory categories.</p>
<p>Table 5: Counts of successfully proved statements formalized from MATH-Algebra in miniF2F v1 split by difficulty. This table corresponds to "MATH Algebra" in Figure 1.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">miniF2F-valid</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">miniF2F-test</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Difficulty Level</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: center;">Metamath/GPT- $f$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">Lean/tidy</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">Lean/GPT- $f$</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">3</td>
</tr>
</tbody>
</table>
<p>Table 6: Counts of successfully proved statements formalized from MATH-Number theory in miniF2F v1 split by difficulty. This table corresponds to "MATH Number Theory" in Figure 1.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">miniF2F-valid</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">miniF2F-test</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Difficulty Level</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: center;">Metamath/GPT- $f$</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">Lean/tidy</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: center;">Lean/GPT- $f$</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">2</td>
</tr>
</tbody>
</table>
<p>More broadly, Lean GPT- $f$ is capable of solving any problem that the tidy baseline or Metamath GPT- $f$ can solve in MiniF2F. Qualitatively, the problems on which it fail either require multiple nontrivial reasoning steps (outside a few exceptions, problems requiring more than 2 non-trivial steps of mathematical reasoning are generally out of reach of these baselines) or require a cut introduction that is hard to generate, such as generating a non trivial witness.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{7}$ https://github.com/jasonrute/lean_proof_recording/commit/
8499f10c2e10dd533152070ed933c4f0b21ecdc0
${ }^{8}$ https://github.com/jesse-michael-han/lean-step-public/commit/
a2b83c237bfe4d6f1c48bb48bc0769b5940e614a&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
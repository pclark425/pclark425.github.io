<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7444 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7444</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7444</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-140.html">extraction-schema-140</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <p><strong>Paper ID:</strong> paper-258967365</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2305.18365v3.pdf" target="_blank">What can Large Language Models do in chemistry? A comprehensive benchmark on eight tasks</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) with strong abilities in natural language processing tasks have emerged and have been applied in various kinds of areas such as science, finance and software engineering. However, the capability of LLMs to advance the field of chemistry remains unclear. In this paper, rather than pursuing state-of-the-art performance, we aim to evaluate capabilities of LLMs in a wide range of tasks across the chemistry domain. We identify three key chemistry-related capabilities including understanding, reasoning and explaining to explore in LLMs and establish a benchmark containing eight chemistry tasks. Our analysis draws on widely recognized datasets facilitating a broad exploration of the capacities of LLMs within the context of practical chemistry. Five LLMs (GPT-4, GPT-3.5, Davinci-003, Llama and Galactica) are evaluated for each chemistry task in zero-shot and few-shot in-context learning settings with carefully selected demonstration examples and specially crafted prompts. Our investigation found that GPT-4 outperformed other models and LLMs exhibit different competitive levels in eight chemistry tasks. In addition to the key findings from the comprehensive benchmark analysis, our work provides insights into the limitation of current LLMs and the impact of in-context learning settings on LLMs' performance across various chemistry tasks. The code and datasets used in this study are available at https://github.com/ChemFoundationModels/ChemLLMBench.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7444.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7444.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ICL vs Zero-shot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Few-shot In-Context Learning (ICL) versus Zero-shot Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison of few-shot in-context learning prompts (ICL) with zero-shot prompts across chemistry tasks; ICL consistently improved performance over zero-shot, often substantially for tasks where demonstrations are informative.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (representative across GPT family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned large language model evaluated alongside GPT-3.5, Davinci-003, Llama, and Galactica; used as the main representative of GPT-series performance.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple chemistry tasks (aggregate across 8 tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Various chemistry benchmark tasks including name prediction, property prediction, yield prediction, reaction prediction, retrosynthesis, text-based molecule design, molecule captioning, and reagents selection.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot natural-language prompt vs few-shot in-context learning prompt (concatenated demonstration examples + task-specific template).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Zero-shot: single standardized instruction asking model to act as a chemist and return only output. ICL: task-specific template with {General Template, Task-Specific Template, ICL, Question}; demonstrations concatenated as '[Input]: [Input_content][Output]: [Output_content]'; retrieval strategy (Random or Scaffold) and k (number of examples) varied; experiments repeated 5 times to average randomness.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>varies by task (accuracy, F1, BLEU, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>ICL prompting improved performance in all tasks compared to zero-shot; example — Yield prediction (Buchwald-Hartwig): GPT-4 zero-shot 32.2% accuracy → GPT-4 few-shot (random, k=8) 80.0% accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Zero-shot example: 32.2% accuracy (Buchwald-Hartwig, GPT-4 zero-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+47.8% absolute (Buchwald-Hartwig example: 80.0% - 32.2%).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Five repeated evaluations; ICL k grid-searched per task (e.g., k in {4,8} for property/yield, k in {5,20} for name/reaction/retrosynthesis), retrieval strategies Random vs Scaffold.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What can Large Language Models do in chemistry? A comprehensive benchmark on eight tasks', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7444.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7444.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ICL example count (k)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Number of In-Context Demonstrations (k) impacts performance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Performance generally increases as the number of in-context examples (k) grows, with notable step-changes observed in several chemistry tasks (larger k often yields better results).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned large language model used in few-shot experiments varying demonstration count.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Yield prediction (Buchwald-Hartwig and Suzuki-Miyaura) and other tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary classification of reaction yield (high vs not-high) and other tasks where number of examples was varied.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot in-context learning with k demonstrations</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / prompt size</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Grid search on k; example ranges: yield prediction used k ∈ {4, 8}; name/reaction/retrosynthesis used k ∈ {5, 20}; in molecule design/captioning k ∈ {5,10} due to token limits.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Buchwald-Hartwig: GPT-4 random k=4 → 57.4% accuracy; k=8 → 80.0% accuracy. Suzuki-coupling: GPT-4 random k=4 → 32.4% accuracy; k=8 → 76.4% accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>k=4 results used as lower-k baseline (e.g., 57.4% for Buchwald).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Buchwald-Hartwig: +22.6% absolute (57.4 → 80.0); Suzuki: +44.0% absolute (32.4 → 76.4).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Random retrieval for these yield experiments; five repeated runs; other tasks also show monotonic improvements with larger k in validation grid search.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What can Large Language Models do in chemistry? A comprehensive benchmark on eight tasks', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7444.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7444.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ICL retrieval: Scaffold vs Random</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>In-context-example retrieval strategy: Scaffold (similarity-based) versus Random</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Selecting demonstrations by scaffold similarity (Tanimoto on Morgan fingerprints) generally yielded better ICL performance than random selection across many tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (and other GPT variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLMs evaluated with different demonstration retrieval strategies: Scaffold uses Tanimoto similarity on Morgan fingerprints for SMILES inputs; for textual inputs, difflib.SequenceMatcher string-similarity was used.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple tasks (name prediction, property prediction, yield prediction, reaction prediction, text-based design, captioning, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where demonstration-example selection can influence the relevance of examples to the query.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot ICL with either Random sampling of examples or Scaffold (similarity)-based retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / demonstration selection</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Scaffold retrieval: compute Tanimoto similarity with 2048-bit Morgan fingerprints (radius=2), choose top-k similar molecules as examples; Random: choose k examples uniformly at random. Grid search chose retrieval strategy per task using a 30-sample validation set.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>various (accuracy, F1, BLEU, exact match)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Authors report scaffold sampling outperforms random sampling in most tasks (cited tables: 4, 6, 7, 10, 11, 14, 15), e.g., validation-based selections favored scaffold retrieval for final test runs.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Random retrieval performance (task-dependent); specific numbers vary by task.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Qualitative improvement reported; exact numeric deltas depend on task (not uniformly quantified for every task in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Validation grid search across retrieval strategies and k, followed by 5 repeated test evaluations using top options.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What can Large Language Models do in chemistry? A comprehensive benchmark on eight tasks', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7444.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7444.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Label semantics in prompt (Property labels)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Inclusion of property label semantics in prompts for property prediction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Embedding the semantic meaning of target labels (e.g., 'inhibit HIV replication') into the prompt significantly improves model performance on property-classification tasks compared to omitting label context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned LLM evaluated on molecular property classification datasets with prompts that either included or omitted explicit label semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Molecule property prediction (HIV, ClinTox, BBBP, BACE, Tox21)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary classification of molecular properties (e.g., HIV activity, clinical toxicity).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language classification prompt that includes label descriptions (yes/no responses), versus an identical prompt with label semantics removed.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt content / semantic context</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Prompts with label context contained explicit human-interpretable descriptions of the labels (e.g., 'inhibit HIV replication' or 'drugs failed clinical trials for toxicity reason'); unlabelled variants removed these descriptions. Experiments run zero-shot and few-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>F1 score and accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>HIV: GPT-4 zero-shot F1 0.977 ± 0.013 (with label info) vs 0.554 ± 0.017 (unlabelled zero-shot); Accuracy 0.986 ± 0.070 vs 0.628 ± 0.016. Few-shot F1: 0.797 ± 0.021 (labelled) vs 0.493 ± 0.030 (unlabelled). ClinTox: zero-shot F1 0.489 ± 0.018 vs 0.438 ± 0.045 (unlabelled); few-shot F1 0.563 ± 0.008 vs 0.478 ± 0.035 (unlabelled).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Unlabelled prompt performance used as contrast/baseline (values above).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>HIV zero-shot F1 change: -0.423 absolute when label semantics removed; HIV accuracy change: -0.358 absolute; ClinTox F1 change smaller (~-0.051 zero-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot and few-shot settings; scaffold sampling used for selecting ICL examples in some experiments; results averaged over repeated runs.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What can Large Language Models do in chemistry? A comprehensive benchmark on eight tasks', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7444.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7444.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SMILES vs SELFIES</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Molecular string representation: SMILES compared to SELFIES</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparative evaluation of two molecule string formats (SMILES and SELFIES) as the input/output representation for LLMs; SMILES yielded better downstream performance for GPT-4 on most evaluated tasks although SELFIES produced fewer invalid generations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLM tested on tasks where molecules are represented either by SMILES strings (commonly used in corpora) or SELFIES (designed to be 100% robust).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Reaction prediction, Text-based molecule design, Molecule captioning</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Predicting reaction products, generating molecules from text, and producing textual captions/descriptions for molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Input and/or output molecules encoded as SMILES strings versus SELFIES strings.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>input modality / representation format</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Direct substitution of representation in prompts and demonstration examples; validity and task metrics computed using RDKit and fingerprint-based similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Top-1 accuracy, invalid-string rate, BLEU, Exact match, validity, fingerprint Tanimoto similarity (FTS), FCD, ROUGE/METEOR for captioning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reaction prediction: SMILES Top-1 0.230 ± 0.022, invalid SMILES 7.0% ± 1.6%; SELFIES Top-1 0.110 ± 0.007, invalid SELFIES 1.0% ± 0.0%. Molecule design: SMILES BLEU 0.816 ± 0.004, Exact 0.174 ± 0.029, Validity 0.888 ± 0.023 vs SELFIES BLEU 0.277 ± 0.009, Exact 0.100 ± 0.016, Validity 0.804 ± 0.022. Molecule captioning: small differences (BLEU-2/4, ROUGE, METEOR similar with slight SMILES advantage).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>SELFIES used as alternative baseline; SMILES generally outperformed SELFIES on high-level performance metrics but produced more invalid outputs in some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Reaction Top-1: SMILES +12.0 percentage points vs SELFIES (23.0% vs 11.0%); invalid rate: SELFIES -6.0 percentage points (1.0% vs 7.0%). Molecule design BLEU: SMILES +0.539 absolute (0.816 vs 0.277); Exact +0.074 absolute (0.174 vs 0.100); Validity +0.084 absolute (0.888 vs 0.804).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>GPT-4 evaluated on SMILES and SELFIES variants across four tasks; authors hypothesize pretraining corpora contain more SMILES and thus models are more attuned to SMILES.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What can Large Language Models do in chemistry? A comprehensive benchmark on eight tasks', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7444.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7444.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Temperature sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Impact of decoding temperature on LLM test performance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Variation of the temperature sampling parameter (0.2 to 1.0) had only marginal effects on model performance in the tested chemistry property tasks, indicating robustness to this decoding hyperparameter within the tested range.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned LLM; decoding temperature varied to assess randomness impact on outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Molecular property prediction (BBBP, BACE)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary property prediction using in-context learning (scaffold sampling, k=8 in this temperature analysis).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language classification prompt with temperature hyperparameter controlling sampling randomness.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>decoding hyperparameter</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Temperatures tested t ∈ {0.2, 0.4, 0.6, 0.8, 1.0}; experiments performed on a 30-sample validation subset and targeted tests on the 100-sample test sets for BBBP and BACE.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>F1 and accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>F1 (BBBP) ranged roughly 0.650–0.712 across temperatures; Accuracy (BACE) varied roughly 0.741–0.757. Observed fluctuations < 0.05 in F1/accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Validation-chosen temperature (varied by task); no single temperature dramatically outperformed others.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Marginal — variations under ~0.05 absolute in F1/accuracy across tested temperatures.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>GPT-4, scaffold sampling k=8, five repeated runs; temperature sweeps to conserve API usage then validated on test subsets.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What can Large Language Models do in chemistry? A comprehensive benchmark on eight tasks', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7444.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7444.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Problem formulation: classification/ranking vs generation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of task formulation (classification/ranking vs free-generation) on performance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLMs perform relatively well when problems are presented as classification or ranking choices, but struggle on generative tasks that require precise SMILES or IUPAC outputs; formulation strongly affects outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (with comparisons to GPT-3.5, Davinci-003, others)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLMs evaluated on tasks reformulated either as classification/ranking (e.g., yield high/low, select reagent from candidates) or generative sequence-to-sequence tasks (e.g., predict SMILES/products, name translation).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Yield prediction, Reagents selection (classification/ranking) vs Reaction prediction, Retrosynthesis, Name prediction (generation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Classification/ranking tasks require choosing among provided categories or candidates; generative tasks require producing exact textual molecular representations (SMILES, IUPAC) as outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Multiple-choice / top-k / binary classification prompts versus open-ended generation prompts requiring exact SMILES/IUPAC outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>question type / output constraint</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Classification/ranking prompts constrain choices or ask yes/no; generation prompts ask for free-form SMILES/IUPAC/structure output, with output restrictions sometimes included to reduce hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy, validity, exact match</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Classification example: reagents selection accuracy ~40–50% (GPT-4/GPT-3.5); Yield prediction GPT-4 (best ICL) 80% (Buchwald) and 76.4% (Suzuki) but baseline UAGNN 96.5%/95.7%. Generative example: name prediction best accuracy max 8% (non-competitive); reaction prediction zero-shot Top-1 accuracy 0.4% (0.004) with 17.4% invalid SMILES.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Baselines such as UAGNN for yield (96.5%/95.7%), Chemformer for reaction prediction (much higher Top-1 accuracy trained on full dataset) reported in paper; exact baseline numbers vary by task.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Generative tasks: GPT models underperform baselines by large margins (e.g., reaction/retrosynthesis ~40–70% lower than task-specific models); classification tasks: GPT models can be competitive though often below fully supervised baselines trained on many examples (e.g., yield: GPT-4 ~16–20% lower than UAGNN).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>When possible, tasks converted to classification/ranking (easier for LLMs); generative tasks included output restrictions and task-specific input explanations to mitigate hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What can Large Language Models do in chemistry? A comprehensive benchmark on eight tasks', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7444.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7444.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Task-specific templates & output restrictions</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Task-specific prompt templates including input explanations and output restrictions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using specialized prompt templates that provide input explanations and enforce output restrictions was used to reduce hallucination and guide LLMs toward chemically reasonable outputs; qualitative reduction in hallucinations reported though quantitative effect varies by task.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (and other evaluated LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLMs used with a standardized zero-shot template and a richer ICL template partitioned into General Template, Task-Specific Template (including Input Explanation, Output Explanation, Output Restrictions), ICL examples, and the Question.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>All chemistry tasks where hallucination risk exists (e.g., reaction prediction, name prediction, retrosynthesis, molecule design)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where precise chemical outputs are required and hallucinations are a concern.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Task-specific natural language templates that explicitly describe input format and required output format, plus constraints (e.g., only return SMILES; separate reactants by '.'; products by '>>').</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / constraints</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Templates included explicit delimiters, input explanations for SMILES formatting, and 'Output Restrictions' to force concise outputs and reduce extraneous text; demonstration examples adhere to the same format.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>validity, accuracy, qualitative hallucination reduction</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitatively reported reduction in hallucinations and more constrained outputs when using the task-specific template and output restrictions; no single global numeric delta reported (task-dependent).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Standardized zero-shot template without task-specific output restrictions (used as baseline in some comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Qualitative improvement in realism/faithfulness and reduction in some hallucinations; exact numeric changes are task-specific and not uniformly reported.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Templates applied across tasks; ICL examples formatted consistently; experiments repeated 5 times to mitigate randomness.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What can Large Language Models do in chemistry? A comprehensive benchmark on eight tasks', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7444.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7444.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (CoT) prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting (suggested, not executed)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper mentions advanced prompting techniques like Chain-of-Thought (CoT) and Decomposed Prompting as prospective methods to improve complex reasoning in chemistry tasks, but does not empirically evaluate them in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Suggested for complex reasoning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Potentially useful for multi-step reasoning tasks in chemistry</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Chain-of-Thought style prompts that encourage step-by-step reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style (mention)</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Authors propose CoT and Decomposed Prompting as promising directions; not implemented in experiments reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What can Large Language Models do in chemistry? A comprehensive benchmark on eight tasks', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Self-referencing embedded strings (SELFIES): A 100% robust molecular string representation <em>(Rating: 2)</em></li>
                <li>STOUT: Smiles to iupac names using neural machine translation <em>(Rating: 2)</em></li>
                <li>Chemformer: a pretrained transformer for computational chemistry <em>(Rating: 2)</em></li>
                <li>Structured prompting: Scaling in-context learning to 1,000 examples <em>(Rating: 1)</em></li>
                <li>Translation between molecules and natural language <em>(Rating: 2)</em></li>
                <li>Predicting organic reaction outcomes using machine learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7444",
    "paper_id": "paper-258967365",
    "extraction_schema_id": "extraction-schema-140",
    "extracted_data": [
        {
            "name_short": "ICL vs Zero-shot",
            "name_full": "Few-shot In-Context Learning (ICL) versus Zero-shot Prompting",
            "brief_description": "Comparison of few-shot in-context learning prompts (ICL) with zero-shot prompts across chemistry tasks; ICL consistently improved performance over zero-shot, often substantially for tasks where demonstrations are informative.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 (representative across GPT family)",
            "model_description": "Instruction-tuned large language model evaluated alongside GPT-3.5, Davinci-003, Llama, and Galactica; used as the main representative of GPT-series performance.",
            "model_size": null,
            "task_name": "Multiple chemistry tasks (aggregate across 8 tasks)",
            "task_description": "Various chemistry benchmark tasks including name prediction, property prediction, yield prediction, reaction prediction, retrosynthesis, text-based molecule design, molecule captioning, and reagents selection.",
            "problem_format": "Zero-shot natural-language prompt vs few-shot in-context learning prompt (concatenated demonstration examples + task-specific template).",
            "format_category": "prompt style",
            "format_details": "Zero-shot: single standardized instruction asking model to act as a chemist and return only output. ICL: task-specific template with {General Template, Task-Specific Template, ICL, Question}; demonstrations concatenated as '[Input]: [Input_content][Output]: [Output_content]'; retrieval strategy (Random or Scaffold) and k (number of examples) varied; experiments repeated 5 times to average randomness.",
            "performance_metric": "varies by task (accuracy, F1, BLEU, etc.)",
            "performance_value": "ICL prompting improved performance in all tasks compared to zero-shot; example — Yield prediction (Buchwald-Hartwig): GPT-4 zero-shot 32.2% accuracy → GPT-4 few-shot (random, k=8) 80.0% accuracy.",
            "baseline_performance": "Zero-shot example: 32.2% accuracy (Buchwald-Hartwig, GPT-4 zero-shot).",
            "performance_change": "+47.8% absolute (Buchwald-Hartwig example: 80.0% - 32.2%).",
            "experimental_setting": "Five repeated evaluations; ICL k grid-searched per task (e.g., k in {4,8} for property/yield, k in {5,20} for name/reaction/retrosynthesis), retrieval strategies Random vs Scaffold.",
            "statistical_significance": null,
            "uuid": "e7444.0",
            "source_info": {
                "paper_title": "What can Large Language Models do in chemistry? A comprehensive benchmark on eight tasks",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "ICL example count (k)",
            "name_full": "Number of In-Context Demonstrations (k) impacts performance",
            "brief_description": "Performance generally increases as the number of in-context examples (k) grows, with notable step-changes observed in several chemistry tasks (larger k often yields better results).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Instruction-tuned large language model used in few-shot experiments varying demonstration count.",
            "model_size": null,
            "task_name": "Yield prediction (Buchwald-Hartwig and Suzuki-Miyaura) and other tasks",
            "task_description": "Binary classification of reaction yield (high vs not-high) and other tasks where number of examples was varied.",
            "problem_format": "Few-shot in-context learning with k demonstrations",
            "format_category": "prompt style / prompt size",
            "format_details": "Grid search on k; example ranges: yield prediction used k ∈ {4, 8}; name/reaction/retrosynthesis used k ∈ {5, 20}; in molecule design/captioning k ∈ {5,10} due to token limits.",
            "performance_metric": "accuracy",
            "performance_value": "Buchwald-Hartwig: GPT-4 random k=4 → 57.4% accuracy; k=8 → 80.0% accuracy. Suzuki-coupling: GPT-4 random k=4 → 32.4% accuracy; k=8 → 76.4% accuracy.",
            "baseline_performance": "k=4 results used as lower-k baseline (e.g., 57.4% for Buchwald).",
            "performance_change": "Buchwald-Hartwig: +22.6% absolute (57.4 → 80.0); Suzuki: +44.0% absolute (32.4 → 76.4).",
            "experimental_setting": "Random retrieval for these yield experiments; five repeated runs; other tasks also show monotonic improvements with larger k in validation grid search.",
            "statistical_significance": null,
            "uuid": "e7444.1",
            "source_info": {
                "paper_title": "What can Large Language Models do in chemistry? A comprehensive benchmark on eight tasks",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "ICL retrieval: Scaffold vs Random",
            "name_full": "In-context-example retrieval strategy: Scaffold (similarity-based) versus Random",
            "brief_description": "Selecting demonstrations by scaffold similarity (Tanimoto on Morgan fingerprints) generally yielded better ICL performance than random selection across many tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 (and other GPT variants)",
            "model_description": "LLMs evaluated with different demonstration retrieval strategies: Scaffold uses Tanimoto similarity on Morgan fingerprints for SMILES inputs; for textual inputs, difflib.SequenceMatcher string-similarity was used.",
            "model_size": null,
            "task_name": "Multiple tasks (name prediction, property prediction, yield prediction, reaction prediction, text-based design, captioning, etc.)",
            "task_description": "Tasks where demonstration-example selection can influence the relevance of examples to the query.",
            "problem_format": "Few-shot ICL with either Random sampling of examples or Scaffold (similarity)-based retrieval",
            "format_category": "prompt style / demonstration selection",
            "format_details": "Scaffold retrieval: compute Tanimoto similarity with 2048-bit Morgan fingerprints (radius=2), choose top-k similar molecules as examples; Random: choose k examples uniformly at random. Grid search chose retrieval strategy per task using a 30-sample validation set.",
            "performance_metric": "various (accuracy, F1, BLEU, exact match)",
            "performance_value": "Authors report scaffold sampling outperforms random sampling in most tasks (cited tables: 4, 6, 7, 10, 11, 14, 15), e.g., validation-based selections favored scaffold retrieval for final test runs.",
            "baseline_performance": "Random retrieval performance (task-dependent); specific numbers vary by task.",
            "performance_change": "Qualitative improvement reported; exact numeric deltas depend on task (not uniformly quantified for every task in main text).",
            "experimental_setting": "Validation grid search across retrieval strategies and k, followed by 5 repeated test evaluations using top options.",
            "statistical_significance": null,
            "uuid": "e7444.2",
            "source_info": {
                "paper_title": "What can Large Language Models do in chemistry? A comprehensive benchmark on eight tasks",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Label semantics in prompt (Property labels)",
            "name_full": "Inclusion of property label semantics in prompts for property prediction",
            "brief_description": "Embedding the semantic meaning of target labels (e.g., 'inhibit HIV replication') into the prompt significantly improves model performance on property-classification tasks compared to omitting label context.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Instruction-tuned LLM evaluated on molecular property classification datasets with prompts that either included or omitted explicit label semantics.",
            "model_size": null,
            "task_name": "Molecule property prediction (HIV, ClinTox, BBBP, BACE, Tox21)",
            "task_description": "Binary classification of molecular properties (e.g., HIV activity, clinical toxicity).",
            "problem_format": "Natural-language classification prompt that includes label descriptions (yes/no responses), versus an identical prompt with label semantics removed.",
            "format_category": "prompt content / semantic context",
            "format_details": "Prompts with label context contained explicit human-interpretable descriptions of the labels (e.g., 'inhibit HIV replication' or 'drugs failed clinical trials for toxicity reason'); unlabelled variants removed these descriptions. Experiments run zero-shot and few-shot.",
            "performance_metric": "F1 score and accuracy",
            "performance_value": "HIV: GPT-4 zero-shot F1 0.977 ± 0.013 (with label info) vs 0.554 ± 0.017 (unlabelled zero-shot); Accuracy 0.986 ± 0.070 vs 0.628 ± 0.016. Few-shot F1: 0.797 ± 0.021 (labelled) vs 0.493 ± 0.030 (unlabelled). ClinTox: zero-shot F1 0.489 ± 0.018 vs 0.438 ± 0.045 (unlabelled); few-shot F1 0.563 ± 0.008 vs 0.478 ± 0.035 (unlabelled).",
            "baseline_performance": "Unlabelled prompt performance used as contrast/baseline (values above).",
            "performance_change": "HIV zero-shot F1 change: -0.423 absolute when label semantics removed; HIV accuracy change: -0.358 absolute; ClinTox F1 change smaller (~-0.051 zero-shot).",
            "experimental_setting": "Zero-shot and few-shot settings; scaffold sampling used for selecting ICL examples in some experiments; results averaged over repeated runs.",
            "statistical_significance": null,
            "uuid": "e7444.3",
            "source_info": {
                "paper_title": "What can Large Language Models do in chemistry? A comprehensive benchmark on eight tasks",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "SMILES vs SELFIES",
            "name_full": "Molecular string representation: SMILES compared to SELFIES",
            "brief_description": "Comparative evaluation of two molecule string formats (SMILES and SELFIES) as the input/output representation for LLMs; SMILES yielded better downstream performance for GPT-4 on most evaluated tasks although SELFIES produced fewer invalid generations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "LLM tested on tasks where molecules are represented either by SMILES strings (commonly used in corpora) or SELFIES (designed to be 100% robust).",
            "model_size": null,
            "task_name": "Reaction prediction, Text-based molecule design, Molecule captioning",
            "task_description": "Predicting reaction products, generating molecules from text, and producing textual captions/descriptions for molecules.",
            "problem_format": "Input and/or output molecules encoded as SMILES strings versus SELFIES strings.",
            "format_category": "input modality / representation format",
            "format_details": "Direct substitution of representation in prompts and demonstration examples; validity and task metrics computed using RDKit and fingerprint-based similarity.",
            "performance_metric": "Top-1 accuracy, invalid-string rate, BLEU, Exact match, validity, fingerprint Tanimoto similarity (FTS), FCD, ROUGE/METEOR for captioning.",
            "performance_value": "Reaction prediction: SMILES Top-1 0.230 ± 0.022, invalid SMILES 7.0% ± 1.6%; SELFIES Top-1 0.110 ± 0.007, invalid SELFIES 1.0% ± 0.0%. Molecule design: SMILES BLEU 0.816 ± 0.004, Exact 0.174 ± 0.029, Validity 0.888 ± 0.023 vs SELFIES BLEU 0.277 ± 0.009, Exact 0.100 ± 0.016, Validity 0.804 ± 0.022. Molecule captioning: small differences (BLEU-2/4, ROUGE, METEOR similar with slight SMILES advantage).",
            "baseline_performance": "SELFIES used as alternative baseline; SMILES generally outperformed SELFIES on high-level performance metrics but produced more invalid outputs in some tasks.",
            "performance_change": "Reaction Top-1: SMILES +12.0 percentage points vs SELFIES (23.0% vs 11.0%); invalid rate: SELFIES -6.0 percentage points (1.0% vs 7.0%). Molecule design BLEU: SMILES +0.539 absolute (0.816 vs 0.277); Exact +0.074 absolute (0.174 vs 0.100); Validity +0.084 absolute (0.888 vs 0.804).",
            "experimental_setting": "GPT-4 evaluated on SMILES and SELFIES variants across four tasks; authors hypothesize pretraining corpora contain more SMILES and thus models are more attuned to SMILES.",
            "statistical_significance": null,
            "uuid": "e7444.4",
            "source_info": {
                "paper_title": "What can Large Language Models do in chemistry? A comprehensive benchmark on eight tasks",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Temperature sensitivity",
            "name_full": "Impact of decoding temperature on LLM test performance",
            "brief_description": "Variation of the temperature sampling parameter (0.2 to 1.0) had only marginal effects on model performance in the tested chemistry property tasks, indicating robustness to this decoding hyperparameter within the tested range.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Instruction-tuned LLM; decoding temperature varied to assess randomness impact on outputs.",
            "model_size": null,
            "task_name": "Molecular property prediction (BBBP, BACE)",
            "task_description": "Binary property prediction using in-context learning (scaffold sampling, k=8 in this temperature analysis).",
            "problem_format": "Natural-language classification prompt with temperature hyperparameter controlling sampling randomness.",
            "format_category": "decoding hyperparameter",
            "format_details": "Temperatures tested t ∈ {0.2, 0.4, 0.6, 0.8, 1.0}; experiments performed on a 30-sample validation subset and targeted tests on the 100-sample test sets for BBBP and BACE.",
            "performance_metric": "F1 and accuracy",
            "performance_value": "F1 (BBBP) ranged roughly 0.650–0.712 across temperatures; Accuracy (BACE) varied roughly 0.741–0.757. Observed fluctuations &lt; 0.05 in F1/accuracy.",
            "baseline_performance": "Validation-chosen temperature (varied by task); no single temperature dramatically outperformed others.",
            "performance_change": "Marginal — variations under ~0.05 absolute in F1/accuracy across tested temperatures.",
            "experimental_setting": "GPT-4, scaffold sampling k=8, five repeated runs; temperature sweeps to conserve API usage then validated on test subsets.",
            "statistical_significance": null,
            "uuid": "e7444.5",
            "source_info": {
                "paper_title": "What can Large Language Models do in chemistry? A comprehensive benchmark on eight tasks",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Problem formulation: classification/ranking vs generation",
            "name_full": "Effect of task formulation (classification/ranking vs free-generation) on performance",
            "brief_description": "LLMs perform relatively well when problems are presented as classification or ranking choices, but struggle on generative tasks that require precise SMILES or IUPAC outputs; formulation strongly affects outcomes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 (with comparisons to GPT-3.5, Davinci-003, others)",
            "model_description": "LLMs evaluated on tasks reformulated either as classification/ranking (e.g., yield high/low, select reagent from candidates) or generative sequence-to-sequence tasks (e.g., predict SMILES/products, name translation).",
            "model_size": null,
            "task_name": "Yield prediction, Reagents selection (classification/ranking) vs Reaction prediction, Retrosynthesis, Name prediction (generation)",
            "task_description": "Classification/ranking tasks require choosing among provided categories or candidates; generative tasks require producing exact textual molecular representations (SMILES, IUPAC) as outputs.",
            "problem_format": "Multiple-choice / top-k / binary classification prompts versus open-ended generation prompts requiring exact SMILES/IUPAC outputs.",
            "format_category": "question type / output constraint",
            "format_details": "Classification/ranking prompts constrain choices or ask yes/no; generation prompts ask for free-form SMILES/IUPAC/structure output, with output restrictions sometimes included to reduce hallucination.",
            "performance_metric": "accuracy, validity, exact match",
            "performance_value": "Classification example: reagents selection accuracy ~40–50% (GPT-4/GPT-3.5); Yield prediction GPT-4 (best ICL) 80% (Buchwald) and 76.4% (Suzuki) but baseline UAGNN 96.5%/95.7%. Generative example: name prediction best accuracy max 8% (non-competitive); reaction prediction zero-shot Top-1 accuracy 0.4% (0.004) with 17.4% invalid SMILES.",
            "baseline_performance": "Baselines such as UAGNN for yield (96.5%/95.7%), Chemformer for reaction prediction (much higher Top-1 accuracy trained on full dataset) reported in paper; exact baseline numbers vary by task.",
            "performance_change": "Generative tasks: GPT models underperform baselines by large margins (e.g., reaction/retrosynthesis ~40–70% lower than task-specific models); classification tasks: GPT models can be competitive though often below fully supervised baselines trained on many examples (e.g., yield: GPT-4 ~16–20% lower than UAGNN).",
            "experimental_setting": "When possible, tasks converted to classification/ranking (easier for LLMs); generative tasks included output restrictions and task-specific input explanations to mitigate hallucination.",
            "statistical_significance": null,
            "uuid": "e7444.6",
            "source_info": {
                "paper_title": "What can Large Language Models do in chemistry? A comprehensive benchmark on eight tasks",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Task-specific templates & output restrictions",
            "name_full": "Task-specific prompt templates including input explanations and output restrictions",
            "brief_description": "Using specialized prompt templates that provide input explanations and enforce output restrictions was used to reduce hallucination and guide LLMs toward chemically reasonable outputs; qualitative reduction in hallucinations reported though quantitative effect varies by task.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 (and other evaluated LLMs)",
            "model_description": "LLMs used with a standardized zero-shot template and a richer ICL template partitioned into General Template, Task-Specific Template (including Input Explanation, Output Explanation, Output Restrictions), ICL examples, and the Question.",
            "model_size": null,
            "task_name": "All chemistry tasks where hallucination risk exists (e.g., reaction prediction, name prediction, retrosynthesis, molecule design)",
            "task_description": "Tasks where precise chemical outputs are required and hallucinations are a concern.",
            "problem_format": "Task-specific natural language templates that explicitly describe input format and required output format, plus constraints (e.g., only return SMILES; separate reactants by '.'; products by '&gt;&gt;').",
            "format_category": "prompt style / constraints",
            "format_details": "Templates included explicit delimiters, input explanations for SMILES formatting, and 'Output Restrictions' to force concise outputs and reduce extraneous text; demonstration examples adhere to the same format.",
            "performance_metric": "validity, accuracy, qualitative hallucination reduction",
            "performance_value": "Qualitatively reported reduction in hallucinations and more constrained outputs when using the task-specific template and output restrictions; no single global numeric delta reported (task-dependent).",
            "baseline_performance": "Standardized zero-shot template without task-specific output restrictions (used as baseline in some comparisons).",
            "performance_change": "Qualitative improvement in realism/faithfulness and reduction in some hallucinations; exact numeric changes are task-specific and not uniformly reported.",
            "experimental_setting": "Templates applied across tasks; ICL examples formatted consistently; experiments repeated 5 times to mitigate randomness.",
            "statistical_significance": null,
            "uuid": "e7444.7",
            "source_info": {
                "paper_title": "What can Large Language Models do in chemistry? A comprehensive benchmark on eight tasks",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Chain-of-Thought (CoT) prompting",
            "name_full": "Chain-of-Thought prompting (suggested, not executed)",
            "brief_description": "The paper mentions advanced prompting techniques like Chain-of-Thought (CoT) and Decomposed Prompting as prospective methods to improve complex reasoning in chemistry tasks, but does not empirically evaluate them in this work.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "",
            "model_size": null,
            "task_name": "Suggested for complex reasoning tasks",
            "task_description": "Potentially useful for multi-step reasoning tasks in chemistry",
            "problem_format": "Chain-of-Thought style prompts that encourage step-by-step reasoning",
            "format_category": "prompt style (mention)",
            "format_details": "Authors propose CoT and Decomposed Prompting as promising directions; not implemented in experiments reported in this paper.",
            "performance_metric": "",
            "performance_value": "",
            "baseline_performance": null,
            "performance_change": null,
            "experimental_setting": "",
            "statistical_significance": null,
            "uuid": "e7444.8",
            "source_info": {
                "paper_title": "What can Large Language Models do in chemistry? A comprehensive benchmark on eight tasks",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2,
            "sanitized_title": "language_models_are_fewshot_learners"
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Self-referencing embedded strings (SELFIES): A 100% robust molecular string representation",
            "rating": 2,
            "sanitized_title": "selfreferencing_embedded_strings_selfies_a_100_robust_molecular_string_representation"
        },
        {
            "paper_title": "STOUT: Smiles to iupac names using neural machine translation",
            "rating": 2,
            "sanitized_title": "stout_smiles_to_iupac_names_using_neural_machine_translation"
        },
        {
            "paper_title": "Chemformer: a pretrained transformer for computational chemistry",
            "rating": 2,
            "sanitized_title": "chemformer_a_pretrained_transformer_for_computational_chemistry"
        },
        {
            "paper_title": "Structured prompting: Scaling in-context learning to 1,000 examples",
            "rating": 1,
            "sanitized_title": "structured_prompting_scaling_incontext_learning_to_1000_examples"
        },
        {
            "paper_title": "Translation between molecules and natural language",
            "rating": 2,
            "sanitized_title": "translation_between_molecules_and_natural_language"
        },
        {
            "paper_title": "Predicting organic reaction outcomes using machine learning",
            "rating": 1,
            "sanitized_title": "predicting_organic_reaction_outcomes_using_machine_learning"
        }
    ],
    "cost": 0.01964175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>What can Large Language Models do in chemistry? A comprehensive benchmark on eight tasks
28 Dec 2023</p>
<p>Taicheng Guo 
University of Notre Dame</p>
<p>Kehan Guo 
University of Notre Dame</p>
<p>Bozhao Nan bnan@nd.edu 
University of Notre Dame</p>
<p>Zhenwen Liang zliang6@nd.edu 
University of Notre Dame</p>
<p>Zhichun Guo 
University of Notre Dame</p>
<p>Nitesh V Chawla nchawla@nd.edu 
University of Notre Dame</p>
<p>Olaf Wiest owiest@nd.edu 
University of Notre Dame</p>
<p>Xiangliang Zhang xzhang33@nd.edu 
University of Notre Dame</p>
<p>What can Large Language Models do in chemistry? A comprehensive benchmark on eight tasks
28 Dec 202394582E0AD883436633068EF15298C2B9arXiv:2305.18365v3[cs.CL]
Large Language Models (LLMs) with strong abilities in natural language processing tasks have emerged and have been applied in various kinds of areas such as science, finance and software engineering.However, the capability of LLMs to advance the field of chemistry remains unclear.In this paper, rather than pursuing state-of-the-art performance, we aim to evaluate capabilities of LLMs in a wide range of tasks across the chemistry domain.We identify three key chemistryrelated capabilities including understanding, reasoning and explaining to explore in LLMs and establish a benchmark containing eight chemistry tasks.Our analysis draws on widely recognized datasets facilitating a broad exploration of the capacities of LLMs within the context of practical chemistry.Five LLMs (GPT-4, GPT-3.5, Davinci-003, Llama and Galactica) are evaluated for each chemistry task in zero-shot and few-shot in-context learning settings with carefully selected demonstration examples and specially crafted prompts.Our investigation found that GPT-4 outperformed other models and LLMs exhibit different competitive levels in eight chemistry tasks.In addition to the key findings from the comprehensive benchmark analysis, our work provides insights into the limitation of current LLMs and the impact of in-context learning settings on LLMs' performance across various chemistry tasks.The code and datasets used in this study are available at https://github.com/ChemFoundationModels/ChemLLMBench.</p>
<p>Introduction</p>
<p>Large language models (LLMs) have recently demonstrated impressive reasoning abilities across a wide array of tasks.These tasks are not limited to natural language processing, but also extend to various language-related applications within scientific domains [56,30,24,10].Much of the research on the capacity of LLMs in science has been focused on tasks such as answering medical [30] and scientific questions [24,25].However, the exploration of their application to practical tasks in the field of chemistry remains underinvestigated.Although some studies [6,27,63,48] have been conducted, they tend to focus on specific case studies rather than a comprehensive or systematic evaluation.The exploration of LLMs' capabilities within the field of chemistry has the potential to revolutionize this domain and expedite research and development activities [62].Thus, the question, "What can LLMs do in chemistry?" is a compelling topic of inquiry for both AI researchers and chemists.Nevertheless, there exist two challenges that hinder the answer to the topic and the further development of LLMs in chemistry:</p>
<p>• Determining the potential capabilities of LLMs in chemistry requires a systematic analysis of both LLMs and the specific requirements of chemistry tasks.There are different kinds of tasks in chemistry, some of which can be formulated to tasks solved by LLMs while others may not.It is necessary to consider the specific knowledge and reasoning required for each task and assess whether LLMs can effectively acquire and utilize that knowledge.</p>
<p>• Conducting reliable and wide-ranging evaluation requires diverse experimental settings and limitations, that is, careful consideration and standardization of evaluation procedures, dataset curation, prompt design, and in-context learning strategies.Additionally, the API call time consumption and the randomness of LLMs limit the size of the testing.To address this knowledge gap, we (a group of AI researchers and chemists) have developed a comprehensive benchmark to provide a preliminary investigation into the abilities of LLMs across a diverse range of practical chemistry tasks.Our aim is to gain insights that will be beneficial to both AI researchers and chemists to advance the application of LLMs in chemistry.For AI researchers, we provide insights into the strengths, weaknesses, and limitations of LLMs in chemistry-related tasks, which can inform the further development and refinement of different AI techniques for more effective applications within the field.For chemists, our study provides a better understanding of the tasks in which they can rely on current LLMs.Utilizing our more extensive experimental setup, a broader range of chemistry tasks can be explored to further evaluate the capabilities of LLMs.</p>
<p>Our investigation focuses on 8 practical chemistry tasks, covering a diverse spectrum of the chemistry domain.These include: 1) name prediction, 2) property prediction, 3) yield prediction, 4) reaction prediction, 5) retrosynthesis (prediction of reactants from products), 6) text-based molecule design, 7) molecule captioning, and 8) reagents selection.Our analysis draws on widely available datasets including BBBP, Tox21 [65], PubChem [32], USPTO [29,53,39], and ChEBI [17,16].Five LLMs (GPT-4, GPT-3.5, Davinci-003, Llama, and Galactica) [43] are evaluated for each chemistry task in zero-shot and few-shot in-context learning settings with carefully selected demonstration examples and specific prompts.We highlight the contributions of this paper as follows:</p>
<p>• We are the first to establish a comprehensive benchmark to evaluate the abilities of LLMs on a wide range of chemistry tasks.These eight selected tasks, in consultation with chemists, not only encompass a diverse spectrum of the chemistry domain but also demand different abilities such as understanding, reasoning, and explaining using domain-specific chemistry knowledge.</p>
<p>• We provide a comprehensive experimental framework for testing LLMs in chemistry tasks.To factor in the impact of prompts and demonstration examples in in-context learning, we have assessed multiple input options, focusing on the description of chemistry tasks.Five representative configurations were chosen based on their performance on a validation set, then these selected options were applied on the testing set.The conclusion is made from five repeated evaluations on each task, since GPTs often yield different outputs at different API calls even though the input is the same.We thus believe that our benchmarking process is both reliable and systematic.</p>
<p>• Our investigations yield broader insights into the performance of LLMs on chemistry tasks.As summarized in Table 2, our findings confirm some anticipated outcomes (e.g., GPT-4 outperforms GPT-3 and Davinci-003), and also reveal unexpected discoveries (e.g., property prediction can be better solved when property label semantics are included in prompts).Our work also contributes to practical recommendations that can guide AI researchers and chemists in leveraging LLMs more effectively in the future (see Section 5).The paper is organized as follows.Related works are presented in Section 2. In section 3, we elaborate on the evaluation process, including an overview of the chemistry tasks, the utilized LLMs and prompts, and the validation and testing settings.In section 4, we summarize the main findings (due to the space limit, evaluation details of each chemistry task can be found in Appendix).Finally, to answer the question "What can LLMs do in chemistry?"we discuss the constraints inherent to LLMs and how different settings related to LLMs affect performance across various chemistry tasks in Section 5.The conclusions are summarized in section 6.</p>
<p>Related Work</p>
<p>Large Language Models.The rise of Large Language Models (LLMs) has marked a significant trend in recent natural language processing (NLP) research.This progress has been fuelled by milestones such as the introduction of GPT-3 [4], T0 [52], Flan-T5 [12], Galactica [56] and LLaMa [57].The recently released GPT-4, an evolution from GPT-3.5 series, has drawn considerable attention for its improvements in language understanding, generation, and planning [43].Despite the vast potential of LLMs, existing research primarily centers on their performance within general NLP tasks [8,9].The scientific disciplines, notably chemistry, have received less focus.The application of LLMs in these specialized domains presents an opportunity for significant advancements.Therefore, we conduct a comprehensive experimental analysis to evaluate the capability of LLMs in chemistry-related tasks.</p>
<p>Large Language Model Evaluations.In recent years, the evaluation of LLMs like GPT has become a significant field of inquiry.[11] showed ChatGPT's proficiency in law exams, while technical aspects of GPT-4 were analyzed in [43].LLMs are also applied in healthcare [14] , mathematical problem [18], and code generation tasks [37].Specifically, in healthcare, the utility and safety of LLMs in clinical settings were explored [42].In the context of mathematical problem-solving, studies [18,7] have highlighted that LLMs encounter challenges with graduate-level problems, primarily due to difficulties in parsing complex syntax.These studies underscored the complexity of achieving task-specific accuracy and functionality with LLMs.Lastly, AGIEval [66] assessed LLMs' general abilities but noted struggles in complex reasoning tasks.</p>
<p>Our work aligns with these evaluations but diverges in its focus on chemical tasks.To our knowledge, this is the first study to transform such tasks to suit LLM processing and to perform a comprehensive evaluation of these models' ability to tackle chemistry-related problems.This focus will contribute to expand our understanding of LLMs' capabilities in specific scientific domains.</p>
<p>Large Language Model for Chemistry.Recent efforts integrating LLMs with the field of chemistry generally fall into two distinct categories.One category aims to create a chemistry agent with LLMs' by leveraging its planning ability to utilize task-related tools.For example, Bran et al [3] developed ChemCrow, which augmented LLMs with chem-expert designed tools for downstream tasks such as organic synthesis and drug discovery.Similarly, by leveraging the planning and execution ability of multiple LLMs, Boiko et al [2] developed an autonomous chemical agent to conduct chemical experiments.The other category involves direct usage of LLMs for downstream tasks in chemistry [27,62,6,28].While these studies have explored the performance of LLMs in chemistry-related tasks, a systematic evaluation of their capabilities within this domain has been lacking.Consequently, there is a noticeable gap that calls for a meticulous benchmark to thoroughly assess the potential of LLMs in chemistry.Such a benchmark is crucial not only for identifying the strengths and limitations of these models in a specialized scientific domain, but also to guide future improvements and applications.</p>
<p>The Evaluation Process and Setting</p>
<p>The evaluation process workflow is depicted in Fig. 1.Guided by co-author Prof. Olaf Wiest (from the Department of Chemistry at the University of Notre Dame), we identify eight tasks in discussion with senior Ph.D. students at the NSF Center for Computer Assisted Synthesis (C-CAS).Following this, we generate, assess, and choose suitable prompts to forward to LLMs.The acquired answers are then evaluated both qualitatively by chemists to identify whether they are helpful in the real-world scenario and quantitatively by selected metrics.</p>
<p>Chemistry tasks.In order to explore the abilities of LLMs in the field of chemistry, we concentrate on three fundamental capabilities: understanding, reasoning, and explaining.We examine these competencies through eight diverse and broadly acknowledged practical chemistry tasks.These tasks are summarized in Table 1, in terms of the task type from the perspective of machine learning, the dataset used for the evaluation, as well as the evaluation metrics.The #ICL candidates refers to the number of candidate examples, from which we select k demonstration examples, either randomly or based on similarity searches.These candidate sets are the training sets used in classical machine learning models, e.g., in training classifiers or generative models.We set the test set of 100 instances, randomly sampled from the original testing dataset (non-overlapping with the training set).To reduce the influence of the LLMs randomness on the results, each evaluation experiment is repeated five times and the mean and variance are reported.</p>
<p>LLMs.For all tasks, we evaluate the performance of five popular LLMs: GPT-4, GPT-3.5 (referred to as GPT-3.5-turbo,also known as ChatGPT), Davinci-003, LLama and Galactica.Zero-shot prompt.For each task, we apply a standardized zero-shot prompt template.As shown in Fig. 2, we instruct the LLMs to act in the capacity of a chemist.The content within the brackets is tailored to each task, adapting to its specific inputs and outputs.The responses from LLMs are confined to only returning the desired output without any explanations.Task-specific ICL prompt.ICL is a new paradigm for LLMs where predictions are based solely on contexts enriched with a few demonstration examples [15].This paper specifically denotes ICL as a few-shot in-context learning approach, excluding the zero-shot paradigm.In order to thoroughly examine the capacities of LLMs within each chemistry-specific task, we design a taskspecific ICL prompt template.As shown in Fig. 3.The format of the template is similar to that used in [48].We also partition our template into four parts: {General Template}{Task-Specific Template}{ICL}{Question}.The {General Template} is almost the same as the zero-shot prompt, instructing the LLMs to play the role of a chemist and specify the chemistry task with its corresponding input and output.Considering that the responses for chemistry-related tasks must be accurate and chemically reasonable, it is crucial to prevent LLMs from generating hallucinated information.To this end, we introduce the {Task-Specific Template} which consists of three main components:   Experiment setup strategy.In property prediction and yield prediction tasks, we perform the grid search of k in {4, 8}.In the name prediction, reaction prediction, and retrosynthesis tasks, we perform the grid search of k in {5, 20}.In text-based molecule design and molecule captioning tasks, we</p>
<p>Experiment Analysis</p>
<p>Due to space limitations, we provide details of the evaluation on each chemistry task in Appendix by the following order: name prediction in section A, property prediction in section B, yield prediction in section C, reaction prediction in section D, reagents selection in section E, retrosynthesis in section F, text-based molecule design in section G, and molecule captioning in section H.The detailed results described in the Appendix allow us to approach the question "What can LLMs do in chemistry?"from several directions.We discuss the key findings from our comprehensive benchmark analysis and provide valuable insights by thoroughly analyzing the limitation of LLMs and how different settings related to LLMs affect performance across various chemistry tasks.</p>
<p>Can LLMs outperform existing baselines in chemistry tasks?</p>
<p>Several classic predictive models based on machine learning (ML) have been developed for specific chemistry tasks.For instance, MolR (Graph Neural Network-based) predicts molecule properties as a binary classification problem [58].UAGNN achieved state-of-the-art performance in yield prediction [34].MolT5-Large, a specialized language model based on T5, excels in translating between molecule and text [17].We conduct a performance analysis of GPT models and compare their results with available baselines, if applicable.The main findings from the investigations are:</p>
<p>• GPT-4 outperforms the other models evaluated.The ranking of the models on 8 tasks can be found in Table 2;</p>
<p>• GPT models exhibit a less competitive performance in tasks demanding precise understanding of molecular SMILES representation, such as name prediction, reaction prediction and retrosynthesis;</p>
<p>• GPT models demonstrate strong capabilities both qualitatively (in Fig. 14 evaluated by chemists) and quantitatively in text-related explanation tasks such as molecule captioning;</p>
<p>• For chemical problems that can be converted to classification tasks or ranking tasks, such as property prediction, and yield prediction, GPT models can achieve competitive performance compared to baselines that use classical ML models as classifiers, or even better, as summarized in Table 2.These conclusions are derived from conducting five repeated evaluations on each task, using the best evaluation setting that was discovered through a grid search on the validation set of each task.We designate the performance of GPT models as three categories and provide in-depth discussion next.</p>
<p>• Tasks with not competitive (NC) performance.In tasks such as reaction prediction and retrosynthesis, GPT models are worse than existing ML baselines trained by large amounts of training data, partially because of the limitation on understanding molecular SMILES strings.In reaction prediction and retrosynthesis, SMILES strings are present in both the input and output of the GPT models.Without an in-depth understanding of the SMILES strings that represent reactants and products, as well as the reaction process that transforms reactants into products, it will be difficult for GPT models to generate accurate responses, as shown in Table 11 and 13.GPT models exhibit poor performance on the task of name prediction as well (see Table 4).This further validates the notion that GPT models struggle with understanding long strings in formats such as SMILES, IUPAC name, and molecular formula, and make correct translations between them.</p>
<p>• Tasks with competitive (C) performance.GPT models can achieve satisfactory results when the chemistry tasks are formulated into the forms of classification (e.g., formatting yield prediction into a high-or-not classification, instead of regression) or ranking (as seen in reagents selection), as illustrated in Fig. 7 and 9.This is understandable, because making choices is inherently simpler than generating products, reactants or names.GPT models can achieve an accuracy of 40% to 50% when asked to select the reactant or solvent or ligand from provided candidates.</p>
<p>Although GPT-4's performance on yield prediction falls short compared to the baseline model UAGNN [34] (with 80% versus 96% on the Buchwald-Hartwig dataset, and 76% versus 96% on the Suzuki-coupling dataset), it demonstrates improved performance when given more demonstration examples within the few-shot in-context learning scenario, as reported in Table 10.It is worth noting that the UAGNN model was trained on thousands of examples for these specific reactions.Last, while GPT models exhibit promising performance for yield prediction on the evaluated High-Throughput Experimentation (HTE) datasets, specifically the Buchwald-Hartwig [1] and Suzuki-Miyaura datasets [50], they perform as bad as other ML baselines on more challenging datasets like USPTO-50k [53].This observation indicates a potential area for future research and improvement in the performance of GPT models on challenging chemistry datasets.</p>
<p>• Tasks with selectively competitive (SC) performance.GPT models are selectively competitive on two types of tasks.</p>
<p>-In the property prediction task on some datasets (HIV, ClinTox), GPT models outperform the baseline significantly, achieving F1 scores and accuracy nearing 1, as reported in Table 6 and 7.This might be due to the fact that the property labels to be predicted are included in the prompts, with GPT models being simply tasked in responding with yes or no.For example, the prompt includes inhibit HIV replication or drugs failed clinical trials for toxicity reason, and we observed a significant decline in the performance of GPT models upon removing property labels from the prompt (refer to Appendix section B).In contrast, baselines employing machine learning models do not include the semantic meaning of these labels in their input.The input for these models only comprises molecular representations in graph form but no labels.-For tasks related to text, such as text-based molecule design and molecule captioning, GPT models exhibit strong performance due to their language generation capabilities.On the task of text-based molecule design, GPT models outperform the baseline when evaluated using NLP metrics such as BLEU and Levenshtein.However, when it comes to exact match, the accuracy is less than 20%, as reported in Table 14 and 15.This suggests that the molecules designed by GPT models may not be exactly the same as the ground truth.Particularly in the context of molecular design/generation, the exact match is a significant metric.Unlike in natural language generation where there is some allowance for deviation from the input, molecular design demands precise accuracy and chemical validity.However, not being precisely identical to the ground truth does not automatically invalidate a result.Molecules generated by GPT models may still prove to be beneficial and could potentially act as viable alternatives to the ground truth, provided they meet the requirements outlined in the input text and the majority (over 89%) are chemically valid (see Table 14).Nonetheless, assessing the true utility of these generated molecules, such as evaluating their novelty in real-world applications, can be a time-consuming undertaking.</p>
<p>The capability of different LLMs</p>
<p>As shown in Table 2, we can find that GPT-4 model shows better chemical understanding, reasoning, and explaining abilities than Davinci-003, GPT-3.5, Llama and Galactica.This further verifies the GPT-4 model outperforms the other models in both basic and realistic scenarios [5].</p>
<p>The effects of the ICL</p>
<p>To investigate the effects of the ICL, we introduced ICL prompting and different ICL retrieval methods, and the different number of ICL examples in each task.Based on the experiments results of 12 different variants of each option and evaluating their performance on the validation set, we have the following three observations:</p>
<p>• In all tasks, the performance of ICL prompting is better than zero-shot prompting.</p>
<p>• In most tasks (in We can observe that the results of using SELFIES in all four tasks are inferior to those of using SMILES.This could be attributed to the fact that the pretraining datasets for LLMs are primarily populated with SMILES-related content rather than SELFIES.Consequently, these models are more attuned to SMILES.However, it's worth mentioning that the occurrence of invalid SELFIES is less frequent than that of invalid SMILES, which aligns with the inherent design of SELFIES to ensure molecular validity.</p>
<p>The impact of temperature parameter of LLMs</p>
<p>One key hyperparameter that affects the performance of LLMs is temperature, which influences the randomness in the model's predictions.To determine the optimal temperature for each task, we randomly sampled 30 data points from the datasets and performed in-context learning experiments across various temperature settings.While optimal temperatures determined on the validation set may not always yield optimal results on the test set, our methodology is primarily designed to conserve token usage and API query time.To address potential discrepancies between validation and test sets, we performed targeted temperature testing on the test sets for two molecular property prediction datasets: BBBP and BACE.Our results are summarized in Table 3.For these tests, we employed the GPT-4 model (using scaffold sampling with k = 8) and set temperature values t = [0.2,0.4, 0.6, 0.8, 1].The result reveal that variations in the temperature parameter have a marginal impact on test performance, with fluctuations of less than 0.05 observed in both F1 and accuracy scores.These results validate the robustness of our initial sampling approach and underscore the reliability of our findings across different settings.</p>
<p>Discussion</p>
<p>Limitation of LLMs on understanding molecular SMILES</p>
<p>A significant limitation of LLMs is their lack of understanding of molecular representations in SMILES strings, which in many cases leads to inaccurate or inconsistent results as shown in Section A for the translation of different ways to name molecules.SMILES (Simplified Molecular Input Line Entry System) [60,61] is a widely used textual representation for chemical structures.For example, the SMILES string for ethanol, a simple alcohol, is "CCO".This string represents a molecule with two carbon atoms (C) connected by a single bond and an oxygen atom (O) connected to the second carbon atom.SMILES strings can serve as both input and output for LLMs, alongside other natural language text.However, several issues make it challenging for LLMs to accurately understand and interpret SMILES strings: 1) Hydrogen atoms are not explicitly represented in SMILES strings, as they can be inferred based on the standard bonding rules.LLMs frequently struggle to infer these implicit hydrogen atoms and may even fail at simple tasks like counting the number of atoms in a molecule [27,6].2) A given molecule can have multiple valid SMILES representations, which can lead to ambiguity if not properly processed or standardized.LLMs may thus fail to consistently recognize and compare molecular structures represented by different SMILES strings.3) LLMs do not have any inherent understanding of SMILES strings, and treat them as a sequence of characters or subwords.When processing long SMILES strings, LLMs rely on the byte-pair encoding tokenization technique, which can break the string into smaller pieces or subwords in ways that do not represent the molecular structure and properties of molecules represented by SMILES strings.Because many tasks in cheminformatics rely on the accurate representation of a molecule by SMILES strings, the non-competitive performance of GPT models in converting structures into SMILES strings (and vice versa) affects downstream tasks such as retrosynthesis, reaction and name prediction.LLMs that have an enhanced ability of handling molecular structures and their specific attributes or coupling to existing tools such as RDKit [35] will be needed.</p>
<p>The limitations of current evaluation methods</p>
<p>Although in Text-Based Molecule Design and Molecule Captioning tasks, GPT models show competitive performance compared to the baseline in some metrics (BLEU, Levenshtein, ROUGE, FCD, etc), we observe that the exact match of GPT models is inferior to the baseline in the Text-Based Molecule Design task and the GPT models generate some descriptions which violate chemical facts.This divergence between metrics and real-world scenarios mainly arises because, unlike many natural language processing tasks that can be suitably evaluated by sentence-level matching evaluation metrics, chemistry-related tasks necessitate exact matching for SMILES and precise terminology in descriptions.These findings spotlight the limitations of current evaluation metrics and underscore the need for the development of chemistry-specific metrics.</p>
<p>Hallucination of LLMs in chemistry</p>
<p>Our evaluation experiments across various tasks reveal two primary types of hallucinations exhibited by LLMs in the domain of chemistry.The first type occurs when the input is given in SMILES format (e.g., name prediction); LLMs occasionally struggle with interpreting these SMILES correctly.For instance, they may fail to recognize the number of atoms or certain functional groups within molecules during name prediction tasks.The second type of hallucination arises when the expected output from LLMs should be in the form of SMILES (e.g., reaction prediction and retrosynthesis).Here, LLMs may produce molecules that are chemically unreasonable, suggesting a gap in understanding what constitutes valid SMILES.Hallucination issues represent a key challenge with LLMs, particularly in the field of chemistry which necessitates exact matching of SMILES and adherence to strict chemical facts [62].Current LLMs need further investigation into this problem.</p>
<p>Prospects of LLMs for chemistry</p>
<p>Overall, through an exhaustive set of experiments and analyses, we outline several promising avenues for the application of LLMs in the field of chemistry.While LLMs underperform relative to baselines across a majority of tasks, it's important to note that LLMs leverage only a few examples to solve chemistry problems, whereas baselines are trained on extensive, task-specific datasets and are limited to certain tasks.This observation provides valuable insights into the potential of LLMs' generalized intelligence in the domain of chemistry.The employment of advanced prompting techniques such as Chain-of-thought (CoT) [59], Decomposed Prompting [31] could potentially boost the capacity of LLMs to perform complex reasoning.On the other hand, LLMs display a considerable amount of hallucinations in chemistry tasks, indicating that current LLMs may not yet possess the necessary capabilities to solve practical chemistry problems effectively.However, with continuous development of LLMs and further research into methods to avoid hallucinations, we are optimistic that LLMs can significantly enhance their problem-solving abilities in the field of chemistry.</p>
<p>Impact of generating harmful chemicals</p>
<p>Our work demonstrate that LLMs can generate chemically valid molecules.However, it's crucial to acknowledge and mitigate the risks of AI misuse, such as generating hazardous substances.While advancements in AI-enabled chemistry have the potential to bring about groundbreaking medicines and sustainable materials, the same technology can be misused to create toxic or illegal substances.This dual-edged potential emphasizes the necessity for stringent oversight.Without careful regulation, these tools could not only pose significant health and safety hazards but also create geopolitical and security challenges.Consequently, as we harness the capabilities of LLMs in the field of chemistry, we concur with earlier research on generative models in chemistry [2,3] that it is vital for developers to establish robust safeguards and ethical guidelines to deter harmful applications.This is akin to the limitations imposed on popular search engines, which can also be exploited to find information about dangerous chemicals or procedures online.</p>
<p>Broader Impacts</p>
<p>Our work has broad impacts across multiple dimensions.First, it offers valuable insights and recommendations for both AI researchers and chemists in academia and industry.These perspectives enhance the effective utilization of LLMs and guide future advancements in the field.Second, our objective evaluation of LLMs helps alleviate concerns regarding the replacement of chemists by AI.This aspect contributes to public education, addressing misconceptions and fostering a better understanding of the role of AI in chemistry.Furthermore, we provide a comprehensive experimental framework for testing LLMs in chemistry tasks, which can also be applicable to other domains.This framework serves as a valuable resource for researchers seeking to evaluate LLMs in diverse fields.However, it is important to recognize the ethical and societal implications associated with our work.Additionally, concerns about job displacement in the chemical industry may arise, and efforts should be made to address these challenges and ensure a responsible and equitable adoption of AI technologies.</p>
<p>Conclusion and Future Work</p>
<p>In this paper, we summarize the required abilities of LLMs in chemistry and construct a comprehensive benchmark to evaluate the five most popular LLMs (GPT-4, GPT-3.5, Davinci-003, LLama and Galactica) on eight widely-used chemistry tasks.The experiment results show that LLMs perform less competitive in generative tasks which require in-depth understanding of molecular SMILES strings, such as reaction prediction, name prediction, and retrosynthesis.LLMs show competitive performance in tasks that are in classification or ranking formats such as yield prediction and reagents selection.LLMs are selectively competitive on tasks involving text in prompts such as property prediction and text-based molecule design, or explainable tasks such as molecule captioning.These experiments indicate the potential of LLMs in chemistry tasks and the need for further improvement.We will collaborate with more chemists in the C-CAS group, progressively integrating a wider range of tasks that are both novel and practical.We hope our work can address the gap between LLMs and the chemistry research field, inspiring future research to explore the potential of LLMs in chemistry.</p>
<p>Appendix A Name Prediction</p>
<p>For one molecule, there are different chemical naming conventions and representations such as SMILES, IUPAC names, and graphic molecular formula.To investigate whether GPT models have the basic chemical name understanding ability, we construct four chemical name prediction tasks that include SMILES to IUPAC name translation (smiles2iupac), IUPAC name to SMILES translation (iupac2smiles), SMILES to molecule formula translation (smiles2formula), and IUPAC name to molecule formula translation (iupac2formula).We collect 630 molecules and their corresponding names including SMILES, IUPAC name, and molecule formula from PubChem3 [32].We randomly sample 500 molecules as the ICL candidates, and other 30 molecules as the validation set, and other 100 molecules as the test set.For all name translation tasks, we use the exact match accuracy as the metric to evaluate the performance.ICL Prompt.One example of the smiles2iupac prediction is shown in Figure 5.For other name translation tasks, we only change the underlined parts that represent different tasks and their corresponding input names and output names.Results.The results are reported in Table 4 (we only report representative methods along with their optimal prompt settings via grid search on validation set).In all four name prediction tasks, the accuracy of the best method is extremely low (0.014 in the iupac2smiles task, 0.086 in the smiles2formula task, 0.118 in the iupac2formula task) or even 0 (in the smiles2iupac task).This indicates the LLMs lack basic chemical name understanding ability.The accuracy of Davinci-003 is considerably inferior to other models.</p>
<p>Case studies.Example results generated by GPT-4 (Scaffold, k=20) method for each task is shown in Table 5.In all tasks, the GPT-4 model gives the wrong answers.In the smiles2formula task, we can observe that GPT models cannot even recognize the number of Carbon and infer the correct number of Hydrogen, demonstrating the bad chemical understanding ability of GPT models.For prospects, some pre-training technologies such as wrapping molecules with text [38] or code-switch [64,20] may be helpful to align different chemical names of the same molecule to help improve LLMs' chemical understanding.STOUT [47] 0.55 0.7 --GPT-4 (zero-shot) 0 0.008±0.0080.048± 0.022 0.092±0.018GPT-4 (Scaffold, k=5) 0 0.014±0.0090.058±0.0150.118±0.022GPT-4 (Scaffold, k=20) 0 0.012±0.0040.086±0.0360.084±0.005GPT-4 (Random, k=20) 0 0.010±0.0070.070±0.0320.076±0.011GPT-3.5 (Scaffold, k=20) 0 0.010±0.0000.052±0.0040.044±0.009Davinci-003 (Scaffold, k=20) 0 0 0.006±0.0050.018±0.004Llama2-13B-chat (Scaffold, k=20) 0 0 0.010±0.0070 GAL-30B (Scaffold, k=10) 0 0 0 0 Table 5: Example results generated by GPT-4 (Scaffold, k=20) method for different tasks
Task Input Ground Truth Output of GPT-4 (Scaffold, k=20) smiles2iupac CCOC(=O)C(C(C)=O)=C(C)N ethyl 2-acetyl-3-aminobut-2-enoate ethyl 2-methyl-5-oxo-2-azahept-4-en-3-oate iupac2smiles ethyl 2-acetyl-3-aminobut-2-enoate CCOC(=O)C(C(C)=O)=C(C)N CCOC(=O)C=C(C)C(=N)C smiles2formula Cc1noc(CCn2cc[nH]c2=O)n1 C8H10N4O2 C9H10N4O2 iupac2formula R)-(1-benzylquinolin-1-ium-4-yl) -(5-ethenyl-1-azabicyclo[2.2.2]octan-2-yl)methanol;chloride C26H29ClN2O C23H27ClN2O</p>
<p>B Molecule Property Prediction</p>
<p>Molecule property prediction [21,58] is a fundamental task in computational chemistry that has been gaining significant attention in recent years due to its potential for drug discovery, material science, and other areas in the chemistry.The task involves using machine learning techniques [22] to predict the chemical and physical properties of a given molecule, based on its molecular structure.We aim to further explore the potential of LLMs in molecular property prediction and assess their performance on a set of benchmark datasets, such as BBBP(MIT license), HIV(MIT license), BACE(MIT license), Tox21(MIT license), and ClinTox(MIT license), which were originally introduced by [65].The datasets are made up of extensive collections of SMILES, paired with binary labels that highlight the particular property being evaluated, such as BBBP: Blood-Brain Barrier Penetration, HIV: inhibit HIV replication, BACE: bindings results for a set of inhibitors of human beta-secretase, Tox21: toxicity of compounds, and ClinTox: drugs failed clinical trials for toxicity reasons.A comprehensive explanation of these datasets can be referenced in the original research conducted by [65].For ICL, we either select k samples randomly, or search the top-k most analogous molecules using RDKit [35] to determine the Tanimoto Similarity.However, it is crucial to mention that using the latter method does not assure an even distribution among classes.In our study, we employ a strategic sampling method for two categories of datasets: balanced and highly imbalanced.For balanced datasets, such as BBBP and BACE, we randomly select 30 samples for the validation process and 100 samples for testing from the original dataset.Contrastingly, for datasets exhibiting substantial label imbalance (39684:1443 ≈ 28:1, take HIV datasets as a example), we select samples from the majority and minority classes to achieve a ratio of 4:1.This strategic approach enables us to maintain a representative sample for the evaluation process, despite the original high imbalance in the dataset.</p>
<p>To evaluate the results, we use the classification accuracy, as well as F1 score as the evaluation metric due to the class imbalance.We benchmark our method against two established baselines from MoleculeNet [65]: RF and XGBoost.Both baselines utilize the 1024-bit circular fingerprint as input to predict the property as a binary classification problem.</p>
<p>ICL Prompt.Figure 6 illustrates a sample of our ICL prompt for property prediction.Within the task-specific template, we include a detailed explanation of the task forecasting the penetration of the brain-blood barrier to assist LLMs in comprehending the input SMILES from the BBBP dataset.Additionally, we establish certain constraints for the output to conform to the specific characteristics of the property prediction task.</p>
<p>Results.The results are reported as F1 in Table 6, accuracy in Table 7.We observed that GPT models outperform the baseline model in terms of F1 on four out of five datasets.In the range of GPT sampling on three distinct datasets (BBBP, BACE, Tox21).A plausible explanation for this could be the structural resemblances between the scaffold-sampled molecules and the query molecule, which potentially biases the GPT models towards more accurate decision.</p>
<p>Label interpretation.The results presented in Table 6 and Table 7 indicate that the GPT-4 model selectively outperforms the baseline models on the HIV and ClinTox datasets.This superior performance likely stems from the inclusion of information directly related to the labels within the ICL prompts.Specifically, in the HIV dataset, the activity test results play a crucial role.Molecules tend to inhibit HIV replication when the activity test is categorized as "confirmed active" or "confirmed moderately active."For the ClinTox dataset, the FDA-approval status of a molecule acts as a predictor of its clinical toxicity.A molecule not having FDA approval is more likely to be clinically toxic.In experiments where we excluded this contextual information from the in-context learning prompts, the F1 and accuracy score of predictions notably declined, as evident from the results in Table 8 and Table 9.</p>
<p>Table 8: Impact to F1 score of removing label context information from the in-context learning prompts.</p>
<p>F1(↑) HIV ClinTox</p>
<p>GPT-4(zero-shot) 0.977 ± (0.013) 0.489 ± (0.018) GPT-4(unlabelled, zero-shot) 0.554 ± (0.017) 0.438 ± (0.045) GPT-4(few-shot) 0.797 ± (0.021) 0.563 ± (0.008) GPT-4(unlabelled, few-shot) 0.493 ± (0.030) 0.478 ± (0.035) Table 9: Impact to accuracy of removing label context information from the in-context learning prompts.</p>
<p>Accuracy(↑)</p>
<p>HIV ClinTox GPT-4(zero-shot) 0.986 ± (0.070) 0.736 ± (0.027) GPT-4(unlabelled, zero-shot) 0.628 ± (0.016) 0.602 ± (0.039) GPT-4(few-shot) 0.836 ± (0.020) 0.856 ± (0.014) GPT-4(unlabelled, few-shot) 0.541 ± (0.032) 0.630 ± (0.014)</p>
<p>C Yield Prediction</p>
<p>Yield prediction [51] is a critical task in chemistry, specifically in the domain of synthetic chemistry, which involves the design and synthesis of new compounds for various applications, such as pharmaceuticals, materials, and catalysts.The yield prediction task aims to estimate the efficiency and effectiveness of a chemical reaction, primarily by quantifying the percentage of the desired product formed from the reactants.We use two High-Throughput experimentation (HTE) datasets: Buchwald-Hartwig [1] (MIT license) and Suzuki-Miyaura dataset [50] (MIT license) for evaluation.These datasets consist of reactions and their corresponding yields, which have been meticulously acquired through standardized and consistent experimental setups.This uniformity ensures that the data within each dataset is coherent, reducing the likelihood of discrepancies arising from variations in experimental procedures or conditions.We formulate the task of yield prediction as a binary classification problem, by determining whether a reaction is a high-yielding reaction or not.We used only random sampling for our ICL examples as reactions in those datasets belong to the same type.For every dataset, we randomly select 30 samples for the validation process and 100 samples for testing from the original dataset.To evaluate the results, we use the classification accuracy as the evaluation metric, with UAGNN [34] serving as baseline.UAGNN reports state-of-the-art performance on yield prediction.It takes the graphs of reactants and products as input, and learns representation of these molecules through a graph neural network, and then predicts the scaled yield .</p>
<p>ICL prompt.We show our ICL prompt for yield prediction with an example from Buchwald-Hartwig dataset.As described in Figure 7, we incorporate an input explanation (wherein the reactants are separated by '.' and the products are split by '&gt;&gt;') to assist large language models.Additionally, output restrictions are enforced to ensure the generation of valid results.10.Our analysis reveals that in the task of yield prediction, GPT models perform below the established baseline model, UAGNN.However, it's worth noting that the UAGNN model was trained on the full training dataset including thousands of examples.Considering the spectrum of GPT models under scrutiny, GPT-4 emerges as the superior model, overshadowing both Davinci-003 and GPT-3.5 in predicting reaction yields.In the process of our investigation, we unearthed supporting evidence that signifies the role of ICL instances in the enhancement of model performance.This suggests an inherent correlation between the quantity of ICL data and the predictive accuracy of the models under consideration.This phenomenon is particularly in the case of GPT-4, we observed a significant improvement in performance when the number of ICL examples was increased from 4 to 8, both in the Buchwald-Hartwig and Suzukicoupling reactions.This indicates that even within the same model architecture, the amount of contextual data can significantly influence the predictive capabilities.Results.The results are reported in Table 14.We can observe that the best ICL prompting GPT models (GPT-4 and Davinci-003) can achieve competitive performance or even outperform the baseline in some metrics (BLEU, Levenshtein).Although the GPT models significantly underperform the baseline in terms of exact match and Morgan FTS metrics, it's important to note that we only utilize a maximum of 10 examples, which is substantially less than the training set (comprising 26,407 training examples) used for the baseline.These results demonstrate the strong few-shot text-based molecule design ability of GPT models.Last, not being exactly the same as the ground truth doesn't necessarily mean it's incorrect, especially in the context of molecular design.The molecules generated by GPT models may still be useful and can serve as alternatives to the ground truth, given they fulfill the requirements described in the input text and a majority (over 89%) are chemically valid.</p>
<p>Case studies.We select three different types of molecules (organic molecule without rings, organic molecule with ring, and metal atom) as examples, and show the generated molecules in Figure 12.</p>
<p>We observe that the structure of molecules generated by the GPT-4 (Scaffold, k=10) method is more similar to the ground truth compared to Davinci-003, GPT-4 (zero-shot), and even the baseline.Additionally, for metal atoms design, GPT models outperform the baseline which wrongly generates the SMILES instead of the metal atom.These cases show promising results of the molecule design ability of GPT models.However, evaluating whether the generated molecules are helpful such as molecule novelty in real-world scenarios is still a difficult problem.Thus we conclude that GPT models have excellent potential in molecule design and there are prospects for investigating this ability.</p>
<p>H Molecule Captioning</p>
<p>Molecule captioning is an important task in computational chemistry, offering valuable insights and applications in various areas such as drug discovery, materials science, and chemical synthesis.Given a molecule as input, the goal of this task is to generate a textual description that accurately describes the key features, properties, and functional groups of the molecule.We also use the ChEBI-20 dataset(CC BY 4.0) and the training set of it as the ICL candidates as discussed in the Text-Based Molecule Design Section.We use traditional captioning metrics including BLEU, ROUGE, and METEOR for evaluation.ICL Prompt.One example of our ICL prompt for molecule captioning is shown in Figure 13.Results.The results are reported in Table 15.We can observe that the best ICL prompting GPT models (GPT-4 and Davinci-003) can achieve competitive performance or even outperform the baseline in some metrics (BLEU-2 and BLEU-4).This indicates the inspiring capability of the GPT models in the molecule captioning task.14.We observe that although the performance of the baseline is close to GPT models, the captions generated by the baseline contain more descriptions that violate the chemical facts.In contrast, the captions generated by GPT-4 models contain only a few inaccurate descriptions, highlighting the excellent explaining ability of GPT models.This highlights the limitations of applying traditional Natural Language Processing (NLP) evaluation metrics to this task.Therefore, it is necessary to create more suitable evaluation metrics for chemistry-related generation tasks.</p>
<p>Figure 1 :
1
Figure 1: Overview of the evaluation process</p>
<p>Figure 2 :
2
Figure 2: The standardized zero-shot prompt template for all tasks.</p>
<p>[</p>
<p>Input explanation], [Output Explanation], and [Output Restrictions], specifically designed to reduce hallucinations.These components are tailored to each task.The {ICL} part is a straightforward concatenation of the demonstration examples and it follows the structure "[Input]: [Input_content][Output]: [Output_content]".The [Input] and [Output] denote the specific names of each task's input and output, respectively.For example, in the reaction prediction task, the [Input] would be "Reactants+Reagents" and the [Input_content] would be the actual SMILES of reactants and reagents.The [Output] would be "Products" and the [Output_content] would be the SMILES of products.Detailed ICL prompts for each task will be presented in their respective sections that follow.The last {Question} part presents the testing case for LLMs to respond to.Fig 5 is example of our name prediction prompt.</p>
<p>Figure 3 :
3
Figure 3: An ICL prompt template for all tasks.</p>
<p>Figure 4 :
4
Figure 4: An ICL prompt example for smiles2iupac prediction ICL strategies.To investigate the impact of the quality and quantity of ICL examples on the performance of each task, we explore two ICL strategies.The quality is determined by the retrieval methods employed for finding similar examples to the sample in question.We conduct a grid search across two strategies: {Random, Scaffold}.In the Random strategy, we randomly select k examples from the ICL candidate pool.In the Scaffold strategy, if the [Input_content] is a molecule SMILES, we use Tanimoto Similarity [55] from Morgan Fingerprint [41] with 2048-bits and radius=2 to calculate the molecular scaffold similarity to find the top-k similar molecule SMILES.If the [Input_content]is a description such as IUPAC name or others, we use Python's built-in difflib.SequenceMatcher tool[49] to find the top-k similar strings.To explore the influence of the quantity of ICL examples on performance, we also perform a grid search for k, the number of ICL examples, in each task.</p>
<p>Figure 5 :
5
Figure 5: An ICL prompt example for smiles2iupac prediction</p>
<p>Figure 7 :
7
Figure 7: An ICL prompt example for yield prediction</p>
<p>Figure 9 :
9
Figure 9: An ICL prompt example for reagents selection</p>
<p>Figure 10 :
10
Figure 10: An ICL prompt example for Retrosynthesis</p>
<p>Figure 11 :
11
Figure 11: An ICL prompt example for Text-Based Molecule Design</p>
<p>Figure 12 :
12
Figure 12: Examples of molecules generated by different models.</p>
<p>Figure 13 :
13
Figure 13: An ICL prompt example for molecule captioning</p>
<p>Table 1 :
1
The statistics of all tasks, datasets, the number of ICL/test samples, and evaluation metrics
AbilityTaskTask TypeDataset#ICL candidates#test Evaluation MetricsName PredictionGenerationPubChem500100AccuracyUnderstandingProperty PredictionClassificationBBBP, HIV, BACE, Tox21, ClinTox2053, 41127, 1514, 8014, 1484100Accuracy, F1 scoreYield PredictionClassificationBuchwald-Hartwig, Suzuki-Miyaura3957, 5650100AccuracyReasoningReaction Prediction Reagents SelectionGeneration RankingUSPTO-Mixed Suzuki-Miyaura409035 5760100 100Accuracy, Validity AccuracyRetrosynthesisGenerationUSPTO-50k40029100Accuracy, ValidityText-Based Molecule Design GenerationChEBI-2026407100BLEU, Exact Match, etcExplainingMolecule CaptioningGenerationChEBI-2026407100BLEU, Chemists, etc</p>
<p>Table 2 :
2
The rank of five LLMs on eight chemistry tasks and performance highlight (NC: not competitive, C: competitive, SC: selectively competitive, acc: accuracy).SC, 2 C, 3 NC perform the grid search of k in {5, 10} because of the maximum token limitation of LLMs.To reduce the time consumption of API requests caused by testing on the large test set, we first construct a validation set of size 30 which is randomly sampled from the original training set.Then we search k and retrieval strategies ({Random, Scaffold}) on the validation set.Based on the validation set results, we take 5 representative options when testing on 100 instances, which are randomly sampled from the original test set.For each task, we run evaluation 5 times and report mean and standard deviation.
TaskGPT-4 GPT-3.5 Davinci-003 Llama2-13B-chat GAL-30B Performance highlight (comparing to baselines if any)Name Prediction12345NC: max. acc. 8% (Table 4)Property Prediction12354SC: outperform RF and XGBoost from MoleculeNet [65] (Table 6)Yield Prediction13254C: but 16-20% lower acc. than UAGNN [34] (Table 10)Reaction Prediction13254NC: 70% lower acc. than Chemformer [26] (Table 11)Reagents Selection21345C: 40-50% acc. (Table 12)Retrosynthesis23154NC: 40% lower acc. than Chemformer [26] (Table 13)Molecule Design13245SC: better than MolT5-Large [17] (Table 14)Molecule Captioning12145SC: better than MolT5-Large [17] (Table 15)Average rank1.252.3752.1254.54.5overall: 3</p>
<p>Table 4 ,
4
6,7,11,13,14,15), using scaffold similarity to retrieve the most similar examples of the question as ICL examples achieves better performance than random sampling.•Inmost tasks (inTable 4, 6, 7, 10, 11, 14, 15), using larger k (more ICL examples) usually achieves better performance than small k (fewer ICL examples).These observations indicate that the quality and quantity of ICL examples plays an important role in the performance of ICL prompting [23, 36].This may inspire that it is necessary to design more chemistry-specific ICL methods to build high-quality ICL examples to further improve the ICL prompting performance.4.4 Are molecule SELFIES representations more suitable for LLMs than SMILES representations?SELFIES [33] representations are more machine-learning-friendly string representations of molecules.To investigate whether the SELFIES representations are more suitable for LLMs than SMILES representations, we conduct experiments on four tasks, including molecule property prediction, reaction prediction, molecule design and molecule captioning.The experiment results are shown in Table 16, 17, 18, 19.</p>
<p>Table 3
3: The F1(↑) and accuracy(↑) score of GPT-4 model(scaffold sampling, k = 8) on differenttemperature setting.F1(↑)BBBPBACEAccuracy(↑)BBBPBACEGPT-4(t=0.2) 0.667 ± 0.029 0.741 ± 0.019GPT-4(t=0.2) 0.650 ± 0.028 0.743 ± 0.019GPT-4(t=0.4) 0.712 ± 0.014 0.728 ± 0.024GPT-4(t=0.6) 0.683 ± 0.016 0.736 ± 0.020GPT-4(t=0.8) 0.686 ± 0.030 0.744 ± 0.025GPT-4(t=1.0) 0.684 ± 0.023 0.756 ± 0.025
GPT-4(t=0.4)0.691 ± 0.017 0.729 ± 0.024 GPT-4(t=0.6)0.659 ± 0.016 0.736 ± 0.019 GPT-4(t=0.8)0.661 ± 0.032 0.745 ± 0.025 GPT-4(t=1.0)0.660 ± 0.021 0.757 ± 0.025</p>
<p>Table 4 :
4
The accuracy (↑) of LLMs in 4 different name prediction tasks.The best LLM is in bold font.Here k is the number of examples used in few-shot ICL.The baseline is underlined and "-" indicates that STOUT cannot solve the smiles2formula and iupac2formula tasks.
Methodsmiles2iupac iupac2smiles smiles2formula iupac2formula</p>
<p>Table 10 :
10
Accuracy (↑) of yield prediction task.k is the number of examples used in few-shot ICL.The best LLM is in bold font, and the baseline is underlined.
Buchwald-Hartwig Suzuki-couplingUAGNN [34]0.9650.957GPT-4 (zero-shot)0.322 ± 0.0340.214 ± 0.019GPT-4 (random, k= 8)0.800±0.0080.764±0.013GPT-4 (random, k= 4)0.574 ± 0.0450.324 ± 0.018GPT-3.5 (random, k= 8)0.585 ± 0.0450.542 ± 0.011Davinci-003 (random, k= 8)0.467 ± 0.0130.341 ± 0.017Llama2-13B-chat0.008 ± 0.0070.006 ± 0.004GAL-30B00.008 ± 0.010</p>
<p>Table 17 :
17
Performance of SMILES and SELFIES of GPT-4 model in reaction prediction task.
Top-1 Accuracy (↑) Invalid SMILES/SELFIES (↓)SMILES0.230 ± 0.0227.0% ± 1.6%SELFIES0.110 ± 0.0071.0% ± 0.0%</p>
<p>Table 18 :
18
Performance of SMILES and SELFIES of GPT-4 model in molecule design task.
BLEU (↑)Exact (↑)Levenshtein (↓)Validity (↑)MACCS FTS (↑) RDK FTS (↑) Morgan FTS (↑)FCD (↓)SMILES 0.816 ± 0.004 0.174 ± 0.029 21.160 ± 0.600 0.888 ± 0.0230.867 ± 0.0050.738 ± 0.0100.672 ± 0.0136.224 ± 0.449SELFIES 0.277 ± 0.009 0.100 ± 0.016 76.162 ± 2.229 0.804 ± 0.0220.619 ± 0.0100.467 ± 0.0180.399 ± 0.01713.557 ± 0.224</p>
<p>Table 19 :
19
Performance of SMILES and SELFIES of GPT-4 model in molecule captioning task.SMILES 0.464 ± 0.008 0.365 ± 0.008 0.545 ± 0.003 0.362 ± 0.003 0.459 ± 0.007 0.519 ± 0.005 SELFIES 0.459 ± 0.012 0.367 ± 0.010 0.530 ± 0.007 0.360 ± 0.005 0.456 ± 0.005 0.490 ± 0.007
BLEU-2 (↑)BLEU-4 (↑)ROUGE-1 (↑) ROUGE-2 (↑) ROUGE-L (↑) METEOR (↑)
https://pubchem.ncbi.nlm.nih.gov
Acknowledgments and Disclosure of FundingThis work was supported by the National Science Foundation (CHE-2202693) through the NSF Center for Computer Assisted Synthesis (C-CAS).0.469 ± 0.025 0.504 ± 0.020 0.994 ± 0.006 0.528±0.0030.924±0.000GPT-3.5 (Scaffold, k= 8) 0.463 ± 0.008 0.406 ± 0.011 0.807 ± 0.021 0.529 ± 0.021 0.369 ± 0.029 Davinci-003 (Scaffold, k= 8) 0.378 ± 0.024 0.649 ± 0.021 0.832 ± 0.020 0.518±0.0090.850 ± 0.020 Llama2-13B-chat (Scaffold, k= 8) 0.002 ± 0.001 0.045 ± 0.015 0.069 ± 0.033 0.047 ± 0.013 0.001 ± 0.003 GAL-30B (Scaffold, k= 8) 0.074 ± 0.019 0.025 ± 0.013 0.014 ± 0.016 0.077 ± 0.046 0.081 ± 0.015 0.396 ± 0.023 0.650 ± 0.021 0.781 ± 0.004 0.682 ± 0.006 0.845 ± 0.010 Llama2-13B-chat (Scaffold, k= 8) 0.002 ± 0.003 0.048 ± 0.017 0.048 ± 0.025 0.053 ± 0.011 0.002 ± 0.004 GAL-30B (Scaffold, k= 8) 0.062 ± 0.007 0.020 ± 0.010 0.012 ± 0.009 0.030 ± 0.018 0.099 ± 0.007 models examined, GPT-4 surpasses both Davinci-003 and GPT-3.5 in predicting molecular properties.In our investigation, we have found evidence to support that the expansion of in-context learning (ICL) instances leads to a measurable enhancement in model performance.This underlines a direct relationship between the extent of ICL data and the predictive precision of our models.Concurrently, our research presents empirical evidence that scaffold sampling exceeds the performance of randomD Reaction PredictionReaction prediction is a central task in the field of chemistry, with significant implications for drug discovery, materials science, and the development of novel synthetic routes.Given a set of reactants, the goal of this task is to predict the most likely products formed during a chemical reaction[54,13,19].In this task, we use the widely adopted USPTO-MIT dataset<a href="MIT license">29</a> to evaluate the performance of GPT models.This dataset contains approximately 470,000 chemical reactions extracted from US patents.In the experiment, we used the USPTO mixed data set, where the reactants and reagents strings are split by '.'.We randomly sampled 30 samples from the original validation set for validation and 100 samples from the original test set for testing.We use the Top-1 Accuracy as the evaluation metric and Chemformer[26]as the baseline due to its superior performance among the machine learning solutions for reaction prediction.Chemformer is a seq2seq model trained to predict the output product when given reactants and reagents as input.We also report the percentage of invalid SMILES generated by each method.One example of our ICL prompt for reaction prediction is shown in Figure8.Given the nature of the reaction prediction task and the characteristics of the USPTO-MIT dataset, we enhance the task-specific template with an input explanation (stating that the input includes reactants and reagents, which are separated by '.') to assist the GPT models in understanding the input SMILES.Moreover, we incorporate output restrictions to guide GPT models in generating chemically valid and reasonable products.Results.The results are reported in Table11.We can observe that compared to the baseline, the performance of GPT models is considerably inferior, especially for the Zero-shot prompting (Top-1 Accuracy is only 0.004 and it generates 17.4% invalid SMILES).The less competitive results of GPT models can be attributed to the lack of in-depth understanding of the SMILES strings that represent reactants and products, as well as the reaction process that transforms reactants into products.It is also worth mentioning that the high accuracy achieved by Chemformer is due to its training on the complete dataset.More conclusions and detailed analysis are summarized in the section 5.E Reagents SelectionReagents selection, also known as reagent recommendation, involves the identification and proposal of the most fitting reagents for a specific chemical reaction or process.Compared to other prediction and generation tasks, these selection tasks might be more fitting for LLMs and carry extensive implications.Reagent recommendation can markedly enhance reaction design by pinpointing optimal reagents and conditions for a given reaction, thereby augmenting efficiency and effectiveness in both academic and industrial settings.Drawing from a vast corpus of chemical knowledge, GPT models may be able to generate suggestions, leading to chemical reactions with a greater likelihood of yielding superior results.In this study, we formulate four reaction component selection task from the Suzuki High-Throughput Experimentation (HTE) dataset.The dataset, created by Perera et al<a href="MIT license">44</a>, evaluates the Suzuki coupling of 5 electrophiles and 7 nucleophiles across a matrix of 11 ligands (with one blank), 7 bases (with one blank), and 4 solvents, resulting in a reaction screening dataset comprising 5,760 data points.The task of reagents selection can be divided into three categories: Reactant selection, Ligand Selection and Solvent selection.For validation, 30 examples were randomly sampled, while 100 examples were used for testing, all taken from the original datasets.Top-1 Accuracy serves as the assessment metric for both reactant and solvent selection, while Top-50% is utilized for ligand selection, as the upper half of the ligands in the list typically provide satisfactory yields in chemical reactions.This task is newly emergent in the field of chemistry, and as such, there are no established baselines yet.ICL prompt.One example of our ICL prompt for reagents selection is shown in Figure9. Considering the structure of the dataset and the characteristics of the reagents, we provide detailed task description and an answer template to guide GPT models towards the desired output.Results.Our results are presented in Table12.From the table, it is evident that GPT-4 and GPT-3.5 perform comparatively well in reagent selection tasks.This suggests a promising potential for GPT models in the realm of reagent selection.F RetrosynthesisRetrosynthesis planning is a crucial task in synthetic organic chemistry that involves identifying efficient synthetic pathways for a target molecule by recursively transforming it into simpler precursor molecules.In contrast to reaction prediction, retrosynthesis planning involves a reverse extrapolation from the target molecule to identify the readily available reactants for its synthesis.In this study, we use the USPTO-50k dataset<a href="MIT license">53</a>, which contains 50,037 chemical reactions.In our Results.The results are reported in Table13.The performance of GPT models is also inferior than the baseline due to the lack of an in-depth understanding of the SMILES strings that represent reactants and products.Detailed analysis are summarized in the later section 5 Discussion.G Text-Based Molecule DesignText-Based Molecule Design is a novel task in computational chemistry and drug discovery.It involves generating new molecules with desired molecule descriptions.In our experiment, we employ the ChEBI-20 dataset which consists of 33,010 molecule-description pairs.The dataset is split into 80/10/10% as the training/validation/test set<a href="CC BY 4.0">17</a>.We use the training set which contains 26407 molecule-description pairs as the ICL candidates.For comparison, we use the MolT5-Large[17]as the baseline.MolT5-Large is the initial effort to investigate the translation between molecules and text, including tasks such as text-based molecule design and molecule captioning.It builds upon T5[46], an encoder-decoder Transformer model, and benefits from pretraining on a large amount of dataset.To comprehensively evaluate the performance, we employ three different types of metrics.The first type of metric is the chemical similarity between the ground-truth molecules and generated molecules, measured by FTS (fingerprint Tanimoto Similarity)[55]in terms of MACCS[49], RDK[35], and Morgan[14].Secondly, we also use FCD (Fréchet ChemNet Distance)[45]which allows comparing molecules based on the latent information used to predict the activity of molecules[17].Since the generated molecules are in SMILES string format, we also employ natural language processing metrics including BLEU, Exact Match[17], and Levenshtein distance[40]between the ground-truth molecules and generated molecules SMILES.Finally, to evaluate whether generated molecules are valid, we use RDKIT[35]to check the validity of generated molecules and report the percent of the valid molecules.ICL Prompt.One ICL prompt example for text-based molecule design is shown in Figure11.I The comparison of SMILES and SELFIES
Predicting reaction performance in c-n cross-coupling using machine learning. Jesús G Derek T Ahneman, Shishi Estrada, Spencer D Lin, Abigail G Dreher, Doyle, Science. 36063852018</p>
<p>Emergent autonomous scientific research capabilities of large language models. Robert Daniil A Boiko, Gabe Macknight, Gomes, arXiv:2304.053322023arXiv preprint</p>
<p>Sam Andres M Bran, Andrew D Cox, Philippe White, Schwaller, arXiv:2304.05376Chemcrow: Augmenting large-language models with chemistry tools. 2023arXiv preprint</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang, Sparks of artificial general intelligence: Early experiments with gpt-4. 2023</p>
<p>Do large language models understand chemistry? a conversation with chatgpt. Cayque Monteiro, Castro Nascimento, André Silva Pimentel, Journal of Chemical Information and Modeling. 6362023</p>
<p>A survey on evaluation of large language models. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie Zhu, Hao Chen, Linyi Yang, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, arXiv:2307.031092023arXiv preprint</p>
<p>Capturing relations between scientific papers: An abstractive model for related work section generation. Xiuying Chen, Hind Alamro, Mingzhe Li, Shen Gao, Xiangliang Zhang, Dongyan Zhao, Rui Yan, Proc. of ACL. of ACL2021</p>
<p>Target-aware abstractive related work generation with contrastive learning. Xiuying Chen, Hind Alamro, Mingzhe Li, Shen Gao, Rui Yan, Xin Gao, Xiangliang Zhang, Proc. of SIGIR. of SIGIR2022</p>
<p>Scientific paper extractive summarization enhanced by citation graphs. Xiuying Chen, Mingzhe Li, Shen Gao, Rui Yan, Xin Gao, Xiangliang Zhang, Proc. of EMNLP. of EMNLP2022</p>
<p>Chatgpt goes to law school. Jonathan Choi, Kristin Hickman, Amy Monahan, Daniel Schwarcz, Journal of Legal Education. 2023</p>
<p>Scaling instruction-finetuned language models. Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, arXiv:2210.114162022arXiv preprint</p>
<p>Prediction of organic reaction outcomes using machine learning. Regina Connor W Coley, Tommi S Barzilay, William H Jaakkola, Green, Klavs, Jensen, ACS central science. 352017</p>
<p>Evaluation of gpt-3.5 and gpt-4 for supporting real-world information needs in healthcare delivery. Debadutta Dash, Rahul Thapa, Juan M Banda, Akshay Swaminathan, Morgan Cheatham, Mehr Kashyap, Nikesh Kotecha, Jonathan H Chen, Saurabh Gombar, Lance Downing, arXiv:2304.137142023arXiv preprint</p>
<p>A survey on in-context learning. Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, Zhifang Sui, 2023</p>
<p>Text2Mol: Cross-modal molecule retrieval with natural language queries. Carl Edwards, Chengxiang Zhai, Heng Ji, 10.18653/v1/2021.emnlp-main.47Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicAssociation for Computational LinguisticsNovember 2021Online and Punta Cana</p>
<p>Carl Edwards, Tuan Lai, Kevin Ros, Garrett Honke, Heng Ji, arXiv:2204.11817Translation between molecules and natural language. 2022arXiv preprint</p>
<p>Simon Frieder, Luca Pinchetti, Ryan-Rhys Griffiths, Tommaso Salvatori, Thomas Lukasiewicz, Philipp Christian Petersen, Alexis Chevalier, Julius Berner, arXiv:2301.13867Mathematical capabilities of chatgpt. 2023arXiv preprint</p>
<p>Modeling non-uniform uncertainty in reaction prediction via boosting and dropout. Taicheng Guo, Changsheng Ma, Xiuying Chen, Bozhao Nan, Kehan Guo, Shichao Pei, V Nitesh, Olaf Chawla, Xiangliang Wiest, Zhang, arXiv:2310.046742023arXiv preprint</p>
<p>Few-shot news recommendation via cross-lingual transfer. Taicheng Guo, Lu Yu, Basem Shihada, Xiangliang Zhang, 10.1145/3543507.3583383Proceedings of the ACM Web Conference 2023, WWW '23, page 1130-1140. the ACM Web Conference 2023, WWW '23, page 1130-1140New York, NY, USAAssociation for Computing Machinery2023</p>
<p>Few-shot graph learning for molecular property prediction. Zhichun Guo, Chuxu Zhang, Wenhao Yu, John Herr, Olaf Wiest, Meng Jiang, Nitesh V Chawla, Proceedings of the Web Conference 2021. the Web Conference 20212021</p>
<p>Graph-based molecular representation learning. Zhichun Guo, Bozhao Nan, Yijun Tian, Olaf Wiest, Chuxu Zhang, Nitesh V Chawla, arXiv:2207.048692022arXiv preprint</p>
<p>Structured prompting: Scaling in-context learning to 1,000 examples. Yaru Hao, Yutao Sun, Li Dong, Zhixiong Han, Yuxian Gu, Furu Wei, 2022</p>
<p>Aligning ai with shared human values. Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, Jacob Steinhardt, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2021</p>
<p>Measuring massive multitask language understanding. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2021</p>
<p>Chemformer: a pretrained transformer for computational chemistry. Ross Irwin, Spyridon Dimitriadis, Jiazhen He, Esben Jannik Bjerrum, Machine Learning: Science and Technology. 31150222022</p>
<p>Is gpt-3 all you need for low-data discovery in chemistry. Kevin Jablonka, Philippe Schwaller, Andrés Ortega-Guerrero, Berend Smit, 10.26434/chemrxiv-2023-fw8n42023</p>
<p>14 examples of how llms can transform materials science and chemistry: A reflection on a large language model hackathon. Kevin Maik, Jablonka , Qianxiang Ai, Alexander Al-Feghali, Shruti Badhwar, Joshua D Bran, Stefan Bringuier, Catherine Brinson, Kamal Choudhary, Defne Circi, Sam Cox, arXiv:2306.062832023arXiv preprint</p>
<p>Predicting organic reaction outcomes with weisfeiler-lehman network. Wengong Jin, Connor W Coley, Regina Barzilay, Tommi Jaakkola, 2017</p>
<p>Chatgptreshaping medical education and clinical management. Rehan Ahmed Khan, Masood Jawaid, Aymen Rehan Khan, Madiha Sajjad, Pakistan Journal of Medical Sciences. 3926052023</p>
<p>Decomposed prompting: A modular approach for solving complex tasks. Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, Ashish Sabharwal, arXiv:2210.024062022arXiv preprint</p>
<p>Pubchem 2019 update: improved access to chemical data. Sunghwan Kim, Jie Chen, Tiejun Cheng, Asta Gindulyte, Jia He, Siqian He, Qingliang Li, Benjamin A Shoemaker, Paul A Thiessen, Bo Yu, Nucleic acids research. 47D12019</p>
<p>Self-referencing embedded strings (SELFIES): A 100% robust molecular string representation. Mario Krenn, Florian Häse, Akshatkumar Nigam, Pascal Friederich, Alan Aspuru-Guzik, 10.1088/2632-2153/aba947Machine Learning: Science and Technology. 1445024oct 2020</p>
<p>Uncertainty-aware prediction of chemical reaction yields with graph neural networks. Youngchun Kwon, Dongseon Lee, Youn-Suk Choi, Seokho Kang, Journal of Cheminformatics. 142022</p>
<p>Rdkit: Open-source cheminformatics software. G A Landrum, 2020</p>
<p>Diverse demonstrations improve in-context compositional generalization. Itay Levy, Ben Bogin, Jonathan Berant, 2022</p>
<p>Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, Lingming Zhang, arXiv:2305.012102023arXiv preprint</p>
<p>Zequn Liu, Wei Zhang, Yingce Xia, Lijun Wu, Shufang Xie, Tao Qin, Ming Zhang, Tie-Yan Liu, arXiv:2305.10688Molxpt: Wrapping molecules with text for generative pre-training. 2023arXiv preprint</p>
<p>Extraction of chemical structures and reactions from the literature. Daniel Mark, Lowe , 2012University of CambridgePhD thesis</p>
<p>Levenshtein distance: Information theory, computer science, string (computer science), string metric, damerau? levenshtein distance, spell checker, hamming distance. Frederic P Miller, Agnes F Vandome, John Mcbrewster, 2009</p>
<p>The generation of a unique machine description for chemical structures-a technique developed at chemical abstracts service. Harry L Morgan, Journal of chemical documentation. 521965</p>
<p>Harsha Nori, Nicholas King, Scott Mayer Mckinney, Dean Carignan, Eric Horvitz, arXiv:2303.13375Capabilities of gpt-4 on medical challenge problems. 2023arXiv preprint</p>
<p>. OpenAI. Gpt-4 technical report. 2023</p>
<p>A platform for automated nanomole-scale reaction screening and micromole-scale synthesis in flow. Damith Perera, Joseph W Tucker, Shalini Brahmbhatt, Christopher J Helal, Ashley Chong, William Farrell, Paul Richardson, Neal W Sach, Science. 35963742018</p>
<p>Fréchet chemnet distance: a metric for generative models for molecules in drug discovery. Kristina Preuer, Philipp Renz, Thomas Unterthiner, Sepp Hochreiter, Gunter Klambauer, Journal of chemical information and modeling. 5892018</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, The Journal of Machine Learning Research. 2112020</p>
<p>Stout: Smiles to iupac names using neural machine translation. Kohulan Rajan, Achim Zielesny, Christoph Steinbeck, Journal of Cheminformatics. 1312021</p>
<p>Bayesian optimization of catalysts with in-context learning. Shane S Mayk Caldas Ramos, Marc D Michtavy, Andrew D Porosoff, White, arXiv:2304.053412023arXiv preprint</p>
<p>Pattern matching: The gestalt approach. David Ratcliff, John W Metzener, 1988</p>
<p>Suzuki-miyaura cross-coupling optimization enabled by automated feedback. Yi-Ming Brandon J Reizman, Stephen L Wang, Klavs F Buchwald, Jensen, Reaction chemistry &amp; engineering. 162016</p>
<p>On the use of real-world datasets for reaction yield prediction. Mandana Saebi, Bozhao Nan, John E Herr, Jessica Wahlers, Zhichun Guo, Andrzej M Zurański, Thierry Kogej, Per-Ola Norrby, Abigail G Doyle, Nitesh V Chawla, Chemical Science. 2023</p>
<p>Multitask prompted training enables zero-shot task generalization. Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, arXiv:2110.082072021arXiv preprint</p>
<p>What's what: The (nearly) definitive guide to reaction role assignment. Nadine Schneider, Nikolaus Stiefl, Gregory A Landrum, Journal of chemical information and modeling. 56122016</p>
<p>Molecular transformer: a model for uncertainty-calibrated chemical reaction prediction. Philippe Schwaller, Teodoro Laino, Théophile Gaudin, Peter Bolgar, Christopher A Hunter, Costas Bekas, Alpha A Lee, ACS central science. 592019</p>
<p>Elementary mathematical theory of classification and prediction. T Taffee, Tanimoto, Journal of Biomedical Science and Engineering. 1958</p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, Robert Stojnic, arXiv:2211.09085Galactica: A large language model for science. 2022arXiv preprint</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Chemical-reaction-aware molecule representation learning. Hongwei Wang, Weijiang Li, Xiaomeng Jin, Kyunghyun Cho, Heng Ji, Jiawei Han, Martin D Burke, arXiv:2109.098882021arXiv preprint</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, Denny Zhou, arXiv:2201.119032022arXiv preprint</p>
<p>Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. David Weininger, J. Chem. Inf. Comput. Sci. 281988</p>
<p>Smiles. 2. algorithm for generation of unique smiles notation. David Weininger, Arthur Weininger, Joseph L Weininger, J. Chem. Inf. Comput. Sci. 291989</p>
<p>The future of chemistry is language. A D White, 2023</p>
<p>Assessment of chemistry knowledge in large language models that generate code. Andrew D White, Glen M Hocky, A Heta, Mehrad Gandhi, Sam Ansari, Geemi P Cox, Subarna Wellawatte, Ziyue Sasmal, Kangxin Yang, Yuvraj Liu, Willmor J Peña Singh, Ccoa, 10.1039/D2DD00087CDigital Discovery. 22023</p>
<p>Are multilingual models effective in code-switching?. Genta Indra Winata, Samuel Cahyawijaya, Zihan Liu, Zhaojiang Lin, Andrea Madotto, Pascale Fung, arXiv:2103.133092021arXiv preprint</p>
<p>Moleculenet: a benchmark for molecular machine learning. Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, S Aneesh, Karl Pappu, Vijay Leswing, Pande, Chemical science. 922018</p>
<p>Agieval: A human-centric benchmark for evaluating foundation models. Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, Nan Duan, arXiv:2304.063642023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
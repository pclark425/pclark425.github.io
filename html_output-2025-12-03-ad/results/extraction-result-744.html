<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-744 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-744</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-744</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-20.html">extraction-schema-20</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <p><strong>Paper ID:</strong> paper-222291391</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2010.04917v1.pdf" target="_blank">Generalized Independent Noise Condition for Estimating Linear Non-Gaussian Latent Variable Graphs</a></p>
                <p><strong>Paper Abstract:</strong> Causal discovery aims to recover causal structures or models underlying the observed data. Despite its success in certain domains, most existing methods focus on causal relations between observed variables, while in many scenarios the observed ones may not be the underlying causal variables (e.g., image pixels), but are generated by latent causal variables or confounders that are causally related. To this end, in this paper, we consider Linear, Non-Gaussian Latent variable Models (LiNGLaMs), in which latent confounders are also causally related, and propose a Generalized Independent Noise (GIN) condition to estimate such latent variable graphs. Specifically, for two observed random vectors $\mathbf{Y}$ and $\mathbf{Z}$, GIN holds if and only if $\omega^{\intercal}\mathbf{Y}$ and $\mathbf{Z}$ are statistically independent, where $\omega$ is a parameter vector characterized from the cross-covariance between $\mathbf{Y}$ and $\mathbf{Z}$. From the graphical view, roughly speaking, GIN implies that causally earlier latent common causes of variables in $\mathbf{Y}$ d-separate $\mathbf{Y}$ from $\mathbf{Z}$. Interestingly, we find that the independent noise condition, i.e., if there is no confounder, causes are independent from the error of regressing the effect on the causes, can be seen as a special case of GIN. Moreover, we show that GIN helps locate latent variables and identify their causal structure, including causal directions. We further develop a recursive learning algorithm to achieve these goals. Experimental results on synthetic and real-world data demonstrate the effectiveness of our method.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e744.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e744.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GIN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generalized Independent Noise condition</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A condition for linear non-Gaussian latent variable models (LiNGLaM) which tests whether a particular linear combination (surrogate) of one observed variable set is independent of another observed set; if so, it implies that causally earlier latent common causes of the first set d-separate it from the second set.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Generalized Independent Noise (GIN) condition</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Construct a surrogate variable E_{Y||Z} = ω^T Y where ω is any non-zero vector satisfying ω^T E[Y Z^T] = 0 (computed from the empirical cross-covariance between Y and Z). Test whether this surrogate ω^T Y is statistically independent of Z (nonparametrically, e.g., with HSIC). If independent, declare (Z, Y) satisfies GIN, which under LiNGLaM assumptions implies that a causally earlier subset of latent common causes of Y d-separates Y from Z. The paper provides mathematical sufficient conditions and graphical interpretations, and uses GIN both to detect latent structure and to infer causal ordering among latent variables.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Synthetic LiNGLaM simulations and Byrne teacher-burnout observational survey</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Non-interactive observational datasets: (1) synthetic datasets generated from LiNGLaM (various synthetic graph cases) used to evaluate cluster recovery and causal ordering; (2) a real-world psychometric dataset (Barbara Byrne teacher burnout study) with 28 observed variables. Not an interactive/virtual lab; no active interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Constructs a linear surrogate variable (ω^T Y) chosen to null cross-covariance with Z, thereby removing influences of shared latent causes; independence tests (HSIC) then detect remaining dependence attributable to other shared latent components.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Latent confounders (shared latent causes), spurious correlations induced by unobserved causal factors, measurement noise</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Solving ω^T E[Y Z^T] = 0 from cross-covariance to produce a surrogate expected to remove shared-latent influence, then applying a nonparametric independence test (HSIC) between ω^T Y and Z to detect residual dependence.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Implicit: the linear surrogate ω^T Y algebraically cancels components that correlate with Z (removes/reduces influence of shared latent sources); no explicit regularization or weighting scheme beyond selection of ω via covariance constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Independence testing (HSIC) of the surrogate vs Z; failure to achieve independence refutes the hypothesis that the considered latent subset d-separates Y and Z (i.e., indicates remaining spurious/shared signals).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Qualitative: GIN-based method achieved the best performance (lowest latent omission/commission and mismeasurement errors) among compared methods (BPC, FOFC, LSTC) on synthetic LiNGLaM cases; causal-order identification accuracy improved towards 1 as sample size increased across cases. Exact numeric metrics reported in paper (latent omission/commission, mismeasurements, correct-ordering rate) but not reproduced here numerically.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GIN provides a principled way to detect and remove spurious correlations induced by causally earlier latent common causes by constructing a surrogate linear combination and testing its independence with other observed sets; this enables locating latent variables, determining their number behind observed groups, and recovering causal order among latent variables in LiNGLaM. GIN outperformed covariance-rank-only methods (BPC, FOFC) and the Triad/LSTC approach in synthetic experiments, especially when multiple latent variables influence observed sets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generalized Independent Noise Condition for Estimating Linear Non-Gaussian Latent Variable Graphs', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e744.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e744.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GIN-alg</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GIN condition-based recursive learning algorithm (Algorithms 1 & 2)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recursive two-step algorithm that (1) finds causal clusters of observed variables sharing the same latent direct causes by testing GIN on subsets, and (2) recovers causal order among the latent variable sets by root-finding using GIN tests.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>GIN-based recursive LiNGLaM estimation algorithm</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Step 1: enumerate candidate subsets (increasing latent-dimension Len), test GIN for (X\P, P) using surrogate construction and independence testing (pairwise tests combined with Fisher's method) to detect causal clusters and estimate Dim(L(P)) = Dim(P)-1; merge overlapping clusters sharing the same latent parents. Step 2: recursively identify root latent variable sets by testing asymmetry-based GIN conditions between halves of cluster children and other clusters, thereby deriving a causal order among latent sets. Uses HSIC for independence tests and Fisher's method to combine pairwise p-values when assessing independence between surrogate and multivariate Z.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Synthetic LiNGLaM simulations and Byrne teacher-burnout observational survey</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Evaluation performed on simulated LiNGLaM graphs (several structural cases with 2-4+ latent variables, sample sizes N=500,1000,2000) and a 28-variable psychometric observational dataset; no interactive interventions used.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Surrogate-variable construction (linear projection ω computed from cross-covariance) to remove influences of certain latent sets; independence testing to detect whether residual dependence (spurious signal) remains; merging/cluster rules rely on these tests to avoid falsely grouping variables influenced by different latent sources.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Latent confounders (multiple latent variables behind observed groups), measurement noise, correlations induced by shared latent structure</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Empirical cross-covariance to find ω such that ω^T E[Y Z^T] = 0, then HSIC independence test between ω^T Y and Z; pairwise independence tests aggregated with Fisher's method when multivariate independence is approximated by pairwise tests.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Algebraic removal via ω (surrogate) rather than statistical downweighting; merging rules reduce influence of falsely detected clusters by requiring structural consistency across GIN tests.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Failure of the GIN independence test (HSIC) serves to refute hypothesized d-separation and thus to prevent incorrect clustering or causal direction claims; consistency checks across multiple GIN tests used in recursive ordering.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Qualitative: On synthetic LiNGLaM datasets, the algorithm recovered causal clusters with lower latent omission and commission rates than BPC and FOFC, and achieved high correct-ordering rates that improved with larger N; in the teacher-burnout data it produced interpretable clusters and causal ordering similar to domain knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The recursive algorithm leveraging GIN can (under LiNGLaM assumptions) locate latent variables, determine how many latents lie behind observed groups, and recover causal order among latent sets; the surrogate independence test is central to detecting and rejecting spurious groupings caused by shared latent structure.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generalized Independent Noise Condition for Estimating Linear Non-Gaussian Latent Variable Graphs', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e744.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e744.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Independent Noise condition (IN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A condition used in linear non-Gaussian causal discovery (LiNGAM) without latent confounders: the residual of regressing an effect on its candidate causes is independent of those causes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DirectLiNGAM: a direct method for learning a linear non-Gaussian structural equation model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Independent Noise (IN) condition</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>For scalar Y and set Z, compute regression residual r = Y - ω^T Z (ω = E[Y Z^T] E[Z Z^T]^{-1}). If r is statistically independent of Z, then all variables in Z are causally earlier than Y and there is no unmeasured common cause between any variable in Z and Y. IN underlies LiNGAM style causal identification without latents.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>General observational linear non-Gaussian settings (LiNGAM literature)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Assumes no latent confounders; used in observational causal discovery procedures where residual independence can be tested (e.g., via HSIC). Not an interactive/experimental procedure.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Designed for the no-latent case; used to detect presence/absence of latent confounding by testing residual independence (so indirectly addresses latent confounders by flagging IN failure).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Regression residual independence testing (e.g., nonparametric tests like HSIC or other independence measures).</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>If residual is dependent on Z, the IN hypothesis is refuted, indicating either some variable in Z is not causally earlier or there is an unmeasured confounder.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The paper shows IN is a special case of GIN (when Z and latent L are deterministically related), motivating a unified treatment: GIN generalizes IN to settings with latent causal variables and allows detection of spurious correlations induced by latents that IN cannot handle.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generalized Independent Noise Condition for Estimating Linear Non-Gaussian Latent Variable Graphs', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e744.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e744.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HSIC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hilbert-Schmidt Independence Criterion</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A kernel-based nonparametric statistical test of independence between random variables, used to detect dependence in non-Gaussian settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A kernel statistical test of independence</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Hilbert-Schmidt Independence Criterion (HSIC) test</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Compute a kernel-based dependence statistic between two (possibly multivariate) variables; significance assessed via permutation or asymptotic approximation. The paper uses HSIC to test independence between the surrogate ω^T Y and Z because data and residuals are non-Gaussian.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Applied to synthetic LiNGLaM data and real psychometric dataset within the GIN testing pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Non-interactive observational datasets; HSIC used as a test component within the GIN-based algorithm to detect residual dependence (i.e., remaining spurious signals).</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Used as the statistical detector for residual dependence after surrogate construction; enables identifying whether supposed removal of latent-induced correlation succeeded.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Residual dependence due to latent confounders or other shared non-Gaussian noise components</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Kernel-based dependence statistic (HSIC) between surrogate ω^T Y and Z; small HSIC (non-significant) indicates successful removal of spurious shared signal.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Significant HSIC rejects the hypothesis that the surrogate is independent of Z, refuting the presumed d-separation and indicating spurious/shared signals remain.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Used successfully as the chosen independence test in experiments; authors set kernel width (e.g., 0.05 for Byrne data) and report good empirical performance of the GIN pipeline using HSIC.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>HSIC is an appropriate nonparametric test within the GIN framework for non-Gaussian LiNGLaM data and successfully identifies remaining dependence indicative of spurious latent-induced correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generalized Independent Noise Condition for Estimating Linear Non-Gaussian Latent Variable Graphs', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e744.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e744.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fisher-comb</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fisher's method for combining p-values</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A classical technique to combine multiple independent p-values into a single test statistic following a chi-square distribution; used here to aggregate pairwise independence tests when assessing multivariate independence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Statistical methods for research workers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Fisher's method (combining p-values)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Compute p_k for each pairwise independence test between components of surrogate and components of Z, then compute statistic -2 * sum(log p_k), which under the null (pairwise independence) follows chi-square with 2c degrees of freedom; used to approximate multivariate independence when directly testing multivariate independence is avoided.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Synthetic LiNGLaM simulations and Byrne teacher-burnout observational survey</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Applied in the GIN-based causal cluster discovery algorithm where direct multivariate independence tests are approximated by aggregating pairwise tests.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Aggregation of multiple pairwise independence p-values into a single test decision to detect residual dependence from distractors across multiple variable components.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Residual dependence across multivariate components due to shared latent causes</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Combines pairwise independence p-values into an overall test statistic; significant aggregate indicates residual dependence (spurious signals) remain.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>A non-significant combined test statistic supports independence (refuting presence of residual spurious dependence), while a significant statistic refutes the independence hypothesis.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Fisher's method is used pragmatically to combine multiple pairwise independence tests when assessing whether the surrogate ω^T Y is independent of a multivariate Z, enabling the practical implementation of the GIN test in multivariate settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generalized Independent Noise Condition for Estimating Linear Non-Gaussian Latent Variable Graphs', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e744.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e744.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LSTC / Triad</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LSTC (Triad condition) / Triad constraints for latent structure</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A previously proposed condition (Triad) and algorithm (LSTC) that use independence relations among triples for discovering latent-variable structure; cited as related work and as a more restrictive special case of GIN.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Triad constraints for learning causal structure of latent variables</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Triad condition / LSTC algorithm</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Triad condition examines independence relations among triples of observed variables to infer latent structure and causal relations among latent variables; LSTC is an algorithm leveraging these triad constraints. The current paper notes Triad is a restrictive/special case of GIN and that LSTC does not handle cases where multiple latent variables underlie two observed variable sets.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Referenced in comparative evaluation on synthetic LiNGLaM structures</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>LSTC applied to observational synthetic data in prior work; in this paper LSTC is used as a baseline in experiments (comparison on cluster recovery and causal order tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Uses triadic independence constraints to detect latent factors and reject spurious pairwise associations, but limited: cannot handle multiple latent variables behind two observed sets per authors' discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Latent confounders (but limited to simpler configurations where triad constraints apply)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Tests of specific triad-based independence relationships among triplets of observed variables (details in Cai et al., 2019); in this paper LSTC is compared empirically.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Triad independence violations refute certain latent-structure hypotheses; limited applicability when multiple latents influence pairs of observed sets.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>In comparisons reported in this paper, LSTC has some strengths (low mismeasurement in certain cases with small N) but cannot handle multiple-latent cases (e.g., Case 4) and generally performs worse than GIN on several metrics; specific numeric comparisons are in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Triad/LSTC is a more restrictive approach than GIN and fails in scenarios with multiple latent variables behind observed groups; GIN generalizes triad constraints by allowing multivariate Y and Z and leveraging cross-covariance-based surrogate construction plus independence testing to handle richer latent structures.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generalized Independent Noise Condition for Estimating Linear Non-Gaussian Latent Variable Graphs', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Triad constraints for learning causal structure of latent variables <em>(Rating: 2)</em></li>
                <li>DirectLiNGAM: a direct method for learning a linear non-Gaussian structural equation model <em>(Rating: 2)</em></li>
                <li>Causal clustering for 1-factor measurement models <em>(Rating: 2)</em></li>
                <li>Causal inference in the presence of latent variables and selection bias <em>(Rating: 2)</em></li>
                <li>Discovering unconfounded causal relationships using linear nongaussian models <em>(Rating: 1)</em></li>
                <li>Learning linear bayesian networks with latent variables <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-744",
    "paper_id": "paper-222291391",
    "extraction_schema_id": "extraction-schema-20",
    "extracted_data": [
        {
            "name_short": "GIN",
            "name_full": "Generalized Independent Noise condition",
            "brief_description": "A condition for linear non-Gaussian latent variable models (LiNGLaM) which tests whether a particular linear combination (surrogate) of one observed variable set is independent of another observed set; if so, it implies that causally earlier latent common causes of the first set d-separate it from the second set.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Generalized Independent Noise (GIN) condition",
            "method_description": "Construct a surrogate variable E_{Y||Z} = ω^T Y where ω is any non-zero vector satisfying ω^T E[Y Z^T] = 0 (computed from the empirical cross-covariance between Y and Z). Test whether this surrogate ω^T Y is statistically independent of Z (nonparametrically, e.g., with HSIC). If independent, declare (Z, Y) satisfies GIN, which under LiNGLaM assumptions implies that a causally earlier subset of latent common causes of Y d-separates Y from Z. The paper provides mathematical sufficient conditions and graphical interpretations, and uses GIN both to detect latent structure and to infer causal ordering among latent variables.",
            "environment_name": "Synthetic LiNGLaM simulations and Byrne teacher-burnout observational survey",
            "environment_description": "Non-interactive observational datasets: (1) synthetic datasets generated from LiNGLaM (various synthetic graph cases) used to evaluate cluster recovery and causal ordering; (2) a real-world psychometric dataset (Barbara Byrne teacher burnout study) with 28 observed variables. Not an interactive/virtual lab; no active interventions.",
            "handles_distractors": true,
            "distractor_handling_technique": "Constructs a linear surrogate variable (ω^T Y) chosen to null cross-covariance with Z, thereby removing influences of shared latent causes; independence tests (HSIC) then detect remaining dependence attributable to other shared latent components.",
            "spurious_signal_types": "Latent confounders (shared latent causes), spurious correlations induced by unobserved causal factors, measurement noise",
            "detection_method": "Solving ω^T E[Y Z^T] = 0 from cross-covariance to produce a surrogate expected to remove shared-latent influence, then applying a nonparametric independence test (HSIC) between ω^T Y and Z to detect residual dependence.",
            "downweighting_method": "Implicit: the linear surrogate ω^T Y algebraically cancels components that correlate with Z (removes/reduces influence of shared latent sources); no explicit regularization or weighting scheme beyond selection of ω via covariance constraints.",
            "refutation_method": "Independence testing (HSIC) of the surrogate vs Z; failure to achieve independence refutes the hypothesis that the considered latent subset d-separates Y and Z (i.e., indicates remaining spurious/shared signals).",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Qualitative: GIN-based method achieved the best performance (lowest latent omission/commission and mismeasurement errors) among compared methods (BPC, FOFC, LSTC) on synthetic LiNGLaM cases; causal-order identification accuracy improved towards 1 as sample size increased across cases. Exact numeric metrics reported in paper (latent omission/commission, mismeasurements, correct-ordering rate) but not reproduced here numerically.",
            "performance_without_robustness": null,
            "has_ablation_study": false,
            "number_of_distractors": null,
            "key_findings": "GIN provides a principled way to detect and remove spurious correlations induced by causally earlier latent common causes by constructing a surrogate linear combination and testing its independence with other observed sets; this enables locating latent variables, determining their number behind observed groups, and recovering causal order among latent variables in LiNGLaM. GIN outperformed covariance-rank-only methods (BPC, FOFC) and the Triad/LSTC approach in synthetic experiments, especially when multiple latent variables influence observed sets.",
            "uuid": "e744.0",
            "source_info": {
                "paper_title": "Generalized Independent Noise Condition for Estimating Linear Non-Gaussian Latent Variable Graphs",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "GIN-alg",
            "name_full": "GIN condition-based recursive learning algorithm (Algorithms 1 & 2)",
            "brief_description": "A recursive two-step algorithm that (1) finds causal clusters of observed variables sharing the same latent direct causes by testing GIN on subsets, and (2) recovers causal order among the latent variable sets by root-finding using GIN tests.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "GIN-based recursive LiNGLaM estimation algorithm",
            "method_description": "Step 1: enumerate candidate subsets (increasing latent-dimension Len), test GIN for (X\\P, P) using surrogate construction and independence testing (pairwise tests combined with Fisher's method) to detect causal clusters and estimate Dim(L(P)) = Dim(P)-1; merge overlapping clusters sharing the same latent parents. Step 2: recursively identify root latent variable sets by testing asymmetry-based GIN conditions between halves of cluster children and other clusters, thereby deriving a causal order among latent sets. Uses HSIC for independence tests and Fisher's method to combine pairwise p-values when assessing independence between surrogate and multivariate Z.",
            "environment_name": "Synthetic LiNGLaM simulations and Byrne teacher-burnout observational survey",
            "environment_description": "Evaluation performed on simulated LiNGLaM graphs (several structural cases with 2-4+ latent variables, sample sizes N=500,1000,2000) and a 28-variable psychometric observational dataset; no interactive interventions used.",
            "handles_distractors": true,
            "distractor_handling_technique": "Surrogate-variable construction (linear projection ω computed from cross-covariance) to remove influences of certain latent sets; independence testing to detect whether residual dependence (spurious signal) remains; merging/cluster rules rely on these tests to avoid falsely grouping variables influenced by different latent sources.",
            "spurious_signal_types": "Latent confounders (multiple latent variables behind observed groups), measurement noise, correlations induced by shared latent structure",
            "detection_method": "Empirical cross-covariance to find ω such that ω^T E[Y Z^T] = 0, then HSIC independence test between ω^T Y and Z; pairwise independence tests aggregated with Fisher's method when multivariate independence is approximated by pairwise tests.",
            "downweighting_method": "Algebraic removal via ω (surrogate) rather than statistical downweighting; merging rules reduce influence of falsely detected clusters by requiring structural consistency across GIN tests.",
            "refutation_method": "Failure of the GIN independence test (HSIC) serves to refute hypothesized d-separation and thus to prevent incorrect clustering or causal direction claims; consistency checks across multiple GIN tests used in recursive ordering.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Qualitative: On synthetic LiNGLaM datasets, the algorithm recovered causal clusters with lower latent omission and commission rates than BPC and FOFC, and achieved high correct-ordering rates that improved with larger N; in the teacher-burnout data it produced interpretable clusters and causal ordering similar to domain knowledge.",
            "performance_without_robustness": null,
            "has_ablation_study": false,
            "number_of_distractors": null,
            "key_findings": "The recursive algorithm leveraging GIN can (under LiNGLaM assumptions) locate latent variables, determine how many latents lie behind observed groups, and recover causal order among latent sets; the surrogate independence test is central to detecting and rejecting spurious groupings caused by shared latent structure.",
            "uuid": "e744.1",
            "source_info": {
                "paper_title": "Generalized Independent Noise Condition for Estimating Linear Non-Gaussian Latent Variable Graphs",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "IN",
            "name_full": "Independent Noise condition (IN)",
            "brief_description": "A condition used in linear non-Gaussian causal discovery (LiNGAM) without latent confounders: the residual of regressing an effect on its candidate causes is independent of those causes.",
            "citation_title": "DirectLiNGAM: a direct method for learning a linear non-Gaussian structural equation model",
            "mention_or_use": "mention",
            "method_name": "Independent Noise (IN) condition",
            "method_description": "For scalar Y and set Z, compute regression residual r = Y - ω^T Z (ω = E[Y Z^T] E[Z Z^T]^{-1}). If r is statistically independent of Z, then all variables in Z are causally earlier than Y and there is no unmeasured common cause between any variable in Z and Y. IN underlies LiNGAM style causal identification without latents.",
            "environment_name": "General observational linear non-Gaussian settings (LiNGAM literature)",
            "environment_description": "Assumes no latent confounders; used in observational causal discovery procedures where residual independence can be tested (e.g., via HSIC). Not an interactive/experimental procedure.",
            "handles_distractors": false,
            "distractor_handling_technique": null,
            "spurious_signal_types": "Designed for the no-latent case; used to detect presence/absence of latent confounding by testing residual independence (so indirectly addresses latent confounders by flagging IN failure).",
            "detection_method": "Regression residual independence testing (e.g., nonparametric tests like HSIC or other independence measures).",
            "downweighting_method": null,
            "refutation_method": "If residual is dependent on Z, the IN hypothesis is refuted, indicating either some variable in Z is not causally earlier or there is an unmeasured confounder.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "The paper shows IN is a special case of GIN (when Z and latent L are deterministically related), motivating a unified treatment: GIN generalizes IN to settings with latent causal variables and allows detection of spurious correlations induced by latents that IN cannot handle.",
            "uuid": "e744.2",
            "source_info": {
                "paper_title": "Generalized Independent Noise Condition for Estimating Linear Non-Gaussian Latent Variable Graphs",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "HSIC",
            "name_full": "Hilbert-Schmidt Independence Criterion",
            "brief_description": "A kernel-based nonparametric statistical test of independence between random variables, used to detect dependence in non-Gaussian settings.",
            "citation_title": "A kernel statistical test of independence",
            "mention_or_use": "use",
            "method_name": "Hilbert-Schmidt Independence Criterion (HSIC) test",
            "method_description": "Compute a kernel-based dependence statistic between two (possibly multivariate) variables; significance assessed via permutation or asymptotic approximation. The paper uses HSIC to test independence between the surrogate ω^T Y and Z because data and residuals are non-Gaussian.",
            "environment_name": "Applied to synthetic LiNGLaM data and real psychometric dataset within the GIN testing pipeline",
            "environment_description": "Non-interactive observational datasets; HSIC used as a test component within the GIN-based algorithm to detect residual dependence (i.e., remaining spurious signals).",
            "handles_distractors": true,
            "distractor_handling_technique": "Used as the statistical detector for residual dependence after surrogate construction; enables identifying whether supposed removal of latent-induced correlation succeeded.",
            "spurious_signal_types": "Residual dependence due to latent confounders or other shared non-Gaussian noise components",
            "detection_method": "Kernel-based dependence statistic (HSIC) between surrogate ω^T Y and Z; small HSIC (non-significant) indicates successful removal of spurious shared signal.",
            "downweighting_method": null,
            "refutation_method": "Significant HSIC rejects the hypothesis that the surrogate is independent of Z, refuting the presumed d-separation and indicating spurious/shared signals remain.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Used successfully as the chosen independence test in experiments; authors set kernel width (e.g., 0.05 for Byrne data) and report good empirical performance of the GIN pipeline using HSIC.",
            "performance_without_robustness": null,
            "has_ablation_study": false,
            "number_of_distractors": null,
            "key_findings": "HSIC is an appropriate nonparametric test within the GIN framework for non-Gaussian LiNGLaM data and successfully identifies remaining dependence indicative of spurious latent-induced correlations.",
            "uuid": "e744.3",
            "source_info": {
                "paper_title": "Generalized Independent Noise Condition for Estimating Linear Non-Gaussian Latent Variable Graphs",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "Fisher-comb",
            "name_full": "Fisher's method for combining p-values",
            "brief_description": "A classical technique to combine multiple independent p-values into a single test statistic following a chi-square distribution; used here to aggregate pairwise independence tests when assessing multivariate independence.",
            "citation_title": "Statistical methods for research workers",
            "mention_or_use": "use",
            "method_name": "Fisher's method (combining p-values)",
            "method_description": "Compute p_k for each pairwise independence test between components of surrogate and components of Z, then compute statistic -2 * sum(log p_k), which under the null (pairwise independence) follows chi-square with 2c degrees of freedom; used to approximate multivariate independence when directly testing multivariate independence is avoided.",
            "environment_name": "Synthetic LiNGLaM simulations and Byrne teacher-burnout observational survey",
            "environment_description": "Applied in the GIN-based causal cluster discovery algorithm where direct multivariate independence tests are approximated by aggregating pairwise tests.",
            "handles_distractors": true,
            "distractor_handling_technique": "Aggregation of multiple pairwise independence p-values into a single test decision to detect residual dependence from distractors across multiple variable components.",
            "spurious_signal_types": "Residual dependence across multivariate components due to shared latent causes",
            "detection_method": "Combines pairwise independence p-values into an overall test statistic; significant aggregate indicates residual dependence (spurious signals) remain.",
            "downweighting_method": null,
            "refutation_method": "A non-significant combined test statistic supports independence (refuting presence of residual spurious dependence), while a significant statistic refutes the independence hypothesis.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": false,
            "number_of_distractors": null,
            "key_findings": "Fisher's method is used pragmatically to combine multiple pairwise independence tests when assessing whether the surrogate ω^T Y is independent of a multivariate Z, enabling the practical implementation of the GIN test in multivariate settings.",
            "uuid": "e744.4",
            "source_info": {
                "paper_title": "Generalized Independent Noise Condition for Estimating Linear Non-Gaussian Latent Variable Graphs",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "LSTC / Triad",
            "name_full": "LSTC (Triad condition) / Triad constraints for latent structure",
            "brief_description": "A previously proposed condition (Triad) and algorithm (LSTC) that use independence relations among triples for discovering latent-variable structure; cited as related work and as a more restrictive special case of GIN.",
            "citation_title": "Triad constraints for learning causal structure of latent variables",
            "mention_or_use": "mention",
            "method_name": "Triad condition / LSTC algorithm",
            "method_description": "Triad condition examines independence relations among triples of observed variables to infer latent structure and causal relations among latent variables; LSTC is an algorithm leveraging these triad constraints. The current paper notes Triad is a restrictive/special case of GIN and that LSTC does not handle cases where multiple latent variables underlie two observed variable sets.",
            "environment_name": "Referenced in comparative evaluation on synthetic LiNGLaM structures",
            "environment_description": "LSTC applied to observational synthetic data in prior work; in this paper LSTC is used as a baseline in experiments (comparison on cluster recovery and causal order tasks).",
            "handles_distractors": true,
            "distractor_handling_technique": "Uses triadic independence constraints to detect latent factors and reject spurious pairwise associations, but limited: cannot handle multiple latent variables behind two observed sets per authors' discussion.",
            "spurious_signal_types": "Latent confounders (but limited to simpler configurations where triad constraints apply)",
            "detection_method": "Tests of specific triad-based independence relationships among triplets of observed variables (details in Cai et al., 2019); in this paper LSTC is compared empirically.",
            "downweighting_method": null,
            "refutation_method": "Triad independence violations refute certain latent-structure hypotheses; limited applicability when multiple latents influence pairs of observed sets.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "In comparisons reported in this paper, LSTC has some strengths (low mismeasurement in certain cases with small N) but cannot handle multiple-latent cases (e.g., Case 4) and generally performs worse than GIN on several metrics; specific numeric comparisons are in the paper.",
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Triad/LSTC is a more restrictive approach than GIN and fails in scenarios with multiple latent variables behind observed groups; GIN generalizes triad constraints by allowing multivariate Y and Z and leveraging cross-covariance-based surrogate construction plus independence testing to handle richer latent structures.",
            "uuid": "e744.5",
            "source_info": {
                "paper_title": "Generalized Independent Noise Condition for Estimating Linear Non-Gaussian Latent Variable Graphs",
                "publication_date_yy_mm": "2020-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Triad constraints for learning causal structure of latent variables",
            "rating": 2,
            "sanitized_title": "triad_constraints_for_learning_causal_structure_of_latent_variables"
        },
        {
            "paper_title": "DirectLiNGAM: a direct method for learning a linear non-Gaussian structural equation model",
            "rating": 2,
            "sanitized_title": "directlingam_a_direct_method_for_learning_a_linear_nongaussian_structural_equation_model"
        },
        {
            "paper_title": "Causal clustering for 1-factor measurement models",
            "rating": 2,
            "sanitized_title": "causal_clustering_for_1factor_measurement_models"
        },
        {
            "paper_title": "Causal inference in the presence of latent variables and selection bias",
            "rating": 2,
            "sanitized_title": "causal_inference_in_the_presence_of_latent_variables_and_selection_bias"
        },
        {
            "paper_title": "Discovering unconfounded causal relationships using linear nongaussian models",
            "rating": 1,
            "sanitized_title": "discovering_unconfounded_causal_relationships_using_linear_nongaussian_models"
        },
        {
            "paper_title": "Learning linear bayesian networks with latent variables",
            "rating": 1,
            "sanitized_title": "learning_linear_bayesian_networks_with_latent_variables"
        }
    ],
    "cost": 0.0182535,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Generalized Independent Noise Condition for Estimating Linear Non-Gaussian Latent Variable Graphs
arXiv:2010.04917v1 [cs.LG] 10 Oct 2020</p>
<p>Feng Xie xiefeng009@gmail.com 
School of Computer Science
Guangdong University of Technology
GuangzhouChina</p>
<p>School of Mathematical Sciences
Peking University
BeijingChina</p>
<p>Ruichu Cai cairuichu@gdut.edu.cn 
School of Computer Science
Guangdong University of Technology
GuangzhouChina</p>
<p>Biwei Huang biweih@andrew.cmu.edu 
Department of Philosophy
Carnegie Mellon University
PittsburghUSA</p>
<p>Clark Glymour 
Department of Philosophy
Carnegie Mellon University
PittsburghUSA</p>
<p>Zhifeng Hao zfhao@gdut.edu.cn 
School of Computer Science
Guangdong University of Technology
GuangzhouChina</p>
<p>School of Mathematics and Big Data
Foshan University
FoshanChina</p>
<p>Kun Zhang 
Department of Philosophy
Carnegie Mellon University
PittsburghUSA</p>
<p>Generalized Independent Noise Condition for Estimating Linear Non-Gaussian Latent Variable Graphs
arXiv:2010.04917v1 [cs.LG] 10 Oct 2020
Causal discovery aims to recover causal structures or models underlying the observed data. Despite its success in certain domains, most existing methods focus on causal relations between observed variables, while in many scenarios the observed ones may not be the underlying causal variables (e.g., image pixels), but are generated by latent causal variables or confounders that are causally related. To this end, in this paper, we consider Linear, Non-Gaussian Latent variable Models (LiNGLaMs), in which latent confounders are also causally related, and propose a Generalized Independent Noise (GIN) condition to estimate such latent variable graphs. Specifically, for two observed random vectors Y and Z, GIN holds if and only if ω ⊺ Y and Z are statistically independent, where ω is a parameter vector characterized from the cross-covariance between Y and Z. From the graphical view, roughly speaking, GIN implies that causally earlier latent common causes of variables in Y d-separate Y from Z. Interestingly, we find that the independent noise condition, i.e., if there is no confounder, causes are independent from the error of regressing the effect on the causes, can be seen as a special case of GIN. Moreover, we show that GIN helps locate latent variables and identify their causal structure, including causal directions. We further develop a recursive learning algorithm to achieve these goals. Experimental results on synthetic and real-world data demonstrate the effectiveness of our method. * These authors contributed equally to this work. The work was done while FX visiting CMU.34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.</p>
<p>Introduction</p>
<p>Identifying causal relationships from observational data, known as causal discovery, has drawn much attention in the fields of empirical science and artificial intelligence [Spirtes et al., 2010, Pearl, 2019. Most causal discovery approaches focus on the situation without latent variables, such as the PC algorithm [Spirtes and Glymour, 1991], Greedy Equivalence Search (GES) [Chickering, 2002], and methods based on the Linear, Non-Gaussian Acyclic Model (LiNGAM) [Shimizu et al., 2006], the Additive Noise Model (ANM) , Peters et al., 2014, and the Post-NonLinear causal model (PNL) [Zhang and Hyvärinen, 2009]. However, although these methods have been used in a range of fields, they may fail to provide a satisfactory result in cases with latent variables, because of the ignorance of the influences from latent variables. Some approaches have been starting to handle causal discovery with latent variables. One line is constraint-based methods, including the FCI algorithm [Spirtes et al., 1995], RFCI [Colombo et al., 2012], and its variants. They focus on estimating the causal relationships between observed variables rather than that between latent variables. However, in real-world scenarios, it may not be the case-there are also causal relationships between latent variables. Later, it was shown that by utilizing vanishing Tetrad conditions [Spearman, 1928] and, more generally, t-separation, one is able to identify latent variables in linear-Gaussian models [Silva et al., 2006, Sullivant et al., 2010. Furthermore, by leveraging an extended t-separation [Spirtes, 2013], a more reliable and faster algorithm, called FindOneFactorClusters (FOFC), was developed [Kummerfeld and Ramsey, 2016]. However, these methods may not be able to identify causal directions between latent variables, and they require strong constraints that each latent variable should have at least three pure measurement variables 2 . Such limitation is because they only rely on rank constraints on the covariance matrix, but fail to take into account higher-order statistics. To make use of higher-order information, one may apply overcomplete independent component analysis (O-ICA) [Hoyer et al., 2008, Shimizu et al., 2009], but it does not consider the causal structure between latent variables and the size of the equivalence class of the identified structure could be very large [Entner andHoyer, 2010, Tashiro et al., 2014]. Anandkumar et al. [2013] only extracts second-order statistics in identifying latent factors, although it uses non-Gaussianity when estimating the causal relations between latent variables. Zhang et al. [2017], Huang* et al. [2020] consider a special type of confounders due to distribution shifts.</p>
<p>Recently, a condition about a particular type of independence relationship between any three variables, called Triad condition, was proposed [Cai et al., 2019], together with the LSTC algorithm to discover the structure between latent variables. Nevertheless, this method does not apply to the case where there are multiple latent variables behind two observed variables.</p>
<p>It is well known that one may use the independent noise condition to recover the causal structure from linear non-Gaussian data without latent variables [Shimizu et al., 2011]. Then a question naturally rises: is it possible to solve the latent-variable problem, by introducing non-Gaussianity and a condition similar to the independent noise condition? Interestingly, we find that it can be achieved by testing the independence between ω ⊺ Y and Z, where Y and Z are two observed random vectors,</p>
<p>and ω is a parameter vector based on the cross-covariance between Y and Z. If ω ⊺ Y and Z are statistically independent, we term this condition Generalized Independent Noise (GIN) condition. We show that the well-known independent noise condition can be seen as a special case of GIN. From the view of graphical models, roughly speaking, if the GIN condition holds, then in the Linear Non-Gaussian Latent variable Model (LiNGLaM), the causally earlier latent common causes of variables in Y d-separate Y from Z. By leveraging GIN, we further develop a practical algorithm to identify important information of the LiNGLaM, including where the latent variables are, the number of latent variables behind any two observed variables, and the causal order of the latent variables.</p>
<p>The contributions of this work are three-fold. 1) We define the GIN condition for an ordered pair of variables sets, provide mathematical conditions that are sufficient for it, and show that the independent noise condition can be seen as its special case. We then further establish a connection between the GIN condition and the graphical patterns in the LiNGLaM, including specific d-separation relations. 2) Next, we exploit GIN to estimate the LiNGLaM, which allows causal relationships between latent variables and multiple latent variables behind any two observed variables. 3) Compared to existing work, a uniquely appealing feature of the proposed method is that it is able to identify the causal order of the latent variables and determine the number of latent variables behind any two observed variables.</p>
<p>Problem Definition</p>
<p>In this paper, we focus on a particular type of linear acyclic latent variable causal models. We use V = X ∪ L to denote the total set of variables, where X denote the set of observed variables, with X = {X 1 , X 2 , ...X m }, and L denote the set of latent variables, with L = {L 1 , L 2 , ...L n }. We assume that any variable in V satisfy the following generating process:
V i = ∑ k(j)&lt;k(i) b ij V j + ε V i , i = 1, 2, .
.., m + n, where k(i) represents the causal order of variables in a directed acyclic graph, so that no later variable causes any earlier variable, b ij denotes the causal strength from V j to V i , and ε V i are independent and identically distributed noise variables. 2 The variable is neither the cause nor the effect of other measurement variables Without loss of generality, we assume that all variables have a zero mean (otherwise can be centered). The definition of our model is given below.  Figure 1: A causal structure involving 4 latent variables and 8 observed variables, where each pair of observed variables in {X 1 , X 2 , X 3 , X 4 } are affected by two latent variables.
L 1 L 2 L 3 L 4 X 1 X 2 X 3 X 4 X 5 X 6 X 7 X 8 a 1 a 2 a3 a 4 b 1 b2 b 3 b 4 c1 c 2 d1 d 2 α γ β σ η θ
The key difference to existing researches considering linear latent models, such as [Bollen, 1989, Silva et al., 2006, is that we introduce the assumptions A2∼A4, allowing us to identify the casual structure over latent variables, including casual directions. Figure 1 shows a simple example that satisfies the LiNGLaM. For Non-Gaussianity Assumption, the non-Gaussian distribution are expected to be ubiquitous, due to Cramér Decomposition Theorem [Cramér, 1962], as stated in [Spirtes and Zhang, 2016]. Notice that the Double-Pure Child Variable Assumption is much milder than that in Tetrad-based methods: for latent variable set L ′ , we only need 2Dim(L ′ ) pure observed variables, while Tetrad needs 2Dim(L ′ ) + 1 pure observed variables. In Section 6, we will briefly discuss the situation where Assumption A4 is violated.</p>
<p>GIN Condition and Its Implications in LiNGLaM</p>
<p>In this section, we first briefly review the Independent Noise (IN) condition in linear non-Gaussian causal models. Then we formulate the Generalized Independent Noise (GIN) condition and show that it contains the independent noise condition as a special case. We further illustrate how GIN is applied to identify causal relations between latent variables of any two considered groups of observed variables. Finally, we present theoretical results regarding the graphical implications of the GIN condition, which can be used to discover latent variable structures.</p>
<p>Independent Noise Condition in Functional Causal Models</p>
<p>Below, we give the independent noise condition, which has been used in causal discovery of linear, non-Gaussian networks without confounders (e.g., in [Shimizu et al., 2011] 
Y ||Z = Y −ω ⊺ Z is independent from Z.
Lemma 1 in [Shimizu et al., 2011]  (A) 1) All variables in Z are causally earlier than Y , and 2) there is no common cause for each variable in Z and Y that is not in Z.
(B) (Z, Y ) satisfies the IN condition.</p>
<p>Generalized Independent Noise Condition</p>
<p>Below, we first give the definition of the GIN condition, followed by an illustrative example. Definition 3 (GIN condition). Let Y and Z be two observed random vectors. Suppose the variables follow the linear non-Gaussian acyclic causal model. Define the surrogate-variable of Y relative to Z, as  [Cai et al., 2019] can be seen as a restrictive, special case of the GIN condition, where Dim(Y) = 2 and Dim(Z) = 1. We give an example to illustrate that there is a connection between this condition and the causal structure. According to the structure in Figure 1 and by assuming faithfulness, we have that ({X 4 , X 5 }, {X 1 , X 2 , X 3 }) satisfies the GIN condition, as explained below. The causal models of latent variables is L 1 = ε L 1 , L 2 = αL 1 + ε L 2 = αε L 1 + ε L 2 , and L 3 = βL 1 + σL 2 + ε L 3 = (β + ασ)ε L 1 + σε L 2 + ε L 3 , and {X 1 , X 2 , X 3 } and {X 4 , X 5 } can then be represented as
E Y||Z ≔ ω ⊺ Y,(1)⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ X 1 X 2 X 3 ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ Y = ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ a 1 b 1 a 2 b 2 a 3 b 3 ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ L 1 L 2 + ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ ε X 1 ε X 2 ε X 3 ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ E Y , X 4 X 5 Z = a 4 b 4 βc 1 σc 1 L 1 L 2 + ε X 4 ε X ′ 5 E Z ,(2)
where ε X ′ 5 = c 2 ε L 3 + ε X 5 . According to the above equations,
ω ⊺ E[YZ ⊺ ] = 0 ⇒ ω = [a 2 b 3 − b 2 a 3 , a 1 b 3 − b 1 a 3 , b 1 a 2 − a 1 b 2 ] ⊺ . Then we can see E Y||Z = ω ⊺ Y = E Y , and further because E Y ⫫ Z, we have E Y||Z ⫫ Z.
That is to say, ({X 4 , X 5 }, {X 1 , X 2 , X 3 }) satisfies the GIN condition. Intuitively, we have E Y||Z ⫫ Z because although {X 1 , X 2 , X 3 } generated by {L 1 , L 2 } and {L 1 , L 2 } are not measurable, E Y||Z , as a particular linear combination of Y = {X 1 , X 2 , X 3 }, successfully removes the influences of {L 1 , L 2 } by properly making use of Z = {X 4 , X 5 } as a "surrogate."</p>
<p>Next, we discuss a situation where GIN is violated. For example, in this structure, ({X 3 , X 6 }, {X 1 , X 2 , X 5 }) violates GIN. Specifically, the corresponding variables satisfy the following equations:
⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ X 1 X 2 X 5 ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ Y = ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ a 1 b 1 a 2 b 2 βc 1 σc 2 ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ L 1 L 2 + ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ ε X 1 ε X 2 ε X ′ 5 ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ E Y , X 3 X 6 Z = a 3 b 3 βc 2 σc 2 L 1 L 2 + ε X 3 ε X ′ 6 E Z ,(3)
where ε X ′ 6 = c 2 ε L 3 + ε X 6 . Then under faithfulness assumption, we can see
ω T Y Z because E Y E Z (there exists common component ε L 3 for ε X ′ 5 and ε X ′ 6 ), no matter ω ⊺ E[YZ ⊺ ] = 0 or not.
In Section 3.3, we will further investigate graphical implications of GIN in LiNGLaM. For the example given in Figure 1, we have the following observation. ({X 4 , X 5 }, {X 1 , X 2 , X 3 }) satisfies the GIN condition, and {L 1 , L 2 }, the latent common causes for {X 1 , X 2 , X 3 }, d-separate {X 1 , X 2 , X 3 } from {X 4 , X 5 }. In contrast, ({X 3 , X 6 }, {X 1 , X 2 , X 5 }) violates GIN, and {X 1 , X 2 , X 5 } and {X 3 , X 6 } are not d-separated conditioning on {L 1 , L 2 }, the latent common causes of {X 1 , X 2 , X 5 }.</p>
<p>The following theorem gives mathematical characterizations of the GIN condition, by providing sufficient conditions for when (Z, Y) satisfies the GIN condition. In the next subsection, we give its implication in LiNGLaM; thanks to the constraints implied by the LiNGLaM, one is able to provide sufficient graphical conditions for GIN to hold. Theorem 1. Suppose that random vectors L, Y, and Z are related in the following way: Theorem 1 gives the mathematical conditions, under which (Z, Y) satisfies the GIN condition. Continue the example in Figure 1.
Y = AL + E Y ,(4)Z = BL + E Z . (5) Denote by l the dimensionality of L. Assume A is of full column rank. Then, if 1) Dim(Y) &gt; l, 2) E Y ⫫ L, 3) E Y ⫫ E Z , 5and 4Let Z = {X 4 , X 5 } and Y = {X 1 , X 2 , X 3 }, and thus L = {L 1 , L 2 }.
One then can find the following facts:
Dim(Y) = 2 &gt; l, E Y ⫫ L and E Y ⫫ E Z according to Eq. 2, and Σ LZ = E[LZ ⊺
] has full row rank, i.e., 2. Therefore, (Z, Y) satisfies the GIN condition.</p>
<p>All proofs are given in Supplementary Material.</p>
<p>The following proposition shows that the IN condition can be seen as a special case of the GIN condition with E Z = 0 (i.e., Z and L are linearly deterministically related).</p>
<p>Proposition 2. LetŸ ≔ (Y, Z). Then the following statements hold:
1. (Z,Ÿ ) follows the GIN condition if and only if (Z, Y ) follows it. 2. If (Z, Y ) follows the IN condition, then (Z,Ÿ ) follows the GIN condition.
Proposition 2 inspires a unified method to handle causal relations between latent variables and those between latent and observed variables. Please see the discussion in Section 6 for more details.</p>
<p>Graphical Criteria of GIN in Terms of LiNGLaM</p>
<p>In this section, we investigate graphical implications of the GIN condition in LiNGLaM, which then inspires us to exploit the GIN condition to discover the graph containing latent variables. Specifically, first, the following theorem shows the connection between GIN and the graphical properties of the variables in terms of LiNGLaM. We denote by L(X q ) the set of latent variables that are the parents of X q and by L(Y) the set of latent variables that are parents of any component of Y.</p>
<p>We say variable set S 1 is an exogenous set relative to variable set S 2 if and only if S 2 ⊆ S 1 or for any variable V that is in S 2 but not in S 1 . According to the (marginalized) causal graph over {V } ∪ S 1 , V does not cause any variable in S 1 and there is no direct common cause for V and any of its parents that is not in S 1 . For instance, according to the structure in Figure 1, let S 1 = {L 1 } and S 2 = {L 3 , L 4 }, S 1 is an exogenous set relative to S 2 . On the contrary, if S 1 = {L 2 , L 3 } and S 2 = {L 3 , L 4 }, S 1 is not an exogenous set relative to S 2 , because L 4 and its direct parent L 2 have direct common cause L 1 , which is not in {L 2 , L 3 }.  Roughly speaking, S 1 is an exogenous set relative to S 2 if S 1 contains causally earlier variables (according to the causal order) in or before S 2 . Hence, intuitively, the theorem states that (Z, Y) satisfies the GIN condition when causally earlier common causes of Y d-separate Y from Z. We can then see the asymmetry of this condition for (Z, Y) relative to L(Y) and L(Z). For instance, assuming faithfulness, according to the structure in Figure 1
L, 0 ≤ k ≤ min(Dim(Y) − 1, Dim(Z)), denoted by S k L , such that 1) S k L is an exogenous set relative to L(Y), that 2) S k L d-separates Y from Z,, ({X 1 , X 2 }, {X 3 , X 4 , X 5 }) satisfies GIN (with S 2 L = {L 1 , L 2 }), but ({X 1 , X 6 }, {X 3 , X 4 , X 5 }) does not.
Next, we discuss how to identify the group of observed variables that share the same set of latent direct causes; we call such a set of observed variables a causal cluster. The following theorem formalizes the property of causal clusters and gives a criterion for finding such causal clusters. 5 Note that we do not assume E Z ⫫ L.</p>
<p>Theorem 3. Let X be the set of all observed variables in a LiNGLaM and Y be a proper subset of X. If (X \ Y, Y) follows the GIN condition and there is no subsetỸ ⊆ Y such that (X \Ỹ,Ỹ) follows the GIN condition, then Y is a causal cluster and Dim(L(Y)) = Dim(Y) − 1.</p>
<p>Consider the example in Figure 1, for {X 5 , X 6 }, one can find ({X 1 , ..., X 4 , X 7 , X 8 }, {X 5 , X 6 }) follows the GIN condition, so {X 5 , X 6 } is a causal cluster and Dim(L({X 5 , X 6 })) = Dim({X 5 , X 6 }) − 1 = 1(i.e., L 3 ). But, for {X 1 , X 2 , X 5 }, ({X 3 , X 4 , X 6 , X 7 , X 8 }, {X 1 , X 2 , X 5 }) violates the GIN condition, thus {X 1 , X 2 , X 5 } is not a causal cluster.</p>
<p>Furthermore, we discuss how to identify the causal direction between latent variables based on their corresponding children. The following theorem shows the asymmetry between the underlying latent variables in terms of the GIN condition.</p>
<p>Theorem 4. Let S p and S q be two causal clusters of a LiNGLaM. Assume there is no latent confounder behind L(S p ) and L(S q ), and L(S p ) ∩ L(S q ) = ∅. Further suppose that S p contains 2Dim(L(S p )) number of variables with S p = {P 1 , P 2 , ..., P 2Dim(L(S p )) } and that
S q contains 2Dim(L(S q )) number of variables with S q = {Q 1 , Q 2 , ..., Q 2Dim(L(S q )) }. Then if ({P Dim(L(S p ))+1 , ...P 2Dim(L(S p )) }, {P 1 , ...., P Dim(L(S p )) , Q 1 , ...Q Dim(L(S q )) }) follows the GIN condition, L(S p ) → L(S q ) holds.
Consider the example in Figure 1. For two clusters {X 1 , X 2 , X 3 , X 4 } and {X 5 , Y 6 }, where their sets of latent direct causes do not have confounders, one can find
({X 3 , X 4 }, {X 1 , X 2 , X 5 }) follows GIN condition, so {L 1 , L 2 } → {L 3 }.</p>
<p>GIN Condition-Based Algorithm for Estimating LiNGLaM</p>
<p>In this section, we leverage the above theoretical results and propose a recursive algorithm to discover the structural information of LiNGLaM. The basic idea of the algorithm is that it first finds all causal clusters from the observed data (Step 1), and then it learns the causal order of the latent variables behind these causal clusters (</p>
<p>Step 2). The completeness of the algorithm is shown in sections 4.1 (Theorem 3 and Proposition 3 for step 1) and 4.2 (Proposition 4 for step 2).</p>
<p>Step 1: Finding the Causal Clusters</p>
<p>To find causal clusters efficiently, one may start with finding clusters with a single latent variable and merge the overlapping culsters, and then increase the number of allowed latent variables until all variables are put in the clusters. We need to consider two practical issues involved in the algorithm. The first is how to find causal clusters and determine how many latent variables they contain, and the second is what clusters should be merged. Theorem 3 answers the first question. Next, for the merge problem, we find that the overlapping clusters can be directly merged into one cluster. This is because the overlapping clusters have the same latent variable as parents in LiNGLaM. The validity of the merge step is guaranteed by Proposition 3, with the algorithm given in Algorithm 1.</p>
<p>Proposition 3. Let S 1 and S 2 be two clusters of a LiNGLaM and Dim(L(S 1 )) = Dim(L(S 2 )). If S 1 and S 2 are overlapping, S 1 and S 2 share the same set of latent variables. Merge all the overlapping sets in S; 10: P ← P \ S, and Len ← Len + 1;</p>
<p>11:</p>
<p>Add variables set T for each cluster in the S into P such that Dim(T ) = Len; 12: until P is empty; 13: Return: S To test the independence (line 5 in Algorithm 1) between two sets of variables, we evaluate the pairwise independence with the Fisher's method [Fisher, 1950] instead of testing the independence between E Y||Z and Z directly. In particular, denote by p k , with k = 1, 2, ..., c, all resulting p-values from pairwise independence tests. We compute the test statistic as −2 ∑ c k=1 logp k , which follows the chi-square distribution with 2c degrees of freedom when all the pairs are independent. Example 1. Consider the example in Figure 1. First, we set Len = 1 to find the clusters with a single latent variable, i.e., we find {X 5 , X 6 } and {X 7 , X 8 } based on Theorem 3 (Line 4-9). Then we set Len = 2 and find the clusters {X 1 , X 2 , X 3 , X 4 } with two latent variables.</p>
<p>Step 2: Learning the Causal Order of Latent Variables</p>
<p>After identifying all clusters, next, we aim to discover the causal order of the set of latent variables of corresponding causal clusters. As an immediate consequence of Theorem 4, the root latent variable can be identified by checking the GIN condition, as stated in the following lemma. Lemma 1. Let S r be a cluster and S k , k ≠ r, be any other cluster of a LiNGLaM. Suppose that S r contains 2Dim(L(S r )) number of variables with S r = {R 1 , R 2 , ..., R 2Dim(L(S r )) } and that S k contains 2Dim(L(S k )) number of variables with
S k = {K 1 , K 2 , ..., K 2Dim(L(S k )) }. if ({R Dim(L(S r ))+1 , ...R 2Dim(L(S r )) , {R 1 , ...., R Dim(L(S r )) , K 1 , ...K Dim(L(S k )) }) follows the GIN condition, then L(S r ) is a root latent variable set.
Now, the key issue is how to use this lemma to recursively discover the "root variable" 6 until the causal order of latent variables is fully determined. Interestingly, we find that in every iteration, we only need to add the children (i.e., the corresponding causal cluster) of the root variable set into the testing set, such that the number of testing latent variables increases when testing the GIN condition in the following steps. Recall the example discussed in Figure 1 GIN condition 7 , which means that L 3 is the "root variable". Intuitively speaking, adding the children of the root variable includes the information of the root variable set and create a new "root variable", which helps further remove the effect from them. Accordingly, we have the following proposition to guarantee the correctness of the above process. The details of the process are given in Algorithm 2. T = T ∪ S r ; 7: end while 8: Return: Causal order K Example 2. Continue to consider the example in Figure 1. We have found the three causal clusters in step 1, i.e., S 1 = {X 1 , X 2 , X 3 }, S 2 = {X 5 , X 6 }, and S 3 = {X 7 , X 8 }. Now, we first find that L(S 1 ) is the root variable because ({X 3 , X 4 }, {X 1 , X 2 , X 5 }) and ({X 3 , X 4 }, {X 1 , X 2 , X 7 }) both satisfy the GIN condition (Line 3). Next, we find L(S 2 ) is the "root variable" because ({X 6 , X 3 , X 4 }, {X5, X7, X 1 , X 2 }) satisfies the GIN condition (Line 3-6). Finally, we return the causal order K = {L(S 1 ), L(S 2 ), L(S 3 )}.
. For L 3 , we find that ({X 6 , X 3 , X 4 }, {X 5 , X 7 , X 1 , X 2 }) satisfies the GIN condition, while for L 4 , ({X 8 , X 3 , X 4 }, {X 5 , X 7 , X 1 , X 2 }) violates the</p>
<p>Experimental Results</p>
<p>To show the efficacy of the proposed approach, we apply it to both synthetic and real-world data.</p>
<p>Synthetic Data</p>
<p>In the following simulation studies, we consider four typical cases: Case 1 &amp; 2 have two latent variables L 1 and L 2 , with L 1 → L 2 ; Case 3 has three latent variables L 1 , L 2 , and L 3 , with L 2 ← L 1 → L 3 , and L 2 → L 3 ; Case 4 has four latent variables {L 1 , L 2 }, L 3 , and L 4 , with {L 1 , L 2 } → L 3 , We compared our algorithm with BPC [Silva et al., 2006], FOFC [Kummerfeld and Ramsey, 2016] 8 , and LSTC [Cai et al., 2019]. We measure the estimation accuracy on two tasks: 1) finding the causal clusters, i.e., locating latent variables, and 2) discovering the causal order of latent variables. Note that BPC and FOFC are only applicable to the first task.</p>
<p>To evaluate the accuracy of the estimated causal cluster, we follow the evaluation metrics from [Cai et al., 2019]. Specifically, we use Latent omission= OL T L , Latent commission= F L T L , and Mismea-surement= MO T O , where OL is the number of omitted latent variables, F L is the number of falsely detected latent variables, T L is the total number of latent variables in the ground truth graph, M O is the number of falsely observed variables that have at least one incorrectly measured latent, and T O is the number of observed variables in the ground truth graph. To evaluate the quality of the causal order, we further use the correct-ordering rate as a metric. Each experiment is repeated 10 times with randomly generated data and the results are averaged. Here, we use the Hilbert-Schmidt Independence Criterion (HSIC) test [Gretton et al., 2008] as the independence test because the data are non-Gaussian. 0.00(0) 0.00(0) 1.00 (10)   As shown in Table 1, our algorithm, GIN, achieves the best performance (the lowest errors) on almost all cases of the structures. We notice that although the Mismeasurements of GIN are higher than LSTC in Case 3 when the sample size is small (N=500), the Latent commission of GIN are lower than LSTC. The BPC and FOFC algorithms (with distribution-free tests) do not perform well, which implies that the rank constraints on covariance matrix is not enough to recover more latent structures. Interestingly, although the LSTC algorithm has low errors of the Latent omission in Case 2 (it may be because the structure in Case 2 can be transformed into equivalent pure structures [Cai et al., 2019]), it can not tell us the number of latent variables behind observed variables. Moreover, LSTC fails to recover Case 4 because of the multiple latent variables. The above results demonstrate a clear advantage of our method over the comparisons.</p>
<p>Considering that BPC and FOFC algorithms can not discover the causal directions of latent variables, we only report the results of LSTC algorithm and our algorithm on causal order learning in Figure 2. As shown in Figure 2, the accuracy of the identified causal ordering of our method gradually increases to 1 with the sample size in all the four cases. LSTC can not handle Case 2 &amp; 4. These findings illustrate that our algorithm can discover the correct causal order.</p>
<p>Real-World Data</p>
<p>Barbara Byrne conducted a study to investigate the impact of organizational (role ambiguity, role conflict, classroom climate, and superior support, etc.) and personality (self-esteem, external locus of control) on three facets of burnout in full-time elementary teachers [Byrne, 2010]. We applied our algorithm to this data set, with 28 observed variables in total (see the detailed variable list in supplementary).</p>
<p>Causal Clusters</p>
<p>Observed variables
S 1 (1) RC 1 , RC 2 , W O 1 , W O 2 , DM 1 , DM 2 S 2 (1) CC 1 , CC 2 ,CC 3 ,CC 4 S 3 (1) P S 1 , P S 2 S 4 (1)
ELC 1 , ELC 2 ,ELC 3 ,ELC 4 , ELC 5 S 5 (2) SE 1 , SE 2 , SE 3 , EE 1 , EE 2 , EE 3 , DP 1 , P A 3 , CC 1 S 6 (3) DP 2 , P A 1 , P A 2 , P A 3 Figure 3: The output of Algorithm 1 in the teacher's burnout study.</p>
<p>In the implementation, the kernel width in the HSIC test is set to 0.05. We first apply Algorithm 1 and receive six causal clusters, including one cluster with 2 latent variables and one cluster with 3 latent variables. The results are given in Table 3. Next, we apply Algorithm 2 and get the final causal order (from root to leaf): L(S 1 ) ≻ L(S 2 ) ≻ L(S 3 ) ≻ L(S 5 ) ≻ L(S 4 ) ≻ L(S 6 ). Specifically, we have the following findings. 1. The identified clusters are similar to the domain knowledge, e.g, S 2 represents the classroom climate, S 3 represents the peer support, S 4 represents the external locus of control, et al. 2. The learned causal order is similar to Byrne's conclusion, e.g., personal accomplishment (L(S 6 )) are caused by other latent factors. In addition, role conflict and decision making (L(S 1 )), classroom climate (L(S 2 )), and peer support (L(S 3 )) cause burnout (including emotional exhaustion, depersonalization, and personal accomplishment (L(S 5 ) and L(S 6 ))).</p>
<p>Discussion and Further Work</p>
<p>The preceding sections presented how to use GIN conditions to locate the latent variables and identify their causal structure in the LiNGLaM. In this procedure we examine whether the ordered pair of two disjoint subsets of the observed variables satisfies GIN. As shown in Proposition 2, the GIN condition actually contains IN as a special case, in which the two subsets of variables have overlapping variables. For instance, suppose we have only two variables with X 1 → X 2 . Then (X 1 , X 2 ) satisties IN, and (X 1 , (X 2 , X 1 )) satisfies GIN. As a consequence, interestingly, even if we allow edges between observed variables in the LiNGLaM, the GIN condition may also be used to identify them, together with them connections. For instance, in figure 4(a), ({X 1 , X 3 }, {X 2 , X 3 , X 4 }) satisfies the GIN condition while ({X 1 , X 4 }, {X 2 , X 3 , X 4 }) violates the GIN condition, which means that there is an edge between X 3 and X 4 and X 3 → X 4 . In contrast, with the GIN condition on pairs of disjoint subsets of variables, one cannot distinguish between structures (a) and (b). Developing an efficient algorithm that is able to recover the the LiNGLaM with directed edges between observed variables in a principled way is part of our future work. Furthermore, in this paper we focus on discovery of the structure of the LiNGLaM, more specifically, the locations of the latent variables and their causal order; as future work, we will also show the (partial) identifiability of the causal coefficients in the model and develop an estimation method for them, to produce a fully specified estimated LiNGLaM (further with edges between observed variables). Figure 4: Two structures that are distinguishable by the GIN condition, while (a) has an edge between observed variables X 3 and X 4 .
L 1 X 1 X 2 X 3 X 4 L 1 L 2 X 1 X 2 X 3 X 4 (a) (b)</p>
<p>Conclusion</p>
<p>We proposed a Generalized Independent Noise (GIN) condition for estimating a particular type of linear non-Gaussian latent variable model, where the typical IN condition is a special case. We showed the graphical implications of the GIN condition, based on which we proposed a recursive learning algorithm to locate latent variables and identify their causal structure. Experimental results on simulation data and real data further verified the usefulness of our algorithm.</p>
<p>Broader Impact</p>
<p>Causal modeling is a fundamental problem in multiple disciplines of science and data analysis, and causal discovery from observational data has attracted much attention. Existing methods for causal discovery usually assume that there is no confounder (a confounder is a latent direct common cause of two measured variables) or that the confounders for different variables are unrelated. However, it is often the case that observed variables are just reflections of the underlying hidden causal variables, which may be causally related to each other. This is particular true in psychology, neuoscience, and social sciences. Unfortunately, existing methods for finding such latent variables all involve very strong assumptions (e.g., factor analysis assumes that the latent factors are rather low-dimensional and mutually independent), and there is no principle approach to estimating the causal relations between them, especially the causal order. The methodologies and the framework developed in the work have the power to infer the right causal structure, including that over the latent variables, and enable us to correctly understand the systems, which then helps make proper policies, avoid bias or discrimination, and achieve a more transparent and fair world.</p>
<p>Supplementary Material</p>
<p>The supplementary material contains • Proof of Proposition 1;</p>
<p>• Proof of Theorem 1;</p>
<p>• Proof of Proposition 2;</p>
<p>• Proof of (and remark on) Theorem 2;</p>
<p>• Proof of Theorem 3;</p>
<p>• Proof of Theorem 4;</p>
<p>• Proof of Proposition 3;</p>
<p>• Proof of Lemma 1;</p>
<p>• Proof of Proposition 4;</p>
<p>• More experimental results of Synthetic data;</p>
<p>• More details of Real-Word data.</p>
<p>A Proofs and Illustrations</p>
<p>We first give an important theorem, which will be used in the proof. [Kagan et al., 1973] Define two random variables X 1 and X 2 as linear combinations of independent random variables e i (i = 1, ..., p):</p>
<p>Darmois-Skitovitch Theorem
X 1 = p i=1 α i e i , X 2 = q i=1 β i e i .(6)
Then, if X 1 and X 2 are independent, all variables e j for which α j β j ≠ 0 are Gaussian. In other words, if there exists a non-Gaussian e j for which α j β j ≠ 0, X 1 and X 2 are dependent.</p>
<p>A.1 Proof of Proposition 1 Proposition 1. Suppose all considered variables follow the linear non-Gaussian acyclic causal model. Let Z be a subset of those variables and Y be a single variable among them. Then the following statements are equivalent.</p>
<p>(A) 1) All variables in Z are causally earlier than Y , and 2) There is no common cause for each variable in Z and Y that is not in Z.
(B) (Z, Y ) satisfies the IN condition.
The proof is straightforward, based on the assumption of linear, non-Gaussian acyclic causal models.</p>
<p>A.2 Proof of Theorem 1 Theorem 1. Suppose that random vectors L, Y, and Z are related in the following way:
Y = AL + E Y ,(7)Z = BL + E Z .(8)
Denote by l the dimensionality of L. Assume A is of full column rank. Proof. Without loss of generality, assume that each component of L has a zero mean, and that both E Y and E Z are zero-mean. If we can find a non-zero vector ω such that ω
Then, if 1) Dim(Y) &gt; l, 2) E Y ⫫ L, 3) E Y ⫫ E Z , 9and 4⊺ A = 0, then ω ⊺ Y = ω ⊺ AL + ω ⊺ E Y = ω ⊺ E Y ,
which will be independent from Z in light of conditions 2) and 3), i.e., the GIN condition for Y given Z holds true.</p>
<p>We now construct the vector ω. If conditions 2) and 3) hold, we have E[YZ ⊺ ] = AΣ LZ , which is determined by (Y, Z). We now show that under conditions 4), for any non-zero vector ω, ω ⊺ A = 0 if and only if ω ⊺ AΣ LZ = 0 or equivalently ω ⊺ E[YZ ⊺ ] = 0 and that such a vector ω exists.</p>
<p>Suppose ω ⊺ A = 0, it is trivial to see ω ⊺ AΣ LZ = 0. Notice that condition 4) implies that rank(AΣ LZ ) ≤ l because rank(AΣ LZ ) ≤ min(rank(A), rank(Σ LZ )) and rank(A) = l. Further according to Sylvester Rank Inequality, we have rank(AΣ LZ ) ≥ rank(A) + rank(Σ LZ ) − l = l. Therefore, rank(AΣ LZ ) = l. Because of condition 1), there must exists a non-zero vector ω, determined by (Y, Z), such that ω
⊺ E[YZ ⊺ ] = ω ⊺ AΣ LZ = 0. Moreover, this equality implies ω ⊺ A = 0
because Σ LZ has l rows and has rank l. With this ω, we have E Y||Z = ω ⊺ E Y and is independent from Z. Thus the theorem holds. Proof. For Statement 1, we first show that (Z,Ÿ ) follows the GIN condition implies that (Z, Y ) follows the GIN condition. If (Z,Ÿ ) follows the GIN condition, then there must exist a non-zero vectorω so thatω
⊺ E[Ÿ Z ⊺ ] = 0. This equality implies ω ⊺ E[ Y Z Z ⊺ ] =ω ⊺ E[Y Z ⊺ ] E[ZZ ⊺ ] = 0. (9) Because E[ZZ ⊺
] is non-singular, we further havë
ω ⊺ E[Y Z ⊺ ]E −1 [ZZ ⊺ ] I = 0.
Let ω be the first Dim(Y ) dimensions ofω. Then we have
ω ⊺ E[Y Z ⊺ ]E −1 [ZZ ⊺ ] = 0, and thus ω ⊺ E[Y Z ⊺ ] = 0.
Furthermore, based on the definition of the GIN condition, we have that EŸ ||Z = ω ⊺Ÿ is independent from Z. It is easy to see that E Y ||Z = ω ⊺ Y is independent from Z. Thus, (Z, Y ) follows the GIN condition.</p>
<p>Next, we show that (Z, Y ) follows the GIN condition implies that (Z,Ÿ ) follows the GIN condition. If (Z, Y ) follows the GIN condition, we have
ω ⊺ E[Y Z ⊺ ] = 0(10)Letω = [ω ⊺ , 0 ⊺ ] ⊺ . We havë ω ⊺ E[Ÿ Z ⊺ ] = [ω ⊺ , 0 ⊺ ]E[ Y Z Z ⊺ ] = ω ⊺ E[Y Z ⊺ ] = 0.(11)
Furthermore, we haveω
⊺Ÿ = [ω ⊺ , 0 ⊺ ] Y Z = ω ⊺ Y . Based on the definition of GIN, E Y ||Z = ω ⊺ Y
is independent from Z. That is to say,ω ⊺ EŸ is independent from Z. Thus, (Z,Ÿ ) follows the GIN condition.</p>
<p>For Statement 2, If (Z, Y ) follows the IN condition, we havẽ 
ω = E[Y Z ⊺ ]E −1 [ZZ ⊺ ].(12)Letω = [1 ⊺ , −ω ⊺ ] ⊺ , we geẗ ω ⊺ E[Ÿ Z ⊺ ] = [1 ⊺ , −ω ⊺ ]E[ Y Z Z ⊺ ] = [1 ⊺ , −ω ⊺ ] E[Y Z ⊺ ] E[ZZ ⊺ ] = E[Y Z ⊺ ] −ωE[ZZ ⊺ ].(= [1 ⊺ , −ω ⊺ ] Y Z = Y −ω ⊺ Z
is independent from Z. Therefore, (Z,Ÿ ) follows the GIN condition. Proof. The "if" part: First suppose that there exists such a subset of the latent variables, S k L , that satisfies the three conditions. Because of condition 1), i.e., that S k L is an exogenous set relative to L(Y) and because according to the LiNGLaM, each Y i is a linear function of L(Y i ) plus independent noise, we know that S k L is also an exogenous set relative to Y. Hence, we know that each component of Y can be written as a linear function of S k L and some independent error (which is independent from S k L ). By a slight abuse of notation, here we use S k L also to denote the vector of the variables in S k L . Then we have
Y = AS k L + E ′ Y ,(14)
where A is an appropriate linear transformation, E ′ Y is independent from S k L , but its components are not necessarily independent from each other. In fact, according to the LiNGLaM, each observed or hidden variable is a linear combination of the underlying noise terms ε i . In equation (14), S k L and E ′ Y are linear combinations of disjoint sets of the noise terms ε i , implied by the directed acyclic structure over all observed and hidden variables.</p>
<p>Let us then write Z as linear combinations of the noise terms. We then show that because of condition 2), i.e., that S k L d-separates Y from Z, if any noise term ε i is present in E ′ Y , it will not be among the noise terms in the expression of Z. Otherwise, if Z j also involves ε i , then the direct effect of ε i , among all observed or hidden variables, is a common cause of Z j and some component of Y. This path between Z j and that component of Y, however, cannot be d-separated by S k L because no component of S k L is on the path, as implied by the fact that when S k L is written as a linear combination of the underlying noise terms, ε i is not among them. Consequently, any noise term in E ′ Y will not contribute to S k L or Z. Hence, we can express Z as
Z = BS k L + E ′ Z ,(15)
where E ′ Z , which is determined by S k L and Z, is independent from E ′ Y . Further considering condition on the dimensionality of S k L and condition 3), one can see that the assumptions in Theorem 1 are satisfied. Therefore, (Z, Y) satisfies the GIN condition.</p>
<p>The "only-if" part: Then we suppose (Z, Y) satisfies GIN (while with the same Z, no proper subset of Y does). Consider all sets S k L that are exogenous relative to L(Y) with k satisfying the condition in the theorem, and we show that at least one of them satisfies conditions 2) and 3). Otherwise, if 2) is always violated, then there is an open path between some leaf node in L(Y), denoted by L(Y k ), and some component of Z, denoted by Z j , and this open path does not go through any common cause of the variables in L(Y). Then they have some common cause that does not cause any other variable in L(Y). Consequently, there exists at least one noise term, denoted by ε i , that contributes to both L(Y k ) (and hence Y k ) and Z j , but not any other variables in Y. Because of the non-Gaussianity of the noise terms and Darmois-Skitovitch Theorem, if any linear projection of Y, ω ⊺ Y is independent from Z, the linear coefficient for Y k must be zero. Hence (Z, Y \ {Y k }) satisfies GIN, which contradicts the assumption in the theorem. Therefore, there must exists some S k L such that 2) holds. Next, if 3) is violated, i.e., the rank of the covariance matrix of S k L and Z is smaller than k. Then the condition ω ⊺ E[YZ ⊺ ] = 0 does not guarantee that ω ⊺ A = 0. Under the faithfulness assumptions, we then do not have that ω ⊺ Y is independent from Z. Hence, condition 3) also holds.</p>
<p>Remark. Roughly speaking, the conditions in this theorem can be interpreted the following way: i.) a causally earlier subset (according to the causal order) of the common causes of Y d-separate Y from Z, and ii.) the linear transformation from that subset of the common causes to Z has full column rank. For instance, for the structure in Figure 1 of the main paper, ({X 3 , X 4 }, {X 1 , X 2 , X 5 }) satisfies GIN, while ({X 3 , X 6 }, {X 1 , X 2 , X 5 }) does not-note that the difference is that in the latter case one of the variables in Z, X 6 , is not d-separated from a component of X, which is X 5 , given the common causes of X. However, when X 6 is replaced by X 4 in Z, whose direct cause is causally earlier, the d-separation relationship holds, and so is the GIN condition.</p>
<p>A.5 Proof of Theorem 3</p>
<p>Theorem 3. Let X be the set of all observed variables in a LiNGLaM and Y be a proper subset of X. If (X \ Y, Y) follows the GIN condition and there is no subsetỸ ⊆ Y such that (X \Ỹ,Ỹ) follows the GIN condition, then Y is a causal cluster and Dim(L(Y)) = Dim(Y) − 1.</p>
<p>Proof. We will prove it by contradiction.
Let Y = (Y ⊺ 1 , ..., Y ⊺ Dim(Y) )
⊺ . There are two cases to consider.</p>
<p>Case 1). Assume that Y is not a causal cluster and show that (X \ Y, Y) violates the GIN condition, leading to the contradiction. Since Y is not a causal cluster, without loss of generality, L(Y) must contain at least two different parental latent variable sets, denoted by L a and L b . Now, we show that there is no non-zero vector ω such that ω ⊺ Y is independent from X \Ỹ. Because there is no subset Y ⊆ Y such that (X \Ỹ, Y) follows the GIN condition, the number of elements containing the components of L a in Y is smaller than Dim(L a ) + 1 and the number of elements containing the components of L b in Y is less than Dim(L b ) + 1. Thus, we obtain that there is no ω ≠ 0 such that
ω ⊺ E[Y((X \ Y) ⊺ ] = 0.
That is to say, ω ⊺ Y is dependent on X \ Y, i.e., (X \ Y, Y) violates the GIN condition, which leads to the contradiction.</p>
<p>Case 2). Assume that Y is a causal cluster but Dim(L(Y)) ≠ Dim(Y) − 1. First, we consider the case where Dim(L(Y)) &lt; Dim(Y) − 1. If Dim(L(Y)) &gt; Dim(Y) − 1, we always can find a subsetỸ ⊆ Y and Dim(Ỹ) = Dim((L(Y)) + 1 such that (X \Ỹ, Y) follows the GIN condition, leading to the contradiction.</p>
<p>We then consider the case where Dim(L(Y)) &gt; Dim(Y) − 1. Due to the linear assumption, each
element in L(Y) contains components {ε L Y 1 , ..., ε L Y Dim(L(Y)) }. Because Dim(L(Y)) &gt; Dim(Y)− 1, ω ⊺ (Y ⊺ 1 , ...., Y ⊺ Dim(Y) ) ⊺ contains ε L Y i , i ∈ {1, .
., Dim(L(Y))}, for any ω ≠ 0. According to the Darmois-Skitovitch Theorem, we have ω
⊺ (Y ⊺ 1 , ...., Y ⊺ Dim(Y) ) ⊺ X \ Y. That is to say, (X \ Y, Y)
violates the GIN condition, which leads to a contradiction.</p>
<p>A.6 Proof of Theorem 4 Theorem 4. Let S p and S q be two causal clusters of a LiNGLaM. Assume there is no latent confounder for L(S p ) and L(S q ), and L(S p ) ∩ L(S q ) = ∅. Further suppose that S p contains 2Dim(L(S p )) number of variables with S p = {P 1 , P 2 , ..., P 2Dim(L(S p )) } and that C q contains 2Dim(L(S q )) number of variables with S q = {Q 1 , Q 2 , ..., Q 2Dim(L(S q )) }. Then if ({P Dim(L(S p ))+1 , ...P 2Dim(L(S p )) }, {P 1 , ...., P Dim(L(S p )) , Q 1 , ...Q Dim(L(S q )) }) follows the GIN condition, L(S p ) → L(S q ) holds.</p>
<p>Proof. For L(S p ) and L(S q ), there are two possible causal relations: L(S p ) → L(S q ) and L(S p ) ← L(S q ). For clarity, let m = Dim(L(S p )) and n = Dim(L(S p )). Further, Let L(S p ) = {L p 1 , ..., L p m } and L(S q ) = {L q 1 , ..., L q n } (note that subscripts denote the causal order). First, we consider case 1: L(S p ) → L(S q ), by leveraging the result of Theorem 1.</p>
<p>According to the linearity assumption, we have
⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ P 1 ⋮ P m Q 1 ⋮ Q n ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ Y = ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ C 11 ⋯ C m1 ⋮ ⋱ ⋮ C m1 ⋯ C mm D 11 ⋯ D n1 ⋮ ⋱ ⋮ D n1 ⋯ D nn ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ A ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ L p 1 ⋮ L p m ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ L + ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ ε P 1 ⋮ ε P m ε ′ Q 1 ⋮ ε ′ Q n ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ E Y(16)
and
⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ P m+1 ⋮ P 2m ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ Z = ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ B 11 ⋯ B m1 ⋮ ⋱ ⋮ B m1 ⋯ B mm ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ B ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ L p 1 ⋮ L p m ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ L + ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ ε P m+1 ⋮ ε P 2m ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ E Z ,(17)
where ε
′ Q i = n ∑ k=1 f k ε L q k + ε Q i .
Now, we verify conditions 1) ∼ 4) in Theorem 1. Based on Equations 16 and 17, we have
Dim(L) = m. For condition 1), Dim(Y) = m + n &gt; m. For condition 2), E Y = (ε P 1 , ..., ε P m , ε ′ Q 1 , ..., ε ′ Q n ) ⊺ is independent from L = {L p 1 , .
.., L p m }, due to the fact that there is no common component between E Y and L and that each component is independent of each other. For condition 3), because ε P k , k = 1, ..., 2m, is independent from L, E Z ⫫ L. For condition 4),
Σ LZ = E[LZ ⊺ ] = Σ L B ⊺ .
Because Dim(B) = m, we obtain that Σ LZ has rank m. Therefore, ({P m+1)+1 , ...P 2m }, {P 1 , ...., P m , Q 1 , ...Q n }) follows the GIN condition.</p>
<p>Next, we consider case 2: L(S p ) ← L(S q ). According to the definition of the GIN condition, we need to find a vector ω ≠ 0 such that ω ⊺ E[(P 1 , ...., P m , Q 1 , ...Q n )(P m+1 , ...., P 2m ) ⊺ ] = 0. Due to the linearity assumption, each element in {P 1 , ...., P 2m } contains the component in ε L P 1 , ..., ε L P m while {Q 1 , ...Q n } not. Because the dimension of ε L P i in {P 1 , ...., P m , Q 1 , ...Q n } is m and Dim(L(S p )) = m, ω ⊺ (P 1 , ...., P m , Q 1 , ...Q n ) contains ε L P i , for any ω ≠ 0. According to the Darmois-Skitovitch Theorem, we have ω ⊺ (P 1 , ...., P m , Q 1 , ...Q n ) (P m+1 , ...., P 2m ) ⊺ . That is to say, ({P m+1)+1 , ...P 2m }, {P 1 , ...., P m , Q 1 , ...Q n }) violates the GIN condition. Therefore, L(S p ) → L(S q ).</p>
<p>A.7 Proof of Proposition 3</p>
<p>Proposition 3. Let S 1 and S 2 be two clusters of a LiNGLaM and Dim(L(S 1 )) = Dim(L(S 2 )). If S 1 and S 2 are overlapping, S 1 and S 2 share the same set of latent variables.</p>
<p>Proof. Because S 1 and S 2 are overlapping, with loss of generality, assume that the shared element of S 1 and S 2 is X k . Furthermore, we have that L(S 1 ) and L(S 2 ) are both parents of X k . Based on the definition of causal cluster and that Dim(L(S 1 )) = Dim(L(S 2 )), we have L(S 1 ) = L(S 2 ). That is to say, S 1 and S 2 share the same set of latent variables.</p>
<p>A.8 Proof of Lemma 1 Lemma 1. Let S r be a cluster and S k , k ≠ r be any other cluster of a LiNGLaM. Suppose that S r contains 2Dim(L(S r )) number of variables with S r = {R 1 , R 2 , ..., R 2Dim(L(S r )) } and that S k contains 2Dim(L(S k )) number of variables with S k = {K 1 , K 2 , ..., K 2Dim(L(S k )) }. if ({R Dim(L(S r ))+1 , ...R 2Dim(L(S r )) , {R 1 , ...., R Dim(L(S r )) , K 1 , ...K Dim(L(S k )) }) follows the GIN condition, then L(S r ) is a root latent variable set.</p>
<p>Proof. (i) Assume that L(S r ) is a root latent variable set. Due to the linearity assumption, there is no latent confounder between L(S r ) and another latent variable set. Based on Theorem 4, we have that ({R |L(S r )| , ...R 2Dim(L(S r )) , {R 1 , ...., R Dim(L(S r )) , K 1 , ...K Dim(L(S k )) }) follows the GIN condition.</p>
<p>(ii) Assume that L(S r ) is not a root latent variable set, that is , L(S r ) has at least one parent set. Let L(S p ) be the parent of L(S r ) and S p = {P 1 , P 2 , ..., P 2Dim(L(S p )) }. Thus, every element in {P 1 , P 2 , ..., P 2Dim(L(S p )) } has the component ε L(S p ) . Based on the definition of the GIN condition, we easily obtain that there is no ω ≠ 0 such that ω ⊺ E[{P 1 , ...., P Dim(L(S p )) , R 1 , ...R Dim(L(S r )) }), ({P Dim(L(S p ))+1 , ...R 2Dim(L(S p )) ) ⊺ ] = 0 because the dimension of ε L(S p ) in {R 1 , ...., R Dim(L(S r )) , K 1 , ...K Dim(L(S k )) } equals Dim(L(S r )). That is to say, ω ⊺ (R 1 , ...., R Dim(L(S r )) , K 1 , ...K Dim(L(S k )) ) must have the component ε L(S p ) . Thus, ω ⊺ (R 1 , ...., R Dim(L(S r )) , K 1 , ...K Dim(L(S k )) ) is dependent on (P Dim(L(S p ))+1 , ...R 2Dim(L(S p )) ) ⊺ based on the Darmois-Skitovitch Theorem. Therefore, ({P Dim(L(S p ))+1 , ...P 2Dim(L(S p )) }, {P 1 , ...., P Dim(L(S p )) , R 1 , ...R Dim(L(S r )) }) violates the GIN condition.</p>
<p>From (ii), the lemma is proven.</p>
<p>Moreover, from (i) and (ii), we show that ({R |L(S r )| , ...R 2Dim(L(S r )) }, {R 1 , ...., R Dim(L(S r )) , K 1 , ...K Dim(L(S k )) }) follows the GIN condition, if and only if L(S r ) is a root latent variable set. If for any one of the remaining S k ∈ R, k ≠ r and S k = {K 1 , K 2 , ..., K 2Dim(L(S k )) } such that ({R Dim(L(S r ))+1 , ..., R 2Dim(L(S r )) ,Ẑ}, {R 1 , ...., R Dim(L(S r )) , K 1 , ...K Dim(L(S k )) ,Ŷ}) follows GIN condition, then L(S r ) is a root latent variable set in R.</p>
<p>Proof. One may treat the causally earlier sets as a new group. Then one can easily prove this result according to Lemma 1.</p>
<p>B More experimental results of Synthetic data</p>
<p>Here, we add more results to show the performance of our algorithm for random generated graphs and more variables. In details, we generated graphs randomly with different numbers of latent variables, where each latent variable only have three observed variables. We run our method and obtain the following results in Table 2. Emotional Exhaustion EE 1 , EE 2 ,EE 3 Denationalization DP 1 , DP 2 , DP 3 Personal Accomplishment P A 1 , P A 2 , P A 3 </p>
<p>) The cross-covariance matrix of L and Z, Σ LZ = E[LZ ⊺ ] has rank l, then E Y||Z ⫫ Z, i.e., (Z, Y) satisfies the GIN condition.</p>
<p>Theorem 2 .
2Let Y and Z be two disjoint subsets of the observed variables of a LiNGLaM. Assume faithfulness holds for the LiNGLaM. (Z, Y) satisfies the GIN condition if and only if there exists a k-size subset of the latent variables</p>
<p>and that 3) the covariance matrix of S k L and Z has rank k, and so does that of S k L and Y.</p>
<p>Algorithm 1
1Identifying Causal Clusters Input: Data set X = {X 1 , ..., X m } Output: Causal cluster set S 1: Initialize S = ∅, Len = 1, and P = X; variable subset P from P such that Dim(P) = Len;5:if E P||(P\P) ⫫ (P \ P) holds then 6: S = S ∪ P;</p>
<p>Proposition 4 .
4Suppose that {S 1 , ...S i , ..., S n } contains all clusters of the LiNGLaM. Denote T = {L(S 1 ), ...L(S i )} and R = {L(S i+1 ), ...L(S n )}, where all elements in T are causally earlier than those in R. LetẐ contain the elements from the half set of the children of each latent variable set in T, andŶ contain the elements from the other half set of the children of each latent variable set in R.Furthermore, Let L(S r ) be a latent variable set of R and S r = {R 1 , R 2 , ..., R 2Dim(L(S r )) }. If for any one of the remaining elements L(S k ) ∈ R, with k ≠ r and S k = {K 1 , K 2 , ..., K 2Dim(L(S k )) } such that ({R Dim(L(S r ))+1 , ...R 2Dim(L(S r )) ,Ẑ}, {R 1 , ...., R Dim(L(S r )) , K 1 , ...K Dim(L(S k )) ,Ŷ}) follows the GIN condition, then L(S r ) is a root latent variable set in R.Algorithm 2 Learning the Causal Order of Latent VariablesInput: Set of causal clusters S Output: Causal order K 1: Initialize L with the root variable sets of each cluster, T = ∅, and K = ∅; 2: while L ≠ ∅ do 3:Find the root node L(S r )</p>
<p>{L 1 , L 2 } → L 4 , and L 3 → L 4 . In all four cases, the data are generated by LiNGLaM and the causal strength b is sampled from a uniform distribution between [−2, −0.5] ∪ [0.5, 2], noise terms are generated from uniform[-1,1] variables to the fifth power, and the sample size N = 500, 1000, 2000. The details of the graph structures are as follows. [Case 1]: Both L 1 and L 2 have two pure observed variables, i.e., L 1 → {X 1 , X 2 } and L 2 → {X 3 , X 4 }. [Case 2]: Add extra edges to the graph in Case 1, such that there exist multiple latent variables. In particular, we add two new variables {X 5 , X 6 }, such that {L1, L2} → {X 5 , X 6 }, and add the edge L 1 → {X 3 , X 4 }. [Case 3]: Each latent variable has three pure observed variables, i.e., L 1 → {X 1 , X 2 , X 3 }, L 2 → {X 4 , X 5 , X 6 }, and L 3 → {X 7 , X 8 , X 9 }. [Case 4]: Add extra latent variables and adjust the observed variables to Case 3 such that it becomes the structure in Figure 1.</p>
<p>The number in parentheses indicates the number of occurrences that the current algorithm cannot correctly solve the problem.</p>
<p>Figure 2 :
2(a-d) Accuracy of the estimated causal order with GIN (purple), and LSTC (green) for Cases 1-4.</p>
<p>) The cross-covariance matrix of L and Z, Σ LZ = E[LZ ⊺ ] has rank l, then E Y||Z ⫫ Z, i.e., (Z, Y) satisfies the GIN condition.9 Note that we do not assume E Z ⫫ L.</p>
<p>.
LetŸ ≔ (Y, Z). Then the following statements hold: 1. (Z,Ÿ ) follows the GIN condition if and only if (Z, Y ) follows it. 2. If (Z, Y ) follows the IN condition, then (Z,Ÿ ) follows the GIN condition.</p>
<p>.
Let Y and Z be two disjoint sets of observed variables of a LiNGLaM. Assume faithfulness holds. for the LiNGLaM. (Z, Y) satisfies the GIN condition (while with the same Z, no proper subset of Y does) if and only if there exists a k-size subset of the latent variables L, 0 ≤ k ≤ min(Dim(Y) − 1, Dim(Z)), denoted by S k L , such that 1) S k L is an exogenous set relative to L(Y), that 2) S k L d-separates Y from Z, and that 3) the covariance matrix of S k L and Z has rank k, and so does that of S k L and Y.</p>
<p>.
Suppose that {S 1 , ...S i , ..., S n } are all the clusters of the LiNGLaM. Denote T = {L(S 1 ), ...L(S i )} and T = {L(S i+1 ), ...L(S n )}, where all elements in T are causally earlier than those in R. LetẐ contain the elements from the half children of each latent variable set in T, andŶ contain the elements from the other half children of each latent variable set in T. Furthermore, Let L(S r ) be a latent variable set of R and S r = {R 1 , R 2 , ..., R 2Dim(L(S r )) }.</p>
<p>Definition 1 (Linear Non-Gaussian Latent Variable Model (LiNGLaM)). A LiNGLaM, besides linear and acyclic assumptions, has the following assumptions: A1. [Measurement Assumption] There is no observed variable in X being an ancestor of any latent variables in L 3 . A2. [Non-Gaussianity Assumption] The noise terms are non-Gaussian. A3. [Double-Pure Child Variable Assumption] Each latent variable set L Purity Assumption] There is no direct edge between observed variables.′ , in which every latent </p>
<p>variable directly causes the same set of observed variables, has at least 2Dim(L </p>
<p>′ </p>
<p>) 
4 pure 
measurement variables as children. 
A4. [</p>
<p>) .
)Definition 2 (IN condition). Let Y be a single variable and Z a set of variables. Suppose all variables follow the linear non-Gaussian acyclic causal model. We say that (Z, Y ) follows the IN condition, if and only if the residual of regressing Y on Z is statistically independent from Z. Mathematically, letω be the vector of regression coefficients, that is,ω ≔ E[Y Z⊺ </p>
<p>]E </p>
<p>−1 </p>
<p>[ZZ </p>
<p>⊺ </p>
<p>]; the </p>
<p>IN condition holds for (Z, Y ) if and only ifẼ </p>
<p>considers the case where Z is a single variable and shows that (Z, Y ) satisfies the IN condition if and only if Z is an exogenous (or root) variable relative to Y , based on which one can identify the causal relation between Y and Z. As a direct extension of this result, we show that in the case where Z contains multiple variables, (Z, Y ) satisfies the IN 3 Here, this assumption follows the definition in[Silva et al., 2006] and it is equivalent to say that there is no observed variable in X being an parent of any latent variables in L 4 It means 2 times the dimension of L condition if and only if all variables in Z are causally earlier than Y and there is no common cause behind any variable in Z and Y . This result is given in the following Proposition.′ 
Proposition 1. Suppose all considered variables follow the linear non-Gaussian acyclic causal 
model. Let Z be a subset of those variables and Y be a single variable. Then the following state-
ments are equivalent. </p>
<p>Table 1 :
1Results with GIN, LSTC, FOFC, and BPC for learning causal clusters.Latent omission 
Latent commission 
Mismeasurements 
Algorithm 
GIN 
LSTC 
FOFC 
BPC 
GIN 
LSTC 
FOFC 
BPC 
GIN 
LSTC 
FOFC 
BPC 
500 </p>
<p>Table 2 :
2Results with different numbers of variables and randomly generated graphs (with sample size=2000). of variables (latent variables) Latent omission Latent commission Mismeasurements Correct-ordering rate C More details of Real-Word data For comparisons, we give the hypothesized factors formulated in[Byrne, 2010] inTable 3.Observed variablesRole Conflict RC 1 , RC 2 , W O 1 , W O 2 , Decision Making DM 1 , DM 2 Classroom Climate CC 1 , CC 2 ,CC 3 ,CC 4 Self-EsteemSE 1 , SE 2 ,SE 3 Peer Support P S 1 , P S 2 External Locus of Control ELC 1 , ELC 2 ,ELC 3 ,ELC 4 ,ELC 5Number 15(5) 
0.02(1) 
0.00(0) 
0.00(0) 
0.90 
30(10) 
0.09(3) 
0.05(3) 
0.04(3) 
0.85 
60(20) 
0.15(6) 
0.12(6) 
0.10(6) 
0.79 </p>
<p>Table 3 :
3The hypothesized factors in[Byrne, 2010].
Note that here we call L is a "root variable" after we have known the variables that causally earlier than L 7 Here, the boxes indicate the elements of the root variable set {L 1 , L 2 }
For BPC and FOFC algorithms, we used these implementations in the TETRAD package, which can be downloaded at http://www.phil.cmu.edu/tetrad/.</p>
<p>Learning linear bayesian networks with latent variables. Animashree Anandkumar, Daniel Hsu, Adel Javanmard, Sham Kakade, International Conference on Machine Learning. Animashree Anandkumar, Daniel Hsu, Adel Javanmard, and Sham Kakade. Learning linear bayesian networks with latent variables. In International Conference on Machine Learning, pages 249-257, 2013.</p>
<p>Structural Equations with Latent Variable. Kenneth A Bollen, John Wiley &amp; SonsKenneth A. Bollen. Structural Equations with Latent Variable. John Wiley &amp; Sons, 1989.</p>
<p>Structural equation modeling with amos: Basic concepts, applications, and programming. M Barbara, Byrne, Barbara M Byrne. Structural equation modeling with amos: Basic concepts, applications, and pro- gramming. 2010.</p>
<p>Triad constraints for learning causal structure of latent variables. Ruichu Cai, Feng Xie, Clark Glymour, Zhifeng Hao, Kun Zhang, Advances in Neural Information Processing Systems. Ruichu Cai, Feng Xie, Clark Glymour, Zhifeng Hao, and Kun Zhang. Triad constraints for learning causal structure of latent variables. In Advances in Neural Information Processing Systems, pages 12863-12872, 2019.</p>
<p>Optimal structure identification with greedy search. David Maxwell, Chickering , Journal of machine learning research. 3David Maxwell Chickering. Optimal structure identification with greedy search. Journal of machine learning research, 3(Nov):507-554, 2002.</p>
<p>Learning highdimensional directed acyclic graphs with latent and selection variables. Diego Colombo, H Marloes, Markus Maathuis, Thomas S Kalisch, Richardson, The Annals of Statistics. Diego Colombo, Marloes H Maathuis, Markus Kalisch, and Thomas S Richardson. Learning high- dimensional directed acyclic graphs with latent and selection variables. The Annals of Statistics, pages 294-321, 2012.</p>
<p>Random variables and probability distributions. H Cramér, Cambridge University PressCambridge2nd editionH. Cramér. Random variables and probability distributions. Cambridge University Press, Cam- bridge, 2nd edition, 1962.</p>
<p>Discovering unconfounded causal relationships using linear nongaussian models. Doris Entner, Patrik O Hoyer, JSAI International Symposium on Artificial Intelligence. SpringerDoris Entner and Patrik O Hoyer. Discovering unconfounded causal relationships using linear non- gaussian models. In JSAI International Symposium on Artificial Intelligence, pages 181-195. Springer, 2010.</p>
<p>Statistical methods for research workers. Statistical methods for research workers. Aylmer Ronald, Fisher, 1950llth ed. revisedRonald Aylmer Fisher. Statistical methods for research workers. Statistical methods for research workers., (llth ed. revised), 1950.</p>
<p>A kernel statistical test of independence. Arthur Gretton, Kenji Fukumizu, H Choon, Le Teo, Bernhard Song, Alex J Schölkopf, Smola, Advances in neural information processing systems. Arthur Gretton, Kenji Fukumizu, Choon H Teo, Le Song, Bernhard Schölkopf, and Alex J Smola. A kernel statistical test of independence. In Advances in neural information processing systems, pages 585-592, 2008.</p>
<p>Estimation of causal effects using linear non-gaussian causal models with hidden variables. O Patrik, Shohei Hoyer, Shimizu, J Antti, Markus Kerminen, Palviainen, International Journal of Approximate Reasoning. 492Patrik O Hoyer, Shohei Shimizu, Antti J Kerminen, and Markus Palviainen. Estimation of causal effects using linear non-gaussian causal models with hidden variables. International Journal of Approximate Reasoning, 49(2):362-378, 2008.</p>
<p>Nonlinear causal discovery with additive noise models. O Patrik, Dominik Hoyer, Janzing, M Joris, Jonas Mooij, Bernhard Peters, Schölkopf, Advances in neural information processing systems. Patrik O Hoyer, Dominik Janzing, Joris M Mooij, Jonas Peters, and Bernhard Schölkopf. Nonlin- ear causal discovery with additive noise models. In Advances in neural information processing systems, pages 689-696, 2009.</p>
<p>Causal discovery from heterogeneous/nonstationary data. * Huang, K Zhang, * , J Zhang, R Sanchez-Romero, C Glymour, B Schölkopf, JMLR. 212020Huang<em>, K. Zhang</em>, J. Zhang, R. Sanchez-Romero, C. Glymour, and B. Schölkopf. Causal discovery from heterogeneous/nonstationary data. In JMLR, volume 21(89), 2020.</p>
<p>Characterization problems in mathematical statistics. Calyampudi Radhakrishna Abram M Kagan, Yurij Rao, Vladimirovich Linnik, Abram M Kagan, Calyampudi Radhakrishna Rao, and Yurij Vladimirovich Linnik. Characterization problems in mathematical statistics. 1973.</p>
<p>Causal clustering for 1-factor measurement models. Erich Kummerfeld, Joseph Ramsey, Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data MiningACMErich Kummerfeld and Joseph Ramsey. Causal clustering for 1-factor measurement models. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1655-1664. ACM, 2016.</p>
<p>The seven tools of causal inference, with reflections on machine learning. Judea Pearl, Commun. ACM. 623Judea Pearl. The seven tools of causal inference, with reflections on machine learning. Commun. ACM, 62(3):54-60, February 2019.</p>
<p>Minimal nonlinear distortion principle for nonlinear independent component analysis. J Peters, J M Mooij, D Janzing, B Schölkopf, Journal of Machine Learning Research. 15J. Peters, J. M. Mooij, Janzing D., and B. Schölkopf. Minimal nonlinear distortion principle for nonlinear independent component analysis. Journal of Machine Learning Research, 15:2009- 2053, 2014.</p>
<p>A linear non-Gaussian acyclic model for causal discovery. Shohei Shimizu, Patrik O Hoyer, Aapo Hyvärinen, Antti Kerminen, Journal of Machine Learning Research. 7Shohei Shimizu, Patrik O Hoyer, Aapo Hyvärinen, and Antti Kerminen. A linear non-Gaussian acyclic model for causal discovery. Journal of Machine Learning Research, 7(Oct):2003-2030, 2006.</p>
<p>Estimation of linear non-gaussian acyclic models for latent factors. Shohei Shimizu, Patrik O Hoyer, Aapo Hyvärinen, Neurocomputing. 727-9Shohei Shimizu, Patrik O Hoyer, and Aapo Hyvärinen. Estimation of linear non-gaussian acyclic models for latent factors. Neurocomputing, 72(7-9):2024-2027, 2009.</p>
<p>DirectLiNGAM: A direct method for learning a linear non-Gaussian structural equation model. Shohei Shimizu, Takanori Inazumi, Yasuhiro Sogawa, Aapo Hyvärinen, Yoshinobu Kawahara, Takashi Washio, Patrik O Hoyer, Kenneth Bollen, Journal of Machine Learning Research. 12Shohei Shimizu, Takanori Inazumi, Yasuhiro Sogawa, Aapo Hyvärinen, Yoshinobu Kawahara, Takashi Washio, Patrik O Hoyer, and Kenneth Bollen. DirectLiNGAM: A direct method for learn- ing a linear non-Gaussian structural equation model. Journal of Machine Learning Research, 12 (Apr):1225-1248, 2011.</p>
<p>Learning the structure of linear latent variable models. Ricardo Silva, Richard Scheine, Clark Glymour, Peter Spirtes, Journal of Machine Learning Research. 7Ricardo Silva, Richard Scheine, Clark Glymour, and Peter Spirtes. Learning the structure of linear latent variable models. Journal of Machine Learning Research, 7(Feb):191-246, 2006.</p>
<p>Pearson's contribution to the theory of two factors. Charles Spearman, British Journal of Psychology. General Section. 191Charles Spearman. Pearson's contribution to the theory of two factors. British Journal of Psychology. General Section, 19(1):95-101, 1928.</p>
<p>Calculation of entailed rank constraints in partially non-linear and cyclic models. Peter Spirtes, Proceedings of the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence. the Twenty-Ninth Conference on Uncertainty in Artificial IntelligenceAUAI PressPeter Spirtes. Calculation of entailed rank constraints in partially non-linear and cyclic models. In Proceedings of the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence, pages 606-615. AUAI Press, 2013.</p>
<p>An algorithm for fast recovery of sparse causal graphs. Peter Spirtes, Clark Glymour, Social science computer review. 91Peter Spirtes and Clark Glymour. An algorithm for fast recovery of sparse causal graphs. Social science computer review, 9(1):62-72, 1991.</p>
<p>Causal discovery and inference: concepts and recent methodological advances. Peter Spirtes, Kun Zhang, Applied informatics. 3Peter Spirtes and Kun Zhang. Causal discovery and inference: concepts and recent methodological advances. In Applied informatics, volume 3, page 3. SpringerOpen, 2016.</p>
<p>Causal inference in the presence of latent variables and selection bias. Peter Spirtes, Christopher Meek, Thomas Richardson, Proceedings of the Eleventh conference on Uncertainty in artificial intelligence. the Eleventh conference on Uncertainty in artificial intelligenceMorgan Kaufmann Publishers IncPeter Spirtes, Christopher Meek, and Thomas Richardson. Causal inference in the presence of latent variables and selection bias. In Proceedings of the Eleventh conference on Uncertainty in artificial intelligence, pages 499-506. Morgan Kaufmann Publishers Inc., 1995.</p>
<p>Automated search for causal relations: Theory and practice. Peter Spirtes, Clark Glymour, Richard Scheines, Robert Tillman, Peter Spirtes, Clark Glymour, Richard Scheines, and Robert Tillman. Automated search for causal relations: Theory and practice. 2010.</p>
<p>Trek separation for gaussian graphical models. Seth Sullivant, Kelli Talaska, Jan Draisma, The Annals of Statistics. 383Seth Sullivant, Kelli Talaska, Jan Draisma, et al. Trek separation for gaussian graphical models. The Annals of Statistics, 38(3):1665-1685, 2010.</p>
<p>ParceLiNGAM: a causal ordering method robust against latent confounders. Tatsuya Tashiro, Shohei Shimizu, Aapo Hyvärinen, Takashi Washio, Neural Computation. 261Tatsuya Tashiro, Shohei Shimizu, Aapo Hyvärinen, and Takashi Washio. ParceLiNGAM: a causal ordering method robust against latent confounders. Neural Computation, 26(1):57-83, 2014.</p>
<p>Causal discovery from nonstationary/heterogeneous data: Skeleton estimation and orientation determination. K Zhang, B Huang, J Zhang, C Glymour, B Schölkopf, International Joint Conference on Artificial Intelligence (IJCAI. K. Zhang, B. Huang, J. Zhang, C. Glymour, and B. Schölkopf. Causal discovery from nonstation- ary/heterogeneous data: Skeleton estimation and orientation determination. In International Joint Conference on Artificial Intelligence (IJCAI), 2017.</p>
<p>On the identifiability of the post-nonlinear causal model. Kun Zhang, Aapo Hyvärinen, Proceedings of the twenty-fifth conference on uncertainty in artificial intelligence. the twenty-fifth conference on uncertainty in artificial intelligenceAUAI PressKun Zhang and Aapo Hyvärinen. On the identifiability of the post-nonlinear causal model. In Proceedings of the twenty-fifth conference on uncertainty in artificial intelligence, pages 647-655. AUAI Press, 2009.</p>            </div>
        </div>

    </div>
</body>
</html>
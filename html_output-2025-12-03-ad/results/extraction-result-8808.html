<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8808 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8808</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8808</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-158.html">extraction-schema-158</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-229923446</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2012.15793v1.pdf" target="_blank">Promoting Graph Awareness in Linearized Graph-to-Text Generation</a></p>
                <p><strong>Paper Abstract:</strong> Generating text from structured inputs, such as meaning representations or RDF triples, has often involved the use of specialized graph-encoding neural networks. However, recent applications of pretrained transformers to linearizations of graph inputs have yielded state-of-the-art generation results on graph-to-text tasks. Here, we explore the ability of these linearized models to encode local graph structures, in particular their invariance to the graph linearization strategy and their ability to reconstruct corrupted inputs. Our findings motivate solutions to enrich the quality of models' implicit graph encodings via scaffolding. Namely, we use graph-denoising objectives implemented in a multi-task text-to-text framework. We find that these denoising scaffolds lead to substantial improvements in downstream generation in low-resource settings.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8808.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8808.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CANONICAL (PENMAN)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Canonical PENMAN linearization (AMR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A standard, human-created linearization format for AMR graphs using PENMAN notation that represents the graph as a parenthesized spanning-tree string with ordered edge labels and node variables; used as the default input representation for transformer finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Text Generation and Systemic-functional Linguistics: Experiences from English and Japanese</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Canonical PENMAN linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Encodes an AMR graph as a parenthesized tree-like string (PENMAN) that includes node variables, parentheses, and labeled edges in a deterministic canonical ordering (annotator-defined). The representation is concise and preserves order information that often correlates with surface sentence ordering.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Abstract Meaning Representation (AMR) graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Produce a PENMAN traversal (spanning-tree) of the AMR graph with ordered edge-labels and parentheses; replace variable names with references and remove word-sense suffixes as preprocessing.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation (graph-to-text)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Primary evaluation using BLEU (SacreBLEU). In reported experiments CANONICAL evaluation yields the best BLEU; authors report models reach ~40 BLEU (CANONICAL) within 2 epochs in a training-speed comparison (CANONICAL/RECONFIGURE/RANDOMIZE reached 40 BLEU at 2/3/5 epochs respectively). Also evaluated with BertScore and the M component of MF-score for semantic fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Preferred over simpler DFS traversals in prior work (Mager et al., 2020); pretrained transformers finetuned on canonical PENMAN typically outperform graph-encoding (GNN/graph-transformer) baselines on generation metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Concise (fits transformer context), empirically yields superior generation vs some alternatives, widely used and standardized (leads to reproducible baselines).</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Contains annotation-order signals that can leak target-sentence ordering (annotation artifacts), enabling models to rely on spurious correlations rather than graph structure; not invariant to alternative linearizations.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Models trained only on canonical PENMAN deteriorate substantially when evaluated on meaning-preserving alternative linearizations (RECONFIGURE, RANDOMIZED), indicating over-reliance on canonical order.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Promoting Graph Awareness in Linearized Graph-to-Text Generation', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8808.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8808.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RECONFIGURE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reconfigured PENMAN linearization (RECONFIGURE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A PENMAN-based linearization that ignores canonical ordering (except for the top node) and may reverse edge labels (e.g., ARG0 -> ARG0-of), producing a meaning-preserving but different linearized string.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Penman: An opensource library and tool for AMR graphs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Reconfigured (RECONFIGURE) linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Creates a PENMAN tree from the AMR graph while purposely discarding canonical ordering constraints (preserves top node), potentially changing relative ordering of subtrees and reversing edge-label orientations where appropriate.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Use the Penman library's RECONFIGURE operation to convert the AMR graph to a PENMAN tree that ignores canonical order information (top preserved); postprocess variables and word-senses similarly to canonical.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation; used in adversarial training and as an auxiliary scaffold task (reconstruct canonical from reconfigured).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>When used as an evaluation linearization, models trained only on canonical show large BLEU drops; when included during adversarial training, models become more robust and RECONFIGURE models attain ~40 BLEU by 3 epochs (slower than canonical). Also used in reordering scaffolding whose loss correlates with semantic-fidelity metrics (M-score).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>More disruptive than simply randomizing node order because it can invert edge labels; compared against CANONICAL and RANDOMIZED in robustness experiments—adversarially training with RECONFIGURE improves robustness vs canonical-only training.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>When used during training, it forces models to rely less on canonical order and better encode graph relations; useful for data augmentation and as a reordering scaffold.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Harder mapping to surface order (edge reversals) can make generation more difficult if the model has not been trained on such variants.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Models trained solely on canonical inputs produce lower-quality output on RECONFIGURE inputs; RECONFIGURE-only inputs at evaluation without adversarial training lead to substantial performance degradation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Promoting Graph Awareness in Linearized Graph-to-Text Generation', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8808.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8808.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RANDOMIZE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Randomized PENMAN linearization (RANDOMIZE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A drastic alternative linearization that builds a PENMAN tree from a randomized ordering of the graph's triples, discarding canonical order signals and testing the model's invariance to traversal order.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Randomized PENMAN linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Constructs a PENMAN traversal from a randomized ordering of the underlying triple set (no canonical order preserved), producing a valid but arbitrarily ordered linearization of the same AMR graph.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs (constructed from triples)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Use Penman to produce a tree from a randomly permuted set of triples, then replace variables and drop word-senses as in other setups.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation used for adversarial robustness experiments and reordering scaffolds.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Models trained only on canonical suffer markedly on RANDOMIZE inputs; adversarial training that includes randomized linearizations improves robustness—RANDOMIZE variants reached ~40 BLEU after more epochs (around 5 epochs). Randomized-trained models still often underperform canonical at evaluation time.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>More drastic than RECONFIGURE; compared in robustness tests showing that including RANDOMIZE in training improves resilience compared to canonical-only training.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Effective as a strong perturbation for adversarial training and data augmentation to reduce reliance on canonical ordering; can help the model learn graph structure rather than order heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Produces input strings very different from human canonical orderings, which can slow training convergence and reduce generation quality if not used carefully in training.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Canonical-trained models perform poorly on RANDOMIZE; even models trained with RANDOMIZE still perform best when evaluated on canonical order (canonical eval remains highest).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Promoting Graph Awareness in Linearized Graph-to-Text Generation', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8808.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8808.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RDF-tokenized</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RDF-triple linearization with special tokens (WebNLG style)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A linearization for RDF triples that prefixes relation, subject, verb, and object elements with dedicated tokens and separates triples with a special separator, yielding a flat, tokenized serialization usable by pretrained seq2seq models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>RDF triple linearization with special tokens</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Serialize each RDF triple as a short token sequence with dedicated markers for relation/subject/verb/object (e.g., <rel> <S> subj <V> verb <O> obj), and join multiple triples with separators to form the model input.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Sets of RDF triples (WebNLG dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>For each triple prepend special tokens for relation and element boundaries and join triples with a dedicated separator token; optionally randomize triple order for adversarial evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>RDF-to-text generation (WebNLG)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Evaluated with BLEU; experiments show strong sensitivity to relation-order randomization (RANDOMIZE of relations reduces performance if not trained with permutations). Exact BLEU numbers for WebNLG experiments are reported in tables in the paper (not all explicit in text).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Used as the standard linearization for RDF tasks in this work (following Ribeiro et al., 2020); pretrained transformer finetuned on this serialization outperforms many graph-based encoders for WebNLG.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Simple, compact, and easy to feed into pretrained transformers; facilitates multiple-triple inputs and keeps sequence lengths short.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Order of triples and tokenization choices can leak target ordering or encourage spurious shortcuts; models are sensitive to permutations unless trained to be robust.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Randomizing relation/triple order at evaluation degrades generation quality for models trained only on a fixed ordering.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Promoting Graph Awareness in Linearized Graph-to-Text Generation', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8808.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8808.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ADV-Permutation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adversarial linearization training (per-epoch permutation augmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A training strategy that presents different meaning-preserving linearizations (canonical, reconfigured, randomized) across epochs to the pretrained seq2seq model to reduce sensitivity to a single linearization strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Adversarial linearization augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>At each training epoch, alter the presented linearization of the same graph (e.g., canonical, reconfigured, randomized) so the model cannot rely on a single fixed input ordering and must learn a more robust implicit graph encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs and RDF triple sets</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Generate alternative linearizations with Penman (for AMR) or randomize RDF triple order; cycle or randomly sample among alternatives each epoch while finetuning the pretrained model.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text and RDF-to-text generation; robustness evaluation across linearizations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Leads to improved robustness across linearizations with only minor cost to canonical-generation BLEU. Example: models trained adversarially achieve similar BLEU but with improved performance when evaluated on RECONFIGURE and RANDOMIZE inputs; training-time overhead minimal (training epochs to reach a BLEU threshold vary: CANONICAL/RECONFIGURE/RANDOMIZE reached 40 BLEU at 2/3/5 epochs respectively).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared against canonical-only training; adversarial augmentation yields notably better robustness than no augmentation and is simpler than architectural graph encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Simple to implement, reduces model dependence on annotation order, improves cross-linearization robustness, small training cost.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Does not fully remove sensitivity to canonical ordering (canonical eval often remains best); pretrained transformer pretraining induces residual order sensitivity that adversarial fine-tuning may not entirely overcome.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Even after adversarial training, models still perform best on canonical linearization at evaluation and may retain pretraining-induced order sensitivities.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Promoting Graph Awareness in Linearized Graph-to-Text Generation', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8808.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8808.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MaskedGraph (graph-denoising)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Masked Graph Modeling (graph-denoising objectives / scaffolding)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Auxiliary denoising objectives applied to linearized graph inputs (masking nodes, edge labels/parentheses, or all tokens) trained in a multitask text-to-text framework to encourage models to learn better implicit graph encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Masked Graph Modeling (graph-denoising scaffolds)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Apply masking to tokens in the linearized graph such that ~15% of tokens are masked (MASS-style masking used); variants target nodes only, graph components (edge labels/parentheses), or all tokens. The denoising objective is optimized jointly with the main graph->text objective.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs (but applicable to any linearized graph input)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Mask selected tokens in the linearized string representation (according to variant), and train the model to reconstruct the masked content or to perform the main generation task in a multi-task setup.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation; used as an auxiliary training scaffold to improve generation fidelity especially in low-resource settings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Substantial BLEU improvements in low-resource regimes (e.g., when training on 1000 examples or less); the paper reports that with <3% of full AMR data, masked-graph scaffolds exceed prior GNN SOTA (Ribeiro et al., 2019 BLEU 27.37). On full-data training, improvements are small and within one std. Also evaluated with BertScore and M-component of MF-score (M-score) where masking leads to modest gains in BertScore but little change in M-score.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Graph masking outperforms standard sentence MLM-style masking for improving AMR-to-text in low-resource setups; various graph-masking variants perform similarly to each other (little advantage from more complex graph-aware masking schemes). Achieves comparable absolute gains to other graph-aware methods but with lower complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Simple to implement, consistently beneficial in low-resource settings, encourages models to encode graph structure rather than memorize order heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Diminishing returns with large amounts of labeled data; different masking strategies show only small differences; does not fully remedy other error types (e.g., word-sense confusion).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>With full dataset (tens of thousands of examples) scaffolding yields minor improvements only; models still drop modifiers or confuse word senses (partly due to preprocessing that removed sense suffixes).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Promoting Graph Awareness in Linearized Graph-to-Text Generation', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8808.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8808.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reordering (scaffold)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Reordering auxiliary objective (reconstruct canonical)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An auxiliary training objective that asks the model to map a reconfigured or randomized linearization back to the canonical PENMAN linearization to force learning of the underlying graph structure.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph Reordering (reconstruct canonical from alternate linearizations)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Provide the model with a RECONFIGURED or RANDOMIZED linearization and train it to output the canonical PENMAN serialization as an auxiliary text-to-text task; the mapping requires nontrivial graph encoding and serves as data augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Use Penman to create alternate linearizations and pair each alternate input with the canonical PENMAN output as an auxiliary supervision signal during multitask training.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation; also used as a scaffolding loss whose magnitude predicts semantic-fidelity (M-score) of generated sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>One of the best-performing scaffolds in low-resource experiments; regression analysis shows the reordering loss is a significant predictor of M-score (semantic fidelity) beyond other covariates. Reported Pearson correlation between scaffolding loss and M-score is ρ = -0.35 (negative relationship; lower scaffold loss -> higher M-score). Improvements in BLEU and human-checked fidelity were seen in low-resource settings; on full data the gains are small.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Provides similar absolute gains to other graph-aware augmentation methods but is simpler and cheaper than methods that jointly regenerate graphs at training time or use extra ranking at decode time.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Encourages faithful graph encoding, provides data augmentation via nondeterministic reconfiguration, correlates with semantic fidelity (useful as a proxy metric), and helps correct argument-order errors that baseline sometimes makes.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Limited incremental benefit when large amounts of supervised data are available; simultaneous generation plus reordering (joint output) did not improve results.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>With large training sets the reordering scaffold's impact is muted; does not fix hallucination or some modifier-dropping errors; cannot recover sense distinctions lost in preprocessing.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Promoting Graph Awareness in Linearized Graph-to-Text Generation', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8808.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8808.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>T5-linearized</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pretrained Transformer (T5) finetuned on linearized graphs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of the pretrained T5 encoder-decoder transformer finetuned on pairs of linearized graph inputs and surface-realization outputs; serves as the primary modeling approach to test whether pretrained LMs can implicitly encode graph structure.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Exploring the limits of transfer learning with a unified text-to-text transformer</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Pretrained Transformer finetuned on linearized graphs (T5)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Treat graph inputs as text via a chosen linearization (PENMAN or tokenized triples) and finetune a pretrained text-to-text transformer (T5) to map the serialized graph string to the target natural-language sentence.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs and RDF triple sets (WebNLG)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Convert graphs to linearized text (PENMAN for AMR, tokenized triples for RDF) and feed into T5 encoder; decode target sentence autoregressively from the decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text and RDF-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>T5 finetuned on linearizations achieves state-of-the-art BLEU on AMR and WebNLG in this line of work, outperforming specialized GNN/graph-transformer encoders; specific reported metrics include BLEU improvements over prior graph encoders and favorable BertScore; numeric highlights: pretrained linearized models surpass earlier graph-encoder methods, and under certain conditions models reach ~40 BLEU on AMR evaluation within a few epochs depending on linearization regime.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Outperforms graph-encoding architectures (GNNs, graph transformers) on multiple automated metrics in this and related work, but is sensitive to input linearization whereas graph encoders explicitly model graph topology.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Strong generalization and fluency due to pretraining, simple pipeline (no graph-specific encoder required), competitive or superior metrics across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Implicit graph encoding can be brittle to linearization permutations and dataset annotation artifacts; may overfit to order-based shortcuts if not trained with augmentation or scaffolding.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Models trained only on a fixed linearization (canonical) fail to generalize to meaning-preserving alternative linearizations; also occasional modifier-dropping and word-sense confusions (exacerbated by preprocessing that removes sense suffixes).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Promoting Graph Awareness in Linearized Graph-to-Text Generation', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8808.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8808.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sequence-to-sequence linearization (prior-work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sequence-to-sequence models trained on serialized graphs (non-pretrained)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Earlier approach that trains seq2seq models from scratch on linearized graph strings (e.g., Pourdamghani et al. 2016; Konstas et al. 2017), historically outperformed by graph encoders but recently overshadowed by pretrained LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Seq2Seq on linearized graphs (from-scratch)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Serialize graph structure into a string (various linearizations) and train an encoder-decoder model (e.g., LSTM-based seq2seq) from scratch to map serialization to surface text.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR and other meaning representations</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Depth-first or other traversal serializations; fixed or randomized traversal orderings used in prior studies.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text and general graph-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Initially outperformed by graph-encoding architectures and later by pretrained linearized models; prior LSTM-based models could be made agnostic to ordering in some setups (Konstas et al., 2017) but lacked the generation quality of pretrained transformers.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Outperformed by GNN or graph-transformer encoders historically, and strongly outperformed by pretrained transformers finetuned on linearizations.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Simpler architectures; demonstrated feasibility of generation from serialized graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Lower performance compared to modern pretrained models and explicit graph encoders; sensitive to linearization choices unless special care taken.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Earlier LSTM systems could overfit to training order or fail to generalize across linearizations without specific randomization strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Promoting Graph Awareness in Linearized Graph-to-Text Generation', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Neural AMR: Sequence-to-sequence models for parsing and generation <em>(Rating: 2)</em></li>
                <li>GPT-too: A language-model-first approach for AMR-to-text generation <em>(Rating: 2)</em></li>
                <li>Enhancing AMR-to-text generation with dual graph representations <em>(Rating: 2)</em></li>
                <li>Penman: An opensource library and tool for AMR graphs <em>(Rating: 2)</em></li>
                <li>Controllable meaning representation to text generation: Linearization and data augmentation strategies <em>(Rating: 2)</em></li>
                <li>Investigating pretrained language models for Graph-to-Text generation <em>(Rating: 2)</em></li>
                <li>Generating English from Abstract Meaning Representations <em>(Rating: 1)</em></li>
                <li>Text-to-text pre-training for data-to-text tasks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8808",
    "paper_id": "paper-229923446",
    "extraction_schema_id": "extraction-schema-158",
    "extracted_data": [
        {
            "name_short": "CANONICAL (PENMAN)",
            "name_full": "Canonical PENMAN linearization (AMR)",
            "brief_description": "A standard, human-created linearization format for AMR graphs using PENMAN notation that represents the graph as a parenthesized spanning-tree string with ordered edge labels and node variables; used as the default input representation for transformer finetuning.",
            "citation_title": "Text Generation and Systemic-functional Linguistics: Experiences from English and Japanese",
            "mention_or_use": "use",
            "representation_name": "Canonical PENMAN linearization",
            "representation_description": "Encodes an AMR graph as a parenthesized tree-like string (PENMAN) that includes node variables, parentheses, and labeled edges in a deterministic canonical ordering (annotator-defined). The representation is concise and preserves order information that often correlates with surface sentence ordering.",
            "graph_type": "Abstract Meaning Representation (AMR) graphs",
            "conversion_method": "Produce a PENMAN traversal (spanning-tree) of the AMR graph with ordered edge-labels and parentheses; replace variable names with references and remove word-sense suffixes as preprocessing.",
            "downstream_task": "AMR-to-text generation (graph-to-text)",
            "performance_metrics": "Primary evaluation using BLEU (SacreBLEU). In reported experiments CANONICAL evaluation yields the best BLEU; authors report models reach ~40 BLEU (CANONICAL) within 2 epochs in a training-speed comparison (CANONICAL/RECONFIGURE/RANDOMIZE reached 40 BLEU at 2/3/5 epochs respectively). Also evaluated with BertScore and the M component of MF-score for semantic fidelity.",
            "comparison_to_others": "Preferred over simpler DFS traversals in prior work (Mager et al., 2020); pretrained transformers finetuned on canonical PENMAN typically outperform graph-encoding (GNN/graph-transformer) baselines on generation metrics.",
            "advantages": "Concise (fits transformer context), empirically yields superior generation vs some alternatives, widely used and standardized (leads to reproducible baselines).",
            "disadvantages": "Contains annotation-order signals that can leak target-sentence ordering (annotation artifacts), enabling models to rely on spurious correlations rather than graph structure; not invariant to alternative linearizations.",
            "failure_cases": "Models trained only on canonical PENMAN deteriorate substantially when evaluated on meaning-preserving alternative linearizations (RECONFIGURE, RANDOMIZED), indicating over-reliance on canonical order.",
            "uuid": "e8808.0",
            "source_info": {
                "paper_title": "Promoting Graph Awareness in Linearized Graph-to-Text Generation",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "RECONFIGURE",
            "name_full": "Reconfigured PENMAN linearization (RECONFIGURE)",
            "brief_description": "A PENMAN-based linearization that ignores canonical ordering (except for the top node) and may reverse edge labels (e.g., ARG0 -&gt; ARG0-of), producing a meaning-preserving but different linearized string.",
            "citation_title": "Penman: An opensource library and tool for AMR graphs",
            "mention_or_use": "use",
            "representation_name": "Reconfigured (RECONFIGURE) linearization",
            "representation_description": "Creates a PENMAN tree from the AMR graph while purposely discarding canonical ordering constraints (preserves top node), potentially changing relative ordering of subtrees and reversing edge-label orientations where appropriate.",
            "graph_type": "AMR graphs",
            "conversion_method": "Use the Penman library's RECONFIGURE operation to convert the AMR graph to a PENMAN tree that ignores canonical order information (top preserved); postprocess variables and word-senses similarly to canonical.",
            "downstream_task": "AMR-to-text generation; used in adversarial training and as an auxiliary scaffold task (reconstruct canonical from reconfigured).",
            "performance_metrics": "When used as an evaluation linearization, models trained only on canonical show large BLEU drops; when included during adversarial training, models become more robust and RECONFIGURE models attain ~40 BLEU by 3 epochs (slower than canonical). Also used in reordering scaffolding whose loss correlates with semantic-fidelity metrics (M-score).",
            "comparison_to_others": "More disruptive than simply randomizing node order because it can invert edge labels; compared against CANONICAL and RANDOMIZED in robustness experiments—adversarially training with RECONFIGURE improves robustness vs canonical-only training.",
            "advantages": "When used during training, it forces models to rely less on canonical order and better encode graph relations; useful for data augmentation and as a reordering scaffold.",
            "disadvantages": "Harder mapping to surface order (edge reversals) can make generation more difficult if the model has not been trained on such variants.",
            "failure_cases": "Models trained solely on canonical inputs produce lower-quality output on RECONFIGURE inputs; RECONFIGURE-only inputs at evaluation without adversarial training lead to substantial performance degradation.",
            "uuid": "e8808.1",
            "source_info": {
                "paper_title": "Promoting Graph Awareness in Linearized Graph-to-Text Generation",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "RANDOMIZE",
            "name_full": "Randomized PENMAN linearization (RANDOMIZE)",
            "brief_description": "A drastic alternative linearization that builds a PENMAN tree from a randomized ordering of the graph's triples, discarding canonical order signals and testing the model's invariance to traversal order.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Randomized PENMAN linearization",
            "representation_description": "Constructs a PENMAN traversal from a randomized ordering of the underlying triple set (no canonical order preserved), producing a valid but arbitrarily ordered linearization of the same AMR graph.",
            "graph_type": "AMR graphs (constructed from triples)",
            "conversion_method": "Use Penman to produce a tree from a randomly permuted set of triples, then replace variables and drop word-senses as in other setups.",
            "downstream_task": "AMR-to-text generation used for adversarial robustness experiments and reordering scaffolds.",
            "performance_metrics": "Models trained only on canonical suffer markedly on RANDOMIZE inputs; adversarial training that includes randomized linearizations improves robustness—RANDOMIZE variants reached ~40 BLEU after more epochs (around 5 epochs). Randomized-trained models still often underperform canonical at evaluation time.",
            "comparison_to_others": "More drastic than RECONFIGURE; compared in robustness tests showing that including RANDOMIZE in training improves resilience compared to canonical-only training.",
            "advantages": "Effective as a strong perturbation for adversarial training and data augmentation to reduce reliance on canonical ordering; can help the model learn graph structure rather than order heuristics.",
            "disadvantages": "Produces input strings very different from human canonical orderings, which can slow training convergence and reduce generation quality if not used carefully in training.",
            "failure_cases": "Canonical-trained models perform poorly on RANDOMIZE; even models trained with RANDOMIZE still perform best when evaluated on canonical order (canonical eval remains highest).",
            "uuid": "e8808.2",
            "source_info": {
                "paper_title": "Promoting Graph Awareness in Linearized Graph-to-Text Generation",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "RDF-tokenized",
            "name_full": "RDF-triple linearization with special tokens (WebNLG style)",
            "brief_description": "A linearization for RDF triples that prefixes relation, subject, verb, and object elements with dedicated tokens and separates triples with a special separator, yielding a flat, tokenized serialization usable by pretrained seq2seq models.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "RDF triple linearization with special tokens",
            "representation_description": "Serialize each RDF triple as a short token sequence with dedicated markers for relation/subject/verb/object (e.g., &lt;rel&gt; &lt;S&gt; subj &lt;V&gt; verb &lt;O&gt; obj), and join multiple triples with separators to form the model input.",
            "graph_type": "Sets of RDF triples (WebNLG dataset)",
            "conversion_method": "For each triple prepend special tokens for relation and element boundaries and join triples with a dedicated separator token; optionally randomize triple order for adversarial evaluation.",
            "downstream_task": "RDF-to-text generation (WebNLG)",
            "performance_metrics": "Evaluated with BLEU; experiments show strong sensitivity to relation-order randomization (RANDOMIZE of relations reduces performance if not trained with permutations). Exact BLEU numbers for WebNLG experiments are reported in tables in the paper (not all explicit in text).",
            "comparison_to_others": "Used as the standard linearization for RDF tasks in this work (following Ribeiro et al., 2020); pretrained transformer finetuned on this serialization outperforms many graph-based encoders for WebNLG.",
            "advantages": "Simple, compact, and easy to feed into pretrained transformers; facilitates multiple-triple inputs and keeps sequence lengths short.",
            "disadvantages": "Order of triples and tokenization choices can leak target ordering or encourage spurious shortcuts; models are sensitive to permutations unless trained to be robust.",
            "failure_cases": "Randomizing relation/triple order at evaluation degrades generation quality for models trained only on a fixed ordering.",
            "uuid": "e8808.3",
            "source_info": {
                "paper_title": "Promoting Graph Awareness in Linearized Graph-to-Text Generation",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "ADV-Permutation",
            "name_full": "Adversarial linearization training (per-epoch permutation augmentation)",
            "brief_description": "A training strategy that presents different meaning-preserving linearizations (canonical, reconfigured, randomized) across epochs to the pretrained seq2seq model to reduce sensitivity to a single linearization strategy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Adversarial linearization augmentation",
            "representation_description": "At each training epoch, alter the presented linearization of the same graph (e.g., canonical, reconfigured, randomized) so the model cannot rely on a single fixed input ordering and must learn a more robust implicit graph encoding.",
            "graph_type": "AMR graphs and RDF triple sets",
            "conversion_method": "Generate alternative linearizations with Penman (for AMR) or randomize RDF triple order; cycle or randomly sample among alternatives each epoch while finetuning the pretrained model.",
            "downstream_task": "AMR-to-text and RDF-to-text generation; robustness evaluation across linearizations.",
            "performance_metrics": "Leads to improved robustness across linearizations with only minor cost to canonical-generation BLEU. Example: models trained adversarially achieve similar BLEU but with improved performance when evaluated on RECONFIGURE and RANDOMIZE inputs; training-time overhead minimal (training epochs to reach a BLEU threshold vary: CANONICAL/RECONFIGURE/RANDOMIZE reached 40 BLEU at 2/3/5 epochs respectively).",
            "comparison_to_others": "Compared against canonical-only training; adversarial augmentation yields notably better robustness than no augmentation and is simpler than architectural graph encoders.",
            "advantages": "Simple to implement, reduces model dependence on annotation order, improves cross-linearization robustness, small training cost.",
            "disadvantages": "Does not fully remove sensitivity to canonical ordering (canonical eval often remains best); pretrained transformer pretraining induces residual order sensitivity that adversarial fine-tuning may not entirely overcome.",
            "failure_cases": "Even after adversarial training, models still perform best on canonical linearization at evaluation and may retain pretraining-induced order sensitivities.",
            "uuid": "e8808.4",
            "source_info": {
                "paper_title": "Promoting Graph Awareness in Linearized Graph-to-Text Generation",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "MaskedGraph (graph-denoising)",
            "name_full": "Masked Graph Modeling (graph-denoising objectives / scaffolding)",
            "brief_description": "Auxiliary denoising objectives applied to linearized graph inputs (masking nodes, edge labels/parentheses, or all tokens) trained in a multitask text-to-text framework to encourage models to learn better implicit graph encodings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Masked Graph Modeling (graph-denoising scaffolds)",
            "representation_description": "Apply masking to tokens in the linearized graph such that ~15% of tokens are masked (MASS-style masking used); variants target nodes only, graph components (edge labels/parentheses), or all tokens. The denoising objective is optimized jointly with the main graph-&gt;text objective.",
            "graph_type": "AMR graphs (but applicable to any linearized graph input)",
            "conversion_method": "Mask selected tokens in the linearized string representation (according to variant), and train the model to reconstruct the masked content or to perform the main generation task in a multi-task setup.",
            "downstream_task": "AMR-to-text generation; used as an auxiliary training scaffold to improve generation fidelity especially in low-resource settings.",
            "performance_metrics": "Substantial BLEU improvements in low-resource regimes (e.g., when training on 1000 examples or less); the paper reports that with &lt;3% of full AMR data, masked-graph scaffolds exceed prior GNN SOTA (Ribeiro et al., 2019 BLEU 27.37). On full-data training, improvements are small and within one std. Also evaluated with BertScore and M-component of MF-score (M-score) where masking leads to modest gains in BertScore but little change in M-score.",
            "comparison_to_others": "Graph masking outperforms standard sentence MLM-style masking for improving AMR-to-text in low-resource setups; various graph-masking variants perform similarly to each other (little advantage from more complex graph-aware masking schemes). Achieves comparable absolute gains to other graph-aware methods but with lower complexity.",
            "advantages": "Simple to implement, consistently beneficial in low-resource settings, encourages models to encode graph structure rather than memorize order heuristics.",
            "disadvantages": "Diminishing returns with large amounts of labeled data; different masking strategies show only small differences; does not fully remedy other error types (e.g., word-sense confusion).",
            "failure_cases": "With full dataset (tens of thousands of examples) scaffolding yields minor improvements only; models still drop modifiers or confuse word senses (partly due to preprocessing that removed sense suffixes).",
            "uuid": "e8808.5",
            "source_info": {
                "paper_title": "Promoting Graph Awareness in Linearized Graph-to-Text Generation",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "Reordering (scaffold)",
            "name_full": "Graph Reordering auxiliary objective (reconstruct canonical)",
            "brief_description": "An auxiliary training objective that asks the model to map a reconfigured or randomized linearization back to the canonical PENMAN linearization to force learning of the underlying graph structure.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Graph Reordering (reconstruct canonical from alternate linearizations)",
            "representation_description": "Provide the model with a RECONFIGURED or RANDOMIZED linearization and train it to output the canonical PENMAN serialization as an auxiliary text-to-text task; the mapping requires nontrivial graph encoding and serves as data augmentation.",
            "graph_type": "AMR graphs",
            "conversion_method": "Use Penman to create alternate linearizations and pair each alternate input with the canonical PENMAN output as an auxiliary supervision signal during multitask training.",
            "downstream_task": "AMR-to-text generation; also used as a scaffolding loss whose magnitude predicts semantic-fidelity (M-score) of generated sentences.",
            "performance_metrics": "One of the best-performing scaffolds in low-resource experiments; regression analysis shows the reordering loss is a significant predictor of M-score (semantic fidelity) beyond other covariates. Reported Pearson correlation between scaffolding loss and M-score is ρ = -0.35 (negative relationship; lower scaffold loss -&gt; higher M-score). Improvements in BLEU and human-checked fidelity were seen in low-resource settings; on full data the gains are small.",
            "comparison_to_others": "Provides similar absolute gains to other graph-aware augmentation methods but is simpler and cheaper than methods that jointly regenerate graphs at training time or use extra ranking at decode time.",
            "advantages": "Encourages faithful graph encoding, provides data augmentation via nondeterministic reconfiguration, correlates with semantic fidelity (useful as a proxy metric), and helps correct argument-order errors that baseline sometimes makes.",
            "disadvantages": "Limited incremental benefit when large amounts of supervised data are available; simultaneous generation plus reordering (joint output) did not improve results.",
            "failure_cases": "With large training sets the reordering scaffold's impact is muted; does not fix hallucination or some modifier-dropping errors; cannot recover sense distinctions lost in preprocessing.",
            "uuid": "e8808.6",
            "source_info": {
                "paper_title": "Promoting Graph Awareness in Linearized Graph-to-Text Generation",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "T5-linearized",
            "name_full": "Pretrained Transformer (T5) finetuned on linearized graphs",
            "brief_description": "Use of the pretrained T5 encoder-decoder transformer finetuned on pairs of linearized graph inputs and surface-realization outputs; serves as the primary modeling approach to test whether pretrained LMs can implicitly encode graph structure.",
            "citation_title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "mention_or_use": "use",
            "representation_name": "Pretrained Transformer finetuned on linearized graphs (T5)",
            "representation_description": "Treat graph inputs as text via a chosen linearization (PENMAN or tokenized triples) and finetune a pretrained text-to-text transformer (T5) to map the serialized graph string to the target natural-language sentence.",
            "graph_type": "AMR graphs and RDF triple sets (WebNLG)",
            "conversion_method": "Convert graphs to linearized text (PENMAN for AMR, tokenized triples for RDF) and feed into T5 encoder; decode target sentence autoregressively from the decoder.",
            "downstream_task": "AMR-to-text and RDF-to-text generation",
            "performance_metrics": "T5 finetuned on linearizations achieves state-of-the-art BLEU on AMR and WebNLG in this line of work, outperforming specialized GNN/graph-transformer encoders; specific reported metrics include BLEU improvements over prior graph encoders and favorable BertScore; numeric highlights: pretrained linearized models surpass earlier graph-encoder methods, and under certain conditions models reach ~40 BLEU on AMR evaluation within a few epochs depending on linearization regime.",
            "comparison_to_others": "Outperforms graph-encoding architectures (GNNs, graph transformers) on multiple automated metrics in this and related work, but is sensitive to input linearization whereas graph encoders explicitly model graph topology.",
            "advantages": "Strong generalization and fluency due to pretraining, simple pipeline (no graph-specific encoder required), competitive or superior metrics across tasks.",
            "disadvantages": "Implicit graph encoding can be brittle to linearization permutations and dataset annotation artifacts; may overfit to order-based shortcuts if not trained with augmentation or scaffolding.",
            "failure_cases": "Models trained only on a fixed linearization (canonical) fail to generalize to meaning-preserving alternative linearizations; also occasional modifier-dropping and word-sense confusions (exacerbated by preprocessing that removes sense suffixes).",
            "uuid": "e8808.7",
            "source_info": {
                "paper_title": "Promoting Graph Awareness in Linearized Graph-to-Text Generation",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "Sequence-to-sequence linearization (prior-work)",
            "name_full": "Sequence-to-sequence models trained on serialized graphs (non-pretrained)",
            "brief_description": "Earlier approach that trains seq2seq models from scratch on linearized graph strings (e.g., Pourdamghani et al. 2016; Konstas et al. 2017), historically outperformed by graph encoders but recently overshadowed by pretrained LMs.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Seq2Seq on linearized graphs (from-scratch)",
            "representation_description": "Serialize graph structure into a string (various linearizations) and train an encoder-decoder model (e.g., LSTM-based seq2seq) from scratch to map serialization to surface text.",
            "graph_type": "AMR and other meaning representations",
            "conversion_method": "Depth-first or other traversal serializations; fixed or randomized traversal orderings used in prior studies.",
            "downstream_task": "AMR-to-text and general graph-to-text generation",
            "performance_metrics": "Initially outperformed by graph-encoding architectures and later by pretrained linearized models; prior LSTM-based models could be made agnostic to ordering in some setups (Konstas et al., 2017) but lacked the generation quality of pretrained transformers.",
            "comparison_to_others": "Outperformed by GNN or graph-transformer encoders historically, and strongly outperformed by pretrained transformers finetuned on linearizations.",
            "advantages": "Simpler architectures; demonstrated feasibility of generation from serialized graphs.",
            "disadvantages": "Lower performance compared to modern pretrained models and explicit graph encoders; sensitive to linearization choices unless special care taken.",
            "failure_cases": "Earlier LSTM systems could overfit to training order or fail to generalize across linearizations without specific randomization strategies.",
            "uuid": "e8808.8",
            "source_info": {
                "paper_title": "Promoting Graph Awareness in Linearized Graph-to-Text Generation",
                "publication_date_yy_mm": "2020-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Neural AMR: Sequence-to-sequence models for parsing and generation",
            "rating": 2,
            "sanitized_title": "neural_amr_sequencetosequence_models_for_parsing_and_generation"
        },
        {
            "paper_title": "GPT-too: A language-model-first approach for AMR-to-text generation",
            "rating": 2,
            "sanitized_title": "gpttoo_a_languagemodelfirst_approach_for_amrtotext_generation"
        },
        {
            "paper_title": "Enhancing AMR-to-text generation with dual graph representations",
            "rating": 2,
            "sanitized_title": "enhancing_amrtotext_generation_with_dual_graph_representations"
        },
        {
            "paper_title": "Penman: An opensource library and tool for AMR graphs",
            "rating": 2,
            "sanitized_title": "penman_an_opensource_library_and_tool_for_amr_graphs"
        },
        {
            "paper_title": "Controllable meaning representation to text generation: Linearization and data augmentation strategies",
            "rating": 2,
            "sanitized_title": "controllable_meaning_representation_to_text_generation_linearization_and_data_augmentation_strategies"
        },
        {
            "paper_title": "Investigating pretrained language models for Graph-to-Text generation",
            "rating": 2,
            "sanitized_title": "investigating_pretrained_language_models_for_graphtotext_generation"
        },
        {
            "paper_title": "Generating English from Abstract Meaning Representations",
            "rating": 1,
            "sanitized_title": "generating_english_from_abstract_meaning_representations"
        },
        {
            "paper_title": "Text-to-text pre-training for data-to-text tasks",
            "rating": 1,
            "sanitized_title": "texttotext_pretraining_for_datatotext_tasks"
        }
    ],
    "cost": 0.02060925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Promoting Graph Awareness in Linearized Graph-to-Text Generation</p>
<p>Alexander Hoyle hoyle@umd.edu 
Department of Computer Science
University of Maryland
College Park</p>
<p>Ana Marasović anam@allenai.org 
Allen Institute for Artificial Intelligence</p>
<p>Paul G. Allen School of Computer Science and Engineering
University of Washington</p>
<p>Noah A Smith 
Allen Institute for Artificial Intelligence</p>
<p>Paul G. Allen School of Computer Science and Engineering
University of Washington</p>
<p>Promoting Graph Awareness in Linearized Graph-to-Text Generation</p>
<p>Generating text from structured inputs, such as meaning representations or RDF triples, has often involved the use of specialized graphencoding neural networks. However, recent applications of pretrained transformers to linearizations of graph inputs have yielded stateof-the-art generation results on graph-to-text tasks. Here, we explore the ability of these linearized models to encode local graph structures, in particular their invariance to the graph linearization strategy and their ability to reconstruct corrupted inputs. Our findings motivate solutions to enrich the quality of models' implicit graph encodings via scaffolding. Namely, we use graph-denoising objectives implemented in a multi-task text-to-text framework. We find that these denoising scaffolds lead to substantial improvements in downstream generation in low-resource settings.</p>
<p>Introduction</p>
<p>Parameter-rich pretrained transformer language models succeed at generating text that is prima facie fluent, but that closer inspection will often reveal to be semantically transgressive (Bisk et al., 2020). Indeed, there is limited practical use for unconditional text generation: we expect language to relate to some identifiable, extrinsic meaning. When a system communicates information to an individual in natural language, it will typically rely on a structured representation of that information. Consequently, generating text that faithfully conveys structured data is an important goal in NLP, where inputs can take the form of tables (ToTTo, Parikh et al., 2020), RDF triples (e.g., WebNLG, Gardent et al., 2017), or Abstract Meaning Representations (AMR, Flanigan et al., 2016).</p>
<p>To accomplish this task, models have used neural architectures that explicitly encode graphs, such as graph neural networks (GNNs, Kipf and Welling, * Work undertaken during an internship at AI2. (1) Linearize graph</p>
<p>(2) Finetune with one linearization</p>
<p>The boy wants to go Pretrained Language Model To go the boy wants Finetuned Language Model (go :arg0 (3) Evaluate with an alternative linearization 2017) and graph transformers, in order to accurately capture the structural properties of the input graph (Zhu et al., 2019;Zhao et al., 2020; to name a few). As an alternative to constraining a model architecture with a graph structure, another line of work linearizes a graph into a string ( Figure 2) and trains a sequenceto-sequence model from scratch (Pourdamghani et al., 2016;Konstas et al., 2017;Vinyals et al., 2015). Initially, this approach was outperformed by graph-based encoders, but such models have recently seen their generation performance far surpassed by pretrained transformer language models (LMs) finetuned on pairs of linearized graphs and their corresponding surface realizations (Mager et al., 2020;Kale and Rastogi, 2020;Harkous et al., 2020;Ribeiro et al., 2020, henceforth termed pretrained linearized models). Moreover, both au-tomated and human assessments indicate that text generated with LMs retains meaning at least as well as graph-encoding baselines (Mager et al., 2020). This is not the sole product of pretrained models' general language knowledge: Mager et al. (2020), using a GPT-2-based (Radford et al., 2019) model, report that ablating structural graph information (e.g., edges) in the linearized representation notably degrades generation performance, particularly in AMR-to-text tasks. The remarkable performance of pretrained linearized models is intriguing: explicit representation of the input graph by way of the model architecture appears to be well-substituted by simply writing the graph as a linear sequence.</p>
<p>In this work, we further investigate the extent to which pretrained models can leverage linearized graph inputs. Focusing on AMR graphs and sets of RDF triples in English-language datasets, we structure our investigation by first testing whether models' encodings are invariant to the linearization strategy-the way in which a graph is traversed and encoded when producing the linearized representation (see Figure 1). We discover that generation suffers under adversarial permutations of the linearization, and embrace a simple-buteffective training strategy to mitigate this problem: adversarial training (Goodfellow et al., 2015). Motivated by this finding, we encourage more faithful encodings of graph structure via denoising objectives in the more complex AMR setting. This multi-task scaffolding  reveals that straightforward masking of the graph input is sufficient to improve generation quality in low resource settings.</p>
<p>Moreover, when treating this denoising performance as a proxy for the quality of models' implicit graph encoding, we find that it explains the semantic fidelity of the resulting generation better than reasonable alternatives, suggesting possibilities for future evaluation metrics. 2 Background: Graph-to-Text Generation</p>
<p>In a graph-to-text setting, we transduce graph inputs g to their corresponding surface realization y = y 1 , . . . , y N via a parameterized probabilsitic model p θ (·). In linearized models specifically, the graph g is first mapped to text by way of a (usually deterministic) linearization function x = l(g), where p θ (·) is an off-the-shelf sequenceto-sequence model. This leads to the likelihood objective:
p θ (y | g) = N i=1 p θ (y i | x, y 1:i−1 ). When p θ (·)
is a left-to-right (autoregressive) pretrained transformer, generation quality far exceeds architectures with encoders specifically engineered to encode graphs (Mager et al., 2020;Kale and Rastogi, 2020;Harkous et al., 2020;Ribeiro et al., 2020).</p>
<p>Graph-to-Text Generation Datasets</p>
<p>We explore two datasets for generation from a graph structure to English text.</p>
<p>Abstract Meaning Representation (AMR, Banarescu et al., 2013) is a formalism intended to represent the propositional meaning of utterances-"who is doing what to whom"-using graphs that have minimal dependence on the surface form. AMR graphs are directed and acyclic with a single "top" node (Goodman, 2020). They can be represented as either a graph, a tree, or sets of triples (van Noord and Bos, 2017). For our data, we use the AMR 2.0 release (LDC2017T10), 1 both because it spans a varied set of domains and styles, and because of its extensive use in prior work.</p>
<p>A simpler graph-to-text problem involves converting a set of RDF triples to natural text realizations of the information contained in the set, exemplified by the WebNLG dataset (Gardent et al., 2017). WebNLG pulls information from an existing knowledge base (DBPedia, Mendes et al., 2012) for a specific subset of 15 categories (e.g., "astro-naut"). To generate the paired sentences, crowdworkers verbalize individual triples. Then, for examples consisting of multiple triples, they merge already-annotated sentences and apply minimal changes (leading to reduced sentence complexity relative to AMR, see perplexity scores in Table 1). There can be multiple surface realizations per input.</p>
<p>Models To study pretrained linearized models' invariance to graph linearization, we use T5 (Raffel et al., 2020), an encoder-decoder transformer (Vaswani et al., 2017) that has led to state-of-the-art generation on AMR (specifically, LDC2017T10) and WebNLG (Kale and Rastogi, 2020;Ribeiro et al., 2020).</p>
<p>We modify the T5 implementation from the transformers library (Wolf et al., 2020). 2 We use the Adafactor optimizer (Shazeer and Stern, 2018) with a learning rate of 0.0001, selected from the set {0.001, 0.0001, 3 × 10 −5 , 1 × 10 −5 , 1 × 10 −6 } after tuning on 1000 training examples across five random seeds. 3 We set the batch size to 6 and train until development set BLEU has not improved for 10 epochs. During decoding, we use a beam size of 10 for WebNLG and 5 for AMR.</p>
<p>Evaluation Measures As a primary metric, we evaluate generated text using BLEU (Papineni et al., 2002), calculated with SacreBLEU (Post, 2018). Despite its limitations in generation settings, BLEU still generally accords with rankings of models, either by human evaluations or by alternate metrics (Manning et al., 2020). We also evaluate our scaffolding models ( §4) using BertScore , which measures token similarity with contextual embeddings, permitting a more nuanced measure of semantic similarity. Lastly, we use the M portion of the MF-score (Opitz and Frank, 2020), which measures how well the source AMR graph can be reconstructed from the generated target sentence using an off-the-shelf parser. Unlike BLEU, which applies corpus-wide, this metric provides a best-guess at sentence-level accuracy. particular method used to linearize the input graph. Motivated by the strong graph-to-text performance of these models, we ask: do they implicitly develop a robust internal encoding of the input graph? Whereas a GNN-based model has an architecture designed for graph representation (e.g., information flows between adjacent nodes in a message-passing update), a linearized model must infer how connections are specified in a sequence during training.</p>
<p>If linearized models do form a representation, then the their estimates of the target sentence should be invariant to an alternative linearization of the same graph, so long as the original linearization is in principle recoverable from this alternative. If a model meets this criterion, we call it linearization-invariant.</p>
<p>Experimental Setup</p>
<p>To better understand models' graph-encoding behavior, we experiment with adversarial linearization strategies in two graph-to-text settings.</p>
<p>Permutations of AMR-Graph Linearizations</p>
<p>Standard AMR corpora are linearized as spanning trees over the graphs in PENMAN notation (Matthiessen and Bateman 1991, see Fig. 2a). In the present work, we also linearize graphs using PENMAN, doing so for several reasons: (1) it is sufficiently flexible to accommodate significant changes to the linearization, discussed below; (2) it is more concise than sets of directed triples, both reducing training time and ensuring that inputs fit in the transformer context window; (3) the format leads to superior generation over reasonable alternatives, e.g., DFS traversal paths (Mager et al., 2020).</p>
<p>We will refer to the human-created linearizations in AMR corpora as CANONICAL, since annotators follow a standardized process. There is evidence that this format, in particular the relative ordering of edge types, leaks information about the associated sentence order (Konstas et al., 2017). We speculate that overparametrized models may overfit to such correlations rather than develop robust implicit graph encodings, since it has been repeatedly reported that large models use dataset shortcuts (Jia and Liang, 2017;Gururangan et al., 2018;Geva et al., 2019, among others).</p>
<p>As an alternative linearization, Goodman (2020) defines the RECONFIGURE operation as creating a tree from an AMR graph, where order information from the canonical linearization is ignored, except for the top node (e.g., and in Figs. 2a and 2b).</p>
<p>(a / and :op1 (d / dream-01 :ARG1 (f / film :ARG0-of (d2 / disturb-01)) :ARG2-of (r / resemble-01 :ARG1 a2)) :op2 (a2 / and :op1 (f2 / fascinate-01 :ARG0 f) :op2 d2)) (a) Canonical (a / and :op1 (d / dream-01 :ARG2-of (r / resemble-01) :ARG1 (f / film :ARG0-of (f2 / fascinate-01) :ARG0-of d2)) :op2 (a2 / and :op2 (d2 / disturb-01) :op1 f2 :ARG1-of r))</p>
<p>(b) Reconfigured</p>
<p>(r / resemble-01 :ARG2 (d / dream-01 :op1-of (a / and :op2 a2) :ARG1 (f / film)) :ARG1 (a2 / and :op1 (f2 / fascinate-01 :ARG0 f) :op2 (d2 / disturb-01 :ARG0 f))) (c) Randomized Figure 2: Three PENMAN-based linearizations of AMR graphs corresponding to the sentence, "The film is a dream and, like a dream, is both fascinating and disturbing." Note that the bolded relation in the graph, (resemble-01 :ARG1 and), is represented differently depending on the linearization.</p>
<p>Although it is not a labeled element in the graph, the top node conveys structural information about the sentence-for instance, it is often the main verb. Reconfiguration can include reversals of edge labels (e.g., ARG0 to ARG0-of), therefore constituting a substantive change to the linearization. We also experiment with a more drastic restructuring of the graph, where we construct a tree from a RANDOMIZED triple set alone, disregarding all order information from the canonical format (Fig. 2c). Since it remains a valid traversal of the graph, in principle a model should be able to use this information to construct the surface sentence.</p>
<p>We parse, reconfigure, and randomize graphs using the Penman library (Goodman, 2020), 4 then replace variable names with their references and remove word sense information, following Ribeiro et al. (2019).</p>
<p>Permutations of RDF-Triple Linearizations</p>
<p>We follow the procedure of Ribeiro et al. (2020) to form our standard linearization: we prepend a special token to each element of the triple, and separate triples with another dedicated token. For the output sentence "Ned is the father of Rod and Todd," we would have:</p>
<p>In: (Ned fatherOf Rod), (Ned fatherOf Todd) Out: <rel> <S> Ned <V> father of <O> Rod <rel> <S> Ned <V> father of <O> Todd For our adversarial permutation, we RANDOMIZE the ordering of the relations.</p>
<p>Encouraging Robustness to Linearization We train additional models with the goal of encouraging an agnosticism to graph linearization strategy. We adopt an adversarial training approach (Goodfellow et al., 2015), and alter the graph linearization 4 github.com/goodmami/penman presented to the model at each epoch. We argue that this scheme ought to reduce any model dependence on the human-derived annotation.</p>
<p>Robustness Results</p>
<p>For both tasks, we train the model on the canonical linearization, then evaluate on the various linearizations described in Section 3.1.</p>
<p>Impact of Adversarial Linearizations</p>
<p>The CANONICAL columns of Table 2 show results for models trained on that linearization, then evaluated on permuted graph linearizations. We note a strong negative impact in models' generation capacity for both tasks, with a starker decrease for the AMR data. These results suggest that pretrained linearized models are not linearization-invariant, failing to learn robust implicit graph representations, even in the case of the much simpler WebNLG data.</p>
<p>The remaining columns of Table 2 show that our straightforward adversarial training technique improves robustness, with only minor cost to generation performance. This is the case even with the more drastic RANDOMIZED AMR linearization. Moreover, it only incurs a minor impact on training time-for AMR, the CANONICAL, RECONFIGURE, and RANDOMIZE variants attain 40 BLEU at 2, 3, and 5 epochs, respectively.</p>
<p>Given that elements of canonical annotations are known to correlate with the target sentence order (Konstas et al., 2017), we do not find it surprising that the models trained and evaluated on the permuted linearizations show decreased performance. However, it is meaningful that the canonical linearization at evaluation time still leads to the best results, even for models trained with the randomized inputs-these models did not learn to associate the canonical ordering signal with the input graph. One possible explanation is that the earlier pretrain- ing induces a sensitivity to input token order that persists despite the adversarial fine-tuning, but the behavior merits further exploration.</p>
<p>RQ2: Better Implicit Graph Encodings with Text-to-Text Scaffolding</p>
<p>The positive results of our adversarial training procedure ( §3.2) suggest that pretrained linearized models can form a robust internal graph representation, even though they rely on linearized inputs. Under substantively different linearizations, models retain the ability to generate accurately (even the RANDOMIZE model outperforms best-in-class graph transformers; . Prior work, involving both GNNs and pretrained linearized models, has explored various ways of improving models' sensitivity to the structure of the input graph. To better maintain fidelity to the graph, previous graph-to-text methods incorporate additional loss terms, specialized architectures, or generation-time ranking to influence the semantic accuracy of generation: ranking outputs by the correctness of the AMR parse (Mager et al., 2020;Harkous et al., 2020), jointly "back-parsing" graphs when decoding (Bai et al., 2020), or using distinct components to model different graph traversals (Ribeiro et al., 2019).</p>
<p>These efforts suggest that explicitly accounting for graph structure can assist generation. Can we expand on this idea, and improve generation quality by inducing more robust internal graph representations? To answer this question, we propose secondary objectives designed to promote graph "awareness." In addition to the above graph-to-text approaches, we also draw inspiration from denoising methods used in language model pretraining (Raffel et al., 2020;Lewis et al., 2020), as well as syntactic scaffolds that support semantic tasks with an auxiliary syntax-dependent loss . Intermediate auxiliary pretraining has been repeatedly shown to be successful in other contexts (Phang et al., 2018;Gururangan et al., 2020).</p>
<p>Experimental Setup</p>
<p>In particular, we propose unsupervised graphdenoising tasks that we train alongside AMR-totext generation, following the multi-task setup of Raffel et al. (2020). For each batch, we either optimize the likelihood in Section 2 or one of the objectives described below. 5</p>
<p>Masked Graph Modeling When training transformers to have wide-ranging natural language capabilities, unsupervised denoising objectives like masked language modeling have proved extremely successful (Devlin et al., 2019;Raffel et al., 2020). We argue that a similar principle ought to apply to graph understanding, and therefore apply masking directly to linearized graphs.</p>
<p>In masked language modeling, each word token is masked with probability 15%. Here, we mask different sets of tokens, depending on the experimental condition, always setting the probability such that 15% of all tokens will be masked. Specifically, we mask: all tokens in the linearized graph, the graph components alone (edge labels and parentheses), and the semantic nodes. We also experiment with standard masking of the surface sentence, which mirrors the unsupervised domain-adapted pretraining employed by Ribeiro et al. (2020)  Graph masking can also be performed on any of the linearization variants defined in Section 3.1. 7</p>
<p>Graph Reordering Building on our findings from Section 3.2, we introduce a reordering objective. Specifically, we provide the model with a RECONFIGURED or RANDOMIZED linearization, then task the model with reconstructing the canonical version. We suspect that learning this mapping requires that the model captures the graph structure better, leading to superior graph-to-text generation. Unlike the joint re-generation approach of Mager et al. (2020), where the input graph is copied alongside the target text, our method both requires a nontrivial encoding of the graph and has the effect of augmenting the data (due to the nondeterministic reconfiguration). 8</p>
<p>Scaffolding Results</p>
<p>We find that, overall, denoising objectives drive substantial improvements over the baseline when training on the reduced n = 1000 dataset (Table 3). In fact, using less than 3% of the full data produces results that exceed that of state-of-the-art GNN models from a year prior to this writing (BLEU 27.37, Ribeiro et al., 2019). Moreover, the results 7 We restrict ourselves to the RECONFIGURE setting given that early results showed little difference from RANDOMIZE. 8 Simultaneously generating the surface text and reordering to the canonical linearization did not improve results.  Table 4: Test-set results of scaffolding objectives and baselines trained on the full AMR dataset (LDC2017T10). Bai et al. (2020) is a state-of-theart graph transformer. Ribeiro et al. (2020) finetunes T5-LARGE, which we re-implement as our baseline model. BS is BertScore , and M is the meaning component of the MF-score (Opitz and Frank, 2020  suggest that focusing on the graph representation itself is most important: standard sentence masking (i.e., MLM-style) is less beneficial than graph masking, although it still outperforms the baseline. Surprisingly, the various graph-masking objectives perform similarly to one another-there is little benefit to more complex strategies that specifically account for the graph structure.</p>
<p>While the increased generation quality from the graph-denoising methods is not drastic relative to the MLM case, we contextualize our gains by noting that other ways of promoting greater graph awareness yield similar improvements in absolute terms-and come at the cost of greater model complexity or generation time. For instance, the use of two graph representations in Ribeiro et al.</p>
<p>(2019) achieve a roughly 1-BLEU increase over the use of one alone.</p>
<p>Based on the findings from the n = 1000 setting (Table 3), we select three of the best-Target Both Norway and Sweden have been spared violent terror acts but authorities in both countries have voiced concern about terrorists or terror financiers operating out of Scandinavia. Baseline Norwegian and Swedish authorities have spared Norway and Sweden from violent acts of terror but have voiced concern about terrorists or financiers of terror operating out of Scandinavia.</p>
<p>Ours</p>
<p>Norway and Sweden have been spared terror acts of violence but Norwegian and Swedish authorities have voiced concern about terrorists or financiers of terror operating out of Scandinavia.</p>
<p>Target</p>
<p>The 30-day simple yield fell to an average 8.19% from 8.22%; the 30-day compound yield slid to an average 8.53% from 8.56%. Baseline The simple 30 day yield fell to 8.22 percent from 8.19 percent on average and the compound 30 day yield slid to 8.56 percent from 8.53 percent on average.</p>
<p>Ours</p>
<p>Simple 30 day yields fell from 8.22 to an average 8.19% and compound 30 day yields slid from 8.56 to an average 8.53%.</p>
<p>Target</p>
<p>Many young Saudi radicals have crossed the long and porous border between the Kingdom and Iraq and joined up with Sunni Muslim insurgents there. Baseline Many young Saudi radicals have crossed the porous border from Iraq to the Kingdom and joined up with Sunni Islamic insurgents there.</p>
<p>Ours</p>
<p>Many young Saudi radicals have crossed the porous long-term border with Iraq and joined up with Sunni Islamic insurgents there. performing scaffolding objectives-mask nodes, reconfigure &amp; mask all tokens, and reorder from reconfigured-and train them at n ∈ {500, 1000, 5000, 10000, N }. Results are shown in Fig. 3. At n = 5000, representing 14% of the data, the impact of scaffolding is no longer strong across all objectives. When evaluating on the full dataset, the difference is minor (Table 4). For both BLEU and BertScore, we observe slight improvement over the baseline on average for the mask nodes case, but it is within a standard deviation of the baseline (estimated over 5 seeds). M-score does not vary between models, but it is also not yet established for fine-grained model selection. It appears that the increased size of the data supplants the need for scaffolding losses: the sheer diversity of the source graphs encourages a graph-reasoning ability sufficient to generate accurate sentences. Of course, in a realistic application, hundreds or thousands of training examples are more attainable than tens of thousands. That such straightforward methods can yield strong gains is extremely promising for future work in low-resource graph-to-text generation.</p>
<p>Qualitative Analysis In a manual analysis of 100 random model predictions, we generally observe broad agreement between the model trained with the reordering-from-reconfigured scaffold and the baseline (73% agreement in fidelity), both trained with the full dataset. However, in three cases, the baseline model fails to capture the order of arguments (e.g., "from y to x" when "from x to y" is correct), whereas the scaffolded model remains true to the graph (see Table 5; we did not note instances of the reverse case). While we fail to note "hallucinations"-material information that is not contained in the graph input-both models occasionally drop modifiers (e.g., adjectives or adverbs). Finally, a common error in both models is word-sense confusion (see the third row in Tab. 5, where "long [in length]" is substituted with "long [in duration]"). This is likely due to the removal of word-sense suffixes during preprocessing to avoid sparsity issues (long-03 → long). While currently standard practice, a system aiming to achieve perfect fidelity would require this data.</p>
<p>Encoding Graphs and Generation Performance</p>
<p>The results of Section 4.2 show that the denoising scaffolds impact generation performance. If we consider the sentence-level scaffolding loss as a proxy for the quality of its implicit graph encoding, can it help explain generation fidelity? In order to determine this relationship, we quantify generation accuracy using the M component of the MFscore (Opitz and Frank, 2020). It is calculated by first using an off-the-shelf parser to create an AMR graph from the generated target sentence, then by measuring the overlap with the gold source AMR (from 0 to 1). As seen in Fig. 4, there is a substantial negative relationship (Pearson's ρ = −0.35 * ) between these two variables, measured using outputs from the model trained with the reorderingfrom-reconfigured scaffold on the full data.  To fully operationalize the above question, we estimate a linear regression on the M score of predicted sentences from the validation set. As covariates, we include the above (logged) scaffolding loss, in addition to other metrics that have a significant independent correlation with generation quality. In particular, we use sentence-BLEU, the number of edges in the graph, graph re-entrancies, words in the target sentence, and the (also logged) sentence generation loss. 9 We use the Bayesian information criterion (BIC) to select the model from all possible combinations of the above covariates. We find that the preferred model with p covariates, p = 1 . . . 6, includes the reordering loss in all but one case (p = 2), suggesting its validity as an indicator of graph fidelity above and beyond other alternatives. As seen in Table 6, it has a significant negative relationship with the M score, larger than that of the comparablyscaled generation loss. These results indicate that the reordering loss captures important information about the quality of the graph encoding.</p>
<p>Related Work</p>
<p>Pretrained transformers for Graph-to-Text Generation Mager et al. (2020) condition GPT-2 (Radford et al., 2019) on a linearized AMR graph, then fine-tune on the corresponding surface representation text. Later work using transformers has also found success on both AMR-to-text and data-to-text tasks (Kale and Rastogi, 2020;Harkous et al., 2020;Ribeiro et al., 2020). To our knowledge, across a diverse set of tasks and automated 10 metrics, a pretrained transformer of sufficient capacity will always outperform a specialized GNN, often by a large margin. Ribeiro et al. (2020), following Gururangan et al. 2020, further pretrain on additional in-domain data, using both supervised (silver AMR parses to text) and unsupervised (denoising target text) objectives. Mager et al. (2020) use various heuristics to improve fidelity. During training, they regenerate the input graph, and in inference, they parse generations and rank their consistency with the original graph. Harkous et al. (2020) instead rank with a trained classifier, and introduce additional "state embeddings" to help indicate the ordering of graph components. The encoder-decoder methods cited in the previous paragraph eschew these approaches and nonetheless perform better. In preliminary replications of the Mager et al. experiments with T5, we find that joint re-generation leads to no improvement and moreover that the longer output sequences increase training time. Experimenting with other graphsensitive embeddings is a valuable direction for future work.</p>
<p>Graph-Dependent Losses</p>
<p>Graph Linearization Other work also studies linearizations for AMR-to-text settings.</p>
<p>As opposed to our efforts, the focus is not on enriching or measuring models' graph encoding, but instead on determining what elements of linearization (e.g., parentheses and edge labels) are necessary for generation.</p>
<p>Closest to our work is Konstas et al. (2017), who experiment with alternative graph traversals by randomizing the edge type order (less drastic than either RECONFIGURE or RANDOMIZE) with an LSTM-based model. Rather than randomizing at each epoch, as in our approach, they employ a consistent random ordering for each example during training, and do not evaluate models across different linearizations. The results help establish that LSTMs can be made agnostic to ordering, but fail to measure the extent to which models overfit to the training order (Section 3.2).</p>
<p>Ribeiro et al. (2020) report paired training and evaluation shuffling results (as in Table 2), but they ignore parentheses, only reodering node labels. Hence, their results cannot establish models' graph-encoding ability, instead revealing that node order is informative of word order, corroborating findings in Konstas et al. (2017). Both works, along with Mager et al. (2020), run ablations by removing parenthetical markers, finding that graph structure is necessary for strong generation.</p>
<p>Finally, Kedzie and McKeown (2020), appearing contemporaneously to our work, seek to control the output generation by manipulating the input linearization order, using a randomization similar to ours as an "uncontrolled" baseline. Given their focus on task-oriented dialogue planning, which uses simpler meaning representations and sentences than the AMR dataset used here (i.e., shallower graphs and limited domains), we view their work as complementary to our own.</p>
<p>Conclusion</p>
<p>In this work, we explore the graph-encoding ability of pretrained transformers through the lens of graph-to-text generation that relies on linearized graph inputs. First, we determine the extent to which these models are invariant to the method by which graphs are linearized, finding that models trained on the fixed, canonical linearizations fail to generalize to meaning-preserving alternatives. We rectify this shortcoming by training models on linearizations corresponding to alternative random traversals of the graph. Following prior work that has used graph-aware losses to improve generation quality, we then explore ways of improving models' sensitivity to the input graphs. Motivated by the success of denoising objectives in other text-to-text settings, we encourage robust internal graph encodings through additional scaffolding losses. Although scaffolding leads to tepid improvements in generation quality when training data is plentiful, it yields substantial gains in low-resource settings.</p>
<p>Figure 1 :
1Diagram of our adversarial evaluation procedure for graph-to-text generation using pretrained language models ( §3.2). (1) A graph can admit multiple possible linearizations. (2) Following standard practice, we train with a single linearization. (3) At evaluation time, we present the model with a meaning-preserving alternative.</p>
<p>Figure 3 :
3Test set BLEU on the AMR dataset (LDC2017T10) under different amounts of training data for selected scaffolding objectives (over 5 seeds).</p>
<p>Figure 4 :
4Sentence-level scaffolding loss and M-score on the validation set, using a model trained with the reordering-from-reconfigured scaffold. M-score is a measure of the generated sentence's semantic fidelity, and the scaffolding loss is a proxy for the graph encoding accuracy.</p>
<p>We organize our investigation around two research questions:RQ1 To what extent are pretrained linearized 
models invariant to graph linearization 
strategy? ( §3) 
RQ2 Does encouraging pretrained linearized 
models' implicit graph representation lead to 
better generation? ( §4) </p>
<p>N Dev. ppl. Avg. edges </p>
<p>LDC2017T10 36k 
21.1 
11.4 
WebNLG 
18k 
9.2 
3.0 </p>
<p>Table 1: Dataset statistics. Perplexity estimated on the 
development set with GPT-2 (Radford et al., 2019) fine-
tuned on the training data using default hyperparame-
ters in the transformers library (Wolf et al., 2020). </p>
<p>Table 3 :
3Development set BLEU across scaffolding ob-
jectives and baselines, trained on 1000-example subsets 
of the AMR dataset (LDC2017T10). Mean (s.d.) over 
5 seeds. </p>
<p>example, when masking components alone: </p>
<p>orig ( stupefy :ARG1 ( we ) 
) 
in ( stupefy <M> 
( we <M> ) 
out original text </p>
<p>). Mean (s.d.) over 5 seeds.500 
(1.4%) 
1,000 
(2.7%) 
5,000 
(13.7%) 
10,000 
(27.4%) 
36,520 
(100.0%) 
Training Set Size </p>
<p>0 </p>
<p>10 </p>
<p>20 </p>
<p>30 </p>
<p>40 </p>
<p>BLEU Score </p>
<p>Baseline 
Mask nodes 
Reconfigured, mask all 
Reorder from reconfigured </p>
<p>Table 5 :
5Selected predictions from the baseline and a model using the reordering-from-reconfigured scaffold 
(trained on the full data). </p>
<p>Table 6 :
6OLS regression results on validation sentence M-score, a measure of semantic fidelity. Model trained with the reordering-from-reconfigured scaffold. *Significance at p &lt; 0.001.
catalog.ldc.upenn.edu/LDC2017T10
RQ1: Robustness to Permutation of Graph LinearizationIn this section, we explore the extent to which pretrained linearized models are invariant to the 2 We use T5-Base for WebNLG and T5-Large for AMR, finding that the larger model did not benefit the WebNLG task.3  Less extensive experiments with the full dataset indicated the same optimal setting, although in general it is relatively robust to learning rate.
Per-task batches proved marginally better than mixing within a batch. The scaffolding task probability is a hyperparameter, which we set to 0.5.6  We use MASS-style masking(Song et al., 2019) for the tokens, rather than the span-replacing of T5, as it performed somewhat better.
We eliminate outliers consisting of the bottom 0.5% of target lengths and M-scores and the top 0.5% of the losses.
Human evaluation has been less thorough, althoughMager et al. (2020) report improved human judgments on AMR-to-text generation. We note similar results in our own experiments.</p>
<p>Online back-parsing for AMR-to-text generation. Xuefeng Bai, Linfeng Song, Yue Zhang, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational LinguisticsXuefeng Bai, Linfeng Song, and Yue Zhang. 2020. On- line back-parsing for AMR-to-text generation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1206-1219, Online. Association for Computa- tional Linguistics.</p>
<p>Abstract Meaning Representation for sembanking. Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, Nathan Schneider, Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse. the 7th Linguistic Annotation Workshop and Interoperability with DiscourseSofia, BulgariaAssociation for Computational LinguisticsLaura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. 2013. Abstract Meaning Representation for sembanking. In Proceedings of the 7th Linguis- tic Annotation Workshop and Interoperability with Discourse, pages 178-186, Sofia, Bulgaria. Associa- tion for Computational Linguistics.</p>
<p>Experience grounds language. Yonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob Andreas, Yoshua Bengio, Joyce Chai, Mirella Lapata, Angeliki Lazaridou, Jonathan May, Aleksandr Nisnevich, Nicolas Pinto, Joseph Turian, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational LinguisticsYonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob Andreas, Yoshua Bengio, Joyce Chai, Mirella Lap- ata, Angeliki Lazaridou, Jonathan May, Aleksandr Nisnevich, Nicolas Pinto, and Joseph Turian. 2020. Experience grounds language. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8718-8735, Online. Association for Computational Linguistics.</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaLong and Short Papers1Association for Computational LinguisticsJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Associ- ation for Computational Linguistics.</p>
<p>Generation from Abstract Meaning Representation using tree transducers. Jeffrey Flanigan, Chris Dyer, Noah A Smith, Jaime Carbonell, 10.18653/v1/N16-1087Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSan Diego, CaliforniaAssociation for Computational LinguisticsJeffrey Flanigan, Chris Dyer, Noah A. Smith, and Jaime Carbonell. 2016. Generation from Abstract Meaning Representation using tree transducers. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, pages 731-739, San Diego, California. Association for Computational Linguistics.</p>
<p>The WebNLG challenge: Generating text from RDF data. Claire Gardent, Anastasia Shimorina, Shashi Narayan, Laura Perez-Beltrachini, 10.18653/v1/W17-3518Proceedings of the 10th International Conference on Natural Language Generation. the 10th International Conference on Natural Language GenerationSantiago de Compostela, SpainAssociation for Computational LinguisticsClaire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. 2017. The WebNLG challenge: Generating text from RDF data. In Pro- ceedings of the 10th International Conference on Natural Language Generation, pages 124-133, San- tiago de Compostela, Spain. Association for Compu- tational Linguistics.</p>
<p>Are we modeling the task or the annotator? an investigation of annotator bias in natural language understanding datasets. Mor Geva, Yoav Goldberg, Jonathan Berant, 10.18653/v1/D19-1107Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsMor Geva, Yoav Goldberg, and Jonathan Berant. 2019. Are we modeling the task or the annotator? an inves- tigation of annotator bias in natural language under- standing datasets. In Proceedings of the 2019 Con- ference on Empirical Methods in Natural Language Processing and the 9th International Joint Confer- ence on Natural Language Processing (EMNLP- IJCNLP), pages 1161-1166, Hong Kong, China. As- sociation for Computational Linguistics.</p>
<p>Explaining and harnessing adversarial examples. Ian J Goodfellow, Jonathon Shlens, Christian Szegedy, International Conference on Learning Representations (ICLR). Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. 2015. Explaining and harnessing adversar- ial examples. In International Conference on Learn- ing Representations (ICLR).</p>
<p>Penman: An opensource library and tool for AMR graphs. Michael Wayne Goodman, 10.18653/v1/2020.acl-demos.35Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations. the 58th Annual Meeting of the Association for Computational Linguistics: System DemonstrationsOnline. Association for Computational LinguisticsMichael Wayne Goodman. 2020. Penman: An open- source library and tool for AMR graphs. In Proceed- ings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstra- tions, pages 312-319, Online. Association for Com- putational Linguistics.</p>
<p>Don't stop pretraining: Adapt language models to domains and tasks. Ana Suchin Gururangan, Swabha Marasović, Kyle Swayamdipta, Iz Lo, Doug Beltagy, Noah A Downey, Smith, 10.18653/v1/2020.acl-main.740Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational LinguisticsSuchin Gururangan, Ana Marasović, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. 2020. Don't stop pretraining: Adapt language models to domains and tasks. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8342-8360, Online. Association for Computational Linguistics.</p>
<p>Annotation artifacts in natural language inference data. Swabha Suchin Gururangan, Omer Swayamdipta, Roy Levy, Samuel Schwartz, Noah A Bowman, Smith, 10.18653/v1/N18-2017Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesNew Orleans, Louisiana2Short Papers. Association for Computational LinguisticsSuchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel Bowman, and Noah A. Smith. 2018. Annotation artifacts in natural lan- guage inference data. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 107-112, New Orleans, Louisiana. Associa- tion for Computational Linguistics.</p>
<p>Have your text and use it too! end-to-end neural data-to-text generation with semantic fidelity. Hamza Harkous, Isabel Groves, Amir Saffari, Proceedings of the 28th International Conference on Computational Linguistics. the 28th International Conference on Computational LinguisticsBarcelona, SpainInternational Committee on Computational LinguisticsHamza Harkous, Isabel Groves, and Amir Saffari. 2020. Have your text and use it too! end-to-end neural data-to-text generation with semantic fidelity. In Proceedings of the 28th International Conference on Computational Linguistics, pages 2410-2424, Barcelona, Spain (Online). International Committee on Computational Linguistics.</p>
<p>Adversarial examples for evaluating reading comprehension systems. Robin Jia, Percy Liang, 10.18653/v1/D17-1215Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingCopenhagen, DenmarkAssociation for Computational LinguisticsRobin Jia and Percy Liang. 2017. Adversarial exam- ples for evaluating reading comprehension systems. In Proceedings of the 2017 Conference on Empiri- cal Methods in Natural Language Processing, pages 2021-2031, Copenhagen, Denmark. Association for Computational Linguistics.</p>
<p>Text-to-text pre-training for data-to-text tasks. Mihir Kale, Abhinav Rastogi, Proceedings of the 13th International Conference on Natural Language Generation. the 13th International Conference on Natural Language GenerationDublin, IrelandAssociation for Computational LinguisticsMihir Kale and Abhinav Rastogi. 2020. Text-to-text pre-training for data-to-text tasks. In Proceedings of the 13th International Conference on Natural Lan- guage Generation, pages 97-102, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Controllable meaning representation to text generation: Linearization and data augmentation strategies. Chris Kedzie, Kathleen Mckeown, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Chris Kedzie and Kathleen McKeown. 2020. Con- trollable meaning representation to text generation: Linearization and data augmentation strategies. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5160-5185, Online. Association for Computa- tional Linguistics.</p>
<p>Semi-supervised classification with graph convolutional networks. Thomas Kipf, M Welling, International Conference on Learning Representations. ICLRThomas Kipf and M. Welling. 2017. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representa- tions (ICLR).</p>
<p>Neural AMR: Sequence-to-sequence models for parsing and generation. Ioannis Konstas, Srinivasan Iyer, Mark Yatskar, Yejin Choi, Luke Zettlemoyer, 10.18653/v1/P17-1014Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational LinguisticsLong Papers)Ioannis Konstas, Srinivasan Iyer, Mark Yatskar, Yejin Choi, and Luke Zettlemoyer. 2017. Neural AMR: Sequence-to-sequence models for parsing and gener- ation. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Vol- ume 1: Long Papers), pages 146-157, Vancouver, Canada. Association for Computational Linguistics.</p>
<p>BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. Mike Lewis, Yinhan Liu, Naman Goyal ; Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer, 10.18653/v1/2020.acl-main.703Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational LinguisticsMike Lewis, Yinhan Liu, Naman Goyal, Mar- jan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre- training for natural language generation, translation, and comprehension. In Proceedings of the 58th An- nual Meeting of the Association for Computational Linguistics, pages 7871-7880, Online. Association for Computational Linguistics.</p>
<p>Story ending prediction by transferable bert. Zhongyang Li, Xiao Ding, Ting Liu, 10.24963/ijcai.2019/249Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19. the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19Zhongyang Li, Xiao Ding, and Ting Liu. 2019. Story ending prediction by transferable bert. In Proceed- ings of the Twenty-Eighth International Joint Con- ference on Artificial Intelligence, IJCAI-19, pages 1800-1806. International Joint Conferences on Ar- tificial Intelligence Organization.</p>
<p>GPT-too: A language-model-first approach for AMR-to-text generation. Manuel Mager, Ramón Fernandez Astudillo, Tahira Naseem, Arafat Md, Young-Suk Sultan, Radu Lee, Salim Florian, Roukos, 10.18653/v1/2020.acl-main.167Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsManuel Mager, Ramón Fernandez Astudillo, Tahira Naseem, Md Arafat Sultan, Young-Suk Lee, Radu Florian, and Salim Roukos. 2020. GPT-too: A language-model-first approach for AMR-to-text gen- eration. In Proceedings of the 58th Annual Meet- ing of the Association for Computational Linguistics, pages 1846-1852, Online. Association for Computa- tional Linguistics.</p>
<p>A human evaluation of AMR-to-English generation systems. Emma Manning, Shira Wein, Nathan Schneider, Proceedings of the 28th International Conference on Computational Linguistics. the 28th International Conference on Computational LinguisticsBarcelona, Spain (OnlineInternational Committee on Computational LinguisticsEmma Manning, Shira Wein, and Nathan Schneider. 2020. A human evaluation of AMR-to-English gen- eration systems. In Proceedings of the 28th Inter- national Conference on Computational Linguistics, pages 4773-4786, Barcelona, Spain (Online). Inter- national Committee on Computational Linguistics.</p>
<p>Text Generation and Systemic-functional Linguistics: Experiences from English and Japanese. M I M Christian, John A Matthiessen, Bateman, Communication in Artificial Intelligence Series. Pinter Pub LtdChristian M.I.M. Matthiessen and John A. Bateman. 1991. Text Generation and Systemic-functional Lin- guistics: Experiences from English and Japanese. Communication in Artificial Intelligence Series. Pin- ter Pub Ltd.</p>
<p>DBpedia: A multilingual cross-domain knowledge base. Pablo Mendes, Max Jakob, Christian Bizer, Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC'12). the Eighth International Conference on Language Resources and Evaluation (LREC'12)Istanbul, TurkeyEuropean Language Resources Association (ELRAPablo Mendes, Max Jakob, and Christian Bizer. 2012. DBpedia: A multilingual cross-domain knowledge base. In Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC'12), pages 1813-1817, Istanbul, Turkey. Eu- ropean Language Resources Association (ELRA).</p>
<p>Towards a decomposable metric for explainable evaluation of text generation from amr. Juri Opitz, Anette Frank, arXiv:2008.08896Juri Opitz and Anette Frank. 2020. Towards a decom- posable metric for explainable evaluation of text gen- eration from amr. arXiv:2008.08896.</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, 10.3115/1073083.1073135Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. the 40th Annual Meeting of the Association for Computational LinguisticsPhiladelphia, Pennsylvania, USAAssociation for Computational LinguisticsKishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic eval- uation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Com- putational Linguistics, pages 311-318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.</p>
<p>ToTTo: A controlled table-totext generation dataset. Ankur Parikh, Xuezhi Wang, Sebastian Gehrmann, Manaal Faruqui, Bhuwan Dhingra, Diyi Yang, Dipanjan Das, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Association for Computational LinguisticsOnlineAnkur Parikh, Xuezhi Wang, Sebastian Gehrmann, Manaal Faruqui, Bhuwan Dhingra, Diyi Yang, and Dipanjan Das. 2020. ToTTo: A controlled table-to- text generation dataset. In Proceedings of the 2020 Conference on Empirical Methods in Natural Lan- guage Processing (EMNLP), pages 1173-1186, On- line. Association for Computational Linguistics.</p>
<p>Bowman. Jason Phang, Thibault Févry, Samuel R , arXiv:1811.01088Sentence encoders on STILTs: Supplementary training on intermediate labeled-data tasks. Jason Phang, Thibault Févry, and Samuel R. Bow- man. 2018. Sentence encoders on STILTs: Supple- mentary training on intermediate labeled-data tasks. arXiv:1811.01088.</p>
<p>A call for clarity in reporting BLEU scores. Matt Post, 10.18653/v1/W18-6319Proceedings of the Third Conference on Machine Translation: Research Papers. the Third Conference on Machine Translation: Research PapersBrussels, BelgiumAssociation for Computational LinguisticsMatt Post. 2018. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186- 191, Brussels, Belgium. Association for Computa- tional Linguistics.</p>
<p>Generating English from Abstract Meaning Representations. Nima Pourdamghani, Kevin Knight, Ulf Hermjakob, 10.18653/v1/W16-6603Proceedings of the 9th International Natural Language Generation conference. the 9th International Natural Language Generation conferenceEdinburgh, UKAssociation for Computational LinguisticsNima Pourdamghani, Kevin Knight, and Ulf Herm- jakob. 2016. Generating English from Abstract Meaning Representations. In Proceedings of the 9th International Natural Language Generation confer- ence, pages 21-25, Edinburgh, UK. Association for Computational Linguistics.</p>
<p>Language Models are Unsupervised Multitask Learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language Models are Unsupervised Multitask Learners.</p>
<p>Exploring the limits of transfer learning with a unified text-totext transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Journal of Machine Learning Research. 21140Colin Raffel, Noam Shazeer, Adam Roberts, Kather- ine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to- text transformer. Journal of Machine Learning Re- search, 21(140):1-67.</p>
<p>Enhancing AMR-to-text generation with dual graph representations. F R Leonardo, Claire Ribeiro, Iryna Gardent, Gurevych, 10.18653/v1/D19-1314Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsLeonardo F. R. Ribeiro, Claire Gardent, and Iryna Gurevych. 2019. Enhancing AMR-to-text genera- tion with dual graph representations. In Proceed- ings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Inter- national Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), pages 3183-3194, Hong Kong, China. Association for Computational Lin- guistics.</p>
<p>Hinrich Schütze, and Iryna Gurevych. 2020. Investigating pretrained language models for Graph-to-Text generation. F R Leonardo, Martin Ribeiro, Schmitt, arXiv:2007.08426Leonardo F. R. Ribeiro, Martin Schmitt, Hinrich Schütze, and Iryna Gurevych. 2020. Investigating pretrained language models for Graph-to-Text gen- eration. arXiv:2007.08426.</p>
<p>Adafactor: Adaptive learning rates with sublinear memory cost. Noam Shazeer, Mitchell Stern, PMLRProceedings of the 35th International Conference on Machine Learning. the 35th International Conference on Machine LearningStockholm, SwedenNoam Shazeer and Mitchell Stern. 2018. Adafac- tor: Adaptive learning rates with sublinear mem- ory cost. In Proceedings of the 35th International Conference on Machine Learning, pages 4596-4604, Stockholm, Sweden. PMLR.</p>
<p>MASS: Masked sequence to sequence pre-training for language generation. Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-Yan Liu, PMLRProceedings of the 36th International Conference on Machine Learning. the 36th International Conference on Machine LearningLong Beach, California, USAKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie- Yan Liu. 2019. MASS: Masked sequence to se- quence pre-training for language generation. In Pro- ceedings of the 36th International Conference on Machine Learning, pages 5926-5936, Long Beach, California, USA. PMLR.</p>
<p>Syntactic scaffolds for semantic structures. Swabha Swayamdipta, Sam Thomson, Kenton Lee, Luke Zettlemoyer, Chris Dyer, Noah A Smith, 10.18653/v1/D18-1412Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational LinguisticsSwabha Swayamdipta, Sam Thomson, Kenton Lee, Luke Zettlemoyer, Chris Dyer, and Noah A. Smith. 2018. Syntactic scaffolds for semantic structures. In Proceedings of the 2018 Conference on Em- pirical Methods in Natural Language Processing, pages 3772-3782, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Neural semantic parsing by character-based translation: Experiments with Abstract Meaning Representations. Rik Van Noord, Johan Bos, Computational Linguistics in the Netherlands Journal. 7Rik van Noord and Johan Bos. 2017. Neural semantic parsing by character-based translation: Experiments with Abstract Meaning Representations. Computa- tional Linguistics in the Netherlands Journal, 7:93- 108.</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems (NeurIPS). Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Pro- cessing Systems (NeurIPS).</p>
<p>Grammar as a foreign language. Oriol Vinyals, Terry Kaiser, Slav Koo, Ilya Petrov, Geoffrey Sutskever, Hinton, Advances in Neural Information Processing Systems. Curran Associates, Inc28Oriol Vinyals, Ł ukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, and Geoffrey Hinton. 2015. Gram- mar as a foreign language. In Advances in Neural Information Processing Systems, volume 28, pages 2773-2781. Curran Associates, Inc.</p>
<p>AMR-to-text generation with graph transformer. Tianming Wang, Xiaojun Wan, Hanqi Jin, 10.1162/tacl_a_00297Transactions of the Association for Computational Linguistics. 8Tianming Wang, Xiaojun Wan, and Hanqi Jin. 2020. AMR-to-text generation with graph transformer. Transactions of the Association for Computational Linguistics, 8:19-33.</p>
<p>Transformers: State-of-the-Art Natural Language Processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Le Xu, Sylvain Scao, Mariama Gugger, Drame, 10.18653/v1/2020.emnlp-demos.6Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2020 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsQuentin Lhoest, and Alexander RushOnline. Association for Computational LinguisticsThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pier- ric Cistac, Tim Rault, Remi Louf, Morgan Funtow- icz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Trans- formers: State-of-the-Art Natural Language Process- ing. In Proceedings of the 2020 Conference on Em- pirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online. Asso- ciation for Computational Linguistics.</p>
<p>BERTScore: Evaluating Text Generation with BERT. Tianyi Zhang, V Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, International Conference on Learning Representations. ICLRTianyi Zhang, V. Kishore, Felix Wu, Kilian Q. Wein- berger, and Yoav Artzi. 2020. BERTScore: Evalu- ating Text Generation with BERT. In International Conference on Learning Representations (ICLR).</p>
<p>Bridging the structural gap between encoding and decoding for data-to-text generation. Chao Zhao, Marilyn Walker, Snigdha Chaturvedi, 10.18653/v1/2020.acl-main.224Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsChao Zhao, Marilyn Walker, and Snigdha Chaturvedi. 2020. Bridging the structural gap between encod- ing and decoding for data-to-text generation. In Pro- ceedings of the 58th Annual Meeting of the Asso- ciation for Computational Linguistics, pages 2481- 2491, Online. Association for Computational Lin- guistics.</p>
<p>Modeling graph structure in transformer for better AMR-to-text generation. Jie Zhu, Junhui Li, Muhua Zhu, Longhua Qian, Min Zhang, Guodong Zhou, 10.18653/v1/D19-1548Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsJie Zhu, Junhui Li, Muhua Zhu, Longhua Qian, Min Zhang, and Guodong Zhou. 2019. Modeling graph structure in transformer for better AMR-to-text gen- eration. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu- ral Language Processing (EMNLP-IJCNLP), pages 5459-5468, Hong Kong, China. Association for Computational Linguistics.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2384 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2384</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2384</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-64.html">extraction-schema-64</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <p><strong>Paper ID:</strong> paper-677cd63110d9fd4f390aecce1d571e4451a13752</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/677cd63110d9fd4f390aecce1d571e4451a13752" target="_blank">HLTCOE at LiveRAG: GPT-Researcher using ColBERT retrieval</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> The HLTCOE LiveRAG submission utilized the GPT-researcher framework for researching the context of the question, filtering the returned results, and generating the final answer, placing 5th in the LiveRAG automatic evaluation for correctness.</p>
                <p><strong>Paper Abstract:</strong> The HLTCOE LiveRAG submission utilized the GPT-researcher framework for researching the context of the question, filtering the returned results, and generating the final answer. The retrieval system was a ColBERT bi-encoder architecture, which represents a passage with many dense tokens. Retrieval used a local, compressed index of the FineWeb10-BT collection created with PLAID-X, using a model fine-tuned for multilingual retrieval. Query generation from context was done with Qwen2.5-7B-Instruct, while filtering was accomplished with m2-bert-80M-8k-retrieval. Up to nine passages were used as context to generate an answer using Falcon3-10B. This system placed 5th in the LiveRAG automatic evaluation for correctness with a score of 1.07.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2384.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2384.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-Researcher</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-Researcher framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-orchestration framework that performs iterative 'research' by (1) issuing retrievals, (2) generating follow-up search queries from retrieved context, (3) retrieving additional passages, (4) chunking and filtering snippets, and (5) synthesizing a final answer. It implements an abstractive summarization paradigm for assembling retrieved evidence into responses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-Researcher (iterative research / query-generation framework)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A pipeline-style, LLM-driven research framework that automates exploratory search: an LLM (Qwen2.5-7B-Instruct in this paper) produces additional search queries from an initial question and retrieved context; a dense retrieval engine (ColBERT with PLAID-X compression) retrieves passages; passages are chunked into overlapping snippets and filtered by embedding cosine similarity (m2-bert-80M-8k-retrieval) against the original question; a generation LLM (Falcon3-10B-Instruct) then synthesizes a concise answer from the retained snippets. The approach is LLM-based orchestration + dense retrieval + embedding-based snippet filtering, producing iterative query refinement rather than explicit formal hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>general scientific research / information retrieval and question answering</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>open-ended exploration (iterative information seeking and synthesis)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>iterative refinement of search queries and filtering thresholds (e.g., cosine-similarity cutoff), but no explicit novelty-feasibility optimization reported</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'HLTCOE at LiveRAG: GPT-Researcher using ColBERT retrieval', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2384.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2384.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen2.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen2.5-7B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B-parameter instruct-tuned large language model used in the paper to generate additional search queries (query formulation) given an initial question plus retrieved context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Qwen2.5 Technical Report</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Qwen2.5-7B-Instruct (LLM-based query generator)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An instruct-tuned LLM used to convert the initial question and retrieved context into multiple refined web-search queries (the paper set max_iterations=2 producing two additional queries). The model is used with an explicit prompt that requests a list of search query strings to form an objective opinion and guide subsequent retrievals. This is an LLM-based automatic idea/query generation method (generates search queries rather than formal scientific hypotheses).</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>general / information retrieval and question answering</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>open-ended exploration (query generation to find relevant evidence)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>reduced number of generated queries (tuned for efficiency); no explicit novelty-feasibility balancing strategy reported</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'HLTCOE at LiveRAG: GPT-Researcher using ColBERT retrieval', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2384.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2384.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>m2-bert-filter</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>m2-bert-80M-8k-retrieval (snippet embedding filter)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A lightweight BERT-based embedding model used to embed the original question and passage snippets and compute cosine similarity for snippet-level filtering (threshold >= 0.35 in this work).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>m2-bert-80M-8k-retrieval (embedding-based snippet filter)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Embeds question and passage snippets (1000-character chunks with 100-character overlap) into vector representations and filters snippets by cosine similarity to the question; snippets with similarity >= 0.35 are retained for answer synthesis. This is an automated relevance-filtering component that reduces noisy or irrelevant retrieved context prior to generation.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>information retrieval / QA pipelines</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>open-ended exploration (context filtering for synthesis)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>threshold-based filtering (fixed cosine-similarity cutoff); discussion suggests future work to explore more aggressive filtering and cross-snippet redundancy removal but no formal optimization of novelty vs feasibility reported</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'HLTCOE at LiveRAG: GPT-Researcher using ColBERT retrieval', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2384.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2384.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ColBERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ColBERT bi-encoder retrieval architecture (with PLAID-X compression)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dense retrieval engine that represents passages and queries with many dense token vectors; used to retrieve top-ranked passages from an in-house FineWeb-10BT index compressed with PLAID-X.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ColBERT (dense token-level bi-encoder retrieval) with PLAID-X compression</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A dense retrieval bi-encoder that creates per-token dense vectors for query and passage, enabling fine-grained matching; documents were split into 450-token passages for indexing, and the index was compressed with PLAID-X to improve serving efficiency. ColBERT supplies the retrieved passages that seed the iterative query-generation and snippet-filtering loop; it is part of the automated research pipeline but does not itself generate hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>information retrieval / cross-lingual dense retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>open-ended exploration (retrieving relevant documents to support synthesis)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>engineering optimizations for latency (index compression, separate hosting of content) rather than novelty-feasibility trade-off optimization</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'HLTCOE at LiveRAG: GPT-Researcher using ColBERT retrieval', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>HLTCOE at TREC 2024 NeuCLIR Track <em>(Rating: 2)</em></li>
                <li>Overview of the TREC 2024 NeuCLIR Track <em>(Rating: 2)</em></li>
                <li>Qwen2.5 Technical Report <em>(Rating: 2)</em></li>
                <li>Translate-Distill: Learning Cross-Language Dense Retrieval by Translation and Distillation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2384",
    "paper_id": "paper-677cd63110d9fd4f390aecce1d571e4451a13752",
    "extraction_schema_id": "extraction-schema-64",
    "extracted_data": [
        {
            "name_short": "GPT-Researcher",
            "name_full": "GPT-Researcher framework",
            "brief_description": "An LLM-orchestration framework that performs iterative 'research' by (1) issuing retrievals, (2) generating follow-up search queries from retrieved context, (3) retrieving additional passages, (4) chunking and filtering snippets, and (5) synthesizing a final answer. It implements an abstractive summarization paradigm for assembling retrieved evidence into responses.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "GPT-Researcher (iterative research / query-generation framework)",
            "system_description": "A pipeline-style, LLM-driven research framework that automates exploratory search: an LLM (Qwen2.5-7B-Instruct in this paper) produces additional search queries from an initial question and retrieved context; a dense retrieval engine (ColBERT with PLAID-X compression) retrieves passages; passages are chunked into overlapping snippets and filtered by embedding cosine similarity (m2-bert-80M-8k-retrieval) against the original question; a generation LLM (Falcon3-10B-Instruct) then synthesizes a concise answer from the retained snippets. The approach is LLM-based orchestration + dense retrieval + embedding-based snippet filtering, producing iterative query refinement rather than explicit formal hypotheses.",
            "research_domain": "general scientific research / information retrieval and question answering",
            "problem_type": "open-ended exploration (iterative information seeking and synthesis)",
            "novelty_metric": null,
            "novelty_score": null,
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": "iterative refinement of search queries and filtering thresholds (e.g., cosine-similarity cutoff), but no explicit novelty-feasibility optimization reported",
            "human_evaluation": false,
            "human_evaluation_results": null,
            "comparative_baseline": null,
            "comparative_results": null,
            "domain_specific_findings": null,
            "uuid": "e2384.0",
            "source_info": {
                "paper_title": "HLTCOE at LiveRAG: GPT-Researcher using ColBERT retrieval",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Qwen2.5",
            "name_full": "Qwen2.5-7B-Instruct",
            "brief_description": "A 7B-parameter instruct-tuned large language model used in the paper to generate additional search queries (query formulation) given an initial question plus retrieved context.",
            "citation_title": "Qwen2.5 Technical Report",
            "mention_or_use": "use",
            "system_name": "Qwen2.5-7B-Instruct (LLM-based query generator)",
            "system_description": "An instruct-tuned LLM used to convert the initial question and retrieved context into multiple refined web-search queries (the paper set max_iterations=2 producing two additional queries). The model is used with an explicit prompt that requests a list of search query strings to form an objective opinion and guide subsequent retrievals. This is an LLM-based automatic idea/query generation method (generates search queries rather than formal scientific hypotheses).",
            "research_domain": "general / information retrieval and question answering",
            "problem_type": "open-ended exploration (query generation to find relevant evidence)",
            "novelty_metric": null,
            "novelty_score": null,
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": "reduced number of generated queries (tuned for efficiency); no explicit novelty-feasibility balancing strategy reported",
            "human_evaluation": false,
            "human_evaluation_results": null,
            "comparative_baseline": null,
            "comparative_results": null,
            "domain_specific_findings": null,
            "uuid": "e2384.1",
            "source_info": {
                "paper_title": "HLTCOE at LiveRAG: GPT-Researcher using ColBERT retrieval",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "m2-bert-filter",
            "name_full": "m2-bert-80M-8k-retrieval (snippet embedding filter)",
            "brief_description": "A lightweight BERT-based embedding model used to embed the original question and passage snippets and compute cosine similarity for snippet-level filtering (threshold &gt;= 0.35 in this work).",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "m2-bert-80M-8k-retrieval (embedding-based snippet filter)",
            "system_description": "Embeds question and passage snippets (1000-character chunks with 100-character overlap) into vector representations and filters snippets by cosine similarity to the question; snippets with similarity &gt;= 0.35 are retained for answer synthesis. This is an automated relevance-filtering component that reduces noisy or irrelevant retrieved context prior to generation.",
            "research_domain": "information retrieval / QA pipelines",
            "problem_type": "open-ended exploration (context filtering for synthesis)",
            "novelty_metric": null,
            "novelty_score": null,
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": "threshold-based filtering (fixed cosine-similarity cutoff); discussion suggests future work to explore more aggressive filtering and cross-snippet redundancy removal but no formal optimization of novelty vs feasibility reported",
            "human_evaluation": false,
            "human_evaluation_results": null,
            "comparative_baseline": null,
            "comparative_results": null,
            "domain_specific_findings": null,
            "uuid": "e2384.2",
            "source_info": {
                "paper_title": "HLTCOE at LiveRAG: GPT-Researcher using ColBERT retrieval",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "ColBERT",
            "name_full": "ColBERT bi-encoder retrieval architecture (with PLAID-X compression)",
            "brief_description": "A dense retrieval engine that represents passages and queries with many dense token vectors; used to retrieve top-ranked passages from an in-house FineWeb-10BT index compressed with PLAID-X.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "ColBERT (dense token-level bi-encoder retrieval) with PLAID-X compression",
            "system_description": "A dense retrieval bi-encoder that creates per-token dense vectors for query and passage, enabling fine-grained matching; documents were split into 450-token passages for indexing, and the index was compressed with PLAID-X to improve serving efficiency. ColBERT supplies the retrieved passages that seed the iterative query-generation and snippet-filtering loop; it is part of the automated research pipeline but does not itself generate hypotheses.",
            "research_domain": "information retrieval / cross-lingual dense retrieval",
            "problem_type": "open-ended exploration (retrieving relevant documents to support synthesis)",
            "novelty_metric": null,
            "novelty_score": null,
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": "engineering optimizations for latency (index compression, separate hosting of content) rather than novelty-feasibility trade-off optimization",
            "human_evaluation": false,
            "human_evaluation_results": null,
            "comparative_baseline": null,
            "comparative_results": null,
            "domain_specific_findings": null,
            "uuid": "e2384.3",
            "source_info": {
                "paper_title": "HLTCOE at LiveRAG: GPT-Researcher using ColBERT retrieval",
                "publication_date_yy_mm": "2025-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "HLTCOE at TREC 2024 NeuCLIR Track",
            "rating": 2
        },
        {
            "paper_title": "Overview of the TREC 2024 NeuCLIR Track",
            "rating": 2
        },
        {
            "paper_title": "Qwen2.5 Technical Report",
            "rating": 2
        },
        {
            "paper_title": "Translate-Distill: Learning Cross-Language Dense Retrieval by Translation and Distillation",
            "rating": 1
        }
    ],
    "cost": 0.01055125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>HLTCOE at LiveRAG: GPT-Researcher using ColBERT retrieval</h1>
<p>Kevin Duh<br>HLTCOE at Johns Hopkins University<br>Baltimore, MD, USA<br>kduh1@jhu.edu</p>
<p>Eugene Yang<br>HLTCOE at Johns Hopkins University<br>Baltimore, MD, USA<br>eugene.yang@jhu.edu</p>
<p>Orion Weller<br>Johns Hopkins University<br>Baltimore, MD, USA<br>oweller2@jhu.edu</p>
<p>Andrew Yates<br>HLTCOE at Johns Hopkins University<br>Baltimore, MD, USA<br>andrew.yates@jhu.edu</p>
<h2>DAwn Lawrie</h2>
<p>HLTCOE at Johns Hopkins University<br>Baltimore, MD, USA<br>lawrie@jhu.edu</p>
<h2>ABSTRACT</h2>
<p>The HLTCOE LiveRAG submission utilized the GPT-researcher framework for researching the context of the question, filtering the returned results, and generating the final answer. The retrieval system was a ColBERT bi-encoder architecture, which represents a passage with many dense tokens. Retrieval used a local, compressed index of the FineWeb10-BT collection created with PLAID-X, using a model fine-tuned for multilingual retrieval. Query generation from context was done with Qwen2.5-7B-Instruct, while filtering was accomplished with m2-bert-80M-8k-retrieval. Up to nine passages were used as context to generate an answer using Falcon3-10B. This system placed 5th in the LiveRAG automatic evaluation for correctness with a score of 1.07 .</p>
<h2>1 INTRODUCTION</h2>
<p>We adopted GPT-Researcher as the main framework for our LiveRAG submission to use an abstractive summarization paradigm to support answer generation. This approach was one of the best performing systems in TREC NeuCLIR's report generation tasks [4]. We used Qwen2.5-7B-Instruct ${ }^{1}$ [7] with 8 bit quantization for query formulation. We used a ColBERT-based search engine [5] for retrieval with a PLAID-X model trained multilingually [9]. The retrieval system returned top-ranked passages. Passages were then chunked into snippets. Passage snippets were filtered based on the cosine similarity between the original question and the snippet using m2-bert-80M-8k-retrieval ${ }^{2}$ [3]. The general architecture appears in Figure 1. Finally, Falcon3-10B-Instruct ${ }^{3}$ [8] was used to generate the final response. The remainder of the paper outlines the detailed architecture of our system as well the prompts we used in the different steps in our pipeline. We describe the steps we took to validate the approach and optimize the performance to be able to perform at the required speed for the competition.</p>
<h2>2 SYSTEM DESIGN</h2>
<p>Our RAG system divided the process into two main stages. In the first stage research was conducted to identify relevant information and in the second stage the response was written. Conducting research included generating queries based on an initial search and then gathering all the responsive data together from the retrieved</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>passages. Writing the response used the gathered context to generate the final output of the system. In Section 2.1 we describe the process for conducting research. Section 2.2 describes the process of generating the response given the research.</p>
<h3>2.1 Conduct Research</h3>
<p>The research stage involves four major steps: (1) issue an initial retrieval to set the context, (2) review the context to determine additional search queries, (3) issue additional search queries, and (4) divide passages into snippets and the snippets based on responsiveness to the original LiveRAG question.</p>
<p>To set the initial context for the research, the top three ranked passages are used based on the query entered by the user, in this case the content of the question field for LiveRAG. Retrieval was done with an in-house index of the FineWeb-10BT collection. A ColBERT architecture search engine [5] was used, which creates a dense vector per token for both the passage and query. We divided documents into non-overlapping passages of 450 tokens each for indexing. For efficiency, the index was compressed using the PLAID-X version ${ }^{4}$ of ColBERT. Since it was possible for FineWeb10BT to contain non-English documents, we chose to use an XLMR model ${ }^{5}$ [1] fine-tuned using translate distill [9] for multilingual retrieval, which is trained with English queries and documents in several languages including English and Spanish. The training dataset is MS MARCO [6]. MS MARCO documents were translated in-house with Sockeye Version 2 [2] to create the training data.</p>
<p>The prompt in Figure 2 was used to determine additional queries that were used for retrieval. We set fields referenced in the prompt as follows:</p>
<ul>
<li>max_iterations - 2</li>
<li>task - the value of the question field</li>
</ul>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>Write {max_iterations} google search queries to search online that form an objective opinion from the following task: " ${$ task $}$ " Assume the current date is ${$ date $}$ if required. You are a seasoned research assistant tasked with generating search queries to find relevant information for the following task: " ${$ task $}$ ". Context: {context}
Use this context to inform and refine your search queries. The context provides real-time web information that can help you generate more specific and relevant queries. Consider any current events, recent developments, or specific details mentioned in the context that could enhance the search queries.
You must respond with a list of strings in the following format: ${{$ dynamic_example $} }$. The response should contain ONLY the list.</p>
<p>Figure 2: Query Generation Prompt</p>
<p>Information: "{context}" - Using the above information, answer the following query or task: "{question}" in one or two sentences. Use at most {total_words} words.</p>
<p>Figure 3: Response Generation Prompt</p>
<ul>
<li>date - the date the software is run</li>
<li>context - a concatenation of the top three ranked passages using the question as the search query</li>
<li>dynamic_examples - based on max_iterations and in this case is "query 1", "query 2"
This prompt was issued to the Qwen2.5-7B-Instruct-Turbo model. The top three passages were kept from each of the two queries generated as well as the original query. This created a list of nine passages. Our PLAID-X search engine was used to return passages for each the queries.</li>
</ul>
<p>The final step of conducting research utilized the m2-bert-80M-8k-retrieval model to embed both the original question and the passage in chunks of 1000 characters with 100 character overlap, referred to as snippets herein. The cosine similarity of the two vectors was computed. Any snippet whose similarity with the question was at least 0.35 was maintained for answer generation.</p>
<h3>2.2 Generate Response</h3>
<p>In order to generate the response to the LiveRAG question, the snippets identified during the research phase are used as source material. In particular, the system role is left blank. The user role is the prompt in Figure 3. This prompt has three attributes:</p>
<ul>
<li>context - a concatenation of the text in each of the snippets that was accepted after the final filtering stage</li>
<li>question - the original LiveRAG question.</li>
<li>total_words - 200</li>
</ul>
<p>A local version of Falcon3-10B-Instruct was used to generate the final response as specified in the rules for LiveRAG. The order of snippets in the prompt is not a rank order. Instead snippet ordering is determined by the rank order of the passage to the individual query that retrieved it as well as the lexical ordering of the chunk in the passage. More precisely, generated query $Q_{1}$ retrieves passages $A, B$ and $C$ in rank order. Generated query $Q_{2}$ retrieves passages $D$, $E$, and $F$ in rank order. The original question $Q_{o}$ retrieves passages $G, H$, and $I$. Passages are ordered $A$ to $I$. While passages are of length 450 tokens, they have a variable number of characters since not all tokens are of the same length. Each of the passages are divided into [length of (Passage) $/ 1000$ ] snippets. Some of the snippets are filtered because they are not sufficiently similar to the original query.</p>
<p>In addition to the prompt for answer generation and the generated answer, LiveRAG asked for a list of documents. The documents were ordered based on their retrieval for the individual queries. The three queries were concatenated together. While nine passages were retrieved, there may have been fewer documents if multiple passages from the same document were retrieved by one or more queries. Only the first time the document occurs is included in the final list.</p>
<h2>3 EXPERIMENTS BEFORE LIVE CHALLENGE DAY</h2>
<p>The LiveRAG system was adapted from the system developed for the NeuCLIR Pilot Report Generation task [10]. Prompts for that system were optimized for Claude and GPT-4. Prompts for LiveRAG were adapted to Qwen and Falcon for the Query Generation Prompt and the Response Generation Prompt respectively. Development data from DataMorgana was used to generate sample questions that could be used to spot check the system, both for the ability to retrieve documents that were responsive to the query as well as to check that answers reflected the information found in the documents.</p>
<p>The main changes made to the system during development were with an eye towards improving efficiency. The number of additional generated queries was reduced from four to two. For each query the number of passages was reduced from four to three.</p>
<p>During the dry run, additional human assessment was undertaken. In general the snippets contained information pertaining to the question and in a vast majority of cases, that information was carried forward to the answer. It was noted that on occasion a snippet contained more specific information that could have yielded a more precise answer than was generated.</p>
<p>The main modification that was made as the challenge approached was to ensure that the retrieval system could return responses at a sufficient rate. Since the generation component expects the content of the retrieved passages from the search engine, PLAID-X service needs to host and serve the body text. We identified that the primary latency came from the excessive duplication of the collection in memory due to the multi-thread search service. We modified the service to host the content separately to reduce the latency sufficiently for the live challenge. Each query, after the modification, takes around 400 ms to serve with an NVIDIA V100 GPU.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 4: Histogram of number of unique documents (left), number of snippets (middle), and corresponding final prompt length (right) per input context for response generation</p>
<p>To ensure that the system would respond to all 500 questions in two hours, the question file was divided in half and two processes were used. We used a single retrieval service that batched queries for efficiency and a single version of each of the LLMs for generation. We were able to complete all generation in just over one hour.</p>
<h2>4 AUTOMATED RESULTS</h2>
<p>The HLTCOE system placed fifth among the participating teams in terms of correctness with a score of 1.070111 . The faithfulness score of 0.340711 was 14th among the 25 teams participating.</p>
<p>Appendix A includes Table 1 that illustrates an example output from our system in detail. Note that 2 queries were generated based on an initial search, after which 7 documents were retrieved in total. The passages are then chunked into smaller-sized snippets and filtered according to embedding similarity with the queries. From the example, we observe that many snippets are potentially relevant, even if nothing directly answers the question, e.g. (1), (6), (7); there are also a few clearly irrelevant snippets, e.g. (16). Note that multiple snippets from the same passage may be included in the context. In this case, we have 16 snippets from the 7 documents. In our system, we observe the final response LLM generally takes a large set of snippets as input. Figure 4 shows the variation in numbers of documents and snippets as well as the final prompt length provided in each prompt over the 500 query challenge set.</p>
<p>We also performed some analysis after the live challenge. Table 2 in Appendix A compares two different LLMs for response generation. The Falcon-10B Instruct model was used for the challenge, while a larger Llama-70B-Instruct model is compared. Anecdotally, we do not observe a significant difference in the quality of the responses; this suggests that improving the document retrieval and context construction steps may play a more important role than the size and family of model to affect the final outcome of the generation.</p>
<h2>5 FUTURE WORK</h2>
<p>In the future we are interested in investigating how the order of content impacts the generation and how aggressive the filtering
should be. While the order in which snippets were represented at generation was not strictly based on relevance, it was not clear how best to structure the information in the generation prompt to ensure that the most relevant information would be included in the output. This is worth further investigation. In these experiments, filtering was assessed for each passage independently; however, a model that can reason over more information at once would be able to prioritize information and remove redundant information. This may of interesting implication for RAG tasks.</p>
<h2>6 SUMMARY</h2>
<p>In conclusion, the HLTCOE team developed an effective RAG system that incorporated state-of-the-art components. Our search engine was able to identify content that was useful for addressing the LiveRAG questions. Our generation process was effective at proposing additional queries and assembling the information into a meaningful answer.</p>
<h2>REFERENCES</h2>
<p>[1] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmn, Edouard Grave, Mylo Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Unsupervised cross-lingual representation learning at scale. arXiv preprint arXiv:1911.02116 (2019).
[2] Tobias Domhan, Michael Denkowski, David Vilar, Xing Niu, Felix Hieber, and Kenneth Hvafield. 2020. The Sockeye 2 Neural Machine Translation Toolkit at AMTA 2020. In Proceedings of the 14th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Track). Association for Machine Translation in the Americas, Virtual, 110-115.
[3] Dan Fu, Simran Arora, Jessica Grogan, Isys Johnson, Evan Sabri Eyuboglu, Armin Thomas, Benjamin Spector, Michael Poli, Atri Rudra, and Christopher R. 2023. Monarch mixer: A simple sub-quadratic gemm-based architecture. Advances in Neural Information Processing Systems 36 (2023), 77546-77603.
[4] Dawn Lawrie, Sean MacAvaney, James Mayfield, Paul McNamee, Douglas W. Oard, Luca Soldanini, and Eugene Yang. 2025. Overview of the TREC 2024 NeuCLIR Track. In The Thirty-Third Text REtrieval Conference (TREC 2024) Proceedings.
[5] Suraj Nair, Eugene Yang, Dawn Lawrie, Kevin Duh, Paul McNamee, Kenton Murray, James Mayfield, and Douglas W. Oard. 2022. Transfer Learning Approaches for Building Cross-Language Dense Retrieval Models. In Proceedings of the 44th European Conference on Information Retrieval (ECIR). https: //arxiv.org/abs/2201.08471
[6] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. MS MARCO: A Human Generated MAchine Reading COmprehension Dataset. arXiv preprint arXiv:1611.09268 (2016).</p>
<p>arXiv:1611.09268 http://arxiv.org/abs/1611.09268
[7] Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. 2025. Qwen2.5 Technical Report. arXiv preprint arXiv:2412.15115 (2025).
[8] TII Team. 2024. The Falcon 3 family of Open Models.
[9] Eugene Yang, Dawn Lawrie, James Mayfield, Douglas W. Oard, and Scott Miller. 2024. Translate-Distill: Learning Cross-Language Dense Retrieval by Translation
and Distillation. In Proceedings of the 46th European Conference on Information Retrieval (ECIR). https://arxiv.org/abs/2401.04810
[10] Eugene Yang, Dawn Lawrie, Orion Weller, and James Mayfield. 2025. HLTCOE at TREC 2024 NeuCLIR Track. In The Thirty-Third Text REtrieval Conference (TREC 2024) Proceedings.</p>
<h2>A EXAMPLES</h2>
<p>Table 1 contains example output. Table 2 compares Falcom to Llama generation.</p>
<p>Table 1: Example system output. Generated queries are the result from the LLM call shown in Figure 2. Filtered passages form input context for the LLM call shown in Figure 3.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Input Question</th>
<th style="text-align: center;">I live close to a park with many geese. Can I get bird flu from their droppings?</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Generated <br> Queries</td>
<td style="text-align: center;">(1) can i get bird flu from geese droppings in 2025; (2) risks of bird flu transmission from geese to humans</td>
</tr>
<tr>
<td style="text-align: center;">Retrieved Docs</td>
<td style="text-align: center;">7 documents in total from 3 searches (input question plus 2 generated queries)</td>
</tr>
<tr>
<td style="text-align: center;">Filtered Snippets (docid:snippet)</td>
<td style="text-align: center;">(1) e7703b83-95fb-497a-859a-c50d8e37e329: "unaffected themselves, the host birds can spread the infection to susceptible species, especially domesticated chickens, turkeys and geese"; (2) ba608cc9-5d39-420c-98ac-b5a3bed83a60: "Droppings dry, then are pulverized in the air or dissolved in the water and from there get to other birds, wild or domestic."; (3) e7703b83-95fb-497a-859ac50d8e37e329: "The outbreak of any epidemic or the widespread of any disease leads to an inevitable slump in the market."; (4) ba608cc9-5d39-420c-98ac-b5a3bed83a60: "Sometimes, poultry also caught it. But people have never got sick or died of it."; (5) 93b69f5d-b805-4be0-bb73-06be6f33ece4: "Q) Can pets get bird flu? A) Yes."; (6) 795b59f7-6303-40dd-ba97-f3872770569c: "Although this virus can spread quickly amongst birds the risk to the public is very low and well cooked eggs and poultry remain safe to eat."; (7) 61120bc5-bc10-4c6d-825a-629a0fc13d22: "There are many types of bird flu, and most are harmless to humans, however occasionally, in rare cases where bird flu does infect humans."; (8) 93b69f5d-b805-4be0-bb73-06be6f33ece4: "Right now there is a type of bird flu called Highly Pathogenic Avian Influenza (HPAI) type H5N1 (also called H5N1 bird flu) that has made many birds and a few people sick elsewhere in the world."; (9) 795b59f7-6303-40dd-ba97-f3872770569c: "a bird keeper in Sulby found 11 of his geese dead near a pond on his property."; (10) 61120bc5-bc10-4c6d-825a-629a0fc13d22: "The incubation period for bird flu is 2-8 days which is longer than seasonal flu"; (11) bd4fd5de-4e73-4875-8f80-6e4c8fd62606: "STP Pest Control Birds are a nuisance creating problems with noise, bird mess, getting in roof spaces and in some cases aggressive behaviour." (12) 93b69f5d-b805-4be0-bb73-06be6f33ece4: "There is no evidence that properly cooked poultry or eggs can be a source of infection from bird flu." (13) 4f0151b0-6571-4fa2-8a2f-fd9756f0e3b0: "a hawk enthusiast may be willing to let his birds have at it, so long as the pigeons are not located in a high traffic area that could endanger the hawks." (14) bd4fd5de-4e73-4875-8f80-6e4c8fd62606: "Ornithosis Ornithosis is a 'flu-like disease most commonly found in pigeon droppings. Fatalities are rare, but occur as with any other influenza virus."; (15) 93b69f5d-b805-4be0-bb73-06be6f33ece4: "Teach children to always wash their hands after playing outside."; (16) 4f0151b0-6571-4fa2-8a2f-fd9756f0e3b0: "news of the victory over Napoleon at Waterloo was carried by military homing pigeons"</td>
</tr>
<tr>
<td style="text-align: center;">Answer</td>
<td style="text-align: center;">Based on the provided sources, there is currently no evidence that properly cooked poultry or eggs can be a source of infection from bird flu. Regarding contact with geese droppings, the sources do not explicitly mention the risk of contracting bird flu from goose droppings in parks or other public places. However, it's generally advised to avoid direct contact with bird droppings due to the potential presence of various bacteria and parasites. To minimize risk, it's recommended to wash hands after being in areas with bird droppings and avoid touching them with bare hands.</td>
</tr>
</tbody>
</table>
<p>Table 2: Comparison of Answers by Falcon3-10B Instruct (F) vs Llama3.1-70B Instruct (L) when given the same input context</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Q</th>
<th style="text-align: left;">What's the difference between perspectivism and the despair of defiance regarding human autonomy?</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">F</td>
<td style="text-align: left;">Perspectivism posits that our understanding of reality is always mediated by individual perspectives, whereas the despair of defiance, as described <br> by thinkers like Nietzsche, suggests a rejection of traditional moral and metaphysical constraints, leading to a state where one acknowledges the <br> lack of absolute truth or meaning, yet defiantly embraces life's chaos and multiplicity.</td>
</tr>
<tr>
<td style="text-align: left;">L</td>
<td style="text-align: left;">Perspectivism acknowledges the multiplicity of perspectives and the fragmented nature of human experience, whereas the despair of defiance <br> regarding human autonomy is a more extreme stance that rejects the idea of human autonomy altogether, often in favor of a nihilistic or <br> deterministic worldview. Perspectivism seeks to understand and navigate the complexities of human experience, whereas the despair of defiance <br> tends to abandon hope for human agency and autonomy.</td>
</tr>
<tr>
<td style="text-align: left;">Q</td>
<td style="text-align: left;">Which tattoo needle arrangements are most suitable for skin preservation?</td>
</tr>
<tr>
<td style="text-align: left;">F</td>
<td style="text-align: left;">Based on the provided information, curved magnum (RM) or curved magnum (CM/M1C) tattoo needle arrangements are most suitable for skin <br> preservation. These needles have a curvature that allows for easier movement on the skin, reducing the risk of the needle edges digging in, which <br> is particularly beneficial for delicate areas with curvature, such as the inner elbow.</td>
</tr>
<tr>
<td style="text-align: left;">L</td>
<td style="text-align: left;">Round curve magnums and curved magnums (RM or CM) are most suitable for skin preservation as they are designed to deflect the skin when <br> moving in and out, reducing the risk of edges digging into the skin and causing damage. These needle arrangements are particularly useful for <br> delicate areas with curvature, such as the inner elbow.</td>
</tr>
<tr>
<td style="text-align: left;">Q</td>
<td style="text-align: left;">What realms wil be the playble nations in the Europa Universalis board game?</td>
</tr>
<tr>
<td style="text-align: left;">F</td>
<td style="text-align: left;">In the Europa Universalis board game, the playable realms will initially focus on European nations, as the game's title and scope suggest a <br> concentration on European history and politics. However, the game may expand to include other regions through future expansions, allowing <br> players to explore and control a broader range of historical territories.</td>
</tr>
<tr>
<td style="text-align: left;">L</td>
<td style="text-align: left;">The playable nations in the Europa Universalis board game will initially be the European nations, with the main map covering Europe and the <br> Mediterranean region. Future expansions may include other parts of the world, such as North and South America, Africa, Asia, and Australia, but <br> the initial focus will be on European nations.</td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://huggingface.co/Qwen/Qwen2.5-72B-Instruct
${ }^{2}$ https://huggingface.co/togethercomputer/m2-bert-80M-8k-retrieval
${ }^{3}$ https://huggingface.co/tiiuae/Falcon3-10B-Instruct&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{4}$ https://pypi.org/project/PLAID-X/
${ }^{5}$ https://huggingface.co/FacebookAI/xlm-roberta-large&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3089 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3089</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3089</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-77.html">extraction-schema-77</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <p><strong>Paper ID:</strong> paper-277955704</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.19314v2.pdf" target="_blank">LiveBench: A Challenging, Contamination-Limited LLM Benchmark</a></p>
                <p><strong>Paper Abstract:</strong> Test set contamination, wherein test data from a benchmark ends up in a newer model's training set, is a well-documented obstacle for fair LLM evaluation and can quickly render benchmarks obsolete. To mitigate this, many recent benchmarks crowdsource new prompts and evaluations from human or LLM judges; however, these can introduce significant biases, and break down when scoring hard questions. In this work, we introduce a new benchmark for LLMs designed to be resistant to both test set contamination and the pitfalls of LLM judging and human crowdsourcing. We release LiveBench, the first benchmark that (1) contains frequently-updated questions from recent information sources, (2) scores answers automatically according to objective ground-truth values, and (3) contains a wide variety of challenging tasks, spanning math, coding, reasoning, language, instruction following, and data analysis. To achieve this, LiveBench contains questions that are based on recently-released math competitions, arXiv papers, news articles, and datasets, and it contains harder, contamination-limited versions of tasks from previous benchmarks such as Big-Bench Hard, AMPS, and IFEval. We evaluate many prominent closed-source models, as well as dozens of open-source models ranging from 0.5B to 405B in size. LiveBench is difficult, with top models achieving below 70% accuracy. We release all questions, code, and model answers. Questions are added and updated on a monthly basis, and we release new tasks and harder versions of tasks over time so that LiveBench can distinguish between the capabilities of LLMs as they improve in the future. We welcome community engagement and collaboration for expanding the benchmark tasks and models.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3089.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3089.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LiveBench_Spatial</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LiveBench Spatial Reasoning Task (LiveBench)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 50-question, handwritten spatial-reasoning task included in the LiveBench benchmark that tests LLMs on 2D/3D intersections and orientation deductions using textual scenario descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>40 evaluated LLMs (top examples: o1-preview-2024-09-12, o1-mini-2024-09-12, claude-3-5-sonnet, meta-llama-3.1-405b-instruct, qwen2.5-72b-instruct, gpt-4o-* variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A heterogeneous set of 40 language models spanning proprietary and open-source families (OpenAI o1/gpt-4o series, Anthropic Claude-3 family, Google's Gemini family, Meta Llama-3.1 variants, Qwen, Mistral, Phi, etc.) evaluated via single-turn API or local inference; models typically use transformer architectures and vary in training data and instruction tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Varied across evaluated models; reported sizes span approximately 0.5B to 405B parameters (paper reports open-source models from 0.5B up to meta-llama-3.1-405B).</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Spatial Reasoning (LiveBench)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Handwritten natural-language questions (50 items) that require reasoning about geometric relations, intersections, orientations, and consequences of spatial configurations in 2D and 3D (examples: tangent spheres producing triangle of tangent points; cutting a regular heptagon — counting resulting pieces).</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Pure text: each puzzle is presented as a natural-language description of the spatial scenario and a clearly specified answer format (e.g., single bolded phrase or integer). No images or explicit grid encodings are used; problems are verbal descriptions of geometric/spatial situations.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Single-turn prompts with temperature=0; LiveBench prompts commonly encourage zero-shot chain-of-thought / 'think step by step' reasoning and request outputs in machine-parseable form (e.g., bolded final answer, boxed LaTeX, or specific tags). Models were run with their standard chat templates (FastChat templates) and bfloat16 for open-weight inference.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>The paper treats spatial reasoning as one of three reasoning tasks (Web of Lies v2, Zebra puzzles, Spatial). The authors report qualitative difficulty (task added in first monthly update) and note that spatial correlates moderately with overall performance (Pearson r ≈ 0.7294, std.err ≈ 0.0968). No per-model per-task accuracy table for the spatial task is provided in the main text; analysis is at the category level (Reasoning) and via correlations. The authors also describe manual inspection and permissive regex-based scoring to avoid false negatives and note the task set is small (50) and handcrafted, limiting detailed statistical breakdowns in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>The paper does not publish a per-model accuracy for the spatial task alone in the provided text. It reports category-level 'Reasoning' scores per model (examples from tables: o1-preview Reasoning ≈ 62.9, o1-mini ≈ 59.2, other models lower) and states LiveBench top models obtain <70% accuracy overall. The spatial task-specific accuracy is not given; only that it is one of the reasoning tasks contributing to the Reasoning category averages.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Task set is small (50 handcrafted items), making fine-grained statistical claims difficult; puzzles are challenging for leading LLMs. The paper highlights general failure modes for hard reasoning tasks: models often fail to follow complex deductive chains and can produce incorrect final answers even if intermediate steps appear plausible. The authors emphasize that LLM-as-judge approaches fail on similar hard problems (see ablation on LLM judges), motivating ground-truth automatic scoring. The handcrafted nature means updates require human effort (spatial task marked as 'No' for automatic updates in the paper's table).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>No direct human baseline for the spatial set is reported. At the category level, some models (o1 and phi series) are outliers in reasoning performance (i.e., relatively stronger than peers), and the best open-source models (meta-llama-3.1-405B and qwen2.5-72B) tie closely with top proprietary models in aggregate. The paper does not provide a per-task head-to-head comparison to human performance for the spatial task.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LiveBench: A Challenging, Contamination-Limited LLM Benchmark', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3089.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3089.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LiveBench_Zebra</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LiveBench Zebra Puzzles Task (LiveBench)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A procedurally generated set of Zebra (Einstein-style) puzzles included in LiveBench that test constraint-following and positional/spatial deduction from natural-language clues.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>40 evaluated LLMs (same pool as LiveBench; evaluated single-turn, temperature=0; examples include o1-preview, o1-mini, claude-3 variants, meta-llama-3.1 variants, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same heterogeneous collection of transformer LLMs described across LiveBench; models vary in size and tuning, evaluated via standardized templates and settings described in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Varied; evaluated models include small (≈0.5B) to very large (≈405B) parameter models.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Zebra Puzzles (Einstein-style logic puzzles)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Logic puzzles where N people (typically 3 or 4 here) are arranged in a left-to-right line (positional/spatial relation) and have several categorical attributes (e.g., Food, Nationality, Hobby); a set of natural-language constraints relating positions and attributes is provided and the model must deduce the attribute(s) requested (e.g., what is the hobby of the person in a given position). These require reasoning about relative positions (left/right/immediate neighbor) and exclusivity constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Pure text: enumerated attributes, constraint sentences (often disambiguated to reduce ambiguity, e.g., 'immediate left' instead of 'left of'), and a question; answer required in a specific textual format (e.g., bold single-word). Puzzle generation uses a procedural generator with varying difficulty levels (levels 10–20) and a solver used to ensure uniqueness.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Single-turn textual prompts; prompts request step-by-step reasoning and specify a strict final-answer format (e.g., **X**); LiveBench encourages zero-shot chain-of-thought in prompts but enforces single-turn outputs for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>Zebra puzzles are explicitly treated as a reasoning task that includes spatial/positional reasoning. The authors proceduralized puzzle generation (varying people count, attributes, and constraint levels) and used a solver to ensure unique solutions. Zebra puzzles correlate well with overall performance (Pearson r ≈ 0.8853, std.err ≈ 0.0935), indicating they are informative about model ability. The paper also used Zebra puzzles in an ablation showing that LLM judges (gpt-4-turbo used as judge) have high error rates when asked to judge solutions to hard reasoning puzzles, demonstrating these puzzles both challenge models and LLM-based evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>The paper does not list per-model accuracy for the Zebra puzzle task in the provided excerpt. It reports that zebra puzzles strongly correlate with overall score (see correlation above). The ablation on LLM-as-judge reports high error rates for LLM judging on Zebra puzzles (judge error rates in Table 8: e.g., GPT-4-Turbo judge error ≈ 0.420 on Zebra puzzles in that judging experiment), which supports that these problems are challenging both to solve and to judge automatically with other LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Larger or higher-level puzzles (more people/attributes/harder constraint levels) become 'exceedingly difficult' for top models; ambiguity in natural-language constraints can create additional difficulty (the authors modified generator to reduce ambiguity). The dataset is synthetic/procedural and can be tuned for hardness; small sample sizes and handcrafted modifications mean performance estimates per-task are limited. LLM judges are unreliable for these puzzles, so ground-truth solution checking is required.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>No direct human performance baselines are provided in the paper. Model-wise, reasoning-category outliers include o1 and phi series (models that do disproportionately better on reasoning tasks relative to their overall score), while some Llama/Gemini/Command-R family models are outliers in instruction following. The paper emphasizes model-to-model differences in relative strengths on reasoning tasks but does not provide a detailed per-model ranking for Zebra puzzles alone in the excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LiveBench: A Challenging, Contamination-Limited LLM Benchmark', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Challenging big-bench tasks and whether chain-of-thought can solve them <em>(Rating: 2)</em></li>
                <li>Einstein's riddle: Riddles, paradoxes, and conundrums to stretch your mind <em>(Rating: 2)</em></li>
                <li>Missed connections: Lateral thinking puzzles for large language models <em>(Rating: 2)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 1)</em></li>
                <li>Large language models are zero-shot reasoners <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3089",
    "paper_id": "paper-277955704",
    "extraction_schema_id": "extraction-schema-77",
    "extracted_data": [
        {
            "name_short": "LiveBench_Spatial",
            "name_full": "LiveBench Spatial Reasoning Task (LiveBench)",
            "brief_description": "A 50-question, handwritten spatial-reasoning task included in the LiveBench benchmark that tests LLMs on 2D/3D intersections and orientation deductions using textual scenario descriptions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "40 evaluated LLMs (top examples: o1-preview-2024-09-12, o1-mini-2024-09-12, claude-3-5-sonnet, meta-llama-3.1-405b-instruct, qwen2.5-72b-instruct, gpt-4o-* variants)",
            "model_description": "A heterogeneous set of 40 language models spanning proprietary and open-source families (OpenAI o1/gpt-4o series, Anthropic Claude-3 family, Google's Gemini family, Meta Llama-3.1 variants, Qwen, Mistral, Phi, etc.) evaluated via single-turn API or local inference; models typically use transformer architectures and vary in training data and instruction tuning.",
            "model_size": "Varied across evaluated models; reported sizes span approximately 0.5B to 405B parameters (paper reports open-source models from 0.5B up to meta-llama-3.1-405B).",
            "puzzle_name": "Spatial Reasoning (LiveBench)",
            "puzzle_description": "Handwritten natural-language questions (50 items) that require reasoning about geometric relations, intersections, orientations, and consequences of spatial configurations in 2D and 3D (examples: tangent spheres producing triangle of tangent points; cutting a regular heptagon — counting resulting pieces).",
            "input_representation": "Pure text: each puzzle is presented as a natural-language description of the spatial scenario and a clearly specified answer format (e.g., single bolded phrase or integer). No images or explicit grid encodings are used; problems are verbal descriptions of geometric/spatial situations.",
            "prompting_method": "Single-turn prompts with temperature=0; LiveBench prompts commonly encourage zero-shot chain-of-thought / 'think step by step' reasoning and request outputs in machine-parseable form (e.g., bolded final answer, boxed LaTeX, or specific tags). Models were run with their standard chat templates (FastChat templates) and bfloat16 for open-weight inference.",
            "spatial_reasoning_analysis": "The paper treats spatial reasoning as one of three reasoning tasks (Web of Lies v2, Zebra puzzles, Spatial). The authors report qualitative difficulty (task added in first monthly update) and note that spatial correlates moderately with overall performance (Pearson r ≈ 0.7294, std.err ≈ 0.0968). No per-model per-task accuracy table for the spatial task is provided in the main text; analysis is at the category level (Reasoning) and via correlations. The authors also describe manual inspection and permissive regex-based scoring to avoid false negatives and note the task set is small (50) and handcrafted, limiting detailed statistical breakdowns in the paper.",
            "performance_metrics": "The paper does not publish a per-model accuracy for the spatial task alone in the provided text. It reports category-level 'Reasoning' scores per model (examples from tables: o1-preview Reasoning ≈ 62.9, o1-mini ≈ 59.2, other models lower) and states LiveBench top models obtain &lt;70% accuracy overall. The spatial task-specific accuracy is not given; only that it is one of the reasoning tasks contributing to the Reasoning category averages.",
            "limitations_or_failure_modes": "Task set is small (50 handcrafted items), making fine-grained statistical claims difficult; puzzles are challenging for leading LLMs. The paper highlights general failure modes for hard reasoning tasks: models often fail to follow complex deductive chains and can produce incorrect final answers even if intermediate steps appear plausible. The authors emphasize that LLM-as-judge approaches fail on similar hard problems (see ablation on LLM judges), motivating ground-truth automatic scoring. The handcrafted nature means updates require human effort (spatial task marked as 'No' for automatic updates in the paper's table).",
            "comparison_to_other_models_or_humans": "No direct human baseline for the spatial set is reported. At the category level, some models (o1 and phi series) are outliers in reasoning performance (i.e., relatively stronger than peers), and the best open-source models (meta-llama-3.1-405B and qwen2.5-72B) tie closely with top proprietary models in aggregate. The paper does not provide a per-task head-to-head comparison to human performance for the spatial task.",
            "uuid": "e3089.0",
            "source_info": {
                "paper_title": "LiveBench: A Challenging, Contamination-Limited LLM Benchmark",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "LiveBench_Zebra",
            "name_full": "LiveBench Zebra Puzzles Task (LiveBench)",
            "brief_description": "A procedurally generated set of Zebra (Einstein-style) puzzles included in LiveBench that test constraint-following and positional/spatial deduction from natural-language clues.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "40 evaluated LLMs (same pool as LiveBench; evaluated single-turn, temperature=0; examples include o1-preview, o1-mini, claude-3 variants, meta-llama-3.1 variants, etc.)",
            "model_description": "Same heterogeneous collection of transformer LLMs described across LiveBench; models vary in size and tuning, evaluated via standardized templates and settings described in the paper.",
            "model_size": "Varied; evaluated models include small (≈0.5B) to very large (≈405B) parameter models.",
            "puzzle_name": "Zebra Puzzles (Einstein-style logic puzzles)",
            "puzzle_description": "Logic puzzles where N people (typically 3 or 4 here) are arranged in a left-to-right line (positional/spatial relation) and have several categorical attributes (e.g., Food, Nationality, Hobby); a set of natural-language constraints relating positions and attributes is provided and the model must deduce the attribute(s) requested (e.g., what is the hobby of the person in a given position). These require reasoning about relative positions (left/right/immediate neighbor) and exclusivity constraints.",
            "input_representation": "Pure text: enumerated attributes, constraint sentences (often disambiguated to reduce ambiguity, e.g., 'immediate left' instead of 'left of'), and a question; answer required in a specific textual format (e.g., bold single-word). Puzzle generation uses a procedural generator with varying difficulty levels (levels 10–20) and a solver used to ensure uniqueness.",
            "prompting_method": "Single-turn textual prompts; prompts request step-by-step reasoning and specify a strict final-answer format (e.g., **X**); LiveBench encourages zero-shot chain-of-thought in prompts but enforces single-turn outputs for evaluation.",
            "spatial_reasoning_analysis": "Zebra puzzles are explicitly treated as a reasoning task that includes spatial/positional reasoning. The authors proceduralized puzzle generation (varying people count, attributes, and constraint levels) and used a solver to ensure unique solutions. Zebra puzzles correlate well with overall performance (Pearson r ≈ 0.8853, std.err ≈ 0.0935), indicating they are informative about model ability. The paper also used Zebra puzzles in an ablation showing that LLM judges (gpt-4-turbo used as judge) have high error rates when asked to judge solutions to hard reasoning puzzles, demonstrating these puzzles both challenge models and LLM-based evaluation.",
            "performance_metrics": "The paper does not list per-model accuracy for the Zebra puzzle task in the provided excerpt. It reports that zebra puzzles strongly correlate with overall score (see correlation above). The ablation on LLM-as-judge reports high error rates for LLM judging on Zebra puzzles (judge error rates in Table 8: e.g., GPT-4-Turbo judge error ≈ 0.420 on Zebra puzzles in that judging experiment), which supports that these problems are challenging both to solve and to judge automatically with other LLMs.",
            "limitations_or_failure_modes": "Larger or higher-level puzzles (more people/attributes/harder constraint levels) become 'exceedingly difficult' for top models; ambiguity in natural-language constraints can create additional difficulty (the authors modified generator to reduce ambiguity). The dataset is synthetic/procedural and can be tuned for hardness; small sample sizes and handcrafted modifications mean performance estimates per-task are limited. LLM judges are unreliable for these puzzles, so ground-truth solution checking is required.",
            "comparison_to_other_models_or_humans": "No direct human performance baselines are provided in the paper. Model-wise, reasoning-category outliers include o1 and phi series (models that do disproportionately better on reasoning tasks relative to their overall score), while some Llama/Gemini/Command-R family models are outliers in instruction following. The paper emphasizes model-to-model differences in relative strengths on reasoning tasks but does not provide a detailed per-model ranking for Zebra puzzles alone in the excerpt.",
            "uuid": "e3089.1",
            "source_info": {
                "paper_title": "LiveBench: A Challenging, Contamination-Limited LLM Benchmark",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Challenging big-bench tasks and whether chain-of-thought can solve them",
            "rating": 2,
            "sanitized_title": "challenging_bigbench_tasks_and_whether_chainofthought_can_solve_them"
        },
        {
            "paper_title": "Einstein's riddle: Riddles, paradoxes, and conundrums to stretch your mind",
            "rating": 2,
            "sanitized_title": "einsteins_riddle_riddles_paradoxes_and_conundrums_to_stretch_your_mind"
        },
        {
            "paper_title": "Missed connections: Lateral thinking puzzles for large language models",
            "rating": 2,
            "sanitized_title": "missed_connections_lateral_thinking_puzzles_for_large_language_models"
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 1,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 1,
            "sanitized_title": "large_language_models_are_zeroshot_reasoners"
        }
    ],
    "cost": 0.0178925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>18 Apr 2025</p>
<p>Colin White crwhite@meta.com 
Instruction Group Instruction Description In IFEval In LiveBench</p>
<p>Samuel Dooley dooley@meta.com 
Instruction Group Instruction Description In IFEval In LiveBench</p>
<p>Manley Roberts 
Instruction Group Instruction Description In IFEval In LiveBench</p>
<p>Arka Pal 
Instruction Group Instruction Description In IFEval In LiveBench</p>
<p>Benjamin Feuer 
Instruction Group Instruction Description In IFEval In LiveBench</p>
<p>Siddhartha Jain 
Instruction Group Instruction Description In IFEval In LiveBench</p>
<p>Ravid Shwartz-Ziv 
Instruction Group Instruction Description In IFEval In LiveBench</p>
<p>Neel Jain 
Instruction Group Instruction Description In IFEval In LiveBench</p>
<p>Khalid Saifullah 
Instruction Group Instruction Description In IFEval In LiveBench</p>
<p>Sreemanti Dey 
Instruction Group Instruction Description In IFEval In LiveBench</p>
<p>Shubh-Agrawal 
Instruction Group Instruction Description In IFEval In LiveBench</p>
<p>Sandeep Singh Sandha 
Instruction Group Instruction Description In IFEval In LiveBench</p>
<p>Siddartha Naidu 
Instruction Group Instruction Description In IFEval In LiveBench</p>
<p>Chinmay Hegde 
Instruction Group Instruction Description In IFEval In LiveBench</p>
<p>Yann Lecun 
Instruction Group Instruction Description In IFEval In LiveBench</p>
<p>Tom Goldstein 
Instruction Group Instruction Description In IFEval In LiveBench</p>
<p>Willie Neiswanger 
Instruction Group Instruction Description In IFEval In LiveBench</p>
<p>Micah Goldblum 
Instruction Group Instruction Description In IFEval In LiveBench</p>
<p>Abacus Ai 
Instruction Group Instruction Description In IFEval In LiveBench</p>
<p>Nyu 
Instruction Group Instruction Description In IFEval In LiveBench</p>
<p>Nvidia 
Instruction Group Instruction Description In IFEval In LiveBench</p>
<p>18 Apr 202599757992B5B58752556973A6F6E2A8D9arXiv:2406.19314v2[cs.CL]
Test set contamination, wherein test data from a benchmark ends up in a newer model's training set, is a well-documented obstacle for fair LLM evaluation and can quickly render benchmarks obsolete.To mitigate this, many recent benchmarks crowdsource new prompts and evaluations from human or LLM judges; however, these can introduce significant biases, and break down when scoring hard questions.In this work, we introduce a new benchmark for LLMs designed to be resistant to both test set contamination and the pitfalls of LLM judging and human crowdsourcing.We release LiveBench, the first benchmark that (1) contains frequently-updated questions from recent information sources, (2) scores answers automatically according to objective ground-truth values, and (3) contains a wide variety of challenging tasks, spanning math, coding, reasoning, language, instruction following, and data analysis.To achieve this, LiveBench contains questions that are based on recently-released math competitions, arXiv papers, news articles, and datasets, and it contains harder, contamination-limited versions of tasks from previous benchmarks such as Big-Bench Hard, AMPS, and IFEval.We evaluate many prominent closed-source models, as well as dozens of open-source models ranging from 0.5B to 405B in size.LiveBench is difficult, with top models achieving below 70% accuracy.We release all questions, code, and model answers.Questions are added and updated on a monthly basis, and we release new tasks and harder versions of tasks over time so that LiveBench can distinguish between the capabilities of LLMs as they improve in the future.We welcome community engagement and collaboration for expanding the benchmark tasks and models.</p>
<p>INTRODUCTION</p>
<p>In recent years, as large language models (LLMs) have risen in prominence, it has become increasingly clear that traditional machine learning benchmark frameworks are no longer sufficient to evaluate new models.Benchmarks are typically published on the internet, and most modern LLMs include large swaths of the internet in their training data.If the LLM has seen the questions of a benchmark during training, its performance on that benchmark will be artificially inflated (referred to as "test set contamination") (Roberts et al., 2024;Dong et al., 2024;Deng et al., 2023;Golchin &amp; Surdeanu, 2023b), hence making many LLM benchmarks unreliable.Recent evidence of test set contamination includes the observation that LLMs' performance on Codeforces plummet after the training cutoff date of the LLM (Roberts et al., 2024;Jain et al., 2024), and before the cutoff date, performance is highly correlated with the number of times the problem appears on GitHub (Roberts et al., 2024).Similarly, a recent hand-crafted variant of the established math dataset, GSM8K, shows evidence that several models have overfit to this benchmark (Zhang et al., 2024;Cobbe et al., 2021).</p>
<p>To lessen dataset contamination, benchmarks using LLM or human prompting and judging have become increasingly popular (Jain et al., 2024;Chiang et al., 2024;Zheng et al., 2024;Li et al., 2024).However, using these techniques comes with significant downsides.While LLM judges have mistral-small-2402 gemma-2-9b-it command-r-plus-08-2024 mixtral-8x22b-instruct-v0.1 claude-3-haiku-20240307 qwen2.5-7b-instruct-turbogemini-1.5-flash-8b-exp-0827gemma-2-27b-it gemini-1.5-flash-001gpt-4o-mini-2024-07-18 claude-3-5-haiku-20241022 meta-llama-3.1-70b-instruct-turbogemini-1.5-pro-001gemini-1.5-flash-exp-0827gpt-4-0125-preview qwen2.5-coder-32b-instructmistral-large-2407 gemini-1.5-flash-002claude-3-opus-20240229 gpt-4-turbo-2024-04-09 chatgpt-4o-latest-0903 dracarys2-72b-instruct gpt-4o-2024-11-20 meta-llama-3.1-405b-instruct-turbogemini-1.5-pro-exp-0827gpt-4o-2024-05-13 gemini-1.5-pro-002gpt-4o-2024-08-06 Right: a radar plot for select models across LiveBench's six categories demonstrating the that ordering of top models varies between each category.</p>
<p>multiple advantages, such as their speed and ability to evaluate open-ended questions, they are prone to making mistakes and can have several biases (Li et al., 2024).Furthermore, LLMs often favor their own answers over other LLMs, and LLMs favor more verbose answers (Li et al., 2024;Dubois et al., 2024;Li et al., 2023b).Additionally, using humans to provide evaluations of LLMs can inject biases such as formatting of the output and the tone of the writing (Chiang et al., 2024).Using humans to generate questions also presents limitations.Human participants might not ask diverse questions, may favor certain topics that do not probe a model's general capabilities, or may construct their prompts poorly (Zheng et al., 2024).</p>
<p>In this work, we introduce a framework for benchmarking LLMs designed to minimize both test set contamination and the pitfalls of LLM judging and human crowdsourcing.We use this framework to create LiveBench, the first benchmark with these three desiderata: (1) LiveBench contains frequently-updated questions based on recent information sources;</p>
<p>(2) LiveBench is scored automatically according to the objective ground truth without the use of an LLM judge; and</p>
<p>(3) LiveBench questions are drawn from a diverse set of six categories.We ensure (2) by only including questions that have an objectively correct answer.LiveBench questions are difficult: no current model achieves higher than 70% accuracy.Questions are added and updated on a monthly basis, and we release new tasks and harder versions of tasks over time so that LiveBench can distinguish among the capabilities of LLMs as they improve in the future.</p>
<p>Overview of tasks.LiveBench currently consists of 18 tasks across 6 categories: math, coding, reasoning, language, instruction following, and data analysis.Each task falls into one of two types:</p>
<p>(1) tasks which use an information source for their questions, e.g., data analysis questions based on recent Kaggle datasets, or fixing typos in recent arXiv abstracts; and (2) tasks which are more challenging or diverse versions of existing benchmark tasks, e.g., from Big-Bench Hard (Suzgun et al., 2023) or IFEval (Zhou et al., 2023a).The categories and tasks included in LiveBench are:</p>
<p>• Language Comprehension: Connections word puzzles, a typo-fixing task, and a movie synopsis unscrambling task for recent movies on IMDb and Wikipedia • Instruction Following: four tasks to paraphrase, simplify, summarize, or generate stories about recent new articles from The Guardian (Guardian Media Group, 1821), subject to one or more instructions such as word limits or incorporating specific elements in the response • Data Analysis: three tasks using recent datasets from Kaggle and Socrata, specifically, table reformatting (among JSON, JSONL, Markdown, CSV, TSV, and HTML), predicting which columns can be used to join two tables, and predicting the correct column type annotation</p>
<p>We evaluate dozens of models, including proprietary models as well as open-source models with sizes ranging from 0.5B to 8x22B.We release all questions, code, and model answers, and we welcome community engagement and collaboration.Our codebase is available at https://github.com/livebench/livebench, and our leaderboard is available at https://livebench.ai.</p>
<p>LIVEBENCH DESCRIPTION</p>
<p>In this section, we introduce LiveBench.It currently has six categories: math, coding, reasoning, data analysis, instruction following, and language comprehension.Categories are diverse with two to four tasks per problem.Each task either includes recent information sources (such as very recent news articles, movie synopses, or datasets) or is a more challenging, more diverse version of an existing benchmark task.</p>
<p>Each task is designed to have 40-100 questions which span a range of difficulty, from easy to very challenging, while loosely aiming for an overall 30-70% success rate on the top models for each task.Prompts are tailored for each category and task but typically include the following: zero-shot chain of thought (Kojima et al., 2022;Wei et al., 2022), asking the model to make its best guess if it does not know the answer, and asking the LLM to output its final answer in a way that is easy to parse, such as in XML tags or in <strong>double asterisks</strong>.We also acknowledge that parsing answers in this way requires some degree of instruction following, and we address this in Appendix A.4.In the following sections, we give a summary of each task from each category.See Appendix A.3 for additional details.</p>
<p>MATH CATEGORY</p>
<p>Evaluating the mathematical abilities of LLMs has been one of the cornerstones of recent research in LLMs, featuring prominently in many releases and reports (Reid et al., 2024;OpenAI, 2023;Bubeck et al., 2023).Our benchmark includes math questions of three types: questions modified from recent high school math competitions, fill-in-the-blank questions from recent olympiad competitions, and questions from our new, harder version of the AMPS dataset (Hendrycks et al., 2021).</p>
<p>Our first two math tasks, Competitions and Olympiad, are based on expert human-designed math problems that offer a wide variety in terms of problem type and solution technique.In Competitions, we include questions from AMC12 2023, SMC 2023, and AIME 2024 modifying the prose and the answer order; in Olympiad, we include questions based on USAMO 2024 and IMO 2024, in which the task is to rearrange masked out equations from the solution into the correct order.These questions test problem solving with algebra, combinatorics, geometry, logic, number theory, probability, and other secondary school math topics (Faires &amp; Wells, 2022).</p>
<p>Finally, we release synthetically generated math questions in the AMPS_Hard task.This task is inspired by the math question generation used to create the MATH and AMPS datasets (Hendrycks et al., 2021).We generate harder questions by drawing random primitives, using a larger and more challenging distribution than AMPS across 10 of the hardest tasks within AMPS.</p>
<p>CODING CATEGORY</p>
<p>The coding ability of LLMs is one of the most widely studied and sought-after skills for LLMs (Mnih et al., 2015;Jain et al., 2024;Li et al., 2023a).We include two coding tasks in LiveBench: a modified version of the code generation task from LiveCodeBench (LCB) (Jain et al., 2024), and a novel code completion task combining LCB problems with partial solutions collected from GitHub.</p>
<p>The LCB Generation assesses a model's ability to parse a competition coding question statement and write a correct answer.We include 78 questions from LiveCodeBench (Jain et al., 2024) which has several tasks to assess the coding capabilities of large language models.</p>
<p>The Completion task specifically focuses on the ability of models to complete a partially correct solution-assessing whether a model can parse the question, identify the function of the existing code, and determine how to complete it.We use LeetCode easy, medium, and hard problems from LiveCodeBench's (Jain et al., 2024) April 2024 release, combined with matching solutions from https://github.com/kamyu104/LeetCode-Solutions,omitting the last 15-70% of each solution and asking the LLM to complete the solution.</p>
<p>REASONING CATEGORY</p>
<p>The reasoning ability of large language models is another highly benchmarked and analyzed skill of LLMs (Wei et al., 2022;Suzgun et al., 2023;Yao et al., 2024).In LiveBench, we include three reasoning tasks: our harder version of a task from Big-Bench Hard (Suzgun et al., 2023), Zebra puzzles, and spatial reasoning questions.</p>
<p>Web of Lies v2 is an advancement of the similarly named task included in Big-Bench (bench authors, 2023) and Big-Bench Hard (Suzgun et al., 2023).The task is to evaluate the truth value of a random Boolean function expressed as a natural-language word problem.We create new, significantly harder questions by including additional deductive components and several types of red herrings.Next, we include spatial reasoning questions.This set of 50 handwritten questions tests a model's ability to make deductions about intersections and orientations of common 2D and 3D shapes.</p>
<p>Finally, we include Zebra Puzzles, a well-known reasoning task (Jeremy, 2009) that tests the ability of the model to follow a set of statements that set up constraints, and then logically deduce the requested information.We build on an existing repository for procedural generation of Zebra puzzles (quint t, 2023).Below, we provide an example question from the Zebra Puzzles task.</p>
<p>An example question from the Zebra Puzzle task.</p>
<p>There are 3 people standing in a line numbered 1 through 3 in a left to right order.Each person has a set of attributes: Food, Nationality, Hobby.The attributes have the following possible values: -Food: nectarine, garlic, cucumber -Nationality: chinese, japanese, thai -Hobby: magic-tricks, filmmaking, puzzles and exactly one person in the line has a given value for an attribute.Given the following premises about the line of people:</p>
<p>-the person that likes garlic is on the far left -the person who is thai is somewhere to the right of the person who likes magic-tricks -the person who is chinese is somewhere between the person that likes cucumber and the person who likes puzzles Answer the following question: What is the hobby of the person who is thai?Return your answer as a single word, in the following format: <strong>X</strong>, where X is the answer.</p>
<p>DATA ANALYSIS CATEGORY</p>
<p>LiveBench includes three practical tasks in which the LLM assists in data analysis or data science: column type annotation, table join prediction, and table reformatting.Each question makes use of a recent dataset from Kaggle or Socrata.</p>
<p>The first task is to predict the type of a column of a data table.To create a question for the column type annotation task (CTA), we randomly sample a table and randomly sample a column from that table.We use the actual name of that column as the ground truth and then retrieve some samples from that column.We provide the name of all the columns from that table and ask the LLM to select the true column name from those options.</p>
<p>Data analysts often also require a table to be reformatted from one type to another, e.g., from some flavor of JSON to CSV or from XML to TSV.We emulate that task in TableReformat by providing a table in one format and asking the LLM to reformat it into the target format.</p>
<p>Finally, another common application of LLMs in data analysis is performing table joins (Goldbloom, 2024;Liu et al., 2024b;Sheetrit et al., 2024).In the TableJoin task, the LLM is presented with two tables with partially overlapping sets of columns.The LLM is tasked with creating a valid join mapping from the first to the second table.</p>
<p>INSTRUCTION FOLLOWING CATEGORY</p>
<p>An important ability of an LLM is its capability to follow instructions.To this end, we include instruction-following questions in our benchmark, inspired by IFEval (Zhou et al., 2023a), which is an instruction-following evaluation for LLMs containing verifiable instructions such as "write more than 300 words" or "Finish your response with this exact phrase: {end_phrase}."While IFEval used a list of 25 verifiable instructions, we use a subset of 16 that excludes instructions that do not reflect real-world use-cases.See Appendix Table 3.Furthermore, in contrast to IFEval, which presents only the task and instructions with a simple prompt like "write a travel blog about Japan", we provide the models with an article from The Guardian (Guardian Media Group, 1821), asking the models to adhere to multiple randomly-drawn instructions while asking the model to complete one of four tasks related to the article: Paraphrase, Simplify, Story Generation, and Summarize.We score tasks purely by their adherence to the instructions.</p>
<p>LANGUAGE COMPREHENSION CATEGORY</p>
<p>Finally, we include multiple language comprehension tasks.These tasks assess the language model's ability to reason about language itself by, (1) completing word puzzles, (2) fixing misspellings while leaving other stylistic changes in place, and (3) reordering scrambled plots of unknown movies.</p>
<p>First, we include the Connections category.Connections is a word puzzle popularized by the New York Times (although similar ideas have existed previously).In this task, we present questions of varying levels of difficulty with 8, 12, and 16-word varieties.The objective of the game is to sort the words into sets of four words, such that each set has a 'connection' between them.</p>
<p>Next, we include the Typos task.The idea behind this task is inspired by the common use case for LLMs in which a user asks the LLM to identify typos and misspellings in some written text but to leave other aspects of the text unchanged.We create the questions for this task from recent ArXiv abstracts, which we ensure originally have no typos, by programmatically injecting common human typos into the text.Below is an example question from the Typos task.</p>
<p>An example question from the Typos task.</p>
<p>Please output this exact text, with no changes at all except for fixing the misspellings.Please leave all other stylistic decisions like commas and US vs British spellings as in the original text.</p>
<p>We inctroduce a Bayesian estimation approach forther passive localization of an accoustic source in shallow water using a single mobile receiver.The proposed probablistic focalization method estimates the timne-varying source location inther presense of measurement-origin uncertainty.In particular, probabilistic data assocation is performed to match tiome-differencesof-arival (TDOA) observations extracted from the acoustic signal to TDOA predicitons provded by the statistical modle.The performence of our approach is evaluated useing rela acoustic data recorded by a single mobile reciever.</p>
<p>Finally, we include the Plot Unscrambling task, which takes the plot synopses of recentlyreleased movies from IMDb or Wikipedia.We randomly shuffle the synopses sentences and then ask the LLM to simply reorder the sentences into the original plot.We find that this task is very challenging for LLMs, as it measures their abilities to reason through plausible sequences of events.</p>
<p>LIVEBENCH UPDATES AND MAINTENANCE PLAN</p>
<p>Maintaining a contamination-limited benchmark requires that we update the set of questions over time.We have so far released two updates, and we plan to continue to release updates to add new questions and remove outdated questions.In each update, we replace 1 /6 of the questions on average, so that the benchmark is fully refreshed roughly every 6 months.We may speed up the turnover rate of questions in the future, based on interest in LiveBench.Each month, we do not release the new questions until one month later, so that the public leaderboard always has 1 /6 questions that are private.We choose tasks to update based primarily on two factors: (1) the oldest tasks, and (2) the currently easiest tasks.In this way, the questions in LiveBench will stay new and continue to challenge the most capable LLMs.See additional details, as well as a longer discussion on different forms of contamination, in Appendix A.6.</p>
<p>Method for sustainability One downside in a frequently-updating benchmark is that it requires consistent work and computational resources each month.Therefore, we have a plan in place to ensure its continued success.We maintain the best (or most popular) 40-50 models on the leaderboard so as to avoid an ever-growing list of models to evaluate each month.For example, we maintain about two versions of each model family on the leaderboard (to show the improvement from the most recent version) but no more.This ensures that we have a tractable set of at most 50 models to evaluate on 200 questions each month, which is easily within the computational budgets of the authors' institutions.Additionally, we have already had community contributions which further reduces the computational burden of the authors.</p>
<p>The only other recurring work is to update the questions themselves each month.While we are excited and able to add novel tasks each month, many of the tasks are synthetic and therefore very fast and simple to create a new set of questions based on fresh data (e.g., updating the typos task using brand new arXiv papers).Additionally, we have also seen community engagement here as well.</p>
<p>Completed monthly updates In the first monthly update, we added 50 questions in a new spatial reasoning task, 28 additional coding generation questions, and 12 additional coding completion questions.The total size of the benchmark after this update became 1000.In the second monthly update, we fully updated the math olympiad questions, and we partially updated the math AMPS_Hard and math_comp questions, for 132 replaced questions, to maintain 1000 questions.</p>
<p>EXPERIMENTS</p>
<p>In this section, first we describe our experimental setup and present full results for 40 LLMs on all 18 tasks of LiveBench.Next, we give an empirical comparison of LiveBench to existing prominent LLM benchmarks, and finally, we present ablation studies.</p>
<p>Experimental setup.Our experiments include 40 LLMs total, with a mix of top proprietary models, large open-source models, and small open-source models.In particular, for proprietary models, we include OpenAI models such as o1-preview, chatgpt-4o, and gpt-4o (Brown et al., 2020;OpenAI, 2023), Anthropic models such as claude-3-5-sonnet-20240620, Google models such as gemini-1.5-pro-002(Reid et al., 2024), and Mistral models such mistral-large-2407 (Jiang et al., 2023).</p>
<p>For open-source models, we include models such as Llama-3.1-405b-instruct,Llama-3.1-70b-instruct (Dubey et al., 2024), deepseek-v2.5,(Liu et al., 2024a), qwen2.5-72b-instruct(Team, 2024b;Yang et al., 2024), command-r-plus-08-2024 (Cohere, 2024;Cohere For AI, 2024), gemma-2-27b-it (Team, 2024a;Team et al., 2024), mixtral-8x22b-instruct-v0.1 (Jiang et al., 2023), and phi-3.5-moe-instruct(Abdin et al., 2024).See Table 4 for a full list of citations.</p>
<p>For all models and tasks, we perform single-turn evaluation with temperature 0, unless otherwise noted in the model card.All models run with their respective templates from our updated version of FastChat (Zheng et al., 2024).We run all open-source models with bfloat16.When running new models, we take care to set up its hyperparameters and chat template as in the model's example code, and we also double check the outputs to make sure that the inference, as well as our automated parsing functions, are working correctly and fairly.See more details in Appendix A.4 and Appendix A.5.For each question, a model receives a score from 0 to 1.For each model, we compute the score on each task as the average of all questions, we compute the score on each of the six categories as the average of all their tasks, and we compute the final LiveBench score as the average of all six categories.In Appendix B, we give additional documentation including average input/output tokens and cost to run LiveBench for each API model.</p>
<p>DISCUSSION OF RESULTS</p>
<p>We compare all 40 models on LiveBench according to the experimental setup described above; see Table 1 and Table 2.We find that o1-preview-2024-09-12 performs the best overall, 6% better than all other models.o1-preview-2024-09-12 substantially outperforms all other models in the data analysis, language, and math categories.The next-best model is claude-3-5-sonnet-20240620, which far outperforms all other models in the coding category (although o1-mini outperforms claude-3.5 in code generation, claude-3.5 has the edge in code completion).o1-mini-2024-09-12 is third overall and is significantly better than all other models in the reasoning category.</p>
<p>The best-performing open-source models are llama-3.1-405b-instructand qwen2.5-72b-instruct,which virtually tie with each other and outperform gpt-4-turbo.</p>
<p>The best-performing small open-source model is phi-3.5-moe-instruct(see Table 2): with only 6.6B active parameters, it outperforms gpt-3.5 and is on par with mixtral-8x22b.</p>
<p>CORRELATION ANALYSES</p>
<p>Now we present analyses involving correlation among different categories and tasks.First, we compute the Pearson correlation coefficient among all pairs of categories and tasks in LiveBench (see Figure 2).We find that unsurprisingly, math, coding, and reasoning all correlate with one another.Interestingly, language correlates fairly well with data analysis, likely due to both categories including tasks that require the LLM to output a large part of the prompt that is modified in a specific way (e.g., by fixing typos or changing the table format).Surprisingly, instruction following correlates relatively weakly with all other categories.Among tasks, we see that math comp correlates the highest with the average LiveBench performance, suggesting that this task is the greatest indicator of overall model performance.This is likely due to these being high-quality, diverse mathematical reasoning questions (which we modified to reduce contamination).</p>
<p>Next, in order to see the strengths and weaknesses of each model, we create a scatterplot of each model's overall LiveBench performance vs. performance on a single category or task (Figure 3).By plotting a best fit line and computing the residuals for each model, we can compute which models are outliers in specific categories -that is, models that are disproportionately stronger in a particular category relative to the best fit line.We see that the o1 and phi series of models are outliers in terms of reasoning (Figure 3, left), while some of the Llama, gemini, and command-r models are outliers in terms of instruction following.We present additional details in Appendix A.1, including a table of each model's relative best and worst tasks (computed as the highest and lowest residuals).</p>
<p>COMPARISON TO OTHER LLM BENCHMARKS</p>
<p>Next, we compare LiveBench to two prominent benchmarks, ChatBot Arena (Chiang et al., 2024) and Arena-Hard (Li et al., 2024).In Figure 4, we show a bar plot comparison among models that are common to both benchmarks, and in Figure 6, we compare the performance of these models to a best-fit line.We also compute the correlation coefficient of model scores among the benchmarks: LiveBench has a 0.91 and 0.88 correlation with ChatBot Arena and Arena-Hard, respectively.</p>
<p>Based on the plots and the correlation coefficients, we see that there are generally similar trends to LiveBench, yet some models are noticeably stronger on one benchmark vs. the other.For example, gpt-4-0125-preview and gpt-4-turbo-2024-04-09 perform substantially better on Arena-Hard compared to LiveBench, likely due to the known bias from using gpt-4 itself as the LLM judge (Li et al., 2024).We hypothesize that the strong performance of some models such as the gemini-1.5models on ChatBot Arena compared to LiveBench may be due to having an output style that is preferred by humans.These observations emphasize the benefit of using ground-truth judging, which is immune to biases based on the style of the output.</p>
<p>Comparison between Ground-Truth and LLM-Judging As an additional comparison between</p>
<p>LiveBench and LLM judge based benchmarks, we give a preliminary study in the Appendix on the efficacy of LLM judging for hard math and reasoning questions.Specifically, we run an initial We compare LiveBench to ChatBot Arena (left) and Arena-Hard (right).We see that while there are generally similar trends, some models are noticeably stronger on one benchmark vs. the other.For example, both GPT-4 models are substantially better on Arena-Hard.
g p t- 4 o -2 0 2 4 -0 5 -1 3 g p t- 4 -t u rb o -2 0 2 4 -0 4 -0 9 g p t- 4 -1 1 0 6 -p re v ie w cl a u d e -3 -o p u s- 2 0 2 4 0 2 2 9 g p t- 4 -0 1 2 5 -p re v ie w g e m in i- 1 .5 -p ro -a p i- 0 5 1 4 g e m in i- 1 .5 -f la sh -a p i- 0 5 1 4 cl a u d e -3 -s o n n e t- 2 0 2 4 0 2 2 9 m e ta -l la m a -3 -7 0 b -i n st ru ct cl a u d e -3 -h a ik u -2 0 2 4 0 3 0 7 g p t- 3 .5 -t u rb o -0 1 2 5 g p t-3 .5 -t u rb o -1 1 0 6 m e ta -l la m a -3 -8 b -i n st ru ct o p e n h e rm e s-2 .5 -m is tr a l-7 b m is tr a l-7 b -i n st ru ct -v 0 .2 p h i-3 -m in i-4 k -i n st ru ct ze p h y r-7 b -a lp h a p h i-3 -m in i-1 2 8 k -i n st
experiment regarding the question, 'if an LLM struggles to answer a hard math or reasoning question, then will the LLM also struggle to determine whether or not a given answer to that question is correct?' Our experiments give evidence that the answer is yes, for zebra puzzles and AMC/AIME questions, but the results are not definitive.See Appendix A.2.</p>
<p>ANALYSIS OF MONTHLY UPDATES</p>
<p>As described in Section 2.7, we have completed two monthly updates for LiveBench so far.The rank correlation between the original and first update, and the first and second update, are both &gt; 0.997, showing that the model rankings have stayed consistent.On the other hand, between the original and the most-recent set of questions, the median and mean average scores (among models included in all iterations of the leaderboard) have both dropped by about 1.2%, showing that the benchmark is becoming harder over time, as newly released models become more capable.</p>
<p>RELATED WORK</p>
<p>We describe the most prominent LLM benchmarks and the ones that are most related to our work.For a comprehensive survey, see (Chang et al., 2024).The Huggingface Open LLM Leaderboard (Gao et al., 2021;Beeching et al., 2023) is a widely-used benchmark suite that consists of static datasets such as Big Bench Hard (Suzgun et al., 2023) and MMLU-Pro (Wang et al., 2024).While this has been incredibly useful in tracking the performance of LLMs, its static nature leaves it prone to test set contamination by models.</p>
<p>LLMs-as-a-judge.AlpacaEval (Li et al., 2023b;Dubois et al., 2023;2024), MT-Bench (Chiang et al., 2024), and Arena-Hard (Li et al., 2024) are benchmarks that employ LLM judges on a fixed set of questions.Using an LLM-as-a-judge is fast, relatively cheap, and has the flexibility of being able to evaluate open-ended questions, instruction-following questions, and chatbots.However, LLM judging also has downsides.First, LLMs have biases towards their own answers (Li et al., 2024).In addition, GPT-4 judges have a noticeable difference in terms of variance and favorability of other models compared to Claude judges.Additionally, LLMs make errors.As one example, question 2 in Arena-Hard asks a model to write a C++ program, yet GPT-4 incorrectly judges gpt-4-0314's solution as incorrect (Li et al., 2024).</p>
<p>Humans-as-a-judge.ChatBot Arena (Chiang et al., 2024;Zheng et al., 2024) leverages human prompting and feedback.Users ask questions and receive outputs of two randomly selected models and pick which output they prefer.This preference feedback is aggregated into an Elo score for each model.While human evaluation is great for capturing the preferences of a crowd, using a human-as-a-judge has disadvantages.First, human-judging can be labor-intensive, especially for certain tasks included in LiveBench such as complex math, coding, or long-context reasoning problems.Whenever humans are involved in annotation (of which judging is a sub-case), design choices or factors can cause high error rates (Lease, 2011), and even in well-designed human-annotation setups, high variability from human to human leads to unpredictable outcomes (Rashkin et al., 2023).</p>
<p>Other benchmarks.LiveCodeBench (Jain et al., 2024) also regularly releases new questions and makes use of ground-truth judging.However, it is limited to only coding tasks.The extensive Omni-MATH benchmark Gao &amp; Liu (2024) encompasses numerous math competitions, although using LLM-as-a-judge grading potentially contributes to a degree of contamination or bias in some of the benchmark's scores; our completely objective correctness-based scoring avoids this concern.The SEAL Benchmark (Scale AI, 2024), uses private questions with expert human scorers, however, the benchmark currently only contains the following categories: Math, Coding, Instruction Following, and Spanish.In Srivastava et al. (2024), the authors modify the original MATH dataset (Hendrycks et al., 2021) by changing numbers in the problem setup.They find declines in model performance for some LLMs, including frontier ones.However, while such work can evaluate LLMs on data that is not in the pretraining set, the data still ends up being highly similar to the kind of data likely seen in the pretraining set.In addition, the hardness of the benchmark remains the same over time.</p>
<p>Finally, we discuss benchmarks that were the basis for tasks in LiveBench.In IFEval (Zhou et al., 2023b), the authors assess how good LLMs are at following instructions by adding one or more constraints in the instruction as to what the output should be.They limit the set of constraints to those in which it can provably be verified that the generation followed the constraints.In Big-Bench (Srivastava et al., 2022), a large number of tasks are aggregated into a single benchmark with the aim of being as comprehensive as possible.Big-Bench-Hard (Suzgun et al., 2022) investigates a subset of Big-Bench tasks that were particularly challenging for contemporaneous models as well as more complex prompting strategies for solving them.</p>
<p>CONCLUSIONS, LIMITATIONS, AND FUTURE WORK</p>
<p>In this work, we introduced LiveBench, an LLM benchmark designed to mitigate both test set contamination and the pitfalls of LLM judging and human crowdsourcing.LiveBench is the first benchmark that (1) contains frequently updated questions from new information sources, in which questions become harder over time, (2) scores answers automatically according to objective groundtruth values, without the use of LLM judges, and (3) contains a wide variety of challenging tasks, spanning math, coding, reasoning, language, instruction following, and data analysis.LiveBench contains questions that are based on recently released math competitions, arXiv papers, and datasets, and it contains harder, contamination-limited versions of previously released benchmarks.We released all questions, code, and model answers, and questions are added and updated on a monthly basis.We welcome community collaboration for expanding the benchmark tasks and models.</p>
<p>Limitations and Future Work.While we attempted to make LiveBench as diverse as possible, there are still additions from which it would benefit.For example, we hope to add non-English language tasks in the future.Furthermore, while ground truth scoring is beneficial in many ways, it still cannot be used for certain use cases, such as 'write a travel guide to Hawaii' in which it is hard to define a ground truth.Finally, while we attempted to make all tasks and categories fair for all models, there are still biases due to certain LLM families favoring certain prompt types.We plan to update the prompts (at the start and end of each question) in the future, as new prompt strategies are developed.Similarly, we plan to continue updating the LiveBench leaderboard as new LLMs are released.</p>
<p>REPRODUCIBILITY STATEMENT</p>
<p>Our work is fully reproducible: we open-source the leaderboard, all questions, all code to run API and open-source models, all model outputs for 40 models, and all code to score the models.In other words, every part of the project is available publicly: https://livebench.ai/.The only exception is that as the benchmark becomes more popular, we withhold releasing the new set of questions each month, so that there are always some questions that are private.These questions are then made public one month later.The readme in the above link gives instructions to download all parts of the project and to score new models.</p>
<p>ETHICS STATEMENT</p>
<p>Our paper introduces a new benchmark for LLMs, which contains frequently-updated questions from new information sources, scores answers according to objective ground-truth values, and contains a wide variety of tasks.We do not see any inherently negative broader societal impacts of our work.</p>
<p>Our hope is that our work will have a positive impact for both practitioners and researchers: by providing a new benchmark with frequently-updated questions, our work has the potential to both accelerate future research and enable more comprehensive and rigorous evaluations of existing and future models.Furthermore, we hope that the general framework of our benchmark -frequentlyupdated questions with new information sources -will catch on, mitigating the negative effects of contamination in future LLM evaluation and making LLM benchmarks more 'future-proof'.Figure 6: The performance of models on different benchmarks, compared to a best-fit line.We compare the different in relative performance of LLMs on LiveBench vs. ChatBot Arena, and LiveBench vs. Arena-Hard.We see that while many models are near the best-fit lines, a few are notable outliers, providing evidence that their output style may be noticeably better or worse than their ability to answer questions.</p>
<p>A ADDITIONAL DETAILS ABOUT LIVEBENCH EXPERIMENTS</p>
<p>In this section, we detail further descriptions about the LiveBench benchmark itself and our experiments.</p>
<p>We include further depictions of the comparisons of LiveBench to ChatBot Arena and Arena-Hard in Figure 6.We display the full results table for LiveBench in Table 2.We display the full bar plot for LiveBench in Figure 5.</p>
<p>We display the list of all verifiable instructions in Table 3.</p>
<p>We display a table with the citations for all models in Table 4. Here, we provide more details from Section 3.2.First, we present the Pearson correlation coefficient and std.error for each category (Table 5) and task (Table 6) compared to the overall average LiveBench score, computed using data from all 40 models.This is a supplement to Figure 2. In Table 7, we compute the relative best and worst task for each model, specifically, the tasks with the highest and lowest residuals of the best fit line vs. overall LiveBench performance.In other words, we compute the task that each model most outperforms and underperforms on, relative to a theoretical model with the same overall performance but has balanced performance across each task.</p>
<p>A.2 DETAILS FROM ABLATION STUDIES</p>
<p>In this section, we ive the details for the ablation study described in Section 3.3.For hard math and reasoning questions, if an LLM struggles to answer the question, then will it also struggle to determine whether or not a given answer to that question is correct?There are some classes of problems for which the answer is surely 'no': any problems that are hard to solve by frontier LLMs, yet easy to check whether an answer is correct or not, such as NP-Hard problems.Another exception is that if an LLM judge is given access to the ground truth, then it will (of course) be able to judge whether or not answers are correct.The tasks in our original experiments (AMC, AIME, and Zebra puzzles) may not fit the class of exceptions.By way of a preliminary study of the above suggestion (that LLM judges cannot judge Zebra puzzles and AMC/AIME questions that they cannot solve), we run an experimental test.We use a judge prompt based on the MT-Bench judge prompt, which is duplicated below.We use gpt-4-turbo-2024-04-09 as the judge.We judge the model outputs of both gpt-4-turbo-2024-04-09 and claude-3-opus-20240229.</p>
<p>See Table 8 and Table 9.We find that the error rate for all tasks is far above a reasonable value, indicating that LLM judges are not appropriate for challenging math and logic tasks.However, we note that there may be other experimental setups which could change the result, such as using a more detailed prompt that is tailored to the task of judging hard math and reasoning problems.</p>
<p>A.3 DETAILED DESCRIPTION OF LIVEBENCH CATEGORIES</p>
<p>In this section, we describe the categories and tasks of LiveBench and the grading methods in more detail.</p>
<p>A.3.1 MATH CATEGORY</p>
<p>Evaluating the mathematical abilities of LLMs has been one of the cornerstones of recent research in LLMs, featuring prominently in many releases and reports (Reid et al., 2024;OpenAI, 2023;Brown et al., 2020;Bubeck et al., 2023).Our benchmark includes math questions of three types: modified questions from recent high school math competitions, fill-in-the-blank questions from recent proof-based USAMO and IMO problems, and questions from our new, harder version of the AMPS dataset (Hendrycks et al., 2021).</p>
<p>Math competitions.Our first math category is based on expert human-designed math problems that offer a wider variety in terms of problem type and solution technique.We focus on high school math competition questions from English-speaking countries: AMC12, AIME, SMC, and USAMO, and also IMO, the international competition.</p>
<p>First, we include questions based on the American Mathematics Competition 12 (AMC12), both AMC12A and AMC12B 2023, released on November 8, 2023 and November 14, 2023, respectively, and the Senior Mathematical Challenge (SMC) 2023, released on October 3, 2023.All three are challenging multiple-choice competitions for high school students in the USA (AMC) and UK (SMC) that build in difficulty, meant as the first step for high school students to qualify for their country's team for the International Mathematical Olympiad (IMO).</p>
<p>The questions test mathematical problem solving with arithmetic, algebra, counting, geometry, number theory, probability, and other secondary school math topics (Faires &amp; Wells, 2022).We modify the questions by updating the prose of the questions that do not affect the answer, by rearranging the order of the multiple choice answers when applicable, and by asking for a different output format than the widely-used source website (https://artofproblemsolving.com/).An example of a problem of this type from the AMC12A 2023 problem set is below:</p>
<p>An example question from the Math Competitions task.How many complex numbers satisfy the equation z 5 = z, where z is the conjugate of the complex number z? (A) 2 (B) 3 (C) 5 (D) 6 (E) 7 If you cannot determine the correct multiple-choice answer, take your best guess.Once you have your answer, please duplicate that letter five times in a single string.For example, if the answer is F, then write FFFFF.</p>
<p>Ground Truth: EEEEE Next, we include the American Invitational Mathematics Examination (AIME), both AIME I and AIME II 2024, released on January 31, 2024 and February 7, 2024, respectively.These are prestigious and challenging tests given to those who rank in the top 5% of the AMC.Each question's answer is an integer from 000 to 999.An example of a problem of this type from the AIME I 2024 problem set is below:</p>
<p>An example question from the Math Competitions task.Real numbers x and y with x, y &gt; 1 satisfy log x (y x ) = log y (x 4y ) = 10.What is the value of xy? Please think step by step, and then display the answer at the very end of your response.The answer is an integer consisting of exactly 3 digits (including leading zeros), ranging from 000 to 999, inclusive.For example, the answer might be 068 or 972.If you cannot determine the correct answer, take your best guess.Remember to have the three digits as the last part of the response.</p>
<p>Ground Truth: 025</p>
<p>Proof-based questions.We consider the USA Math Olympiad (USAMO) 2024 and International Math Olympiad (IMO) 2024 competitions, released on March 20, 2024 and July 22, 2024, respectively.These contests are primarily proof-based and non-trivial to evaluate in an automated way.One possibility is to use LLMs to evaluate the correctness of the natural language proof.However, we then have no formal guarantees on the correctness of the evaluation.Another possibility is to auto-formalize the proofs into a formal language such as Lean and then run a proof checker.However, while there have been notable recent improvements in auto-formalization, such a process still does not have formal guarantees on the correctness of the auto-formalization -and thus that of the evaluation.To tackle this, we formulate a novel task which can test the ability of an LLM in the context of proofs.Specifically, for a proof, we mask out a subset of the formulae in the proof.We then present the masked out formulae in a scrambled order to the LLM and ask it to reinsert the formulae in the correct positions.Such a task tests the mathematical, deductive, and instruction following abilities of the LLM.In particular, if the LLM is strong enough to generate the correct proof for a question, then one would expect it to also solve the far easier task of completing a proof which has some missing formulae -especially if the formulae are already given to it in a scrambled order.Note that this also allows us to easily control the level of difficulty of the question by changing the number of formulae that we mask.</p>
<p>We generate 3 hardness variants for each problem, masking out 10%, 50% and 80% of the equations in the proof.We evaluate by computing the edit distance between the ground truth ranking order and the model predicted ranking order.[NB : in preliminary testing we also evaluated using the accuracy metric and the model rankings remained nearly the same].Models perform worse on IMO compared to USAMO, in line with expectations.We also looked at the performance as separated by question hardness.The scores are greatly affected by question hardness going from as high as 96.8 for the easiest questions (10% masked out, GPT-4o) to as low as 36 for the hardest (80% masked out).The full results are in Table 10 and Table 11. 1 Synthetically generated math questions.Finally, we release synthetic generated math questions.This technique is inspired from math question generation used to create the MATH and AMPS datasets (Hendrycks et al., 2021).In particular, we randomly generate a math problem of one of several types, such as taking the derivative or integral of a function, completing the square, or factoring a polynomial.We generate questions by drawing random primitives, using a larger (and therefore more challenging) distribution than AMPS.Note that, for problem types such as integration, this simple technique of drawing a random function and taking its derivative results in a wide variety of integration problems of varying difficulty.For example, problem solutions may involve applying the chain rule, the product/quotient rule, trigonometric identities, or use a change of variables.In order to extract the answer, we ask the model to use the same 'latex boxed answer' technique as in the MATH dataset (Hendrycks et al., 2021).We judge the correctness of answers as in the EleutherAI Eval Harness (Gao et al., 2021) using Sympy (Meurer et al., 2017) where we check for semantic as well as numerical equivalence of mathematical expressions.An example of a integral problem is as follows:</p>
<p>is in the true 'web of lies', while Kayla may lead to a series of steps ending in a dead end.Overall, the number of total red herring sentences is drawn from a uniform distribution ranging from 0 to 19.For (3), we simply assign each name to a location and give sentences of the form 'Devika is at the museum.The person at the museum says the person at the ice skating rink lies.'We find that this makes the task significantly harder for leading LLMs, even without shuffling the sentences into a random order.</p>
<p>An example question from the Web of Lies v2 task.</p>
<p>In this question, assume each person either always tells the truth or always lies.Tala is at the movie theater.The person at the restaurant says the person at the aquarium lies.Ayaan is at the aquarium.Ryan is at the botanical garden.The person at the park says the person at the art gallery lies.The person at the museum tells the truth.Zara is at the museum.Jake is at the art gallery.The person at the art gallery says the person at the theater lies.Beatriz is at the park.The person at the movie theater says the person at the train station lies.Nadia is at the campground.The person at the campground says the person at the art gallery tells the truth.The person at the theater lies.The person at the amusement park says the person at the aquarium tells the truth.Grace is at the restaurant.The person at the aquarium thinks their friend is lying.Nia is at the theater.Kehinde is at the train station.The person at the theater thinks their friend is lying.The person at the botanical garden says the person at the train station tells the truth.The person at the aquarium says the person at the campground tells the truth.The person at the aquarium saw a firetruck.The person at the train station says the person at the amusement park lies.Mateo is at the amusement park.Does the person at the train station tell the truth?Does the person at the amusement park tell the truth?Does the person at the aquarium tell the truth?Think step by step, and then put your answer in <strong>bold</strong> as a list of three words, yes or no (for example, <strong>yes, no, yes</strong>).If you don't know, guess.</p>
<p>Ground Truth: no, yes, yes Zebra puzzles.The second reasoning task we include is Zebra puzzles.Zebra puzzles, also called Einstein's riddles or Einstein's puzzles, are a well-known (Jeremy, 2009) reasoning task that tests the ability of the model to follow a set of statements that set up constraints, and then logically deduce the requested information.The following is an example with three people and three attributes:</p>
<p>An example question from the Zebra Puzzle task.</p>
<p>There are 3 people standing in a line numbered 1 through 3 in a left to right order.Each person has a set of attributes: Food, Nationality, Hobby.The attributes have the following possible values:</p>
<p>• Food: nectarine, garlic, cucumber</p>
<p>• Nationality: chinese, japanese, thai</p>
<p>• Hobby: magic-tricks, filmmaking, puzzles and exactly one person in the line has a given value for an attribute.Given the following premises about the line of people:</p>
<p>• the person that likes garlic is on the far left • the person who is thai is somewhere to the right of the person who likes magic-tricks</p>
<p>• the person who is chinese is somewhere between the person that likes cucumber and the person who likes puzzles Answer the following question: What is the hobby of the person who is thai?Return your answer as a single word, in the following format: <strong><em>X</em></strong>, where X is the answer.</p>
<p>Ground Truth: filmmaking</p>
<p>We build on an existing repository for procedural generation of Zebra puzzles (quint t, 2023); the repository allows for randomizing the number of people, the number of attributes, and the set of constraint statements provided.For the attribute randomization, they are drawn from a set of 10 possible categories (such as Nationality, Food, Transport, Sport) and for each of these categories there are between 15 and 40 possible values to be taken.For the constraint statements, the implementation allows for up to 20 'levels' of constraint in ascending order of intended difficulty.For example, level 1 could include a statement such as 'The person who likes garlic is on the left of the person who plays badminton' and a level 10 statement could be 'The person that watches zombie movies likes apples or the person that watches zombie movies likes drawing, but not both'.Higher levels also include lower level statements in their possible set of statements to draw from, but this set narrows progressively as the level increases from 12 to 20 by removing the possibility of having lower-level statements (starting with removing level 1, then removing level 2, etc).</p>
<p>The repository also includes a solver for the puzzles, which we use to ensure there is a (unique) solution to all of our generated puzzles.</p>
<p>Our modifications to the original repository primarily target the reduction of ambiguity in the statements (e.g.changing 'X is to the left of Y' to 'X is to the immediate left of Y').For generation, we pick either 3 or 4 people with 50% probability, either 3 or 4 attributes with 50% probability, and we draw the levels from the integer interval [10, 20] with uniform probability.In preliminary testing, we found that larger puzzles proved exceedingly difficult for even the top performing LLMs.</p>
<p>Spatial reasoning.The final reasoning task is spatial reasoning questions.This set of 50 handwritten questions tests a model's ability to make deductions about intersections and orientations of common 2D and 3D shapes.Two example questions are below.</p>
<p>Example question one from the Spatial Reasoning task.Suppose I have three spheres of radius 3 resting on a plane.Each sphere is tangent to the other two spheres.If I consider a new shape whose vertices are equal to the set of tangent points of the pairs of spheres, what is the new shape?Is it a square, tetrahedron, triangle, circle, line segment, or rhombus?Think step by step, and then put your answer in <strong>bold</strong> as a single phrase (for example, <strong>circle</strong>).If you don't know, guess.</p>
<p>Ground Truth: triangle</p>
<p>Example question two from the Spatial Reasoning task.Suppose I have a regular heptagon, and I can make four straight cuts.Each cut cannot pass through any of the vertices of the heptagon.Also, exactly two of the cuts must be parallel.What is the maximum number of resulting pieces?Think step by step, and then put your answer in <strong>bold</strong> as a single integer (for example, <strong>0</strong>).If you don't know, guess.</p>
<p>Ground Truth: 10 A.3.4 DATA ANALYSIS LiveBench includes three practical tasks in which the LLM assists in data analysis or data science: column type annotation, table join prediction, and table reformatting.Each question makes use of a recent dataset from Kaggle or Socrata.</p>
<p>Owing to the limited output context lengths of the current generation of LLMs and the comparatively high per-token costs of generating responses, we upper bound the size of our tables with respect to cell length, column count and row count.Even with these limitations, we find that our tasks remain sufficiently challenging even for the current state-of-the-art models.</p>
<p>Example questions from the Data Analysis category can be lengthy, so examples can be viewed here.</p>
<p>Column type annotation.Consider a table A with t columns and r rows.We denote each column C ∈ A as a function which maps row indices to strings; i.e., for 0 ≤ i &lt; t, we have C i : N → Σ * , where i is the column index.Let L ⊆ Σ * denote a label set; these are our column types to be annotated.Standard CTA assumes a fixed cardinality for this label set, indexed by a variable we call j.Given the above definitions, we define single-label CT A ⊂ A × L as a relation between tables and labels: Scoring.To evaluate the model's performance on instruction following, we use a scoring method that considers two key factors: whether all instructions were correctly followed for a given prompt, i.e.Prompt-level accuracy, and what fraction of the individual instructions were properly handled, i.e.Instruction-level accuracy.The first component of the score checks if the model successfully followed every instruction in the prompt and assigns 1 or 0 if it missed any of the instructions.The second component looks at each individual instruction and checks whether it was properly followed or not.The final score is the average of these two components, scaled to lie between 0 and 1.A score of 1 represents perfect adherence to all instructions, while lower scores indicate varying degrees of failure in following the given instructions accurately.
∀C, ∃l j | (C i , l j ) ∈ CT A (1)
Example questions from the Instruction Following category can be lengthy, so examples can be viewed here.</p>
<p>A.3.6 LANGUAGE COMPREHENSION</p>
<p>Finally, we include multiple language comprehension tasks.These tasks assess the language model's ability to reason about language itself by, (1) completing word puzzles, (2) fixing misspellings while leaving other stylistic changes in place, and (3) reordering scrambled plots of unknown movies.</p>
<p>Connections.First we include the 'Connections' category3 .Connections is a word puzzle category introduced by the New York Times (although similar ideas have existed previously).Sixteen words are provided in a random order; the objective of the game is to sort these into four sets of four words, such that each set has a 'connection' between them.Such connections could include the words belonging to a related category, e.g., 'kiwi, grape, pear, peach' (types of fruits); the words being anagrams, the words being homophones, or being words that finish a certain context, e.g., 'ant, drill, island, opal' being words that come after the word 'fire' to make a phrase.Due to the variety of possible connection types that can exist, the wider knowledge required to understand some connections, as well as some words potentially being 'red herrings' for connections, this task is challenging for LLMs -prior work (Todd et al., 2024) has comprehensively tested the task on the GPT family of models, as well as on sentence embedding models derived from, e.g., BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019).The authors found that GPT-4 has an overall completion rate below 40% on the puzzles (when allowed multiple tries to get it correct), concluding that 'large language models in the GPT family are able to solve these puzzles with moderate reliability, indicating that the task is possible but remains a formidable challenge.'In our work, we assess the single-turn performance and test performance on a much larger set of models.</p>
<p>The original task provided for a number of 'retry' attempts in the event of an incorrect submission for a category.To fit into the framework of our benchmark we take the model's answer from a single turn; to ameliorate the increased difficulty of this setting, we use fewer words/groups for some questions.</p>
<p>The split we use is 15 questions of eight words, 15 questions of twelve words and 20 questions of sixteen words.An example prompt is as follows: The score for this task is the fraction of groups that the model outputs correctly.</p>
<p>Typo corrections.Next, we include details about the Typos task.The idea behind this task is inspired by the common use-case for LLMs where a user will ask the system to identify typos and misspellings in some written text.The challenge for the systems is to fix just the typos or misspellings, but to leave other aspects of the text unchanged.It is common for the LLM to impose its own writing style onto that of the input text, such as switching from British to US spellings or adding the serial comma, which may not be desirable.</p>
<p>To create the questions for this task, we take text from recent ArXiv abstracts.These abstracts may themselves start with misspellings and grammatical errors.Therefore, our first step is to manually pass over the abstracts and fix typos and grammar issues.Next, we assemble a list of common misspellings as found online.This is done so as to replicate common misspellings performed by humans, even though we synthetically generate the questions.Finally, for each question, we sample a probability p ∼ U (0.5, 0.7) of flipping correctly spelled words to misspelled words.We then use that probability to replace every correctly spelled word with a common misspelling with that probability p.This allows there to be variability in the difficulty of the problem included in the benchmark.In our first release, we include 50 questions.Finally, to score this problem, we merely ask whether the ground truth abstract is contained in the output provided by the LLM.</p>
<p>An example question from the Typos task.</p>
<p>Please output this exact text, with no changes at all except for fixing the misspellings.Please leave all other stylistic decisions like commas and US vs British spellings as in the original text.We introducehten consept of a k-token signed graph adn studdy some of its combinatorial and algebraical properties.We prove that twpo switching isomorphic signed graphs ahve switching isomorphic token graphs.Moreover, we sohw tyhat the Laplacian spectum of a balanced signed graph is contained in the Laplacian spectra of its k-token signed graph.Besides, we introduce and studdyther unbalance levle of a signed graph, which is a new parameter tyhat measures how far a signed graph is frome beng balanced.Moreover, we study the relation bewteen the frustration index anbdther unballance level of signed graphs adn their token signed graphs.</p>
<p>Ground Truth: We introduce the concept of a k-token signed graph and study some of its combinatorial and algebraic properties.We prove that two switching isomorphic signed graphs have switching isomorphic token graphs.Moreover, we show that the Laplacian spectrum of a balanced signed graph is contained in the Laplacian spectra of its k-token signed graph.Besides, we introduce and study the unbalance level of a signed graph, which is a new parameter that measures how far a signed graph is from being balanced.Moreover, we study the relation between the frustration index and the unbalance level of signed graphs and their token signed graphs.</p>
<p>Plot unscrambling.Finally, we include a movie synopsis unscrambling task.We obtain movie plot synopses from IMDb or Wikipedia for feature-length films released after January 1st 2024.These synopses are then split into their constituent sentences and are randomly shuffled.The lengths of the synopses vary from as few as 7 sentences to as many as 66 sentences; at the upper end, this is a very challenging task.The LLM is provided the shuffled sentences with the prompt: 'The following plot Scoring the task involves two decision points: 1) how to deal with transcription errors -those in which the model modifies the lines when producing its output 2) given the ground truth ordering of sentences and the LLM's ordering, choosing an appropriate scoring metric.For 1), one option is to permit only strict matching -that is, the LLM must transcribe perfectly.However, although the strongest models do perform well on this (we find they achieve over 95% transcription accuracy), we find that LLMs often correct grammatical errors or spelling mistakes in the source data when transcribing.As we are primarily interested in testing the models' capabilities for causal language reasoning in this task, rather than precise transcription accuracy, we instead apply a fuzzy-match using difflib (Team, 2008) to determine the closest match using a version of the Ratcliff/Obershelp algorithm (Ratcliff &amp; Metzener, 1988).For 2), we calculate the score as 1 − d n_sentences_gt , where n_sentences_gt is the number of sentences in the ground truth synopsis, and d is the Levenshtein distance (Levenshtein, 1966) of the model's sentence ordering to the ground truth synopsis ordering.Thus if the model's sentence ordering perfectly matches the ground truth, the distance d would be 0, and the score would be 1 for that sample.</p>
<p>One might think that it is plausible that synopsis unscrambling cannot always be solved with the information provided.However, note that even if the set of sentences do not create a distinct causal ordering, the task is essentially asking the LLM to maximize the probability that a given arrangement of sentences is a real movie.In addition to causal reasoning, the LLM can use subtle cues to reason about what ordering creates the most compelling plot.Furthermore, even if there does exist an upper bound on the score that can be achieved that is strictly below 100%, it can still be a useful metric for distinguishing models' relative strengths.An analogous metric is that of next-token perplexity in language modelling; although it is likely that a perfect prediction of the next token is impossible to achieve, and we do not even know what the obtainable lower bound on perplexity is, it is still a powerful metric for determining language-modelling performance.</p>
<p>Example questions from the plot unscrambling task can be lengthy, so examples can be viewed in our repo here.</p>
<p>In Table 13, we also present a summary table consisting of the number of questions and source data for each task in LiveBench.</p>
<p>A.4 GRADING METHODOLOGY</p>
<p>LiveBench makes use of automatic regex-based grading methods in order to avoid the biases and other downsides of LLM judging, as detailed in Section 1.On the other hand, automated grading methods have two main pitfalls, which we take care to circumvent.The first is that an instruction-following element is added to the task.For example, a reasoning task with a complex answer instruction format tests instruction-following as well as reasoning, rather than pure reasoning.The second is that care must be made to allow for different answer formats so as not to favor a particular LLM (even after giving exact, unambiguous instructions in the prompt, regarding the format of the answer).</p>
<p>When creating questions, we make sure that the prompt is written in such a way that the answer format is fully specified and unambiguous.We typically give an example of the answer format in the prompt, and specify any potential parts that are unclear.</p>
<p>Additionally, our grading methodology is designed to be fully permissive, accommodating all answer formats within reason.As a rule of thumb, if a human reading the LLM's answer would be able to comprehend the answer (and assuming it is correct), then the answer should be marked correct.For example, in our math category, models can respond with answers in either latex 'boxed{}' or 'fbox{} ' environments.For multiple choice questions, we ask for the letter answer, but we accept either the letter or the raw answer.The reason for our less-strict judging is because some models may have been instruction-tuned to a particular format, giving them an advantage or disadvantage.We achieved this by using flexible regex patterns that capture different notation styles or response structures.This approach allows us to evaluate models based on their task-specific skills, rather than their ability to follow specific instructions.</p>
<p>In order to ensure a given task is not overly testing instruction following (in addition to its actual category), and also to ensure scoring accuracy and fairness, we regularly manually inspect a random sample of responses which were labeled incorrect for each model.We look for common answer formats that are incorrect and not picked up by the regex-based parser.This allows us to regularly improve our automatic scoring functions to admit as many answer formats as possible, within reason.</p>
<p>We continue to monitor model outputs and update scoring functions as needed to capture answers appropriately.When adding new models or tasks to the benchmark, we re-evaluate and refine scoring methodologies to ensure fairness and accuracy.This ongoing evaluation and maintenance process enables us to maintain the integrity of our benchmark, providing a comprehensive assessment of models' strengths and weaknesses.By decoupling instruction following capabilities from task-specific evaluations, we can accurately evaluate and compare models across various tasks and categories.</p>
<p>A.5 QUALITY CONTROL FOR NEW MODELS When integrating new models into LiveBench, we conduct thorough quality control to ensure consistent and accurate evaluation.This process guarantees that our benchmark remains reliable and effective in assessing model capabilities.</p>
<p>New Model Setup</p>
<p>We perform all setup in order to accurately run inference on the model.New models come with a README or example code on the chat template, system message, and hyperparameters to run the model, and so we match this setup in LiveBench's code.For API models from an existing model family, there is typically very little additional setup.For an API model from a new family, such as the O1 series, we add the same hyperparameters, system message, etc., as its example code.Similarly, for open-weight models, we make sure to match the published example code.Note that we generally prefer to use (trusted) APIs, even for open-weight models, because APIs are more controlled and fewer things can go wrong.As an example, new models occasionally have bugs when they are first released.</p>
<p>Model Output Evaluation Next, we evaluate the new model on LiveBench and compare the new model's scores to other models of similar average score, or models from the same family or base.We look for tasks that scored unusually low, relative to similar models.This is not a comprehensive test, but it helps to more quickly flag tasks that have an unexpectedly low result, to debug potential errors in the parsing code.The first definition is the one that we use in Section 1: when the test data (the questions and answers from the benchmark) are present in the training data itself.This is the definition that is commonly used in the literature, often called 'data contamination', 'data leakage', 'test set contamination', or even simply 'contamination' (Oren et al., 2023;Singh et al., 2024;Kalal et al., 2024;Golchin &amp; Surdeanu, 2023a).</p>
<p>The second definition is related to the question: how well do LLMs generalize today?A low level of generalization is, e.g., training on AMC questions, and generalizing the same questions where the order of the answer choices, and other prose in the questions, are changed, while a high level of generalization is, e.g., training on AMC 12A 2023 and testing on AMC 12A 2024.The latter example, despite LLMs already exhibiting some degree of this type of high generalization, is still 'fair game' with respect to pretraining and fine-tuning: many LLMs are trained on competitive math problem tasks and then evaluated on new, unseen test problems from the same distribution.</p>
<p>Despite it being accepted practice, we attempt to guard against excessive uses of (2): we do not publicly release any of the code used to generate LiveBench questions, we always modify the generating code when updating the LiveBench questions (making the questions harder), and we do not publicly release the new questions for one month.</p>
<p>We also acknowledge that LiveBench does not fully satisfy (1): while nearly all questions are from June 2024 or more recent, there are some coding questions from November 2023, and the AMC questions have only undergone a low level of modification from their November 2023 version.Therefore, a limited fraction of LiveBench is likely contaminated on all recent LLMs.</p>
<p>B ADDITIONAL DOCUMENTATION</p>
<p>In this section, we give additional documentation for our benchmark.For the full details, see https://github.com/LiveBench/LiveBench/blob/main/README.md.</p>
<p>B.1 AUTHOR RESPONSIBILITY AND LICENSE</p>
<p>We, the authors, bear all responsibility in case of violation of rights.The license of our repository is the Apache License 2.0.The benchmark is available on HuggingFace at https://huggingface.co/livebench.</p>
<p>We actively maintain and update the benchmark, already having added multiple updates, and we continue to welcome contributions from the community.</p>
<p>B.3 CODE OF CONDUCT</p>
<p>Our Code of Conduct is from the Contributor Covenant, version 2.0.See https://www.contributor-covenant.org/version/2/0/code_of_conduct.html.</p>
<p>B.4 DATASHEET</p>
<p>We include a datasheet (Gebru et al., 2021) for LiveBench in https://github.com/LiveBench/LiveBench/blob/main/docs/DATASHEET.md.</p>
<p>B.5 BENCHMARK STATISTICS</p>
<p>Here, we give statistics on the number of questions and average number of output tokens per task, and the total cost of running LiveBench with common API models.For the number of questions for each task, as well as the mean and std.dev number of input and output tokens per question for gpt-4-turbo-2024-04-09, see Table 15.Across the 1000 questions in our current problem set, the mean number of input tokens per question was 1612, and the mean number of output tokens was 395.See Table 16 for the price to run GPT and Claude models on LiveBench, as of October 1, 2024.o1-preview-2024-09-12 is the most expensive at $47.87, while claude-3-haiku is the cheapest, at $0.90.</p>
<p>Figure 1 :
1
Figure 1: Left: results on LiveBench for top models, showing 95% bootstrap confidence intervals.Right: a radar plot for select models across LiveBench's six categories demonstrating the that ordering of top models varies between each category.</p>
<p>Figure 2 :Figure 3 :
23
Figure 2: Correlations among categories and groups in LiveBench.For each pair of categories (left) and tasks (right) in LiveBench, we compute the Pearson correlation coefficient based on the results for all 40 models.</p>
<p>Figure 4 :
4
Figure4: Comparison of LiveBench to other LLM benchmarks.We compare LiveBench to ChatBot Arena (left) and Arena-Hard (right).We see that while there are generally similar trends, some models are noticeably stronger on one benchmark vs. the other.For example, both GPT-4 models are substantially better on Arena-Hard.</p>
<p>Figure 5 :
5
Figure 5: Results on LiveBench for all models, showing 95% bootstrap confidence intervals.This is the full version of Figure 1 (left).</p>
<p>act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question displayed below.Your evaluation should consider correctness alone.Identify and correct any mistakes.Be as objective as possible.After providing your explanation, you must rate the response as either 1 (correct) or 0 (incorrect) by strictly following this format: "[[rating]]", for example: "Rating: [[1]]" [Question] question [The Start of Assistant's Answer] answer [The End of Assistant's Answer]</p>
<p>Table 1 :
1
LiveBench results across the 15 top-performing models.We display in this table the highest-performing models on LiveBench, outputting the results on each main category, as well as each model's overall performance.See Table2for the results on all 40 models.
ModelLiveBench CodingDataInstruction Language Math ReasoningScoreAnalysis Followingo1-preview-2024-09-1264.750.864.074.668.762.967.4claude-3-5-sonnet-2024102258.567.152.869.353.851.356.7claude-3-5-sonnet-2024062058.260.856.768.053.253.357.2o1-mini-2024-09-1256.748.154.165.440.959.272.3gemini-exp-111456.052.457.577.138.754.955.7gemini-exp-112156.050.457.080.140.062.845.8step-2-16k-20241155.146.954.979.944.548.955.5gpt-4o-2024-08-0653.851.452.968.647.648.253.9gemini-1.5-pro-00253.448.852.370.843.357.447.9gpt-4o-2024-05-1352.649.452.468.250.046.049.6gemini-1.5-pro-exp-082752.440.950.869.346.156.150.9meta-llama-3.1-405b-instruct-turbo51.143.853.572.843.240.552.8gpt-4o-2024-11-2050.646.147.264.947.442.555.7dracarys2-72b-instruct50.156.649.165.233.550.645.8chatgpt-4o-latest-090350.147.448.766.445.342.150.5</p>
<p>Table 2 :
2
LiveBench results across 40 models.We output the results for each model on each main category, as well as each model's overall LiveBench score.
ModelLiveBench CodingDataInstruction Language Math ReasoningScoreAnalysis Followingo1-preview-2024-09-1264.750.864.074.668.762.967.4claude-3-5-sonnet-2024102258.567.152.869.353.851.356.7claude-3-5-sonnet-2024062058.260.856.768.053.253.357.2o1-mini-2024-09-1256.748.154.165.440.959.272.3gemini-exp-111456.052.457.577.138.754.955.7gemini-exp-112156.050.457.080.140.062.845.8step-2-16k-20241155.146.954.979.944.548.955.5gpt-4o-2024-08-0653.851.452.968.647.648.253.9gemini-1.5-pro-00253.448.852.370.843.357.447.9gpt-4o-2024-05-1352.649.452.468.250.046.049.6gemini-1.5-pro-exp-082752.440.950.869.346.156.150.9meta-llama-3.1-405b-instruct-turbo51.143.853.572.843.240.552.8gpt-4o-2024-11-2050.646.147.264.947.442.555.7dracarys2-72b-instruct50.156.649.165.233.550.645.8chatgpt-4o-latest-090350.147.448.766.445.342.150.5gpt-4-turbo-2024-04-0949.649.051.360.844.342.749.5claude-3-opus-2024022948.438.654.363.950.443.439.8gemini-1.5-flash-00247.641.944.278.827.947.245.7mistral-large-240747.047.146.663.739.543.741.6qwen2.5-coder-32b-instruct45.056.843.458.723.245.942.1gpt-4-0125-preview44.841.854.157.239.233.443.2gemini-1.5-flash-exp-082744.140.647.973.029.628.944.5gemini-1.5-pro-00143.332.352.860.240.436.937.0meta-llama-3.1-70b-instruct-turbo43.232.750.369.734.334.437.9claude-3-5-haiku-2024102242.451.442.461.935.435.528.1gpt-4o-mini-2024-07-1840.243.244.556.828.635.632.8gemini-1.5-flash-00136.634.344.052.631.632.324.9gemma-2-27b-it36.435.943.658.132.626.222.1gemini-1.5-flash-8b-exp-082736.128.735.370.020.827.833.8qwen2.5-7b-instruct-turbo33.437.932.851.014.638.226.2claude-3-haiku-2024030733.224.541.555.329.122.925.9mixtral-8x22b-instruct-v0.131.232.031.752.321.824.524.9command-r-plus-08-202431.119.535.957.629.719.324.8gemma-2-9b-it28.322.535.152.625.519.514.5mistral-small-240227.521.231.956.418.918.517.9command-r-08-202427.017.931.355.616.719.521.0command-r-plus-04-202426.619.524.659.519.716.819.8meta-llama-3.1-8b-instruct-turbo25.019.732.251.517.916.612.2phi-3-mini-128k-instruct21.915.034.039.19.214.619.6phi-3-mini-4k-instruct21.115.029.536.48.615.022.2</p>
<p>Table 3 :
3
(Zhou et al., 2023a)uctions used in(Zhou et al., 2023a), and the 16 that are both 'real-world' and automatically verifiable, which we used in LiveBench.Descriptions are from(Zhou et al., 2023a).There should be {N} paragraphs.Paragraphs and only paragraphs are separated with each other by two line breaks.The {i}-th paragraph must start with word {first_word}.
LanguageResponse Lan-guageYour ENTIRE response should be in {language}, no other language is allowed.✓Length ConstraintsNumber graphsPara-Your response should contain {N} paragraphs. You separate paragraphs using the markdown divider: * * <em>✓✓Length ConstraintsNumber WordsAnswer with at least / around / at most {N} words.✓✓Length ConstraintsNumber tencesSen-Answer with at least / around / at most {N} sentences.✓✓Length ConstraintsNumber graphs + First Para-✓✓Word in i-thParagraphDetectable ContentPostscriptAt the end of your response, please explicitly add a postscript starting with {postscript marker} ✓✓Detectable ContentNumber Place-holderThe response must contain at least {N} placeholders represented by square brackets, such as [address].✓Detectable FormatNumber BulletsYour answer must contain exactly {N} bullet points. Use the markdown bullet points such as: * This is a point.✓✓Detectable FormatTitleYour answer must contain a title, wrapped in double angular brackets, such as &lt;<poem of joy>&gt;.✓✓Detectable FormatChoose FromAnswer with one of the following options: {options}✓Detectable FormatMinimum Num-ber HighlightedHighlight at least {N} sections in your answer with markdown, i.e. </em>highlighted section<em>✓SectionDetectable FormatMultiple Sections Your response must have {N} sections. Mark the beginning of each section with {sec-tion_splitter} X.✓✓Detectable FormatJSON FormatEntire output should be wrapped in JSON format.✓✓CombinationRepeat PromptFirst, repeat the request without change, then give your answer (do not say anything before repeating the request; the request you need to repeat does not include this sentence)✓✓CombinationTwo ResponsesGive two different responses. Responses and only responses should be separated by 6 asterisk symbols: </em><strong>*</strong>.✓✓Change CasesAll UppercaseYour entire response should be in English, capital letters only.✓Change CasesAll LowercaseYour entire response should be in English, and in all lowercase letters. No capital letters are allowed.✓Change CasesFrequency of All-capital WordsIn your response, words with all capital letters should appear at least / around / at most {N} times.✓Start with / End with End CheckerFinish your response with this exact phrase {end_phrase}. No other words should follow this phrase.✓✓Start with / End with QuotationWrap your entire response with double quotation marks.✓✓PunctuationNo CommasIn your entire response, refrain from the use of any commas.✓</p>
<p>Table 5 :
5
Pearson correlation coefficient and std.error for each category compared to the overall average LiveBench score, computed using data from all 40 models.
CategoryCorrelation Std Errormath0.94770.0643reasoning0.94390.0709data_analysis0.93150.0489coding0.89700.0840language0.89700.0831instruction_following0.81740.0811</p>
<p>Table 6 :
6
Pearson correlation coefficient and std.error for each task compared to the overall average LiveBench score, computed using data from all 40 models.
CategoryCorrelation Std Errorweb_of_lies_v20.91360.1454math_comp0.90350.0986tablejoin0.89840.0961zebra_puzzle0.88530.0935coding_completion0.86230.1212cta0.85560.0486plot_unscrambling0.85530.0881LCB_generation0.85500.0816connections0.83680.1188AMPS_Hard0.82450.1357olympiad0.81920.1171summarize0.80440.0904paraphrase0.78840.0953simplify0.77650.0829typos0.77220.1474story_generation0.74430.1018spatial0.72940.0968tablereformat0.68400.1058A.1 DETAILS FROM CORRELATION ANALYSES</p>
<p>Table 7 :
7
Relative best and worst task for each model, computed as the tasks with the highest and lowest residuals of the best fit line vs. overall LiveBench performance, for each model.</p>
<p>Table 8 :
8
LLM judges cannot accurately evaluate challenging math and reasoning questions.Error rate of LLM-as-a-judge scoring on challenging math (AMC, AIME, SMC) and reasoning (Zebra puzzles) tasks.On all tasks, the error rate is surprisingly high, showing that LLMs are not reliable judges for these tasks.
ModelJudgeAMC12 2024 AIME 2024 SMC 2023 Zebra PuzzlesGPT-4-TurboGPT-4-Turbo0.3800.2140.3530.420Claude-3-Opus GPT-4-Turbo0.3880.1030.2940.460Table 9: Model Performance on math and reasoning tasks with both ground-truth (GT) or LLMjudging (LLM-Jdg.)AMC12 2024AIME 2024SMC 2023Zebra PuzzlesGT LLM-Jdg.GT LLM-Jdg.GT LLM-Jdg. GT LLM-Jdg.GPT-4-Turbo5464.000 13.79335.714 70.58858.8243868Claude-3-Opus5642.8576.89717.241 58.82452.9413452</p>
<p>Table 10 :
10
IMO/USAMO results for each of 34 models across all hardness levels.
ModelIMO USAMO Avg.gpt-4o-2024-05-1360.2467.47 63.85gpt-4-1106-preview58.1667.17 62.66claude-3-opus-2024022952.5663.66 58.11gpt-4-turbo-2024-04-0950.9664.80 57.88gemini-1.5-pro-latest52.1159.15 55.63gpt-4-0125-preview43.0460.66 51.85Meta-Llama-3-70B-Instruct43.2459.55 51.40claude-3-sonnet-2024022944.7852.97 48.87command-r-plus48.3344.55 46.44gpt-3.5-turbo-110640.3749.65 45.01mistral-large-240238.6550.41 44.53claude-3-haiku-2024030741.5147.31 44.41gpt-3.5-turbo-012538.4447.17 42.80Qwen1.5-72B-Chat34.3548.47 41.41Mixtral-8x22B-Instruct-v0.1 33.0048.62 40.81mistral-small-240234.5144.78 39.64Meta-Llama-3-8B-Instruct36.0536.59 36.32Qwen1.5-110B-Chat23.9346.78 35.35Mistral-7B-Instruct-v0.236.0034.31 35.15command-r31.3629.38 30.37Phi-3-mini-128k-instruct25.8433.54 29.69Mixtral-8x7B-Instruct-v0.126.5232.50 29.51Phi-3-mini-4k-instruct26.6030.33 28.46Qwen1.5-7B-Chat22.1031.84 26.97Starling-LM-7B-beta14.9928.70 21.84zephyr-7b-alpha25.9916.43 21.21vicuna-7b-v1.5-16k23.1416.69 19.91Yi-6B-Chat18.1720.05 19.11zephyr-7b-beta9.5722.57 16.07Llama-2-7b-chat-hf20.0011.53 15.77Qwen1.5-4B-Chat11.9016.78 14.34vicuna-7b-v1.516.199.87 13.03Qwen1.5-0.5B-Chat9.2710.619.94Qwen1.5-1.8B-Chat0.989.135.06</p>
<p>Table 11 :
11
IMO/USAMO results for each hardness level across 34 models.
Hardness level Avg.IMO USAMOEasy57.48 54.6860.27Medium29.60 25.7933.41Hard19.11 15.9622.25</p>
<p>Table 12 :
12
The prompt for each subtask used in each of the four instruction following tasks.
SubtaskSubtask PromptParaphrasePlease paraphrase based on the sentences provided.SummarizePlease summarize based on the sentences provided.SimplifyPlease explain in simpler terms what this text means.Story Generation Please generate a story based on the sentences provided.</p>
<p>An example question from the Connections task.You are given 8 words/phrases below.Find two groups of four items that share something in common.Here are a few examples of groups: bass, flounder, salmon, trout (all four are fish); ant, drill, island, opal (all four are two-word phrases that start with 'fire'); are, why, bee, queue (all four are homophones of letters); sea, sister, sin, wonder (all four are members of a septet).Categories will be more specific than e.g., '5-letter-words', 'names', or 'verbs'.There is exactly one solution.Think step-by-step, and then give your answer in <strong>bold</strong> as a list of the 8 items separated by commas, ordered by group (for example, <strong>bass, founder, salmon, trout, ant, drill, island, opal</strong>).If you don't know the answer, make your best guess.The items are: row, drift, curl, tide, current, press, fly, wave.
Ground Truth: current, drift, tide, wave, curl, fly, press, row</p>
<p>Table 13 :
13
Summary for tasks in LiveBench.
CategoryTaskNum. Data Sourcedata_analysiscta50 Kaggle and Socrata datasetsdata_analysistablejoin50 Kaggle and Socrata datasetsdata_analysistablereformat50 Kaggle and Socrata datasetsinstruction_following summarize50 The Guardian articlesinstruction_following paraphrase50 The Guardian articlesinstruction_following story_generation50 The Guardian articlesinstruction_following simplify50 The Guardian articleslanguagetypos50 ArXiv abstractslanguageconnections50 NYT daily puzzleslanguageplot_unscrambling40 IMDb plot synopsesreasoningweb_of_lies_v250 N/A (synthetic)reasoningzebra_puzzle50 N/A (synthetic)reasoningspatial50 N/A (manually created)matholympiad36 International Olympiad 2024mathAMPS_Hard100 N/A (synthetic)mathmath_comp96 AMC 2023 and AIME 2024codingcoding_completion50 LiveCodeBenchcodingLCB_generation78 LiveCodeBench and Leetcode
summary of a movie has had the sentences randomly reordered.Rewrite the plot summary with the sentences correctly ordered.Begin the plot summary with <PLOT_SUMMARY>.'.</p>
<p>Table 14 :
14
Description for whether tasks in LiveBenchcan be updated automatically.rerun (and/or update) synthetic generation script, but source can only be updated yearly math AMPS_Hard Yes -rerun (and/or update) the synthetic generation script math math_comp Yes<em> -but can only be updated yearly, or by using different contests coding coding_completion Yes -replace with more recent LeetCode questions coding LCB_generation Yes -replace with more recent LeetCode questions A.7 CONTAMINATION IN LIVEBENCH We point out two different definitions of contamination: (1) test set contamination, and (2) task contamination -or train test distribution similarity.
CategoryTaskCan be updated automatically using a script?data_analysisctaYes -replace with newer datasetsdata_analysistablejoinYes -replace with newer datasetsdata_analysistablereformatYes -replace with newer datasetsinstruction_following summarizeYes -replace with newer articles and/or update synthetic generation scriptinstruction_following paraphraseYes -swap out news articles and/or update synthetic generation scriptinstruction_following story_generationYes -swap out news articles and/or update synthetic generation scriptinstruction_following simplifyYes -swap out news articles and/or update synthetic generation scriptlanguagetyposYes -replace with newer paper abstractslanguageconnectionsYes -replace with newer (daily) puzzleslanguageplot_unscrambling Yes -replace with newer imdb plot summariesreasoningweb_of_lies_v2Yes -rerun (and/or update) the synthetic generation scriptreasoningzebra_puzzleYes -rerun (and/or update) the synthetic generation scriptreasoningspatialNo -create new questions by handmatholympiadYes</em> -</p>
<p>Table 15 :
15
Statistics for tasks in LiveBench.This table gives the number of questions for each task, as well as the mean and std.dev number of output tokens per question for gpt-4-turbo-2024-04-09.
Input TokensOutput Tokens</p>
<p>Table 16 :
16
Prices for running GPT and Claude models on LiveBench.This table gives the approximate cost for running models on LiveBench as of Oct 1, 2024.Note that we used the gpt-4-turbo tokenizer for all computations, so all other prices are approximate.
ModelPrice in USDo1-preview-2024-09-12≈ 47.87o1-mini-2024-09-12≈ 9.57gpt-4o-2024-05-13≈ 13.98gpt-4-turbo-2024-04-0927.97gpt-4-1106-preview≈ 27.97gpt-3.5-turbo-0125≈ 1.40claude-3-opus≈ 53.80claude-3-5-sonnet≈ 10.76claude-3-sonnet≈ 10.76claude-3-haiku≈ 0.90
Note that these experiments were run in June
, before models such as claude-3.5-sonnet were released.
https://open-platform.theguardian.com/
See https://www.nytimes.com/games/connections.
meta-llama-3.1-8b-instruct-turbocommand-r-plus-04-2024 command-r-08-2024 mistral-small-2402 gemma-2-9b-it command-r-plus-08-2024 mixtral-8x22b-instruct-v0.1 claude-3-haiku-20240307 qwen2.5-7b-instruct-turbogemini-1.5-flash-8b-exp-0827gemma-2-27b-it gemini-1.5-flash-001gpt-4o-mini-2024-07-18 claude-3-5-haiku-20241022 meta-llama-3.1-70b-instruct-turbogemini-1.5-pro-001gemini-1.5-flash-exp-0827gpt-4-0125-preview qwen2.5-coder-32b-instructmistral-large-2407 gemini-1.5-flash-002claude-3-opus-20240229 gpt-4-turbo-2024-04-09 chatgpt-4o-latest-0903 dracarys2-72b-instruct gpt-4o-2024-11-20 meta-llama-3.1-405b-instruct-turbogemini-1.5-pro-exp-0827gpt-4o-2024-05-13 gemini-1.5-pro-002gpt-4o-2024-08-06 step-2-16k-202411 gemini-exp-1121 gemini-exp-1114 o1-mini-2024-09-12 claude-3-5-sonnet-20240620 claude-3-5-sonnet-20241022 o1-preview-2024-09-12 Table 4: List of models evaluated (across all LiveBench versions) and their respective citations.Model Name Citation chatgpt-4o-latest-0903 (Hurst et al., 2024) claude-3-5-haiku-20241022 https://www.anthropic.com/claude/haikuclaude-3-5-sonnet-20240620 https://www.anthropic.com/news/claude-3-5-sonnetclaude-3-5-sonnet-20241022 https://www.anthropic.com/news/claude-3-5-sonnetclaude-3-haiku-20240307 (Anthropic, 2024) claude-3-opus-20240229 (Anthropic, 2024) claude-3-sonnet-20240229 (Anthropic, 2024) command-r (Cohere, 2024) command-r-08-2024 (Cohere, 2024) command-r-plus (Cohere, 2024) command-r-plus-08-2024 (Cohere, 2024) deepseek-coder-v2 (DeepSeek-AI et al., 2024) deepseek-coder-v2-lite-instruct (DeepSeek-AI et al., 2024) deepseek-v2-lite-chat (DeepSeek-AI et al., 2024) deepseek-v2.5 (DeepSeek-AI et al., 2024) dracarys-72b-instruct https://huggingface.co/abacusai/Dracarys-72B-Instruct dracarys-llama-3.1-70b-instructhttps://huggingface.co/abacusai/Dracarys-Llama-3.1-70B-Instructdracarys2-72b-instruct https://huggingface.co/abacusai/Dracarys2-72B-Instruct dracarys2-llama-3.1-70b-instructhttps://huggingface.co/abacusai/Dracarys2-Llama-3.1-70B-Instructgemini-1.5-flash-002(Reid et al., 2024) gemini-1.5-flash-8b-exp-0827(Reid et al., 2024) gemini-1.5-flash-api-0514(Reid et al., 2024) gemini-1.5-flash-exp-0827(Reid et al., 2024) gemini-1.5-pro-002(Reid et al., 2024) gemini-1.5-pro-api-0514(Reid et al., 2024) gemini-1.5-pro-exp-0801(Reid et al., 2024) gemini-1.5-pro-exp-0827(Reid et al., 2024) gemini-exp-1114 https://ai.google.dev/gemini-api/docs/models/experimental-modelsgemini-exp-1121 https://ai.google.dev/gemini-api/docs/models/experimental-modelsgemma-1.1-7b-it(Team, 2024a) gemma-2-27b-it (Team et al., 2024) gemma-2-2b (Team et al., 2024) gemma-2-9b-it (Team et al., 2024) gpt-3.5-turbo-0125(Brown et al., 2020) gpt-3.5-turbo-1106(Brown et al., 2020) gpt-4-0125-preview (OpenAI, 2023) gpt-4-0613 (OpenAI, 2023) gpt-4-1106-preview (OpenAI, 2023) gpt-4-turbo-2024-04-09 (OpenAI, 2023) gpt-4o-2024-05-13 (Hurst et al., 2024) gpt-4o-2024-08-06 (Hurst et al., 2024) gpt-4o-2024-11-20 (Hurst et al., 2024) gpt-4o-mini-2024-07-18 (Hurst et al., 2024) grok-2 https://x.ai/blog/grok-2grok-2-mini https://x.ai/blog/grok-2llama-2-7b-chat-hf (Touvron et al., 2023) llama-3.1-nemotron-70b-instruct(Adler et al., 2024) meta-llama-3-70b-instruct (Meta, 2024) meta-llama-3-8b-instruct (Meta, 2024) meta-llama-3.1-405b-instruct-turbo(Dubey et al., 2024) meta-llama-3.1-70b-instruct-turbo(Dubey et al., 2024) meta-llama-3.1-8b-instruct-turbo(Dubey et al., 2024) mistral-7b-instruct-v0.2 (Jiang et al., 2023) mistral-7b-instruct-v0.3 (Jiang et al., 2023) mistral-large-2402 (Jiang et al., 2023) mistral-large-2407 (Jiang et al., 2023) mistral-small-2402 (Jiang et al., 2023) mixtral-8x22b-instruct-v0.1 (Jiang et al., 2023) mixtral-8x7b-instruct-v0.1 (Jiang et al., 2023) o1-mini-2024-09-12https://openai.com/index/openai-o1-system-card/o1-preview-2024-09-12 https://openai.com/index/openai-o1-system-card/open-mistral-nemo (Jiang et al., 2023)phi-3-medium-128k-instruct(Abdin et al., 2024) phi-3-mini-128k-instruct(Abdin et al., 2024) phi-3-small-128k-instruct(Abdin et al., 2024) phi-3.5-mini-instruct(Abdin et al., 2024) phi-3.5-moe-instruct(Abdin et al., 2024) qwen1.5-0.5b-chat (Bai et al., 2023) qwen1.5-72b-chat (Bai et al., 2023) qwen1.5-110b-chat (Bai et al., 2023) qwen1.5-7b-chat (Bai et al., 2023) qwen2-0.5b-instruct(Yang et al., 2024) qwen2-1.5b-instruct(Yang et al., 2024) qwen2-72b-instruct(Yang et al., 2024) qwen2-7b-instruct(Yang et al., 2024)qwen2.5-72b-instruct(Team, 2024b) qwen2.5-7b-instruct-turbo(Team, 2024b) starling-lm-7b-beta(Zhu et al., 2023) step-2-16k-202411https://www.stepfun.com/#step2vicuna-7b-v1.5(Chiang et al., 2023) vicuna-7b-v1.5-16k (Chiang et al., 2023) yi-6b-chat https://huggingface.co/01-ai/Yi-6B zephyr-7b-alpha(Tunstall et al., 2023) zephyr-7b-beta(Tunstall et al., 2023)An example question from the AMPS Hard task.Find an indefinite integral (which can vary by a constant) of the following function: 5 sec 2 (5x + 1) − 8 sin(7 − 8x).Please put your final answer in a boxed{}.Ground Truth: − sin(7) sin(8x) − cos(7) cos(8x) + tan(5x + 1)The coding ability of LLMs is one of the most widely studied and sought-after skills for LLMs(Mnih et al., 2015;Jain et al., 2024;Li et al., 2023a).We include two coding tasks in LiveBench: a modified version of the code generation task from LiveCodeBench(Jain et al., 2024), and a novel code completion task combining LiveCodeBench problems with partial solutions collected from GitHub sources.Examples of questions from the Coding tasks can be found here.Code generation.In the LCB Generation task, we assess a model's ability to parse a competition coding question statement and write a correct answer.LiveCodeBench(Jain et al., 2024)included several tasks to assess the coding capabilities of large language models.We have taken 78 randomly selected problems from the April 2024 release of LiveCodeBench, selecting only problems released in or after November 2023.The problems are competition programming problems from LeetCode(Team, 2015)and AtCoder(Team, 2012), defined with a textual description and solved by writing full programs in Python 3 code.These problems are presented as in LiveCodeBench's Code Generation task, with minor prompting differences and with only one chance at generating a correct solution per question, per model.We report pass@1, a metric which describes the proportion of questions that a given model solved completely (a solution is considered correct if and only if it passes all public and private test cases).Code completion.In this task, we assess the ability of the model to successfully complete a partially provided solution to a competition coding question statement.The setup is similar to the Code Generation task above, but a partial (correct) solution is provided in the prompt and the model is instructed to complete it to solve the question.We use LeetCode easy, medium, and hard problems from LiveCodeBench's(Jain et al., 2024)April 2024 release, combined with matching solutions from https://github.com/kamyu104/LeetCode-Solutions,omitting the last 15% of each medium/hard solution and 30-70% of each easy solution and asking the LLM to complete the solution.As with Code Generation, we report pass@1.A.3.3 REASONING CATEGORYThe reasoning abilities of large language models is another highly-benchmarked and analyzed skill of LLMs(Wei et al., 2022;Suzgun et al., 2023;Yao et al., 2024).In LiveBench, we include two reasoning tasks: a harder version of a task from Big-Bench Hard(Suzgun et al., 2023), and Zebra puzzles.Web of lies v2.Web of Lies is a task included in Big-Bench (bench authors, 2023) and Big-Bench Hard(Suzgun et al., 2023).The task is to evaluate the truth value of a random Boolean function expressed as a natural-language word problem.In particular, the LLM must evaluate f n (f n−1 (...f 1 (x)...)), where each f i is either negation or identity, and x is True or False.We represent x by the sentence: X 0 {tells the truth, lies}, and we represent f i by a sentence: X i says X i−1 {tells the truth, lies}.The sentences can be presented in a random order for increased difficulty.For example, a simple n = 2 version is as follows: 'Ka says Yoland tells the truth.Yoland lies.Does Ka tell the truth?' Already by October 2022, LLMs achieved near 100% on this task, and furthermore, there are concerns that Big-Bench tasks leaked into the training data of GPT-4, despite using canary strings(OpenAI, 2023).For LiveBench, we create a new, significantly harder version of Web of Lies.We make the task harder with a few additions: (1) adding different types of red herrings, (2) asking for the truth values of three people, instead of just one person, and (3) adding a simple additional deductive component.For (1), we maintain a list of red herring names, so that the red herrings do not affect the logic of the answer while still potentially leading LLMs astray.For example, 'Fred says Kayla lies,' where Fred Published as a conference paper at ICLR 2025We seek a generative method M : Σ * → Σ * that comes closest to satisfying the following properties:For further details on the task, please refer toFeuer et al. (2023).Implementation details.For each benchmark instance, we retrieve a random A from our available pool of recent tables.We randomly and uniformly sample C from A, use the actual column name of A as our CTA ground-truth L, and retrieve σ 1 • • • σ 5 column samples from C, with replacement, providing them as context for the LLM.Metrics.We report Accuracy @ 1 over all instances, accepting only case-insensitive exact string matches as correct answers.Table reformatting. Given a tableA rendered according to a plaintext-readable and valid schema for storing tabular information a s , we instruct the LLM to output the same table with the contents unchanged but the schema modified to a distinct plaintext-readable valid schema b s .Implementation details.We use the popular library Pandas to perform all of our conversions to and from text strings.We allow the following formats for both input and output: "JSON", "JSONL", "Markdown", "CSV", "TSV", "HTML".As tabular conversion from JSON to Pandas is not standardized, we accept several variations.At inference time, we ingest the LLM response table directly into Pandas.Metrics.We report Accuracy @ 1 over all instances.An instance is accepted only if it passes all tests (we compare column count, row count, and exact match on row contents for each instance).Join-column prediction.Given two tables A and B, with columns a 1 , . . .and b 1 , . . .respectively, the join-column prediction task is to suggest a pair (a k , b l ) of columns such that the equality condition a k = b l can be used to join the the tables in a way that matches with the provided ground-truth mapping M : A → B. The mapping is usually partial injective: not every column in B is mapped from A, not every column in A is mapped to B. For further details, please refer toYan &amp; He (2020).Implementation details.We randomly sample columns with replacement from our entire collection of tables, generating a fixed column pool C. We retain half the rows of A to provide as context to the LLM.The remaining rows are used to generate a new table B. For each instance, we randomly sample columns from both the target table and the column pool and join them to B. We anonymize the column names in B, then pass both A and B to the LLM and ask it to return a valid join mapping M. Metrics.We report the F1 score over columns, with TPs scored as exact matches between ground truth and the LLM output, FPs scored as extraneous mappings, FNs scored as missing mappings, and incorrect mappings counting as FP + FN.A.3.5 INSTRUCTION FOLLOWINGAn important ability of an LLM is its capability to follow instructions.To this end, we include instruction following questions in our benchmark, inspired by IFEval(Zhou et al., 2023a).Generating live prompts and instruction.IFEval, or instruction-following evaluation for LLMs, contains verifiable instructions such as "write more than 300 words" or "Finish your response with this exact phrase: {end_phrase}."These instructions are then appended to prompts like "write a short blog about the a visit in Japan".We use this modular nature between the prompt and instruction to construct live prompts.For our live source, we considered news articles from The Guardian; we are able to obtain 200 articles using their API 2 .Using the first n sentences article text as the source text, we consider four different tasks using the text: paraphrase, summarize, simplify, and story generation.The exact prompts can be seen in Table12.For the instructions, we use the code provided byZhou et al. (2023a), making a few modifications such as increasing the max number of keywords from two to five.Additionally, we compose different instructions together by sampling from a uniform distribution from 2 to 5.However, since the instructions can be conflicting, we deconflict the instructions.This results in approximate normal distribution of the number of instructions per example with the majority of the containing two or three instructions.A full list of the instructions can be found in Appendix Table3.To construct, the full prompt, containing the news article sentences, the prompt, and the instructions, we use the following meta prompt: "The following are the beginning sentences of a news article from the Guardian.\n---\n{guardianarticle}\n---\n{subtask prompt} {instructions}".Manual Verification Next, we manually inspect the incorrect answers from the flagged tasks above, as well as a random sample of the responses across all tasks.The goal is to make sure that the wrong answers are truly incorrect, rather than a correct answer in the wrong format, or in such a way that it is marked incorrect by the parsing function.(Note that these checks are focused on catching false negatives.It is extremely unlikely for a false positive to occur, but we checked for false positives in the initial development of LiveBench, and occasionally check answers that are scored correct, for this reason.)Scoring Function Updates Based on our evaluation and verification, we update our scoring functions to accommodate any unique response patterns or notation styles exhibited by the new model.This maintains the permissiveness and fairness of our grading methodology, ensuring accurate scoring.Note that we made a few such updates when new models came out that hit new edge cases of our scoring functions, in the first month of LiveBench's release, but now we have not needed to make a scoring function update for a few months.Re-Running the Evaluation Pipeline After potentially updating scoring functions for some tasks, we re-score all models with the new scoring functions.This ensures consistent and accurate results.A.6 QUESTION UPDATE POLICYTo ensure the continued relevance and effectiveness of LiveBench, we have implemented a thorough question update policy.That said, we still maintain a degree of flexibility in updating questions, as well as updating the LiveBench policies as a whole, in order to be able to adapt to new developments and trends in the field (such as the release of models that employ search at inference time, or the popularity of a new type of LLM capability).In this section, we give more details on the update policy laid out in Section 2.7.Regular UpdatesWe update questions and tasks on a monthly basis, incorporating new and challenging prompts that reflect the evolving landscape of large language models.In each update, we replace 1 /6 of the questions on average, so that the benchmark is fully refreshed roughly every 6 months.We may speed up the turnover rate of questions in the future, based on interest in LiveBench.Each month, we do not release the new questions until one month later, so that the public leaderboard always has 1 /6 questions that are private and completely contamination-free.We choose tasks to update based primarily on two factors: (1) the oldest tasks, and (2) the currently easiest tasks.In this way, the questions in LiveBench will stay new and continue to challenge the most capable LLMs.We also update tasks if there is any suspected contamination or other reason for updating a task.Nearly all tasks can be updated simply by running a script.See Table14for a breakdown of each task.Although most tasks can be auotmatically updated with a script, we always modify the generation script when creating new questions, in order to make sure that the distribution of questions is different and harder over time.Evaluation Re-Runs After updating questions and tasks, we re-run the evaluation pipeline on all models, ensuring consistent and accurate results.By implementing this comprehensive question update policy, we guarantee that LiveBench remains a vibrant, dynamic, and relevant benchmark for evaluating large language models.Version Control We maintain version control over all updates, ensuring transparency and reproducibility.This allows researchers to track changes and compare results across different versions of LiveBench.Community Engagement We encourage community engagement and collaboration in expanding and improving LiveBench.Researchers and developers can contribute new tasks, provide feedback on existing ones, and participate in shaping the benchmark's evolution.
Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, arXiv:2404.14219Harkirat Behl, et al. Phi-3 technical report: A highly capable language model locally on your phone. 2024arXiv preprint</p>
<p>Bo Adler, Niket Agarwal, Ashwath Aithal, Dong H Anh, Pallab Bhattacharya, Annika Brundyn, Jared Casper, Bryan Catanzaro, Sharon Clay, Jonathan Cohen, arXiv:2406.11704Nemotron-4 340b technical report. 2024arXiv preprint</p>
<p>; Yunfei Ai Anthropic, Zeyu Chu, Kai Cui, Xiaodong Dang, Yang Deng, Wenbin Fan, Yu Ge, Fei Han, Huang, arXiv:2309.16609The claude 3 model family: Opus, sonnet, haiku. Claude-3 Model Card. Jinze Bai, Shuai Bai, 2024. 2023arXiv preprintQwen technical report</p>
<p>. Edward Beeching, Clémentine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, Thomas Wolf, Open llm leaderboard. 2023</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. BIG bench authors. 2023</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023arXiv preprint</p>
<p>A survey on evaluation of large language models. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, ACM Transactions on Intelligent Systems and Technology. 1532024</p>
<p>Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, April 2023. 2023146</p>
<p>Chatbot arena: An open platform for evaluating llms by human preference. Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E Gonzalez, arXiv:2403.041322024arXiv preprint</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.141682021arXiv preprint</p>
<p>Command r: Retrieval-augmented generation at production scale. Cohere, March 2024</p>
<p>Cohere For AI. c4ai-command-r-plus-08-2024. 2024</p>
<p>. Deepseek-Ai , Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H Zhang, Hanwei Xu, Hao Yang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J L Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jin Chen, Jingyang Yuan, Junjie Qiu, Junxiao Song, Kai Dong, Kaige Gao, Kang Guan, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qihao Zhu, Qinyu Chen, Qiushi Du, R J Chen, R L Jin, Ruiqi Ge, Ruizhe Pan, Runxin Xu, Ruyi Chen, S S Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Size Zheng, T Wang, Tian Pei, Tian Yuan, Tianyu Sun, W L Xiao, Wangding Zeng, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wentao Zhang, X Q Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang Wang, Xin Liu, Xin Xie, Xingkai Yu, Xinnan Song, Xinyi Zhou, Xinyu Yang, Xuan Lu, Xuecheng Su, Y Wu, Y K Li, Y X Wei, Y X Zhu, Yanhong Xu, Yanping Huang, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Li, Yaohui Wang, Yi Zheng, Yichao Zhang, Yiliang Xiong, Yilong Zhao, Ying He, Ying Tang, Yishi Piao, Yixin Dong, Yixuan Tan, Yiyuan Liu ; Zehui Ren, Zhangli Sha, Zhe Fu, Zhen Huang, Zhen Zhang, Zhenda Xie, Zhewen Hao, Zhihong Shao, Zhiniu Wen, Zhipeng Xu, Zhongyu Zhang, Zhuoshu Li, Zihan Wang, Zihui Gu, Z. Z. Ren. 2024Yuxuan LiuZilin Li, and Ziwei Xie. Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model</p>
<p>Investigating data contamination in modern benchmarks for large language models. Chunyuan Deng, Yilun Zhao, Xiangru Tang, Mark Gerstein, Arman Cohan, arXiv:2311.097832023arXiv preprint</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.048052018arXiv preprint</p>
<p>Generalization or memorization: Data contamination and trustworthy evaluation for large language models. Yihong Dong, Xue Jiang, Huanyu Liu, Zhi Jin, Ge Li, arXiv:2402.15938arXiv:2407.21783The llama 3 herd of models. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, 2024. 2024arXiv preprint</p>
<p>Alpacafarm: A simulation framework for methods that learn from human feedback. Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, 2023</p>
<p>Yann Dubois, Balázs Galambosi, Percy Liang, Tatsunori B Hashimoto, arXiv:2404.04475Length-controlled alpacaeval: A simple way to debias automatic evaluators. 2024arXiv preprint</p>
<p>The Contest Problem Book VIII: American Mathematics Competitions (AMC 10). Douglas Faires, David Wells, 2000-2007. 2022American Mathematical Society19</p>
<p>ArcheType: A Novel Framework for Open-Source Column Type Annotation using Large Language Models. Benjamin Feuer, Yurong Liu, Chinmay Hegde, Juliana Freire, arXiv:2310.18208October 2023</p>
<p>Omni-math: A universal olympiad level mathematic benchmark for large language models. Bofei Gao, Tianyu Liu, 2024</p>
<p>A framework for few-shot language model evaluation. Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony Dipofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle Mcdonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, Andy Zou, September 2021</p>
<p>Datasheets for datasets. Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé Iii, Kate Crawford, Communications of the ACM. 64122021</p>
<p>Data contamination quiz: A tool to detect and estimate contamination in large language models. Shahriar Golchin, Mihai Surdeanu, arXiv:2311.062332023aarXiv preprint</p>
<p>Time travel in llms: Tracing data contamination in large language models. Shahriar Golchin, Mihai Surdeanu, arXiv:2308.084932023barXiv preprint</p>
<p>The overlooked genai use case. Anthony Goldbloom, October 2024</p>
<p>Guardian Media, Group , The guardian. 2024-01-20</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, arXiv:2103.038742021arXiv preprint</p>
<p>Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, Akila Ostrow, Alan Welihinda, Alec Hayes, Radford, arXiv:2410.21276Gpt-4o system card. 2024arXiv preprint</p>
<p>Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, arXiv:2403.079742024arXiv preprint</p>
<p>Einstein's riddle: Riddles, paradoxes, and conundrums to stretch your mind. Jeremy, 2009Bloomsbury USA</p>
<p>. Alexandre Albert Q Jiang, Arthur Sablayrolles, Chris Mensch, Devendra Bamford, Diego Singh Chaplot, Florian De Las Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, arXiv:2310.068252023Mistral 7b. arXiv preprint</p>
<p>Suresh Vishakha, Andrew Kalal, Sean Parry, Macavaney, arXiv:2411.02284Training on the test model: Contamination in ranking distillation. 2024arXiv preprint</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS). the Annual Conference on Neural Information Processing Systems (NeurIPS)2022</p>
<p>On quality control and machine learning in crowdsourcing. Matthew Lease, Workshops at the twenty-fifth AAAI conference on artificial intelligence. 2011</p>
<p>Binary codes capable of correcting deletions, insertions, and reversals. I Vladimir, Levenshtein, Soviet Physics Doklady. 1081966</p>
<p>Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, arXiv:2305.06161Starcoder: may the source be with you!. 2023aarXiv preprint</p>
<p>From live data to high-quality benchmarks: The arena-hard pipeline. Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Banghua Zhu, Joseph E Gonzalez, Ion Stoica, April 2024</p>
<p>Alpacaeval: An automatic evaluator of instruction-following models. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, 2023b</p>
<p>Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, arXiv:2405.04434Deepseek-v2: A strong, economical, and efficient mixture-ofexperts language model. 2024aarXiv preprint</p>
<p>. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, 2019Roberta: A robustly optimized bert pretraining approach</p>
<p>Enhancing biomedical schema matching with llm-based training data generation. Yurong Liu, Aécio Santos, Eduardo Hm Pena, Roque Lopez, Eden Wu, Juliana Freire, NeurIPS Third Table Representation Learning Workshop. 2024b</p>
<p>Introducing meta llama 3: The most capable openly available llm to date. Meta. April 2024. June 4, 2024</p>
<p>Sympy: symbolic computing in python. Aaron Meurer, Christopher P Smith, Mateusz Paprocki, Ondřej Čertík, B Sergey, Matthew Kirpichev, Amit Rocklin, Sergiu Kumar, Jason K Ivanov, Sartaj Moore, Thilina Singh, Sean Rathnayake, Brian E Vig, Richard P Granger, Francesco Muller, Harsh Bonazzi, Shivam Gupta, Fredrik Vats, Fabian Johansson, Matthew J Pedregosa, Andy R Curry, Štěpán Terrel, Ashutosh Roučka, Isuru Saboo, Sumith Fernando, Robert Kulal, Anthony Cimrman, Scopatz, 10.7717/peerj-cs.103PeerJ Computer Science. 2376-59923e103January 2017</p>
<p>Human-level control through deep reinforcement learning. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, Demis Hassabis, 10.1038/nature14236Nature. 002808365187540February 2015</p>
<p>Gpt-4 technical report. 2023OpenAITechnical Report</p>
<p>Proving test set contamination in black box language models. Yonatan Oren, Nicole Meister, Niladri Chatterji, Faisal Ladhak, Tatsunori B Hashimoto, arXiv:2310.176232023. 2023arXiv preprintquint t. Puzzle generator and puzzle solver</p>
<p>Measuring attribution in natural language generation models. Vitaly Hannah Rashkin, Matthew Nikolaev, Lora Lamm, Michael Aroyo, Dipanjan Collins, Slav Das, Gaurav Petrov, Iulia Singh Tomar, David Turc, Reitter, Computational Linguistics. 4942023</p>
<p>Pattern matching: The gestalt approach. John W Ratcliff, David E Metzener, Dr. Dobb's Journal. 461988</p>
<p>Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-Baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, arXiv:2403.055302024arXiv preprint</p>
<p>To the cutoff... and beyond? a longitudinal perspective on llm data contamination. Manley Roberts, Himanshu Thakur, Christine Herlihy, Colin White, Samuel Dooley, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2024</p>
<p>. A I Scale, Seal Leaderboards, May 2024</p>
<p>Rematch: Retrieval enhanced schema matching with llms. Eitam Sheetrit, Menachem Brief, Moshik Mishaeli, Oren Elisha, arXiv:2403.015672024arXiv preprint</p>
<p>Evaluation data contamination in llms: how do we measure it and (when) does it matter?. K Aaditya, Muhammed Singh, Andrew Yusuf Kocyigit, David Poulton, Maria Esiobu, Gergely Lomeli, Dieuwke Szilvasy, Hupkes, arXiv:2411.039232024arXiv preprint</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam Adam R Brown, Aditya Santoro, Adrià Gupta, Garriga-Alonso, arXiv:2206.046152022arXiv preprint</p>
<p>Functional benchmarks for robust evaluation of reasoning performance, and the reasoning gap. Saurabh Srivastava, P V Anto, Shashank Menon, Ajay Sukumar, Alan Philipose, Stevin Prince, Sooraj Thomas, arXiv:2402.194502024arXiv preprint</p>
<p>Challenging big-bench tasks and whether chain-of-thought can solve them. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Ed H Quoc V Le, Denny Chi, Zhou, arXiv:2210.092612022arXiv preprint</p>
<p>Challenging big-bench tasks and whether chain-of-thought can solve them. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, Findings of the Association for Computational Linguistics: ACL 2023. 2023</p>
<p>Atcoder Team, Atcoder, Gemma Team. Gemma, 2024a. 2012</p>
<p>Improving open language models at a practical size. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, arXiv:2408.00118Qwen Team. Qwen2.5: A party of foundation models. 2024. 2015. 2008. September 2024b2arXiv preprintLeetCode Team. Leetcode</p>
<p>Missed connections: Lateral thinking puzzles for large language models. Graham Todd, Tim Merino, Sam Earle, Julian Togelius, 2024</p>
<p>Llama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.092882023arXiv preprint</p>
<p>Mmlu-pro: A more robust and challenging multi-task language understanding benchmark. Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro Von Werra, Clémentine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M Rush, Thomas Wolf, ; Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, arXiv:2406.015742023. 2024arXiv preprintZephyr: Direct distillation of lm alignment</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS). the Annual Conference on Neural Information Processing Systems (NeurIPS)2022</p>
<p>Auto-Suggest: Learning-to-Recommend Data Preparation Steps Using Data Science Notebooks. Cong Yan, Yeye He, 10.1145/3318464.3389738Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data. the 2020 ACM SIGMOD International Conference on Management of DataPortland OR USAACMJune 2020</p>
<p>An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, arXiv:2407.10671Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zhihao Fan. Qwen2 technical report. Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu2024arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan, Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS). the Annual Conference on Neural Information Processing Systems (NeurIPS)202436</p>
<p>A careful examination of large language model performance on grade school arithmetic. Hugh Zhang, Jeff Da, Dean Lee, Vaughn Robinson, Catherine Wu, Will Song, Tiffany Zhao, Pranav Raja, Dylan Slack, Qin Lyu, arXiv:2405.003322024arXiv preprint</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Advances in Neural Information Processing Systems. 362024</p>
<p>Instruction-following evaluation for large language models. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, Le Hou, arXiv:2311.079112023aarXiv preprint</p>
<p>Instruction-following evaluation for large language models. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, Le Hou, arXiv:2311.079112023barXiv preprint</p>
<p>Starling-7b: Improving llm helpfulness &amp; harmlessness with rlaif. Banghua Zhu, Evan Frick, Tianhao Wu, Hanlin Zhu, Jiantao Jiao, November 2023</p>            </div>
        </div>

    </div>
</body>
</html>
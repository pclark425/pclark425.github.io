<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9570 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9570</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9570</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-165.html">extraction-schema-165</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or synthesize qualitative laws, principles, or generalizable rules from large numbers of scholarly input papers, including details of the methods, domains, evaluation, and results.</div>
                <p><strong>Paper ID:</strong> paper-278910800</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.20779v4.pdf" target="_blank">CHIMERA: A Knowledge Base of Scientific Idea Recombinations for Research Analysis and Ideation</a></p>
                <p><strong>Paper Abstract:</strong> A hallmark of human innovation is recombination -- the creation of novel ideas by integrating elements from existing concepts and mechanisms. In this work, we introduce CHIMERA, a large-scale Knowledge Base (KB) of over 28K recombination examples automatically mined from the scientific literature. CHIMERA enables large-scale empirical analysis of how scientists recombine concepts and draw inspiration from different areas, and enables training models that propose novel, cross-disciplinary research directions. To construct this KB, we define a new information extraction task: identifying recombination instances in scientific abstracts. We curate a high-quality, expert-annotated dataset and use it to fine-tune a large language model, which we apply to a broad corpus of AI papers. We showcase the utility of CHIMERA through two applications. First, we analyze patterns of recombination across AI subfields. Second, we train a scientific hypothesis generation model using the KB, showing that it can propose novel research directions that researchers rate as inspiring. We release our data and code at https://github.com/noy-sternlicht/CHIMERA-KB.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9570.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9570.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or synthesize qualitative laws, principles, or generalizable rules from large numbers of scholarly input papers, including details of the methods, domains, evaluation, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral-7B (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral-7B (fine-tuned via LoRA for recombination extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source 7B-parameter autoregressive transformer (Mistral-7B) fine-tuned with LoRA on a curated, expert-annotated corpus to perform end-to-end information extraction of 'recombination' relations from scientific abstracts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Mistral-7B</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>7B-parameter autoregressive transformer (Mistral-7B) fine-tuned with LoRA (low-rank adapters) on the CHIMERA annotated dataset; training ran for 500 steps on an NVIDIA RTX A6000 with batch size 1, max sequence length 4096, learning rate 6e-5 and weight decay 0.1.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Scientific information extraction / meta-science (AI research literature)</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Expert-annotated corpus of 500+ recombination-labeled abstracts used for fine-tuning; model applied at scale to arXiv CS abstracts (2019–2024) to extract >28K recombination edges forming the CHIMERA KB after filtering and normalization.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_type</strong></td>
                            <td>Patterns of idea recombination (domain-level regularities: inspiration vs. blend tendencies)</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_example</strong></td>
                            <td>Inspirations tend to span broader, cross-domain sources while blends typically combine concepts within the same or closely related domains (e.g., inspirations link more often to cognitive science and zoology; blends mainly occur intra-domain).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_methodology</strong></td>
                            <td>Supervised fine-tuning of an LLM (Mistral-7B) as an end-to-end IE model to jointly classify abstract-level recombination presence, relation type (blend vs. inspiration), and extract entity spans; training used soft matching evaluation and LoRA parameter-efficient fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Evaluation vs. held-out gold annotations with soft precision/recall/F1 for entities and relations; comparison to inter-annotator agreement; large-scale quality assessed by an LLM judge (GPT-4.1) validated against a domain expert; downstream utility validated via user study.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Fine-tuned Mistral-7B yielded the best performance of tested automatic methods across subtasks (classification, entity extraction, relation extraction) though humans still outperform models. The extraction pipeline produced a KB of 28K+ recombinations; large-sample LLM-judge estimated extraction accuracy ≈80.55%. Errors often involve subtle phrasing, boundary or uninformative spans, and multiple recombinations per abstract.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Compared against specialized models (SciBERT token classifier, PURE), few-shot GPT-4o, and GoLLIE zero-shot, the fine-tuned Mistral E2E model achieved the best automatic extraction performance reported; nonetheless, human inter-annotator agreement remains higher than all automated systems. Concept co-occurrence and general IE schemas (e.g., SciERC-style extraction) produced noisier, less precise recombination outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Extraction quality gaps remain relative to humans; abstraction-limited scope (only abstracts, not full papers); occasional uninformative or mislocalized entity spans; sensitivity to subtle or implied recombinations; some filtered leaks during query generation.</td>
                        </tr>
                        <tr>
                            <td><strong>bias_or_hallucination_issues</strong></td>
                            <td>The paper reports systematic extraction errors (uninformative spans, missed subtle recombinations) and leakage concerns during context extraction; no explicit large-scale hallucination claim, but reliance on LLM-based judgments required validation, and some automated outputs were judged noisy or borderline.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CHIMERA: A Knowledge Base of Scientific Idea Recombinations for Research Analysis and Ideation', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9570.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9570.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or synthesize qualitative laws, principles, or generalizable rules from large numbers of scholarly input papers, including details of the methods, domains, evaluation, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o / GPT-4o-mini / GPT-4.1 (auxiliary roles)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o family (GPT-4o, GPT-4o-mini) and GPT-4.1 used for auxiliary tasks (domain classification, context extraction, span similarity) and as a large-scale judge (GPT-4.1)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Proprietary GPT-4 family variants used for multiple auxiliary operations: extracting context strings, judging span similarity, assigning entity domains, reranking predictions, and (GPT-4.1) performing large-scale quality judgments validated against human experts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4o / GPT-4o-mini / GPT-4.1</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Proprietary GPT-4 family variants. GPT-4o-mini used for context extraction and span-similarity judgments; GPT-4o used for domain classification, reranking (via RankGPT) and auxiliary enrichment; GPT-4.1 used as a judge for large-scale correctness assessments. GPT-4o has an information cutoff (noted Oct 2023) affecting unfamiliarity with later test-set content.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Scientific information extraction, meta-science analysis, evaluation and data enrichment for AI literature</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Applied to the CHIMERA extracted examples and gold-annotated abstracts: used GPT-4o-mini to extract context strings from abstracts and detect leaks (22% pairs discarded), GPT-4o to classify entity domains (assign arXiv categories / broader branches), and GPT-4.1 to judge correctness on a 2,000-sample set; judge validated on 100 human-reviewed examples.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_type</strong></td>
                            <td>Synthesized empirical patterns across the KB (domain-level tendencies and statistics of recombination behavior)</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_example</strong></td>
                            <td>LLM-based analysis revealed that inspirations connect more frequently across diverse external disciplines (e.g., cognitive science → NLP) whereas blends more often link within the same arXiv subfield; temporal trends showed decreasing within-domain inspiration for cs.CL and increasing influence of cs.CL on cs.CV.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_methodology</strong></td>
                            <td>Prompt-based zero-shot/few-shot use: GPT-4o-mini for template-based context extraction and leak detection; GPT-4o zero-shot classification for assigning arXiv categories and broader scientific domains; GPT-4.1 used as a prompted judge following an evaluation template to assess entity meaningfulness and relation correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Validated LLM judge (GPT-4.1) by comparing to a human domain expert on 100 examples yielding F1=0.912; then used GPT-4.1 to evaluate 2,000 extracted examples (estimated 80.55% accuracy). Domain assignment quality and context-leak detection were spot-checked against human review (leak detection agreement ≈87% on 50 samples).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>GPT-4 variants enabled scalable auxiliary labeling and evaluation: reliably extracted contexts, assigned domains, detected query leaks, and judged extraction accuracy at scale. The LLM judge produced high agreement with a human expert and supported an estimated extraction accuracy of ≈80.55%. These tools also powered analyses that distilled generalizable patterns (inspiration vs. blend behaviors and temporal trends).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>LLM-based judgments and classifications were used in place of full manual review and were validated against human annotations; GPT-4.1 judge showed substantial agreement (F1=0.912) with expert labels, making it a practical proxy for larger-scale evaluation compared to purely manual assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Dependence on model cutoffs (GPT-4o info cutoff Oct 2023) and potential leakage from context extraction; some domain assignments labeled 'Other' due to ambiguous or noisy entities; human evaluation scale limited, so LLM judgments were necessary but not perfect.</td>
                        </tr>
                        <tr>
                            <td><strong>bias_or_hallucination_issues</strong></td>
                            <td>Authors note risks from uninformative or overly-general entity spans and borderline cases where LLM judgments may misclassify; they validate the judge but caution that LLM-based evaluation can propagate extraction noise and that leakage detection and domain classification require careful prompts and human spot checks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CHIMERA: A Knowledge Base of Scientific Idea Recombinations for Research Analysis and Ideation', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9570.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9570.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or synthesize qualitative laws, principles, or generalizable rules from large numbers of scholarly input papers, including details of the methods, domains, evaluation, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GoLLIE-13B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GoLLIE-13B (guided LLM for zero-shot IE with annotation guidelines)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 13B-parameter information-extraction model (GoLLIE) evaluated in zero-shot mode by supplying annotation-guideline templates to extract recombination relations without task-specific fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GoLLIE-13B</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>GoLLIE-13B is a model fine-tuned to follow annotation guidelines in zero-shot settings (finetuned from CODE-LLaMA2); in experiments authors applied GoLLIE-13B with guideline templates, using a single NVIDIA RTX A6000 (48GB), 1-beam search and a new-token limit of 128.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Information extraction from scientific abstracts (recombination extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Tested on the CHIMERA annotated corpus as a zero-shot end-to-end IE baseline by providing data-class style guidelines describing expected objects and properties to extract.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_type</strong></td>
                            <td>Extraction of explicit recombination relations (identifying blends and inspirations) rather than discovery of abstract qualitative laws, but used to surface structured relation instances that underpin later pattern analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_example</strong></td>
                            <td>Not reported as yielding distilled qualitative laws directly; used to extract candidate recombinations (e.g., extract 'inspiration: X -> Y') which contribute to the aggregated KB used to reveal patterns such as cross-domain inspiration prevalence.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_methodology</strong></td>
                            <td>Zero-shot guideline-driven prompting: providing annotation schema templates to GoLLIE-13B to extract recombination type and entity spans without fine-tuning; selection heuristics applied when multiple outputs returned.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compared as an extraction baseline to the fine-tuned Mistral-7B and other specialized models; performance assessed on the annotated test set with the same soft matching metrics and by running multiple few-shot trials for robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>GoLLIE provided a viable zero-shot approach for recombination extraction when supplied with clear guidelines, but it did not outperform the supervised fine-tuned Mistral-7B E2E model. It occasionally returned multiple recombinations; selection heuristics were applied to choose the first.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Served as a stronger zero-shot baseline relative to naive few-shot LLM prompting, but remained below supervised fine-tuned models and human annotation in extraction quality.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Zero-shot guideline method sometimes produced multiple extractions (<10% of cases) or missed subtle/reframed recombinations; guideline engineering and output post-processing were required.</td>
                        </tr>
                        <tr>
                            <td><strong>bias_or_hallucination_issues</strong></td>
                            <td>No explicit hallucination claims, but outputs could be noisy and required selection heuristics; quality depended on the guideline clarity and the model's ability to follow them.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CHIMERA: A Knowledge Base of Scientific Idea Recombinations for Research Analysis and Ideation', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9570.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9570.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or synthesize qualitative laws, principles, or generalizable rules from large numbers of scholarly input papers, including details of the methods, domains, evaluation, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RankGPT (GPT-4o reranker)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RankGPT with GPT-4o as reranker</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GPT-based reranking approach (RankGPT) using GPT-4o to re-score top retrieval candidates for recombination-prediction tasks, applied to the top-20 candidates from neural retrievers to improve final suggestion rankings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>RankGPT (with GPT-4o)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>RankGPT is a reranking pipeline leveraging GPT-4o to score and reorder a small candidate set (top-20) from a retriever; authors used GPT-4o with a window size of 10, step size 5 and a custom prompt; application cost reported (~$60 for reranking experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Recombination prediction / suggestion ranking for scientific ideation</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Applied to candidate suggestion lists produced by fine-tuned dense retrievers (all-mpnet-base-v2 finetuned, etc.) over CHIMERA graph nodes (12,751 test set nodes); reranker re-scored top-20 candidates for each query.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_type</strong></td>
                            <td>Improved ranking of candidate recombination suggestions (not a 'law' per se) enabling better presentation of plausible idea recombinations to users; indirectly supports discovery of general patterns by surfacing high-quality candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_example</strong></td>
                            <td>Using GPT-4o-based reranking increased Hits@10 for recombination prediction but sometimes reduced Hits@3/Hits@5 and MRR, due to promoting alternative plausible answers or semantically similar variants above the annotated gold.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_methodology</strong></td>
                            <td>Post-retrieval LLM reranking (RankGPT): top-20 candidates from a fine-tuned bi-encoder retriever are input to GPT-4o-based reranker with a modified prompt to rank candidates by relevance to the query and context.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Measured retrieval metrics (Hits@k, MRR, MedR) on an honest test split (post-2024) and performed qualitative error analysis on 30 representative reranker failure cases; user study also evaluated helpfulness of final suggestions.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>RankGPT improved Hits@10 but sometimes degraded stricter metrics (Hits@3, Hits@5, MRR) because it could prefer alternative plausible answers or paraphrases over the annotated gold. Error analysis highlighted cases of multiple valid answers and semantically similar variants.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Applied on top of strong fine-tuned retrievers (which had shown large gains from fine-tuning), RankGPT provided mixed gains: better at broad recall (Hits@10) but potentially harmful for top-ranked precision metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Reranker may lower the gold rank when multiple plausible answers are present or when semantically similar variants crowd the candidate list; cost considerations for using proprietary LLMs at scale were noted.</td>
                        </tr>
                        <tr>
                            <td><strong>bias_or_hallucination_issues</strong></td>
                            <td>Reranker errors stemmed from preference for plausible alternatives rather than hallucination; authors note redundancy/variant issues and the need to handle conceptual equivalence to avoid downgrading gold answers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CHIMERA: A Knowledge Base of Scientific Idea Recombinations for Research Analysis and Ideation', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>GoLLIE: Annotation guidelines improve zero-shot information-extraction <em>(Rating: 2)</em></li>
                <li>Is chatgpt good at search? investigating large language models as re-ranking agent <em>(Rating: 2)</em></li>
                <li>Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction <em>(Rating: 2)</em></li>
                <li>Scideator: Human-LLM scientific idea generation grounded in research-paper facet recombination <em>(Rating: 1)</em></li>
                <li>Accelerating innovation through analogy mining <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9570",
    "paper_id": "paper-278910800",
    "extraction_schema_id": "extraction-schema-165",
    "extracted_data": [
        {
            "name_short": "Mistral-7B (fine-tuned)",
            "name_full": "Mistral-7B (fine-tuned via LoRA for recombination extraction)",
            "brief_description": "An open-source 7B-parameter autoregressive transformer (Mistral-7B) fine-tuned with LoRA on a curated, expert-annotated corpus to perform end-to-end information extraction of 'recombination' relations from scientific abstracts.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_model_name": "Mistral-7B",
            "llm_model_description": "7B-parameter autoregressive transformer (Mistral-7B) fine-tuned with LoRA (low-rank adapters) on the CHIMERA annotated dataset; training ran for 500 steps on an NVIDIA RTX A6000 with batch size 1, max sequence length 4096, learning rate 6e-5 and weight decay 0.1.",
            "application_domain": "Scientific information extraction / meta-science (AI research literature)",
            "input_corpus_description": "Expert-annotated corpus of 500+ recombination-labeled abstracts used for fine-tuning; model applied at scale to arXiv CS abstracts (2019–2024) to extract &gt;28K recombination edges forming the CHIMERA KB after filtering and normalization.",
            "qualitative_law_type": "Patterns of idea recombination (domain-level regularities: inspiration vs. blend tendencies)",
            "qualitative_law_example": "Inspirations tend to span broader, cross-domain sources while blends typically combine concepts within the same or closely related domains (e.g., inspirations link more often to cognitive science and zoology; blends mainly occur intra-domain).",
            "extraction_methodology": "Supervised fine-tuning of an LLM (Mistral-7B) as an end-to-end IE model to jointly classify abstract-level recombination presence, relation type (blend vs. inspiration), and extract entity spans; training used soft matching evaluation and LoRA parameter-efficient fine-tuning.",
            "evaluation_method": "Evaluation vs. held-out gold annotations with soft precision/recall/F1 for entities and relations; comparison to inter-annotator agreement; large-scale quality assessed by an LLM judge (GPT-4.1) validated against a domain expert; downstream utility validated via user study.",
            "results_summary": "Fine-tuned Mistral-7B yielded the best performance of tested automatic methods across subtasks (classification, entity extraction, relation extraction) though humans still outperform models. The extraction pipeline produced a KB of 28K+ recombinations; large-sample LLM-judge estimated extraction accuracy ≈80.55%. Errors often involve subtle phrasing, boundary or uninformative spans, and multiple recombinations per abstract.",
            "comparison_to_baseline": "Compared against specialized models (SciBERT token classifier, PURE), few-shot GPT-4o, and GoLLIE zero-shot, the fine-tuned Mistral E2E model achieved the best automatic extraction performance reported; nonetheless, human inter-annotator agreement remains higher than all automated systems. Concept co-occurrence and general IE schemas (e.g., SciERC-style extraction) produced noisier, less precise recombination outputs.",
            "reported_limitations": "Extraction quality gaps remain relative to humans; abstraction-limited scope (only abstracts, not full papers); occasional uninformative or mislocalized entity spans; sensitivity to subtle or implied recombinations; some filtered leaks during query generation.",
            "bias_or_hallucination_issues": "The paper reports systematic extraction errors (uninformative spans, missed subtle recombinations) and leakage concerns during context extraction; no explicit large-scale hallucination claim, but reliance on LLM-based judgments required validation, and some automated outputs were judged noisy or borderline.",
            "uuid": "e9570.0",
            "source_info": {
                "paper_title": "CHIMERA: A Knowledge Base of Scientific Idea Recombinations for Research Analysis and Ideation",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "GPT-4o / GPT-4o-mini / GPT-4.1 (auxiliary roles)",
            "name_full": "GPT-4o family (GPT-4o, GPT-4o-mini) and GPT-4.1 used for auxiliary tasks (domain classification, context extraction, span similarity) and as a large-scale judge (GPT-4.1)",
            "brief_description": "Proprietary GPT-4 family variants used for multiple auxiliary operations: extracting context strings, judging span similarity, assigning entity domains, reranking predictions, and (GPT-4.1) performing large-scale quality judgments validated against human experts.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_model_name": "GPT-4o / GPT-4o-mini / GPT-4.1",
            "llm_model_description": "Proprietary GPT-4 family variants. GPT-4o-mini used for context extraction and span-similarity judgments; GPT-4o used for domain classification, reranking (via RankGPT) and auxiliary enrichment; GPT-4.1 used as a judge for large-scale correctness assessments. GPT-4o has an information cutoff (noted Oct 2023) affecting unfamiliarity with later test-set content.",
            "application_domain": "Scientific information extraction, meta-science analysis, evaluation and data enrichment for AI literature",
            "input_corpus_description": "Applied to the CHIMERA extracted examples and gold-annotated abstracts: used GPT-4o-mini to extract context strings from abstracts and detect leaks (22% pairs discarded), GPT-4o to classify entity domains (assign arXiv categories / broader branches), and GPT-4.1 to judge correctness on a 2,000-sample set; judge validated on 100 human-reviewed examples.",
            "qualitative_law_type": "Synthesized empirical patterns across the KB (domain-level tendencies and statistics of recombination behavior)",
            "qualitative_law_example": "LLM-based analysis revealed that inspirations connect more frequently across diverse external disciplines (e.g., cognitive science → NLP) whereas blends more often link within the same arXiv subfield; temporal trends showed decreasing within-domain inspiration for cs.CL and increasing influence of cs.CL on cs.CV.",
            "extraction_methodology": "Prompt-based zero-shot/few-shot use: GPT-4o-mini for template-based context extraction and leak detection; GPT-4o zero-shot classification for assigning arXiv categories and broader scientific domains; GPT-4.1 used as a prompted judge following an evaluation template to assess entity meaningfulness and relation correctness.",
            "evaluation_method": "Validated LLM judge (GPT-4.1) by comparing to a human domain expert on 100 examples yielding F1=0.912; then used GPT-4.1 to evaluate 2,000 extracted examples (estimated 80.55% accuracy). Domain assignment quality and context-leak detection were spot-checked against human review (leak detection agreement ≈87% on 50 samples).",
            "results_summary": "GPT-4 variants enabled scalable auxiliary labeling and evaluation: reliably extracted contexts, assigned domains, detected query leaks, and judged extraction accuracy at scale. The LLM judge produced high agreement with a human expert and supported an estimated extraction accuracy of ≈80.55%. These tools also powered analyses that distilled generalizable patterns (inspiration vs. blend behaviors and temporal trends).",
            "comparison_to_baseline": "LLM-based judgments and classifications were used in place of full manual review and were validated against human annotations; GPT-4.1 judge showed substantial agreement (F1=0.912) with expert labels, making it a practical proxy for larger-scale evaluation compared to purely manual assessment.",
            "reported_limitations": "Dependence on model cutoffs (GPT-4o info cutoff Oct 2023) and potential leakage from context extraction; some domain assignments labeled 'Other' due to ambiguous or noisy entities; human evaluation scale limited, so LLM judgments were necessary but not perfect.",
            "bias_or_hallucination_issues": "Authors note risks from uninformative or overly-general entity spans and borderline cases where LLM judgments may misclassify; they validate the judge but caution that LLM-based evaluation can propagate extraction noise and that leakage detection and domain classification require careful prompts and human spot checks.",
            "uuid": "e9570.1",
            "source_info": {
                "paper_title": "CHIMERA: A Knowledge Base of Scientific Idea Recombinations for Research Analysis and Ideation",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "GoLLIE-13B",
            "name_full": "GoLLIE-13B (guided LLM for zero-shot IE with annotation guidelines)",
            "brief_description": "A 13B-parameter information-extraction model (GoLLIE) evaluated in zero-shot mode by supplying annotation-guideline templates to extract recombination relations without task-specific fine-tuning.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_model_name": "GoLLIE-13B",
            "llm_model_description": "GoLLIE-13B is a model fine-tuned to follow annotation guidelines in zero-shot settings (finetuned from CODE-LLaMA2); in experiments authors applied GoLLIE-13B with guideline templates, using a single NVIDIA RTX A6000 (48GB), 1-beam search and a new-token limit of 128.",
            "application_domain": "Information extraction from scientific abstracts (recombination extraction)",
            "input_corpus_description": "Tested on the CHIMERA annotated corpus as a zero-shot end-to-end IE baseline by providing data-class style guidelines describing expected objects and properties to extract.",
            "qualitative_law_type": "Extraction of explicit recombination relations (identifying blends and inspirations) rather than discovery of abstract qualitative laws, but used to surface structured relation instances that underpin later pattern analysis.",
            "qualitative_law_example": "Not reported as yielding distilled qualitative laws directly; used to extract candidate recombinations (e.g., extract 'inspiration: X -&gt; Y') which contribute to the aggregated KB used to reveal patterns such as cross-domain inspiration prevalence.",
            "extraction_methodology": "Zero-shot guideline-driven prompting: providing annotation schema templates to GoLLIE-13B to extract recombination type and entity spans without fine-tuning; selection heuristics applied when multiple outputs returned.",
            "evaluation_method": "Compared as an extraction baseline to the fine-tuned Mistral-7B and other specialized models; performance assessed on the annotated test set with the same soft matching metrics and by running multiple few-shot trials for robustness.",
            "results_summary": "GoLLIE provided a viable zero-shot approach for recombination extraction when supplied with clear guidelines, but it did not outperform the supervised fine-tuned Mistral-7B E2E model. It occasionally returned multiple recombinations; selection heuristics were applied to choose the first.",
            "comparison_to_baseline": "Served as a stronger zero-shot baseline relative to naive few-shot LLM prompting, but remained below supervised fine-tuned models and human annotation in extraction quality.",
            "reported_limitations": "Zero-shot guideline method sometimes produced multiple extractions (&lt;10% of cases) or missed subtle/reframed recombinations; guideline engineering and output post-processing were required.",
            "bias_or_hallucination_issues": "No explicit hallucination claims, but outputs could be noisy and required selection heuristics; quality depended on the guideline clarity and the model's ability to follow them.",
            "uuid": "e9570.2",
            "source_info": {
                "paper_title": "CHIMERA: A Knowledge Base of Scientific Idea Recombinations for Research Analysis and Ideation",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "RankGPT (GPT-4o reranker)",
            "name_full": "RankGPT with GPT-4o as reranker",
            "brief_description": "A GPT-based reranking approach (RankGPT) using GPT-4o to re-score top retrieval candidates for recombination-prediction tasks, applied to the top-20 candidates from neural retrievers to improve final suggestion rankings.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_model_name": "RankGPT (with GPT-4o)",
            "llm_model_description": "RankGPT is a reranking pipeline leveraging GPT-4o to score and reorder a small candidate set (top-20) from a retriever; authors used GPT-4o with a window size of 10, step size 5 and a custom prompt; application cost reported (~$60 for reranking experiments).",
            "application_domain": "Recombination prediction / suggestion ranking for scientific ideation",
            "input_corpus_description": "Applied to candidate suggestion lists produced by fine-tuned dense retrievers (all-mpnet-base-v2 finetuned, etc.) over CHIMERA graph nodes (12,751 test set nodes); reranker re-scored top-20 candidates for each query.",
            "qualitative_law_type": "Improved ranking of candidate recombination suggestions (not a 'law' per se) enabling better presentation of plausible idea recombinations to users; indirectly supports discovery of general patterns by surfacing high-quality candidates.",
            "qualitative_law_example": "Using GPT-4o-based reranking increased Hits@10 for recombination prediction but sometimes reduced Hits@3/Hits@5 and MRR, due to promoting alternative plausible answers or semantically similar variants above the annotated gold.",
            "extraction_methodology": "Post-retrieval LLM reranking (RankGPT): top-20 candidates from a fine-tuned bi-encoder retriever are input to GPT-4o-based reranker with a modified prompt to rank candidates by relevance to the query and context.",
            "evaluation_method": "Measured retrieval metrics (Hits@k, MRR, MedR) on an honest test split (post-2024) and performed qualitative error analysis on 30 representative reranker failure cases; user study also evaluated helpfulness of final suggestions.",
            "results_summary": "RankGPT improved Hits@10 but sometimes degraded stricter metrics (Hits@3, Hits@5, MRR) because it could prefer alternative plausible answers or paraphrases over the annotated gold. Error analysis highlighted cases of multiple valid answers and semantically similar variants.",
            "comparison_to_baseline": "Applied on top of strong fine-tuned retrievers (which had shown large gains from fine-tuning), RankGPT provided mixed gains: better at broad recall (Hits@10) but potentially harmful for top-ranked precision metrics.",
            "reported_limitations": "Reranker may lower the gold rank when multiple plausible answers are present or when semantically similar variants crowd the candidate list; cost considerations for using proprietary LLMs at scale were noted.",
            "bias_or_hallucination_issues": "Reranker errors stemmed from preference for plausible alternatives rather than hallucination; authors note redundancy/variant issues and the need to handle conceptual equivalence to avoid downgrading gold answers.",
            "uuid": "e9570.3",
            "source_info": {
                "paper_title": "CHIMERA: A Knowledge Base of Scientific Idea Recombinations for Research Analysis and Ideation",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "GoLLIE: Annotation guidelines improve zero-shot information-extraction",
            "rating": 2,
            "sanitized_title": "gollie_annotation_guidelines_improve_zeroshot_informationextraction"
        },
        {
            "paper_title": "Is chatgpt good at search? investigating large language models as re-ranking agent",
            "rating": 2,
            "sanitized_title": "is_chatgpt_good_at_search_investigating_large_language_models_as_reranking_agent"
        },
        {
            "paper_title": "Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction",
            "rating": 2,
            "sanitized_title": "multitask_identification_of_entities_relations_and_coreference_for_scientific_knowledge_graph_construction"
        },
        {
            "paper_title": "Scideator: Human-LLM scientific idea generation grounded in research-paper facet recombination",
            "rating": 1,
            "sanitized_title": "scideator_humanllm_scientific_idea_generation_grounded_in_researchpaper_facet_recombination"
        },
        {
            "paper_title": "Accelerating innovation through analogy mining",
            "rating": 1,
            "sanitized_title": "accelerating_innovation_through_analogy_mining"
        }
    ],
    "cost": 0.019809999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>CHIMERA: A Knowledge Base of Scientific Idea Recombinations for Research Analysis and Ideation
29 Jul 2025</p>
<p>Noy Sternlicht 
School of Computer Science and Engineering
The Hebrew University of Jerusalem</p>
<p>Tom Hope 
School of Computer Science and Engineering
The Hebrew University of Jerusalem</p>
<p>The Allen Institute for AI
AI2)</p>
<p>Zonglin Yang 
Wanhao Liu 
Ben Gao 
Yujie Liu 
Wei Li 
CHIMERA: A Knowledge Base of Scientific Idea Recombinations for Research Analysis and Ideation
29 Jul 2025F4A43B63273861649DDEE85BC12F024EarXiv:2505.20779v4[cs.CL]Packaged resources to advance general chinese embedding. Preprint, arXiv:2309.07597.
A hallmark of human innovation is recombination-the creation of novel ideas by integrating elements from existing concepts and mechanisms.In this work, we introduce CHIMERA, a large-scale Knowledge Base (KB) of over 28K recombination examples automatically mined from the scientific literature.CHIMERA enables large-scale empirical analysis of how scientists recombine concepts and draw inspiration from different areas, and enables training models that propose novel, cross-disciplinary research directions.To construct this KB, we define a new information extraction task: identifying recombination instances in scientific abstracts.We curate a high-quality, expertannotated dataset and use it to fine-tune a large language model, which we apply to a broad corpus of AI papers.We showcase the utility of CHIMERA through two applications.First, we analyze patterns of recombination across AI subfields.Second, we train a scientific hypothesis generation model using the KB, showing that it can propose novel research directions that researchers rate as inspiring.We release our data and code at https://github.com/noy-sternlicht/CHIMERA-KB.</p>
<p>Introduction</p>
<p>Recombination-the creation of novel conceptual or physical solutions by combining existing mechanisms, methods, perspectives-is a widely recognized mechanism of ideation and innovation (Uzzi et al., 2013;Youn et al., 2015;Shi and Evans, 2023).It involves reinterpreting prior ideas by breaking them into components and blending them into new solutions (Knoblich et al., 1999;McCaffrey, 2012).This often requires forming abstract structural mappings across domains (Gentner et al., 1997;Gentner and Markman, 1997;Gentner and Kurtz, 2005;Chan et al., 2011;Frich et al., 2019)-e.g., as in bio-inspired algorithms that apply biological principles to computational problems.In this work, we introduce a new task: extracting recombinations from scientific papers.We present CHIMERA, a large-scale knowledge base (KB) of recombination examples automatically mined from papers.Figure 1 shows one such case, where a robotic design is inspired by animal mechanics.CHIMERA enables exploring, analyzing, and training models on such examples, capturing a fundamental pattern of human ingenuity.</p>
<p>Unlike simpler concept co-occurrence methods (Krenn et al., 2022) or general scientific extraction schemas (Luan et al., 2018), CHIMERA targets cases where authors explicitly describe recombination as central to their contribution.We focus on two broad recombination types: blends, which combine concepts into novel approaches (e.g., augmenting classical ML with quantum computing), and inspirations, where ideas from one domain spark solutions in another (e.g., using bird flock behavior to coordinate drones).CHIMERA captures both</p>
<p>Applications</p>
<p>Study how sources of inspiration for scientific disciplines vary over time Find all recombinations (e.g., blends) being used in specific topics.</p>
<p>Automatically predict new sources of inspiration for input problems.(2) the model extracts recombinations from arXiv abstracts to build a large-scale KB.Applications: CHIMERA supports diverse use cases, including computational ideation, exploration of recombination patterns across scientific domains, and meta-scientific analysis.</p>
<p>intra-and cross-domain cases, including analogies, abstractions, and reductions.</p>
<p>The resulting KB and methods enable diverse uses (Figure 2).In this paper, we focus on two applications that have seen growing interest in recent years: Science Analysis (Fortunato et al., 2018;Wahle et al., 2023;Pramanick et al., 2025) and Scientific Ideation (Wang et al., 2024;Si et al., 2024;Radensky et al., 2024;Garikaparthi et al., 2025).</p>
<p>Science Analysis.</p>
<p>We demonstrate how CHIMERA supports meta-scientific analysis (also known as science of science or scientometrics) (Fortunato et al., 2018): empirical studies of how innovation unfolds.Researchers conducting metascience analyses aim to understand how a field (e.g., AI) evolves over time and identify trends (e.g., emerging connections across areas).CHIMERA allows analysis of how ideas are combined within and across domains (Shi and Evans, 2019), and of how disciplines, topics and concepts inspire one another.This provides a direct and precise alternative to traditional citation-based (Wang et al., 2015;Myers et al., 2013;Wahle et al., 2023) or co-occurrence-based approaches (Frohnert et al., 2024), which are often coarse and noisy.Unlike these methods, CHIMERA allows to identify how a scientific idea is formed by blending concepts or by taking inspiration from another concept, unlocking new and also more granular analyses.</p>
<p>Scientific Ideation.We show how CHIMERA supports training and evaluating scientific hypothesis generation models (Wang et al., 2024), by learning from patterns of past recombinations to propose novel concept blends or inspirations (e.g., new analogical inspirations).Prior work has explored suggesting analogical recombinations via unsupervised discovery (Radensky et al., 2024;Hope et al., 2017); in contrast, CHIMERA provides the first largescale resource with real, author-described examples of how research problems were addressed via recombination.This enables supervised recombination models to observe many examples of how recombinations have been applied to specific problems (e.g., the cross-domain inspiration in Figure 1), and learn to suggest relevant blends or inspiration directions for new problems.</p>
<p>Finally, CHIMERA also enables faceted search and exploration (Katz et al., 2024).Researchers can search the KB to find cases of cross-domain inspirations within a topic of interest (e.g., search for all robotics ideas inspired by zoology), sparking new creative directions.</p>
<p>To conclude, our contributions are as follows:</p>
<p>• We present CHIMERA, the first knowledge base of idea recombination examples described by authors in scientific papers.CHIMERA distinguishes between two core types: blends and inspirations, enabling nuanced analysis in downstream tasks.</p>
<p>• We define a novel extraction task to identify recombinations in scientific abstracts, and release a high-quality, expert-verified dataset of 500+ manually annotated examples, accompanied by fine-tuned extraction baselines.</p>
<p>• We show CHIMERA's utility through two applications: a) Meta-scientific analysis of recombination patterns, and b) Computational ideation, where models trained on CHIMERA propose novel recombination directions.</p>
<p>Related Work</p>
<p>Recombinant creativity Blending concepts and analogical inspiration are core mechanisms of</p>
<p>Recombination extraction examples</p>
<p>Abstract: "...Current archaeology depends on trained experts to carry out bronze dating... we propose to integrate advanced deep learning techniques and archaeological knowledge..."</p>
<p>Blend: "advanced deep learning techniques" ←→ "archaeological knowledge" Abstract: "...Traditional approaches to enhance dialogue planning in LLMs, ... either face efficiency issues or deliver suboptimal performance.Inspired by the dual-process theory in psychology... we propose the Dual-Process Dialogue Planning (DPDP) framework..."</p>
<p>Inspiration: "the dual-process theory in psychology" −→ "enhance dialogue planning in LLMs".</p>
<p>Table 1: Example blend and inspiration.Note that blend is a symmetric relation, while inspiration is not.</p>
<p>ideation and innovation in cognitive science and creativity research (McKeown, 2014;201, 2019;Holyoak and Thagard, 1994).These processes involve combining or re-representing existing ideas to produce novel concepts and solutions.</p>
<p>Recent work explores how idea recombination can enhance LLM-powered ideation tools.For example, CreativeConnect (Choi et al., 2023) lets users recombine keywords to generate graphic sketches, while Luminate (Suh et al., 2023) supports recombination of dimensional values to produce diverse LLM responses.Scideator (Radensky et al., 2024) is another recent work that helps researchers explore ideas through interactive concept recombination.Other studies focus on recombining ideas from input and analogous artifacts (Srinivasan and Chan, 2024;Chilton et al., 2019) or searching for useful recombinations via iterative idea generation (Yang et al., 2025a,b).</p>
<p>In this work, we build CHIMERA, the first KB of scientific idea recombinations, and show how it enables a new approach for recombinant ideation: training models that learn from past examples of how ideas have been recombined in scientific texts, to suggest new recombination directions.</p>
<p>Scientific information extraction</p>
<p>Information extraction (IE) from scientific texts has been widely studied in NLP.A foundational resource is SciERC (Luan et al., 2018), which labels scientific entities (e.g., methods, tasks, metrics) and generic relations (e.g., conjunction) across 500 abstracts.Later datasets, such as SciREX (Jain et al., 2020) and SciDMTAL (Pan et al., 2024), expand IE to full documents, but similarly focus on standard schema involving scientific concepts and their relations.However, existing extraction approaches are not designed to capture recombination relationships, often resulting in noisy, irrelevant, or misleading outputs, as we illustrate in Appendix G, Figure 20.</p>
<p>In this work, we introduce a focused IE schema tailored specifically to idea recombination, along with a taxonomy that distinguishes between key recombination types: blend and inspiration.This enables a more precise and semantically rich analysis of cross-domain ideation.For instance, our knowledge base includes numerous analogical inspirations identified in AI research (Figure 1) -patterns that existing scientific IE schemas fail to capture.</p>
<p>Extracting Recombinations</p>
<p>Problem definition We focus on scientific abstracts where authors explicitly link their contribution to a novel combination or clear source of inspiration.As outlined in the introduction, we capture this with two coarse-grained relation types: blend and inspiration.Blend refers to the fusion of multiple concepts-such as methods, models, or theories-into a new solution or framework.We use the terms "concept blend" and "concept combination" interchangeably.Inspiration, by contrast, refers to transferring knowledge or insight from one entity (the source) to another (the target).This transfer may be realized through analogies, abstraction, or more general links to influential prior work.</p>
<p>Each relation is defined over free-form text spans that represent scientific concepts (see Figure 1; additional examples in Table 1).In blend relations, we refer to the participating entities as combinationelements; in inspiration relations, we refer to them as the inspiration-source and inspiration-target.This schema captures diverse recombination phenomena, such as metaphor, reduction, or abstraction (as illustrated in Appendix D.2) while remaining conceptually clear and efficient to annotate.It offers practical annotation advantages and strong alignment with ideation theory (McKeown, 2014;201, 2019;Holyoak and Thagard, 1994)  Table 2: Human-annotated corpus.We include also negative examples without recombinations ("not-present").</p>
<p>Recombination Mining</p>
<p>We begin by curating a dataset of annotated recombination examples, which we use to train an information extraction model.The trained model is then applied to extract recombinations at scale.This process is illustrated in Figure 2.</p>
<p>Data sourcing We annotate AI-related papers from the unarXive corpus (Saier and Färber, 2020) 1 .The data undergo an initial keyword-based filtering to identify works that are more likely to specify idea recombination.Table 8 in Appendix B.1 lists the keywords used in this process.We then assign the remaining abstracts to annotators.</p>
<p>Annotation process Our annotation setup follows standard IE practices, using two trained annotators and expert review to balance quality and feasibility (Naik et al., 2024;Sharif et al., 2024;Pramanick et al., 2025).Following a screening phase, we recruited two annotators with scientific PhDs via Upwork2 , selected from a pool of highly experienced workers we had previously collaborated with.Large-scale mining We use abstracts from the arXiv dataset4 , which updates monthly and includes more recent papers than unarXive (Saier and Färber, 2020).We apply our fine-tuned extraction model over publications from 2019-2024 within the same CS categories used for the annotation task.We then filter out predictions that don't conform to the data schema or cannot be parsed.</p>
<p>Categorization We apply GPT-4o to identify the scientific domain of each extracted entity given the abstract.This enables analyses we perform in Section 4.3.Further, each node is assigned a higherlevel discipline-either the arXiv group name (e.g., "computer-science" for cs.AI) or a relevant non-arXiv domain.Additional technical details regarding this step appear in Appendix C.</p>
<p>KB building</p>
<p>We normalize entities by clustering semantically similar ones.Next, we enrich each edge in the graph with the publication date and arXiv categories of the paper citing it.For simplicity, we focus on binary relations.A predicted entity may match at most one gold entity, and vice versa; extra matches are ignored.We compute precision, recall, and F1 under this soft matching.For relations, we use partial matching: a predicted relation contributes to the true positive count proportionally to the number of correctly matched entities in a gold relation of the same type.</p>
<p>We measure inter-annotator agreement using the same precision, recall, and F1 metrics, following standard practice in information extraction, where one annotator is treated as the gold reference (Naik et al., 2023;Sharif et al., 2024).</p>
<p>Extraction baselines</p>
<p>We evaluate several extraction baselines, including end-to-end (E2E) models that jointly predict whether an abstract discusses 5 Performed on par with GPT-4o.</p>
<p>recombination, identify its type, and extract the involved entities.In these models, the prediction of any relation is treated as a positive signal for abstract-level classification.We also assess specialized models for individual sub-tasks: Abstract classifiers, which predict whether the text discusses recombination, and Entity extractors, identify relevant entities.Implementation details for the baselines are in Appendix B.2.To contextualize model performance, we compare results against interannotator agreement, used as a proxy for humanlevel performance.Appendix A presents additional details concerning agreement computation.</p>
<p>Extraction Results</p>
<p>Table 4 reports results for abstract classification, entity extraction, and relation extraction.Human agreement scores are 0.760, 0.675, and 0.651 respectively, aligning with soft annotator agreement reported in similar complex extraction tasks (Naik et al., 2023;Sharif et al., 2024).Cohen's κ also indicates moderate to substantial agreement: κ = 0.578 for abstract classification, 0.631 for entity extraction, and 0.542 for relation extraction.Analysis of annotator disagreement is provided in Appendix A.1-most disagreements concern the presence of a recombination or the identification of its constituent entities, while disagreements over the recombination type are relatively rare.</p>
<p>Fine-tuning Mistral-7B on our data yields the  Figure 3: Recombinations between areas.cs.*, q-bio.ncand math.oc are arXiv categories.Inspirational connections are often cross-domain (Figure 3a), whereas blends tend to occur within the same domain (Figure 3b). Figure 3c zooms in on a few domains, for example, revealing that robotics often draws inspiration from zoology.</p>
<p>best performance across all subtasks.We observe that entity and relation extraction are more challenging than classification for both humans and SOTA LLMs.However, humans still significantly outperform automatic extraction approaches.Appendix B.5 presents an analysis of extraction errors.Interestingly, focusing on a smaller portion of the recombination extraction task is not necessarily easier than performing it end-to-end, as seen in the lower performance of abstract classifiers.We discuss this point further in Appendix B.3.</p>
<p>Large-scale evaluation</p>
<p>To assess extraction quality at scale, we evaluate 2, 000 CHIMERA examples using a strong LLM-based judge (GPT-4.1).An example is labeled correct if (1) the extracted entities reflect meaningful scientific concepts, and (2) their relation captures a central recombination explicitly described in the abstract.We first validate the judge's reliability by showing high agreement with human annotations on a representative subset.Applied to the full sample, the judge estimates an extraction accuracy of 80.55%, supporting the robustness of our approach.Notably, most extrac- ration and blend relations in CHIMERA (above the 0.9 quantile).The analysis reveals an interesting pattern of a distinct difference in behavior between inspirations and blends: inspirations span a broader range of domains, while blends tend to link within the same or similar domains.This suggests that when human researchers take inspiration they tend to look across more areas other than their own, but tend to look within their own domain when they build approaches by integrating together mechanisms.Inspirations also link more often to areas not covered by the arXiv taxonomy, e.g., cognitive science and zoology.More research building on our initial analysis and KB can shed additional light on the different ways in which scientists combine concepts to form ideas.  Figure 4 shows the percentage of target nodes in domains drawing inspiration from cs.CL (NLP) over five years.We observe two trends: a decrease in intra-domain inspiration (where cs.CL concepts inspire other cs.CL concepts), and an increase in cs.CV (Computer Vision) concepts drawing inspiration from cs.CL.</p>
<p>Recombination Prediction</p>
<p>We demonstrate how CHIMERA could be used to train supervised models that recombine concepts and generate novel scientific ideas.</p>
<p>Figure 5 illustrates the recombination prediction task.Given a context string (e.g., "Recent advancements in video generation have struggled to model complex narratives...") and a query about recombining a graph node (e.g., "What would be a good source of inspiration for video generation?") the goal is to predict a suitable entity to complete the recombination (e.g., "The concept of storyboarding...").Formally, given a query with a context string (e.g., a problem, experimental settings, goals), an entity e and a recombination type τ , the task is to predict a different entity e ′ such that (e, τ, e ′ ) is a valid edge in CHIMERA.</p>
<p>Data preparation</p>
<p>We start by converting edges to pairs of queries and answers.The queries de-Baseline H@3 H@5 H@10 H@50 H@100 MRR MedR Interestingly, reranking the top-20 answers using RankGPT boosts the H@10 but slightly reduces H@3,5 and MRR.</p>
<p>scribe the task inputs: a single graph node, the edge recombination type, and a context string, which we extract from the corresponding abstract using GPT-4o-mini.Note that this process might leak information regarding the answer (the other graph node) into the query.Therefore, we follow it by applying GPT-4o-mini to identify leakages (see examples and implementation details for this step in Appendix E.1).We discard approximately 22% of pairs due to leaks and split the remainder by publication year, with all papers published after 2024 in the test set.Table 5 summarizes the data splits.</p>
<p>Prediction We experiment with zero-shot and finetuned retrievers based on encoders trained before the test set cutoff year (2024).We next explore applying a GPT-4o-based reranker (Sun et al., 2023) to the top 20 retrieved results to improve our predictions further.The GPT-4o data cutoff is October 2023, meaning the reranker is also unfamiliar with our test set.Appendix E.2 provides additional implementation details for the prediction baselines.</p>
<p>Prediction Results</p>
<p>Table 6 presents our results.Fine-tuning greatly improves retrievers, decreasing the median rank of the gold answer by an order of magnitude.The last row reports results using RankGPT (Sun et al., 2023) with GPT-4o as a reranker, applied to the top-20 candidates from the best-performing retriever (all-mpnet-base-v2 finetuned ).While reranking improves Hits@10, it lowers performance on Hits@3, Hits@5, and MRR.These seemingly counterintuitive results are further examined in Appendix E.3.We find that the reranker can inadvertently lower the rank of the gold answer in cases where (i) multiple plausible answers are present, or (ii) the gold answer appears alongside semantically similar variants, making it difficult to distinguish between highly relevant alternatives and the annotated gold.User study We recruit five volunteers with verified research experience (at least one published paper) and assign them examples based on their expertise.Each example includes an inspiration query and suggestions from six sources: (1) Ours: our method, including reranking (2) Gold: the gold answer, (3) Random: a random test-set node, (4) GPT-4o: a GPT-4o generated suggestion, (5) ZS-CHIMERA: zero-shot prediction using our test nodes as candidates, and (6) ZS-SciERC: zero-shot prediction using SciERC-extracted candidates (Luan et al., 2018).For baselines returning a ranked list of suggestions, we only use the top result.</p>
<p>Annotators ranked baseline suggestions by their helpfulness in inspiring interesting ideas.Figure 6 reports the median and average rank across 100 examples, where lower values indicate better performance.Our approach receives a similar rank as the gold answer, and annotators prefer it to all other baselines.This gives a complementary signal to the automatic evaluation, showing that our recombination prediction approach learns to create helpful recombinations.Appendix F includes further study details and examples of model predictions that participants found especially inspiring.</p>
<p>Conclusions</p>
<p>We present CHIMERA, a novel knowledge base of 28K+ scientist-authored recombinations, capturing how scientists blend concepts and draw inspiration from different areas.CHIMERA supports a wide range of applications-we show its utility for metascientific analysis and for fine-tuning models that predict novel, inspiring recombination directions.</p>
<p>Limitations</p>
<p>Extraction quality As with any automaticallyextracted knowledge base, CHIMERA naturally contains some extraction errors (see Appendix B.5). Importantly, our work is the first to explore the new task of extracting recombinations from papers, revealing a gap between the performance of extraction models and humans on the task.As is the case with newly-introduced NLP tasks, future methods trained on our annotated corpus are expected to further improve extraction results, and hence the quality of CHIMERA.However, our analysis shows we already reach good extraction quality overall with minor errors (see Section 4.2), and our downstream applications further demonstrate that the data in CHIMERA can be used to derive utility in scientific meta-analysis and ideation.</p>
<p>Abstract-level scope CHIMERA focuses on extracting recombination instances from scientific abstracts rather than full papers.This design choice, common in scientific IE tasks (Gonzalez et al., 2023;Zhang et al., 2024;Naik et al., 2024), enables more scalable annotation and leverages the fact that abstracts typically summarize key contributions-including conceptual recombinations.In our setting, we focus on capturing cases where a recombination is at the core of a paper's contribution, hence likely to appear in the abstract.Confirming this intuition, we further conduct an analysis that finds that abstracts cover the vast majority of these cases.Extending extraction methods to full papers could reveal additional recombination patterns in future work.</p>
<p>Recombination prediction evaluation</p>
<p>As in other open-ended creative tasks (Jentzsch and Kersting, 2023;Meng et al., 2023;Huot et al., 2024), the recombination prediction task admits no single correct answer.Given a problem description, there are many valid ways to blend ideas or draw inspiration, which can lead to false negatives and an overly conservative estimate of model performance.To mitigate this, we conduct a complementary human evaluation.However, due to the expertise required from evaluators, the scale and depth of this assessment are necessarily limited.</p>
<p>Experimenting with additional models Our work leverages a diverse set of models for extraction and prediction, including open-source LLMs (e.g., Mistral-7B, all-mpnet-base), proprietary models (e.g., GPT-4o), and non-generative baselines (e.g., PURE).GPT-4o is used for auxiliary tasks, such as evaluation (judging entity span similarity), analysis (identifying entity's scientific domain), and to enrich our data (generating a context string for the extracted recombinations).As our primary focus is on building and analyzing the recombination knowledge base, we limit our experiments to these models.Exploring a broader range of models for these auxiliary tasks is an important direction for future work.</p>
<p>Ethical Considerations</p>
<p>To collect human-annotated recombination examples, we recruited crowdworkers through the Upwork platform.All annotators were informed in advance about the nature, purpose, and scope of the annotation task.They were compensated fairly for their time, at rates ranging from $26 to $30 per hour.Annotation quality was monitored through overlapping assignments and expert review to ensure reliability and accuracy.</p>
<p>For our human evaluation study, three volunteers with prior research experience participated in ranking model outputs.Participation was entirely voluntary, and no personal or identifying information about the annotators or participants is collected or disclosed.</p>
<p>To support transparency and reproducibility, we release our code, model checkpoints, and the annotated data under an open license.We used AIbased coding assistants (e.g., GitHub Copilot) and language tools for minor code and grammar refinements during development.</p>
<p>A Annotator Agreement</p>
<p>Following standard practice in information extraction (Naik et al., 2023;Sharif et al., 2024), we assess inter-annotator agreement using precision, recall, and F1 scores.Agreement is computed by treating one annotator's labels as the reference and the other's as predictions.In addition to measuring entity-level and relation-level agreement, we also evaluate agreement on recombination presence-that is, whether a text expresses a recombination instance, regardless of its type.</p>
<p>We apply the soft entity and relation matching procedure described in Section 4.1 to compute entity and relation agreement.All agreement scores are based on the 49 documents annotated by both annotators (approximately 10% of the full dataset).We treat these agreement measures as a proxy for human-level performance on this task.</p>
<p>A.1 Disagreement Analysis</p>
<p>To better understand the sources of annotation disagreement, we conducted a qualitative analysis.The most common cause stems from differences in identifying whether a recombination is present at all (see examples in Table 7).In such cases, disagreements were resolved via discussion and expert adjudication.The primary criterion for resolution was whether the authors explicitly describe a recombination as contributing to their approach.</p>
<p>Interestingly, once annotators agreed that a recombination was present, they rarely disagreed on its type, and only a single example exhibited this form of conflict.However, disagreements over which entities the recombination includes were more frequent.These typically fell into two categories:</p>
<ol>
<li>
<p>Boundary disagreements, where annotators selected different spans with overlapping meaning.Here, the expert favored the span that preserved more context (e.g., "reinforcement learning which uses traditional time series stock price data" was preferred over "traditional time series stock price data").</p>
</li>
<li>
<p>Conceptual disagreements, where annotators identified fundamentally different entities.These were resolved through further discussion and clarification.</p>
</li>
</ol>
<p>B Additional Extraction Details B.1 Recombination keywords</p>
<p>We use keyword-based filtering to identify works that are more likely to discuss recombination before assigning papers to human annotators.Resolution: Upon expert review, Annotator 1's interpretation was selected, as the authors explicitly describe how the two sources of data serve complementary roles in their method.</p>
<p>Abstract: "...we propose an integrated system that can perform large-scale autonomous flights and real-time semantic mapping in challenging under-canopy environments.We detect and model tree trunks and ground planes from LiDAR data, which are associated across scans and used to constrain robot poses as well as tree trunk models.The autonomous navigation module utilizes a multi-level planning and mapping framework and computes dynamically feasible trajectories that lead the UAV to build a semantic map of the user-defined region of interest in a computationally and storage efficient manner.A drift-compensation mechanism is designed to minimize the odometry drift using semantic SLAM outputs in real time, while maintaining planner optimality and controller stability..."</p>
<p>Annotator 1: [Blend: "LiDAR data" ←→ "a multi-level planning and mapping framework"] Annotator 2: [] Resolution: Annotator 2's judgment was selected after expert review, as the relation between the two components is not clearly described as a recombination.</p>
<p>Your paragraph text</p>
<p>You are an AI assistant tasked with analyzing scientific abstracts for idea recombination.Your goal is to identify the most salient recombination in the given abstract and format it as a JSON string.Follow these instructions carefully:</p>
<ol>
<li>First, familiarize yourself with the possible entity types for recombinations: <entity_types> combination-element: An idea, method, model, technique, or approach combined in the text with other elements.inspiration-source: A concept, idea, problem, approach, or domain the authors drew inspiration from.inspiration-target: A concept, idea, problem, approach, or domain in which the authors utilize the inspiration they drew from the inspiration source.2021), which we use with the default rank of 64.</li>
</ol>
<p>The evaluation uses the corresponding repository, mistral-inference7 .We rerun the same experiment using Llama-3.1-8B as a backbone, using an additional 500 warm-up steps, a learning rate of 2e − 5 and a weight decay of 0.01. Figure 7 presents the prompt for these experiments.</p>
<p>@dataclass class Inspiration(Template): """An inspiration describes drawing inspiration or similarities from one concept, idea, problem, approach, or domain and implementing it in another.For example, taking inspiration from the human brain to design a learning algorithm, performing a reduction from one problem to another, or using a technique from one domain in another."""inspiration_src: str # The source of the inspiration (e.g., the human brain) inspiration_target: str # The target of the inspiration (e.g., a learning algorithm) @dataclass class Combination(Template):</p>
<p>"""A combination describes joining two ideas, methods, models, techniques to obtain a certain goal.For example, combining two models to improve performance, combining two methods to solve a problem, or combining two ideas to create a new concept."""comb_element_1: str # The first element of the combination (e.g., model A) comb_element_2: str # The second element of the combination (e.g., model B) In addition to fine-tuning LLMs on our data, we experiment with GoLLIE (Sainz et al., 2023), a general IE model fine-tuned to follow any annotation guidelines in a zero-shot fashion.We apply GollIE-13B on our data, using a single NVIDIA RTX A6000 48GB GPU, 1-beam search, and limit the new token number to 128.GoLLIE is finetuned from CODE-LLaMA2, and receives guidelines in the form of data classes describing what objects and properties the model should extract.Figure 8 depicts the guidelines we used to test GoLLIE as an E2E recombination extraction model.In the rare cases where the model returns more than a single recombination type (&lt; 10), we select the first.</p>
<p>We also experiment with GPT-4o in few-shot settings.We select 45 examples for each example type (blend, inspiration, not-present) from the training data (a total of 135).As  9 presents the prompt for this experiment.</p>
<p>Specialized baselines</p>
<p>The recombination extraction model has to execute multiple tasks at once (classifying the document, extracting entities, inferring relations), which might be more challenging than performing them separately.To explore this question, we examine our model classification and extraction abilities against designated models for each task.We use Mistral-7B as a specialized classifier and experiment with two versions of the training data.The first includes binary responses (present, not-present), while the other contains a short CoT-style analysis string as well as the gold class.We construct the analysis string by incorporating the human entity annotations into predetermined templates (e.g., "This paper discusses a recombination since the authors take inspiration from [inspiration-source] and implement it in [inspiration-target]").</p>
<p>To evaluate entity extraction, we compare our model against GPT-4o in few-shot settings and include 45 cases per example type, similarly to the E2E experiment.To account for variability due to example selection, we run each experiment 5 times, sampling a new set of few-shot examples in each, and report the average.The total cost of this process sums up to 50$.The prompt template for this experiment is available on Figure 10.</p>
<p>We experiment with non-generative approaches as well, and compare our model to a SciBERT (Zhong and Chen, 2021) based token classifier.The encoder uses a standard Hugging-Face implementation of SciBERT, which we train on a single NVIDIA RTX A6000 48GB GPU over 500 steps.We use a weight decay of 0.1, a learning rate of 6.e − 5 and a batch size of 1.We also experiment with PURE (Zhong and Chen, 2021), a well-known information extraction baseline.We finetune PURE over our train set using the default parameters, except for max_span_length, which we set to 40 to accommodate for the longer entities in our data.</p>
<p>B.3 E2E vs Specialized extraction</p>
<p>This section reflects on the results described in Section 4, drawing on implementation details of the baselines (described in Appendix B.2).In Section 4, we observe that narrowing the focus to a smaller portion of the recombination extraction task does not always improve performance -in fact, it can lead to worse results.This pat-</p>
<p>Your paragraph text</p>
<p>You are an AI assistant tasked with analyzing scientific abstracts for idea recombination.Your goal is to identify the most salient recombination in a given abstract and format it as a JSON string.Follow these instructions carefully:</p>
<ol>
<li>First, familiarize yourself with the possible entity types for recombinations: <entity_types> comb-element: An idea, method, model, technique, or approach combined in the text with other elements.inspiration-src: A concept, idea, problem, approach, or domain the authors drew inspiration from.inspiration-target: A concept, idea, problem, approach, or domain in which the authors utilize the inspiration they drew from the inspiration source.</entity_types> 2. Review the following examples to understand the expected output format and the process of identifying recombinations: <examples>{EXAMPLES}</examples> 3. Now, carefully read the following scientific abstract: <abstract>{TEXT}</abstract> 4. Your task is to extract the most salient recombination from this abstract.A recombination can be either: a) Combination: The authors combine two or more ideas, methods, models, techniques, or approaches to obtain a certain goal.b) Inspiration: The authors draw inspiration or similarities from one concept, idea, problem, approach, or domain and implement it in another.5.After identifying the recombination, you will format it as a JSON string in the following structure: <recombination>{recombination_type: {entity_type_1: [ent_1, ent_2], entity_type_2: [ent_3],...}}</recombination> If you don't think the text discusses a recombination, or that the recombination is not a central part of the work, return an empty JSON object: {}. 6.Before providing your final answer, use the following scratchpad to think through the process: <scratchpad> 1. Identify the main ideas, methods, or approaches discussed in the abstract.2. Determine if there is a clear combination of ideas or if one idea inspired the application in another domain.3. Identify the specific entities involved in the recombination.4. Classify the entities according to the provided entity types.5. Determine the recombination type (combination or inspiration).</scratchpad> 7. Now, provide your final output in the specified JSON format.Ensure that the output is a valid JSON string.If the output is empty, return {}.Place your answer within <recombination> tags.</li>
</ol>
<p>Remember to carefully analyze the abstract and only identify a recombination if it is clearly present and central to the work described.You are tasked with identifying specific types of entities in a given scientific abstract.The entity types you need to identify are:</p>
<ol>
<li>comb-element: An idea, method, model, technique, or approach combined in the text with other elements.2. inspiration-src: A concept, idea, problem, approach, or domain the authors drew inspiration from.3. inspiration-target: A concept, idea, problem, approach, or domain in which the authors utilize the inspiration they drew from the inspiration source.</li>
</ol>
<p>Here is the text you need to analyze: <text>{TEXT}</text> Please read the text carefully and identify all entities that belong to the types listed above.Pay close attention to the context and relationships between concepts to accurately categorize each entity.</p>
<p>After identifying the entities, you should output them in a valid JSON format.Use the entity types as keys and lists of entities as values.For example: {"comb-element": ["entity1", "entity2"], "inspiration-src": ["entity3"], "inspiration-target": ["entity4", "entity5"]} Ensure that your JSON output is valid: -Use double quotes around strings -Do not include a trailing comma after the last item in a list or object -Escape any double quotes that appear within entity names Enclose your final JSON output in <output_json> tags.</p>
<p>Remember to review your output for accuracy and completeness before submitting your final answer.tern emerges across three Mistral-based classifiers: the end-to-end version (E2E), the specialized version (Abstract-classifier), and the specialized version trained with synthetic CoT strings (Abstractclassifier-CoT).We hypothesize that identifying recombination relations in text may be analogous to Chain-of-Thought prompting (CoT), a technique known to enhance LLM performance across various tasks (Wei et al., 2022).This hypothesis is supported by the superior performance of Abstractclassifier-CoT compared to its non-CoT counterpart.</p>
<p>B.4 Span similarity</p>
<p>We provide our span similarity prompt in Figure B.4.We use it in the extraction evaluation process as discussed in Section 4.1.To mitigate position bias, we query the model twice per pair with reversed orderings, accepting a match only if both judgments are positive.We prefer GPT-4o-mini over GPT-4o based on a comparison which found only 3 disagreements across the test set.</p>
<p>You are tasked with comparing two spans extracted from a scientific text to determine if they discuss the same {ENTITY_TYPE}.Follow these instructions carefully:</p>
<ol>
<li>
<p>First, read the full text for context: <full_text>{TEXT}</full_text> 2. Now, consider these two spans extracted from the text above: <span1>{SPAN1}</span1> <span2>{SPAN2}</span2> 3.Your task is to carefully analyze these two spans and determine if they discuss the same {ENTITY_TYPE}.The idea the spans discuss should be exactly the same, up to minor lexical or semantic variations.</p>
</li>
<li>
<p>In your analysis, consider the following: a.The main topic or idea presented in each span b.The context in which these spans appear in the full text c.Any potential contradictions between the spans 5.After your analysis, provide a justification for your determination.Explain your reasoning clearly, referencing specific elements from the spans and the full text if necessary.</p>
</li>
<li>
<p>Based on your analysis and justification, provide a "Yes" or "No" answer to whether the spans discuss the same {ENTITY_TYPE}.</p>
</li>
<li>
<p>Present your response in the following format: <justification>[Your detailed justification here]</justification> <answer>[Your "Yes" or "No" answer here]</answer> Figure 11: Span similarity prompt.{ENTITY_TYPE} is either "combination-element", "inspiration-source" or "inspiration-target". {TEXT} is a placeholder for the paper's abstract.{SPAN1}, {SPAN2} are placeholders for the compared spans.</p>
</li>
</ol>
<p>B.5 Extraction error analysis</p>
<p>We perform analysis over the test set, revealing different sources of error which may inspire future improvements.Our focus is on understanding how different types of input texts can influence the result, specifically, in cases where the extraction model struggles.We use our best-performing fine-tuned E2E model for this analysis.</p>
<p>Context dependent or subtle phrasing</p>
<p>We observe that, unsurprisingly, cases in which the recombination is implied or subtle are more challenging for the model.For instance (see also Table 9, row 1), "Kahneman &amp; Tversky's prospect theory" inspires the design of a loss function that "directly maximizes the utility of generations", but this is not stated directly.Moreover, abstracts that express idea recombination while referencing previously mentioned entities are also harder to detect.</p>
<p>Multiple recombinations Some papers present a salient recombination along with other insignificant ones.We notice that in those cases, the model might extract a non-salient recombination or mix multiple ones (see Table 9, row 2 for such a case).Abstract: "...In order to characterize model flaws and choose a desirable representation, model builders often need to compare across multiple embedding spaces, a challenging analytical task supported by few existing tools.We first interviewed nine embedding experts in a variety of fields to characterize the diverse challenges they face and techniques they use when analyzing embedding spaces.Informed by these perspectives, we developed a novel system called Emblaze that integrates embedding space comparison within a computational notebook environment..." Gold = [Blend: "embedding space comparison" ←→ "...notebook environment"] Pred = [] Table 9: In the first row, the extraction model misses an inspiration relation because of subtle phrasing.In the second row, when analyzing an abstract with multiple recombinations, the model fails to identify the most important one and confuses entities across different relations.In the third row, the model fails to detect a weak recombination example.</p>
<p>Borderline cases</p>
<p>The role of a recombination as a core element in the work is sometimes debatable.Table 9, row 3 presents an example of such a case where the authors explicitly mention integrating "embedding space comparison" with "computational notebook environment", which may be interpreted as a recombination (the usage of notebook in these environments is completely new and novel), or simply as a way to present the tool's environment.We notice that the extraction model tends to miss those cases.</p>
<p>B.6 Extraction examples</p>
<p>Table 10 presents examples of interdisciplinary, automatically extracted inspiration recombinations.</p>
<p>B.7 Large-scale extraction assessment</p>
<p>You are tasked with reviewing outputs from an information extraction system that processes scientific abstracts.Your job is to assess whether a particular extracted relation and its associated entities are both meaningful and accurate, according to strict scientific criteria.Below is the information you will use for your review: combination: A combination indicates that the authors describe joining together two or more ideas, methods, models, techniques, or approaches to achieve a specified goal.inspiration: Inspiration means the authors describe taking inspiration, analogy, abstraction, reduction, reformulation, or similarities from one concept, idea, problem, approach, or domain and applying it to another.Please evaluate the model's extraction according to the two criteria below: Criterion 1: Do the extracted entities (Entity 1 and Entity 2) each represent clear, meaningful scientific concepts, methods, models, problems, or approaches?Criterion 2: Is the stated relation (combination or inspiration) one that described in the abstract as occurring between these two entities?Provide your answer by filling in the YES/NO values for each criterion below: <answer> { "1": "[YES/NO]", "2": "[YES/NO]" } </answer> Replace [YES/NO] with your judgment for each criterion.Make sure your response is a valid JSON object and fits the format exactly.Do not provide additional explanation or commentary outside of the JSON object.To complement our human annotation efforts and enable large-scale evaluation, we conducted a qualitative assessment of the automatically extracted recombination examples in CHIMERA using GPT-4.1 as an LLM-based judge.</p>
<p>Validating the LLM Judge.We first assessed GPT-4.1'sreliability by comparing its judgments against those of a domain expert.A PhD student with NLP expertise manually reviewed 100 randomly sampled recombination examples and labeled each as correct if: (1) the extracted entities corresponded to meaningful scientific concepts, and (2) the relation between them captured a central recombination explicitly described in the abstract.Upon analyzing the identified extraction errors, we observe a significant portion stems from extracting correct recombinations with uninformative entities (criteria 2) and not from a conceptual misunderstanding of the text.We provide examples of such cases in Table 11.</p>
<p>GPT-4.1 was prompted with the same examples using an evaluation template aligned with the assessment criteria (see Figure 12).Given the imbalance nature of the data, we report the F1 score instead of Cohen's κ, following the recommendations of previous work (Delgado and Tibau, 2019).The resulting F1 score of 0.912 indicates substantial agreement, supporting the use of GPT-4.1 as a reliable proxy for large-scale quality assessment.</p>
<p>Large-Scale Evaluation.Following validation, we applied GPT-4.1 to a larger sample of 2,000 automatically extracted examples from CHIMERA.The model labeled 799 of these examples as correct, resulting in an estimated extraction accuracy of 80.55%.These results provide further evidence for the overall quality and robustness of our extraction pipeline.</p>
<p>C Graph nodes domains</p>
<p>We identify the scientific domain of each entity using GPT-4o in a zero-shot setting.Given the abstract and the extracted recombination entities, the model assigns to each entity an arXiv category and a broader scientific branch.If the model successfully assigns an arXiv category, we treat it as the entity's domain.</p>
<p>Otherwise, the model selects a branch from a predefined list of outer-arXiv domains (see Table 12) and sets it as the domain.If neither a standard arXiv category nor a branch can be assigned, the entity is labeled as belonging to the Other domain.</p>
<p>Entities in the Other domain are excluded from the analysis in Section 4.3, as they are often too noisy, overly broad, or miscellaneous to interpret reliably.Figures 13 and 14  Bad Extraction Examples (arXiv)</p>
<p>Abstract: "The detection of allusive text reuse is particularly challenging due to the sparse evidence on which allusive references rely-commonly based on none or very few shared words.Arguably, lexical semantics can be resorted to since uncovering semantic relations between words has the potential to increase the support underlying the allusion and alleviate the lexical sparsity.A further obstacle is the lack of evaluation benchmark corpora, largely due to the highly interpretative character of the annotation process.In the present paper, we aim to elucidate the feasibility of automated allusion detection.We approach the matter from an Information Retrieval perspective in which referencing texts act as queries and referenced texts as relevant documents to be retrieved, and estimate the difficulty of benchmark corpus compilation by a novel inter-annotator agreement study on query segmentation..."</p>
<p>Automatic extraction (incorrect entities): [Inspiration: "In an Information Retrieval perspective, referencing texts act as queries and referenced texts as relevant documents to be retrieved" −→ "a task-oriented dialog system"] Abstract: "Supervised deep learning with pixel-wise training labels has great successes on multi-person part segmentation.However, data labeling at pixel-level is very expensive.To solve the problem, people have been exploring to use synthetic data...the results are much worse compared to those using real data and manual labeling.The degradation of the performance is mainly due to the domain gap, i.e., the discrepancy of the pixel value statistics between real and synthetic data.In this paper, we observe that real and synthetic humans both have a skeleton (pose) representation.We found that the skeletons can effectively bridge the synthetic and real domains during the training.Our proposed approach takes advantage of the rich and realistic variations of the real data and the easily obtainable labels of the synthetic data to learn multi-person part segmentation on real images without any human-annotated labels... "</p>
<p>Automatic extraction (uninformative entities): [Combination: "real" ←→ "synthetic humans"] Abstract: "...In this paper we propose an LLM feature-based framework for dialogue constructiveness assessment that combines the strengths of feature-based and neural approaches, while mitigating their downsides.The framework first defines a set of dataset-independent and interpretable linguistic features, which can be extracted by both prompting an LLM and simple heuristics.Such features are then used to train LLM feature-based models...We also find that the LLM feature-based model learns more robust prediction rules instead of relying on superficial shortcuts, which often trouble neural models."</p>
<p>Automatic extraction (uninformative entities): [Combination: "neural" ←→ "featurebased"]  12: Non-arXiv scientific domains.We complement arXiv category taxonomy using a broader list of scientific fields.</p>
<p>Your paragraph text</p>
<p>You are an AI assistant tasked with analyzing a scientific abstract to determine the arXiv categories and scientific branches of combined elements.</p>
<p>Your goal is to identify the most appropriate arxiv taxonomy category and most suitable scientific domain for each element provided.</p>
<p>Here is the abstract you will be analyzing: <abstract>{ABSTRACT}</abstract> And here is the list of combined elements identified from the abstract: <elements>{ELEMENTS}</elements></p>
<p>Here is a list of the standard arXiv categories: <arxiv>{ARXIV}</arxiv> And here is a list of scientific branches: <branches>{BRANCHES}</branches></p>
<p>For each element in the list, you need to: 1. Identify the best matching arXiv taxonomy category from the provided list.If it doesn't match any category, use "other".If there's insufficient information, use "insufficient-info". 2. Identify the scientific branch from the provided branches list.If there's insufficient information, use "insufficient-info".If no branch name in the list describes the source properly, use "other".</p>
<p>Return your output in the following format: <output> [{"text": "element1", "arxiv_category": "category1", "scientific_branch": "branch1"}, {"text": "element2", "arxiv_category": "category2", "scientific_branch": "branch2"}, ...] </output> Format your response as a valid JSON string.</p>
<p>Now, analyze the provided elements from the abstract and generate your response in the specified JSON format.Make sure to include all elements from the provided list, and ensure that your output is properly formatted as a valid JSON string.</p>
<p>Figure 13: blend domain analysis prompt.{ELEMENTS} is a placeholder for the recombination entities extracted from {ABSTRACT}.{ARXIV} is a placeholder for full arXiv category names and their descriptions.{BRANCHES} is a placeholder for the list of non-arXiv domains given in Appendix C, Table 12.</p>
<p>Your paragraph text</p>
<p>You will be analyzing the scientific branches and arXiv taxonomy categories of an inspiration source and target based on an abstract from a scientific paper.Here's the information you'll be working with:
<abstract>{ABSTRACT}</abstract> <inspiration_source>{INSPIRATION_SOURCE}</inspiration_source> <inspiration_target>{INSPIRATION_TARGET}</inspiration_target> <arxiv>{ARXIV}</arxiv> <branches>{BRANCHES}</branches>
Your task is to identify the arXiv taxonomy category and most suitable scientific branch for both the inspiration source and the inspiration target.</p>
<p>For the inspiration source: 1. Identify the best matching arXiv taxonomy category from the provided list.If it doesn't match any category, use "other".If there's insufficient information, use "insufficient-info". 2. Identify the scientific branch from the provided branches list.If there's insufficient information, use "insufficient-info".If no branch name in the list describes the source properly, use "other".</p>
<p>Repeat the same process for the inspiration target.</p>
<p>Provide your analysis in the following format:</p>
<p><source-branch>[Insert the scientific branch of the inspiration source here]</source-branch> <source-arXiv>[Insert the arXiv taxonomy category of the inspiration source here]</source-arXiv> <target-branch>[Insert the scientific branch of the inspiration target here]</target-branch> <target-arXiv>[Insert the arXiv taxonomy category of the inspiration target here]</target-arXiv> Ensure that you only include the requested information within each tag, without any additional explanation or reasoning.</p>
<p>Figure 14: inspiration domain analysis prompt.{INSPIRATION_SOURCE} and {INSPIRATION_TARGET} are placeholders for the inspiration entities extracted from {ABSTRACT}.{ARXIV} is a placeholder for full arXiv category names and their descriptions.{BRANCHES} is a placeholder for the list of non-arXiv domains given in Appendix C, Table 12.</p>
<p>respectively.Running this classification process over the full corpus cost approximately 250$.</p>
<p>The Other domain We assign the Other domain to nodes the model fails to classify.In total, 2,127 graph nodes fall into this category.We manually examined a sample of 150 such nodes and found that many were either too ambiguous or too general to categorize meaningfully.Interestingly, some of these nodes refer to non-academic or highly niche concepts (see examples in Table 13).</p>
<p>domain grouping To avoid sparsity, we group similar domains as displayed in Table 14.Table 15 presents the node distribution of common domains after applying this grouping process.</p>
<p>D Additional Knowledge Base Analysis D.1 Predominant recombination relations</p>
<p>We provide a tabular version of Figure 3 in Section 4.3 on Table 16 for better readability.</p>
<p>D.2 Nuanced recombination types</p>
<p>In Section 3, we defined the two broad recombination types used in CHIMERA: blends and inspirations.</p>
<p>In this section, we demonstrate that this taxonomy is both robust and expressive, offering broad coverage of more nuanced recombination phenomena.</p>
<p>To support this, we perform a qualitative analysis of 30 inspiration examples from the CHIMERA dataset.We identify distinct subtypes of inspiration, such as analogy, metaphor, reduction, abstraction, and application of existing knowledge.These subtypes emerge naturally within our current schema, illustrating its extensibility and broad coverage.Table 17 provides examples for each.This analysis lays the groundwork for future refinement and expansion of the taxonomy.</p>
<p>E Additional Prediction Details E.1 Prediction data preprocessing</p>
<p>Context extraction and leakage filtering We use GPT-4o-mini to extract a few sentences from each abstract describing the background or motivation of the authors using recombination (See prompt on Figure 15).Adding these contexts to the queries helps them be more specific and limits the search space.However, this might introduce leaks into the queries -cases where the extracted context reveals the answer.Table 18 presents leak examples.We utilize GPT-4o-mini again to filter out such cases from the data, using the prompt shown in Figure 16.In a qualitative analysis of 50 randomly sampled query-answer pairs, we find that a human annotator agrees with 87% of the model's predictions (whether there is a leak).Finally, we divide the remaining query-answer pairs into splits as described in Table 5 is Section 5.</p>
<p>E.2 Prediction baselines</p>
<p>We use a bi-encoder architecture for recombination prediction and experiment with three popular encoders as backbones: all-mpnet-base-v2 (109M parameters), bge-large-en-v1.5 (Xiao et al., 2023) (335M parameters) and e5-large-v2 (Wang et al., 2022) (335M parameters).These models' checkpoints predate 2024, meaning they are unfamiliar with our test set.The model receives a query string composed of a context description, a graph entity, and a relation type and returns a ranked list of answers (other graph nodes).We perform HPO (random grid search of 10 trails) to select the number of training epochs, warmup ratio and learning rate for each model.We use contrastive loss and generate 30 negatives per positive example.Following the literature standard (Teach et al., 2020), we report metrics in the filtered settings to avoid false negatives.Given the difficulty of the task we focus on ranking only the 12751 test set entities.A full summary of our data splits is available on 5.The examples we use to train and evaluate our prediction models contain all collected nodes, including those classified as belonging to the "other" domain.</p>
<p>We utilize RankGPT (Sun et al., 2023) as a strong reranker and apply it to rerank the top-20 predicted results.We employ RankGPT with GPT-4o, a window size of 10 and a step size of 5. Note the information cutoff of GPT-4o is October 20238 , meaning it is unfamiliar with our test set as well.We use the implementation available in9 .However, we find that adjusting the default prompt works better for our task.Figure 17 shows the modified reranking prompt.The cost of applying the reranker to our data was 60$.</p>
<p>E.3 Reranker error analysis</p>
<p>In Section 5.1, we show that reranking the top-20 answers retrieved by our best-performing prediction model (all-mpnet-base-v2 finetuned ) can some-Figure 15: Context extraction prompt.{{ABSTRACT}} is a placeholder for the input abstract.{{METHODOL-OGY_STATEMENT}} is a sentence describing the recombination.We build it by filling one of the following templates with the extracted recombination entities: "Combine <source-entity> and <target-entity>" for blends and "Take inspiration from <source-entity> and apply it to <target-entity>" for inspirtions.times lower the rank of the gold candidate.To better understand the underlying causes of such reranking failures, we conduct an error analysis of 30 representative cases.Our goal in this section is to describe common patterns in these errors and highlight particularly challenging scenarios that may inform future progress.</p>
<p>Inspirations</p>
<p>(i) Multiple plausible answers.In some cases, the reranker correctly identifies a strong and highly relevant candidate, and ranks it above the gold even though both answers are valid.These errors stem</p>
<p>Nuanced recombination types examples</p>
<p>Reduction</p>
<p>Abstract: "Register allocation is one of the most important problems for modern compilers...This work demonstrates the use of casting the register allocation problem as a graph coloring problem..."</p>
<p>Inspiration: "a graph coloring problem" −→ "Register allocation"</p>
<p>Metaphor</p>
<p>Abstract: "Affective sharing within groups strengthens coordination and empathy...we propose HeartBees, a bio-feedback system for visualizing collective emotional states, which maps a multi-dimensional emotion model into a metaphorical visualization of flocks of birds..."</p>
<p>Inspiration: "flocks of birds" −→ "a multi-dimensional emotion model"</p>
<p>Analogy</p>
<p>Abstract: "Physics-informed Graph Neural Networks have achieved remarkable performance...by mitigating common GNN challenges...Despite these advancements, the development of a simple yet effective paradigm that appropriately integrates previous methods for handling all these challenges is still underway.In this paper, we draw an analogy between the propagation of GNNs and particle systems in physics, proposing a model-agnostic enhancement framework..."</p>
<p>Inspiration: "particle systems in physics" −→ "the propagation of GNNs"</p>
<p>Application of existing knowledge</p>
<p>Abstract: "Object detection in high-resolution satellite imagery is emerging as a scalable alternative to on-the-ground survey data collection...However, performing object detection over large geographies can still be prohibitively expensive due to the high cost of purchasing imagery and compute.Inspired by traditional survey data collection strategies, we propose an approach to estimate object count statistics over large geographies through sampling..."</p>
<p>Inspiration: "traditional survey data collection strategies" −→ "Object detection in highresolution satellite imagery" Abstraction Abstract: "While visual question-answering (VQA) benchmarks have catalyzed the development of reasoning techniques, they have focused on vertical thinking.Effective problem-solving also necessitates lateral thinking...To bridge this gap, we formulate visual lateral thinking as a multiple-choice question-answering task..."</p>
<p>Inspiration: "a multiple-choice question-answering task" −→ "visual lateral thinking"</p>
<p>Query Answer</p>
<p>Understanding the human brain's processing capabilities can inspire advancements in machine learning algorithms and architectures.Previous methods in brain research were limited to identifying regions of interest for one subject at a time, restricting their applicability and scalability across multiple subjects.</p>
<p>What would be a good source of inspiration for "a highly efficient processing unit"?</p>
<p>The human brain Existing models for link prediction in knowledge graphs primarily focus on representing triplets in either distance or semantic space, which limits their ability to fully capture the information of head and tail entities and utilize hierarchical level information effectively.This indicates a need for improved methods that can leverage both types of information for better representation learning in knowledge graphs.</p>
<p>What could we blend with "distance measurement space" to address the described settings?</p>
<p>Semantic measurement space</p>
<p>F User study additional details</p>
<p>We request each to fill out a form asking in what scientific domains they feel comfortable reading papers and a short description of their research area.</p>
<p>We then used granite-embedding-125m-english to retrieve semantically similar contexts to this description Please read the guidelines carefully before you start.Your goal is to assess how helpful AI-generated suggestions are in helping researchers generate interesting ideas and gain fresh perspectives.</p>
<p>You will be provided with:</p>
<p>A context describing the problem, specific settings, goal, etc.A query requesting a suggestion relevant to the context.A list of AI-generated suggestions.</p>
<p>Rank the suggestions based on how helpful they are for generating interesting ideas.Consider the following: Is the suggestion thought provoking and interesting?Does it address the query and fit the context?Is it clear and actionable?</p>
<p>Figure 18: User study guidelines.</p>
<p>from the relevant arXiv categories.We manually verify that the retrieved contexts match the description and discard examples with poorly extracted information (e.g., the context begins with "This study reviews the problem of..." instead of directly describing the source study problem).In addition, we let the volunteers mark an example as "ill-defined", in which case we ignore their inputs.We conduct a 10-minute training session with each volunteer, requesting them to read the instructions and explain the task.Figure 18 presents the instructions given to the participants in the study.Figure 19 presents the web interface of the annotation platform.</p>
<p>Reranking error examples (i) Multiple plausible answers</p>
<p>Query: "Traditional reasoning methods in language models often rely on historical information and employ a uni-directional reasoning strategy...This leads to suboptimal decision-making... What would be a good source of inspiration for enhancing the decision rationality of language models?"</p>
<p>Pre-reranking (top-20)</p>
<p>F.1 Predictions examples</p>
<p>Table 20 shows a selection of model predictions that participants rated as most helpful for inspiring research directions.These examples highlight how CHIMERA-trained models can move beyond surface-level associations to propose insightful cross-domain inspirations, for instance, linking harmful meme detection to visual commonsense reasoning, or drawing on neuroscience to improve LLM knowledge retention.Such predictions demonstrate CHIMERA's potential to power ideation tools that help researchers identify novel, actionable directions for future work.</p>
<p>G Comparison to other information extraction methods</p>
<p>Both general scientific extraction and concept cooccurrence struggle to capture concise and accurate recombination relations, as can be seen in Figure 20. Figure 20a 20b shows how recombination extraction using concept co-occurrence might be misleading.In this method, each pair of canonical scientific concepts (e.g, neural networks) that co-occur within the same abstract are considered a recombination.The figure presents an example of using AI-related concepts curated by Krenn et al. (2022) for recombination extraction, alongside recombination extracted using our designated approach.Note that when using concept co-occurrence, the extracted recombinations are essentially {concepts} 2 , which might be imprecise, and capture meaningless recombinations (e.g., "wide application" recombined with "final prediction") or misleading recombinations (e.g., "question answering" with "language models", which explicitly presented by the authors as a lacking approach for the task).In comparison, our new extraction schema neatly models the main recombiant relation presented in the text as taking inspiration from "the step-by-step reasoning behavior of humans" for "temporal question answering."</p>
<p>Prediction Examples</p>
<p>Query: Existing large language models (LLMs) underperform in legal judgment prediction due to challenges in understanding case complexities and distinguishing between similar charges.This highlights a need for improved methodologies that can effectively address these issues to enhance judicial efficiency.In this context, what would be a good source of inspiration for legal judgment prediction?</p>
<p>Prediction: The Issue, Rule, Application, Conclusion (Issue, Rule, Application, Conclusion) framework, a sequential reasoning approach used by lawyers.</p>
<p>Query: The exponential increase in scientific literature makes it challenging for researchers to stay current with recent advances and identify meaningful research directions.Existing methods for idea generation either trivially prompt large language models or directly expose them to extensive literature without indicating useful information, highlighting a need for more effective approaches to research ideation.In this context, what would be a good source of inspiration for automating the generation of novel research ideas?</p>
<p>Prediction: Emulate the human process of inducing collective creativity through engaging discussions with participants from diverse backgrounds and perspectives.</p>
<p>Query:The challenge of detecting harmful memes lies in the implicit meanings embedded within them, which are not explicitly conveyed through surface text and images.Existing harmful meme detection methods fail to provide readable explanations that reveal these implicit meanings, highlighting a significant gap in the ability to support detection decisions.In this context, what would be a good source of inspiration for detecting harmful memes?</p>
<p>Prediction: Visual commonsense discovery in computer vision.</p>
<p>Query: Large language models (LLMs) often struggle to provide up-to-date information due to their one-time training and the constantly evolving nature of the world.Existing approaches to keep LLMs current face difficulties in extracting stored knowledge, highlighting a need for improved methods of knowledge acquisition from raw documents.In this context, what would be a good source of inspiration for improving an llm's ability to effectively acquire new knowledge from raw documents?</p>
<p>Prediction: Neuroscience, where the human brain often sheds outdated information to improve the retention of crucial knowledge and facilitate the acquisition of new information.20a: General recombination extraction schemas lack fitting relation types to capture recombinations, which results in capturing plenty of irrelevant relations ("Early diagnosis" ←→ "professional intervention").Figure 20b: Recombination extraction using concept co-occurrence might be nonsensical ("wide application" ←→ "final prediction") or even misleading ("question answering" ←→ "language models")).</p>
<p>"</p>
<p>the flexibility and resilience of dragonfly wings" "a novel design for a biomimetic drone propeller" Authors explicitly describe idea recombination "There is a growing need for vertical take-off and landing vehicles, including drones, which are safe to use and can adapt to collisions.... Inspired by the flexibility and resilience of dragonfly wings, we propose a novel design for a biomimetic drone propeller called Tombo propeller..."</p>
<p>Figure 1 :
1
Figure 1: We propose a new task of extracting recombinations: examples of how scientists connect ideas in novel ways.The extracted information enables applications in research analysis and automated ideation.</p>
<p>Figure 2 :
2
Figure 2: CHIMERA KB construction and applications.Construction: (1) We use human-annotated recombination examples to fine-tune an LLM for information extraction; (2) the model extracts recombinations from arXiv abstracts to build a large-scale KB.Applications: CHIMERA supports diverse use cases, including computational ideation, exploration of recombination patterns across scientific domains, and meta-scientific analysis.</p>
<p>(a) Frequent domains in inspiration edges.(b) Frequent domains in blend edges.</p>
<p>Figure 6 :
6
Figure 6: Researchers find our recombination suggestions almost as helpful as the gold answer in inspiring ideas, validating our automated evaluation.</p>
<p>Figure 7 :
7
Figure 7: E2E extraction prompt.{TEXT} is the placeholder for the input abstract text.</p>
<p>Figure 8 :
8
Figure 8: GoLLIE guidelines.</p>
<p>Figure 9 :
9
Figure 9: E2E ICL prompt.{TEXT} is a placeholder for the abstract text, and {EXAMPLES} for the ICL examples.</p>
<p>Figure 10 :
10
Figure 10: Entity extraction prompt.{TEXT} is a placeholder the input abstract.</p>
<p>Bad extraction examples (human annotated test set) Abstract: "...Kahneman &amp; Tversky's prospect theory tells us that humans perceive random variables in a biased but well-defined manner (1992) ... Using a Kahneman-Tversky model of human utility, we propose a HALO [Human Aware Loss Function] that directly maximizes the utility of generations instead of maximizing the log-likelihood of preferences, as current methods do..." Gold = [Inspiration: "Kahneman &amp; Tversky's prospect theory" −→ "a HALO"] Pred = [] Abstract: "...We address the problem by proposing a Wasserstein GAN combined with a new reverse mask operator, namely Reverse Masking Network (R-MNet), a perceptual adversarial network for image inpainting ... Additionally, we propose a new loss function computed in feature space to target only valid pixels combined with adversarial training..." Gold = [Blend: "a Wasserstein GAN" ←→ "...R-MNet"] Pred = [Blend: "a Wasserstein GAN" ←→ "...R-MNet" ←→ "a new loss function"]</p>
<p>Figure 12 :
12
Figure 12: Large-scale evaluation prompt.{AB-STRACT} is a placeholder for the original abstract text.{EXTRACTED_RELATION}, {ENTITY1}, and {EN-TITY2} are placeholders for the relation type and entities extracted by our model.</p>
<p>Figure 16 :Figure 17 :
1617
Figure 16: Leak detection prompt.</p>
<p>Figure 19 :
19
Figure 19: User study interface.</p>
<p>Early diagnosis and professional intervention can help children with autism spectrum disorder (ASD) return to normal life... numerous paradigms have been proposed that use computer technology to assist or independently conduct ASD interventions...However, these paradigms often lack a foundation in clinical intervention methods and suffer from a lack of personalization.Addressing these concerns, we propose ASD-Chat, a social intervention system based on VB-MAPP (Verbal Behavior Milestones Assessment and Placement Program) and powered by ChatGPT as the backbone for dialogue generation... we designed intervention paradigms and prompts based on the clinical intervention method VB-MAPP and utilized ChatGPT's generative capabilities to facilitate social dialogue interventions... Conjunction={(Early diagnosis, professional intervention), (professional intervention, autism spectrum disorder (ASD))}, Used-For={(computer technology, ASD interventions), (ChatGPT, social intervention system), (ChatGPT, dialogue generation), (clinical intervention method VB-MAPP, intervention paradigms and prompts), (ChatGPT, social dialogue interventions)} Blend = { VB-MAPP , Chat-GPT } Comparison to recombination extraction using a general scientific IE schema (SciERC)Knowledge graphs ... have received increasing attention due to its wide applications on natural language processing.However, its use case on temporal question answering (QA) has not been well-explored.... existing methods are developed based on pre-trained language models, which might not be capable to learn temporalspecific presentations of entities in terms of temporal KGQA task.... we propose a novel Time-aware Multiway Adaptive (TMA) fusion network.Inspired by the step-by-step reasoning behavior of humans....TMA ... extracts the relevant concepts from the KG... to produce a temporal-specific representation of the question.This representation can be incorporated with the pre-trained KG embedding to generate the final prediction.Empirical results verify that the proposed model achieves better performance than the state-of-the-art models in the benchmark dataset.... results of TMA on the CronQuestions dataset's complex questions are absolutely improved ... TMA ... can provide interpretability by analyzing the proportion of information in question representations.Inspiration = {Source: the step-by-step reasoning behavior of humans , Target: temporal question answering (QA) } Comparison to recombination extraction using concept co-occurrence.</p>
<p>Figure 20 :
20
Figure20: Comparison of our designate recombination extraction method to alternative approaches.Figure20a: General recombination extraction schemas lack fitting relation types to capture recombinations, which results in capturing plenty of irrelevant relations ("Early diagnosis" ←→ "professional intervention").Figure20b: Recombination extraction using concept co-occurrence might be nonsensical ("wide application" ←→ "final prediction") or even misleading ("question answering" ←→ "language models")).</p>
<p>Figure</p>
<p>Figure20: Comparison of our designate recombination extraction method to alternative approaches.Figure20a: General recombination extraction schemas lack fitting relation types to capture recombinations, which results in capturing plenty of irrelevant relations ("Early diagnosis" ←→ "professional intervention").Figure20b: Recombination extraction using concept co-occurrence might be nonsensical ("wide application" ←→ "final prediction") or even misleading ("question answering" ←→ "language models")).</p>
<p>Table 3 :
3
CHIMERA contains over 28K recombinations, a quarter of them interdisciplinary.
most salient recombination from the text, if oneexists. The model must determine whether the textdiscusses recombination, infer its type, and iden-tify entities in a single query. We devise the test setfrom examples where at least two annotators (outof three) agree on the recombination type (or ab-sence), ensuring high-quality, low-ambiguity data.</p>
<p>Table 2
2
summarizes the train and test sets.3.2TheCHIMERA Knowledge BaseWe construct the CHIMERA knowledge base by mining recombination examples from scientific abstracts, categorizing them, and representing them in a graph where nodes are scientific concepts and edges denote recombination relations.</p>
<p>Table 4 :
4
Recombination extraction results.Bold text signifies the best result, while underlined text signifies the second-best.We observe that surprisingly large and capable models struggle with the extraction tasks.
4 Results4.1 Experimental SettingsEvaluation criteria We evaluate (1) Abstractclassification-does the text discuss recombina-tion?, (2) Entity extraction-what entities are de-scribed? and (3) Relation extraction-what is therelation discussed? For abstract classification, wereport precision, recall, and F1. For entity andrelation extraction, we adopt a soft matching ap-proach: two entities of the same type match ifthey refer to semantically similar concepts. Weuse GPT-4o-mini 5 to judge similarity (see promptand details in Appendix B.4).</p>
<p>video generation?"</p>
<p>Context: "Recent advancements in video generation have struggled to model complex narratives and maintain character consistency ..." Query: "What would be a good source of inspiration for Recombination prediction.Given a context string and a query about recombining a graph node, a model trained on CHIMERA suggests plausible recombination directions, leveraging patterns learned from prior examples.
Suggestions1"The concept ofstoryboarding, whichdisassembles a script intoindividual shots""video generation"is a graph nodeKnowledge Base Recombinationmodel Predictiontradition of Narrative Art" "The time immemorial 5Figure 5:cs.CL (Computation &amp; Language)cs.CV (Computer Vision)cs.AI (Artificial Intelligence)cs.LG (Machine Learning)cs.IR (Information Retrieval)50%40%30%20%10%0%201920202021202220232024Figure 4: Prevalent domains inspired by cs.CL concepts(NLP). Note the decrease in within-domain inspiration.tion errors are minor, typically involving correctrecombinations where the extracted entities are lessinformative than those in the original abstract (seeexamples and additional details in Appendix B.7).4.3 KB Meta-Science AnalysisBlends vs. inspirations Figures 3a and 3bpresent the predominant domain pairs for inspi-</p>
<p>Table 16 in Appendix D.1 provides a tabular view of this analysis for clarity.
Split# Inspiration # Blend # TotalTrain5,40819,90925,317Validation119411530Test2,0268,59110,617Inspiration analysis We next analyze how dif-ferent fields draw inspiration from each other. Fig-ure 3c shows the top 10% cross-domain inspira-tion sources for three prevalent domains in thegraph: cs.RO (Robotics), cs.CV (Computer Vision)and cs.CL (NLP). We observe that while somesources of inspiration (like cognitive-science) arecommonly shared across related fields, domainsmay draw inspiration from unique sources (e.g.,from zoology to cs.RO as seen in Figure 1). Inter-estingly, cs.CV takes more inspiration from cs.CLthan vice versa. cs.CL also takes considerably moreinspiration from cognitive science than cs.CV, andalso takes inspiration from psychology (see exam-ple in Table 1), while cs.CV takes more inspirationfrom biomedical sources. cs.CV also takes inspira-tion from mathematical topics (discrete math, op-timization and control). Appendix B.6 presentsexamples of such interdisciplinary inspirations.</p>
<p>Table 5 :
5
We divide prediction data by the publication years associated with each query (training and validation sets &lt; 2024, test set ≥ 2024) to avoid contamination.</p>
<p>Table 6 :
6
Recombination prediction results.MedR = Median Rank.Fine-tuning on CHIMERA improves MedR 10×.
all-mpnet-base-v20.033 0.0420.0610.1260.1700.0331305bge-large-en-v1.50.041 0.0530.0760.1510.1990.0411135e5-large-v20.024 0.0330.0500.1130.1550.0261590all-mpnet-base-v2 f inetuned0.110 0.1350.1780.3200.4020.106194bge-large-en-v1.5 f inetuned0.104 0.1300.1680.3060.3920.102222e5-large-v2 f inetuned0.107 0.1330.1730.3170.3970.103212all-mpnet-base-v2 f inetuned + RankGPT0.100 0.1300.1920.3200.4020.097194</p>
<p>Table 8 presents the list of keywords used for this step.". . .This research proposed a framework based on Long Short-Term Memory (LSTM) deep learning network to generate day-ahead hourly temperature forecast. . .A case study is shown which uses historical in-situ observations and Internet of Things (IoT) observations for New York City, USA.By leveraging the historical air temperature data from in-situ observations, the LSTM model can be exposed to more historical patterns that might not be present in the IoT observations.
Abstract:B.2 Extraction baselines implementation E2E recombination extraction We use Mistral-7B as the backbone for our recombina-tion extraction baseline. We fine-tune the model using mistral-finetune 6 on a single NVIDIA RTX A6000 48GB GPU over 500 steps. The training was conducted using the default learning rate of 6.Annotators' Disagreement Examples
e − 5 and weight decay of 0.1.We use a batch size of 1 and a maximum sequence length of 4096 tokens.mistral-finetuneimplementsLow-RankAdaptation of LLM (LoRA), a parameter efficient fine-tuning method(Hu et al., Meanwhile, by using IoT observations, the spatial resolution of air temperature predictions is significantly improved..."Annotator 1: [Blend: "Internet of Things (IoT) observations for New York City, USA" ←→ "historical air temperature data from in-situ observations"] Annotator 2: []</p>
<p>Table 7 :
7
Examples of annotation disagreements and resolutions by expert review.
Recombination keywordscombinesanalogiesaggregateintermingleunifyblendingcombinedequivalenceaggregationintermingling unificationblendscombineequivalentalignjoinweaveblendcombination reductionalignmentjoiningweavingblendscombinations reframingamalgamatejuxtaposehybridmergecombiningreframeamalgamation juxtaposition mergemergesmixingreformulating assemblelinkmergesunitesmixturecastingassemblinglinkagemerginganalogymixcastassociatemeldmergedanalogizemixedcastsassociationmeldingconflationanalogiesintegratesviewingbondmeshcoupleequivalenceintegratingviewedbondingmeshinguniteequivalentintegrateviewbridgeperceiveunitescorrelateintegratedinspirebridgingperceptioninterplaycorrelationconnectioninspiredcoalescerelateinterconnect envisionsynergyinspirationcoalescencerelationharmonizeenvisioningfusioninspirescomposespliceharmonyharmonizefusesinspiringcompositionsplicingincorporate harmonyunifyinterconnectincorporation synthesisreductionsynthesisaggregatealigninspiringinspirecoupleconjunctionaggregationreframinginspirationfuseuniteconjoinalignmentreframeinspiressynthesis</p>
<p>Table 8 :
8
Recombination keywords.We use a predefined list of keywords to identify works that are more likely to discuss idea recombination.</p>
<p>Identify the main ideas, methods, or approaches discussed in the abstract.2.Determine if there is a clear combination of ideas or if one idea inspired the application in another domain.3.Identify the specific entities involved in the recombination.4.Classify the entities according to the provided entity types.5.Determine the recombination type (combination or inspiration).</scratchpad>6.Now, provide your final output in the specified JSON format.Ensure that the output is a valid JSON string.If the output is empty, return {}.Place your answer within <answer> tags.Remember to carefully analyze the abstract and only identify a recombination if it is clearly present and central to the work described.</p>
<p></entity_types> 2. Now, carefully read the following scientific abstract: <abstract>{TEXT}</abstract> 3.Your task is to extract the most salient recombination from this abstract.A recombination can be either: a) Combination: The authors combine two or more ideas, methods, models, techniques, or approaches to obtain a certain goal.b) Inspiration: The authors draw inspiration or similarities from one concept, idea, problem, approach, or domain and implement it in another.4.After identifying the recombination, you will format it as a JSON string in the following structure: <recombination>{recombination_type: {entity_type_1: [ent_1, ent_2], entity_type_2: [ent_3],...}}</recombination> If you don't think the text discusses a recombination, or that the recombination is not a central part of the work, return an empty JSON object: {}. 5. Before providing your final answer, use the following scratchpad to think through the process: <scratchpad> 1.</p>
<p>Table 2 describes, the training set only has 45 inspiration examples (as opposed to &gt; 100 blend and not-present examples).45 is, therefore, the maximal number of examples per class we can sample while keeping the ICL set balanced.We run each experiment 5 times, sampling a new set of few-shot examples in each, and report the average.Figure</p>
<p>present the prompts used for analyzing blend and inspiration relations,
Abstract"Efficient exploration of large-scale environments remains a critical challenge inrobotics... The presented bio-inspired framework heuristically models frontierexploration similar to the shepherding behavior of herding dogs. This isachieved by modeling frontiers as a sheep swarm reacting to robots modeled asshepherding dogs...""Histopathological image classification constitutes a pivotal task in computer-aided diagnostics... In the diagnostic process of pathologists, a multi-tieredapproach is typically employed to assess abnormalities in cell regions at dif-ferent magnifications...Inspired by the multi-granular diagnostic approachof pathologists, we perform feature extraction on cell structures at coarse,medium, and fine granularity, enabling the model to fully harness the informa-tion in histopathological images...""...Traditional approaches to enhance dialogue planning in LLMs, ... eitherface efficiency issues or deliver suboptimal performance. Inspired by thedualprocess theory in psychology, which identifies two distinct modes ofthinking -intuitive (fast) and analytical (slow), we propose the Dual-ProcessDialogue Planning (DPDP) framework. DPDP embodies this theory throughtwo complementary planning systems: an instinctive policy model for familiarcontexts and a deliberative Monte Carlo Tree Search (MCTS) mechanism forcomplex, novel scenarios...""Click-Through Rate (CTR) prediction is a pivotal task in product andcontent recommendation, where learning effective feature embeddings is ofgreat significance....inspired by the Global Workspace Theory in consciousprocessing, which posits that only a specific subset of the product featuresare pertinent while the rest can be noisy and even detrimental to human-clickbehaviors, we propose a CTR model that enables Dynamic Embedding Learningwith Truncated Conscious Attention for CTR prediction, termed DELTA..."Detected InspirationInspiration-Source:"the shepherding behavior of herding dogs" [zoology]Inspiration-Target:"Frontier exploration" [cs.ro]Inspiration-Source:"the multi-granular diagnostic approach of pathologists" [biomedi-cal sciences]Inspiration-Target:"Histopathological image classification" [cs.cv]Inspiration-Source:"the dual-process theory in psychology" [psychology]Inspiration-Target:"enhance dialogue planning in LLMs" [cs.cl]Inspiration-Source:"the Global Workspace Theory in conscious processing" [cogni-tive science]Inspiration-Target:"learning effective feature embeddings for CTR prediction" [cs.lg]
Table 10: Inter-domain inspiration examples from the CHIMERA knowledge graph.</p>
<p>Table 11 :
11
Representative examples of bad automatic extraction.Many errors stem from uninformative entity spans, as presented by the two bottom examples.
Non-arXiv scientific domainsAgricultural ScienceAnatomyAnimal ScienceAnthropologyArchaeologyBehavioral ScienceBiochemistryBioinformaticsBioclimatologyBiomedical Engineering BiophysicsBiotechnologyBotanyCardiologyChemical EngineeringCivil EngineeringClinical PsychologyCognitive ScienceCriminologyCryosphere ScienceCytologyDemographyDentistryDermatologyDevelopmental Biology EcologyEcotoxicologyEconomicsEducational PsychologyElectrical EngineeringEmergency MedicineEndocrinologyEnergy ScienceEngineering ScienceEntomologyEnvironmental EngineeringEnvironmental ScienceEpidemiologyEthologyFood ScienceForestryGastroenterologyGeneticsGenomicsGeographyGeologyGeophysicsGlaciologyHealth InformaticsHistopathologyHydrogeologyHydrologyImmunogeneticsImmunologyIndustrial/Organizational Psychology Landscape ArchitectureLinguisticsMarine BiologyMaterials ScienceMechanical Engineering Medical MicrobiologyMeteorologyMicrobiologyMineralogyMolecular BiologyMycologyNanotechnologyNeurologyNeuroscienceNuclear EngineeringNutritional ScienceObstetricsOceanographyOncologyOphthalmologyOrnithologyOrthopedicsOtologyPaleoclimatologyPaleontologyPathobiologyPathologyPediatric MedicinePedagogyPetrologyPharmacogenomicsPharmacologyPhilosophyPhysiologyPolitical ScienceProteomicsPsychiatryPsychologyPsychopathologyPublic HealthPulmonologyRadiologyRheumatologySeismologySocial PsychologySociologySurgerySystems BiologyThermodynamicsToxicologyUrban PlanningUrologyVeterinary ScienceVirologyVolcanologyWildlife BiologyZoology</p>
<p>Table</p>
<p>Table 16 :
16
Predominant inspiration and blend relations.The above is a tabular version of Figures 3b, 3a in Section 4.3.It presents edges with (source-domain, target-domain) pairs frequency above the 0.98 quantile.
Blends</p>
<p>Table 17 :
17
Examples of nuanced inspiration types found within CHIMERA.While all examples are labeled as inspiration, they illustrate finer-grained mechanisms such as metaphor, reduction, and analogy.This suggests that our taxonomy is expressive enough to capture a rich diversity of recombination strategies.</p>
<p>Table 18 :
18
Leakages examples.Examples of leaks -queries that reveal or strongly imply the answer.
not from a lack of understanding, but from the pres-ence of several equally reasonable responses. Forinstance, in Table 19 (top), the reranker promotes aconceptually grounded strategy from game theoryover a more generic gold response about rationaldecision principles.(ii) Semantically similar variants. Anothercommon error involves the reranker prioritizingparaphrased or reformulated versions of the goldanswer. While these candidates are semanticallyclose to the gold, the gold itself may fall in rank dueto redundancy. As shown in Table 19 (bottom), sev-eral variants of "Direct Preference Optimization"receive high rankings, but the original mention ofthe method is pushed downward, possibly due tolexical overlap penalties or insufficient canonical-ization.These examples highlight nuanced challenges inreranking systems, such as handling redundancyand conceptual equivalence.</p>
<p>Query: "Prior methods for aligning large language models face challenges in tuning to maximize non-differentiable and non-binary objectives...This highlights a need for a more flexible approach that can generalize to various user preferences... while maintaining alignment... What could we blend with reinforcement learning via human feedback to address the described settings?"
Post-reranking (top-20)1. the inherent human attribute of engaging in1. the Level-K framework from game theory andlogical reasoning to facilitate decision-makingbehavioral economics, which extends reasoning2. principles of rational decision-makingfrom simple reactions to structured strategic depth3. the Level-K framework from game theory and2. Bayesian inference: conditioning a prior onbehavioral economics, which extends reasoningevidencefrom simple reactions to structured strategic depth......6. principles of rational decision-makingQuery: "...while Large Language Models (LLMs) excel in various NLP tasks, their ability togenerate comprehensive data stories remains underexplored... What would be a good source ofinspiration for Data-driven storytelling?"Pre-reranking (top-20)Post-reranking (top-20)1. the human storytelling process1. story analysis and generation systems2. story writing2. generative artificial intelligence (Gen-AI)-3. Interactive digital storiesdriven narrative personalization...3. narrative structure designs9. narrative structure designs4. the human storytelling process......(ii) Semantically similar variantsPre-reranking (top-20)Post-reranking (top-20)1. aligning Large Language Models with human1. Direct Preference Optimization for preferencepreferencesalignment2. Direct Preference Optimization for preference2. State-of-the-art language model fine-tuningalignmenttechniques, such as Direct Preference Optimiza-3. direct preference optimizationtion...3. contrastive learning-based methods like Direct5. State-of-the-art language model fine-tuningPreference Optimizationtechniques, such as Direct Preference Optimiza-4. a Semi-Policy Preference Optimization methodtion5. direct preference optimization......</p>
<p>Table 19 :
19
Illustrative examples where the reranker preferred a different answer over the gold one.</p>
<p>presents how general scientific IE schemas lack relation types to model recombinations.The figure presents the results of our specialized extraction method besides a transformerbased extraction model (Hennen et al., 2024) finetuned on SciERC (Luan et al., 2018), a general IE schema.While our new data schema easily models the recombinant connection between two techniques: "BV-MAPP (Verbal Behavior Milestones Assessment and Placement Program)", "ChatGPT" as a concept blend, the SciERC extraction schema isn't equipped with proper relation types for this.As a result, it captures mostly irrelevant informa-tion for our task (e.g background details as "Early diagnosis" or "professional intervention").Figure</p>
<p>Table 20 :
20
Examples of recombination directions predicted by our model and rated as most inspiring by user study participants.Each prediction links a scientific challenge with a cross-domain concept, illustrating CHIMERA 's potential to support creative research ideation.</p>
<p>We focus on the following arXiv categories: cs.AI, cs.CL, cs.CV, cs.CY, cs.HC, cs.IR, cs.LG, cs.RO, cs.SI
https://www.upwork.com
https://tinyurl.com/zy27uhdp
https://tinyurl.com/mrzksbky
https://github.com/mistralai/mistral-finetune
https://github.com/mistralai/mistral-inference
As stated in https://platform.openai.com/docs/models/gpt-4o
9 https://github.com/sunnweiwei/RankGPT/tree/main
Type ExamplesNon-Academic "the snap-through action of a steel hairclip", "yoga", "origami, the traditional Japanese paper-folding technique, is a powerful metaphor for design and fabrication of reconfigurable structures", "Tangram, a game that requires replicating an abstract pattern from seven dissected shapes"Noisy "a deep", "word-", "at the context level", "a neural part", "post", "text-audio", "end-toend multi-modal model only X-VLM only X-VLM only X-VLM only X-VLM only X-VLM only X-VLM only X-VLM only X-VLM only X-VLMs", "a user's long-term"Overly-general "human experiences", "a styling method", "local search method", "a pipeline inspired by experts' work", "a new modality", "feature based approaches" Misclassified "Reinforcement learning, or RL", "Facial Expressions Recognition(FER)", "a Kullback-Liebler regularization function", "K-nearest neighbors algorithm", "Shapley values from game theory", "Gaussian Stochastic Weight Averaging"Your paragraph textYou are tasked with extracting the rationale behind the selection of a specific methodology in a scientific study.You will be provided with an abstract and a statement about the methodology used.Your goal is to extract the reasons for choosing this methodology from the abstract.First, carefully read the following abstract: <abstract>{{ABSTRACT}}</abstract> Next, inspect the following examples of background descriptions:1. Large language models (LLMs) commonly employ autoregressive generation during inference, leading to high memory bandwidth demand and consequently extended latency.2. Reconstructing deformable tissues from endoscopic videos is essential in many downstream surgical applications.However, existing methods suffer from slow rendering speed, greatly limiting their practical use.3. Many industrial tasks-such as sanding, installing fasteners, and wire harnessing-are difficult to automate due to task complexity and variability.4. Multi-legged robots offer enhanced stability in complex terrains, yet autonomously learning natural and robust motions in such environments remains challenging.Now, consider this methodology statement: <methodology_statement>{{METHODOLOGY_STATEMENT}}</methodology_statement>To complete this task, follow these steps:1. Analyze the abstract thoroughly, focusing on:-The context or reasons that justify the methodology choice -Any challenges, limitations, or research needs the methodology addresses -Mentions of previous research or knowledge gaps that the methodology aims to target 2. When formulating your response:-Phrase your response as a general 1-2 sentence description of a challenge, limitation research needs, etc.-Use exclusively the information from the abstract.Do not incorporate external knowledge or assumptions.-Minimize including information from the methodology statement in your answer.-Do not include information about the used methodology in your answer.-If the background details are unclear, return an empty response.3. Format your response as follows: <background> [1-2 background sentences] </background> Remember to base your response strictly on the provided abstract and statement.Do not include additional information or assumptions.
The cambridge handbook of creativity. 2019</p>
<p>On the benefits and pitfalls of analogies for innovative design: Ideation performance based on analogical distance, commonness, and modality of examples. Joel Chan, Katherine Fu, Christian Schunn, Jonathan Cagan, Kristin Wood, Kenneth Kotovsky, Journal of mechanical design. 1338810042011</p>
<p>Visiblends: A flexible workflow for visual blends. Lydia B Chilton, S Petridis, Maneesh Agrawala, Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems. the 2019 CHI Conference on Human Factors in Computing Systems2019</p>
<p>Creativeconnect: Supporting reference recombination for graphic design ideation with generative ai. Daeun Choi, Sumin Hong, Jeongeon Park, John Joon , Young Chung, Juho Kim, Proceedings of the CHI Conference on Human Factors in Computing Systems. the CHI Conference on Human Factors in Computing Systems2023</p>
<p>Why cohen's kappa should be avoided as performance measure in classification. Rosario Delgado, Xavier-Andoni Tibau, PloS one. 149e02229162019</p>
<p>Science of science. Santo Fortunato, Carl T Bergstrom, Katy Börner, James A Evans, Dirk Helbing, Stasa Milojevic, Alexander Michael Petersen, Filippo Radicchi, Roberta Sinatra, Brian Uzzi, Alessandro Vespignani, Ludo Waltman, Dashun Wang, Albert Ĺaszló Barabási, Nature. 2142018</p>
<p>Mapping the landscape of creativity support tools in hci. Jonas Frich, Lindsay Macdonald Vermeulen, Christian Remy, Mose Michael, Peter Biskjaer, Dalsgaard, Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems. the 2019 CHI Conference on Human Factors in Computing Systems2019</p>
<p>Discovering emergent connections in quantum physics research via dynamic word embeddings. Felix Frohnert, Xuemei Gu, Mario Krenn, P L Evert, Van Nieuwenburg, Machine Learning: Science and Technology. 62024</p>
<p>Iris: Interactive research ideation system for accelerating scientific discovery. Aniketh Garikaparthi, Manasi Patwardhan, arXiv:2504.167282025PreprintLovekesh Vig, and Arman Cohan</p>
<p>Analogy and creativity in the works of johannes kepler. Creative thought: An investigation of conceptual structures and processes. Dedre Gentner, Sarah Brem, Ron Ferguson, Philip Wolff, Arthur B Markman, Forbus, 1997</p>
<p>Relational Categories. Dedre Gentner, Kenneth J Kurtz, Categorization inside and outside the laboratory: Essays in honor of Douglas L. Medin, APA decade of behavior series. Washington, DC, USAmerican Psychological Association2005</p>
<p>Structure mapping in analogy and similarity. Dedre Gentner, Arthur B Markman, American psychologist. 521451997</p>
<p>Fernando Gonzalez, Zhijing Jin, Bernhard Schölkopf, Tom Hope, arXiv:2305.05471Mrinmaya Sachan, and Rada Mihalcea. 2023. Beyond good intentions: Reporting the research landscape of nlp for social good. arXiv preprint</p>
<p>Iter: Iterative transformer-based entity recognition and relation extraction. Florian Moritz Hennen, Michaela Babl, Geierhos, Conference on Empirical Methods in Natural Language Processing. 2024</p>
<p>Mental leaps: Analogy in creative thought. Keith J Holyoak, Paul Thagard, 1994</p>
<p>Accelerating innovation through analogy mining. Tom Hope, Joel Chan, Aniket Kittur, Dafna Shahaf, 10.1145/3097983.3098038Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '17. the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '17New York, NY, USAACM2017</p>
<p>Lora: Low-rank adaptation of large language models. J Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Weizhu Chen, ArXiv, abs/2106.096852021</p>
<p>Agents' room: Narrative generation through multi-step collaboration. Fantine Huot, Reinald Kim Amplayo, Jennimaria Palomaki, Alice Shoshana Jakobovits, Elizabeth Clark, Mirella Lapata, ArXiv, abs/2410.026032024</p>
<p>Scirex: A challenge dataset for document-level information extraction. Sarthak Jain, Madeleine Van Zuylen, Hannaneh Hajishirzi, Iz Beltagy, Annual Meeting of the Association for Computational Linguistics. 2020</p>
<p>Chatgpt is fun, but it is not funny! humor is still challenging large language models. Sophie F Jentzsch, K Kersting, ArXiv, abs/2306.045632023</p>
<p>Knowledge navigator: Llm-guided browsing framework for exploratory search in scientific literature. Uri Katz, Mosh Levy, Yoav Goldberg, arXiv:2408.158362024Preprint</p>
<p>Constraint relaxation and chunk decomposition in insight problem solving. G Knoblich, S Ohlsson, H Haider, D Rhenius, 1534-1555. 00691Journal of Experimental Psychology: Learning, Memory, and Cognition. 199925</p>
<p>Forecasting the future of artificial intelligence with machine learning-based link prediction in an exponentially growing knowledge network. Mario Krenn, Lorenzo Buffoni, Bruno Coutinho, Sagi Eppel, Jacob Gates Foster, Andrew Gritsevskiy, Harlin Lee, Yichao Lu, João P Moutinho, Nima Sanjabi, Rishi Sonthalia, Ngoc M Tran, Francisco Valente, Yangxinyu Xie, Rose Yu, Michael Kopp, Nature Machine Intelligence. 52022</p>
<p>Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction. Yi Luan, Luheng He, Mari Ostendorf, Hannaneh Hajishirzi, ArXiv, abs/1808.096022018</p>
<p>Innovation Relies on the Obscure: A Key to Overcoming the Classic Problem of Functional Fixedness. T Mccaffrey, Psychological Science. 2331172012</p>
<p>The cognitive science of science: explanation, discovery, and conceptual change. Céline Mckeown, Ergonomics. 572014</p>
<p>Followupqg: Towards informationseeking follow-up question generation. Yan Meng, Liangming Pan, Yixin Cao, Min-Yen Kan, International Joint Conference on Natural Language Processing. 2023</p>
<p>Atypical combinations and scientific impact. K William, Simon J Myers, Yaser George, James R Nejaty-Jahromy, R Swartz, Britt, 2013In unknown</p>
<p>Care: Extracting experimental findings from clinical literature. Aakanksha Naik, Bailey Kuehl, Erin Bransom, Doug Downey, Tom Hope, ArXiv, abs/2311.097362023</p>
<p>Care: Extracting experimental findings from clinical literature. Aakanksha Naik, Bailey Kuehl, Erin Bransom, Doug Downey, Tom Hope, Findings of the Association for Computational Linguistics: NAACL 2024. 2024</p>
<p>Scidmt: A large-scale corpus for detecting scientific mentions. Huitong Pan, Qi Zhang, Cornelia Caragea, International Conference on Language Resources and Evaluation. Jan Latecki. 2024Eduard Constantin Dragut, and Longin</p>
<p>Lighttag: Text annotation platform. Tal Perry, Conference on Empirical Methods in Natural Language Processing. 2021</p>
<p>The nature of NLP: Analyzing contributions in NLP papers. Aniket Pramanick, Yufang Hou, M Saif, Iryna Mohammad, Gurevych, Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 63rd Annual Meeting of the Association for Computational LinguisticsVienna, AustriaAssociation for Computational Linguistics20251</p>
<p>Scideator: Human-llm scientific idea generation grounded in research-paper facet recombination. Marissa Radensky, Simra Shahid, Raymond Fok, Pao Siangliulue, Tom Daniel S Weld, Hope, arXiv:2409.146342024arXiv preprint</p>
<p>unarxive: a large scholarly data set with publications' full-text, annotated in-text citations, and links to metadata. Tarek Saier, Michael Färber, Scientometrics. 1252020</p>
<p>Gollie: Annotation guidelines improve zero-shot information-extraction. Oscar Sainz, Iker García-Ferrero, Rodrigo Agerri, ArXiv, abs/2310.036682023Oier López de Lacalle, German Rigau, and Eneko Agirre</p>
<p>Surprising combinations of research contents and contexts are related to impact and emerge with scientific outsiders from distant disciplines. Omar Sharif, Joseph Gatto, Madhusudan Basak, Sarah Masud Preum, Conference on Empirical Methods in Natural Language Processing. Feng Shi and James Evans. 2024. 2023141641Explicit, implicit, and scattered: Revisiting event extraction to capture complex arguments</p>
<p>Surprising combinations of research contents and contexts are related to impact and emerge with scientific outsiders from distant disciplines. Feng Shi, James Allen Evans, arXiv:2409.04109Can llms generate novel research ideas? a largescale human study with 100+ nlp researchers. Chenglei Si, Diyi Yang, Tatsunori Hashimoto, 2019. 202414arXiv preprint</p>
<p>Improving selection of analogical inspirations through chunking and recombination. Arvind Srinivasan, Joel Chan, Proceedings of the 16th Conference on Creativity &amp; Cognition. the 16th Conference on Creativity &amp; Cognition2024</p>
<p>Luminate: Structured generation and exploration of design space with large language models for human-ai co-creation. Sangho Suh, Meng Chen, Bryan Min, Toby Jia-Jun, Haijun Li, Xia, Proceedings of the CHI Conference on Human Factors in Computing Systems. the CHI Conference on Human Factors in Computing Systems2023</p>
<p>Is chatgpt good at search? investigating large language models as re-ranking agent. Weiwei Sun, Lingyong Yan, Xinyu Ma, Pengjie Ren, Dawei Yin, Zhaochun Ren, ArXiv, abs/2304.095422023</p>
<p>You can teach an old dog new tricks! on training knowledge graph embeddings. Can You, Daniel Teach, Samuel Ruffinelli, Rainer Broscheit, Gemulla, International Conference on Learning Representations. 2020</p>
<p>Atypical combinations and scientific impact. Brian Uzzi, Satyam Mukherjee, Michael Stringer, Ben Jones, Science. 34261572013</p>
<p>We are who we cite: Bridges of influence between natural language processing and other academic fields. Jan Philip Wahle, Terry Ruas, Mohamed Abdalla, Bela Gipp, Saif Mohammad, ArXiv, abs/2310.148702023</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7318 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7318</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7318</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-139.html">extraction-schema-139</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <p><strong>Paper ID:</strong> paper-257766897</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2303.14814v1.pdf" target="_blank">WinCLIP: Zero-/Few-Shot Anomaly Classification and Segmentation</a></p>
                <p><strong>Paper Abstract:</strong> Visual anomaly classification and segmentation are vital for automating industrial quality inspection. The focus of prior research in the field has been on training custom models for each quality inspection task, which requires task-specific images and annotation. In this paper we move away from this regime, addressing zero-shot and few-normal-shot anomaly classification and segmentation. Recently CLIP, a vision-language model, has shown revolutionary generality with competitive zero-/few-shot performance in comparison to full-supervision. But CLIP falls short on anomaly classification and segmentation tasks. Hence, we propose window-based CLIP (WinCLIP) with (1) a compositional ensemble on state words and prompt templates and (2) efficient extraction and aggregation of window/patch/image-level features aligned with text. We also propose its few-normal-shot extension WinCLIP+, which uses complementary information from normal images. In MVTec-AD (and VisA), without further tuning, WinCLIP achieves 91.8%/85.1% (78.1%/79.6%) AUROC in zero-shot anomaly classification and segmentation while WinCLIP+ does 93.1%/95.2% (83.8%/96.4%) in 1-normal-shot, surpassing state-of-the-art by large margins.</p>
                <p><strong>Cost:</strong> 0.005</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7318",
    "paper_id": "paper-257766897",
    "extraction_schema_id": "extraction-schema-139",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0048765,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>WinCLIP: Zero-/Few-Shot Anomaly Classification and Segmentation</p>
<p>Jongheon Jeong 
KAIST</p>
<p>Yang Zou 
AWS AI Labs</p>
<p>Taewan Kim 
AWS AI Labs</p>
<p>Dongqing Zhang 
AWS AI Labs</p>
<p>Avinash Ravichandran 
AWS AI Labs</p>
<p>‡ Onkar Dabeer 
AWS AI Labs</p>
<p>WinCLIP: Zero-/Few-Shot Anomaly Classification and Segmentation</p>
<p>Visual anomaly classification and segmentation are vital for automating industrial quality inspection. The focus of prior research in the field has been on training custom models for each quality inspection task, which requires task-specific images and annotation. In this paper we move away from this regime, addressing zero-shot and few-normal-shot anomaly classification and segmentation. Recently CLIP, a vision-language model, has shown revolutionary generality with competitive zero-/few-shot performance in comparison to full-supervision. But CLIP falls short on anomaly classification and segmentation tasks. Hence, we propose window-based CLIP (WinCLIP) with (1) a compositional ensemble on state words and prompt templates and (2) efficient extraction and aggregation of window/patch/image-level features aligned with text. We also propose its few-normal-shot extension Win-CLIP+, which uses complementary information from normal images. In MVTec-AD (and VisA), without further tuning, WinCLIP achieves 91.8%/85.1% (78.1%/79.6%) AU-ROC in zero-shot anomaly classification and segmentation while WinCLIP+ does 93.1%/95.2% (83.8%/96.4%) in 1normal-shot, surpassing state-of-the-art by large margins.</p>
<p>Introduction</p>
<p>Visual anomaly classification (AC) and segmentation (AS) classify and localize defects in industrial manufacturing, respectively, predicting an image or a pixel as normal or anomalous. Visual inspection is a long-tail problem. The objects and their defects vary widely in color, texture, and size across a wide range of industrial domains, including aerospace, automobile, pharmaceutical, and electronics. These result in two main challenges in the field.</p>
<p>First, defects are rare with wide range of variations, leading to a lack of representative anomaly samples in the † Work done during an Amazon internship. * The authors contributed equally. ‡ Work done as part of AWS AI Labs. 1 few-shot and few-normal-shot are used interchangeably in our case. training data. Consequently, existing works have mainly focused on one-class or unsupervised anomaly detection [2,7,8,20,29,31,53,59], which only requires normal images. These methods typically fit a model to the normal images and treat any deviations from it as anomalous. When hundreds or thousands of normal images are available, many methods achieve high-accuracy on public benchmarks [3,8,31]. But in the few-normal-shot regime, there is still room to improve performance [14,32,39,59], particularly in comparison with the fully-supervised upper bound. Second, prior work has focused on training a bespoke model for each visual inspection task, which is not scalable across the long-tail of tasks. This motivates our interest in zero-shot anomaly classification and segmentation. But many defects are defined with respect to a normal image. For example, a missing component on a circuit board is most easily defined with respect to a normal circuit board with all components present. For such cases, at least a few normal images are needed. So in addition to the zero-shot case, we also consider the case of few-normal-shot anomaly classification and segmentation. Since only few normal images are available, there is no segmentation supervision for localizing anomalies, making this a challenging problem across the long-tail of tasks. Figure 2. Motivation of language guided visual inspection. (a) Language helps describe and clarify normality and anomaly; (b) Aggregating multi-scale features helps identify local defects; (c) Normal images provide rich referencing content to visually define normality Vision-language models [1,18,27,36] have shown promise in zero-shot classification tasks. Large-scale training with vision-language annotated pairs learns expressive representations that capture broad concepts. Without additional fine-tuning, text prompts can then be used to extract knowledge from such models for zero-/few-shot transfer to downstream tasks including image classification [27], object detection [11] and segmentation [46]. Since CLIP is one of the few open-source vision-language models, these works build on top of CLIP, benefiting from its generalization ability, and showing competitive low-shot performances in both seen and unseen objects compared to full supervision.</p>
<p>In this paper, we focus on zero-shot and few-normalshot (1 to 4) regime, which has received limited attention [14,32,39]. Our hypothesis is that language is perhaps even more important for zero-shot/few-normal-shot anomaly classification and segmentation. This hypothesis stems from multiple observations. First, "normal" and "anomalous" are states [17] of an object that are context-dependent, and language helps clarify these states. For example, "a hole in a cloth" may be a desirable or undesirable depending upon whether distressed fashion or regular fashion clothes are being manufactured. Language can bring such context and specificity to the broad "normal" and "anomalous" states. Second, language can provide additional information to distinguish defects from acceptable deviations from normality. For example, in Figure 2(a), language provides information on the soldering defect, while minor scratches/stains on background are acceptable. In spite of these advantages, we are not aware of prior work leveraging vision-language models for anomaly classification and segmentation. In this work, with the pre-trained CLIP as a base model, we show and verify our hypothesis that language aids zero-/few-shot anomaly classification/segmentation.</p>
<p>Since CLIP is one of the few open-source vision-language models, we build on top of it. Previously, CLIP-based methods have been applied for zero-shot classification [27]. CLIP can be applied in the same way to anomaly classification, using text prompts for "normal" and "anomalous" as classes. However, we find naïve prompts are not effective (see Table 3). So we improve the naïve baseline with a state-level word ensemble to better describe normal and anomalous states. Another challenge is that CLIP is trained to enforce cross-modal alignment only on the global embeddings of image and text. However, for anomaly segmentation we seek pixel-level classification and it is non-trivial to extract dense visual features aligned with language for zero-shot anomaly segmentation. Therefore, we propose a new Window-based CLIP (WinCLIP), which extracts and aggregates the multiscale features while ensuring vision-language alignment. The multiple scales used are illustrated in Figure 2(b). To leverage normal images available in the few-normal-shot setting, we introduce WinCLIP+, which aggregates complementary information from the language driven WinCLIP and visual cues from the normal reference images, such as the one shown in Figure 2(c). We emphasize that our zero-shot models do not require any tuning for individual cases, and the few-normal-only setup does not use any segmentation annotation, facilitating applicability across a broad range of visual inspection tasks. As a sample, Figure 1 illustrates WinCLIP and WinCLIP+ qualitative results for a few cases.</p>
<p>To summarize, our main contributions are:</p>
<p>• We introduce a compositional prompt ensemble, which improves zero-shot anomaly classification over the naïve CLIP based zero-shot classification.</p>
<p>• Using the pre-trained CLIP model, we propose Win-CLIP, that efficiently extract and aggregate multi-scale spatial features aligned with language for zero-shot anomaly segmentation. As far as we know, we are the first to explore language-guided zero-shot anomaly classification and segmentation.</p>
<p>• We propose a simple reference association method, which is applied to multi-scale feature maps for image based few-shot anomaly segmentation. WinCLIP+ combines the language-guided and vision-only methods for few-normal-shot anomaly recognition.</p>
<p>• We show via extensive experiments on MVTec-AD and VisA benchmarks that our proposed methods Win-CLIP/WinCLIP+ outperform the state-of-the-art methods in zero-/few-shot anomaly classification and segmentation with large margins.  [11,30,41,46]. Good prompt engineering and tuning can non-trivially benefit generalization performances [27,58]. Moreover, some other works [28,56,57] leverage the pre-trained CLIP for language guided detection and segmentation with promising performances.</p>
<p>Related work</p>
<p>Anomaly classification and segmentation. Due to the scarcity of anomalies, the major focus has been on oneclass methods with many normal images [7,8,20,49,51,53]. </p>
<p>Background</p>
<p>Anomaly classification and segmentation. Given an image x ∈ X , both anomaly classification and segmentation (ACS) aim to predict "abnormality" in x. Specifically, we consider anomaly classification (AC) as a binary classifi-cation X → {−, +} where "+" indicates the presence of anomaly in image-level. And anomaly segmentation (AS) is its pixel-level extension to output the location of anomalies via X → {−, +} h×w for a certain image with size h × w. In practice, the tasks are often cast into problems of predicting anomaly scores. For example, anomaly classification typically models a mapping ascore : X → [0, 1] so that a binary classification can be performed by thresholding ascore(x).</p>
<p>Due to the lack of anomalous (or positive) samples in practice, the one-class scenario, where the training data D :
= {(x i , −)} K i=1
consists of only normal (or negative) samples, has been widely used. In this paper, we follow the one-class protocol, particularly focusing on extreme cases of few-shot (K = 1 to 4) and the unexplored zero-shot setups for both AC and AS. And we assume an available list of task-specific texts tags, e.g., for objects and relevant defects.</p>
<p>Zero-shot classification with CLIP. Contrastive Language Image Pre-training (CLIP) [27] is a large-scale pretraining method offering a joint vision-language representation. Given million-scale image-text pairs {(x t , s t )} T t=1 from the web, CLIP trains an image encoder f and a text encoder g via contrastive learning [6,55] to maximize the correlation between f (x t ) and g(s t ) across t in terms of cosine similarity ⟨f (x), g(s)⟩. Given an input x and a closed set of free-form texts S = {s 1 , · · · , s k }, CLIP can perform zero-shot classification via a k-way categorical distribution:
p(s = s i |x; s ∈ S) := exp(⟨f (x), g(s i )⟩/τ ) s∈S exp(⟨f (x), g(s)⟩/τ ) ,(1)
where τ &gt; 0 is the temperature hyperparameter. For a set of class words C = {c 1 , · · · , c k }, it has shown that accompanying each label word c ∈ C with a prompt template, e.g., "a photo of a [c]", improves accuracy over the case without templates. Moreover, an ensemble of prompt embeddings that aggregates multiple (80) templates e.g., "a cropped photo of a [c]", can further boost the performance [27]. Overall, we are essentially "retrieving" the visual knowledge of CLIP through the language interface in appropriate manners. In this paper, we further explore how to extract the knowledge of CLIP in a way more suitable for anomaly recognition.</p>
<p>WinCLIP and WinCLIP+</p>
<p>In this section, we first establish a novel binary zeroshot anomaly classification framework with a Compositional Prompt Ensemble to improve CLIP for anomaly classification (Section 4.1). Next, we propose a simple-yeteffective Window-based CLIP (WinCLIP) for efficient zeroshot anomaly segmentation (Section 4.2). Lastly, we propose an extension WinCLIP+ to benefit from few normal reference images, while maintaining the complementary benefits of language-guided predictions (Section 4.3).</p>
<p>Language-driven zero-shot AC</p>
<p>Two-class design.</p>
<p>We introduce a binary zero-shot anomaly classification framework CLIP-AC by adapting CLIP with two class prompts [c] -"normal [o]" vs. "anomalous [o]". [o] is an object-level label, e.g., "bottle" when available, or simply "object". In addition, we also test a one-class design by only using the normal prompt s − := "normal [o]" to define anomaly score as "−⟨f (x), g(s − )⟩". We observe the simple two-class design from CLIP already yields a non-trial performance and outperforms one-class design significantly in experiments (Table 3). This demonstrates (a) CLIP pre-trained by large web dataset provides a powerful representation with good alignment between text and images for anomaly tasks (b) specific definition about anomaly is necessary for good performance. Compositional prompt ensemble (CPE). Unlike objectlevel classifiers, CLIP-AC performs classification between two states of a given object, i.e., either "normal" or "anomalous", which are subjective with various definitions depending on tasks. For example, "missing transistor" is "anomalous" for a circuit board while "cracked" is "anomalous" for wood. To better define the two abstract states of objects, we propose a Compositional Prompt Ensemble to generate all combinations of pre-defined lists of (a) state words per label and (b) text templates, rather than freely writing definitions. The state words include common states shared by most objects, e.g., "flawless" for normality/"damaged" for anomaly. Also we can optionally add task-specific state words given prior knowledge of defects, e.g., "bad soldering" on PCB. Moreover, we curate a template list specifically for anomaly tasks e.g., "a photo of a [c] for visual inspection". Check details on prompt engineering in supplementary. As in top-left of Figure 4, after getting all the combinations of states and templates, we compute the average of text embeddings per label to represent the normal and anomalous classes. Note that CPE is different from CLIP prompt ensemble that does not explain object labels (e.g., "cat") and only augments templates selected by trial-and-error for object classification, including the ones unsuitable for anomaly tasks, e.g., "a cartoon [c]". Thus, the texts from CPE are more aligned with images in CLIP's joint embedding space for anomaly tasks. We denote the zero-shot scoring model with CPE as ascore 0 : R d → [0, 1] for an image embedding f (x). Remark. Our two-class design with CPE is a novel approach to define anomaly compared to standard one-class methods [31,33]. Anomaly detection is an ill-posed problem due to the open-ended nature. Previous methods model normality only by normal images regarding any deviation from normality as anomaly. Such solution is by nature hard to distinguish true anomalies from acceptable deviations from normality, e.g., "scratch on circuit" vs. "tiny yet acceptable scratch". But language can define states in concrete words. Figure 3. WinCLIP feature extraction in multiple scales of windows through CLIP image encoder, e.g., ViT taking a sequence of (nonmasked) patches as input. Window embeddings encode the global information (e.g., from the class token) within each window.</p>
<p>WinCLIP for zero-shot AS</p>
<p>Given the language guided anomaly scoring model from CPE, we propose Window-based CLIP (WinCLIP) for zeroshot anomaly segmentation to predict pixel-level anomalies. WinCLIP extracts dense visual features with good language alignment and local details for x, followed by applying ascore 0 spatially to obtain the anomaly segmentation map. Specifically, given an image x of resolution h × w and an image encoder f , WinCLIP obtains a map of d-dimensional feature map F W ∈ R h×w×d as follows:</p>
<ol>
<li>
<p>Generate a set of sliding windows {w ij } ij , where each window w ij ∈ {0, 1} h×w is a binary mask that is active locally for a k × k kernel around (i, j).</p>
</li>
<li>
<p>Collect each output embedding F W ij , computed from the active area of x after applying each w ij , defined by:
F W ij := f (x ⊙ w ij ),(2)
where ⊙ is the element-wise product (see Figure 3). Figure 3 illustrates the dense feature extraction of WinCLIP with ViT while it is also applicable to CNN. In addition, we also explore a natural dense representation candidate, penultimate feature map, the last feature map before pooling. Specifically, for patch embedding map F P (other than the class token [CLS]) of ViT-based CLIP, top of Figure 3, we apply ascore 0 patch-wisely for segmentation. However, we observe that such patch-level features are not aligned with the language space, leading to a poor dense predictions (Table 8). We conjecture this is caused by those features have not been directly supervised with language signal in CLIP. Also these patch features have already aggregated the global context due to self-attention, hindering capturing local details for segmentation.</p>
</li>
</ol>
<p>Compared to the penultimate features F P , we remark dense features from WinCLIP is more aligned with language: e.g., for ViT-based CLIP, all the features in F W are now from class tokens which are directly aligned to texts in CLIP pre-training. Also the features focus more on local details via sliding windows. Lastly, WinCLIP can be efficiently computed, especially with ViT architecture. Concretely, the computation of (2) can directly benefit from just dropping all the masked patches before forwarding them, in a similar manner to masked autoencoder [12].</p>
<p>Harmonic aggregation of windows. For each local window, the zero-shot anomaly score M W 0,ij is similarity between the window feature F W ij and text embeddings from compositional prompt ensemble. This score is distributed to every pixel of the local window. Then at each pixel, we aggregate multiple scores from all overlapping windows to improve segmentation by harmonic averaging (3), weighting more on scores towards normality prediction (zero value).
M W 0,ij := 1 u,v (w uv ) ij u,v (w uv ) ij M W 0,uv −1 .(3)
Multi-scale aggregation. The kernel size k corresponds to the amount of surrounding context for each location in computing WinCLIP features (2). It controls the balance between local details and global information in segmentation.</p>
<p>To capture defects of sizes ranging from small to large scale, we aggregate predictions from multi-scale features: e.g., (a) small-scale (2 × 2 in patch scales of ViT; corresponds to 32×32 in pixels), (b) mid-scale (3×3 in ViT; 48×48), and (c) image-scale feature (ViT class token capturing image context due to self-attention). We also adopt harmonic averaging for aggregation. Figure 3 illustrates the features on each scale.</p>
<p>WinCLIP+ with few-normal-shots</p>
<p>For a comprehensive anomaly classification and segmentation, language guided zero-shot approach is not enough as certain defects can only be defined via visual reference rather than only text. For example, "Metal-nut" in MVTec-AD [3] has an anomaly type labeled as "flipped upsidedown", which can only be identified relatively from a normal image. To define and recognize the anomalies more precisely, we propose an extension of WinCLIP, WinCLIP+, by incorporating K normal reference images D :
= {(x i , −)} K i=1
. WinCLIP+ combines the complementary prediction from both language-guided and visual based approachs for better anomaly classification and segmentation.</p>
<p>We first propose a reference association as the key module to incorporate given reference images, which can simply store and retrieve the memory features R of D based on the cosine similarity. Given such module and the corresponding (e.g., patch-level 2 ) features F ∈ R h×w×d extracted from a query image, a prediction M ∈ [0, 1] h×w for anomaly segmentation can be made by:
M ij := min r∈R 1 2 (1 − ⟨F ij , r⟩).(4)
Then we apply this association module at multiple scales of feature maps that are obtained from WinCLIP (see Fig-2 Nevertheless, the module is generally applicable for other scales. ure 4 for the overall illustration). Specifically, given few-shot samples, we construct separate reference memories from three different features: (a) WinCLIP features at small-scale F W s , (b) those at mid-scale F W m , and also (c) from penultimate features F P with global context (e.g., the patch tokens in ViT capturing image context due to self-attention). Even though F P is not aligned with language, it still useful to define normality and anomaly.</p>
<p>As a result, WinCLIP+ gets three reference memories: R W s , R W m , and R P . Then, we average their multi-scale predictions (4) for anomaly segmentation for a given query,
M W := 1 3 (M P + M W s + M W m ),(5)
and then fusing with our language-guided predictionM W 0 . To perform anomaly classification, we combine the maximum value of M W and the WinCLIP zero-shot classification score. The two scores have complementary information to collaborative with, specifically (a) one from the spatial features of few-shot references, and (b) the other one from the CLIP knowledge retrieved via language:
ascore W (x) := 1 2 ascore 0 (f (x)) + max ij M W ij . (6)</p>
<p>Experiments</p>
<p>We perform an array of experiments to evaluate the performance of WinCLIP-based ACS under low-shot regimes, covering recent challenging benchmarks on industrial anomaly classification and segmentation that we are focusing on. We also conduct an extensive ablation study to validate the individual effectiveness of our proposed components. The detailed setups, e.g., pre-processing, metrics, and other implementation details, are given in the supplementary.</p>
<p>Datasets. Our experiments are based on MVTec-AD [3]</p>
<p>and VisA [59] datasets. Both benchmarks have diverse subsets of different objects, e.g., capsules, circuit boards. They contain high-resolution images (e.g., 700 2 -1024 2 for MVTec-AD, and roughly 1.5K × 1K for VisA) of common objects with the full pixel-level annotations.</p>
<p>Evaluation metrics. For classification, we report (a) Area Under the Receiver Operating Characteristic (AUROC) following the literature [8,31,49], as well as (b) Area Under the Precision-Recall curve (AUPR) and (c) F 1 -score at optimal threshold (F 1 -max) for a clearer view against potential data imbalance [59]). For segmentation, we report (a) pixel-wise AUROC (pAUROC) and (b) Per-Region Overlap (PRO) [4] scores [8,20], and (c) (pixel-wise) F 1 -max in a similar manner to the anomaly classification evaluation.</p>
<p>Implementation details. We adopt the CLIP implementation of OpenCLIP 3 and its public pre-trained models in our 3 https://github.com/mlfoundations/open_clip experiments: namely, we use the LAION-400M [37] based CLIP with ViT-B/16+ [16] unless otherwise noted. We apply WinCLIP with stride 1 on ViT patch embeddings, which is equivalent to stride 16 in pixel-level in case of ViT-B/16+.</p>
<p>Zero-/few-shot anomaly classification</p>
<p>In Table 1 we compare zero-shot and few-normal-shot anomaly classification results with prior works.</p>
<p>For zero-shot setup, we compare WinCLIP with two prior models: CLIP-AC (first row of Table 1), which is the original CLIP zero-shot classification [27] with labels of the form {"normal [c]", "anomalous [c]"}, and CLIP-AC with the prompt ensemble (second row in Table 1) from [27] engineered for ImageNet [19]. We see that Win-CLIP significantly improves over using these naïve adaptations of CLIP on both MVTec-AD and VisA. Section 5.4 presents ablation study on a break-down of this gain.</p>
<p>For the few-normal-shot setup, we see the same trend: WinCLIP+ outperforms prior works by a wide margin across all metrics on both benchmarks. In particular, we improve upon the state-of-the-art PatchCore [31] by 9.7% on 1-shot MVTec-AD and by 5.3% on 1-shot VisA. On MVTec-AD, we note that zero-shot WinCLIP outperforms the few-shot versions of prior works. Furthermore, WinCLIP+ 1/2/4-shot performance is better than WinCLIP 0-shot performance, highlighting the additional value of reference normal images.</p>
<p>Zero-/few-shot anomaly segmentation</p>
<p>In Table 4 we compare zero-shot and few-normal-shot anomaly segmentation results with prior works. While there are no prior works on zero-shot anomaly segmentation, we adapt two methods developed for other problems to our setup. First, Trans-MM [5] is a recent model interpretation method applicable to Transformers that provides a pixel-level mask. Second, MaskCLIP [57] is a general semantic segmentation model based on CLIP. We see that WinCLIP outperforms both methods by a wide margin on both MVTec-AD and VisA, highlighting that generic adaptations of CLIP do not perform as well as WinCLIP.</p>
<p>For the few-normal-shot setup, we compare with three prior works, which are designed specifically for anomaly localization. We see that WinCLIP+ again outperforms these prior methods across all metrics on both benchmarks, showing the additional value provided by language prompts. In Figure 5, we show qualitative results for a number of objects and defects. We see that in all cases, 1-shot WinCLIP+ provides a mask that is more concentrated on the ground truth compared to prior works. We also see that 1/2/4-normal-shot WinCLIP+ is better than 0-shot WinCLIP, demonstrating the complementary benefits of language driven prediction and visual only based model based on reference normal images. </p>
<p>Comparison with many-shot methods</p>
<p>In Table 2 we compare our zero-/few-shot results with full-shot results of several prior works on MVTec-AD. Our 4-shot WinCLIP+ is competitive with CutPaste [20], a recent method that utilizes the full-shot samples for model tuning. Also, our 0-shot WinCLIP outperforms recent few-shot methods in AC, such as DifferNet [32] and TDG [39], even compared to their results with more than 10-shots. Recently, a new setup of aggregated few-shot is proposed [14], where one is free to use all the training samples but for the target class which is restricted to k-shot. Our 4-shot WinCLIP+ outperforms RegAD's aggregated 4-shot [14] performance.</p>
<p>Ablation study</p>
<p>We perform component-wise analysis on MVTec-AD [3]. A further study, e.g., comparison with CLIP-based Patch-Core, effect of different backbones, discussion on failure cases, etc., can be found in the supplementary material.</p>
<p>WinCLIP for AC: In Table 3, we report the individual effect of components that constitute our zero-shot AC model. Firstly, we observe (a) the textual supervision for the word "anomalous" is crucial to achieve a reasonable performance ("One-class"; Section 4.1), suggesting the effectiveness of CLIP knowledge about "abnormality". Next, we confirm that having a diversity in both (b) state-level and (c) prompt-level texts are the key source of gains. And we remark the proposed state ensemble as a more significant component. Finally, we observe (d) applying multi-crop prediction [13] could also yield a minor improvement.</p>
<p>WinCLIP for AS: Table 8 validates not only the efficiency of WinCLIP to extract local features for zero-shot AS, but also the effectiveness of multi-scale and harmonic averaging to boost the results. To this end, we consider the following additional baselines that also extract patch-level features: (i) Patch-token (Section 4.2): it takes the patch features at the last layer, and (ii) Image tiling: it first performs dense "tiling" on an image and then obtains "tile" embeddings for segmentation by forwarding each tile with resizing. Overall, the comparison shows that patch-tokens are not aligned with language despite its fast inference time, while "Image tiling" makes a significant computational overhead although it does benefit from their local features. WinCLIP achieves accelerated inference due to its window-based computation of local features, with even better performance. Also based on the multi-scale study, we observe that segmentation benefits from both features with image-level, and middle/local context. Note that the scores from last patch embeddings of ViT encodes global context thanks to self-attention, which contributes to a comprehensive localization in WinCLIP.</p>
<p>WinCLIP+ for AC and AS:</p>
<p>We ablate on different factors to define WinCLIP+ scores for AC (6) and AS (5) respectively. For AC, from Table 5, we clearly remark the effectiveness of ascore 0 upon max M W . Interestingly, we observe ascore 0 is beneficial even in higher-shot regimes where max M W can be better, confirming their complementary effects. For AS, in Table 6, we notice the effect of adding M W m (or M W s ) upon M P , i.e., the prediction from WinCLIP features: apart from the good performance of M P , M W could still provide useful information from its local-awareness.</p>
<p>WinCLIP with task-specific defects: As mentioned in Section 4.1, besides using the generic state words and tem-   plates (Fig. 6 of supplementary) to cover common cases, our compositional prompt ensemble also supports task-specific state words, e.g., "missing part" on PCB/"burnt" pipe fryum; both VisA and MVTec-AD release specific defect types. Ablation study in Table 7 shows that specific state words further improve zero-shot classification in VisA by 0.8% average AUROC with 5.3% gain on the challenging PCB2.</p>
<p>Conclusion</p>
<p>We propose a novel framework to define normality and anomaly via both fine-grained textual definitions and normal reference images for comprehensive anomaly classification and segmentation. First, we show that the CLIP pre-trained on large-scale web data provides a powerful representation with good alignment between texts and images for anomaly recognition tasks. The compositional prompt ensemble defines the normality and anomaly in text and helps to distill knowledge from the pre-trained CLIP for better zero-shot anomaly recognition. WinCLIP efficiently aggregates multiscale features with image-text alignment from window and image-level to perform zero-shot segmentation. Moreover, given a few normal samples, vision based reference association provides complementary information about the two states to language definitions, leading to few-shot WinCLIP+. In recent benchmarks, WinCLIP and WinCLIP+ outperform state-of-the-arts in zero-/few-shot setups with considerable margins. We believe our work will bring values complementary to standard one-class methods. For further improvement, vision-language pre-training with industrial domain data is a promising direction that is left as a future work. Compositional prompt ensemble. Figure 6 provides a detailed list of prompts we adopt to perform compositional prompt ensemble proposed in Section 4.1 of the main text. Recall that we consider two levels of prompts: i.e., (a) state-level, and (b) template level. A complete prompt can be composed by replacing the token [c] in a template-level prompt with one of state-level prompt, either from the normal or anomaly states. Each of the state-level prompt takes an object-level label [o]. In our experiments, we use the object name words available for both MVTec-AD and VisA per dataset to replace [o].</p>
<p>References</p>
<p>Data pre-processing. For CLIP-based models, including our proposed WinCLIP and WinCLIP+, we apply the data preprocessing pipeline given in OpenCLIP [16] for both MVTec-AD and VisA datasets to minimize potential train-test discrepancy. Specifically, it performs a channel-wise standardization with the pre-computed mean [0.48145466, 0.4578275, 0.40821073] and standard deviation [0.26862954, 0.26130258, 0.27577711] after normalizing each RGB image into [0, 1], followed by a bicubic re-sizing based on the Pillow implementation. By default, we make the input resolution to be 240 for the shorter edge from the re-sizing, to be compatible with ViT-B/16+ in our experiments. This re-sizing policy also applies to other baseline models for fairer comparisons, although we keep the remaining parts of their original data pre-processing pipelines. In addition, similar policy can also be used in other backbones with input of different resolutions.</p>
<p>Evaluation metrics. Although the AUROC is a good metric for balanced dataset, it provides an inflated view of model performance in imbalanced dataset, especially in anomaly segmentation where the normal pixels dominate anomalies. This is also discussed by Zou et al. [59]. F 1 -max is computed from the precision and recall for the anomalous samples at the optimal threshold, which is a more straightforward metric to measure the upper bound of anomaly prediction performance across thresholds. Thus we acknowledge that the low-shot anomaly segmentation is still not solved since our best model only achieves &lt; 60% F 1 -max for both MVTec-AD and VisA, even though WinCLIP+ achieves &gt; 95% pixel-AUROC. In addition, our WinCLIP and WinCLIP+ outperform all the compared methods in terms of all these metrics on the setups, demonstrating the effectiveness of the proposed methods. • "a cropped photo of a [c]."</p>
<p>• "a close-up photo of a [c]."</p>
<p>• "a close-up photo of the [c]."</p>
<p>• "a bright photo of a [c]."</p>
<p>• "a bright photo of the [c]."</p>
<p>• "a dark photo of the [c]."</p>
<p>• "a dark photo of a [c]."</p>
<p>• "a jpeg corrupted photo of a [c]."</p>
<p>• "a jpeg corrupted photo of the [c]."</p>
<p>• (cont'd) "a blurry photo of the [c]."</p>
<p>• "a blurry photo of a [c]."</p>
<p>• "a photo of a [c]."</p>
<p>• "a photo of the [c]."</p>
<p>• "a photo of a small [c]."</p>
<p>• "a photo of the small [c]."</p>
<p>• "a photo of a large [c]."</p>
<p>• "a photo of the large [c]."</p>
<p>• "a photo of the [c] for visual inspection."</p>
<p>• "a photo of a [c] for visual inspection."</p>
<p>• "a photo of the [c] for anomaly detection."</p>
<p>• "a photo of a [c] for anomaly detection."   Table 8. Comparison of few-shot performances on MVTec-AD. We report the mean and standard deviation over 5 random seeds for each measurement. Bold indicates the best performance. as in (b) the input resolution (224 2 → 240 2 ; 196 → 225 tokens); (ii) We note that CLIP models require the square-shaped resolution, e.g., 240 2 for ViT-B/16+, to be compatible with the attention layers inside. Although the MVTec-AD benchmark already consists of square images, most of images in the VisA benchmark are non-squared (e.g., 1500 × 1000) and simply taking a crop can affect the anomaly status of the given images. In this respect, to enable CLIP-based models properly handle non-squared images in our experiments, we perform a simple "image tiling" scheme. Specifically, for such non-squared images, we first extract multiple overlapping (squared) "tiles" of size the shorter edge L s , by taking a sliding window across the longer edge. Then we average the predictions from the tiles to get the final (either in image-and pixel-level) prediction. The stride for the sliding is set to 0.8 · L s at most, i.e., the tiles have overlaps with its neighbors at least in 0.2 · L s ; (iii) In addition, for the baseline results, we use our re-implementation of SPADE [7] and PaDiM [8], and adopt the official implementation of PatchCore 4 in our experiments.</p>
<p>B. Additional results on ablation study</p>
<p>Comparison with CLIP-based PatchCore: PatchCore [31], a current state-of-the-art considered in our experiments, is originally based on the internal features of convolutional network: e.g., WideResNet-50-2 (WRN-50-2) [52] pre-trained on ImageNet. In Table 8, we test whether PatchCore can further benefit from the CLIP-based backbone that our WinCLIP+ is based on. Specifically, we additionally consider two variants of PatchCore that take the patch-token features of CLIP-based ViT-B/16+ backbone, one from (a) the 6 th -and 9 th -layer of ViT (which corresponds to block2 and block3 in ResNet-like models as considered by [31]; "hidden"), and the other one from (b) the last layer of ViT ("last"). Overall, we have the following observations. First, in case of the ViT-B/16+ backbone, PatchCore performs better with the last layer, which is in contrast to the cases of convolutional backbones. Second, compared to the original PatchCore, the CLIP-based variants achieve no better performances. Third, WinCLIP+ significantly outperforms "PatchCore (last)" where our WinCLIP+ also utilizes the last patch-token features, namely as referred as F P (Section 4.3 of the main text). The results confirm the effectiveness of (a) our simple association-based module over a more sophisticated PatchCore 5 in ViT, and (b) the WinCLIP features F W .</p>
<p>Effect of different CLIP backbones: Table 9, on the other hand, explores the effect of different CLIP architectures to the WinCLIP zero-shot performance. Specifically, on zero-shot setups, we compare AUROC (and pixel-AUROC) from WinCLIP in AC (and AS) testing over the CLIP pre-trained models available at OpenCLIP, 6 including our default choice of ViT-B/16+. To apply WinCLIP for ResNet-based backbones, we notice that the CLIP implementation of ResNet architectures incorporates an attention layer to perform the feature pooling, namely as attention pooling, similar to ViT-based architectures. In this respect, for the CLIP-ResNet models, we apply our window-based inference to perform zero-shot AS from the convolutional feature map before the attention pooling, in the same way of applying WinCLIP for ViTs. Here, we remark that the effective patch size of each pixel on the last feature map (before the pooling) of ResNet-based models is designed to be 32 (the downsampling rate), which is larger than those of ViTs we test, e.g., of 16. Overall, we observe that ViT-based models generally show better performance compared to ResNets, in both AC and AS. The particular gap in AS is possibly due to the bigger patch sizes in ResNets, which can result in more blurry outputs. Still, we observe the performance benefits from larger models or resolutions in both types of architecture.</p>
<p>C. Additional qualitative results</p>
<p>In Figure 7-10, we provide further qualitative results obtained from our (zero-shot) WinCLIP and (few-shot) WinCLIP+ for anomaly segmentation, both in MVTec-AD and VisA considered in our experiments. Specifically, we report MVTec-AD results in Figure 7 and 8, and VisA results in Figure 9 and 10.    Failure cases. We present some failure examples from both MVTec-AD and VisA for language driven zero-shot WinCLIP in Figure 11. Note that the normal images are shown just for better illustration and are not used in model prediction. The first major factor causing the failure is the logical anomaly [2] illustrated in Figure 11(a), e.g., misplaced axis in cable, missing text on capsule, missing capacitor in PCB1 and bent component in PCB3. Such type of anomalies need to be clarified by normal reference images while language might be not sufficient. The issues are alleviated by our few-normal-shot WinCLIP+. The second major factor refers to tiny defect illustrated in Figure 11(b), such as the ones in carpet, wood, capsule, macaroni1. We conjecture that spatial features with more local details might improve these cases, which is left for future exploration. The third major factor is the irrelevant deviation from normality that are not defects of interests illustrated in Figure 11(c), e.g., the tiny red/white dots in pill/hazelnut, extra ingredient on cashew, designed holes and acceptable scratches in PCB2. We hypothesize that more clarification on these deviation and a pre-trained model with better understanding on these states might alleviate the problem. Lastly, although WinCLIP can roughly localize anomalies such as the cases in bottle, tile, PCB4 and fryum, it makes some errors around the true positives, illustrated in Figure 11(d). However, we argue this is minor as the rough anomaly localization is sufficient to explain where the defects are for visual inspection. </p>
<p>D. Detailed quantitative results</p>
<p>In this section, we report the detailed, subset-level performance values for the evaluation metrics provided in Table 1 and 4 of the main text. Specifically, we report MVTec-AD results in Table 10-15 and VisA results in Table 16-21. MVTec-AD (AC)  Table 10. Comparison of anomaly classification (AC) performance in terms of class-wise AUROC on MVTec-AD. We report the mean and standard deviation over 5 random seeds for each measurement.
K = 0 K = 1 K = 2 K = 4 AUROC WinCLIP SPADE
MVTec-AD (AC)  Table 11. Comparison of anomaly classification (AC) performance in terms of class-wise AUPR on MVTec-AD. We report the mean and standard deviation over 5 random seeds for each measurement.
K = 0 K = 1 K = 2 K = 4 AUPR WinCLIP SPADE
MVTec-AD (AC) K = 0 K = 1 K = 2 K = 4  Table 12. Comparison of anomaly classification (AC) performance in terms of class-wise F1-max on MVTec-AD. We report the mean and standard deviation over 5 random seeds for each measurement.</p>
<p>MVTec-AD (AS)</p>
<p>VisA (AC) Table 16. Comparison of anomaly classification (AC) performance in terms of class-wise AUROC on VisA. We report the mean and standard deviation over 5 random seeds for each measurement.</p>
<p>VisA (AC) WinCLIP+  Table 17. Comparison of anomaly classification (AC) performance in terms of class-wise AUPR on VisA. We report the mean and standard deviation over 5 random seeds for each measurement.
K = 0 K = 1 K = 2 K = 4 AUPR WinCLIP SPADE PaDiM PatchCore WinCLIP+ SPADE PaDiM PatchCore WinCLIP+ SPADE PaDiM PatchCore
VisA (AC) K = 0 K = 1 K = 2 K = 4 F 1 -max WinCLIP SPADE PaDiM PatchCore WinCLIP+ SPADE PaDiM PatchCore WinCLIP+ SPADE PaDiM PatchCore WinCLIP+  Table 18. Comparison of anomaly classification (AC) performance in terms of class-wise F1-max on VisA. We report the mean and standard deviation over 5 random seeds for each measurement.</p>
<p>VisA (AS)
K = 0 K = 1 K = 2 K = 4 pAUROC
WinCLIP SPADE PaDiM PatchCore WinCLIP+ SPADE PaDiM PatchCore WinCLIP+ SPADE PaDiM PatchCore WinCLIP+  Table 19. Comparison of anomaly segmentation (AS) performance in terms of class-wise pixel-AUROC on VisA. We report the mean and standard deviation over 5 random seeds for each measurement.</p>
<p>VisA (AS)
K = 0 K = 1 K = 2 K = 4 PRO WinCLIP SPADE PaDiM PatchCore WinCLIP+ SPADE PaDiM PatchCore WinCLIP+ SPADE PaDiM PatchCore WinCLIP+
Figure 1 .
1Language guided zero-/one-shot 1 anomaly segmentation from WinCLIP/WinCLIP+. Best viewed in color and zoom in.</p>
<p>Figure 4 .
4Workflows of WinCLIP/WinCLIP+ (upper/entire pane). Various states and templates are composited and converted to two text embeddings as class prototypes via CLIP text encoder (Section 4.1). The class prototypes are correlated with the multi-scale features from CLIP image encoder(Figure 3) for zero-shot AC/AS in WinCLIP. WinCLIP+ applies the reference association on patch, small-/mid-window (Patch/WindowAssociation) for vision-based anomaly score maps, which are aggregated for few-shot AS/AC with language-guided scores.</p>
<p>Figure 5 .
5Qualitative comparison of 1-shot anomaly segmentation results on MVTec-AD and VisA benchmarks. 1±0.0 64.6±0.0 31.7±0.0 79.6±0.0 56.8±0.0 14.8±0.0</p>
<p>•
Other implementation details. (i) The ViT-B/16+ architecture [16], that we mainly adopt in our experiments, is a modification of ViT-B/16 [42] with (a) an increased dimension in both image (768 → 896) and text (512 → 640) embeddings, as well (a) State-level (normal) "a cropped photo of the [c]."</p>
<p>Figure 6 .
6Lists of multi-level prompts considered in this paper to construct compositional prompt ensemble.</p>
<p>Figure 7 .
7Additional qualitative results from WinCLIP (0-shot), tested on MVTec-AD.</p>
<p>Figure 8 .
8Additional qualitative results from few-shot WinCLIP+ (4-shot), tested on MVTec-AD.</p>
<p>Figure 9 .
9Additional qualitative results from WinCLIP (0-shot), tested on VisA.</p>
<p>Figure 10 .
10Additional qualitative results from few-shot WinCLIP+ (4-shot), tested on VisA.</p>
<p>Figure 11 .
11Curated illustrations of failure cases from zero-shot WinCLIP.</p>
<p>Prompt ens. [27] 74.1±0.0 89.5±0.0 87.8±0.0 58.2±0.0 66.4±0.0 74.0±0.0Table 1. Comparison of anomaly classification (AC) performance on MVTec-AD and VisA benchmarks. We report the mean and standard deviation over 5 random seeds for each measurement. Bold indicates the best performance.Anomaly Classification </p>
<p>MVTec-AD 
VisA </p>
<p>Setup Method 
AUROC 
AUPR 
F1-max 
AUROC 
AUPR 
F1-max </p>
<p>0-shot </p>
<p>CLIP-AC [27] 
74.0±0.0 89.1±0.0 88.5±0.0 59.3±0.0 67.0±0.0 74.4±0.0 
+ WinCLIP (ours) 
91.8±0.0 96.5±0.0 92.9±0.0 78.1±0.0 81.2±0.0 79.0±0.0 </p>
<p>1-shot </p>
<p>SPADE [7] 
81.0±2.0 90.6±0.8 90.3±0.8 79.5±4.0 82.0±3.3 80.7±1.9 
PaDiM [8] 
76.6±3.1 88.1±1.7 88.2±1.1 62.8±5.4 68.3±4.0 75.3±1.2 
PatchCore [31] 
83.4±3.0 92.2±1.5 90.5±1.5 79.9±2.9 82.8±2.3 81.7±1.6 </p>
<p>WinCLIP+ (ours) 93.1±2.0 96.5±0.9 93.7±1.1 83.8±4.0 85.1±4.0 83.1±1.7 </p>
<p>2-shot </p>
<p>SPADE [7] 
82.9±2.6 91.7±1.2 91.1±1.0 80.7±5.0 82.3±4.3 81.7±2.5 
PaDiM [8] 
78.9±3.1 89.3±1.7 89.2±1.1 67.4±5.1 71.6±3.8 75.7±1.8 
PatchCore [31] 
86.3±3.3 93.8±1.7 92.0±1.5 81.6±4.0 84.8±3.2 82.5±1.8 </p>
<p>WinCLIP+ (ours) 94.4±1.3 97.0±0.7 94.4±0.8 84.6±2.4 85.8±2.7 83.0±1.4 </p>
<p>4-shot </p>
<p>SPADE [7] 
84.8±2.5 92.5±1.2 91.5±0.9 81.7±3.4 83.4±2.7 82.1±2.1 
PaDiM [8] 
80.4±2.5 90.5±1.6 90.2±1.2 72.8±2.9 75.6±2.2 78.0±1.2 
PatchCore [31] 
88.8±2.6 94.5±1.5 92.6±1.6 85.3±2.1 87.5±2.1 84.3±1.3 </p>
<p>WinCLIP+ (ours) 95.2±1.3 97.3±0.6 94.7±0.8 87.3±1.8 88.8±1.8 84.2±1.6 </p>
<p>Methods 
Setup 
AC 
AS </p>
<p>WinCLIP (ours) 
0-shot 
91.8 85.1 
WinCLIP+ (ours) 
1-shot 
93.1 95.2 
WinCLIP+ (ours) 
4-shot 
95.2 96.2 </p>
<p>DifferNet [32] 
16-shot 
87.3 
-
TDG [39] 
10-shot 
78.0 
-
RegAD-L [14] 
2-shot 
81.5 93.3 
RegAD [14] 
4 + agg. 88.2 95.8 </p>
<p>MKD [35] 
full-shot 87.7 90.7 
P-SVDD [49] 
full-shot 92.1 95.7 
CutPaste [20] 
full-shot 95.2 96.0 
PatchCore [31] 
full-shot 99.6 98.2 </p>
<p>Table 2. Comparison with existing 
many-shot ACS methods in AUROC 
(or pixel-) on MVTec-AD. </p>
<p>Method 
AUROC AUPR F1-max </p>
<p>(a) One-class 
34.2 
68.9 
83.5 </p>
<p>Two-class 
74.0 
89.1 
88.5 
(b) + State ens. 
89.8 
95.6 
92.2 
(c) + Prompt ens. 
90.8 
96.1 
92.5 
(d) + Multi-crop 
91.8 
96.5 
92.9 </p>
<p>Table 3. Comparison of AC perfor-
mance on MVTec-AD across Win-
CLIP ablations in AC (Section 4.1). </p>
<p>Table 5 .
5Table 6. k-shot AS ablations: MVTec-AD. Bold/underline indicate the best/runner-up.k-shot AC ablations: MVTec-AD. 
Bold/underline indicate the best/runner-up. </p>
<p>WinCLIP+ (AS) </p>
<h1>shots (pAUROC)</h1>
<p>M P M W </p>
<p>m </p>
<p>M W </p>
<p>s </p>
<p>1 
2 
4 
8 </p>
<p>✓ 
✗ 
✗ 
94.5 94.8 95.4 95.8 </p>
<p>✓ 
✓ 
✗ 
95.1 95.7 96.3 96.6 
✓ 
✓ 
✓ 
95.2 96.0 96.2 96.5 </p>
<p>Method 
PCB2 PCB4 Pipe fryum Mean </p>
<p>WinCLIP 
51.2 
79.6 
69.7 
78.1 
+ specific states 
56.5 
82.7 
70.4 
78.9 </p>
<p>Table 7. Ablation on specific states: VisA. </p>
<p>[ 1 ]
1Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. Flamingo: A visual language model for few-shot learning. Hila Chefer, Shir Gur, and Lior Wolf. Generic attention-model explainability for interpreting bi-modal and encoder-decoder Transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 397-406, 2021. 6, 8 [6] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International Conference on Machine Learning, pages 1597-1607. PMLR, 2020. 3 [7] Niv Cohen and Yedid Hoshen. Sub-image anomaly detection with deep pyramid correspondences. Stanislav Fort, Jie Ren, and Balaji Lakshminarayanan. Exploring the limits of out-of-distribution detection. Geoffrey E Hinton, Alex Krizhevsky, and Ilya Sutskever. Ima-geNet classification with deep convolutional neural networks. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Ima-geNet classification with deep convolutional neural networks. Karsten Roth, Latha Pemula, Joaquin Zepeda, Bernhard Schölkopf, Thomas Brox, and Peter Gehler. Towards total recall in industrial anomaly detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé Jégou. Training data-efficient image transformers &amp; distillation through attention. In International Conference on Machine Learning, pages 10347-10357. PMLR, 2021. 12 [43] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Max Welling and Thomas N Kipf. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations, 2017. 3 [45] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. SUN database: Large-scale scene recognition from abbey to zoo. In IEEE Computer SocietyarXiv preprint 
arXiv:2204.14198, 2022. 2, 3 
[2] Paul Bergmann, Kilian Batzner, Michael Fauser, David Sat-
tlegger, and Carsten Steger. Beyond dents and scratches: 
Logical constraints in unsupervised anomaly detection and 
localization. International Journal of Computer Vision, 
130(4):947-969, 2022. 1, 17 
[3] Paul Bergmann, Michael Fauser, David Sattlegger, and 
Carsten Steger. MVTec AD -A comprehensive real-world 
dataset for unsupervised anomaly detection. In Proceedings 
of the IEEE/CVF conference on Computer Vision and Pattern 
Recognition, pages 9592-9600, 2019. 1, 3, 5, 6, 7 
[4] Paul Bergmann, Michael Fauser, David Sattlegger, and 
Carsten Steger. Uninformed students: Student-teacher 
anomaly detection with discriminative latent embeddings. 
In Proceedings of the IEEE/CVF Conference on Computer 
Vision and Pattern Recognition, pages 4183-4192, 2020. 6 
[5] arXiv preprint 
arXiv:2005.02357, 2020. 1, 3, 7, 8, 13 
[8] Thomas Defard, Aleksandr Setkov, Angelique Loesch, and 
Romaric Audigier. PaDiM: a patch distribution modeling 
framework for anomaly detection and localization. In Inter-
national Conference on Pattern Recognition, pages 475-489. 
Springer, 2021. 1, 3, 6, 7, 8, 13 
[9] Advances 
in Neural Information Processing Systems, 34:7068-7081, 
2021. 3 
[10] Gabriel Goh, Nick Cammarata, Chelsea Voss, Shan Carter, 
Michael Petrov, Ludwig Schubert, Alec Radford, and 
Chris Olah. Multimodal neurons in artificial neural net-
works. Distill, 2021. https://distill.pub/2021/ 
multimodal-neurons. 3 
[11] Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui. Open-
vocabulary object detection via vision and language knowl-
edge distillation. In International Conference on Learning 
Representations, 2021. 2, 3 
[12] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr 
Dollár, and Ross Girshick. Masked autoencoders are scalable 
vision learners. In Proceedings of the IEEE/CVF Conference 
on Computer Vision and Pattern Recognition, pages 16000-
16009, 2022. 5 
[13] Advances in Neural Information Processing Systems, 25(1106-
1114):1, 2012. 3, 7 
[14] Chaoqin Huang, Haoyan Guan, Aofan Jiang, Ya Zhang, 
Michael Spratlin, and Yanfeng Wang. Registration based 
few-shot anomaly detection. In European Conference on 
Computer Vision, 2022. 1, 2, 3, 7 
[15] Drew A Hudson and Christopher D Manning. GQA: A new 
dataset for real-world visual reasoning and compositional 
question answering. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition, pages 
6700-6709, 2019. 3 
[16] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade 
Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal 
Shankar, Hongseok Namkoong, John Miller, Hannaneh Ha-
jishirzi, Ali Farhadi, and Ludwig Schmidt. OpenCLIP. Zen-
odo, July 2021. 3, 6, 12 
[17] Phillip Isola, Joseph J Lim, and Edward H Adelson. Dis-
covering states and transformations in image collections. In 
Proceedings of the IEEE Conference on Computer Vision and 
Pattern Recognition, pages 1383-1391, 2015. 2, 3 
[18] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, 
Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom 
Duerig. Scaling up visual and vision-language representation 
learning with noisy text supervision. In International Confer-
ence on Machine Learning, pages 4904-4916. PMLR, 2021. 
2, 3 
[19] Communications of the ACM, 60(6):84-90, 2017. 6 
[20] Chun-Liang Li, Kihyuk Sohn, Jinsung Yoon, and Tomas Pfis-
ter. CutPaste: Self-supervised learning for anomaly detection 
and localization. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition, pages 
9664-9674, 2021. 1, 3, 6, 7 
[21] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq 
Joty, Caiming Xiong, and Steven Chu Hong Hoi. Align 
before fuse: Vision and language representation learning 
with momentum distillation. Advances in neural information 
processing systems, 34:9694-9705, 2021. 3 
[22] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mot-
taghi, and Aniruddha Kembhavi. Unified-IO: A unified model 
for vision, language, and multi-modal tasks. arXiv preprint 
arXiv:2206.08916, 2022. 3 
[23] M Mancini, MF Naeem, Y Xian, and Zeynep Akata. Open 
world compositional zero-shot learning. In 34th IEEE Con-
ference on Computer Vision and Pattern Recognition. IEEE, 
2021. 3 
[24] Massimiliano Mancini, Muhammad Ferjad Naeem, Yongqin 
Xian, and Zeynep Akata. Learning graph embeddings for 
open world compositional zero-shot learning. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence, 2022. 
3 
[25] Pankaj Mishra, Riccardo Verk, Daniele Fornasier, Claudio 
Piciarelli, and Gian Luca Foresti. VT-ADL: A vision trans-
former network for image anomaly detection and localization. 
In 30th IEEE/IES International Symposium on Industrial Elec-
tronics (ISIE), June 2021. 3 
[26] MF Naeem, Y Xian, F Tombari, and Zeynep Akata. Learn-
ing graph embeddings for compositional zero-shot learning. 
In 34th IEEE Conference on Computer Vision and Pattern 
Recognition. IEEE, 2021. 3 
[27] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya 
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, 
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning 
transferable visual models from natural language supervision. 
In International Conference on Machine Learning, pages 
8748-8763. PMLR, 2021. 2, 3, 6, 7 
[28] Yongming Rao, Wenliang Zhao, Guangyi Chen, Yansong 
Tang, Zheng Zhu, Guan Huang, Jie Zhou, and Jiwen Lu. 
Denseclip: Language-guided dense prediction with context-
aware prompting. In Proceedings of the IEEE Conference on 
Computer Vision and Pattern Recognition (CVPR), 2022. 3 
[29] Nicolae-Catalin Ristea, Neelu Madan, Radu Tudor Ionescu, 
Kamal Nasrollahi, Fahad Shahbaz Khan, Thomas B Moes-
lund, and Mubarak Shah. Self-supervised predictive convolu-
tional attentive block for anomaly detection. In Proceedings 
of the IEEE/CVF Conference on Computer Vision and Pattern 
Recognition, 2022. 1 
[30] Robin Rombach, Andreas Blattmann, Dominik Lorenz, 
Patrick Esser, and Björn Ommer. High-resolution image 
synthesis with latent diffusion models, 2021. 3 
[31] Recognition, pages 14318-14328, 2022. 1, 3, 4, 6, 7, 8, 13 
[32] Marco Rudolph, Bastian Wandt, and Bodo Rosenhahn. Same 
same but DifferNet: Semi-supervised defect detection with 
normalizing flows. In Proceedings of the IEEE/CVF Winter 
Conference on Applications of Computer Vision, pages 1907-
1916, 2021. 1, 2, 3, 7 
[33] Lukas Ruff, Robert A. Vandermeulen, Nico Görnitz, Lucas 
Deecke, Shoaib A. Siddiqui, Alexander Binder, Emmanuel 
Müller, and Marius Kloft. Deep one-class classification. In 
Proceedings of the 35th International Conference on Machine 
Learning, volume 80, pages 4393-4402, 2018. 4 
[34] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, 
Aditya Khosla, Michael Bernstein, et al. ImageNet large 
scale visual recognition challenge. International Journal of 
Computer Vision, 115(3):211-252, 2015. 3 
[35] Mohammadreza Salehi, Niousha Sadjadi, Soroosh 
Baselizadeh, Mohammad H Rohban, and Hamid R Rabiee. 
Multiresolution knowledge distillation for anomaly detection. 
In Proceedings of the IEEE/CVF Conference on Computer 
Vision and Pattern Recognition, pages 14902-14912, 2021. 7 
[36] Christoph Schuhmann, Romain Beaumont, Richard Vencu, 
Cade W Gordon, Ross Wightman, Mehdi Cherti, Theo 
Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts-
man, Patrick Schramowski, Srivatsa R Kundurthy, Katherine 
Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia 
Jitsev. LAION-5B: An open large-scale dataset for training 
next generation image-text models. In Thirty-sixth Confer-
ence on Neural Information Processing Systems Datasets and 
Benchmarks Track, 2022. 2, 3 </p>
<p>[37] Christoph Schuhmann, Richard Vencu, Romain Beaumont, 
Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo 
Coombes, Jenia Jitsev, and Aran Komatsuzaki. LAION-
400M: Open dataset of CLIP-filtered 400 million image-text 
pairs. arXiv preprint arXiv:2111.02114, 2021. 6 
[38] Lavanya Sharan, Ce Liu, Ruth Rosenholtz, and Edward H 
Adelson. Recognizing materials using perceptually in-
spired features. International Journal of Computer Vision, 
103(3):348-371, 2013. 3 
[39] Shelly Sheynin, Sagie Benaim, and Lior Wolf. A hierarchical 
transformation-discriminating generative model for few shot 
anomaly detection. In Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision, pages 8495-8504, 
2021. 1, 2, 3, 7 
[40] Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, 
Benjamin Recht, and Ludwig Schmidt. Measuring robustness 
to natural distribution shifts in image classification. Advances 
in Neural Information Processing Systems, 33:18583-18599, 
2020. 3 
[41] Yoad Tewel, Yoav Shalev, Idan Schwartz, and Lior Wolf. 
ZeroCap: Zero-shot image-to-text generation for visual-
semantic arithmetic. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition, pages 
17918-17928, 2022. 3 
[42] Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and 
Hongxia Yang. OFA: Unifying architectures, tasks, and 
modalities through a simple sequence-to-sequence learning 
framework. In International Conference on Machine Learn-
ing, pages 23318-23340. PMLR, 2022. 3 
[44] Conference on Computer Vision and Pattern Recognition, 
pages 3485-3492. IEEE, 2010. 3 
[46] Mengde Xu, Zheng Zhang, Fangyun Wei, Yutong Lin, Yue 
Cao, Han Hu, and Xiang Bai. A simple baseline for open-
vocabulary semantic segmentation with pre-trained vision-
language model. In European Conference on Computer Vi-
sion, pages 736-753. Springer, 2022. 2, 3 
[47] Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Bin Xiao, Ce 
Liu, Lu Yuan, and Jianfeng Gao. Unified contrastive learning 
in image-text-label space. In Proceedings of the IEEE/CVF 
Conference on Computer Vision and Pattern Recognition, 
pages 19163-19173, 2022. 3 
[48] Minghui Yang, Peng Wu, Jing Liu, and Hui Feng. MemSeg: 
A semi-supervised method for image surface defect detec-
tion using differences and commonalities. arXiv preprint 
arXiv:2205.00908, 2022. 3 
[49] Jihun Yi and Sungroh Yoon. Patch SVDD: Patch-level SVDD 
for anomaly detection and segmentation. In Proceedings of 
the Asian Conference on Computer Vision, 2020. 3, 6, 7 
[50] Aron Yu and Kristen Grauman. Fine-grained visual com-
parisons with local learning. In Proceedings of the IEEE 
Conference on Computer Vision and Pattern Recognition, 
pages 192-199, 2014. 3 
[51] Jiawei Yu, Ye Zheng, Xiang Wang, Wei Li, Yushuang Wu, 
Rui Zhao, and Liwei Wu. Fastflow: Unsupervised anomaly 
detection and localization via 2d normalizing flows. arXiv 
preprint arXiv:2111.07677, 2021. 3 
[52] Sergey Zagoruyko and Nikos Komodakis. Wide residual net-
works. In Edwin R. Hancock Richard C. Wilson and William 
A. P. Smith, editors, Proceedings of the British Machine Vi-
sion Conference (BMVC), pages 87.1-87.12. BMVA Press, 
September 2016. 13 
[53] Vitjan Zavrtanik, Matej Kristan, and Danijel Skočaj. DRAEM 
-A discriminatively trained reconstruction embedding for 
surface anomaly detection. In Proceedings of the IEEE/CVF 
International Conference on Computer Vision, pages 8330-
8339, 2021. 1, 3 
[54] Si Zhang, Hanghang Tong, Jiejun Xu, and Ross Maciejew-
ski. Graph convolutional networks: a comprehensive review. 
Computational Social Networks, 6(1):1-23, 2019. 3 
[55] Yuhao Zhang, Hang Jiang, Yasuhide Miura, Christopher D 
Manning, and Curtis P Langlotz. Contrastive learning of 
medical visual representations from paired images and text. 
arXiv preprint arXiv:2010.00747, 2020. 3 
[56] Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li, 
Noel Codella, Liunian Harold Li, Luowei Zhou, Xiyang Dai, 
Lu Yuan, Yin Li, et al. Regionclip: Region-based language-
image pretraining. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition, pages 
16793-16803, 2022. 3 
[57] Chong Zhou, Chen Change Loy, and Bo Dai. Extract free 
dense labels from CLIP. In European Conference on Com-
puter Vision, volume 3, page 8, 2022. 3, 6, 8 
[58] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei 
Liu. Learning to prompt for vision-language models. Interna-
tional Journal of Computer Vision, 130(9):2337-2348, 2022. 
3 
[59] Yang Zou, Jongheon Jeong, Latha Pemula, Dongqing Zhang, 
and Onkar Dabeer. SPot-the-Difference self-supervised pre-
training for anomaly detection and segmentation. In Proceed-
ings of the European Conference on Computer Vision, 2022. 
1, 3, 6, 12 
Supplementary Material </p>
<p>WinCLIP: Zero-/Few-Shot Anomaly Classification and Segmentation </p>
<p>A. Experimental details </p>
<p>https://github.com/amazon-science/patchcore-inspection 5 Technically, PatchCore incorporates several techniques upon a patch-level memory scheme, e.g., local patch aggregation, clustering and score re-weighting. 6 https://github.com/mlfoundations/open_clip
Table 13. Comparison of anomaly segmentation (AS) performance in terms of class-wise pixel-AUROC on MVTec-AD. We report the mean and standard deviation over 5 random seeds for each measurement.MVTec-AD (AS)Table 20. Comparison of anomaly segmentation (AS) performance in terms of class-wise PRO on VisA. We report the mean and standard deviation over 5 random seeds for each measurement.VisA (AS)Table 21. Comparison of anomaly segmentation (AS) performance in terms of class-wise F1-max on VisA. We report the mean and standard deviation over 5 random seeds for each measurement.
Comparison of anomaly segmentation (AS) performance in terms of class-wise PRO on MVTec-AD. We report the mean and standard. Table 14Table 14. Comparison of anomaly segmentation (AS) performance in terms of class-wise PRO on MVTec-AD. We report the mean and standard</p>
<p>Comparison of anomaly segmentation (AS) performance in terms of class-wise F1-max on MVTec-AD. We report the mean and. Table 15. Table 15. Comparison of anomaly segmentation (AS) performance in terms of class-wise F1-max on MVTec-AD. We report the mean and</p>            </div>
        </div>

    </div>
</body>
</html>
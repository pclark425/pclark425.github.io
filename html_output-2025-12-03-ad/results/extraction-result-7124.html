<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7124 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7124</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7124</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-133.html">extraction-schema-133</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the representational format of conceptual knowledge in the brain at a functional level, including theoretical models, their descriptions, claimed representational formats, supporting or contradictory empirical evidence, experimental paradigms, key findings, and citation information.</div>
                <p><strong>Paper ID:</strong> paper-12ec492b6b2ad7c2cc47a5612e8d2a9b7fb35f91</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/12ec492b6b2ad7c2cc47a5612e8d2a9b7fb35f91" target="_blank">Identifying natural images from human brain activity</a></p>
                <p><strong>Paper Venue:</strong> Nature</p>
                <p><strong>Paper TL;DR:</strong> A decoding method based on quantitative receptive-field models that characterize the relationship between visual stimuli and fMRI activity in early visual areas is developed and it is suggested that it may soon be possible to reconstruct a picture of a person’s visual experience from measurements of brain activity alone.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7124.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7124.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the representational format of conceptual knowledge in the brain at a functional level, including theoretical models, their descriptions, claimed representational formats, supporting or contradictory empirical evidence, experimental paradigms, key findings, and citation information.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gabor wavelet pyramid</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gabor wavelet pyramid receptive field model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A feature-based image representation in which each image is decomposed into a set of Gabor wavelets varying in position, size, orientation, spatial frequency and phase; voxel responses are predicted as a linear function of contrast energy from quadrature Gabor pairs (a linearized receptive field model).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>Gabor Wavelet Pyramid Model</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>feature-based vector</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Visual stimuli are represented as high-dimensional vectors of Gabor feature contrast-energies (space × orientation × spatial frequency × scale); individual voxel responses are modeled as weighted linear sums of these features after an energy transform.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_claims</strong></td>
                            <td>Early visual cortex voxels are selectively tuned to spatial location, orientation and spatial frequency; accurate forward models based on these features allow prediction of voxel responses to novel natural images and permit decoding/identification of viewed images.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>fMRI study</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_paradigm</strong></td>
                            <td>Two-stage fMRI paradigm: model estimation using 1,750 natural images (to fit voxel RFs) and image identification using 120 novel natural images (decoding by predicting voxel patterns for candidate images and selecting best match).</td>
                        </tr>
                        <tr>
                            <td><strong>key_result</strong></td>
                            <td>RF models based on the Gabor wavelet pyramid accurately predicted voxel responses in V1–V3 and enabled image identification of 120 novel images with up to 92% correct (repeated-trial) in one subject; performance scaled well to large candidate sets and exceeded performance of spatial-only models.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_theory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence</strong></td>
                            <td>Model fit degrades in higher visual areas (e.g., V4); voxels show only slight orientation bias so orientation contributes less than spatial frequency; fMRI BOLD nonlinearity and limited spatial/temporal resolution constrain the interpretation of underlying neural representation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Kay, Naselaris, Prenger & Gallant, 2008</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Identifying natural images from human brain activity', 'publication_date_yy_mm': '2008-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7124.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7124.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the representational format of conceptual knowledge in the brain at a functional level, including theoretical models, their descriptions, claimed representational formats, supporting or contradictory empirical evidence, experimental paradigms, key findings, and citation information.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Retinotopy-only model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retinotopic spatial-tuning receptive field model (location and size only)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simplified receptive field model that captures only the spatial location and size of each voxel's receptive field and omits orientation and spatial-frequency tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>Retinotopic Spatial Tuning Model</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>feature-based vector (spatial-only)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Voxels are represented as spatial filters that respond according to where contrast falls in the visual field (position and receptive field size) without additional feature selectivity dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_claims</strong></td>
                            <td>Retinotopic spatial tuning encodes substantial information about stimulus location and coarse image content, and is an important organizing principle of early visual cortex.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>fMRI study</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_paradigm</strong></td>
                            <td>Same two-stage fMRI paradigm (model estimation on many images, identification on novel images) comparing performance of retinotopy-only forward model versus full Gabor-based model.</td>
                        </tr>
                        <tr>
                            <td><strong>key_result</strong></td>
                            <td>The retinotopy-only model produced substantially worse identification performance and scaled poorly with candidate set size: it reached 10% correct at ~10^5.1 images versus ~10^9.5 images for the Gabor model, indicating spatial tuning alone is insufficient for high-fidelity identification.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_theory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence</strong></td>
                            <td>Although retinotopy explains much coarse information, it cannot account for the high identification performance observed when orientation and spatial frequency tuning are included; therefore spatial-only representations are insufficient for detailed image decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Kay, Naselaris, Prenger & Gallant, 2008</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Identifying natural images from human brain activity', 'publication_date_yy_mm': '2008-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7124.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7124.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the representational format of conceptual knowledge in the brain at a functional level, including theoretical models, their descriptions, claimed representational formats, supporting or contradictory empirical evidence, experimental paradigms, key findings, and citation information.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Linearized contrast-energy model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Contrast-energy linearized receptive field model (quadrature Gabor pairs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A receptive field model that computes contrast energy from quadrature Gabor wavelet pairs (a nonlinear transform) and then predicts voxel responses as a linear function of these energies, effectively linearizing a nonlinear stimulus–response relationship.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>Contrast-Energy Linearized RF Model</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>feature-based vector</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Stimuli are transformed into contrast-energy features (quadrature pair energies), and voxel activity is represented functionally as linear weights over those energy features, allowing linear regression to estimate tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_claims</strong></td>
                            <td>Applying an energy nonlinearity followed by a linear readout captures essential aspects of voxel selectivity to natural images and enables accurate forward-model predictions needed for decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>fMRI study</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_paradigm</strong></td>
                            <td>RF estimation from responses to 1,750 natural images; prediction/identification on separate 120-image set using linear models fit to contrast-energy features.</td>
                        </tr>
                        <tr>
                            <td><strong>key_result</strong></td>
                            <td>The contrast-energy linearized model provided accurate predictions for V1–V3 voxels and underpinned the high identification performance; it performed poorly in higher visual areas, indicating limitations of this transform outside early visual cortex.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_theory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence</strong></td>
                            <td>The assumption that the energy transform plus linear readout suffices fails for some higher-level areas and may not capture all neural nonlinearities; BOLD signal nonlinearity and indirect measurement further complicate exact mapping to neural energy computations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Kay, Naselaris, Prenger & Gallant, 2008</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Identifying natural images from human brain activity', 'publication_date_yy_mm': '2008-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7124.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7124.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the representational format of conceptual knowledge in the brain at a functional level, including theoretical models, their descriptions, claimed representational formats, supporting or contradictory empirical evidence, experimental paradigms, key findings, and citation information.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Model-based statistical inference</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Receptive field model-based statistical inference (reconstruction) framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A probabilistic inversion approach that uses forward receptive field models to infer the most likely stimulus image given measured brain activity (e.g., MAP or Bayesian reconstruction), cited as a path from identification to full image reconstruction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>Model-based Statistical Inference Reconstruction</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>probabilistic distribution / high-dimensional latent image space</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>The brain's sensory representation is modeled forward (stimulus → predicted activity); reconstruction inverts this model probabilistically to estimate the posterior over images given observed activity, typically requiring priors over natural images and accurate forward models.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_claims</strong></td>
                            <td>If forward RF models are sufficiently accurate, Bayesian inversion can reconstruct images from measured activity; reconstruction quality depends critically on model fidelity and measurement noise.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>references to fMRI and single-unit reconstruction studies, theoretical/computational literature</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_paradigm</strong></td>
                            <td>Discussed in context of prior work (Thirion et al. inverse retinotopy; Stanley et al. LGN reconstructions) and proposed as next step beyond identification; no full reconstruction performed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_result</strong></td>
                            <td>Authors argue that their RF models have sufficient predictive power to enable identification and therefore likely support an inference-based route to reconstruction, but they did not perform full reconstruction in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_theory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence</strong></td>
                            <td>Prior successful reconstructions (e.g., Stanley et al.) relied on linear relationships at single-unit LGN recordings that may not hold for fMRI BOLD (which is indirect and nonlinear), presenting practical challenges to direct application of reconstruction frameworks with fMRI.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Kay, Naselaris, Prenger & Gallant, 2008</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Identifying natural images from human brain activity', 'publication_date_yy_mm': '2008-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7124.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7124.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the representational format of conceptual knowledge in the brain at a functional level, including theoretical models, their descriptions, claimed representational formats, supporting or contradictory empirical evidence, experimental paradigms, key findings, and citation information.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Population vector reconstruction (theoretical)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vector reconstruction from neuronal population activity (population-vector framework)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A theoretical framework proposing that continuous stimulus variables can be reconstructed by weighted combinations (vectors) of neuronal firing rates across a population; cited as foundational formalism for reconstruction/decoding approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Vector reconstruction from firing rates</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>Population Vector Reconstruction</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>vector-based population code</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Information about stimulus variables is encoded in the pattern of firing rates across a neuronal population; downstream decoders can reconstruct represented variables by linearly combining population responses (population vectors) or via other decoding rules.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_claims</strong></td>
                            <td>Neuronal populations provide sufficient information to reconstruct continuous stimulus variables; provides a principled decoder for translating ensemble activity into stimulus estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>theoretical/computational papers and referenced experimental reconstructions (cited literature)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_paradigm</strong></td>
                            <td>Computational/theoretical analyses and application in single-unit ensemble reconstruction experiments (e.g., LGN movie reconstructions); mentioned as conceptual basis for reconstruction efforts.</td>
                        </tr>
                        <tr>
                            <td><strong>key_result</strong></td>
                            <td>Paper cites population-vector and reconstruction theory (Salinas & Abbott; Zhang et al.) as conceptual underpinnings for reconstructing stimuli from population responses and as part of the rationale for model-based reconstruction approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_theory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence</strong></td>
                            <td>Application to fMRI is limited by the indirect nature of the BOLD signal and its nonlinearities; single-unit linear relationships used in some successful reconstructions (LGN) may not translate directly.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Kay, Naselaris, Prenger & Gallant, 2008</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Identifying natural images from human brain activity', 'publication_date_yy_mm': '2008-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7124.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7124.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the representational format of conceptual knowledge in the brain at a functional level, including theoretical models, their descriptions, claimed representational formats, supporting or contradictory empirical evidence, experimental paradigms, key findings, and citation information.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Distributed voxel patterns</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Distributed patterns of voxel activity in visual cortex</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The functional representational format in which visual information is encoded in spatially distributed activity across voxels (multivoxel patterns), including fine-grained voxel-to-voxel variations that carry information about orientation and spatial frequency.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>Distributed Population Encoding in fMRI Voxels</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>high-dimensional distributed pattern</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Conceptual/visual content is represented not in single localized units but across patterns of activity across many voxels; fine-grained differences among voxels contribute to representational richness and decodability.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_claims</strong></td>
                            <td>Multivoxel patterns contain rich stimulus-related information enabling decoding of complex natural images, single-trial decoding above chance, and stable decoding across months; fine-grained voxel variations (including slight orientation/spatial-frequency biases) contribute to decoding performance.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>fMRI study</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_paradigm</strong></td>
                            <td>Multivoxel pattern analysis used for image identification (predicting voxel patterns for candidate images and correlating with measured patterns); comparisons across models demonstrate contribution of feature dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>key_result</strong></td>
                            <td>High identification accuracy achieved using distributed voxel patterns (e.g., 92% for one subject on 120-image set repeated-trial); single-trial identification above chance (51% and 32% for two subjects), and successful identification months later demonstrates stability.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_theory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence</strong></td>
                            <td>fMRI voxel measurements are indirect and spatially coarse compared to neuronal populations; some representational detail (e.g., orientation) only weakly biased at voxel scale, meaning observed multivoxel information may reflect pooled biases rather than columnar-level encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Kay, Naselaris, Prenger & Gallant, 2008</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Identifying natural images from human brain activity', 'publication_date_yy_mm': '2008-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Inverse retinotopy: Inferring the visual content of images from brain activation patterns <em>(Rating: 2)</em></li>
                <li>Predicting the orientation of invisible stimuli from activity in human primary visual cortex <em>(Rating: 2)</em></li>
                <li>Decoding the visual and subjective contents of the human brain <em>(Rating: 2)</em></li>
                <li>Reconstruction of natural scenes from ensemble responses in the lateral geniculate nucleus <em>(Rating: 2)</em></li>
                <li>Vector reconstruction from firing rates <em>(Rating: 2)</em></li>
                <li>Interpreting neuronal population activity by reconstruction: Unified framework with application to hippocampal place cells <em>(Rating: 2)</em></li>
                <li>Functional magnetic resonance imaging (fMRI) "brain reading": Detecting and classifying distributed patterns of fMRI activity in human visual cortex <em>(Rating: 1)</em></li>
                <li>Predicting the stream of consciousness from activity in human visual cortex <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7124",
    "paper_id": "paper-12ec492b6b2ad7c2cc47a5612e8d2a9b7fb35f91",
    "extraction_schema_id": "extraction-schema-133",
    "extracted_data": [
        {
            "name_short": "Gabor wavelet pyramid",
            "name_full": "Gabor wavelet pyramid receptive field model",
            "brief_description": "A feature-based image representation in which each image is decomposed into a set of Gabor wavelets varying in position, size, orientation, spatial frequency and phase; voxel responses are predicted as a linear function of contrast energy from quadrature Gabor pairs (a linearized receptive field model).",
            "citation_title": "",
            "mention_or_use": "use",
            "theory_name": "Gabor Wavelet Pyramid Model",
            "theory_type": "feature-based vector",
            "theory_description": "Visual stimuli are represented as high-dimensional vectors of Gabor feature contrast-energies (space × orientation × spatial frequency × scale); individual voxel responses are modeled as weighted linear sums of these features after an energy transform.",
            "functional_claims": "Early visual cortex voxels are selectively tuned to spatial location, orientation and spatial frequency; accurate forward models based on these features allow prediction of voxel responses to novel natural images and permit decoding/identification of viewed images.",
            "evidence_source": "fMRI study",
            "experimental_paradigm": "Two-stage fMRI paradigm: model estimation using 1,750 natural images (to fit voxel RFs) and image identification using 120 novel natural images (decoding by predicting voxel patterns for candidate images and selecting best match).",
            "key_result": "RF models based on the Gabor wavelet pyramid accurately predicted voxel responses in V1–V3 and enabled image identification of 120 novel images with up to 92% correct (repeated-trial) in one subject; performance scaled well to large candidate sets and exceeded performance of spatial-only models.",
            "supports_theory": true,
            "counter_evidence": "Model fit degrades in higher visual areas (e.g., V4); voxels show only slight orientation bias so orientation contributes less than spatial frequency; fMRI BOLD nonlinearity and limited spatial/temporal resolution constrain the interpretation of underlying neural representation.",
            "citation": "Kay, Naselaris, Prenger & Gallant, 2008",
            "uuid": "e7124.0",
            "source_info": {
                "paper_title": "Identifying natural images from human brain activity",
                "publication_date_yy_mm": "2008-03"
            }
        },
        {
            "name_short": "Retinotopy-only model",
            "name_full": "Retinotopic spatial-tuning receptive field model (location and size only)",
            "brief_description": "A simplified receptive field model that captures only the spatial location and size of each voxel's receptive field and omits orientation and spatial-frequency tuning.",
            "citation_title": "",
            "mention_or_use": "use",
            "theory_name": "Retinotopic Spatial Tuning Model",
            "theory_type": "feature-based vector (spatial-only)",
            "theory_description": "Voxels are represented as spatial filters that respond according to where contrast falls in the visual field (position and receptive field size) without additional feature selectivity dimensions.",
            "functional_claims": "Retinotopic spatial tuning encodes substantial information about stimulus location and coarse image content, and is an important organizing principle of early visual cortex.",
            "evidence_source": "fMRI study",
            "experimental_paradigm": "Same two-stage fMRI paradigm (model estimation on many images, identification on novel images) comparing performance of retinotopy-only forward model versus full Gabor-based model.",
            "key_result": "The retinotopy-only model produced substantially worse identification performance and scaled poorly with candidate set size: it reached 10% correct at ~10^5.1 images versus ~10^9.5 images for the Gabor model, indicating spatial tuning alone is insufficient for high-fidelity identification.",
            "supports_theory": true,
            "counter_evidence": "Although retinotopy explains much coarse information, it cannot account for the high identification performance observed when orientation and spatial frequency tuning are included; therefore spatial-only representations are insufficient for detailed image decoding.",
            "citation": "Kay, Naselaris, Prenger & Gallant, 2008",
            "uuid": "e7124.1",
            "source_info": {
                "paper_title": "Identifying natural images from human brain activity",
                "publication_date_yy_mm": "2008-03"
            }
        },
        {
            "name_short": "Linearized contrast-energy model",
            "name_full": "Contrast-energy linearized receptive field model (quadrature Gabor pairs)",
            "brief_description": "A receptive field model that computes contrast energy from quadrature Gabor wavelet pairs (a nonlinear transform) and then predicts voxel responses as a linear function of these energies, effectively linearizing a nonlinear stimulus–response relationship.",
            "citation_title": "",
            "mention_or_use": "use",
            "theory_name": "Contrast-Energy Linearized RF Model",
            "theory_type": "feature-based vector",
            "theory_description": "Stimuli are transformed into contrast-energy features (quadrature pair energies), and voxel activity is represented functionally as linear weights over those energy features, allowing linear regression to estimate tuning.",
            "functional_claims": "Applying an energy nonlinearity followed by a linear readout captures essential aspects of voxel selectivity to natural images and enables accurate forward-model predictions needed for decoding.",
            "evidence_source": "fMRI study",
            "experimental_paradigm": "RF estimation from responses to 1,750 natural images; prediction/identification on separate 120-image set using linear models fit to contrast-energy features.",
            "key_result": "The contrast-energy linearized model provided accurate predictions for V1–V3 voxels and underpinned the high identification performance; it performed poorly in higher visual areas, indicating limitations of this transform outside early visual cortex.",
            "supports_theory": true,
            "counter_evidence": "The assumption that the energy transform plus linear readout suffices fails for some higher-level areas and may not capture all neural nonlinearities; BOLD signal nonlinearity and indirect measurement further complicate exact mapping to neural energy computations.",
            "citation": "Kay, Naselaris, Prenger & Gallant, 2008",
            "uuid": "e7124.2",
            "source_info": {
                "paper_title": "Identifying natural images from human brain activity",
                "publication_date_yy_mm": "2008-03"
            }
        },
        {
            "name_short": "Model-based statistical inference",
            "name_full": "Receptive field model-based statistical inference (reconstruction) framework",
            "brief_description": "A probabilistic inversion approach that uses forward receptive field models to infer the most likely stimulus image given measured brain activity (e.g., MAP or Bayesian reconstruction), cited as a path from identification to full image reconstruction.",
            "citation_title": "",
            "mention_or_use": "mention",
            "theory_name": "Model-based Statistical Inference Reconstruction",
            "theory_type": "probabilistic distribution / high-dimensional latent image space",
            "theory_description": "The brain's sensory representation is modeled forward (stimulus → predicted activity); reconstruction inverts this model probabilistically to estimate the posterior over images given observed activity, typically requiring priors over natural images and accurate forward models.",
            "functional_claims": "If forward RF models are sufficiently accurate, Bayesian inversion can reconstruct images from measured activity; reconstruction quality depends critically on model fidelity and measurement noise.",
            "evidence_source": "references to fMRI and single-unit reconstruction studies, theoretical/computational literature",
            "experimental_paradigm": "Discussed in context of prior work (Thirion et al. inverse retinotopy; Stanley et al. LGN reconstructions) and proposed as next step beyond identification; no full reconstruction performed in this paper.",
            "key_result": "Authors argue that their RF models have sufficient predictive power to enable identification and therefore likely support an inference-based route to reconstruction, but they did not perform full reconstruction in this study.",
            "supports_theory": null,
            "counter_evidence": "Prior successful reconstructions (e.g., Stanley et al.) relied on linear relationships at single-unit LGN recordings that may not hold for fMRI BOLD (which is indirect and nonlinear), presenting practical challenges to direct application of reconstruction frameworks with fMRI.",
            "citation": "Kay, Naselaris, Prenger & Gallant, 2008",
            "uuid": "e7124.3",
            "source_info": {
                "paper_title": "Identifying natural images from human brain activity",
                "publication_date_yy_mm": "2008-03"
            }
        },
        {
            "name_short": "Population vector reconstruction (theoretical)",
            "name_full": "Vector reconstruction from neuronal population activity (population-vector framework)",
            "brief_description": "A theoretical framework proposing that continuous stimulus variables can be reconstructed by weighted combinations (vectors) of neuronal firing rates across a population; cited as foundational formalism for reconstruction/decoding approaches.",
            "citation_title": "Vector reconstruction from firing rates",
            "mention_or_use": "mention",
            "theory_name": "Population Vector Reconstruction",
            "theory_type": "vector-based population code",
            "theory_description": "Information about stimulus variables is encoded in the pattern of firing rates across a neuronal population; downstream decoders can reconstruct represented variables by linearly combining population responses (population vectors) or via other decoding rules.",
            "functional_claims": "Neuronal populations provide sufficient information to reconstruct continuous stimulus variables; provides a principled decoder for translating ensemble activity into stimulus estimates.",
            "evidence_source": "theoretical/computational papers and referenced experimental reconstructions (cited literature)",
            "experimental_paradigm": "Computational/theoretical analyses and application in single-unit ensemble reconstruction experiments (e.g., LGN movie reconstructions); mentioned as conceptual basis for reconstruction efforts.",
            "key_result": "Paper cites population-vector and reconstruction theory (Salinas & Abbott; Zhang et al.) as conceptual underpinnings for reconstructing stimuli from population responses and as part of the rationale for model-based reconstruction approaches.",
            "supports_theory": null,
            "counter_evidence": "Application to fMRI is limited by the indirect nature of the BOLD signal and its nonlinearities; single-unit linear relationships used in some successful reconstructions (LGN) may not translate directly.",
            "citation": "Kay, Naselaris, Prenger & Gallant, 2008",
            "uuid": "e7124.4",
            "source_info": {
                "paper_title": "Identifying natural images from human brain activity",
                "publication_date_yy_mm": "2008-03"
            }
        },
        {
            "name_short": "Distributed voxel patterns",
            "name_full": "Distributed patterns of voxel activity in visual cortex",
            "brief_description": "The functional representational format in which visual information is encoded in spatially distributed activity across voxels (multivoxel patterns), including fine-grained voxel-to-voxel variations that carry information about orientation and spatial frequency.",
            "citation_title": "",
            "mention_or_use": "use",
            "theory_name": "Distributed Population Encoding in fMRI Voxels",
            "theory_type": "high-dimensional distributed pattern",
            "theory_description": "Conceptual/visual content is represented not in single localized units but across patterns of activity across many voxels; fine-grained differences among voxels contribute to representational richness and decodability.",
            "functional_claims": "Multivoxel patterns contain rich stimulus-related information enabling decoding of complex natural images, single-trial decoding above chance, and stable decoding across months; fine-grained voxel variations (including slight orientation/spatial-frequency biases) contribute to decoding performance.",
            "evidence_source": "fMRI study",
            "experimental_paradigm": "Multivoxel pattern analysis used for image identification (predicting voxel patterns for candidate images and correlating with measured patterns); comparisons across models demonstrate contribution of feature dimensions.",
            "key_result": "High identification accuracy achieved using distributed voxel patterns (e.g., 92% for one subject on 120-image set repeated-trial); single-trial identification above chance (51% and 32% for two subjects), and successful identification months later demonstrates stability.",
            "supports_theory": true,
            "counter_evidence": "fMRI voxel measurements are indirect and spatially coarse compared to neuronal populations; some representational detail (e.g., orientation) only weakly biased at voxel scale, meaning observed multivoxel information may reflect pooled biases rather than columnar-level encoding.",
            "citation": "Kay, Naselaris, Prenger & Gallant, 2008",
            "uuid": "e7124.5",
            "source_info": {
                "paper_title": "Identifying natural images from human brain activity",
                "publication_date_yy_mm": "2008-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Inverse retinotopy: Inferring the visual content of images from brain activation patterns",
            "rating": 2
        },
        {
            "paper_title": "Predicting the orientation of invisible stimuli from activity in human primary visual cortex",
            "rating": 2
        },
        {
            "paper_title": "Decoding the visual and subjective contents of the human brain",
            "rating": 2
        },
        {
            "paper_title": "Reconstruction of natural scenes from ensemble responses in the lateral geniculate nucleus",
            "rating": 2
        },
        {
            "paper_title": "Vector reconstruction from firing rates",
            "rating": 2
        },
        {
            "paper_title": "Interpreting neuronal population activity by reconstruction: Unified framework with application to hippocampal place cells",
            "rating": 2
        },
        {
            "paper_title": "Functional magnetic resonance imaging (fMRI) \"brain reading\": Detecting and classifying distributed patterns of fMRI activity in human visual cortex",
            "rating": 1
        },
        {
            "paper_title": "Predicting the stream of consciousness from activity in human visual cortex",
            "rating": 1
        }
    ],
    "cost": 0.01460975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>NIH Public Access</h1>
<h2>Author Manuscript</h2>
<p>Published in final edited form as:
Nature. 2008 March 20; 452(7185): 352-355. doi:10.1038/nature06713.</p>
<h2>Identifying natural images from human brain activity</h2>
<p>Kendrick N. Kay ${ }^{\mathrm{a}}$, Thomas Naselaris ${ }^{\mathrm{b}}$, Ryan J. Prenger ${ }^{\mathrm{c}}$, and Jack L. Gallant ${ }^{\mathrm{a}, \mathrm{b}}$<br>${ }^{\text {a }}$ Department of Psychology, University of California, Berkeley, CA 94720, USA<br>${ }^{\text {b }}$ Helen Wills Neuroscience Institute, University of California, Berkeley, CA 94720, USA<br>${ }^{\text {c }}$ Department of Physics, University of California, Berkeley, CA 94720, USA</p>
<h4>Abstract</h4>
<p>A challenging goal in neuroscience is to be able to read out, or decode, mental content from brain activity. Recent functional magnetic resonance imaging (fMRI) studies have decoded orientation ${ }^{1,2}$, position ${ }^{3}$, and object category ${ }^{4,5}$ from activity in visual cortex. However, these studies typically used relatively simple stimuli (e.g. gratings) or images drawn from fixed categories (e.g. faces, houses), and decoding was based on prior measurements of brain activity evoked by those same stimuli or categories. To overcome these limitations, we develop a decoding method based on quantitative receptive field models that characterize the relationship between visual stimuli and fMRI activity in early visual areas. These models describe the tuning of individual voxels for space, orientation, and spatial frequency, and are estimated directly from responses evoked by natural images. We show that these receptive field models make it possible to identify, from a large set of completely novel natural images, which specific image was seen by an observer. Identification is not a mere consequence of the retinotopic organization of visual areas; simpler receptive field models that describe only spatial tuning yield much poorer identification performance. Our results suggest that it may soon be possible to reconstruct a picture of a person's visual experience from brain activity measurements alone.</p>
<h2>Keywords</h2>
<p>decoding; brain reading; receptive field; Gabor wavelets; visual cortex; fMRI</p>
<p>Imagine a general brain-reading device that could reconstruct a picture of a person's visual experience at any moment in time ${ }^{6}$. This general visual decoder would have great scientific and practical utility. For example, we could use the decoder to investigate differences in perception across people, to study covert mental processes such as attention, and perhaps even to access the visual content of purely mental phenomena such as dreams and imagery. The decoder would also serve as a useful benchmark of our understanding of how the brain represents sensory information.</p>
<p>How do we build a general visual decoder? We consider as a first step the problem of image identification ${ }^{3,7,8}$. This problem is analogous to the classic "pick a card, any card" magic</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>trick: We begin with a large, arbitrary set of images. The observer picks an image from the set and views it while brain activity is measured. Is it possible to use the measured brain activity to identify which specific image was seen?</p>
<p>To ensure that a solution to the image identification problem will be applicable to general visual decoding, we introduce two challenging requirements ${ }^{6}$. First, it must be possible to identify novel images. Conventional classification-based decoding methods can be used to identify images if brain activity evoked by those images has been measured previously, but they cannot be used to identify novel images (see Supplementary Discussion). Second, it must be possible to identify natural images. Natural images have complex statistical structure ${ }^{9}$ and are much more difficult to parameterize than simple artificial stimuli such as gratings or pre-segmented objects. Because neural processing of visual stimuli is nonlinear, a decoder that can identify simple stimuli may fail when confronted with complex natural images.</p>
<p>Our experiment consisted of two stages (Fig. 1). In the first stage, model estimation, fMRI data were recorded from visual areas V1, V2, and V3 while each subject viewed 1,750 natural images. These data were used to estimate a quantitative receptive field model ${ }^{10}$ for each voxel (Fig. 2). The model was based on a Gabor wavelet pyramid ${ }^{1113}$ and described tuning along the dimensions of space ${ }^{3,1419}$, orientation ${ }^{1,2,20}$, and spatial frequency ${ }^{21,22}$. (See Supplementary Discussion for a comparison of our receptive field analysis to those of previous studies.)</p>
<p>In the second stage, image identification, fMRI data were recorded while each subject viewed 120 novel natural images. This yielded 120 distinct voxel activity patterns for each subject. For each voxel activity pattern we attempted to identify which image had been seen. To do this, the receptive field models estimated in the first stage of the experiment were used to predict the voxel activity pattern that would be evoked by each of the 120 images. The image whose predicted voxel activity pattern was most correlated (Pearson's $r$ ) with the measured voxel activity pattern was selected.</p>
<p>Identification performance for one subject is illustrated in Fig. 3. For this subject $92 \%$ $(110 / 120)$ of the images were identified correctly (subject S1), while chance performance is just $0.8 \%(1 / 120)$. For a second subject $72 \%(86 / 120)$ of the images were identified correctly (subject S2). These high performance levels demonstrate the validity of our decoding approach, and indicate that our receptive field models accurately characterize the selectivity of individual voxels to natural images.</p>
<p>A general visual decoder would be especially useful if it could operate on brain activity evoked by a single perceptual event. However, because fMRI data are noisy the results reported above were obtained using voxel activity patterns averaged across 13 repeated trials. We therefore attempted identification using voxel activity patterns from single trials. Single-trial performance was $51 \%(834 / 1620)$ and $32 \%(516 / 1620)$ for subjects S1 and S2, respectively (Fig. 4a); once again, chance performance is just $0.8 \%$ (13.5/1620). These results suggest that it may be feasible to decode the content of perceptual experiences in real-time ${ }^{7,23}$.</p>
<p>We have so far demonstrated identification of a single image drawn from a set of 120 images, but a general visual decoder should be able to handle much larger sets of images. To investigate this issue we measured identification performance for various set sizes up to 1,000 images (Fig. 4b). As set size increased 10 -fold from 100 to 1,000 , performance only declined slightly, from $92 \%$ to $82 \%$ (subject S1, repeated-trial). Extrapolation of these measurements (see Supplementary Methods) suggests that performance for this subject would remain above $10 \%$ even up to a set size of $\sim 10^{11.3}$ images. This is more than 100</p>
<p>times larger than the number of images currently indexed by Google ( $\sim 10^{8.9}$ images; source: http://www.google.com/whatsnew/, June 4, 2007).</p>
<p>Early visual areas are organized retinotopically, and voxels are known to reflect this organization ${ }^{14,16,18}$. Could our results be a mere consequence of retinotopy? To answer this question we attempted identification using an alternative model that captures the location and size of each voxel's receptive field but discards orientation and spatial frequency information (Fig. 4c). Performance for this retinotopy-only model declined to $10 \%$ correct at a set size of just $\sim 10^{5.1}$ images, whereas performance for the Gabor wavelet pyramid model did not decline to $10 \%$ correct until $\sim 10^{9.5}$ images were included in the set (repeated-trial, performance extrapolated and averaged across subjects). This result indicates that spatial tuning alone does not yield optimal identification performance; identification improves substantially when orientation and spatial frequency tuning are included in the model.</p>
<p>To further investigate the impact of orientation and spatial frequency tuning, we measured identification performance after imposing constraints on the orientation and spatial frequency tuning of the Gabor wavelet pyramid model (Supplementary Fig. 8). The results indicate that both orientation and spatial frequency tuning contribute to identification performance, but that the latter makes the larger contribution. This is consistent with recent studies demonstrating that voxels have only slight orientation bias ${ }^{1,2}$. We also find that voxel-to-voxel variation in orientation and spatial frequency tuning contributes to identification performance. This reinforces the growing realization in the fMRI community that information may be present in fine-grained patterns of voxel activity ${ }^{6}$.</p>
<p>To be practical our identification algorithm must perform well even when brain activity is measured long after estimation of the receptive field models. To assess performance over time ${ }^{2,4,6,23}$ we attempted identification for a set of 120 novel natural images that were seen approximately two months after the initial experiment. In this case $82 \%$ (99/120) of the images were identified correctly (chance performance $0.8 \%$; subject S1, repeated-trial). We also evaluated identification performance for a set of 12 novel natural images that were seen more than a year after the initial experiment. In this case $100 \%$ (12/12) of the images were identified correctly (chance performance $8 \%$; subject S1, repeated-trial). These results demonstrate that the stimulus-related information that can be decoded from voxel activity remains largely stable over time.</p>
<p>Why does identification sometimes fail? Inspection revealed that identification errors tended to occur when the selected image was visually similar to the correct image. This suggests that noise in measured voxel activity patterns causes the identification algorithm to confuse images that have similar features.</p>
<p>Functional MRI signals have modest spatial resolution and reflect hemodynamic activity that is only indirectly coupled to neural activity ${ }^{24,25}$. Despite these limitations we have shown that fMRI signals can be used to achieve remarkable levels of identification performance. This indicates that fMRI signals contain a considerable amount of stimulusrelated information ${ }^{4}$ and that this information can be successfully decoded in practice.</p>
<p>Identification of novel natural images brings us close to achieving a general visual decoder. The final step will require devising a way to reconstruct the image seen by the observer, instead of selecting the image from a known set. Stanley and co-workers ${ }^{26}$ reconstructed natural movies by modeling the luminance of individual image pixels as a linear function of single-unit activity in cat LGN. This approach assumes a linear relationship between luminance and the activity of the recorded units, but this condition does not hold in $\mathrm{fMRI}^{27,28}$.</p>
<p>An alternative approach to reconstruction is to incorporate receptive field models into a statistical inference framework. In such a framework, receptive field models are used to infer the most likely image given a measured activity pattern. This model-based approach has a long history in both theoretical and experimental neuroscience^{29,30}. Recently, Thirion and co-workers^{3} used it to reconstruct spatial maps of contrast from fMRI activity in human visual cortex. The success of the approach depends critically on how well the receptive field models predict brain activity. The present study demonstrates that our receptive field models have sufficient predictive power to enable identification of novel natural images, even for the case of extremely large sets of images. We are therefore optimistic that the model-based approach will make possible the reconstruction of natural images from human brain activity.</p>
<h2>Methods</h2>
<h3>Stimuli</h3>
<p>The stimuli consisted of sequences of natural photos. Photos were obtained from a commercial digital library (Corel Stock Photo Libraries from Corel Corporation, Ontario, Canada), the Berkeley Segmentation Dataset (http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/segbench/), and the authors' personal collections. The content of the photos included animals, buildings, food, humans, indoor scenes, manmade objects, outdoor scenes, and textures. Photos were converted to grayscale, downsampled so that the smaller of the two image dimensions was 500 px, linearly transformed so that the 1/10th and 99 9/10th percentiles of the original pixel values were mapped to the minimum (0) and maximum (255) pixel values, cropped to the central 500 px × 500 px, masked with a circle, and placed on a gray background (Supplementary Fig. 1a). The luminance of the background was set to the mean luminance across photos, and the outer edge of each photo (10% of the radius of the circular mask) was linearly blended into the background.</p>
<p>The size of the photos was 20° × 20° (500 px × 500 px). A central white square served as the fixation point, and its size was 0.2° × 0.2° (4 px × 4 px). Photos were presented in successive 4-s trials; in each trial, a photo was presented for 1 s and the gray background was presented for 3 s. Each 1-s presentation consisted of a photo being flashed ON-OFF-ON-OFF-ON where ON corresponds to presentation of the photo for 200 ms and OFF corresponds to presentation of the gray background for 200 ms (Supplementary Fig. 1b). The flashing technique increased the signal-to-noise ratio of voxel responses relative to that achieved by presenting each photo continuously for 1 s (data not shown).</p>
<p>Visual stimuli were delivered using the VisuaStim goggles system (Resonance Technology, Northridge, CA). The display resolution was 800 × 600 at 60 Hz. A PowerBook G4 computer (Apple Computer, Cupertino, CA) controlled stimulus presentation using software written in MATLAB 5.2.1 (The Mathworks, Natick, MA) and Psychophysics Toolbox 2.53 (http://psychtoolbox.org).</p>
<h3>MRI parameters</h3>
<p>The experimental protocol was approved by the UC-Berkeley Committee for the Protection of Human Subjects. MRI data were collected at the Brain Imaging Center at UC-Berkeley using a 4 T INOVA MR scanner (Varian, Inc. Palo Alto, CA) and a quadrature transmit/receive surface coil (Midwest RF, LLC, Hartland, WI). Data were acquired using coronal slices that covered occipital cortex: 18 slices, slice thickness 2.25 mm, slice gap 0.25 mm, field-of-view 128 × 128 mm^{2}. (In one scan session, a slice gap of 0.5 mm was used.) For functional data, a T2*-weighted, single-shot, slice-interleaved, gradient-echo EPI pulse sequence was used: matrix size 64 × 64, TR 1 s, TE 28 ms, flip angle 20°. The nominal spatial resolution of the functional data was 2 × 2 × 2.5 mm^{3}. For anatomical data, a T1-</p>
<p>weighted gradient-echo multislice sequence was used: matrix size $256 \times 256$, TR 0.2 s , TE 5 ms , flip angle $40^{\circ}$.</p>
<h1>Data collection</h1>
<p>Data for the model estimation and image identification stages of the experiment were collected in the same scan sessions. Two subjects were used: S1 (author T.N., age 33) and S2 (author K.N.K., age 25). Subjects were healthy and had normal or corrected-to-normal vision.</p>
<p>Five scan sessions of data were collected from each subject. Each scan session consisted of five model estimation runs and two image identification runs. Model estimation runs (11 min each) were used for the model estimation stage of the experiment. Each model estimation run consisted of 70 distinct images presented 2 times each. Image identification runs ( 12 min each) were used for the image identification stage of the experiment. Each image identification run consisted of 12 distinct images presented 13 times each. Images were randomly selected for each run and were mutually exclusive across runs. The total number of distinct images used in the model estimation and image identification runs was 1,750 and 120, respectively. (For additional details on experimental design, see Supplementary Methods.)</p>
<p>Three additional scan sessions of data were collected from subject S1. Two of these were held $\sim 2$ months after the main experiment, and consisted of five image identification runs each. The third was held $\sim 14$ months after the main experiment, and consisted of one image identification run. The images used in these additional scan sessions were randomly selected and were distinct from the images used in the main experiment.</p>
<h2>Data preprocessing</h2>
<p>Functional brain volumes were reconstructed and then coregistered to correct differences in head positioning within and across scan sessions. Next, voxel-specific response timecourses were estimated and deconvolved from the time-series data. This produced, for each voxel, an estimate of the amplitude of the response (a single value) to each image used in the model estimation and image identification runs. Finally, voxels were assigned to visual areas based on retinotopic mapping data ${ }^{17}$ collected in separate scan sessions. (Details on these procedures are given in Supplementary Methods.)</p>
<h2>Model estimation</h2>
<p>A receptive field model was estimated for each voxel based on its responses to the images used in the model estimation runs. The model was based on a Gabor wavelet pyramid ${ }^{1113}$. In the model each image is represented by a set of Gabor wavelets differing in size, position, orientation, spatial frequency, and phase (Supplementary Fig. 2). The predicted response is a linear function of the contrast energy contained in quadrature wavelet pairs (Supplementary Fig. 3). Because contrast energy is a nonlinear quantity, this is a linearized model ${ }^{10}$. The model was able to characterize responses of voxels in visual areas V1, V2, and V3 (Supplementary Table 1) but it did a poor job of characterizing responses in higher visual areas such as V4.</p>
<p>Alternative receptive field models were also used, including the retinotopy-only model and several constrained versions of the Gabor wavelet pyramid model. Details on these models and model estimation procedures are given in Supplementary Methods.</p>
<h1>Image identification</h1>
<p>Voxel activity patterns were constructed from voxel responses evoked by the images used in the image identification runs. For each voxel activity pattern, the estimated receptive field models were used to identify which specific image had been seen. The identification algorithm is described in the main text. See Supplementary Fig. 4 and Supplementary Methods for details concerning voxel selection, performance for different set sizes, and noise ceiling estimation. See Supplementary Discussion for a comparison of identification to the decoding problems of classification and reconstruction.</p>
<h2>Supplementary Material</h2>
<p>Refer to Web version on PubMed Central for supplementary material.</p>
<h2>Acknowledgments</h2>
<p>This work was supported by an NDSEG fellowship (K.N.K.), NIH, and UC-Berkeley intramural funds. We thank B. Inglis for MRI assistance, K. Hansen for retinotopic mapping assistance, D. Woods and X. Kang for acquisition of whole-brain anatomical data, and A. Rokem for assistance with scanner operation. We also thank C. Baker, M. D'Esposito, R. Ivry, A. Landau, M. Merolle, F. Theunissen, and the anonymous referees for comments on the manuscript. Finally, we thank S. Nishimoto, R. Redfern, K. Schreiber, B. Willmore, and B. Yu for their help in various aspects of this research.</p>
<h2>References</h2>
<ol>
<li>Haynes JD, Rees G. Predicting the orientation of invisible stimuli from activity in human primary visual cortex. Nature Neurosci. 2005; 8:686-691. [PubMed: 15852013]</li>
<li>Kamitani Y, Tong F. Decoding the visual and subjective contents of the human brain. Nature Neurosci. 2005; 8:679-685. [PubMed: 15852014]</li>
<li>Thirion B, et al. Inverse retinotopy: Inferring the visual content of images from brain activation patterns. Neuroimage. 2006; 33:1104-1116. [PubMed: 17029988]</li>
<li>Cox DD, Savoy RL. Functional magnetic resonance imaging (fMRI) "brain reading": Detecting and classifying distributed patterns of fMRI activity in human visual cortex. Neuroimage. 2003; 19:261270. [PubMed: 12814577]</li>
<li>Haxby JV, et al. Distributed and overlapping representations of faces and objects in ventral temporal cortex. Science. 2001; 293:2425-2430. [PubMed: 11577229]</li>
<li>Haynes JD, Rees G. Decoding mental states from brain activity in humans. Nature Rev Neurosci. 2006; 7:523-534. [PubMed: 16791142]</li>
<li>Hung CP, Kreiman G, Poggio T, DiCarlo JJ. Fast readout of object identity from macaque inferior temporal cortex. Science. 2005; 310:863-866. [PubMed: 16272124]</li>
<li>Tsao DY, Freiwald WA, Tootell RB, Livingstone MS. A cortical region consisting entirely of faceselective cells. Science. 2006; 311:670-674. [PubMed: 16456083]</li>
<li>Simoncelli EP, Olshausen BA. Natural image statistics and neural representation. Annu Rev Neurosci. 2001; 24:1193-1216. [PubMed: 11520932]</li>
<li>Wu MC, David SV, Gallant JL. Complete functional characterization of sensory neurons by system identification. Annu Rev Neurosci. 2006; 29:477-505. [PubMed: 16776594]</li>
<li>Daugman JG. Uncertainty relation for resolution in space, spatial frequency, and orientation optimized by two-dimensional visual cortical filters. J Opt Soc Am A. 1985; 2:1160-1169. [PubMed: 4020513]</li>
<li>Jones JP, Palmer LA. An evaluation of the two-dimensional Gabor filter model of simple receptive fields in cat striate cortex. J Neurophysiol. 1987; 58:1233-1258. [PubMed: 3437332]</li>
<li>Lee TS. Image representation using 2D Gabor wavelets. IEEE Trans Pattern Anal. 1996; 18:959971 .</li>
<li>
<p>DeYoe EA, et al. Mapping striate and extrastriate visual areas in human cerebral cortex. Proc Natl Acad Sci USA. 1996; 93:2382-2386. [PubMed: 8637882]</p>
</li>
<li>
<p>Dumoulin SO, Wandell BA. Population receptive field estimates in human visual cortex. Neuroimage. 2008; 39:647-660. [PubMed: 17977024]</p>
</li>
<li>Engel SA, et al. fMRI of human visual cortex. Nature. 1994; 369:525. [PubMed: 8031403]</li>
<li>Hansen KA, David SV, Gallant JL. Parametric reverse correlation reveals spatial linearity of retinotopic human V1 BOLD response. Neuroimage. 2004; 23:233-241. [PubMed: 15325370]</li>
<li>Sereno MI, et al. Borders of multiple visual areas in humans revealed by functional magnetic resonance imaging. Science. 1995; 268:889-893. [PubMed: 7754376]</li>
<li>Smith AT, Singh KD, Williams AL, Greenlee MW. Estimating receptive field size from fMRI data in human striate and extrastriate visual cortex. Cereb Cortex. 2001; 11:1182-1190. [PubMed: 11709489]</li>
<li>Sasaki Y, et al. The radial bias: A different slant on visual orientation sensitivity in human and nonhuman primates. Neuron. 2006; 51:661-670. [PubMed: 16950163]</li>
<li>Olman CA, Ugurbil K, Schrater P, Kersten D. BOLD fMRI and psychophysical measurements of contrast response to broadband images. Vision Res. 2004; 44:669-683. [PubMed: 14751552]</li>
<li>Singh KD, Smith AT, Greenlee MW. Spatiotemporal frequency and direction sensitivities of human visual areas measured using fMRI. Neuroimage. 2000; 12:550-564. [PubMed: 11034862]</li>
<li>Haynes JD, Rees G. Predicting the stream of consciousness from activity in human visual cortex. Curr Biol. 2005; 15:1301-1307. [PubMed: 16051174]</li>
<li>Heeger DJ, Ress D. What does fMRI tell us about neuronal activity? Nature Rev Neurosci. 2002; 3:142-151. [PubMed: 11836522]</li>
<li>Logothetis NK, Wandell BA. Interpreting the BOLD signal. Annu Rev Physiol. 2004; 66:735-769. [PubMed: 14977420]</li>
<li>Stanley GB, Li FF, Dan Y. Reconstruction of natural scenes from ensemble responses in the lateral geniculate nucleus. J Neurosci. 1999; 19:8036-8042. [PubMed: 10479703]</li>
<li>Haynes JD, Lotto RB, Rees G. Responses of human visual cortex to uniform surfaces. Proc Natl Acad Sci USA. 2004; 101:4286-4291. [PubMed: 15010538]</li>
<li>Rainer G, Augath M, Trinath T, Logothetis NK. Nonmonotonic noise tuning of BOLD fMRI signal to natural images in the visual cortex of the anesthetized monkey. Curr Biol. 2001; 11:846-854. [PubMed: 11516645]</li>
<li>Salinas E, Abbott LF. Vector reconstruction from firing rates. J Comput Neurosci. 1994; 1:89-107. [PubMed: 8792227]</li>
<li>Zhang K, Ginzburg I, McNaughton BL, Sejnowski TJ. Interpreting neuronal population activity by reconstruction: Unified framework with application to hippocampal place cells. J Neurophysiol. 1998; 79:1017-1044. [PubMed: 9463459]</li>
</ol>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1. Schematic of experiment
The experiment consisted of two stages. In the first stage, model estimation, fMRI data were recorded while each subject viewed a large collection of natural images. These data were used to estimate a quantitative receptive field model ${ }^{10}$ for each voxel. The model was based on a Gabor wavelet pyramid ${ }^{1113}$ and described tuning along the dimensions of space ${ }^{3,1419}$, orientation ${ }^{1,2,20}$, and spatial frequency ${ }^{21,22}$. In the second stage, image identification, fMRI data were recorded while each subject viewed a collection of novel natural images. For each measurement of brain activity, we attempted to identify which specific image had been seen. This was accomplished by using the estimated receptive field models to predict brain activity for a set of potential images and then selecting the image whose predicted activity most closely matches the measured activity.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2. Receptive field model for a representative voxel
a, Spatial envelope. The intensity of each pixel indicates the sensitivity of the receptive field $(\mathrm{RF})$ to that location. The white circle delineates the bounds of the stimulus $\left(20^{\circ} \times 20^{\circ}\right)$ and the green square delineates the estimated RF location. Horizontal and vertical slices through the spatial envelope are shown below and to the left. These intersect the peak of the spatial envelope, as indicated by yellow tick marks. The thickness of each slice profile indicates $\pm 1$ s.e.m. This RF is located in the left hemifield, just below the horizontal meridian. b, Orientation and spatial frequency tuning curves. The top matrix depicts the joint orientation and spatial frequency tuning of the RF, and the bottom two plots give the marginal orientation and spatial frequency tuning curves. Error bars indicate $\pm 1$ s.e.m. This RF has broadband orientation tuning and high-pass spatial frequency tuning. For additional RF examples and population summaries of RF properties, see Supplementary Figs. 911.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3. Identification performance
In the image identification stage of the experiment, fMRI data were recorded while each subject viewed 120 novel natural images that had not been used to estimate the receptive field models. For each of the 120 measured voxel activity patterns we attempted to identify which image had been seen. This figure illustrates identification performance for one subject (S1). The color at the $m$ th column and $n$th row represents the correlation between the measured voxel activity pattern for the $m$ th image and the predicted voxel activity pattern for the $n$th image. The highest correlation in each column is designated by an enlarged dot of the appropriate color, and indicates the image selected by the identification algorithm. For this subject $92 \%(110 / 120)$ of the images were identified correctly.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4. Factors that impact identification performance
a, Summary of identification performance. The bars indicate empirical performance for a set size of 120 images, the marker above each bar indicates the estimated noise ceiling (i.e. the theoretical maximum performance given the level of noise in the data), and the dashed green line indicates chance performance. The noise ceiling estimates suggest that the difference in performance across subjects is due to intrinsic differences in the level of noise. b, Scaling of identification performance with set size. The $x$-axis indicates set size, the $y$-axis indicates identification performance, and the number to the right of each line gives the estimated set size at which performance declines to $10 \%$ correct. In all cases performance scaled very well with set size. c, Retinotopy-only model versus Gabor wavelet pyramid model. Identification was attempted using an alternative retinotopy-only model that captures only the location and size of each voxel's receptive field. This model performed substantially worse than the Gabor wavelet pyramid model, indicating that spatial tuning alone is insufficient to achieve optimal identification performance. (Results reflect repeated-trial performance averaged across subjects; see Supplementary Fig. 5 for detailed results.)</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>Correspondence should be addressed to Jack L. Gallant, University of California at Berkeley, 3210 Tolman Hall #1650, Berkeley, CA 94720. gallant@berkeley.edu.</p>
<p>Author Contributions K.N.K. designed and conducted the experiment and was first author on the paper. K.N.K. and T.N. analyzed the data. R.J.P. provided mathematical ideas and assistance. J.L.G. provided guidance on all aspects of the project. All authors discussed the results and commented on the manuscript.
Supplementary Information is linked to the online version of the paper at www.nature.com/nature.
The authors declare no competing financial interests.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6790 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6790</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6790</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-131.html">extraction-schema-131</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <p><strong>Paper ID:</strong> paper-277781064</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.09440v2.pdf" target="_blank">Enhancing Mathematical Reasoning in Large Language Models with Self-Consistency-Based Hallucination Detection</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have demonstrated strong mathematical reasoning capabilities but remain susceptible to hallucina-tions—producing plausible yet incorrect state-ments—especially in theorem proving, symbolic manipulation, and numerical computation. While self-consistency (SC) has been explored as a means to improve factuality in LLMs, existing approaches primarily apply SC to final-answer selection, neglecting the logical consistency of intermediate reasoning steps. In this work, we introduce a structured self-consistency framework designed to enhance the reliability of mathematical reasoning. Our method enforces self-consistency across intermediate steps and final outputs, reducing logical inconsistencies and hallucinations. We evaluate our approach across three core mathematical tasks: theorem proving, symbolic transformation, and numerical computation. Experimental results demonstrate that SC significantly improves proof validity, symbolic reasoning accuracy, and numerical stability while maintaining computational efficiency. Further analysis reveals that structured self-consistency not only enhances problem-solving accuracy but also reduces the variance of model-generated outputs. These findings highlight self-consistency as a robust mechanism for improving mathematical reasoning in LLMs, paving the way for more reliable and interpretable AI-driven mathematics.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6790.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6790.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SSC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Structured Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hierarchical self-consistency framework that verifies mathematical reasoning at atomic statement, logical dependency, and global reasoning-graph levels using ensemble agreement, entropy minimization, and structural isomorphism detection, plus adaptive sampling and repair mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4, Claude 3, Gemini Ultra, Mixtral 8x22B (evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Applied to state-of-the-art foundation transformer LLMs and their mathematics-specialized variants; the paper treats models as black-box samplers from P_theta(y|x) and runs the SSC pipeline on sampled outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer (no architectural modification to base models); SSC is a verification/decoding layer operating on model outputs (reasoning-graph analysis, spectral relaxation for graph isomorphism)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Structured self-consistency (hierarchical verification of intermediate steps and final outputs; ensemble agreement, entropy reduction, structural isomorphism of reasoning graphs; adaptive sampling; repair of hallucinated nodes/edges)</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>In-house mathematical evaluation suite (theorem proving, symbolic manipulation, numerical computation)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Three-domain evaluation covering formal theorem proving (proof generation and validity), symbolic transformation (expression equivalence and transformation-step correctness using AST/tree-edit and algebraic equivalence checks), and numerical computation (accuracy and variance/stability over iterative/numerical tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Proof generation/verification, symbolic expression equivalence/transformation, numerical computation and stability</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Proof validity (%), Expression equivalence (%), Numerical accuracy (%), Variance reduction (%), Spearman correlation with human judgments</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Proof validity 79.1% ± 1.8; Expression equivalence 81.2% ± 1.9; Numerical accuracy 84.7% ± 1.9; Variance reduction 42.8% ± 2.8; Spearman ρ = 0.87 with human experts</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Compared to single-sample baseline: +8.3% proof validity, +9.6% expression equivalence, 42.8% reduction in numerical variance; vs. fixed majority-vote baseline SSC reduced computational overhead by 56.3% via adaptive sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Extending self-consistency to intermediate reasoning steps and structural graph verification substantially improves proof validity, symbolic equivalence, and numerical stability while preserving efficiency with adaptive sampling; structural SC correlates strongly with human judgments (ρ=0.87).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Higher compute than single-sample inference; vulnerable to 'collective hallucination' where multiple samples consistently produce the same incorrect reasoning; requires domain-specific adaptations (ASTs, equivalence checkers, graph alignment); performance depends on base LLM quality; focused on textual mathematical reasoning (multi-modal content not addressed).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Mathematical Reasoning in Large Language Models with Self-Consistency-Based Hallucination Detection', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6790.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6790.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Majority / Ensemble Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard self-consistency approach that aggregates multiple independent samples (majority voting or clustering) to improve factual reliability, traditionally focused on final-answer agreement rather than intermediate-step verification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied across GPT-4, Claude 3, Gemini Ultra, Mixtral 8x22B in baselines</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Black-box sampling from transformer LLMs; ensemble of k sampled outputs aggregated by frequency or clustering.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer (decoding/aggregation strategy)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Ensemble sampling and majority voting / clustering over final answers (standard SC)</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Same three-domain suite (theorem proving, symbolic, numerical) used in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Final-answer level evaluation across proofs, symbolic transformations, and numerical problems.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Proof validity, expression equivalence, numerical accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Proof validity (%), Expression equivalence (%), Numerical accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Representative single-sample (SS) and majority-vote (MV) baseline numbers from paper: SS proof validity 62.4% ±3.1, MV 67.8% ±2.8; SS expression equivalence 65.7% ±3.3, MV 70.3% ±2.9; SS numerical accuracy 68.3% ±3.5, MV 74.6% ±3.0</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Standard MV improves substantially over SS (e.g., +5.4% proof validity) but lags behind SSC which verifies intermediate structure.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Final-answer self-consistency (majority voting) helps but is insufficient for multi-step mathematical reasoning where intermediate-step errors cascade.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Focus on final answers misses logical inconsistencies in intermediate steps; fixed large-sample MV is computationally expensive (10 samples fixed in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Mathematical Reasoning in Large Language Models with Self-Consistency-Based Hallucination Detection', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6790.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6790.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompting technique that elicits step-by-step internal reasoning from LLMs to improve multi-step problem solving; used as a baseline here.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4, Claude 3, Gemini Ultra, Mixtral 8x22B (evaluated with CoT prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer LLMs prompted to produce explicit intermediate reasoning tokens (chain-of-thought).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer with chain-of-thought prompting (no model architecture change)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Chain-of-Thought prompting (explicit step-by-step generation)</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Theorem proving / symbolic / numerical tasks (same internal suite)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Multi-step reasoning tasks evaluated with CoT-style prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Proof generation, symbolic transformations, numerical computation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Proof validity (%), Expression equivalence (%), Numerical accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>CoT results from paper: Proof validity 70.2% ±2.5; Expression equivalence 72.8% ±2.6; Numerical accuracy 76.9% ±2.7</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>CoT improves substantially over single-sample SS (e.g., +7.8% proof validity vs SS) but is outperformed by SSC which adds structural verification.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CoT helps by exposing intermediate steps, but without structural verification it still allows hallucinated intermediate steps to persist.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Does not provide automated verification of intermediate steps; still susceptible to cascading errors and collective hallucination; moderate computational cost (5 avg. samples in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Mathematical Reasoning in Large Language Models with Self-Consistency-Based Hallucination Detection', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6790.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6790.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ToT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Search-based multi-path reasoning technique that explores branching thought sequences (breadth-first search with beam) to find valid reasoning chains; used as a baseline (b=5, d=3).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4, Claude 3, Gemini Ultra, Mixtral 8x22B (evaluated with ToT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer LLMs guided by tree search over generated steps (ToT algorithm), implemented breadth-first with beam width 5 and depth 3 in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer + tree search decoding (ToT)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Tree-of-Thought search (beam/breadth-first exploration of reasoning branches)</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Same internal three-domain suite</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Multi-step logical tasks where branching search can find valid derivations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Proof generation, symbolic transformation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Proof validity (%), Expression equivalence (%), Numerical accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>ToT results from paper: Proof validity 72.6% ±2.3; Expression equivalence 74.2% ±2.4; Numerical accuracy 78.5% ±2.5</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>ToT outperforms CoT and MV in these setups but remains below SSC which applies structural consistency across sampled graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Search-based exploration improves correctness by finding alternate valid reasoning branches but is more computationally expensive than SSC's adaptive sampling for comparable accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>High computational overhead (reported avg. overhead 12.6× and avg. samples 8.4), complexity grows with beam/depth, still lacks structural-level ensemble verification across independent samplings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Mathematical Reasoning in Large Language Models with Self-Consistency-Based Hallucination Detection', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6790.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6790.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EV</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>External Verification (tool-augmented verification)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline approach integrating external specialized tools (e.g., Wolfram Alpha, Isabelle/HOL) to verify or compute parts of the reasoning, used here as a comparator.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4, Claude 3, Gemini Ultra, Mixtral 8x22B (evaluated with external verification)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer LLMs whose outputs are post-processed or checked with external symbolic/numeric/formal tools.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer + external verifier/toolchain (hybrid)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>External verification using tools (e.g., Wolfram Alpha for computations, Isabelle/HOL for formal proof checking) to validate or correct outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Integrates specialized tools (paper mentions Wolfram Alpha and Isabelle/HOL) to check computations and formal proof steps; used as an external verification baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Same internal evaluation across theorem proving, symbolic, numerical</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Tasks where external engines can check algebraic equivalence, numeric results, or formal proof validity.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Proof verification/generation, algebraic equivalence checking, numerical computation validation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Proof validity (%), Expression equivalence (%), Numerical accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>EV results from paper: Proof validity 74.9% ±2.1; Expression equivalence 76.1% ±2.2; Numerical accuracy 80.2% ±2.3</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>EV outperforms CoT/ToT/MV in many metrics but SSC still provided higher accuracy while being more parameter-efficient and avoiding specialized tool integration.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>External tools improve correctness but introduce tool-specific integration complexity and computational overhead; SSC approaches similar or better gains without heavy tool dependence.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires domain-specific tool integration, may be computationally costly or brittle when tool interfaces or coverage are limited; not as parameter-efficient as SSC's ensemble approach.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Mathematical Reasoning in Large Language Models with Self-Consistency-Based Hallucination Detection', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6790.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6790.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Evaluated Foundation Models</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4, Claude 3, Gemini Ultra, Mixtral 8x22B (evaluation set)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Set of state-of-the-art foundation transformer LLMs evaluated in experiments; treated as black-box samplers for SSC and baseline methods.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4; Claude 3; Gemini Ultra; Mixtral 8x22B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large transformer-based foundation models (proprietary/closed-source for some) and a specific Mixtral variant noted as '8x22B'; used without architectural modification, with SSC and baseline decoding/reasoning methods applied at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Mixtral 8x22B reported; other model sizes not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer family (foundation LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Evaluated under multiple reasoning methods: Single-sample, Temperature Sampling, Majority Voting, Chain-of-Thought, Tree-of-Thought, External Verification, Structured Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>In-paper three-domain suite (theorem proving, symbolic manipulation, numerical computation)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Custom evaluation tasks designed to probe strict mathematical/logical reasoning across proof generation, AST-based symbolic transforms, and numerical stability tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Formal theorem proving, symbolic algebra, numerical computation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Reported per-method aggregated metrics (see per-method entries); SSC numbers above represent best-performing method across these models</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Paper reports aggregated baseline vs SSC improvements; individual model-by-model breakdown not provided in main text (aggregate averaged across models and math-specialized variants).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>All evaluated foundation models benefit from SSC; absolute gains depend on base model quality (diminishing returns for already-strong models).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Paper reports aggregate results across models; per-model sizes, training corpora, or fine-tuning details are not disclosed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Mathematical Reasoning in Large Language Models with Self-Consistency-Based Hallucination Detection', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>SelfCheckGPT: Zero-resource black-box hallucination detection for generative large language models <em>(Rating: 2)</em></li>
                <li>Self-contradictory hallucinations of large language models: Evaluation, detection and mitigation <em>(Rating: 2)</em></li>
                <li>Detecting hallucinations in large language models using semantic entropy <em>(Rating: 2)</em></li>
                <li>DeepSeek-Prover: Advancing theorem proving in llms through large-scale synthetic data <em>(Rating: 1)</em></li>
                <li>Critique-out-loud reward models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6790",
    "paper_id": "paper-277781064",
    "extraction_schema_id": "extraction-schema-131",
    "extracted_data": [
        {
            "name_short": "SSC",
            "name_full": "Structured Self-Consistency",
            "brief_description": "A hierarchical self-consistency framework that verifies mathematical reasoning at atomic statement, logical dependency, and global reasoning-graph levels using ensemble agreement, entropy minimization, and structural isomorphism detection, plus adaptive sampling and repair mechanisms.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4, Claude 3, Gemini Ultra, Mixtral 8x22B (evaluated)",
            "model_description": "Applied to state-of-the-art foundation transformer LLMs and their mathematics-specialized variants; the paper treats models as black-box samplers from P_theta(y|x) and runs the SSC pipeline on sampled outputs.",
            "model_size": null,
            "architecture_type": "Transformer (no architectural modification to base models); SSC is a verification/decoding layer operating on model outputs (reasoning-graph analysis, spectral relaxation for graph isomorphism)",
            "training_data": null,
            "reasoning_method": "Structured self-consistency (hierarchical verification of intermediate steps and final outputs; ensemble agreement, entropy reduction, structural isomorphism of reasoning graphs; adaptive sampling; repair of hallucinated nodes/edges)",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "In-house mathematical evaluation suite (theorem proving, symbolic manipulation, numerical computation)",
            "benchmark_description": "Three-domain evaluation covering formal theorem proving (proof generation and validity), symbolic transformation (expression equivalence and transformation-step correctness using AST/tree-edit and algebraic equivalence checks), and numerical computation (accuracy and variance/stability over iterative/numerical tasks).",
            "task_type": "Proof generation/verification, symbolic expression equivalence/transformation, numerical computation and stability",
            "performance_metric": "Proof validity (%), Expression equivalence (%), Numerical accuracy (%), Variance reduction (%), Spearman correlation with human judgments",
            "performance_value": "Proof validity 79.1% ± 1.8; Expression equivalence 81.2% ± 1.9; Numerical accuracy 84.7% ± 1.9; Variance reduction 42.8% ± 2.8; Spearman ρ = 0.87 with human experts",
            "comparison_with_baseline": "Compared to single-sample baseline: +8.3% proof validity, +9.6% expression equivalence, 42.8% reduction in numerical variance; vs. fixed majority-vote baseline SSC reduced computational overhead by 56.3% via adaptive sampling.",
            "key_findings": "Extending self-consistency to intermediate reasoning steps and structural graph verification substantially improves proof validity, symbolic equivalence, and numerical stability while preserving efficiency with adaptive sampling; structural SC correlates strongly with human judgments (ρ=0.87).",
            "limitations": "Higher compute than single-sample inference; vulnerable to 'collective hallucination' where multiple samples consistently produce the same incorrect reasoning; requires domain-specific adaptations (ASTs, equivalence checkers, graph alignment); performance depends on base LLM quality; focused on textual mathematical reasoning (multi-modal content not addressed).",
            "uuid": "e6790.0",
            "source_info": {
                "paper_title": "Enhancing Mathematical Reasoning in Large Language Models with Self-Consistency-Based Hallucination Detection",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Self-Consistency (baseline)",
            "name_full": "Majority / Ensemble Self-Consistency",
            "brief_description": "Standard self-consistency approach that aggregates multiple independent samples (majority voting or clustering) to improve factual reliability, traditionally focused on final-answer agreement rather than intermediate-step verification.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Applied across GPT-4, Claude 3, Gemini Ultra, Mixtral 8x22B in baselines",
            "model_description": "Black-box sampling from transformer LLMs; ensemble of k sampled outputs aggregated by frequency or clustering.",
            "model_size": null,
            "architecture_type": "Transformer (decoding/aggregation strategy)",
            "training_data": null,
            "reasoning_method": "Ensemble sampling and majority voting / clustering over final answers (standard SC)",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "Same three-domain suite (theorem proving, symbolic, numerical) used in this paper",
            "benchmark_description": "Final-answer level evaluation across proofs, symbolic transformations, and numerical problems.",
            "task_type": "Proof validity, expression equivalence, numerical accuracy",
            "performance_metric": "Proof validity (%), Expression equivalence (%), Numerical accuracy (%)",
            "performance_value": "Representative single-sample (SS) and majority-vote (MV) baseline numbers from paper: SS proof validity 62.4% ±3.1, MV 67.8% ±2.8; SS expression equivalence 65.7% ±3.3, MV 70.3% ±2.9; SS numerical accuracy 68.3% ±3.5, MV 74.6% ±3.0",
            "comparison_with_baseline": "Standard MV improves substantially over SS (e.g., +5.4% proof validity) but lags behind SSC which verifies intermediate structure.",
            "key_findings": "Final-answer self-consistency (majority voting) helps but is insufficient for multi-step mathematical reasoning where intermediate-step errors cascade.",
            "limitations": "Focus on final answers misses logical inconsistencies in intermediate steps; fixed large-sample MV is computationally expensive (10 samples fixed in experiments).",
            "uuid": "e6790.1",
            "source_info": {
                "paper_title": "Enhancing Mathematical Reasoning in Large Language Models with Self-Consistency-Based Hallucination Detection",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "CoT",
            "name_full": "Chain-of-Thought prompting",
            "brief_description": "Prompting technique that elicits step-by-step internal reasoning from LLMs to improve multi-step problem solving; used as a baseline here.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4, Claude 3, Gemini Ultra, Mixtral 8x22B (evaluated with CoT prompting)",
            "model_description": "Transformer LLMs prompted to produce explicit intermediate reasoning tokens (chain-of-thought).",
            "model_size": null,
            "architecture_type": "Transformer with chain-of-thought prompting (no model architecture change)",
            "training_data": null,
            "reasoning_method": "Chain-of-Thought prompting (explicit step-by-step generation)",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "Theorem proving / symbolic / numerical tasks (same internal suite)",
            "benchmark_description": "Multi-step reasoning tasks evaluated with CoT-style prompts.",
            "task_type": "Proof generation, symbolic transformations, numerical computation",
            "performance_metric": "Proof validity (%), Expression equivalence (%), Numerical accuracy (%)",
            "performance_value": "CoT results from paper: Proof validity 70.2% ±2.5; Expression equivalence 72.8% ±2.6; Numerical accuracy 76.9% ±2.7",
            "comparison_with_baseline": "CoT improves substantially over single-sample SS (e.g., +7.8% proof validity vs SS) but is outperformed by SSC which adds structural verification.",
            "key_findings": "CoT helps by exposing intermediate steps, but without structural verification it still allows hallucinated intermediate steps to persist.",
            "limitations": "Does not provide automated verification of intermediate steps; still susceptible to cascading errors and collective hallucination; moderate computational cost (5 avg. samples in experiments).",
            "uuid": "e6790.2",
            "source_info": {
                "paper_title": "Enhancing Mathematical Reasoning in Large Language Models with Self-Consistency-Based Hallucination Detection",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "ToT",
            "name_full": "Tree-of-Thought",
            "brief_description": "Search-based multi-path reasoning technique that explores branching thought sequences (breadth-first search with beam) to find valid reasoning chains; used as a baseline (b=5, d=3).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4, Claude 3, Gemini Ultra, Mixtral 8x22B (evaluated with ToT)",
            "model_description": "Transformer LLMs guided by tree search over generated steps (ToT algorithm), implemented breadth-first with beam width 5 and depth 3 in experiments.",
            "model_size": null,
            "architecture_type": "Transformer + tree search decoding (ToT)",
            "training_data": null,
            "reasoning_method": "Tree-of-Thought search (beam/breadth-first exploration of reasoning branches)",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "Same internal three-domain suite",
            "benchmark_description": "Multi-step logical tasks where branching search can find valid derivations.",
            "task_type": "Proof generation, symbolic transformation",
            "performance_metric": "Proof validity (%), Expression equivalence (%), Numerical accuracy (%)",
            "performance_value": "ToT results from paper: Proof validity 72.6% ±2.3; Expression equivalence 74.2% ±2.4; Numerical accuracy 78.5% ±2.5",
            "comparison_with_baseline": "ToT outperforms CoT and MV in these setups but remains below SSC which applies structural consistency across sampled graphs.",
            "key_findings": "Search-based exploration improves correctness by finding alternate valid reasoning branches but is more computationally expensive than SSC's adaptive sampling for comparable accuracy.",
            "limitations": "High computational overhead (reported avg. overhead 12.6× and avg. samples 8.4), complexity grows with beam/depth, still lacks structural-level ensemble verification across independent samplings.",
            "uuid": "e6790.3",
            "source_info": {
                "paper_title": "Enhancing Mathematical Reasoning in Large Language Models with Self-Consistency-Based Hallucination Detection",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "EV",
            "name_full": "External Verification (tool-augmented verification)",
            "brief_description": "Baseline approach integrating external specialized tools (e.g., Wolfram Alpha, Isabelle/HOL) to verify or compute parts of the reasoning, used here as a comparator.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4, Claude 3, Gemini Ultra, Mixtral 8x22B (evaluated with external verification)",
            "model_description": "Transformer LLMs whose outputs are post-processed or checked with external symbolic/numeric/formal tools.",
            "model_size": null,
            "architecture_type": "Transformer + external verifier/toolchain (hybrid)",
            "training_data": null,
            "reasoning_method": "External verification using tools (e.g., Wolfram Alpha for computations, Isabelle/HOL for formal proof checking) to validate or correct outputs.",
            "external_tool_used": true,
            "external_tool_description": "Integrates specialized tools (paper mentions Wolfram Alpha and Isabelle/HOL) to check computations and formal proof steps; used as an external verification baseline.",
            "benchmark_name": "Same internal evaluation across theorem proving, symbolic, numerical",
            "benchmark_description": "Tasks where external engines can check algebraic equivalence, numeric results, or formal proof validity.",
            "task_type": "Proof verification/generation, algebraic equivalence checking, numerical computation validation",
            "performance_metric": "Proof validity (%), Expression equivalence (%), Numerical accuracy (%)",
            "performance_value": "EV results from paper: Proof validity 74.9% ±2.1; Expression equivalence 76.1% ±2.2; Numerical accuracy 80.2% ±2.3",
            "comparison_with_baseline": "EV outperforms CoT/ToT/MV in many metrics but SSC still provided higher accuracy while being more parameter-efficient and avoiding specialized tool integration.",
            "key_findings": "External tools improve correctness but introduce tool-specific integration complexity and computational overhead; SSC approaches similar or better gains without heavy tool dependence.",
            "limitations": "Requires domain-specific tool integration, may be computationally costly or brittle when tool interfaces or coverage are limited; not as parameter-efficient as SSC's ensemble approach.",
            "uuid": "e6790.4",
            "source_info": {
                "paper_title": "Enhancing Mathematical Reasoning in Large Language Models with Self-Consistency-Based Hallucination Detection",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Evaluated Foundation Models",
            "name_full": "GPT-4, Claude 3, Gemini Ultra, Mixtral 8x22B (evaluation set)",
            "brief_description": "Set of state-of-the-art foundation transformer LLMs evaluated in experiments; treated as black-box samplers for SSC and baseline methods.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4; Claude 3; Gemini Ultra; Mixtral 8x22B",
            "model_description": "Large transformer-based foundation models (proprietary/closed-source for some) and a specific Mixtral variant noted as '8x22B'; used without architectural modification, with SSC and baseline decoding/reasoning methods applied at inference.",
            "model_size": "Mixtral 8x22B reported; other model sizes not specified in paper",
            "architecture_type": "Transformer family (foundation LLMs)",
            "training_data": null,
            "reasoning_method": "Evaluated under multiple reasoning methods: Single-sample, Temperature Sampling, Majority Voting, Chain-of-Thought, Tree-of-Thought, External Verification, Structured Self-Consistency",
            "external_tool_used": null,
            "external_tool_description": null,
            "benchmark_name": "In-paper three-domain suite (theorem proving, symbolic manipulation, numerical computation)",
            "benchmark_description": "Custom evaluation tasks designed to probe strict mathematical/logical reasoning across proof generation, AST-based symbolic transforms, and numerical stability tasks.",
            "task_type": "Formal theorem proving, symbolic algebra, numerical computation",
            "performance_metric": "Reported per-method aggregated metrics (see per-method entries); SSC numbers above represent best-performing method across these models",
            "performance_value": null,
            "comparison_with_baseline": "Paper reports aggregated baseline vs SSC improvements; individual model-by-model breakdown not provided in main text (aggregate averaged across models and math-specialized variants).",
            "key_findings": "All evaluated foundation models benefit from SSC; absolute gains depend on base model quality (diminishing returns for already-strong models).",
            "limitations": "Paper reports aggregate results across models; per-model sizes, training corpora, or fine-tuning details are not disclosed.",
            "uuid": "e6790.5",
            "source_info": {
                "paper_title": "Enhancing Mathematical Reasoning in Large Language Models with Self-Consistency-Based Hallucination Detection",
                "publication_date_yy_mm": "2025-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "SelfCheckGPT: Zero-resource black-box hallucination detection for generative large language models",
            "rating": 2,
            "sanitized_title": "selfcheckgpt_zeroresource_blackbox_hallucination_detection_for_generative_large_language_models"
        },
        {
            "paper_title": "Self-contradictory hallucinations of large language models: Evaluation, detection and mitigation",
            "rating": 2,
            "sanitized_title": "selfcontradictory_hallucinations_of_large_language_models_evaluation_detection_and_mitigation"
        },
        {
            "paper_title": "Detecting hallucinations in large language models using semantic entropy",
            "rating": 2,
            "sanitized_title": "detecting_hallucinations_in_large_language_models_using_semantic_entropy"
        },
        {
            "paper_title": "DeepSeek-Prover: Advancing theorem proving in llms through large-scale synthetic data",
            "rating": 1,
            "sanitized_title": "deepseekprover_advancing_theorem_proving_in_llms_through_largescale_synthetic_data"
        },
        {
            "paper_title": "Critique-out-loud reward models",
            "rating": 1,
            "sanitized_title": "critiqueoutloud_reward_models"
        }
    ],
    "cost": 0.01391325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Enhancing Mathematical Reasoning in Large Language Models with Self-Consistency-Based Hallucination Detection
20 May 2025</p>
<p>Mingshan Liu 
The Hong Kong University of Science and Technology</p>
<p>Shi Bo 
Jialing Fang 
Fudan University</p>
<p>Enhancing Mathematical Reasoning in Large Language Models with Self-Consistency-Based Hallucination Detection
20 May 2025946BEDD05366038B6EA4D07615830773arXiv:2504.09440v2[cs.AI]
Large language models (LLMs) have demonstrated impressive mathematical reasoning capabilities but remain susceptible to hallucinations-plausible yet incorrect statements-particularly in complex domains requiring rigorous logical deduction.Current approaches to improve reliability often neglect the logical consistency of intermediate reasoning steps, focusing primarily on final answer verification.We propose a structured self-consistency (SC) framework that systematically evaluates factual concordance across both intermediate reasoning steps and final outputs, thereby creating a hierarchical verification mechanism for mathematical reasoning.Our framework employs a probabilistic formulation that quantifies consistency through ensemble agreement, entropy minimization, and structural isomorphism detection in reasoning graphs.We evaluate our approach on three fundamental mathematical tasks: formal theorem proving, symbolic transformation, and numerical computation.Experimental results demonstrate that our method achieves significant improvements over baseline approaches: proof validity increases by 8.3% (p &lt; 0.01), symbolic reasoning accuracy by 9.6%, and numerical stability by 42.8% while reducing computational overhead by 56.3%.Further analysis reveals that our structured SC framework exhibits strong correlation with human expert evaluation (ρ = 0.87), suggesting its efficacy as a reliable proxy for mathematical correctness.These findings establish selfconsistency as a parameter-efficient mechanism for enhancing mathematical reasoning in LLMs, with implications for trustworthy AI in domains requiring formal verification.</p>
<p>Introduction</p>
<p>Large language models (LLMs) have achieved remarkable breakthroughs in mathematical reasoning tasks, demonstrating capabilities that approach expert-level performance in theorem proving, symbolic manipulation, and numerical problemsolving.Despite these advances, LLMs remain susceptible to hallucinations-generating outputs that appear plausible but contain factual inaccuracies or logical inconsistencies.In mathematical reasoning, where correctness is strictly binary and errors propagate through derivation chains, hallucinations pose a fundamental challenge to the reliability and trustworthiness of AI systems.</p>
<p>Mathematical hallucinations in LLMs manifest in diverse forms, including incorrect computation (e.g., 7 × 8 = 54), invalid algebraic transformations (e.g., √ a 2 + b 2 = a + b), and unsound logical deductions (e.g., claiming P ⇒ Q from premises that only establish P ⇒ R).These errors undermine the utility of LLMs in high-stakes domains such as scientific computing, formal verification, and educational applications.The challenge is particularly acute in multi-step reasoning tasks, where a single hallucinated statement can invalidate an entire derivation chain, even if all other steps are correct.This "cascading error" phenomenon necessitates verification mechanisms that operate at both local (individual statement) and global (entire reasoning chain) levels.</p>
<p>Current approaches to mitigate hallucinations in LLMs include fine-tuning on curated datasets (Xin et al., 2024b), integrating external verification systems (Ankner et al., 2024), and hybrid neuro-symbolic architectures (Feldstein et al., 2024).While these methods have shown promise, they often incur substantial computational costs, require domain-specific adaptations, or involve complex architectural modifications.A more parameter-efficient approach is self-consistency (SC), which leverages agreement among multiple sampled responses to enhance factual reliability (Manakul et al., 2023a;Farquhar et al., 2024;Mündler et al., 2024).However, existing SC implementations primarily focus on final answer ver-ification, neglecting the logical coherence of intermediate reasoning steps-a critical limitation for complex mathematical tasks that require step-bystep derivation.</p>
<p>The fundamental challenge lies in developing a verification mechanism that maintains logical consistency throughout multi-step mathematical reasoning without prohibitive computational overhead.This paper addresses this challenge by proposing a structured self-consistency framework that systematically evaluates both intermediate steps and final outputs, creating a hierarchical verification mechanism for mathematical reasoning.Our approach is motivated by the insight that mathematical derivations form directed acyclic graphs (DAGs) of logical dependencies, where consistency must be maintained not only at individual nodes but across the entire graph structure.</p>
<p>We formalize this intuition through a probabilistic framework that quantifies self-consistency using ensemble agreement, entropy minimization, and structural isomorphism detection in reasoning graphs.By extending SC beyond simple majority voting to incorporate structural verification, we create a more robust mechanism for detecting and correcting hallucinations in mathematical content.Furthermore, we develop adaptive sampling strategies that optimize the trade-off between reasoning accuracy and computational efficiency, addressing a key limitation of existing SC methods.</p>
<p>To validate our approach, we conduct a comprehensive empirical study on three core mathematical reasoning tasks: formal theorem proving, symbolic transformation, and numerical computation.These tasks represent fundamental challenges in mathematical reasoning, requiring precise logical deduction, algebraic manipulation, and computational stability, respectively.Our experiments demonstrate that structured SC significantly improves reasoning accuracy and reduces hallucinations across all three domains while maintaining computational efficiency.</p>
<p>Contributions.This paper makes the following key contributions:</p>
<p>(1) We formulate a novel self-consistency framework that extends verification to intermediate reasoning steps through probabilistic modeling of logical dependencies in mathematical derivations;</p>
<p>(2) We develop efficient algorithms for struc-tural consistency verification that analyze isomorphism in reasoning graphs, enabling more robust hallucination detection in multi-step mathematical derivations;</p>
<p>(3) We present adaptive sampling strategies that optimize the trade-off between reasoning accuracy and computational cost, addressing a key limitation of existing self-consistency methods;</p>
<p>(4) We conduct a comprehensive empirical evaluation across three distinct mathematical reasoning domains (theorem proving, symbolic manipulation, and numerical computation), demonstrating significant improvements in accuracy and stability;</p>
<p>(5) We provide theoretical analysis of the relationship between self-consistency and mathematical correctness, establishing formal bounds on error rates and propagation dynamics.</p>
<p>Related Work</p>
<p>Hallucinations in Large Language Models</p>
<p>Hallucinations in LLMs refer to the generation of content that is plausible but factually incorrect or internally inconsistent (Yin et al., 2023;Xiong et al., 2024).Recent studies have categorized hallucinations into factual errors (contradicting established knowledge), logical inconsistencies (violating rules of inference), and self-contradictions (inconsistency within the generated content) (Huang et al., 2023;Bai et al., 2022).In mathematical reasoning, hallucinations often manifest as plausiblelooking but incorrect derivations, computations, or deductions (Feldstein et al., 2024;Wang et al., 2024b).Detection approaches for hallucinations include analyzing internal model activations (Azaria and Mitchell, 2023;Burns et al., 2023), monitoring attention patterns (Simhi et al., 2024;Zhang et al., 2024), and applying uncertainty estimation techniques (Farquhar et al., 2024;Kossen et al., 2024).Mitigation strategies have focused on dataset-level interventions (Lee et al., 2023;Zhou et al., 2024;Elaraby et al., 2023) and reinforcement learning with human feedback (Ouyang et al., 2022;Bai et al., 2022).Despite these efforts, mathematical hallucinations remain particularly challenging due to the specialized nature of mathematical knowledge and the strict correctness requirements in formal reasoning.</p>
<p>Self-Consistency for Improving Factual Reliability</p>
<p>Self-consistency (SC) has emerged as an effective technique for enhancing factual reliability in LLM outputs.Initially proposed as a verification mechanism for question-answering tasks (Wang et al., 2023), SC leverages agreement among independently sampled responses to identify and filter out hallucinations.The fundamental insight is that correct statements tend to appear consistently across multiple samples, while hallucinations exhibit higher variance (Manakul et al., 2023b;Farquhar et al., 2024).</p>
<p>Early SC approaches focused on majority voting for final answers (Li et al., 2022;Shi et al., 2022), but these methods are limited to tasks with well-defined, discrete output spaces.Recent extensions have adapted SC for open-ended generation using clustering techniques (Thirukovalluru et al., 2024), iterative refinement (Mündler et al., 2024), and statement-level verification (Chen et al., 2023;Wang et al., 2024a).However, these methods typically treat each statement independently, neglecting the logical relationships that are crucial in mathematical reasoning.</p>
<p>The theoretical foundations of SC have been explored through information-theoretic perspectives (Desai and Durrett, 2020), Bayesian inference (Jiang et al., 2021), and uncertainty quantification (Glushkova et al., 2021;Duan et al., 2024).These frameworks provide formal justification for using consistency as a proxy for factual correctness, but they have not been specifically adapted to the structural constraints of mathematical reasoning.</p>
<p>Mathematical Reasoning in Large</p>
<p>Language Models</p>
<p>Recent work in LLM-based mathematical reasoning spans three primary areas: theorem proving, symbolic manipulation, and numerical computation.In theorem proving, LLMs have demonstrated capabilities in generating formal proofs (Xin et al., 2024a;Lightman et al., 2023b), but they often produce logically unsound deductions or overlook critical assumptions.Symbolic reasoning research has focused on algebraic manipulation (Wang et al., 2024c;Feldstein et al., 2024), where LLMs struggle with complex transformations and sometimes produce invalid steps.Numerical computation studies have examined sta-bility and precision in calculation tasks (He et al., 2024;Jain et al., 2024a), highlighting issues with arithmetic errors and inconsistent rounding.</p>
<p>Existing approaches for improving mathematical reasoning include process supervision (Lightman et al., 2023b), tree-based search (Wu et al., 2024), and verification models (Ankner et al., 2024).While these methods enhance performance, they often require specialized training, complex search procedures, or external verifiers.</p>
<p>A key research direction is developing lightweight verification mechanisms that can integrate with existing LLMs without substantial architectural modifications or computational overhead.</p>
<p>Decoding Strategies for Hallucination Mitigation</p>
<p>Beyond direct training interventions, several decoding-based strategies have been proposed to mitigate hallucinations at inference time.Contrastive decoding techniques (Burns et al., 2023;Chuang et al., 2024b) adjust token probabilities to favor factual content, while inference-time intervention methods (Li et al., 2024) manipulate attention mechanisms to improve factuality.Other approaches include lookback decoding (Chuang et al., 2024a) for consistency enforcement and constrained sampling for imposing logical constraints (Brown et al., 2024).While these methods improve general factuality, they have not been specifically tailored to the requirements of mathematical reasoning, where constraints include not only factual correctness but also strict adherence to formal rules of inference, algebraic transformations, and numerical precision.Our work addresses this gap by developing specialized self-consistency mechanisms for mathematical content that incorporate both factual and structural verification.</p>
<p>Methodology</p>
<p>Our structured self-consistency framework systematically validates both intermediate steps and final outputs in mathematical reasoning, ensuring logical coherence throughout the derivation process.</p>
<p>Hierarchical Self-Consistency Framework</p>
<p>Our framework operates at three levels: atomic statement verification, logical dependency verifi-cation, and global reasoning verification, forming an integrated hierarchical structure.At the atomic level, we verify individual mathematical statements by measuring their consistency across multiple sampled responses.For a statement s, the basic self-consistency score is defined as:
SC atomic (s) = 1 k k i=1 I(s ∈ y i )(1)
where I(s ∈ y i ) indicates whether statement s appears in response y i .To handle semantic equivalence beyond exact matching, we refine this verification through embedding-based similarity, which is particularly important for identifying equivalent mathematical statements with different expressions (e.g., "a 2 − b 2 = (a + b)(a − b)" and "the difference of squares equals the product of sum and difference").</p>
<p>At the logical dependency level, we verify the relationships between statements to ensure coherent reasoning.For a pair of statements (s i , s j ) where s j logically follows from s i , we compute:
SC logical (s i , s j ) = 1 k k l=1 I((s i , s j ) ∈ E y l ) (2)
where E y l represents the edges in the reasoning graph of response y l .This score quantifies the consistency of logical dependencies across multiple reasoning paths, capturing the complex logical structures common in mathematical proofs.</p>
<p>At the global level, we evaluate the coherence of the entire reasoning process by analyzing the structural similarity of reasoning graphs across multiple responses:
SC global = 2 k(k − 1) k i=1 k j=i+1 Iso(G y i , G y j ) (3)
Here, Iso(G y i , G y j ) represents the structural isomorphism between two reasoning graphs, identifying potential hallucinations at the global level that might not be detectable through local verification alone.These three levels of verification collectively form a comprehensive framework for detecting and filtering hallucinations in mathematical reasoning at different granularities.</p>
<p>Domain-Specific Adaptations</p>
<p>We adapt our framework to three key mathematical reasoning domains: theorem proving, symbolic manipulation, and numerical computation.</p>
<p>For theorem proving, we represent proofs as sequences of statements with logical dependencies.Given a theorem statement T and multiple proof attempts P, we first align corresponding steps across different proofs using semantic similarity, then quantify the structural consistency of proofs:
SC theorem = β • SC proof + (1 − β) • Sound(P) (4)
where SC proof evaluates the structural consistency of proofs, while Sound(P) verifies that each deduction step validly follows from its premises, and β controls the relative importance of consistency versus soundness.</p>
<p>For symbolic manipulation, we represent expressions as abstract syntax trees (ASTs) and compare their structure using tree edit distance.We combine structural similarity with algebraic equivalence checking to evaluate the consistency of symbolic transformations:
SC symbolic = λ • TS(E) + (1 − λ) • AE(E) (5)
where TreeSimilarity evaluates structural similarity based on tree edit distance, and AlgebraicEquivalence verifies algebraic equivalence through symbolic computation, even when surface forms differ.</p>
<p>For numerical computation, we evaluate consistency through statistical dispersion measures, providing a robust measure of numerical stability:
SC numerical = 1 − σ(N ) µ(N )(6)
where σ(N ) is the standard deviation and µ(N ) is the mean, providing a normalized measure of numerical consistency.</p>
<p>Adaptive Sampling and Structural Verification</p>
<p>A key challenge in applying self-consistency is determining the optimal number of samples to balance verification quality and computational efficiency.We introduce an adaptive sampling strategy that dynamically adjusts the sampling rate based on consistency metrics:</p>
<p>Algorithm 1 Adaptive Self-Consistency Sampling
Require: Query x, LLM M θ , parameters k 0 , k max , τ low , τ high Ensure: Optimal response y * 1: Y ← Sample k 0 responses from M θ (x) 2: t ← k 0 , Λ t ← ComputeConsistencyScore(Y ) 3: while t &lt; k max ∧ Λ t ≤ τ high do 4: δ ← max (1, ⌈α • (τ low − min(Λ t , τ low )) • k max ⌉) 5: Y ← Y ∪ Sample δ responses from M θ (x) 6: t ← t + δ, Λ t ← ComputeConsistencyScore(Y ) 7: end while
The algorithm begins with k 0 initial samples and adaptively increases sampling density when consistency is low, terminating early when high consistency is detected to avoid unnecessary computation.This approach reduces average computational cost by 56.3% compared to fixed largesample approaches while maintaining comparable accuracy.</p>
<p>For structural verification, we develop a polynomial-time approximation algorithm for reasoning graph isomorphism detection, combining semantic node embeddings with spectral relaxation of Laplacian matrices.This approach enables efficient structural consistency verification, particularly effective when dealing with complex proofs with non-isomorphic reasoning paths.</p>
<p>Beyond detection, our framework includes mechanisms for repairing hallucinated content in mathematical reasoning.The repair process operates on the verified reasoning graph, replacing hallucinated nodes and edges with consistent alternatives.This enables the transformation of partially hallucinated mathematical content into fully consistent reasoning, enhancing the utility of LLMgenerated mathematics beyond simple filtering.</p>
<p>Models and Baselines</p>
<p>Our evaluation compares the proposed structured self-consistency (SSC) framework against established benchmarks using four state-of-theart foundation models (GPT-4, Claude 3, Gemini Ultra, Mixtral 8x22B) and their mathematicsspecialized variants.Baseline methods include: Single Sample (SS) generation (temperature τ = 0.7); Majority Voting (MV) (Manakul et al., 2023a;Niels et al., 2024) which aggregates responses from k = 10 samples through frequency counting; Temperature Sampling (TS) (Wang et al., 2024d) using 10 samples with temperatures from 0.1 to 1.9; Chain-of-Thought (CoT) (Lightman et al., 2023a) with explicit stepby-step prompting; Tree-of-Thought (ToT) (Jain et al., 2024b) implementing breadth-first exploration (beam width b = 5, depth d = 3); and External Verification (EV) (Xin et al., 2024b) integrating specialized tools like Wolfram Alpha and Isabelle/HOL.All evaluations use consistent prompt templates, fixed random seeds, and identical computational resources (8× NVIDIA A100 GPUs), with implementation details for each baseline in Section 5.</p>
<p>Evaluation Metrics</p>
<p>Our evaluation employs a comprehensive metric suite spanning three mathematical domains and efficiency dimensions: 5 Results and Analysis</p>
<p>Theorem Proving Performance (RQ1)</p>
<p>Table 1 presents the performance of our structured self-consistency (SSC) framework on theorem proving tasks compared to baseline methods.</p>
<p>Our approach achieves significant improvements in proof validity and logical coherence across all difficulty levels.The results demonstrate that our SSC framework outperforms all baselines across all metrics, with an average improvement of 8.3% in proof validity over the single-sample baseline.This improvement is statistically significant (p &lt; 0.01, paired t-test with Bonferroni correction).</p>
<p>Figure 2 illustrates the performance across different theorem difficulty levels, showing that SSC maintains robust performance even for hard theorems where baselines exhibit substantial degradation.</p>
<p>To understand the types of hallucinations effectively mitigated by our approach, we conduct an error analysis of 100 randomly selected proofs.Figure 3 shows the distribution of hallucination types in theorem proving before and after applying SSC.</p>
<p>The error analysis reveals that SSC is particularly effective at reducing logical fallacies (45.3% reduction), invalid deductions (38.7% reduction), and missing conditions (42.1% reduction).These improvements stem from the hierarchical verification approach, which identifies inconsistencies at multiple levels of the reasoning process.</p>
<p>Symbolic Reasoning Performance (RQ2)</p>
<p>Table 2 summarizes the performance on symbolic reasoning tasks, demonstrating the effectiveness of our approach in mitigating hallucinations during algebraic manipulations.Our SSC framework achieves a 9.6% improvement in expression equivalence over the singlesample baseline, with particularly strong performance on complex algebraic manipulations requiring multiple transformation steps.</p>
<p>Figure 4 shows the performance breakdown by problem type, illustrating that SSC provides the most significant improvements for problems involving multiple algebraic rules (e.g., polynomial factorization, trigonometric identities).</p>
<p>A qualitative analysis of 50 symbolic manipulation examples reveals that SSC effectively identifies and corrects common algebraic errors, including:</p>
<p>• Incorrect factorization patterns (e.g., incorrectly factoring x 2 + 2x + 1 as x(x + 2) + 1 instead of (x + 1) 2 )</p>
<p>• Sign errors during distribution (e.g., expanding (a− b)(c− d) as ac− ad− bc− bd instead of ac − ad − bc + bd)</p>
<p>• Invalid simplifications (e.g., simplifying</p>
<p>x 2 −1</p>
<p>x−1 as x without noting the domain restriction x = 1)</p>
<p>The success in symbolic reasoning can be attributed to the combination of structural verification (comparing expression trees) and semantic verification (checking algebraic equivalence), which together provide robust detection of hallucinations in algebraic manipulations.</p>
<p>Numerical Computation Performance (RQ3)</p>
<p>Table 3 presents results on numerical computation tasks, highlighting the enhanced stability achieved through structured self-consistency.Our approach demonstrates a substantial 42.8% reduction in output variance, indicating significantly enhanced numerical stability.This improvement is particularly pronounced for problems involving iterative calculations or numerical integration, where error propagation is a critical concern.</p>
<p>Figure 5 visualizes the distribution of numerical errors before and after applying SSC, showing a marked reduction in large errors and outliers.The distribution of errors after SSC exhibits both lower variance and greater concentration around zero, indicating more reliable numerical computations.</p>
<p>To understand the impact of SC on different types of numerical problems, we analyze performance across four subcategories:</p>
<p>• Basic Arithmetic: Elementary calculations involving addition, subtraction, multiplication, and division</p>
<p>• Matrix Operations: Linear algebra computations including matrix multiplication, inversion, and eigenvalue calculation</p>
<p>• Calculus: Numerical integration, differentiation, and series approximation</p>
<p>• Optimization: Iterative algorithms for finding minima/maxima of functions Table 4 shows the accuracy improvements across these subcategories.The results indicate that SSC provides the greatest improvements for complex numerical tasks involving iterative algorithms and multi-step calculations, where error propagation is more severe.This aligns with our theoretical analysis, which predicts that the benefits of self-consistency increase with the complexity of the dependency structure in the reasoning process.</p>
<p>Computational Efficiency (RQ4)</p>
<p>Table 5 compares the computational efficiency of different methods, demonstrating that our adaptive sampling strategy achieves a favorable balance between accuracy and efficiency.Our SSC framework achieves a 56.3% reduction in computational cost compared to the fixed majority voting baseline while maintaining superior accuracy.This efficiency gain is attributed to our adaptive sampling strategy, which allocates computational resources based on problem complexity and consistency metrics.</p>
<p>Figure 6 illustrates the accuracy-efficiency tradeoff for different methods, plotting accuracy against computational overhead.SSC consistently achieves the best balance, providing higher accuracy than alternatives at comparable computational cost.</p>
<p>To further analyze the efficiency characteristics of adaptive sampling, we examine the distribution of sample counts across different problem difficulty levels:</p>
<p>The results in Table 6 demonstrate that adaptive sampling intelligently allocates resources based on problem difficulty, using fewer samples for easy problems where consistency is achieved quickly, and more samples for challenging problems requiring deeper verification.</p>
<p>Correlation with Human Evaluation (RQ5)</p>
<p>Table 7 presents the Spearman's rank correlation between our self-consistency scores and human expert ratings, demonstrating strong alignment between automatic metrics and human judgment.Our SSC framework achieves a strong correlation of ρ = 0.87 with human judgments of mathematical correctness, significantly outperforming all baseline methods.This indicates that our structured self-consistency approach provides a reliable proxy for expert evaluation of mathematical reasoning.</p>
<p>Figure 7 visualizes the relationship between SSC scores and human ratings, showing a clear monotonic relationship.</p>
<p>The correlation is strongest for mathematical correctness and logical coherence, with slightly lower but still substantial correlation for clarity.</p>
<p>To investigate which components of the hierarchical framework contribute most to alignment with human judgment, we perform an ablation study comparing the full SSC framework with variants using only specific verification levels:</p>
<p>The ablation results demonstrate that while individual verification levels provide substantial improvements over baselines, the full hierarchical framework yields the strongest correlation with</p>
<p>Conclusion</p>
<p>We introduced a structured self-consistency framework that enhances mathematical reasoning in LLMs through hierarchical verification of both intermediate steps and final outputs.Our approach achieves significant improvements across theorem proving (+8.3% proof validity), symbolic reasoning (+9.6% expression equivalence), and numerical computation (42.8% variance reduction) while reducing computational overhead by 56.3% through adaptive sampling strategies.</p>
<p>Strong correlation with human expert evaluation (ρ = 0.87) demonstrates our framework's reliability as a proxy for mathematical correctness.</p>
<p>The theoretical guarantees on error bounds and convergence properties establish a solid foundation for self-consistency in mathematical domains, with exponential reductions in cumulative error compared to final-answer verification alone.Future work includes integration with formal verification systems, domain-specific adaptations for specialized mathematical areas, cross-modal verification spanning multiple representation forms, adaptive learning to optimize self-consistency during fine-tuning, and explainable verification for educational applications.Our framework represents a significant step toward more reliable and trustworthy AI systems for mathematical applications by addressing hallucination detection at multiple granularities.</p>
<p>Limitations</p>
<p>Despite promising results, our framework has several limitations worth addressing in future work.First, the computational overhead, while reduced through adaptive sampling, still exceeds that of single-sample approaches, presenting challenges for real-time applications.Second, our method relies on statistical agreement across samples, which may fail to detect consistent but incorrect reasoning patterns that appear across multiple samples-a phenomenon we term "collective hallucination."Third, domain-specific adaptations require mathematical expertise to implement effectively, potentially limiting broader accessibility.Fourth, the framework's performance depends on the quality of the underlying LLM, with diminishing returns observed for models with already high mathematical reasoning capabilities.Finally, our approach focuses primarily on textual mathematical reasoning and may require substantial modifications to handle multi-modal mathematical content involving diagrams, graphs, or interactive manipulations, which are common in educational and research contexts.</p>
<p>A Theoretical Analysis</p>
<p>A.1 Error Bounds and Convergence Guarantees</p>
<p>We establish theoretical guarantees for the convergence of self-consistency estimates to true math-ematical correctness under appropriate sampling conditions:</p>
<p>Theorem 1 (Error Bound for Self-Consistency).</p>
<p>Let τ be the error rate of the base model in generating mathematically correct statements, and let k be the number of independent samples.The probability that the majority self-consistency estimate disagrees with the true correctness is bounded by:
P(error) ≤ e −k(1−2τ ) 2 /2 (7)
when τ &lt; 0.5.</p>
<p>Proof.Let X i be a Bernoulli random variable indicating whether the i-th sample is mathematically correct, with P(X i = 1) = 1 − τ and
P(X i = 0) = τ . The majority estimate is correct if k i=1 X i &gt; k/2
. By Hoeffding's inequality:
P( X−(1−τ ) ≤ −(1/2−τ )) ≤ e −2k(1/2−τ ) 2 (8)
where X = 1 k k i=1 X i .When τ &lt; 0.5, we have 1/2 − τ &gt; 0, and X &lt; 1/2 corresponds to an incorrect majority estimate.Simplifying:
P(error) ≤ e −2k(1/2−τ ) 2 = e −k(1−2τ ) 2 /2 (9)
This bound indicates exponential decay in error probability with increased samples, providing formal justification for our approach.For typical LLMs with mathematical error rates around 20-30%, even modest sample sizes (k=5-10) yield reliable self-consistency estimates with error probabilities below 0.01.</p>
<p>Corollary 1 (Sample Complexity).</p>
<p>To achieve an error probability of at most δ in self-consistency estimation, the required number of samples is:
k ≥ 2 ln(1/δ) (1 − 2τ ) 2(10)
This corollary provides a practical guideline for determining the minimum number of samples needed to achieve a desired reliability level.For instance, with an error rate τ = 0.3 and target error probability δ = 0.01, the required sample size is k ≥ 11.5, suggesting that 12 samples are sufficient.</p>
<p>A.2 Error Propagation Analysis</p>
<p>Mathematical reasoning often involves sequential steps where errors can propagate, amplifying overall inaccuracy.We derive bounds on cumulative error in multi-step reasoning:</p>
<p>Theorem 2 (Error Propagation Bound).In a reasoning chain with T steps and dependency graph G = (V, E), where each step has independent error probability ǫ, and the maximum in-degree of any node is d, the probability of error in the final conclusion is bounded by:
P(final error) ≤ 1 − (1 − ǫ) T • (1 − dǫ) |E|−T +1 (11)
Proof.We model the error propagation as a directed acyclic graph (DAG) where each node represents a reasoning step.A node produces an error if either: (1) it directly generates an incorrect statement with probability ǫ, or (2) at least one of its parent nodes contains an error, which propagates with probability 1 (worst-case assumption).For a node with d i parents, the probability of error is therefore:
P(error at node i) ≤ ǫ + (1 − ǫ)(1 − (1 − P(parent error)) d i )(12
) For the base case (source nodes with no parents), P(error) = ǫ.By induction and using the inequality (1 − (1 − p) d ) ≤ dp for p ∈ [0, 1], we can show that for a node at depth h in the DAG:
P(error at depth h) ≤ 1−(1−ǫ) h •(1−dǫ) e h (13)
where e h is the number of edges leading to nodes at depth ≤ h.Setting h to the maximum depth and noting that the final node is at depth ≤ T , we obtain the desired bound.This analysis reveals that error probability grows with both the length of the reasoning chain (T ) and the complexity of the dependency structure (d, |E|).Our structured self-consistency framework mitigates this by verifying individual steps (reducing ǫ) and enforcing logical coherence across the graph structure (eliminating inconsistent dependencies).</p>
<p>The practical implications of Theorem 2 are illustrated in Figure 8, which shows how error probability accumulates for different graph structures and step-wise error rates.The analysis demonstrates that self-consistency verification at intermediate steps provides exponential reductions in cumulative error compared to final-answer verification alone.</p>
<p>Corollary 2 (Benefit of Intermediate Verification).Given a reasoning chain with T steps, reducing the per-step error rate from ǫ to ǫ ′ &lt; ǫ through intermediate verification decreases the final error probability by a factor of at least:
P(final error with ǫ) P(final error with ǫ ′ ) ≥ 1 − ǫ ′ 1 − ǫ T (14)
This result quantifies the exponential benefit of intermediate verification in multi-step reasoning processes, providing theoretical justification for our hierarchical self-consistency approach.</p>
<p>A.3 Information-Theoretic Analysis</p>
<p>From an information-theoretic perspective, selfconsistency can be understood as reducing the entropy of the solution space.We formalize this intuition in the following theorem: Proof.By the definition of conditional entropy and mutual information:
H(Y |X, C) = H(Y |X) − I(Y ; C|X) (16) = H(Y |X) − (H(Y |X) − H(Y |X, C)) (17) Since mutual information is non-negative (I(Y ; C|X) ≥ 0), we have H(Y |X, C) ≤ H(Y |X).
This theorem establishes that self-consistency constraints reduce uncertainty in the response space, with the reduction quantified by the mutual information between responses and consistency constraints.The more informative the consistency metric, the greater the entropy reduction and hence the more concentrated the probability mass around correct responses.</p>
<p>For hierarchical self-consistency, we can derive a stronger result:</p>
<p>Corollary 3 (Hierarchical Entropy Reduction).Given a sequence of increasingly stringent consistency constraints C 1 , C 2 , ..., C n , the entropy reduction is cumulative:
H(Y |X, C 1 , ..., C n ) ≤ H(Y |X) − n i=1 I(Y ; C i |X, C 1 , ..., C i−1 )(18)
This result demonstrates the additive benefit of hierarchical verification, where each level of consistency checking further reduces uncertainty in the response space.</p>
<p>B Theoretical Foundation B.1 Formalization of Self-Consistency</p>
<p>We begin by formalizing the concept of selfconsistency within a probabilistic framework, establishing theoretical guarantees for its application to mathematical reasoning tasks.</p>
<p>Definition 1 (Mathematical Reasoning Task).A mathematical reasoning task is defined as a tuple T = (x, D, C), where x represents a query, D is a domain of discourse (e.g., real numbers, logical propositions), and C is a correctness criterion that evaluates the validity of responses within D.</p>
<p>Let X represent the space of mathematical queries, and Y the space of possible responses.Given a query x ∈ X , an LLM defines a conditional probability distribution P θ (y|x) over possible responses y ∈ Y, where θ represents the model parameters.The self-consistency framework samples k independent responses Y x = {y 1 , y 2 , ..., y k } from P θ (y|x) and evaluates their agreement.</p>
<p>For a mathematical statement s within a response y, we define the factuality score f (s) as:
f (s) = E y∼P θ (y|x) [I(s ∈ y ∧ correct(s, y))] (19)
where I(•) is the indicator function, and correct(s, y) evaluates whether statement s is mathematically correct within response y.Since ground truth correctness is not directly accessible during inference, we approximate f (s) using the self-consistency score:
f (s) = 1 k k i=1 I(s ∈ y i )(20)
To establish the theoretical reliability of this approximation, we provide the following convergence guarantee:</p>
<p>Theorem 4 (Self-Consistency Convergence).Let s be a mathematical statement and Y x = {y 1 , y 2 , ..., y k } be independently sampled responses.If the probability of generating an incorrect statement is bounded by ǫ &lt; 0.5, then:
P f (s) − f (s) &gt; δ ≤ 2 exp −2kδ 2 (21)
for any δ &gt; 0, where k is the number of samples.</p>
<p>This theorem establishes that the selfconsistency estimate converges exponentially to the true factuality score as the number of samples increases, providing a theoretical foundation for using SC as a proxy for mathematical correctness.</p>
<p>For statements with binary correctness (either true or false), we can establish a stronger result:</p>
<p>Corollary 4 (Binary Correctness).For a mathematical statement s with binary correctness (correct or incorrect), if the probability of the model generating s correctly is p c &gt; 0.5 and incorrectly is p i &lt; 0.5, then the probability that the majority vote among k samples disagrees with the true correctness decreases exponentially with k:
P(majority vote is incorrect) ≤ exp − k(p c − p i ) 2 2 (22)
These results establish the theoretical foundation for using self-consistency as a verification mechanism in mathematical reasoning tasks.</p>
<p>B.2 Mathematical Reasoning as Directed Acyclic Graphs</p>
<p>Mathematical reasoning typically proceeds through a sequence of logically connected steps, forming dependencies that can be modeled as a directed acyclic graph (DAG).</p>
<p>Definition 2 (Reasoning Graph).A reasoning graph for a mathematical task is a DAG G = (V, E), where each vertex v ∈ V represents a statement in the reasoning process, and each edge (u, v) ∈ E represents a logical dependency indicating that statement v depends on statement u.</p>
<p>For a mathematical query x, a response y induces a reasoning graph G y = (V y , E y ).Given multiple sampled responses Y x = {y 1 , y 2 , ..., y k }, we obtain a set of reasoning graphs G x = {G y 1 , G y 2 , ..., G y k }.The structural consistency of these graphs provides a measure of the reliability of the reasoning process.</p>
<p>To quantify structural similarity between reasoning graphs, we employ the concept of maximum common subgraph (MCS):</p>
<p>Definition 3 (Structural Isomorphism).The structural isomorphism between two reasoning graphs G y i and G y j is defined as:
Iso(G y i , G y j ) = |M CS(G y i , G y j )| |V y i ∪ V y j | (23)
where |M CS(G y i , G y j )| is the size of the maximum common subgraph between G y i and G y j , and |V y i ∪ V y j | is the total number of unique vertices in both graphs.</p>
<p>The average structural consistency across all pairs of reasoning graphs is given by:
Ψ(G x ) = 2 k(k − 1) k i=1 k j=i+1
Iso(G y i , G y j ) (24) This metric quantifies the agreement in reasoning structure across multiple sampled responses, providing a more robust measure of selfconsistency than simple answer agreement.</p>
<p>Proposition 1 (Structural Consistency vs. Correctness).Under mild conditions on the probability distribution P θ (y|x), higher structural consistency Ψ(G x ) correlates positively with the correctness of the reasoning process:
E[correctness|Ψ(G x ) = ψ] is monotonically increasing in ψ (25)
This proposition formalizes the intuition that consistent reasoning structures are more likely to be correct, establishing the theoretical basis for using structural consistency as a verification mechanism.</p>
<p>B.3 Information-Theoretic Perspective</p>
<p>From an information-theoretic perspective, selfconsistency can be understood as minimizing the entropy of the response distribution.This perspective provides additional theoretical justification for using consistency as a proxy for correctness.</p>
<p>Definition 4 (Response Entropy).Given sampled responses Y x = {y 1 , y 2 , ..., y k }, the empirical entropy of the response distribution is: H( P (y|x)) = − y∈Yx P (y|x) log P (y|x) (26) where P (y|x) is the empirical probability of response y in the sample.</p>
<p>Lower entropy indicates higher agreement among sampled responses, suggesting more reliable mathematical reasoning.This leads to our entropy-based consistency score:
Φ(Y x ) = 1 − H( P (y|x)) log(k)(27)
where log(k) is the maximum possible entropy for k distinct responses.The score Φ(Y x ) ranges from 0 (complete disagreement) to 1 (perfect agreement).</p>
<p>Combining structural and entropy-based measures, we define our comprehensive selfconsistency score:
Λ(Y x ) = α • Ψ(G x ) + (1 − α) • Φ(Y x ) (28)
where α ∈ [0, 1] is a weighting parameter that balances structural and distributional consistency.</p>
<p>Lemma 1 (Entropy-Correctness Relationship).For any set of sampled responses Y x with corresponding reasoning graphs G x , the comprehensive consistency score Λ(Y x ) achieves maximum value if and only if all responses are identical, and decreases monotonically with increasing response variance.</p>
<p>Proof.The entropy component H( P (y|x)) is minimized (and thus Φ(Y x ) is maximized) when all responses are identical, giving a single outcome with probability 1.As the distribution becomes more uniform, entropy increases and Φ(Y x ) decreases.Similarly, Ψ(G x ) is maximized when all graphs are isomorphic (identical) and decreases as structural differences increase.Since Λ(Y x ) is a convex combination of these two monotonic functions, it inherits their properties, achieving its maximum when both components are maximized (all responses identical) and decreasing monotonically as response variance increases.This lemma establishes the theoretical foundation for using Λ(Y x ) as an optimization objective in self-consistency verification.</p>
<p>Theorem 3 (
3
Entropy Reduction).Let H(Y |X)be the conditional entropy of response space Y given query X, and let H(Y |X, C) be the conditional entropy given both the query and a selfconsistency constraint C. Then:H(Y |X, C) ≤ H(Y |X) − I(Y ; C|X) (15)where I(Y ; C|X) is the conditional mutual information between responses and consistency constraints.</p>
<p>Table 1 :
1
Performance on Theorem Proving Tasks (Mean ± Std)
MethodProof Validity (%) Logical Flow Proof StepProofConsistencyPrecisionCompletenessSS MV CoT ToT EV SSC (Ours)62.4 ± 3.1 67.8 ± 2.8 70.2 ± 2.5 72.6 ± 2.3 74.9 ± 2.1 79.1 ± 1.80.68 ± 0.04 0.71 ± 0.03 0.72 ± 0.03 0.75 ± 0.02 0.74 ± 0.03 0.77 ± 0.02 0.76 ± 0.02 0.79 ± 0.02 0.79 ± 0.02 0.81 ± 0.02 0.83 ± 0.02 0.85 ± 0.010.65 ± 0.04 0.70 ± 0.03 0.72 ± 0.03 0.75 ± 0.02 0.77 ± 0.02 0.81 ± 0.02</p>
<p>Table 2 :
2
Performance on Symbolic Reasoning Tasks (Mean ± Std)
MethodExpression Equivalence (%) Algebraic Simplification Transformation StepScoreAccuracySS MV CoT ToT EV SSC (Ours)65.7 ± 3.3 70.3 ± 2.9 72.8 ± 2.6 74.2 ± 2.4 76.1 ± 2.2 81.2 ± 1.90.70 ± 0.04 0.74 ± 0.03 0.76 ± 0.03 0.78 ± 0.02 0.80 ± 0.02 0.85 ± 0.020.72 ± 0.03 0.76 ± 0.03 0.78 ± 0.02 0.80 ± 0.02 0.82 ± 0.02 0.87 ± 0.01</p>
<p>Table 3 :
3
Performance on Numerical Computation Tasks (Mean ± Std)
MethodNumerical Accuracy (%) Variance Reduction (%) Threshold Consistency (%)SS MV CoT ToT EV SSC (Ours)68.3 ± 3.5 74.6 ± 3.0 76.9 ± 2.7 78.5 ± 2.5 80.2 ± 2.3 84.7 ± 1.9-28.4 ± 4.2 32.6 ± 3.8 35.1 ± 3.5 38.7 ± 3.2 42.8 ± 2.873.2 ± 3.1 79.5 ± 2.7 81.8 ± 2.5 83.6 ± 2.3 85.3 ± 2.1 89.1 ± 1.8</p>
<p>Table 4 :
4
Numerical Accuracy by Problem Type (SS vs. SSC)
Problem TypeSS Accuracy (%) SSC Accuracy (%)Basic Arithmetic82.590.3 (+7.8)Matrix Operations64.781.2 (+16.5)Calculus63.179.8 (+16.7)Optimization58.982.3 (+23.4)</p>
<p>Table 5 :
5
Computational Efficiency Metrics
MethodSample Efficiency ComputationalMemory(avg. samples)Overhead (×) Footprint (GB)SS1.01.0×2.3MV10.010.0×4.6CoT5.06.2×3.8ToT8.412.6×5.2EV3.24.8×3.5SSC (Ours)4.35.4×3.7</p>
<p>Table 6 :
6
Average Sample Count by Difficulty Level
MethodAvg. Samples by DifficultyEasy MediumHardMV (Fixed)10.010.010.0SSC (Adaptive)3.24.16.7</p>
<p>Table 7 :
7
Spearman's Correlation with Human Evaluation
MethodMathematicalLogicalClarity OverallCorrectnessCoherenceSS0.620.580.510.57MV0.700.650.600.65CoT0.730.680.640.68ToT0.750.710.670.71EV0.790.740.700.74SSC (Ours)0.870.820.760.82</p>
<p>Table 8 :
8
Ablation Study: Correlation with Human Judgment
VariantMathematicalLogicalClarity OverallCorrectnessCoherenceAtomic Only0.730.650.620.68Logical Only0.760.780.610.72Global Only0.790.760.680.75Atomic + Logical0.820.800.710.78Atomic + Global0.840.780.730.79Logical + Global0.830.810.720.80Full SSC0.870.820.760.82human judgment. This suggests that different ver-ification levels capture complementary aspects ofmathematical quality that collectively align betterwith holistic human evaluation.</p>
<p>Critique-out-loud reward models. Zachary Ankner, Mansheej Paul, Brandon Cui, Jonathan D Chang, Prithviraj Ammanabrolu, arXiv:2408.117912024Preprint</p>
<p>The internal state of an llm knows when it's lying. Amos Azaria, Tom Mitchell, The 2023 Conference on Empirical Methods in Natural Language Processing. 2023</p>
<p>Training a helpful and harmless assistant with reinforcement learning from human feedback. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova Dassarma, arXiv:2204.058622022arXiv preprintDawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, and 1 others</p>
<p>Large language monkeys: Scaling inference compute with repeated sampling. Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V Le, Christopher Ré, Azalia Mirhoseini, arXiv:2407.217872024Preprint</p>
<p>Discovering latent knowledge in language models without supervision. Collin Burns, Haotian Ye, Dan Klein, Jacob Steinhardt, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Universal self-consistency for large language model generation. Xinyun Chen, Renat Aksitov, Uri Alon, Jie Ren, Kefan Xiao, Pengcheng Yin, Sushant Prakash, Charles Sutton, Xuezhi Wang, Denny Zhou, arXiv:2311.173112023arXiv preprint</p>
<p>Yung-Sung Chuang, Linlu Qiu, Cheng-Yu Hsieh, Ranjay Krishna, Yoon Kim, James Glass, arXiv:2407.07071Lookback lens: Detecting and mitigating contextual hallucinations in large language models using only attention maps. 2024aarXiv preprint</p>
<p>DoLa: Decoding by contrasting layers improves factuality in large language models. Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James R Glass, Pengcheng He, The International Conference on Learning Representations. 2024b</p>
<p>Calibration of pre-trained transformers. Shrey Desai, Greg Durrett, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Association for Computational Linguistics2020</p>
<p>Shifting attention to relevance: Towards the predictive uncertainty quantification of free-form large language models. Jinhao Duan, Hao Cheng, Shiqi Wang, Alex Zavalny, Chenan Wang, Renjing Xu, Bhavya Kailkhura, Kaidi Xu, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. the 62nd Annual Meeting of the Association for Computational LinguisticsPapersAssociation for Computational Linguistics20241</p>
<p>Halo: Estimation and reduction of hallucinations in opensource weak large language models. Mohamed Elaraby, Mengyin Lu, Jacob Dunn, Xueying Zhang, Yu Wang, Shizhu Liu, Pingchuan Tian, Yuping Wang, Yuxuan Wang, arXiv:2308.117642023arXiv preprint</p>
<p>Detecting hallucinations in large language models using semantic entropy. Sebastian Farquhar, Jannik Kossen, Lorenz Kuhn, Yarin Gal, Nature. 63080172024</p>
<p>Mapping the neurosymbolic ai landscape by architectures: A handbook on augmenting deep learning through symbolic reasoning. Jonathan Feldstein, Paulius Dilkas, Belle Vaishak, Efthymia Tsamoura, arXiv:2410.220772024arXiv preprint</p>
<p>Uncertainty-aware machine translation evaluation. Taisiya Glushkova, Chrysoula Zerva, Ricardo Rei, F T André, Martins, Findings of the Association for Computational Linguistics: EMNLP 2021. Association for Computational Linguistics2021</p>
<p>Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, Maosong Sun, arXiv:2402.140082024Preprint</p>
<p>Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, arXiv:2311.05232Xiaocheng Feng, Bing Qin, and 1 others. 2023. A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. arXiv preprint</p>
<p>Armando Solar-Lezama, Koushik Sen, and Ion Stoica. 2024a. Livecodebench: Holistic and contamination free evaluation of large language models for code. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, arXiv:2403.07974Preprint</p>
<p>LiveCodeBench: A holistic and contamination-free evaluation of code LLMs. Sanjay Jain, Shiven Khurana, Albert Wan, Haitham El-Refai, Jesse Maddison, Kathleen Fisher, Luke Zettlemoyer, Tao Xie, Steven Bird, International Conference on Machine Learning. 2024bRalf Adams, and 1 others</p>
<p>How can we know when language models know? On the calibration of language models for question answering. Zhengbao Jiang, Jun Araki, Haibo Ding, Graham Neubig, Transactions of the Association for Computational Linguistics. 92021</p>
<p>Semantic entropy probes: Robust and cheap hallucination detection in llms. Jannik Kossen, Jiatong Han, Muhammed Razzak, Lisa Schut, Shreshth Malik, Yarin Gal, arXiv:2406.159272024arXiv preprint</p>
<p>Platypus: Quick, cheap, and powerful refinement of llms. Ariel Lee, Cole Hunter, Nataniel Ruiz, NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following. 2023</p>
<p>Inferencetime intervention: Eliciting truthful answers from a language model. Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, Martin Wattenberg, Advances in Neural Information Processing Systems. 202436</p>
<p>Agustin Dal Lago, and 1 others. 2022. Competitionlevel code generation with alphacode. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Science. 3786624</p>
<p>Let's verify step by step. Hendrik Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Chen, Wen Hao Toh, Jeff Wu, Kimin Lee, Timothy Lin, Meng Fang, Chelsea Huang, John Langford, Liam Sutawika, Advances in Neural Information Processing Systems. Jonathan Grégoire, Jason WestonJan Leike, and Franc ¸ois Charton. 2023a</p>
<p>Vineet Hunter Lightman, Yura Kosaraju, Harri Burda, John Edwards ; Leike, Schulman, arXiv:2305.20050Ilya Sutskever, and Karl Cobbe. 2023b. Let's verify step by step. Bowen Baker, Teddy LeeJanPreprint</p>
<p>SelfCheckGPT: Zero-resource blackbox hallucination detection for generative large language models. Potsawee Manakul, Adian Liusie, Adian Dunbar, J F Mark, Shangmin Zhang, Yung-Sung Gong, Dan Chuang, Yulan Jurafsky, Hinrich Ren, Poon, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023a</p>
<p>SelfCheckGPT: Zero-resource black-box hallucination detection for generative large language models. Potsawee Manakul, Adian Liusie, Mark Gales, Proceedings of the Conference on Empirical Methods in Natural Language Processing. the Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational Linguistics2023b</p>
<p>Self-contradictory hallucinations of large language models: Evaluation, detection and mitigation. Niels Mündler, Jingxuan He, Slobodan Jenko, Martin Vechev, The International Conference on Learning Representations. 2024</p>
<p>Selfcontradict: Large language models can be unreliable fact-checkers. Reggie Niels, Edward J Hu, Tianle Shen, Neel Shah, Kevin J Lin, Michael Chang, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. the 62nd Annual Meeting of the Association for Computational Linguistics2024</p>
<p>Alex Ray, and 1 others. 2022. Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Advances in neural information processing systems. </p>
<p>Natural language to code translation with execution. Freda Shi, Daniel Fried, Marjan Ghazvininejad, Luke Zettlemoyer, Sida I Wang, Proceedings of the Conference on Empirical Methods in Natural Language Processing. the Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational Linguistics2022</p>
<p>Adi Simhi, Jonathan Herzig, Idan Szpektor, Yonatan Belinkov, arXiv:2404.09971Constructing benchmarks and interventions for combating hallucinations in llms. 2024arXiv preprint</p>
<p>Atomic self-consistency for better long form generations. Raghuveer Thirukovalluru, Yukun Huang, Bhuwan Dhingra, Proceedings of the Conference on Empirical Methods in Natural Language Processing. the Conference on Empirical Methods in Natural Language Processing2024</p>
<p>Improving LLM generations via fine-grained self-endorsement. Ante Wang, Linfeng Song, Baolin Peng, Lifeng Jin, Ye Tian, Haitao Mi, Jinsong Su, Dong Yu, Findings of the Association for Computational Linguistics: ACL. Association for Computational Linguistics2024a</p>
<p>Drt-o1: Optimized deep reasoning translation via long chain-of-thought. Jiaan Wang, Fandong Meng, Yunlong Liang, Jie Zhou, arXiv:2412.174982024bPreprint</p>
<p>Math-shepherd: Verify and reinforce llms step-by-step without human annotations. Peiyi Wang, Lei Li, Zhihong Shao, R X Xu, Damai Dai, Yifei Li, Deli Chen, Y Wu, Zhifang Sui, arXiv:2312.089352024cPreprint</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, The International Conference on Learning Representations. 2023</p>
<p>HelpSteer 2.0: An open-source dataset for training high-quality assistants. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, arXivpreprintarXiv:2404.051802024d</p>
<p>Inference scaling laws: An empirical analysis of compute-optimal inference for problem-solving with language models. Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, Yiming Yang, arXiv:2408.007242024Preprint</p>
<p>Deepseek-prover: Advancing theorem proving in llms through large-scale synthetic data. Huajian Xin, Daya Guo, Zhihong Shao, Zhizhou Ren, Qihao Zhu, Bo Liu, Chong Ruan, Wenda Li, Xiaodan Liang, arXiv:2405.143332024aPreprint</p>
<p>DeepSeek-Prover: Advancing theorem proving with language models. Yingqiang Xin, Honghua Dong, Hongpeng Fu, Zengzhuang Wen, Hanlin Ma, Haiyan Chang, Qingxing Mo, Qun Chen, Yue Li, Chenglong Fei, Chenguang Wang, Hinrich Poon, Zheng Wang, Zhihong Xu, Zhilin Sun, Keqiang He, Xuanyu Wang, Li Zhang, Wanjun Zhong, International Conference on Learning Representations. 2024band 29 others</p>
<p>Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms. Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, Bryan Hooi, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Do large language models know what they don't know?. Zhangyue Yin, Qiushi Sun, Qipeng Guo, Jiawen Wu, Xipeng Qiu, Xuan-Jing Huang, Findings of the Association for Computational Linguistics: ACL 2023. 2023</p>
<p>Truthx: Alleviating hallucinations by editing large language models in truthful space. Shaolei Zhang, Tian Yu, Yang Feng, arXiv:2402.178112024arXiv preprint</p>
<p>Lima: Less is more for alignment. Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Advances in Neural Information Processing Systems. and 1 others. 2024</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3994 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3994</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3994</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-93.html">extraction-schema-93</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including agreement rates, qualitative differences, limitations, failure cases, and any aspects that are lost or degraded when using LLMs as judges instead of humans.</div>
                <p><strong>Paper ID:</strong> paper-d348af8d0bcf03207f1a2ca1fda45ba03f6ef05d</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/d348af8d0bcf03207f1a2ca1fda45ba03f6ef05d" target="_blank">The PRISM Alignment Project: What Participatory, Representative and Individualised Human Feedback Reveals About the Subjective and Multicultural Alignment of Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3994.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3994.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including agreement rates, qualitative differences, limitations, failure cases, and any aspects that are lost or degraded when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-a-judge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Language Model used as an automated judge/evaluator of model outputs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The practice of using LLMs to simulate human raters or to automatically judge the quality/alignment of model responses, referenced as an emerging alternative to costly human evaluation and as a target for calibration against diverse human feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_setting</strong></td>
                            <td>Reference to protocols that use LLMs as automated judges to evaluate chatbot/LLM responses (LLM-as-a-judge evaluation protocols).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_domain</strong></td>
                            <td>Dialogue / chatbot response evaluation and alignment (human preference feedback / RLHF evaluation settings).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_differences</strong></td>
                            <td>Paper notes that relying on LLM-as-a-judge risks losing interpersonal and contextual human nuance: subjective, cultural and personal preferences can be relational and non-separable from the person or community, which LLM judges may not capture; cardinal rating comparability across individuals is already limited and synthetic judges could further degrade representation of plural opinions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>The paper highlights several limitations of LLM-as-a-judge usage as discussed or implied: (i) potential domain and sample shifts when simulating humans, (ii) risk of cultural homogenisation and subsuming hidden annotator contexts as universalities, (iii) synthetic judges may reproduce or amplify biases and power dynamics from training data, and (iv) reduced participation/active input from real people leading to thin or unjust participation. It also emphasizes the need to calibrate such protocols to diverse rater pools.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_strengths</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Recommend calibrating LLM-as-a-judge protocols against diverse, documented human rater pools (e.g., use PRISM to calibrate), maintain transparency about aggregations and disagreements, and avoid treating synthetic LLM judgments as a drop-in replacement for participatory human feedback in value-laden contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>The PRISM Alignment Dataset: What Participatory, Representative and Individualised Human Feedback Reveals About the Subjective and Multicultural Alignment of Large Language Models</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3994.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3994.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including agreement rates, qualitative differences, limitations, failure cases, and any aspects that are lost or degraded when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Simulated-human synthetic feedback</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Simulating humans with LLMs to generate alignment/evaluation data</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The practice of using LLMs to generate synthetic human-like preference labels or evaluations (instead of collecting real human feedback), discussed as an increasingly common but potentially problematic shortcut for creating alignment data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_setting</strong></td>
                            <td>Use of LLMs to simulate human raters and produce synthetic alignment/evaluation data for RLHF and related alignment pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_domain</strong></td>
                            <td>Alignment data generation / human preference simulation for training or evaluating LLMs (general across tasks, but particularly dialogue/assistant outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_differences</strong></td>
                            <td>Paper emphasises that simulated feedback can miss active, participatory aspects of human feedback, and is likely to lose individual-specific context (demographics, values, lived experience) that shape subjective judgments; this risks producing homogenised alignment signals that overlook cultural and interpersonal disagreement.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Documented concerns include incentives to prefer cheaper synthetic data over costly human collection, the potential for synthetic data to misrepresent real preference distributions, and the risk of reinforcing biases from model training data; the paper calls out the need for calibration against diverse human datasets to detect such failures.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_strengths</strong></td>
                            <td>The paper implicitly acknowledges the scaling advantage of simulated/synthetic feedback (lower cost, easier to produce) but does not present empirical cases where simulated judges match human evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use datasets like PRISM to calibrate and validate LLM-generated synthetic judgments against documented, diverse human raters; favour transparency and participatory collection where subjectivity and cultural variation matter; do not substitute synthetic labels for human input in high-stakes or value-laden domains without calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>The PRISM Alignment Dataset: What Participatory, Representative and Individualised Human Feedback Reveals About the Subjective and Multicultural Alignment of Large Language Models</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena <em>(Rating: 2)</em></li>
                <li>Can LLM be a Personalized Judge? <em>(Rating: 2)</em></li>
                <li>AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback <em>(Rating: 1)</em></li>
                <li>The illusion of artificial inclusion <em>(Rating: 1)</em></li>
                <li>Human Feedback is not Gold Standard <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3994",
    "paper_id": "paper-d348af8d0bcf03207f1a2ca1fda45ba03f6ef05d",
    "extraction_schema_id": "extraction-schema-93",
    "extracted_data": [
        {
            "name_short": "LLM-as-a-judge",
            "name_full": "Large Language Model used as an automated judge/evaluator of model outputs",
            "brief_description": "The practice of using LLMs to simulate human raters or to automatically judge the quality/alignment of model responses, referenced as an emerging alternative to costly human evaluation and as a target for calibration against diverse human feedback.",
            "citation_title": "",
            "mention_or_use": "mention",
            "evaluation_setting": "Reference to protocols that use LLMs as automated judges to evaluate chatbot/LLM responses (LLM-as-a-judge evaluation protocols).",
            "task_or_domain": "Dialogue / chatbot response evaluation and alignment (human preference feedback / RLHF evaluation settings).",
            "llm_model_name": null,
            "agreement_rate": null,
            "qualitative_differences": "Paper notes that relying on LLM-as-a-judge risks losing interpersonal and contextual human nuance: subjective, cultural and personal preferences can be relational and non-separable from the person or community, which LLM judges may not capture; cardinal rating comparability across individuals is already limited and synthetic judges could further degrade representation of plural opinions.",
            "limitations_or_failure_cases": "The paper highlights several limitations of LLM-as-a-judge usage as discussed or implied: (i) potential domain and sample shifts when simulating humans, (ii) risk of cultural homogenisation and subsuming hidden annotator contexts as universalities, (iii) synthetic judges may reproduce or amplify biases and power dynamics from training data, and (iv) reduced participation/active input from real people leading to thin or unjust participation. It also emphasizes the need to calibrate such protocols to diverse rater pools.",
            "counterexamples_or_strengths": null,
            "recommendations_or_best_practices": "Recommend calibrating LLM-as-a-judge protocols against diverse, documented human rater pools (e.g., use PRISM to calibrate), maintain transparency about aggregations and disagreements, and avoid treating synthetic LLM judgments as a drop-in replacement for participatory human feedback in value-laden contexts.",
            "citation": "The PRISM Alignment Dataset: What Participatory, Representative and Individualised Human Feedback Reveals About the Subjective and Multicultural Alignment of Large Language Models",
            "uuid": "e3994.0"
        },
        {
            "name_short": "Simulated-human synthetic feedback",
            "name_full": "Simulating humans with LLMs to generate alignment/evaluation data",
            "brief_description": "The practice of using LLMs to generate synthetic human-like preference labels or evaluations (instead of collecting real human feedback), discussed as an increasingly common but potentially problematic shortcut for creating alignment data.",
            "citation_title": "",
            "mention_or_use": "mention",
            "evaluation_setting": "Use of LLMs to simulate human raters and produce synthetic alignment/evaluation data for RLHF and related alignment pipelines.",
            "task_or_domain": "Alignment data generation / human preference simulation for training or evaluating LLMs (general across tasks, but particularly dialogue/assistant outputs).",
            "llm_model_name": null,
            "agreement_rate": null,
            "qualitative_differences": "Paper emphasises that simulated feedback can miss active, participatory aspects of human feedback, and is likely to lose individual-specific context (demographics, values, lived experience) that shape subjective judgments; this risks producing homogenised alignment signals that overlook cultural and interpersonal disagreement.",
            "limitations_or_failure_cases": "Documented concerns include incentives to prefer cheaper synthetic data over costly human collection, the potential for synthetic data to misrepresent real preference distributions, and the risk of reinforcing biases from model training data; the paper calls out the need for calibration against diverse human datasets to detect such failures.",
            "counterexamples_or_strengths": "The paper implicitly acknowledges the scaling advantage of simulated/synthetic feedback (lower cost, easier to produce) but does not present empirical cases where simulated judges match human evaluations.",
            "recommendations_or_best_practices": "Use datasets like PRISM to calibrate and validate LLM-generated synthetic judgments against documented, diverse human raters; favour transparency and participatory collection where subjectivity and cultural variation matter; do not substitute synthetic labels for human input in high-stakes or value-laden domains without calibration.",
            "citation": "The PRISM Alignment Dataset: What Participatory, Representative and Individualised Human Feedback Reveals About the Subjective and Multicultural Alignment of Large Language Models",
            "uuid": "e3994.1"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena",
            "rating": 2
        },
        {
            "paper_title": "Can LLM be a Personalized Judge?",
            "rating": 2
        },
        {
            "paper_title": "AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback",
            "rating": 1
        },
        {
            "paper_title": "The illusion of artificial inclusion",
            "rating": 1
        },
        {
            "paper_title": "Human Feedback is not Gold Standard",
            "rating": 1
        }
    ],
    "cost": 0.012183,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>The PRISM Alignment Dataset: What Participatory, Representative and Individualised Human Feedback Reveals About the Subjective and Multicultural Alignment of Large Language Models</h1>
<p>Hannah Rose Kirk ${ }^{1 *}$ Alexander Whitefield ${ }^{2}$ Paul Röttger ${ }^{3}$ Andrew Bean ${ }^{1}$<br>Katerina Margatina ${ }^{4}$ Juan Ciro ${ }^{5,11}$ Rafael Mosquera ${ }^{5,6}$ Max Bartolo ${ }^{7,8}$<br>Adina Williams ${ }^{9}$ He He ${ }^{10}$ Bertie Vidgen ${ }^{1,111}$ Scott A. Hale ${ }^{1,12 \dagger}$<br>${ }^{1}$ University of Oxford ${ }^{2}$ University of Pennsylvania ${ }^{3}$ Bocconi University<br>${ }^{4}$ AWS AI Labs ${ }^{5}$ ML Commons ${ }^{6}$ Factored AI ${ }^{7}$ UCL ${ }^{8}$ Cohere<br>${ }^{9}$ MetaAI ${ }^{10}$ New York University ${ }^{11}$ Contextual AI ${ }^{12}$ Meedan</p>
<h4>Abstract</h4>
<p>Human feedback is central to the alignment of Large Language Models (LLMs). However, open questions remain about methods (how), domains (where), people (who) and objectives (to what end) of feedback processes. To navigate these questions, we introduce PRISM, a dataset that maps the sociodemographics and stated preferences of 1,500 diverse participants from 75 countries, to their contextual preferences and fine-grained feedback in 8,011 live conversations with 21 LLMs. With PRISM, we contribute (i) wider geographic and demographic participation in feedback; (ii) census-representative samples for two countries (UK, US); and (iii) individualised ratings that link to detailed participant profiles, permitting personalisation and attribution of sample artefacts. We target subjective and multicultural perspectives on value-laden and controversial issues, where we expect interpersonal and cross-cultural disagreement. We use PRISM in three case studies to demonstrate the need for careful consideration of which humans provide what alignment data.</p>
<p>Data \&amp; Code: github.com/HannahKirk/prism-alignment
Data \&amp; Dataset Card: huggingface.co/datasets/HannahRoseKirk/prism-alignment</p>
<h2>1 Introduction</h2>
<p>Human feedback serves a direct role for the alignment of large language models (LLMs), defined as the steering of AI behaviour towards a set of preferences or values. This increased emphasis on human feedback raises unresolved questions: how we collect human feedback when designing methodologies that rely on ordinal or cardinal scales, broad or fine-grained desiderata, and explicit or implicit signals; where we focus human labour when selecting domains, topics or tasks to collect feedback over; who we ask for feedback when recruiting participants to voice their idiosyncratic preferences, values, or beliefs [1]; and to what end when specifying an objective to pursue personalised alignment [2-4] or to aggregate individual preferences into collective outcomes favourable for societies at large [5-9].
Despite the success of human feedback learning [10, 11], answering these questions is constrained by gaps in existing datasets, such as (i) over-reliance on binary A/B comparisons, without fine-grained ratings or explanations [12]; (ii) small or biased samples recruited from narrow crowdwork or tech communities [10, 13] (iii) limited sample information (annotator IDs or sociodemographics) [14]; and (iv) scarce documentation for how values are operationalised [15, 16]. Most datasets rely only on</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The Prism dataset. In Stage 1, 1,500 participants fill in the Survey detailing their background, familiarity with LLMs and stated preferences over behaviours (§ 2.1). Demographic and geographic breakdowns are in Tab. 5 and Tab. 8). Participants then progress to Stage 2, where they converse with LLMs on topics of their choosing, rate the responses on a cardinal scale, and give fine-grained feedback (§ 2.2). In the first turn, four models respond to the opening prompt ( $\bigcirc$ ). $\odot$, $\odot$, $\odot$ ). In subsequent turns, the conversation continues with two responses sampled from the highest-rated model at a non-deterministic temperature ( $\bigcirc$ ). $\odot$ ). There are 8,011 Conversations between participants ( $\bigcirc$ ) and LLMs ( $\odot$ ), forming 27,172 Interactions (human message with a set of model responses), and 68,371 Utterances (triples of ${$ human message, model response, score $}$ ).
revealed or contextual preferences [1], ${ }^{2}$ and much attention is devoted to technical or statistical issues in feedback learning [18-20], rather than data-centric human factors. Relying on 'generic' human data teaches behaviours which are reductionist because values are relational and non-separable from the person, community or operating context [21-23]; and non-generalisable because the indiscriminate aggregation of data subsumes hidden annotator contexts as universalities [24-28].
We introduce Prism, a new resource for navigating empirical questions of human feedback. We employ both the ask and observe principles of social science by mapping detailed survey responses of humans around the world onto their live conversations with LLMs (Fig. 1). This setup permits alignment methods relying on either contextual preference comparisons typical for RLHF [29-31], or stated preferences and principles like constitutional AI [6, 32]. In addition to pairing stated and contextual preferences, Prism has the following features. Participatory: To ensure wider active participation in alignment data [25, 33], we recruit 1,500 English-speaking crowdworkers from diverse geographies and demographics; Representative: As units for preference aggregation, we include two census-representative samples (UK, US); and Individualised: To expose hidden human context and permit personalised preferences, each rating links to a pseudonymous ID and detailed participant profile. We source Subjective and Multicultural perspectives to avoid value-monism and cultural homogenisation in the opinions that LLMs represent [34-36] and operate in the descriptive paradigm without guidelines that characterise 'good' responses [15, 16]. Opinion diversity varies along the objective-subjective spectrum (e.g. what is the capital of France? vs. is abortion wrong?), so we prime participants for values and controversy guided dialogues but also collect neutral unguided dialogues as a baseline. To our knowledge, Prism is the first human feedback dataset to target cross-cultural controversies and value-laden prompts, where interpersonal disagreement is rife. After introducing Prism (§ 2), we demonstrate its value via three case studies (§ 3): (1) Do different people initiate different discussions with LLMs? (2) Do people prefer differently aligned models, and (3) How do sampling decisions affect welfare outcomes? Prism provides many more research avenues such as engineers targeting personalised alignment [2] or consensus across opinion distributions [5, 37]; social scientists examining how exposure to LLMs affects public attitudes; or policymakers seeking democratic input on AI-citizen interactions on topics like immigration, abortion or euthanasia. Alignment cannot be neatly bifurcated into technical and normative components [38]. Prism assists in navigating these complexities with more human voices adjudicating alignment norms.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Schematic of fine-grained attribute ratings. The same attributes appear in three places in our task: A is asked once in the survey; B and C are asked per conversation. For performance attributes, we ask participants to consider only the highest-rated model in the first conversation turn; for choice attributes, we ask them to consider this highest-rated model relative to other models in the first turn.</p>
<h1>2 The Prism Alignment Dataset</h1>
<p>Prism maps the characteristics and preferences of diverse humans onto their real-time interactions with LLMs (Fig. 1). Participants complete a Survey (§ 2.1) with questions about their demographics and stated preferences, then proceed to the Conversations with LLMs (§ 2.2), where they input prompts, rate responses and give fine-grained feedback in a series of multi-turn interactions. With the two-stage setup: (i) we avoid over-generalising from a "generic human" by matching ratings to detailed participant characteristics; (ii) we track how contextual preferences (in local conversations) depart from stated preferences (in survey); and (iii) we give participants autonomy to communicate in their own words what is important and why [39, 25]. Both stages received ethics board approval and ran with informed consent (App. D). Participants were paid $£ 9 /$ hour and the task took 70 minutes on average. Data collection ran from 22nd November to 22nd December 2023. ${ }^{2}$ We provide a data statement in App. B, data clause in App. C, and full codebooks detailing each variable in App. V.</p>
<h3>2.1 The Survey</h3>
<p>Prior to starting the survey, we ensure that all participants are over 18, obtain their informed consent, give a brief primer on LLMs (or AI language models), and dissuade LLM-written responses. The survey constructs a participant profile containing five features:
LLM familiarity and usage We ask about participants' familiarity with LLMs ( $61 \%$ are somewhat familiar, $28 \%$ very familiar and $10 \%$ not familiar at all) and whether to their knowledge they have used them indirectly (in products like LinkedIn post-writing tool); or directly (via a specialised interface like ChatGPT). Individuals that have used LLMs directly or indirectly ( $84 \%$ ) are branched to questions on frequency of use ( $7 \%$ every day, $21 \%$ every week, and $20 \%$ every month) and purpose of use (the most popular tasks are research overviews selected by $49 \%$, professional work by $37 \%$, creative writing by $31 \%$ and programming help by $27 \%$ ). Full results in App. I.
Self-written system string ("constitution") System strings can guide LLM behaviours as a high-level global instruction prompts prepended to all subsequent interactions [40, 41], and have been analogised as "constitutions" or governing principles for AI [32]. Factuality, professionalism, humanness and harmlessness all emerged as key principles (App. M.1) from the following instruction:</p>
<p>Imagine you are instructing an AI language model how to behave. You can think of this like a set of core principles that the AI language model will always try to follow, no matter what task you ask it to perform. In your own words, describe what characteristics, personality traits or features you believe the AI should consistently exhibit. You can also instruct the model what behaviours or content you don't want to see. If you envision the AI behaving differently in various contexts (e.g. professional assistance vs. storytelling), please specify the general adaptations you'd like to see.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Stated preferences for LLM behaviours In contrast to this open-ended preference elicitation, we collect structured ratings on fine-grained behaviour attributes. Participants score the importance of each attribute on a visual analog scale [42] (Fig. 2). A statement like "It is important that an AI language model produces factual and informative responses" maps $(0,100)$ where the ends of scale are (Strongly disagree, Strongly agree). Numeric scores are recorded, but not shown to participants to avoid anchoring and dependency biases. We only collect responses to these statements once before participants interact with LLMs but the same attributes appear in the Conversations stage; so, we can track how stated 'abstract' preferences relate to contextual 'in-situ' preferences. ${ }^{4}$ Overall, we find clusters of subjective attributes (values, creativity and diversity) versus objective attributes (factuality, fluency and helpfulness; App. N.1). While the majority of participants agree that these more objective attributes are important (highly-skewed positive distribution, $\mu \in[86,89], \sigma \in[14,16]$ ), there is little agreement on the meta-importance of subjective attributes (App. N.2). In fact, responses for whether value alignment itself is important follow an almost normal distribution ( $\mu=54, \sigma=26$ ).</p>
<p>Self-written description Values and preferences are subjective and personal. We ascribe participants autonomy to communicate salient aspects of their identity in a short profile, beyond essentialising associations with structured demographics alone. Honesty, hard work and empathy emerged as common values (App. M.2) from the following instruction:</p>
<p>Please briefly describe your values, core beliefs, guiding principles in life, or other things that are important to you. For example, you might include values you'd want to teach to your children or qualities you look for in friends. There are no right or wrong answers. Please do not provide any personally identifiable details like your name, address or email.</p>
<p>Basic demographics We ask standard demographics: age, gender, employment status, martial status, educational attainment, ethnicity, religious affiliation, English proficiency, country of birth, and country of residence. There is always a "Prefer not to say" option. For gender, participants can select Male, Female, Non-Binary, or self-describe. We collect self-described ethnicity and religion because no pre-set groups exhaust how individuals may self-identify across cultures and global regions. We provide a manual annotation of these strings into aggregated categorisations for statistical analysis (App. F). Because of how we recruit participants (§ 2.3), our sample covers diverse demographics (App. G) and geographies (App. H), with representation from people born in 75 countries. However, the sample still skews White, Western and educated, and only contains English-language speakers.</p>
<h1>2.2 The Conversations</h1>
<p>After completing the survey, participants move to the second stage, consisting of real-time conversations with LLMs via a custom-built interface on the Dynabench platform [43, 44].
Selecting conversation type We prime participants to diversify their prompts along the objectivesubjective spectrum by asking them to complete two conversations across three conditions or conversation types (six in total). ${ }^{5}$ They select the type before inputting their opening prompt:</p>
<div class="codehilite"><pre><span></span><code>$\bigcirc$ Unguided. Ask, request or talk to the model about anything. It is up to you!
\(\bigcirc\) Values guided. Ask, request or talk to the model about something important to you or that represents your values. This
could be related to work, religion, family and relationship, politics or culture.
\(\bigcirc\) Controversy guided. Ask, request or talk to the model about something controversial or where people would disagree in
your community, culture or country.
</code></pre></div>

<p>Opening the conversation Participants construct a free-text prompt of their choosing and receive up to four responses from different LLMs. ${ }^{6}$ The participants then rate each response on a visual analogue scale (VAS) [42, 45] from "Terrible" to "Perfect". We record the slider position as a score from 1-100 but do not show participants the number to avoid anchoring or conditional dependence of scores across conversations. We opt for this cardinal feedback for three reasons: (i) it encourages subjectivity; (ii) it permits studying the relative merit of cardinality versus ordinality for reward</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>modelling because ratings can be converted to rankings but not vice versa; (iii) it allows expression of preference intensity above and beyond chosen:rejected pairs. ${ }^{7}$ However, we acknowledge that the cardinal scale introduces some intrapersonal measurement noise from a more cognitively demanding task and carries less interpersonal comparability than ordinal preferences, see Limitations (§ 5).
Continuing the conversation The highest-scoring LLM from the opening turn is locked into subsequent turns, with random tie-breaks in the case of identical scores. Participants must continue the conversation for at least another turn, but are asked to vary their conversations between 2 and 10 turns to avoid introducing a dataset artefact. We encourage some variation in conversation length $\left(\mu_{T}=3.4, \sigma_{T}=1.6\right)$ but there is a strong drop off after the second turn (App. O). Participants then rate two responses on a VAS like before, but both are now sampled from the selected model with a non-deterministic temperature. These within-model responses are more similar in style and content than across-model responses (in the first turn), and score deviations are narrower (App. O).
Collecting fine-grained feedback After the conversation ends, participants first rate statements about the performance of their highest-rated model like "The response was well-written" on a VAS from Performed very poorly to Performed very well, or select N/A if the statement is irrelevant for the context. We then ask participants to consider why they chose this model, rating statements like "I chose this response because it was well-written" on a VAS from Very unimportant to Very important (or select N/A). Attributes are shared with the Survey (Fig. 2). We find strong correlations between performance attributes and choice attributes (except safety) but weak correlations of these pairs to stated preferences given in the Survey, perhaps due to conversational, model or task-design confounders (App. N.1). In general, the distribution of scores over performance and choice attributes is narrower and more positively skewed (bunched to 100) compared to stated preferences (App. N.2). Finally, we collect open-ended natural language feedback on the whole conversation. Participants contributed both content and stylistic feedback ( $\mu=29$ words, $\sigma=19$, App. M.3).</p>
<p>Give some feedback on the conversation as whole. Hypothetically, what would an ideal interaction for you look like here? What was good and what was bad? What (if anything) was missing? What would you change to make it better?</p>
<h1>2.3 The Sample</h1>
<p>Our sampling aims were depth in the demographics represented within countries and breadth across global regions. We recruit English-speaking participants from Prolific in two distinct paths:
Census-representative sample (UK, US) Samples matched to simplified census data (age, ethnicity, gender) were only available for the UK and US. The minimum pool size for a statistical guarantee of representativeness was 300, which set a lower bound for participant quota. After collecting data, we observed some skew in our 'representative' samples between observed and expected distributions in recent census data, which we partially correct for (App. L). These samples permit future studies on more representative populations that can be replicated across two countries; however their inclusion biases PRISM as a whole towards two Western nations already over-represented in AI research.
Balanced samples (rest of world) The distribution of Prolific workers outside the US and the UK skews strongly to Europe and Northern America, and some countries dominate continental counts (App. J). To avoid more active workforces biasing the sample, we set up 33 country-specific studies where there is $&gt;1$ eligible worker, and allocate sample quotas so that each global region is similarly represented. ${ }^{8}$ We balance each national sample by gender where possible (Tab. 10).
Included models The rapidly evolving landscape necessitates a model-agnostic approach to avoid data staleness. We include 21 different LLMs ( 9 open-access, 12 commercial-API) from various model families and parameter sizes, which diversifies the training data, capabilities, and degree of existing safeguards or alignment biases. To avoid text length confounding preferences [46] and to reduce participant fatigue, we include system prompts instructing models to limit their responses to $\leq 50$ words. We show the full list of models, decoding parameters and generation details in App. P.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Topic prevalence by conversation types and participant identity. We show total prompts clustered into topics (bars), and total members in each group (top panels). Per group and topic, we plot the overrepresentation factor of observed vs. expected group proportions and show significant regression coefficients (base category indicated by $\dagger$ ). All coefficients are in Fig. 23, topic-group counts in Fig. 27 and centroid prompts in Tab. 22. Location is by birth region (with UK and US split out), but most regions have few countries (App. H). Key results (§ 3.1): Priming participants to select a conversation type (unguided, values or controversy guided) significantly influenced diversity of prompts. Identity factors have some significant interactions with prompt choice but each topic contains prompts authored by intersectionally-diverse participants.</p>
<h1>3 Experiments with PRISM</h1>
<h3>3.1 Case Study I: Do Different People Initiate Different Discussions with LLMs?</h3>
<p>Methods We use a pre-trained sentence transformer (all-mpnet-base-v2) to embed each opening prompt in 768-D, then apply UMAP to reduce to 20-D, before clustering with HDBScan [47]. 70\% of prompts are assigned to 22 topic clusters and $30 \%$ remain as outliers. We name each cluster by prompting gpt-4-turbo with the top n-grams extracted with TF-IDF and closest texts to the cluster centroid. We define an over-representation factor as $\frac{N_{g, i} / N_{t}}{b_{g}}$, to compute observed versus expected topic prevalence per identity group. For the partial contribution of identity attributes, we estimate an OLS regression for each topic $y^{t}(t \in 1 \ldots 22)$ and cluster standard errors at the individual level: $y_{i, c}^{t}=\alpha^{t}+\operatorname{gender}<em 1="1">{i}^{t} \beta</em>}^{t}+\operatorname{age<em 2="2">{i}^{t} \beta</em>}^{t}+\operatorname{birth} \operatorname{region<em 3="3">{i}^{t} \beta</em>}^{t}+\operatorname{ethnicity<em 4="4">{i}^{t} \beta</em>}^{t}+\operatorname{religion<em 5="5">{i}^{t} \beta</em>}^{t}+\operatorname{prompt<em 6="6">{i}^{t} \beta</em>\right}}^{t}+\varepsilon_{i, c}$, where $y_{i, c}^{t}=1$ if the prompt of participant $i$ in conversation $c$ is categorised as topic $t$. The identity vectors (e.g. gender) represent sets of variables, with a base category removed (indicated in Fig. 3). The coefficients of interest are contained in vectors $\left{\beta_{d}^{t<em d="d">{d=1}^{8}$, where component $g$ of $\beta</em>$ is interpreted as the increase in probability of a participant choosing topic $t$ if they are in the group indexed by $g$ (e.g. Female) compared to the base group (e.g. Male). See App. R for extended methods.}^{t</p>
<p>Results Our instructions had a significant priming effect, resulting in a high density of controversial and value-laden topics (Fig. 3). Topics significantly correlated with controversy guidance are Gender \&amp; LGBTQ+ Identity, Israel-Palestine Conflict, and Discussions on Abortion, while topics significantly correlated with the values guidance are Managing Relationships, Job Search, and Religion \&amp; Spirituality. In contrast, the 'unguided' condition correlates with task-oriented and</p>
<p>neutral topics like Popular Culture, Recipes \&amp; Cooking and Travel Recommendations. Only Climate Change is not significantly correlated to conversation type. Controlling for conversation type, $11 \%$ of coefficients are significant ( $\alpha=99 \%$ ); so, identity factors have some predictive power on topic prevalence. Significant relationships include: women and non-binary people discuss gender and LGBTQ+ issues more than men; older people discuss elections and travel more than younger people; Black participants discuss climate change less than White participants, and all regions question LLMs about abortion less often than US participants. When we examine granular regions in embedding space using a single-link hierarchical clustering algorithm (App. S), local prompt neighbourhoods tend to be intersectionally-diverse: $84 \%$ of them meet or exceed entropy across intersectional demographics that would be expected under random sampling. During this local exploration, we retrieve regions of semantically-identical prompts rated by multiple diverse individuals (e.g. one neighbourhood "Does God exist?" has 7 religious and 7 irreligious participants), finding that interpersonal differences in contextual preferences persist even when dialogue context is fixed (App. S.4). So, despite Prism containing semantically-diverse prompts, people from different backgrounds occupy common discussion spaces, providing an anchor to examine diverse perspectives to shared issues.</p>
<h1>3.2 Case Study II: Do Different People Prefer Differently-Aligned Models?</h1>
<p>Methods Observed preference differences at the model-level are confounded by interactions of topic prevalence and model aptitude (e.g. men ask more about aliens and gpt-4 is poor on extraterrestrial knowledge). Evidence of shared dialogue spaces (§ 3.1) and group-topic score differences (App. T.2) mitigate some concern, but to further control for context, we use opening prompts from the balanced subset of participants $(\mathrm{n}=1,246)$ with equal conversations per type $(\mathrm{n}=6,669)$. The mean participant rates $14 / 21$ LLMs but unseen ratings are missing at random. Our aggregation (social choice) function over participant ratings is derived from Pairwise Rank Centrality $(\mathcal{P})$ [48] and Convergence Voting [49], both inspired by PageRank [50]. Each model is a node in a graph and transition probabilities between nodes are calculated by the proportion of pairwise battle wins. This process simulates a random walk on a Markov chain, leading to a stationary distribution of scores that reflect the collective preference intensity across models. Here, we compute $\mathcal{P}$ over subsamples using a regularisation parameter of 1 and tie threshold of 5, but present extended methods and robustness checks in App. T.</p>
<p>Results We find rankings are sensitive to idiosyncratic, contextual, and group-wise variance. Samples of 100 people introduce significant noise, resulting in a fairly even distribution of collective preference among the top 10 models (Fig. 4). Rankings are sensitive to what participants talk about: zephyr-7b performs highly on controversy but not in unguided domains, while claude-2 has the opposite trend; and where they are from: relative to overall rank, palm-2 drops 4 places for participants in the US, llama-7b drops 7 places in Asia, while mistral-7b gains 7 places in Africa. We further observe that Prism produces surprising ranks relative to other leaderboards. We apply our method to ChatbotAreNA data [51], finding gpt models fare significantly worse in Prism, while open models like zephyr-7b do significantly better ( $95 \%$ CI over 1,000 bootstraps, App. T.9). This may be due to domain shift (task-orientated/coding prompts vs. controversial/cultural prompts), sample diversity or task incentives. To identify drivers of score differences, we generate hypotheses by qualitatively examining battles between command and gpt-4/-turbo, then test these with an OLS regression on all model responses (App. T.8). We find that formatting and refusals partially explain score differences with significant positive effects from additional characters, ending in a question mark ("Would you like to know more?") and enumeration, but significant negative effect of line breaks. De-anthropomorphic phrases ("As an AI, I don't have personal opinions.") significantly reduce score but not as substantially as refusals ("Sorry I cannot engage."). The proportion of explained variance in score by these factors is low $\left(R^{2}=0.06\right)$, so we encourage more sophisticated methods in future work for partialling out the effect of style versus content, or participant, model and conversation fixed-effects, as determinants of score.</p>
<h3>3.3 Case Study III: How do Sampling Decisions Affect Welfare Outcomes?</h3>
<p>Methods We use 'welfare' to capture the extent to which a chosen LLM aligns with the preferences of a user population. We consider two welfare measures: average model rating (MEANRating), and average likelihood that a model is chosen (rated highest in the opening turn, MEANCHOICE). Previous experiments indicate dialogue and preference diversity across people, suggesting that the welfare of downstream LLM users may depend on who provides feedback. To test this, we first randomly generate seven sub-samples of individuals 'in the seat of power' to select their favourite LLM (based</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Sources of variation in model preferences. Panel A shows idiosyncratic variance in distributions of Pairwise Rank Centrality scores for 100 randomly-drawn participants (over 1,000 bootstraps). For Panels B and C, we show conversational context variation and group-wise variation. We show overall rank based on Pairwise Rank Centrality over n=6,669 balanced conversations (numbered circles). We then trace how rank changes by sampling the group on $x$ (e.g. filtering to only values guided conversations, or only US participants). Across these subsamples, we show most spots climbed ( $\Delta$ ) and spots fallen ( $\nabla$ ) by each model relative to overall rank. Key results (\$ 3.2): Rankings are sensitive to sample composition, varying with which participants are sampled (Panel A,C) and what they talk about (B). Rankings differ from other leaderboards, explained by PRISM's characteristics (sample diversity, domain shifts) as well as response characteristics (length, formatting, refusals).
on mean rating). Four sampling schemes randomly draw $N$ individuals from a representative sample $(N \in{10,20,50,100})$. Three schemes randomly draw 100 individuals from specific low-diversity sub-populations (male, white, and $\geq 45$ years old). For each draw, we then measure the distribution of welfare from this LLM being imposed on different stakeholder populations [9]: the entire population, non-male individuals, non-white individuals, and individuals $&lt;45$ years old. We report the distribution of average welfare outcomes across random draws from each sampling scheme. We conduct this experiment for the UK and US representative samples. Extended methods are in App. U.
Results We find as sample size falls, the probability of choosing a LLM with worse mean welfare rises. Larger samples from the target sub-population appear to first order stochastically dominate ${ }^{9}$ (FOSD) smaller samples from the target sub-population. Sampling exclusively from a specific group tends to reduce the welfare of out-group individuals. For example, when consider the welfare of the representative US sample (Fig. 5), sampling from US males is FOSD by sampling from the full US sample. Furthermore, average measures can conceal the welfare of minority groups: sampling 100 white individuals appears to FOSD sampling 100 representative individuals when assessing welfare of the population at large, but minority stakeholders (non-white population) are worse off under this scheme. Finally, regardless of the model chosen, a large proportion of participants prefer a different model. For the US, the model that maximises MEANCHOICE only</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Welfare distributions for the US. The distribution of mean welfare for four subpopulations in the US (welfare pop) induced by seven sampling schemes (in the seat of power). The $y$ axis is the sampled subpopulation (e.g. Rep is a 'representative' sample of the population) and sample size in brackets (e.g (100)). Each violin shows the distribution of mean welfare for the panel's subpopulation induced by a sampling scheme. The top four Rating comparisons use the MEANRating welfare measure and the bottom Choice comparisons use the MEANCHOICE measure. The red distributions are FOSD by Rep (100) in blue (i.e. less optimal scheme). Key results (§ 3.3): Large representative samples mostly outperform smaller or demographically-restricted samples and sampling exclusively from a specific group tends to reduce the welfare of out-group participants (male vs. non-male, white vs. non-white). No single model achieves majority preference (max 45\% MEANCHOICE).
achieves a probability of $45 \%$. If a participant is shown the winning model, and three other models at random, the probability that they will choose the winning model is $&lt;50 \%$. The probability they will pick the winning model over all other 20 LLMs can only be lower. This suggests that we should not expect a single LLM to satisfy everyone's preferences in a given population. We repeat the welfare analysis for the UK sample and conduct robustness checks with imputed missing data in App. U.</p>
<h1>4 Related Work</h1>
<p>Participation \&amp; Representation in Science \&amp; Technology There is a long history of technologies failing diverse users who lack consultation during design [52-54]. Conscious participation can be intrinsically valuable as an act of justice [55, 56]. However, in internet-harvested pre-training data, participation is involuntary or cooptative [55, 33], and unequal representation risks cultural homogenisation and minority stereotyping [57-62]. Labelling data or giving feedback is active procedural participation [53] but often relies on narrow specifications from technology providers of what counts as high-quality language or preferable outputs [15, 16, 63, 64]. In ML or NLP data, variability in subjective experience is commonly collapsed into majority votes [27, 65-68], without sufficient documentation of annotator artefacts or disagreements [69-73], despite evidence that sociodemographics affect labels [74-79]. Multiple scientific fields are guilty of over-generalising conclusions from the 'generic human' drawn from 'WEIRD' societies [80, 81]. Prism releases participant IDs and characteristics to spotlight sample diversity while acknowledging sample specificity [82].
Learning from Human Feedback Using human feedback to condition the loss function for training LLMs overcomes challenges of specifying rewards [83-85]. Combining human feedback, reinforce-</p>
<p>ment learning and natural language generation has a history in machine translation [86-88] and dialogue [89-94]. RLHF pipelines rely on binary comparisons [29-31, 85], principles or rules [32, 95], fine-grained feedback [12], or natural language [96], to reward dimensions like helpfulness, honesty and harmlessness [97, 30]. Reward models then update LLMs via algorithms like PPO [98] or Reinforce [99, 100]; but reward model free techniques are competitive, e.g. DPO [18], supervised fine-tuning [101] and rejection sampling [102, 5, 103]. There is rising demand for high-quality human feedback [104, 105], but the complexity and cost of collecting data incentivises scraping preferences, e.g. on Reddit [29, 106] or StackOverflow [107], or simulating humans with LLMs [108110]. Similar to Prism, ChatbotAreNA [51], LMSYS-1M [111] and WildChat [112] feature user-rated model interactions, but for narrow communities (HuggingFace Spaces) and domains (coding, task-orientated). Unlike these datasets, OPENCONyOS [113] collect optional contributor demographics, and Dices [79] provide demographics for multiple raters per conversation. Other datasets target specific behaviours [30, 114], or multilingual coverage [115]. Surveys on attitudes towards AI $[116,117]$ and community assemblies $[6,118,119]$ offer another lens on public priorities. To our knowledge, Prism is the first to link preference ratings and detailed survey responses.</p>
<h1>5 Limitations, Discussions and Conclusions</h1>
<p>Ethical Considerations and Limitations We collect informed consent, pseudononymise IDs, check for PII (App. E) and disallow deanonymisation in our terms (App. C), but privacy risks remain, especially given the sensitive nature of conversations. Asking participants to engage with controversies expands human preference data to discursive areas with the greatest expected degree of interpersonal disagreement, but risks encouraging hateful, bigoted, biased or otherwise harmful content. Prism is less toxic than previous datasets ( $0.06 \%$, App. E). We do not moderate prior to release to permit conversational safety research. There are many sources of variance in Prism and alternative divisions of the data may yield different outcomes [120]. Granting free choice of dialogue, using cardinal feedback scales and focusing on many kinds of models and participants introduces diversity and subjective freedom but complicates controlled experiments and limits statistical power. Prism is still biased towards English-speaking crowdworkers whose task-specific incentives may not align with wider populations. We expand on ethical risks and limitations in our data statement (App. B).
We raise three discussion points on the boundaries of where we collect preferences, for what end and with what lasting impact. First, aligning LLMs via 'preference-based utilitarianism' [121] may not be synonymous with individual or societal well-being, prompting the question of whether there are limits for "legitimate" human feedback. Preferences may be (i) at odds with self-interest due to myopia or information asymmetries (e.g. participants who want anthropomorphic LLMs despite evidenced harms [122-126]) or (ii) incompatible with others' interest (e.g. participants who prefer 'anti-woke' LLMs that argue in a debate vs. those who favour neutrality). Relying on decontextualized preference observations carries the risk of silently reinforcing biases from those in power [61, 65]; so we recommend transparency surrounding individual disagreements before aggregation decisions [9, 127], especially if participant positionality affects their epistemic legitimacy to define harm [59, 128, 129]. Second, irreconcilable personal preferences and morals matter more when the 'unit of alignment' is operationalised as a group, culture or even species, rather than an individual. Prism permits personalised or steerable alignment using participant profiles and specific ratings [2-4, 37] as well as collective alignment via opinion consensus or distribution of rewards [5-8, 28]; though group deliberation in groups may yield different outcomes than gathering data from one person at a time $[6,118,119]$. With growing use of synthetic alignment data, Prism can assist in calibrating LLM-as-judge protocols to more diverse rater pools [51, 130]. Finally, Prism was motivated by participation as justice via inclusionary alignment practices that, relative to passive roles in annotation tasks or pre-training data, prioritise active input from local citizens with specialised knowledge of their own and communities' needs [55]. However, participation remains thin because the humans crucial to the success of RLHF do not typically share in downstream benefits or profits [33, 131]. Ultimately, the impact of our work depends on those developing, researching and regulating LLMs because effective participation requires being asked and being heard [53].
In their early demonstrations of aligning AI systems to human feedback, Bai et al. discuss alignment data as a public good. We echo this sentiment with Prism—a new feedback dataset from 1,500 diverse humans, motivated by the need for inclusive, participatory and open scientific research into the pressing question of what it means to align LLMs to human preferences in a pluralistic world.</p>
<h1>Acknowledgments and Disclosure of Funding</h1>
<p>This project was awarded the MetaAI Dynabench Grant "Optimising feedback between humans-and-models-in-the-loop". For additional compute support, the project was awarded the Microsoft Azure Accelerating Foundation Model Research Grant. For additional annotation support, we received funding from the OpenPhil grant and NSF grant (IIS-2340345) via New York University. We are grateful for support received in the form of research access or credits from OpenAI, Anthropic, Aleph Alpha, Google, HuggingFace and Cohere. Hannah Rose Kirk's PhD is supported by the Economic and Social Research Council grant ES/P000649/1. Paul Röttger is a member of the Data and Marketing Insights research unit of the Bocconi Institute for Data Science and Analysis, and is supported by a MUR FARE 2020 initiative under grant agreement Prot. R20YSMBZ8S (INDOMITA). Andrew Bean's PhD is supported by the Clarendon Fund Scholarships at the University of Oxford. We are particularly grateful to Maximilian Kasy for his valuable input and advice on the welfare experiments. We are indebted to the incredible effort and time that our Prolific annotators put into our task, as well as the expert advice from Prolific consultant Andrew Gordon. We also thank any Beta testers, including friends, family and colleagues at Oxford and New York University, for their help in piloting (and debugging!) our task. Lastly, we thank Jakob Mökander, Nathan Lambert, Natasha Jacques, Felix Simon, Nino Scherrer, Maximilian Kroner Dale, Saffron Huang, Amanda Curtis and Joanna Rivera-Carlisle for their feedback on the paper in its various eras. We use scientific colour maps in our figures [132].</p>
<h2>Author Contribution Statement</h2>
<table>
<thead>
<tr>
<th style="text-align: center;">Project Conception</th>
<th style="text-align: center;">[Kirk, Hale, VIDGEN]</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Data Collection Design</td>
<td style="text-align: center;">[Kirk, Hale, VIDGEn, RÖTtGER, MARGatina]</td>
</tr>
<tr>
<td style="text-align: center;">Frontend Design and Development</td>
<td style="text-align: center;">[Kirk, Ciro]</td>
</tr>
<tr>
<td style="text-align: center;">Backend Design and Development</td>
<td style="text-align: center;">[Kirk, Mosquera]</td>
</tr>
<tr>
<td style="text-align: center;">Analysis Advisory</td>
<td style="text-align: center;">[Hale, VIDGEn, Röttger, Bartolo, Bean, Williams, He]</td>
</tr>
<tr>
<td style="text-align: center;">Literature and Dataset Comparison</td>
<td style="text-align: center;">[Kirk, Bean]</td>
</tr>
<tr>
<td style="text-align: center;">Metadata Processing</td>
<td style="text-align: center;">[Kirk, Margatina, Bean]</td>
</tr>
<tr>
<td style="text-align: center;">Manual Annotation</td>
<td style="text-align: center;">[Kirk, Bean, Röttger, Bartolo]</td>
</tr>
<tr>
<td style="text-align: center;">Results and Codebase</td>
<td style="text-align: center;">[Kirk, Whitefield]</td>
</tr>
<tr>
<td style="text-align: center;">Manuscript Writing</td>
<td style="text-align: center;">[Kirk, Whitefield]</td>
</tr>
<tr>
<td style="text-align: center;">Manuscript Editing and Feedback</td>
<td style="text-align: center;">[EVERYONE]</td>
</tr>
</tbody>
</table>
<h2>References</h2>
<p>[1] Iason Gabriel. Artificial Intelligence, Values and Alignment. Minds and Machines, 30(3):411-437, September 2020. ISSN 0924-6495, 1572-8641. doi: 10.1007/s11023-020-09539-2.
[2] Hannah Rose Kirk, Bertie Vidgen, Paul Röttger, and Scott A. Hale. The benefits, risks and bounds of personalizing the alignment of large language models to individuals. Nature Machine Intelligence, pages 1-10, April 2024. ISSN 2522-5839. doi: 10.1038/s42256-024-00820-y. URL https://www.nature. com/articles/s42256-024-00820-y. Publisher: Nature Publishing Group.
[3] Joel Jang, Seungone Kim, Bill Yuchen Lin, Yizhong Wang, Jack Hessel, Luke Zettlemoyer, Hannaneh Hajishirzi, Yejin Choi, and Prithviraj Ammanabrolu. Personalized Soups: Personalized Large Language Model Alignment via Post-hoc Parameter Merging, October 2023. URL http://arxiv.org/abs/ 2310.11564. arXiv:2310.11564 [cs].
[4] Xinyu Li, Zachary C. Lipton, and Liu Leqi. Personalized Language Modeling from Personalized Human Feedback, February 2024. URL http://arxiv.org/abs/2402.05133. arXiv:2402.05133 [cs].
[5] Michiel A. Bakker, Martin J. Chadwick, Hannah R. Sheahan, Michael Henry Tessler, Lucy CampbellGillingham, Jan Balaguer, Nat McAleese, Amelia Glaese, John Aslanides, Matthew M. Botvinick, and Christopher Summerfield. Fine-tuning language models to find agreement among humans with diverse preferences. In Advances in neural information processing systems, volume 35, pages 38176-38189. Curran Associates, Inc., November 2022. URL https://proceedings.neurips.cc/paper_files/ paper/2022/file/f978c8f3b5f399cae464e85f72e28503-Paper-Conference.pdf. _eprint: 2211.15006v1.</p>
<p>[6] Anthropic. Collective Constitutional AI: Aligning a Language Model with Public Input. Technical report, 2023. URL https://www.anthropic.com/news/collective-constitutional-ai-aligning-a-language-model-with-public-input.
[7] Souradip Chakraborty, Jiahao Qiu, Hui Yuan, Alec Koppel, Furong Huang, Dinesh Manocha, Amrit Singh Bedi, and Mengdi Wang. MaxMin-RLHF: Towards Equitable Alignment of Large Language Models with Diverse Human Preferences, February 2024. URL http://arxiv.org/abs/2402.08925. arXiv:2402.08925 [cs].
[8] Dexun Li, Cong Zhang, Kuicai Dong, Derrick Goh Xin Deik, Ruiming Tang, and Yong Liu. Aligning Crowd Feedback via Distributional Preference Reward Modeling, February 2024. URL http://arxiv. org/abs/2402.09764. arXiv:2402.09764 [cs].
[9] Vincent Conitzer, Rachel Freedman, Jobst Heitzig, Wesley H. Holliday, Bob M. Jacobs, Nathan Lambert, Milan Mossé, Eric Pacuit, Stuart Russell, Hailey Schoelkopf, Emanuel Tewolde, and William S. Zwicker. Social Choice for AI Alignment: Dealing with Diverse Human Feedback, April 2024. URL http: //arxiv.org/abs/2404.10271. arXiv:2404.10271 [cs].
[10] Hannah Kirk, Andrew Bean, Bertie Vidgen, Paul Rottger, and Scott Hale. The Past, Present and Better Future of Feedback Learning in Large Language Models for Subjective Human Preferences and Values. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 2409-2430, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.148. URL https: //aclanthology.org/2023.emnlp-main. 148.
[11] Nathan Lambert, Thomas Krendl Gilbert, and Tom Zick. The History and Risks of Reinforcement Learning and Human Feedback, November 2023. URL http://arxiv.org/abs/2310.13595. arXiv:2310.13595 [cs].
[12] Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A. Smith, Mari Ostendorf, and Hannaneh Hajishirzi. Fine-Grained Human Feedback Gives Better Rewards for Language Model Training, June 2023. URL http://arxiv.org/abs/2306.01693. arXiv:2306.01693 [cs].
[13] Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jérémy Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, Tony Wang, Samuel Marks, CharbelRaphaël Segerie, Micah Carroll, Andi Peng, Phillip Christoffersen, Mehul Damani, Stewart Slocum, Usman Anwar, Anand Siththaranjan, Max Nadeau, Eric J. Michaud, Jacob Pfau, Dmitrii Krasheninnikov, Xin Chen, Lauro Langosco, Peter Hase, Erdem Bıyık, Anca Dragan, David Krueger, Dorsa Sadigh, and Dylan Hadfield-Menell. Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback, September 2023. URL http://arxiv.org/abs/2307.15217. arXiv:2307.15217 [cs].
[14] Vinodkumar Prabhakaran, Aida Mostafazadeh Davani, and Mark Diaz. On Releasing Annotator-Level Labels and Information in Datasets. In Proceedings of the Joint 15th Linguistic Annotation Workshop (LAW) and 3rd Designing Meaning Representations (DMR) Workshop, pages 133-138, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/ 2021.law-1.14. URL https://aclanthology.org/2021.law-1.14.
[15] Paul Rottger, Bertie Vidgen, Dirk Hovy, and Janet Pierrehumbert. Two Contrasting Data Annotation Paradigms for Subjective NLP Tasks. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 175190, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022. naacl-main.13. URL https://aclanthology.org/2022.naacl-main. 13.
[16] Hannah Rose Kirk, Bertie Vidgen, Paul Röttger, and Scott A. Hale. The Empty Signifier Problem: Towards Clearer Paradigms for Operationalising "Alignment" in Large Language Models. In Socially Responsible Language Modelling Research (SoLaR). arXiv, November 2023. doi: 10.48550/arXiv.2310.02457. URL http://arxiv.org/abs/2310.02457.
[17] Andreu Mas-Colell, Michael Dennis Whinston, Jerry R Green, et al. Microeconomic theory, volume 1. Oxford university press, New York, 1995.
[18] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D. Manning, Stefano Ermon, and Chelsea Finn. Direct Preference Optimization: Your Language Model is Secretly a Reward Model. In Advances in Neural Information Processing Systems, volume 36, February 2024. URL https://proceedings.neurips.cc/paper_files/paper/2023/hash/ a85b405ed65c6477a4fe8302b5e06ce7-Abstract-Conference.html.</p>
<p>[19] Banghua Zhu, Jiantao Jiao, and Michael I. Jordan. Principled Reinforcement Learning with Human Feedback from Pairwise or \$K\$-wise Comparisons, February 2024. URL http://arxiv.org/abs/ 2301.11270. arXiv:2301.11270 [cs, math, stat].
[20] Banghua Zhu, Michael I. Jordan, and Jiantao Jiao. Iterative Data Smoothing: Mitigating Reward Overfitting and Overoptimization in RLHF, January 2024. URL http://arxiv.org/abs/2401.16335. arXiv:2401.16335 [cs, stat].
[21] Alexey Turchin. AI Alignment Problem: "Human Values" Don’t Actually Exist. PhilArchive, 2019. URL https://philarchive.org/rec/TURAAP.
[22] Brian D. Earp, Killian L. McLoughlin, Joshua T. Monrad, Margaret S. Clark, and Molly J. Crockett. How social relationships shape moral wrongness judgments. Nature Communications, 12(1):5776, October 2021. ISSN 2041-1723. doi: 10.1038/s41467-021-26067-4. URL https://www.nature.com/ articles/s41467-021-26067-4. Publisher: Nature Publishing Group.
[23] Michael F Mascolo, Allison DiBianca Fasoli, and David Greenway. A Relational Approach to Moral Development in Societies, Organizations and Individuals. Integral Review, 17(1), 2021.
[24] Judith Butler, Ernesto Laclau, and Slavoj Žižek. Contingency, hegemony, universality: contemporary dialogues on the left. Phronesis. Verso, London, 2000. ISBN 978-1-85984-757-2 978-1-85984-278-2. OCLC: ocm44780799.
[25] Mona Sloane. Controversies, contradiction, and "participation" in AI. Big Data \&amp; Society, 11(1): 20539517241235862, March 2024. ISSN 2053-9517. doi: 10.1177/20539517241235862. URL https: //doi.org/10.1177/20539517241235862. Publisher: SAGE Publications Ltd.
[26] Zeerak Talat, Hagen Blix, Josef Valvoda, Maya Indira Ganesh, Ryan Cotterell, and Adina Williams. On the machine learning of ethical judgments from natural language. In Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz, editors, Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 769-779, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.56. URL https://aclanthology.org/2022.naacl-main.56.
[27] Lora Aroyo and Chris Welty. Truth Is a Lie: Crowd Truth and the Seven Myths of Human Annotation. AI Magazine, 36(1):15-24, March 2015. ISSN 2371-9621. doi: 10.1609/aimag.v36i1.2564. URL https://ojs.aaai.org/index.php/aimagazine/article/view/2564. Number: 1.
[28] Anand Siththaranjan, Cassidy Laidlaw, and Dylan Hadfield-Menell. Distributional Preference Learning: Understanding and Accounting for Hidden Context in RLHF, December 2023. URL http://arxiv. org/abs/2312.08358. arXiv:2312.08358 [cs, stat].
[29] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. In Advances in Neural Information Processing Systems, volume 33, pages 3008-3021. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/hash/ 1f89885d556929e98d3ef9b86448f951-Abstract.html.
[30] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback, April 2022. URL http: //arxiv.org/abs/2204.05862. arXiv:2204.05862 [cs].
[31] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems, volume 35, pages 27730-27744, December 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/hash/ b1efde53be364a73914f58805a001731-Abstract-Conference.html.
[32] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie</p>
<p>Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. Constitutional AI: Harmlessness from AI Feedback, December 2022. URL http: //arxiv.org/abs/2212.08073. arXiv:2212.08073 [cs].
[33] Abeba Birhane, William Isaac, Vinodkumar Prabhakaran, Mark Diaz, Madeleine Clare Elish, Iason Gabriel, and Shakir Mohamed. Power to the People? Opportunities and Challenges for Participatory AI. In Equity and Access in Algorithms, Mechanisms, and Optimization, EAAMO '22, pages 1-8, New York, NY, USA, October 2022. Association for Computing Machinery. ISBN 978-1-4503-9477-2. doi: 10.1145/3551624.3555290. URL https://doi.org/10.1145/3551624.3555290.
[34] Esin Durmus, Karina Nyugen, Thomas I. Liao, Nicholas Schiefer, Amanda Askell, Anton Bakhtin, Carol Chen, Zac Hatfield-Dodds, Danny Hernandez, Nicholas Joseph, Liane Lovitt, Sam McCandlish, Orowa Sikder, Alex Tamkin, Janel Thamkul, Jared Kaplan, Jack Clark, and Deep Ganguli. Towards Measuring the Representation of Subjective Global Opinions in Language Models, June 2023. URL http://arxiv.org/abs/2306.16388. arXiv:2306.16388 [cs].
[35] Badr AlKhamissi, Muhammad ElNokrashy, Mai AlKhamissi, and Mona Diab. Investigating Cultural Alignment of Large Language Models, February 2024. URL http://arxiv.org/abs/2402.13231. arXiv:2402.13231 [cs].
[36] Michael J. Ryan, William Held, and Diyi Yang. Unintended Impacts of LLM Alignment on Global Representation, February 2024. URL http://arxiv.org/abs/2402.15018. arXiv:2402.15018 [cs].
[37] Taylor Sorensen, Jared Moore, Jillian Fisher, Mitchell Gordon, Niloofar Mireshghallah, Christopher Michael Rytting, Andre Ye, Liwei Jiang, Ximing Lu, Nouha Dziri, Tim Althoff, and Yejin Choi. A Roadmap to Pluralistic Alignment, February 2024. URL http://arxiv.org/abs/2402.05070. arXiv:2402.05070 [cs].
[38] Iason Gabriel and Vafa Ghazavi. The Challenge of Value Alignment: from Fairer Algorithms to AI Safety, January 2021. URL http://arxiv.org/abs/2101.06060. arXiv:2101.06060 [cs].
[39] Jonathan Stray. Aligning AI Optimization to Community Well-Being. International Journal of Community Well-Being, 3(4):443-463, December 2020. ISSN 2524-5309. doi: 10.1007/s42413-020-00086-3. URL https://doi.org/10.1007/s42413-020-00086-3.
[40] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open Foundation and Fine-Tuned Chat Models, July 2023. URL http://arxiv.org/abs/2307.09288. arXiv:2307.09288 [cs].
[41] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7B, October 2023. URL http://arxiv.org/abs/2310.06825. arXiv:2310.06825 [cs].
[42] Audrey G. Gift. Visual Analogue Scales: Measurement of Subjective Phenomena. Nursing Research, 38(5):286, October 1989. ISSN 0029-6562. URL https://journals. lww.com/nursingresearchonline/citation/1989/09000/visual_analogue_scales_ _measurement_of_subjective.6.aspx?casa_token=a0_mhu6sQyEAAAAA:y06v3LLFRZeutMmvlWTDebC4T_Je8nE_dS4M_qu96DJ6C_gR8Ro37158bzqwrw5zSexya6bpnQspDJLfY8UX5rf.</p>
<p>[43] Douwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vidgen, Grusha Prasad, Amanpreet Singh, Pratik Ringshia, Zhiyi Ma, Tristan Thrush, Sebastian Riedel, Zeerak Waseem, Pontus Stenetorp, Robin Jia, Mohit Bansal, Christopher Potts, and Adina Williams. Dynabench: Rethinking Benchmarking in NLP. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4110-4124, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.324. URL https: //aclanthology.org/2021.naacl-main. 324.
[44] Tristan Thrush, Kushal Tirumala, Anmol Gupta, Max Bartolo, Pedro Rodriguez, Tariq Kane, William Gaviria Rojas, Peter Mattson, Adina Williams, and Douwe Kiela. Dynatask: A Framework for Creating Dynamic AI Benchmark Tasks. In Valerio Basile, Zornitsa Kozareva, and Sanja Stajner, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 174-181, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-demo.17. URL https://aclanthology.org/2022.acl-demo.17.
[45] R. C. Aitken. Measurement of feelings using visual analogue scales. Proceedings of the Royal Society of Medicine, 62(10):989-993, October 1969. ISSN 0035-9157. URL https://www.ncbi.nlm.nih.gov/ pmc/articles/PMC1810824/.
[46] Prasann Singhal, Tanya Goyal, Jiacheng Xu, and Greg Durrett. A Long Way to Go: Investigating Length Correlations in RLHF, October 2023. URL http://arxiv.org/abs/2310.03716. arXiv:2310.03716 [cs].
[47] Ricardo J. G. B. Campello, Davoud Moulavi, and Joerg Sander. Density-Based Clustering Based on Hierarchical Density Estimates. In Jian Pei, Vincent S. Tseng, Longbing Cao, Hiroshi Motoda, and Guandong Xu, editors, Advances in Knowledge Discovery and Data Mining, Lecture Notes in Computer Science, pages 160-172, Berlin, Heidelberg, 2013. Springer. ISBN 978-3-642-37456-2. doi: $10.1007 / 978-3-642-37456-2 _14$.
[48] Sahand Negahban, Sewoong Oh, and Devavrat Shah. Iterative ranking from pair-wise comparisons. In Advances in Neural Information Processing Systems, volume 25. Curran Associates, Inc., 2012. URL https://papers.nips.cc/paper_files/paper/2012/hash/ 9ade82fffb5444e81fa0ce8ad8afe7a-Abstract.html.
[49] Gergei Bana, Wojciech Jamroga, David Naccache, and Peter Y. A. Ryan. Convergence Voting: From Pairwise Comparisons to Consensus, March 2021. URL http://arxiv.org/abs/2102.01995. arXiv:2102.01995 [cs].
[50] Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. The PageRank Citation Ranking: Bringing Order to the Web., November 1999. URL http://ilpubs.stanford.edu:8090/422/?doi= 10.1.1.31.1768. Type: Techreport.
[51] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging LLM-as-aJudge with MT-Bench and Chatbot Arena. Advances in Neural Information Processing Systems, 36:4659546623, December 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/ hash/91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html.
[52] Safiya Umoja Noble. Algorithms of oppression: how search engines reinforce racism. New York University Press, New York, 2018. ISBN 978-1-4798-4994-9 978-1-4798-3724-3.
[53] Christopher M. Kelty. The Participant - A Century of Participation in Four Stories. The University of Chicago press, Chicago (Ill.) London, 2019. ISBN 978-0-226-66662-4 978-0-226-66676-1.
[54] Caroline Criado-Perez. Invisible women: exposing data bias in a world designed for men. Chatto \&amp; Windus, London, 2019. ISBN 978-1-78474-172-3 978-1-78474-292-8.
[55] Mona Sloane, Emanuel Moss, Olaitan Awomolo, and Laura Forlano. Participation Is not a Design Fix for Machine Learning. In Proceedings of the 2nd ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization, EAAMO '22, pages 1-6, New York, NY, USA, October 2022. Association for Computing Machinery. ISBN 978-1-4503-9477-2. doi: 10.1145/3551624.3555285. URL https://dl.acm.org/doi/10.1145/3551624.3555285.</p>
<p>[56] Travis Greene, Copenhagen Business School, Galit Shmueli, National Tsing Hua University, Soumya Ray, and National Tsing Hua University. Taking the Person Seriously: Ethically Aware IS Research in the Era of Reinforcement Learning-Based Personalization. Journal of the Association for Information Systems, 24(6):1527-1561, 2023. ISSN 15369323. doi: 10.17705/1jais.00800. URL https://aisel. aisnet.org/jais/vol24/iss6/6/.
[57] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. WebGPT: Browserassisted question-answering with human feedback. December 2021. URL http://arxiv.org/abs/ 2112.09332v3.
[58] Abeba Birhane, Vinay Uday Prabhu, and Emmanuel Kahembwe. Multimodal datasets: misogyny, pornography, and malignant stereotypes. arXiv preprint arXiv:2110.01963, 2021.
[59] Ruha Benjamin. Race After Technology: Abolitionist Tools for the New Jim Code. John Wiley \&amp; Sons, July 2019. ISBN 978-1-5095-2643-7.
[60] Su Lin Blodgett, Solon Barocas, Hal Daumé III, and Hanna Wallach. Language (Technology) is Power: A Critical Survey of "Bias" in NLP. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5454-5476, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.485. URL https://aclanthology.org/2020.aclmain. 485 .
[61] Daniel Hershcovich, Stella Frank, Heather Lent, Miryam de Lhoneux, Mostafa Abdou, Stephanie Brandl, Emanuele Bugliarello, Laura Cabello Piqueras, Ilias Chalkidis, Ruixiang Cui, Constanza Fierro, Katerina Margatina, Phillip Rust, and Anders Søgaard. Challenges and Strategies in Cross-Cultural NLP. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6997-7013, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acllong.482. URL https://aclanthology.org/2022.acl-long. 482.
[62] Wenxuan Wang, Wenxiang Jiao, Jingyuan Huang, Ruyi Dai, Jen-tse Huang, Zhaopeng Tu, and Michael R. Lyu. Not All Countries Celebrate Thanksgiving: On the Cultural Dominance in Large Language Models, February 2024. URL http://arxiv.org/abs/2310.12481. arXiv:2310.12481 [cs] version: 2.
[63] Suchin Gururangan, Dallas Card, Sarah K. Dreier, Emily K. Gade, Leroy Z. Wang, Zeyu Wang, Luke Zettlemoyer, and Noah A. Smith. Whose Language Counts as High Quality? Measuring Language Ideologies in Text Data Selection, January 2022. URL http://arxiv.org/abs/2201.10474. arXiv:2201.10474 [cs].
[64] Josh Dzieza. Inside the AI Factory, June 2023. URL https://www.theverge.com/features/ 23764584/ai-artificial-intelligence-data-notation-labor-scale-surge-remotasks-openai-chatbots.
[65] Shakir Mohamed, Marie-Therese Png, and William Isaac. Decolonial AI: Decolonial Theory as Sociotechnical Foresight in Artificial Intelligence. Philosophy \&amp; Technology, 33(4):659-684, December 2020. ISSN 2210-5441. doi: 10.1007/s13347-020-00405-8. URL https://doi.org/10.1007/s13347-020-00405-8.
[66] Massimo Airoldi. Machine habitus: toward a sociology of algorithms. Polity Press, Cambridge ; Medford, MA, 2022. ISBN 978-1-5095-4327-4 978-1-5095-4328-1. OCLC: on1247827618.
[67] Zeerak Talat, Hagen Blix, Josef Valvoda, Maya Indira Ganesh, Ryan Cotterell, and Adina Williams. On the Machine Learning of Ethical Judgments from Natural Language. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 769-779, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.56. URL https://aclanthology.org/2022.naacl-main.56.
[68] Mark Díaz, Ian Kivlichan, Rachel Rosen, Dylan Baker, Razvan Amironesei, Vinodkumar Prabhakaran, and Emily Denton. CrowdWorkSheets: Accounting for Individual and Collective Identities Underlying Crowdsourced Dataset Annotation. In 2022 ACM Conference on Fairness, Accountability, and Transparency, FAccT '22, pages 2342-2351, New York, NY, USA, June 2022. Association for Computing Machinery. ISBN 978-1-4503-9352-2. doi: 10.1145/3531146.3534647. URL https://doi.org/10.1145/3531146.3534647.</p>
<p>[69] Emily M. Bender and Batya Friedman. Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science. Transactions of the Association for Computational Linguistics, 6:587-604, 2018. doi: 10.1162/tacl_a_00041. URL https://aclanthology.org/Q181041. Place: Cambridge, MA Publisher: MIT Press.
[70] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. Model Cards for Model Reporting. Proceedings of the Conference on Fairness, Accountability, and Transparency - FAT* '19, pages 220-229, 2019. doi: 10.1145/3287560.3287596. URL http://arxiv.org/abs/1810.03993. arXiv: 1810.03993.
[71] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé Iii, and Kate Crawford. Datasheets for datasets. Communications of the ACM, 64(12): 86-92, December 2021. ISSN 15577317. doi: 10.1145/3458723. Publisher: Association for Computing Machinery.
[72] Aida Mostafazadeh Davani, Mark Díaz, and Vinodkumar Prabhakaran. Dealing with Disagreements: Looking Beyond the Majority Vote in Subjective Annotations. Transactions of the Association for Computational Linguistics, 10:92-110, January 2022. ISSN 2307-387X. doi: 10.1162/tacl_a_00449. URL https://doi.org/10.1162/tacl_a_00449.
[73] Mitchell L. Gordon, Michelle S. Lam, Joon Sung Park, Kayur Patel, Jeff Hancock, Tatsunori Hashimoto, and Michael S. Bernstein. Jury Learning: Integrating Dissenting Voices into Machine Learning Models. In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems, CHI '22, pages 1-19, New York, NY, USA, April 2022. Association for Computing Machinery. ISBN 978-1-4503-9157-3. doi: $10.1145 / 3491102.3502004$. URL https://doi.org/10.1145/3491102.3502004.
[74] Barbara Plank, Dirk Hovy, and Anders Søgaard. Learning part-of-speech taggers with inter-annotator agreement loss. In Shuly Wintner, Sharon Goldwater, and Stefan Riezler, editors, Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 742-751, Gothenburg, Sweden, April 2014. Association for Computational Linguistics. doi: 10.3115/v1/E14-1078. URL https://aclanthology.org/E14-1078.
[75] Yixin Nie, Xiang Zhou, and Mohit Bansal. What Can We Learn from Collective Human Opinions on Natural Language Inference Data? In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9131-9143, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.734. URL https://aclanthology.org/2020.emnlp-main. 734.
[76] Maximilian Wich, Christian Widmer, Gerhard Hagerer, and Georg Groh. Investigating Annotator Bias in Abusive Language Datasets. In Ruslan Mitkov and Galia Angelova, editors, Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2021), pages 1515-1525, Held Online, September 2021. INCOMA Ltd. URL https://aclanthology.org/2021. ranlp-1.170.
[77] Maarten Sap, Swabha Swayamdipta, Laura Vianna, Xuhui Zhou, Yejin Choi, and Noah A. Smith. Annotators with Attitudes: How Annotator Beliefs And Identities Bias Toxic Language Detection. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5884-5906, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.431. URL https://aclanthology.org/2022.naacl-main. 431.
[78] Nitesh Goyal, Ian D. Kivlichan, Rachel Rosen, and Lucy Vasserman. Is Your Toxicity My Toxicity? Exploring the Impact of Rater Identity on Toxicity Annotation. Proceedings of the ACM on HumanComputer Interaction, 6(CSCW2):363:1-363:28, November 2022. doi: 10.1145/3555088. URL https: //dl.acm.org/doi/10.1145/3555088.
[79] Lora Aroyo, Alex S. Taylor, Mark Diaz, Christopher M. Homan, Alicia Parrish, Greg Serapio-Garcia, Vinodkumar Prabhakaran, and Ding Wang. DICES Dataset: Diversity in Conversational AI Evaluation for Safety, June 2023. URL http://arxiv.org/abs/2306.11247. arXiv:2306.11247 [cs].
[80] Joseph Henrich, Steven J. Heine, and Ara Norenzayan. Most people are not WEIRD. Nature, 466 (7302):29-29, July 2010. ISSN 1476-4687. doi: 10.1038/466029a. URL https://www.nature.com/ articles/466029a. Number: 7302 Publisher: Nature Publishing Group.
[81] Dante A. Urbina and Alberto Ruiz-Villaverde. A Critical Review of Homo Economicus from Five Approaches. The American Journal of Economics and Sociology, 78(1):63-93, 2019. ISSN 15367150. doi: 10.1111/ajes. 12258. URL https://onlinelibrary.wiley.com/doi/abs/10.1111/ ajes. 12258. _eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/ajes. 12258.</p>
<p>[82] Coren Apicella, Ara Norenzayan, and Joseph Henrich. Beyond WEIRD: A review of the last decade and a look ahead to the global laboratory of the future. Evolution and Human Behavior, 41(5):319329, September 2020. ISSN 1090-5138. doi: 10.1016/j.evolhumbehav.2020.07.015. URL https: //www.sciencedirect.com/science/article/pii/S1090513820300957.
[83] Andrew Ng and Stuart J. Russell. Algorithms for Inverse Reinforcement Learning. 2000.
[84] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep Reinforcement Learning from Human Preferences. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/ paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html.
[85] Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-Tuning Language Models from Human Preferences. September 2019. URL http://arxiv.org/abs/1909.08593v2.
[86] Shachar Mirkin and Jean-Luc Meunier. Personalized machine translation: Predicting translational preferences. In Proceedings of the 2015 conference on empirical methods in natural language processing, pages 2019-2025, Lisbon, Portugal, September 2015. Association for Computational Linguistics. doi: 10.18653/v1/D15-1238. URL https://aclanthology.org/D15-1238.
[87] Khanh Nguyen, Hal Daumé III, and Jordan Boyd-Graber. Reinforcement Learning for Bandit Neural Machine Translation with Simulated Human Feedback. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1464-1474, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1153. URL https://aclanthology.org/D17-1153.
[88] Julia Kreutzer, Artem Sokolov, and Stefan Riezler. Bandit Structured Prediction for Neural Sequence-toSequence Learning. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1503-1513, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1138. URL https://aclanthology.org/P171138 .
[89] Marilyn A Walker. An application of reinforcement learning to dialogue strategy selection in a spoken dialogue system for email. Journal of Artificial Intelligence Research, 12:387-416, 2000.
[90] Jost Schatzmann, Karl Weilhammer, Matt Stuttle, and Steve Young. A survey of statistical user simulation techniques for reinforcement-learning of dialogue management strategies. The knowledge engineering review, 21(2):97-126, 2006.
[91] Pei-Hao Su, Milica Gasic, Nikola Mrksic, Lina Maria Rojas-Barahona, Stefan Ultes, David Vandyke, Tsung-Hsien Wen, and Steve J. Young. Continuously learning neural dialogue management. abs/1606.02689, 2016. URL http://arxiv.org/abs/1606.02689.
[92] Jiwei Li, Alexander H. Miller, Sumit Chopra, Marc'Aurelio Ranzato, and Jason Weston. Dialogue learning with human-in-the-loop. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https://openreview.net/forum?id=HJgXCV9xx.
[93] Natasha Jaques, Asma Ghandeharioun, Judy Hanwen Shen, Craig Ferguson, Agata Lapedriza, Noah Jones, Shixiang Gu, and Rosalind Picard. Way Off-Policy Batch Deep Reinforcement Learning of Implicit Human Preferences in Dialog, July 2019. URL http://arxiv.org/abs/1907.00456. arXiv:1907.00456 [cs, stat].
[94] Natasha Jaques, Judy Hanwen Shen, Asma Ghandeharioun, Craig Ferguson, Agata Lapedriza, Noah Jones, Shixiang Gu, and Rosalind Picard. Human-centric dialog training via offline reinforcement learning. In Proceedings of the 2020 conference on empirical methods in natural language processing (EMNLP), pages 3985-4003, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.327. URL https://aclanthology.org/2020.emnlp-main. 327.
[95] Amelia Glaese, Nat McAleese, Maja Trębacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, Lucy Campbell-Gillingham, Jonathan Uesato, Po-Sen Huang, Ramona Comanescu, Fan Yang, Abigail See, Sumanth Dathathri, Rory Greig, Charlie Chen, Doug Fritz, Jaume Sanchez Elias, Richard Green, Soňa Mokrá, Nicholas Fernando, Boxi Wu, Rachel Foley, Susannah Young, Iason Gabriel, William Isaac, John Mellor, Demis Hassabis, Koray Kavukcuoglu, Lisa Anne Hendricks, and Geoffrey Irving. Improving alignment of dialogue agents via targeted human judgements. September 2022. URL http://arxiv.org/abs/2209.14375v1.</p>
<p>[96] Jérémy Scheurer, Jon Ander Campos, Jun Shern Chan, Angelica Chen, Kyunghyun Cho, and Ethan Perez. Training Language Models with Language Feedback, November 2022. URL http://arxiv.org/abs/ 2204.14146. arXiv:2204.14146 [cs].
[97] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Jackson Kernion, Kamal Ndousse, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, and Jared Kaplan. A General Language Assistant as a Laboratory for Alignment, December 2021. URL http://arxiv.org/abs/2112.00861. arXiv:2112.00861 [cs].
[98] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal Policy Optimization Algorithms, August 2017. URL http://arxiv.org/abs/1707.06347. arXiv:1707.06347 [cs].
[99] Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8(3):229-256, May 1992. ISSN 1573-0565. doi: 10.1007/BF00992696. URL https://doi.org/10.1007/BF00992696.
[100] Arash Ahmadian, Chris Cremer, Matthias Gallé, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Üstün, and Sara Hooker. Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs, February 2024. URL http://arxiv.org/abs/2402.14740. arXiv:2402.14740 [cs] version: 1.
[101] Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. LIMA: Less Is More for Alignment, May 2023. URL http://arxiv.org/abs/2305.11206. arXiv:2305.11206 [cs].
[102] Jacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song, Martin Chadwick, Mia Glaese, Susannah Young, Lucy Campbell-Gillingham, Geoffrey Irving, and Nat McAleese. Teaching language models to support answers with verified quotes, March 2022. URL http://arxiv.org/abs/ 2203.11147. arXiv:2203.11147 [cs].
[103] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Vincent Zhao, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Pranesh Srinivasan, Laichee Man, Kathleen MeierHellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toja Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed Chi, and Quoc Le. LaMDA: Language Models for Dialog Applications, February 2022. URL http://arxiv.org/abs/2201.08239. arXiv:2201.08239 [cs].
[104] Nathan Lambert, Valentina Pyatkin, Jacob Morrison, L. J. Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, Noah A. Smith, and Hannaneh Hajishirzi. RewardBench: Evaluating Reward Models for Language Modeling, March 2024. URL http://arxiv.org/abs/2403.13787. arXiv:2403.13787 [cs].
[105] Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. KTO: Model Alignment as Prospect Theoretic Optimization, February 2024. URL http://arxiv.org/abs/2402. 01306. arXiv:2402.01306 [cs].
[106] StanfordNLP. Stanford Human Preferences Dataset, September 2023. URL https://huggingface. co/datasets/stanfordnlp/SHP.
[107] Nathan Lambert, Lewis Tunstall, Nazneen Rajani, and Tristan Thrush. HuggingFace H4 Stack Exchange Preference Dataset, 2023. URL https://huggingface.co/datasets/HuggingFaceH4/stack-exchange-preferences.
[108] William Agnew, A. Stevie Bergman, Jennifer Chien, Mark Díaz, Seliem El-Sayed, Jaylen Pittman, Shakir Mohamed, and Kevin R. McKee. The illusion of artificial inclusion, February 2024. URL http://arxiv.org/abs/2401.08572. arXiv:2401.08572 [cs].
[109] Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback, January 2024. URL http://arxiv.org/abs/2305.14387. arXiv:2305.14387 [cs].</p>
<p>[110] Yiju Guo, Ganqu Cui, Lifan Yuan, Ning Ding, Jiexin Wang, Huimin Chen, Bowen Sun, Ruobing Xie, Jie Zhou, Yankai Lin, Zhiyuan Liu, and Maosong Sun. Controllable Preference Optimization: Toward Controllable Multi-Objective Alignment, February 2024. URL http://arxiv.org/abs/2402.19085. arXiv:2402.19085 [cs, eess].
[111] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zhuohan Li, Zi Lin, Eric P. Xing, Joseph E. Gonzalez, Ion Stoica, and Hao Zhang. LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset, March 2024. URL http://arxiv.org/abs/ 2309.11998. arXiv:2309.11998 [cs].
[112] Wenting Zhao, Xiang Ren, Jack Hessel, Claire Cardie, Yejin Choi, and Yuntian Deng. (InThe)WildChat: 570K ChatGPT Interaction Logs In The Wild. October 2023. URL https://openreview.net/forum? id=B18u7ZR1bM.
[113] Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Richárd Nagyfi, Shahuf ES, Sameer Suri, David Glushkov, Arnav Dantuluri, Andrew Maguire, Christoph Schuhmann, Huu Nguyen, and Alexander Mattick. OpenAssistant Conversations - Democratizing Large Language Model Alignment, October 2023. URL http://arxiv.org/abs/2304.07327. arXiv:2304.07327 [cs].
[114] Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, Andy Jones, Sam Bowman, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Nelson Elhage, Sheer El-Showk, Stanislav Fort, Zac Hatfield-Dodds, Tom Henighan, Danny Hernandez, Tristan Hume, Josh Jacobson, Scott Johnston, Shauna Kravec, Catherine Olsson, Sam Ringer, Eli Tran-Johnson, Dario Amodei, Tom Brown, Nicholas Joseph, Sam McCandlish, Chris Olah, Jared Kaplan, and Jack Clark. Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned, November 2022. URL http://arxiv.org/abs/2209.07858. arXiv:2209.07858 [cs].
[115] Shivalika Singh, Freddie Vargus, Daniel Dsouza, Börje F. Karlsson, Abinaya Mahendiran, Wei-Yin Ko, Herumb Shandilya, Jay Patel, Deividas Mataciunas, Laura OMahony, Mike Zhang, Ramith Hettiarachchi, Joseph Wilson, Marina Machado, Luisa Souza Moura, Dominik Krzemiński, Hakimeh Fadaei, Irem Ergün, Ifeoma Okoh, Aisha Alaagib, Oshan Mudannayake, Zaid Alyafeai, Vu Minh Chien, Sebastian Ruder, Surya Guthikonda, Emad A. Alghamdi, Sebastian Gehrmann, Niklas Muennighoff, Max Bartolo, Julia Kreutzer, Ahmet Üstün, Marzieh Fadaee, and Sara Hooker. Aya Dataset: An Open-Access Collection for Multilingual Instruction Tuning, February 2024. URL http://arxiv.org/abs/2402.06619. arXiv:2402.06619 [cs].
[116] The Alan Turing Institute and The Ada Lovelace Institute. How do people feel about AI? A nationally representative survey of public attitudes to artificial intelligence in Britain. Technical report, 2023. URL https://attitudestoai.uk/assets/documents/Ada-Lovelace-Institute-The-Alan-Turing-Institute-How-do-people-feel-about-AI.pdf.
[117] Jimin Mun, Liwei Jiang, Jenny Liang, Inyoung Cheong, Nicole DeCario, Yejin Choi, Tadayoshi Kohno, and Maarten Sap. Particip-AI: A Democratic Surveying Framework for Anticipating Future AI Use Cases, Harms and Benefits, March 2024. URL http://arxiv.org/abs/2403.14791. arXiv:2403.14791 [cs].
[118] Samuel Chang, Estelle Ciesla, Michael Finch, James Fishkin, Lodewijk Gelauff, Ashish Goel, Ricky Hernandez Marquez, Shoaib Mohammed, and Alice Siu. Meta Community Forum: Results Analysis. Technical report, Deliberative Democracy Lab, Stanford University, April 2024.
[119] Stevie Bergman, Nahema Marchal, John Mellor, Shakir Mohamed, Iason Gabriel, and William Isaac. STELA: a community-centred approach to norm elicitation for AI alignment. Scientific Reports, 14(1): 6616, March 2024. ISSN 2045-2322. doi: 10.1038/s41598-024-56648-4. URL https://www.nature. com/articles/s41598-024-56648-4. Publisher: Nature Publishing Group.
[120] R. Silberzahn, E. L. Uhlmann, D. P. Martin, P. Anselmi, F. Aust, E. Awtrey, Š. Bahník, F. Bai, C. Bannard, E. Bonnier, R. Carlsson, F. Cheung, G. Christensen, R. Clay, M. A. Craig, A. Dalla Rosa, L. Dam, M. H. Evans, I. Flores Cervantes, N. Fong, M. Gamez-Djokic, A. Glenz, S. Gordon-McKeon, T. J. Heaton, K. Hederos, M. Heene, A. J. Hofelich Mohr, F. Högden, K. Hui, M. Johannesson, J. Kalodimos, E. Kaszubowski, D. M. Kennedy, R. Lei, T. A. Lindsay, S. Liverani, C. R. Madan, D. Molden, E. Molleman, R. D. Morey, L. B. Mulder, B. R. Nijstad, N. G. Pope, B. Pope, J. M. Pernoveau, F. Rink, E. Robusto, H. Roderique, A. Sandberg, E. Schlüter, F. D. Schönbrodt, M. F. Sherman, S. A. Sommer, K. Sotak, S. Spain, C. Spörlein, T. Stafford, L. Stefanutti, S. Tauber, J. Ullrich, M. Vianello, E.-J. Wagenmakers, M. Witkowiak, S. Yoon, and B. A. Nosek. Many Analysts, One Data Set: Making Transparent How Variations in Analytic Choices Affect Results. Advances in Methods and Practices in Psychological</p>
<p>Science, 1(3):337-356, September 2018. ISSN 2515-2459. doi: 10.1177/2515245917747646. URL https://doi.org/10.1177/2515245917747646. Publisher: SAGE Publications Inc.
[121] John Tasioulas. Artificial Intelligence, Humanistic Ethics. Daedalus, 151(2):232-243, May 2022. ISSN 0011-5266. doi: 10.1162/daed_a_01912. URL https://doi.org/10.1162/daed_a_01912.
[122] Diane Proudfoot. Anthropomorphism and AI: Turing's much misunderstood imitation game. Artificial Intelligence, 175(5):950-957, April 2011. ISSN 0004-3702. doi: 10.1016/j.artint.2011.01.006. URL https://www.sciencedirect.com/science/article/pii/S000437021100018X.
[123] David Watson. The Rhetoric and Reality of Anthropomorphism in Artificial Intelligence. Minds and Machines, 29(3):417-440, September 2019. ISSN 1572-8641. doi: 10.1007/s11023-019-09506-6. URL https://doi.org/10.1007/s11023-019-09506-6.
[124] Laura Weidinger, Jonathan Uesato, Maribeth Rauh, Conor Griffin, Po-Sen Huang, John Mellor, Amelia Glaese, Myra Cheng, Borja Balle, Atoosa Kasirzadeh, Courtney Biles, Sasha Brown, Zac Kenton, Will Hawkins, Tom Stepleton, Abeba Birhane, Lisa Anne Hendricks, Laura Rimell, William Isaac, Julia Haas, Sean Legassick, Geoffrey Irving, and Iason Gabriel. Taxonomy of Risks posed by Language Models. In 2022 ACM Conference on Fairness, Accountability, and Transparency, pages 214-229, Seoul Republic of Korea, June 2022. ACM. ISBN 978-1-4503-9352-2. doi: 10.1145/3531146.3533088. URL https://dl.acm.org/doi/10.1145/3531146.3533088.
[125] Gavin Abercrombie, Amanda Cercas Curry, Tanvi Dinkar, Verena Rieser, and Zeerak Talat. Mirages. On Anthropomorphism in Dialogue Systems. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 4776-4790, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/ 2023.emnlp-main.290. URL https://aclanthology.org/2023.emnlp-main. 290.
[126] Myra Cheng, Kristina Gligoric, Tiziano Piccardi, and Dan Jurafsky. AnthroScore: A Computational Linguistic Measure of Anthropomorphism, February 2024. URL http://arxiv.org/abs/2402.02056. arXiv:2402.02056 [cs].
[127] Sian Gooding and Hassan Mansoor. The Impact of Preference Agreement in Reinforcement Learning from Human Feedback: A Case Study in Summarization, November 2023. URL http://arxiv.org/ abs/2311.04919. arXiv:2311.04919 [cs].
[128] Catherine D’Ignazio and Lauren F. Klein. Data Feminism. The MIT Press, March 2020. ISBN 978-0-262-35852-1. doi: 10.7551/mitpress/11805.001.0001. URL https://direct.mit.edu/books/book/ 4660/Data-Feminism.
[129] Abeba Birhane. Algorithmic injustice: a relational ethics approach. Patterns, 2(2):100205, February 2021. ISSN 26663899. doi: 10.1016/j.patter.2021.100205. URL https://linkinghub.elsevier. com/retrieve/pii/S2666389921000155.
[130] Yijiang River Dong, Tiancheng Hu, and Nigel Collier. Can LLM be a Personalized Judge?, June 2024. URL http://arxiv.org/abs/2406.11657. arXiv:2406.11657 [cs].
[131] Billy Perrigo. Inside OpenAI's Plan to Make AI More 'Democratic', February 2024. URL https: //time.com/6684266/openai-democracy-artificial-intelligence/.
[132] Fabio Crameri, Grace E. Shephard, and Philip J. Heron. The misuse of colour in science communication. Nature Communications, 11(1):5444, October 2020. ISSN 2041-1723. doi: 10.1038/s41467-020-19160-7. URL https://www.nature.com/articles/s41467-020-19160-7. Number: 1 Publisher: Nature Publishing Group.
[133] Nenad Tomasev, Kevin R. McKee, Jackie Kay, and Shakir Mohamed. Fairness for Unobserved Characteristics: Insights from Technological Impacts on Queer Communities. In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society, AIES '21, pages 254-265, New York, NY, USA, July 2021. Association for Computing Machinery. ISBN 978-1-4503-8473-5. doi: 10.1145/3461702.3462540. URL https://doi.org/10.1145/3461702.3462540.
[134] Tom Hosking, Phil Blunsom, and Max Bartolo. Human Feedback is not Gold Standard, January 2024. URL http://arxiv.org/abs/2309.16349. arXiv:2309.16349 [cs].
[135] Amos Tversky and Itamar Simonson. Context-Dependent Preferences. Management Science, 39(10):11791189, October 1993. ISSN 0025-1909. doi: 10.1287/mnsc.39.10.1179. URL https://pubsonline. informs.org/doi/abs/10.1287/mnsc.39.10.1179. Publisher: INFORMS.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{9}$ A probability distribution with $\operatorname{CDF} F_{p}$ is said to First Order Stochastically Dominate another probability distribution with $\operatorname{CDF} F_{\eta}$ if both distributions have a finite mean, and $F_{p}(t) \leq F_{\eta}(t) \quad \forall t \in \mathbb{R}$.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
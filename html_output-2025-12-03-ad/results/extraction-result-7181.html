<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7181 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7181</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7181</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-135.html">extraction-schema-135</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <p><strong>Paper ID:</strong> paper-7c593f8b5525851c2b453253835ddac6b1104a3b</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/7c593f8b5525851c2b453253835ddac6b1104a3b" target="_blank">Step-by-Step: Separating Planning from Realization in Neural Data-to-Text Generation</a></p>
                <p><strong>Paper Venue:</strong> North American Chapter of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> The results demonstrate that decoupling text planning from neural realization indeed improves the system’s reliability and adequacy while maintaining fluent output, and improvements both in BLEU scores and in manual evaluations are observed.</p>
                <p><strong>Paper Abstract:</strong> Data-to-text generation can be conceptually divided into two parts: ordering and structuring the information (planning), and generating fluent language describing the information (realization). Modern neural generation systems conflate these two steps into a single end-to-end differentiable system. We propose to split the generation process into a symbolic text-planning stage that is faithful to the input, followed by a neural generation stage that focuses only on realization. For training a plan-to-text generator, we present a method for matching reference texts to their corresponding text plans. For inference time, we describe a method for selecting high-quality text plans for new inputs. We implement and evaluate our approach on the WebNLG benchmark. Our results demonstrate that decoupling text planning from neural realization indeed improves the system’s reliability and adequacy while maintaining fluent output. We observe improvements both in BLEU scores and in manual evaluations. Another benefit of our approach is the ability to output diverse realizations of the same input, paving the way to explicit control over the generated text structure.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7181.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7181.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TextPlanTree</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ordered Sentence-Plan Tree (Text Plan)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A symbolic, sentence-level plan that represents how an input RDF graph's facts are grouped into sentences and ordered; each sentence is an ordered, labeled tree whose arcs correspond to RDF triplets annotated with relation direction. The plan guarantees each input triplet is realized exactly once in some direction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Ordered sentence-plan tree (Text Plan)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Encodes the graph as a sequence of sentence plans; each sentence plan is a labeled, ordered tree with nodes = input entities and arcs = (head, (relation, direction), modifier). Chains and sibling structures represent shared-entity fact succession. A full text plan is a sequence of such sentence trees covering every RDF triplet exactly once (either as forward or reversed edge).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>hierarchical (ordered trees) / symbolic</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Construct sentence trees by grouping RDF triplets into disjoint sentence-sets and arranging each set as an undirected graph then performing DFS traversals to produce ordered trees (exhaustive enumeration of possible trees via different DFS orders and starting nodes).</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>WebNLG</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>data-to-text generation / text planning</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Non-neural symbolic planner (used with Plan-to-Text NMT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Symbolic planner (deterministic procedure + product-of-experts ranking) that enumerates possible sentence-plan trees and ranks them using MLE-based experts; not a learned neural encoder — used to produce plans fed to a neural realizer.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Used indirectly; downstream evaluation measured with BLEU, METEOR, ROUGE_L, CIDEr and manual faithfulness counts</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>By supplying explicit, faithful plans to the neural realizer, the decomposition improved downstream faithfulness (manual counts) and allowed the realizer to focus on short sentence-level mapping; enabled diverse realizations by selecting different high-scoring plans.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Plan construction is dataset-dependent (matching reference texts to plans succeeded for ~76% of training refs); exhaustive plan enumeration is combinatorially explosive (authors report O(2^{2n}+ n * n!) possible plans) making the approach infeasible for larger graphs without better search; plan design currently omits discourse-level referring-expression modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Compared to end-to-end neural approaches, the explicit tree-based plan yields substantially better semantic faithfulness while maintaining fluency; it is more structured and verifiable than black-box encodings but less flexible in modeling discourse phenomena than systems that integrate document-level or coreference modeling (e.g., Puduppully et al., 2018).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Step-by-Step: Separating Planning from Realization in Neural Data-to-Text Generation', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7181.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7181.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LinearizedPlan</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Preorder Linearized Text Plan (Bracketed Traversal)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sequential token representation of a sentence-plan tree produced for NMT: perform a pre-order traversal of the ordered tree and emit tokens with bracket markers to indicate tree structure; relations are encoded as direction token + relation token sequence; entities are replaced with unique entity tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Preorder linearization with bracketed structure</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each sentence-plan tree is linearized via pre-order traversal; brackets signal hierarchical subtree boundaries. Directed relations (r, d) are serialized as a small token sequence: a token indicating direction (e.g., arrow token) followed by relation token(s) (relation tokenized by splitting underscores/CamelCase). Recognized entities are replaced by single, entity-unique placeholder tokens to enable copying.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>hierarchical->sequential / token-based (linearized tree with structural markers)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Preorder (depth-first) traversal of the sentence tree; add explicit bracket tokens to mark tree structure; relations expressed as direction token + split relation tokens; entities delexicalized to single unique tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>WebNLG</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>plan-to-text realization (graph->text via linearized plan)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenNMT (vanilla NMT) with copy-attention and pretrained GloVe embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Standard seq2seq NMT trained to translate linearized plans into surface text using OpenNMT toolkit, copy-attention mechanism (copy_attn flag), and GloVe 6B embeddings to initialize relation and text tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Automatic: BLEU, METEOR, ROUGE_L, CIDEr (via nlg-eval); Manual: faithfulness counts (expressed/omitted/wrong-lex/over-generation); plan-realizer consistency (percent entities realized and order preserved).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Feeding linearized, bracketed plans resulted in realizations that are as fluent as end-to-end NMT systems but more faithful to input facts; allowed reliable realization of high-scoring plans (e.g., seen-test best-plan: 98.9% of sentence plans realized with all requested entities and 100% preserved order).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Does not model discourse-level referring expressions (no coreference generation); linearization and delexicalization rely on entity-recognition heuristics during training (Levenshtein matching) causing imperfect plan-text alignment; scalability issues stem from upstream exhaustive plan generation rather than linearization itself.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Compared to black-box encoding of RDF triplets (e.g., flat triple sequences), this linearized-tree encoding preserves higher-level structuring decisions explicitly, improving faithfulness; orthogonal to copy/checklist mechanisms — those can augment the same linearized inputs but do not replace explicit planning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Step-by-Step: Separating Planning from Realization in Neural Data-to-Text Generation', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7181.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7181.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EntityDelex</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Entity-Unique Token Delexicalization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A preprocessing representation that replaces recognized entities in linearized plans with single, unique entity tokens so the neural realizer can copy or re-insert full entity strings at post-processing time.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Entity-unique token delexicalization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Entities identified in the reference texts are replaced with single, unique placeholder tokens in the plan input to the NMT; after generation, placeholders are replaced with the original entity strings from the input graph. Dates and numeric units are lexicalized with deterministic post-processing rules.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>token-based / delexicalized</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Greedy string/entity recognition in references (Levenshtein token matching + tuned thresholds and chrono-python for dates) to map textual mentions to graph entities; replace matched mentions with unique entity tokens in both plans and reference targets during training.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>WebNLG</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>plan-to-text realization / delexicalized NMT training</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenNMT with copy-attention (same as LinearizedPlan realizer)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Seq2seq NMT with copy mechanism that benefits from delexicalized entity tokens which enable copying of entity placeholders rather than generating long or variable entity strings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Indirect: improves copy behavior and reduces lexical errors; measured via manual wrong-lexicalization counts and plan-realizer entity realization percentages.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Enables robust copying of entities and reduces vocabulary sparsity of entity names; contributed to high entity-realization rates (e.g., seen best-plan entity coverage 98.9%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Entity matching during dataset construction is heuristic (Levenshtein distance + thresholds), resulting in some false positives/negatives; does not solve coreference/referring-expression generation; some lexicalization mistakes remained (authors note date ordering error caused by faulty entity matching).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Standard delexicalization is a common practice in data-to-text systems; here it is combined with a bracketed linearized plan and a copy-attention realizer — complementary to checklist and coverage mechanisms used in some end-to-end systems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Step-by-Step: Separating Planning from Realization in Neural Data-to-Text Generation', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The webnlg challenge: Generating text from dbpedia data <em>(Rating: 2)</em></li>
                <li>Data-to-text generation with content selection and planning <em>(Rating: 2)</em></li>
                <li>Challenges in data-to-document generation <em>(Rating: 2)</em></li>
                <li>Inducing document plans for concept-to-text generation <em>(Rating: 2)</em></li>
                <li>Probabilistic text structuring: Experiments with sentence ordering <em>(Rating: 1)</em></li>
                <li>Globally coherent text generation with neural checklist models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7181",
    "paper_id": "paper-7c593f8b5525851c2b453253835ddac6b1104a3b",
    "extraction_schema_id": "extraction-schema-135",
    "extracted_data": [
        {
            "name_short": "TextPlanTree",
            "name_full": "Ordered Sentence-Plan Tree (Text Plan)",
            "brief_description": "A symbolic, sentence-level plan that represents how an input RDF graph's facts are grouped into sentences and ordered; each sentence is an ordered, labeled tree whose arcs correspond to RDF triplets annotated with relation direction. The plan guarantees each input triplet is realized exactly once in some direction.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Ordered sentence-plan tree (Text Plan)",
            "representation_description": "Encodes the graph as a sequence of sentence plans; each sentence plan is a labeled, ordered tree with nodes = input entities and arcs = (head, (relation, direction), modifier). Chains and sibling structures represent shared-entity fact succession. A full text plan is a sequence of such sentence trees covering every RDF triplet exactly once (either as forward or reversed edge).",
            "representation_type": "hierarchical (ordered trees) / symbolic",
            "encoding_method": "Construct sentence trees by grouping RDF triplets into disjoint sentence-sets and arranging each set as an undirected graph then performing DFS traversals to produce ordered trees (exhaustive enumeration of possible trees via different DFS orders and starting nodes).",
            "canonicalization": false,
            "average_token_length": null,
            "dataset_name": "WebNLG",
            "task_name": "data-to-text generation / text planning",
            "model_name": "Non-neural symbolic planner (used with Plan-to-Text NMT)",
            "model_description": "Symbolic planner (deterministic procedure + product-of-experts ranking) that enumerates possible sentence-plan trees and ranks them using MLE-based experts; not a learned neural encoder — used to produce plans fed to a neural realizer.",
            "performance_metric": "Used indirectly; downstream evaluation measured with BLEU, METEOR, ROUGE_L, CIDEr and manual faithfulness counts",
            "performance_value": null,
            "impact_on_training": "By supplying explicit, faithful plans to the neural realizer, the decomposition improved downstream faithfulness (manual counts) and allowed the realizer to focus on short sentence-level mapping; enabled diverse realizations by selecting different high-scoring plans.",
            "limitations": "Plan construction is dataset-dependent (matching reference texts to plans succeeded for ~76% of training refs); exhaustive plan enumeration is combinatorially explosive (authors report O(2^{2n}+ n * n!) possible plans) making the approach infeasible for larger graphs without better search; plan design currently omits discourse-level referring-expression modeling.",
            "comparison_with_other": "Compared to end-to-end neural approaches, the explicit tree-based plan yields substantially better semantic faithfulness while maintaining fluency; it is more structured and verifiable than black-box encodings but less flexible in modeling discourse phenomena than systems that integrate document-level or coreference modeling (e.g., Puduppully et al., 2018).",
            "uuid": "e7181.0",
            "source_info": {
                "paper_title": "Step-by-Step: Separating Planning from Realization in Neural Data-to-Text Generation",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "LinearizedPlan",
            "name_full": "Preorder Linearized Text Plan (Bracketed Traversal)",
            "brief_description": "A sequential token representation of a sentence-plan tree produced for NMT: perform a pre-order traversal of the ordered tree and emit tokens with bracket markers to indicate tree structure; relations are encoded as direction token + relation token sequence; entities are replaced with unique entity tokens.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Preorder linearization with bracketed structure",
            "representation_description": "Each sentence-plan tree is linearized via pre-order traversal; brackets signal hierarchical subtree boundaries. Directed relations (r, d) are serialized as a small token sequence: a token indicating direction (e.g., arrow token) followed by relation token(s) (relation tokenized by splitting underscores/CamelCase). Recognized entities are replaced by single, entity-unique placeholder tokens to enable copying.",
            "representation_type": "hierarchical-&gt;sequential / token-based (linearized tree with structural markers)",
            "encoding_method": "Preorder (depth-first) traversal of the sentence tree; add explicit bracket tokens to mark tree structure; relations expressed as direction token + split relation tokens; entities delexicalized to single unique tokens.",
            "canonicalization": false,
            "average_token_length": null,
            "dataset_name": "WebNLG",
            "task_name": "plan-to-text realization (graph-&gt;text via linearized plan)",
            "model_name": "OpenNMT (vanilla NMT) with copy-attention and pretrained GloVe embeddings",
            "model_description": "Standard seq2seq NMT trained to translate linearized plans into surface text using OpenNMT toolkit, copy-attention mechanism (copy_attn flag), and GloVe 6B embeddings to initialize relation and text tokens.",
            "performance_metric": "Automatic: BLEU, METEOR, ROUGE_L, CIDEr (via nlg-eval); Manual: faithfulness counts (expressed/omitted/wrong-lex/over-generation); plan-realizer consistency (percent entities realized and order preserved).",
            "performance_value": null,
            "impact_on_training": "Feeding linearized, bracketed plans resulted in realizations that are as fluent as end-to-end NMT systems but more faithful to input facts; allowed reliable realization of high-scoring plans (e.g., seen-test best-plan: 98.9% of sentence plans realized with all requested entities and 100% preserved order).",
            "limitations": "Does not model discourse-level referring expressions (no coreference generation); linearization and delexicalization rely on entity-recognition heuristics during training (Levenshtein matching) causing imperfect plan-text alignment; scalability issues stem from upstream exhaustive plan generation rather than linearization itself.",
            "comparison_with_other": "Compared to black-box encoding of RDF triplets (e.g., flat triple sequences), this linearized-tree encoding preserves higher-level structuring decisions explicitly, improving faithfulness; orthogonal to copy/checklist mechanisms — those can augment the same linearized inputs but do not replace explicit planning.",
            "uuid": "e7181.1",
            "source_info": {
                "paper_title": "Step-by-Step: Separating Planning from Realization in Neural Data-to-Text Generation",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "EntityDelex",
            "name_full": "Entity-Unique Token Delexicalization",
            "brief_description": "A preprocessing representation that replaces recognized entities in linearized plans with single, unique entity tokens so the neural realizer can copy or re-insert full entity strings at post-processing time.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Entity-unique token delexicalization",
            "representation_description": "Entities identified in the reference texts are replaced with single, unique placeholder tokens in the plan input to the NMT; after generation, placeholders are replaced with the original entity strings from the input graph. Dates and numeric units are lexicalized with deterministic post-processing rules.",
            "representation_type": "token-based / delexicalized",
            "encoding_method": "Greedy string/entity recognition in references (Levenshtein token matching + tuned thresholds and chrono-python for dates) to map textual mentions to graph entities; replace matched mentions with unique entity tokens in both plans and reference targets during training.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "WebNLG",
            "task_name": "plan-to-text realization / delexicalized NMT training",
            "model_name": "OpenNMT with copy-attention (same as LinearizedPlan realizer)",
            "model_description": "Seq2seq NMT with copy mechanism that benefits from delexicalized entity tokens which enable copying of entity placeholders rather than generating long or variable entity strings.",
            "performance_metric": "Indirect: improves copy behavior and reduces lexical errors; measured via manual wrong-lexicalization counts and plan-realizer entity realization percentages.",
            "performance_value": null,
            "impact_on_training": "Enables robust copying of entities and reduces vocabulary sparsity of entity names; contributed to high entity-realization rates (e.g., seen best-plan entity coverage 98.9%).",
            "limitations": "Entity matching during dataset construction is heuristic (Levenshtein distance + thresholds), resulting in some false positives/negatives; does not solve coreference/referring-expression generation; some lexicalization mistakes remained (authors note date ordering error caused by faulty entity matching).",
            "comparison_with_other": "Standard delexicalization is a common practice in data-to-text systems; here it is combined with a bracketed linearized plan and a copy-attention realizer — complementary to checklist and coverage mechanisms used in some end-to-end systems.",
            "uuid": "e7181.2",
            "source_info": {
                "paper_title": "Step-by-Step: Separating Planning from Realization in Neural Data-to-Text Generation",
                "publication_date_yy_mm": "2019-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The webnlg challenge: Generating text from dbpedia data",
            "rating": 2
        },
        {
            "paper_title": "Data-to-text generation with content selection and planning",
            "rating": 2
        },
        {
            "paper_title": "Challenges in data-to-document generation",
            "rating": 2
        },
        {
            "paper_title": "Inducing document plans for concept-to-text generation",
            "rating": 2
        },
        {
            "paper_title": "Probabilistic text structuring: Experiments with sentence ordering",
            "rating": 1
        },
        {
            "paper_title": "Globally coherent text generation with neural checklist models",
            "rating": 1
        }
    ],
    "cost": 0.011261,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Step-by-Step: Separating Planning from Realization in Neural Data-to-Text Generation*</h1>
<p>Amit Moryossef ${ }^{\dagger}$ Yoav Goldberg ${ }^{\dagger \ddagger}$ Ido Dagan ${ }^{\dagger}$<br>amitmoryossef@gmail.com, {yogo,dagan}@cs.biu.ac.il<br>${ }^{\dagger}$ Bar Ilan University, Ramat Gan, Israel<br>${ }^{\ddagger}$ Allen Institute for Artificial Intelligence</p>
<h4>Abstract</h4>
<p>Data-to-text generation can be conceptually divided into two parts: ordering and structuring the information (planning), and generating fluent language describing the information (realization). Modern neural generation systems conflate these two steps into a single end-to-end differentiable system. We propose to split the generation process into a symbolic text-planning stage that is faithful to the input, followed by a neural generation stage that focuses only on realization. For training a plan-to-text generator, we present a method for matching reference texts to their corresponding text plans. For inference time, we describe a method for selecting high-quality text plans for new inputs. We implement and evaluate our approach on the WebNLG benchmark. Our results demonstrate that decoupling text planning from neural realization indeed improves the system's reliability and adequacy while maintaining fluent output. We observe improvements both in BLEU scores and in manual evaluations. Another benefit of our approach is the ability to output diverse realizations of the same input, paving the way to explicit control over the generated text structure.</p>
<h2>1 Introduction</h2>
<p>Consider the task of data-to-text generation, as exemplified in the WebNLG corpus (Colin et al., 2016). The system is given a set of RDF triplets describing facts (entities and relations between them) and has to produce a fluent text that is faithful to the facts. An example of such triplets is:</p>
<p>John, birthPlace, London
John, employer, IBM
With a possible output:</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>1. John, who was born in London, works for IBM.</p>
<p>Other outputs are also possible:
2. John, who works for IBM, was born in London.
3. London is the birthplace of John, who works for IBM.
4. IBM employs John, who was born in London.</p>
<p>These variations result from different ways of structuring the information: choosing which fact to mention first, and in which direction to express each fact. Another choice is to split the text into two different sentences, e.g.,</p>
<h2>5. John works for IBM. John was born in London.</h2>
<p>Overall, the choice of fact ordering, entity ordering, and sentence splits for these facts give rise to 12 different structures, each of them putting the focus on somewhat different aspect of the information. Realistic inputs include more than two facts, greatly increasing the number of possibilities.</p>
<p>Another axis of variation is in how to verbalize the information for a given structure. For example, (2) can also be verbalized as</p>
<p>2a. John works for IBM and was born in London.
and (5) as:
5a. John is employed by IBM. He was born in London.
We refer to the first set of choices (how to structure the information) as text planning and to the second (how to verbalize a plan) as plan realization. ${ }^{\dagger}$</p>
<p>The distinction between planning and realization is at the core of classic natural language generation (NLG) works (Reiter and Dale, 2000; Gatt and Krahmer, 2017). However, a recent wave of neural NLG systems ignores this distinction</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>and treat the problem as a single end-to-end task of learning to map facts from the input to the output text (Gardent et al., 2017; Dušek et al., 2018). These neural systems encode the input facts into an intermediary vector-based representation, which is then decoded into text. While not stated in these terms, the neural system designers hope for the network to take care of both the planning and realization aspect of text generation. A notable exception is the work of Puduppully et al. (2018), who introduce a neural content-planning module in the end-to-end architecture.</p>
<p>While the neural methods achieve impressive levels of output fluency, they also struggle to maintain coherency on longer texts (Wiseman et al., 2017), struggle to produce a coherent order of facts, and are often not faithful to the input facts, either omitting, repeating, hallucinating or changing facts (the NLG community refers to such errors as errors in adequacy or correctness of the generated text). When compared to templatebased methods, the neural systems win in fluency but fall short regarding content selection and faithfulness to the input (Puzikov and Gurevych, 2018). Also, they do not allow control over the output's structure. We speculate that this is due to demanding too much of the network: while the neural system excels at capturing the language details required for fluent realization, they are less well equipped to deal with the higher levels text structuring in a consistent and verifiable manner.</p>
<p>Proposal we propose an explicit, symbolic, text planning stage, whose output is fed into a neural generation system. The text planner determines the information structure and expresses it unambiguously-in our case as a sequence of ordered trees. This stage is performed symbolically and is guaranteed to remain faithful and complete with regards to the input facts. Once the plan is determined, ${ }^{2}$ a neural generation system is used to transform it into fluent, natural language text. By being able to follow the plan structure closely, the network is alleviated from the need to determine higher-level structural decisions and can track what was already covered more easily. This allows the network to perform the task it excels in, producing fluent, natural language outputs.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>We demonstrate our approach on the WebNLG corpus and show it results in outputs which are as fluent as neural systems, but more faithful to the input facts. The method also allows explicit control of the output structure and the generation of diverse outputs (some diversity examples are available in the Appendix). We release our code and the corpus extended with matching plans in https://github.com/ AmitMY/chimera.</p>
<h2>2 Overview of the Approach</h2>
<p>Task Description Our method is concerned with the task of generating texts from inputs in the form of RDF sets. Each input can be considered as a graph, where the entities are nodes, and the RDF relations are directed labeled edges. Each input is paired with one or more reference texts describing these triplets. The reference can be either a single sentence or a sequence of sentences. Formally, each input $G$ consists of a set of triplets of the form $\left(s_{i}, r_{i}, o_{i}\right)$, where $s_{i}, o_{i} \in V$ ("subject" and "object") correspond to entities from DBPedia, and $r_{i} \in R$ is a labeled DBPedia relation ( $V$ and $R$ are the sets of entities and relations, respectively). For example, Figure 1a shows a triplet set $G$ and Figure 1d shows a reference text. We consider the data set as a set of input-output pairs $(G, \mathrm{ref})$, where the same $G$ may appear in several pairs, each time with a different reference.</p>
<p>Method Overview We split the generation process into two parts: text planning and sentence realization. Given an input $G$, we first generate a text plan plan $(G)$ specifying the division of facts to sentences, the order in which the facts are expressed in each sentence, and the ordering of the sentences. This data-to-plan step is non-neural (Section 3). Then, we generate each sentence according to the plan. This plan-to-sentence step is achieved through an NMT system (Section 4).</p>
<p>Figure 1 demonstrates the entire process.
To facilitate our plan-based architecture, we devise a method to annotate $(G, r e f)$ pairs with the corresponding plans (Section 3.1), and use it to construct a dataset which is used to train the plan-to-text translation. The same dataset is also used to devise a plan selection method (Section 3.2).</p>
<p>General Applicability It is worth considering the dataset-specific vs. general applicability aspects of our method. On the low-level details, this</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" />
(c) Linearization of the text plan
A.T.. Charlie Johnson $\leftarrow$ editor [ AIP_Advances $\rightarrow$ issn number [ 2158-3226 ] ] .
A.T.. Charlie_Johnson $\rightarrow$ residence [ United_States ] $\rightarrow$ alma mater [ Harvard_University ]
(d) Possible output sentence
A.T. Charlie Johnson is the editor of AIP Advances which has the ISSN number 2158-3226.</p>
<p>He lives in the United States, and graduated from Harvard University.
Figure 1: Summary of our proposed generation process: the planner takes the input RDF triplets in (a), and generates the explicit plan in (b). The plan is then linearized (c) and passed to a neural generation system, producing the output (d).
work is very much dataset dependent. We show how to represent plans for specific datasets, and, importantly for this work, how to automatically construct plans for this dataset given inputs and expected natural language outputs. The method of plan construction will likely not generalize "as is" to other datasets, and the plan structure itself may also be found to be lacking for more demanding generation tasks. However, on a higher level, our proposal is very general: intermediary plan structures can be helpful, and one should consider ways of obtaining them, and of using them. In the short term, this will likely take the form of ad-hoc explorations of plan structures for specific tasks, as we do here, to establish their utility. In the longer term, research may evolve to looking into how general-purpose plan are structured. Our main message is that the separation of planning from realization, even in the context of neural generation, is a useful one to be considered.</p>
<h2>3 Text Planning</h2>
<p>Plan structure Our text plans capture the division of facts to sentences and the ordering of the sentences. Additionally, for each sentence, the plan captures (1) the ordering of facts within the sentence; (2) The ordering of entities within a fact, which we call the direction of the relation. For example, the ${\mathrm{A}$, location, B$}$ relation can be expressed as either $A$ is located in $B$ or $B$ is the location of $A$; (3) the structure between facts that share an entity, namely chains and sibling struc-
tures as described below.
<img alt="img-1.jpeg" src="img-1.jpeg" />
(c) Combination: John lives in London, the capital of England, and works as a bartender.</p>
<p>Figure 2: Fact construction structure.
A text plan is modeled as a sequence of sentence plans, to be realized in order. Each sentence plan is modeled as an ordered tree, specifying the structure in which the information should be realized. Structuring each sentence as a tree enables a clear succession between different facts through shared entities. Our text-plan design assumes that each entity is mentioned only once in a sentence, which holds in the WebNLG corpus. The ordering of the entities and relations within a sentence is determined by a pre-order traversal of the tree.</p>
<p>Figure 1b shows an example of a text plan. Formally, given the input $G$, a text plan $T$ is a sequences of sentence plans $T=s_{1}, \ldots, s_{N_{T}}$. A sen-</p>
<p>tence plan $s$ is a labeled, ordered tree, with arcs of the form $(h, \ell, m)$, where $h, m \in V$ are head and modifier nodes, each corresponding to an input entity, and $\ell=(r, d)$ is the relation between nodes, where $r \in R$ is the RDF relation, and $d \in{\rightarrow, \leftarrow}$ denotes the direction in which the relation is expressed: $d=\rightarrow$ if $(h, r, m) \in G$, and $d=\leftarrow$ if $(m, r, h) \in G$. A text plan $T$ is said to match an input $G$ iff every triplet $(s, r, o)$ in $G$ is expressed in $T$ exactly once, either as an edge $(s,(r, \rightarrow), o)$ or as an edge $\left(o,\left(r_{i}, \leftarrow\right), s\right)$.</p>
<p>Chains $\left(h, \ell_{1}, m\right),\left(m, \ell_{2}, x\right)$ represent a succession of facts that share a middle entity (Figure 2a), while siblings - nodes with the same parent $-\left(h, \ell_{1}, m_{1}\right),\left(h, \ell_{2}, m_{2}\right)$ represents a succession of facts about the same entity (Figure 2b). Sibling and chain structures can be combined (Figure 2c). An example of an input we addressed in the WebNLG corpus, and matching text plan is given in Figure 1b.
Exhaustive generation For small-ish input graphs $G$-such as those in the WebNLG task we consider here-it is trivial to generate all possible plans by first considering all the ways of grouping the input into sets, then from each set generating all possible trees by arranging it as an undirected graph and performing several DFS traversals starting from each node, where each DFS traversal follows a different order of children. ${ }^{3}$</p>
<h3>3.1 Adding Plans to Training Data</h3>
<p>While the input RDFs and references are present in the training dataset, the plans are not. We devise a method to recover the latent plans for most of the input-reference pairs in the training set, constructing a new dataset of $(G, r e f, T)$ triplets of inputs, reference texts, and corresponding plans.</p>
<p>We define the reference ref, and the text-plan $T$ to be consistent with each other iff (a) they exhibit the same splitting into sentences-the facts in every sentence in ref are grouped as a sentence plan in $T$, and (b) for each corresponding sentence and sentence-plan, the order of the entities is identical.</p>
<p>The matching of plans to references is based on the observations that (a) it is relatively easy to identify entities in the reference texts, and a pair of entities in an input is unique to a fact; (b) it is relatively easy to identify sentence splits; (c) a reference text and its matching plan must share the</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>same entities in the same order, and with the same sentence splits.
Sentence split consistency We define a set of triplets to be potentially consistent with a sentence iff each triplet contains at least one entity from the sentence (either its subject or object appear in the sentence), and each entity in the sentence is covered by at least one triplet. Given a reference text, we split it into sentences using NLTK (Bird and Loper, 2004), and look for divisions of $G$ into disjoint sets such that each set is consistent with a corresponding sentence. For each such division, we consider the exhaustive set of all induced plans. Facts order consistency A natural criterion would be to consider a reference sentence and a sentence-plan originating from the corresponding RDF as matching iff the sets of entities in the sentence and the plan are identical, and all entities appear in the same order. ${ }^{4}$ Based on this, we could represent each sentence and each plan as a sequence of entities, and verify the sequences match.</p>
<p>However, using this criterion is complicated by the fact that it is not trivial to map between the entities in the plan (that originate from the RDF triplets) and the entities in the text. In particular, due to language variability, the same plan entity may appear in several forms in the textual sentences. Some of these variations (i.e. "A.F.C Fylde" vs. "AFC Fylde") can be recognized heuristically, while others require external knowledge ("UK conservative party" vs. "the Tories"), and some are ambiguous and require full-fledged co-reference resolution ("them", "he", "the former"). Hence, we relax our matching criterion to allow for possible unrecognized entities in the text.</p>
<p>Concretely, we represent each sentence plan as a sequence of its entities $\left(p e_{1}, \ldots, p e_{k}\right)$, and each sentence as the sequence of its entities which we managed to recognize and to match with an input entity $\left(s e_{1}, \ldots, s e_{m}\right), m \leq k .^{5}$</p>
<p>We then consider a sentence and a sentenceplan to be consistent if the following two condi-</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>tions hold: (1) The sentence entities $\left(s e_{1}, \ldots, s e_{m}\right)$ are a proper sub-sequence of the plan entities $\left(p e_{1}, \ldots, p e_{k}\right)$; and (2) each of the remaining entities in the plan already appeared previously in the plan. The second condition accounts for the fact that most un-identified entities are due to pronouns and similar non-lexicalized referring expressions, and that these only appear after a previous occurrence of the same entity in the text. ${ }^{6}$</p>
<h3>3.2 Test-time Plan Selection</h3>
<p>To select the plan to be realized, we propose a mechanism for ranking the possible plans. Our plan scoring method is a product-of-experts model, where each expert is a conditional probability estimate for some property of the plan. The conditional probabilities are MLE estimates based on the plans in the training set constructed in section 3.1. Estimates involving relation names are smoothed using Lidstone smoothing to account for unseen relations. We use the following experts:
Relation direction For every relation $r \in R$, we compute its probability to be expressed in the plan in its original order $(d=\rightarrow)$ or in the reverse order $(d=\leftarrow): p_{\text {dir }}(d=\rightarrow \mid R)$. This captures the tendency of certain relations to be realized in the reversed order to how they are defined in the knowledge base. For example, in the WebNLG corpus the relation "manager" is expressed as a variation of "is managed by" instead of one of "is the manager of" in $68 \%$ of its occurrences $\left(p_{\text {dir }}(d=\leftarrow \mid\right.$ manager $)=0.68$ ).
Global direction We find that while the probability of each relation to be realized in a reversed order is usually below 0.5 , still in most plans of longer texts there are one or two relations that appear in the reversed order. We capture this tendency using an expert that considers the conditional probability $p_{g d}(n r=n||G|)$ of observing $n$ reversed edges in an input with $|G|$ triplets.
Splitting tendencies For each input size, we keep track of the possible ways in which the set of facts can be split to subsets of particular sizes. That is, we keep track of probabilities such as $p_{s}(s=[3,2,2] \mid 7)$ of realizing an input of 7 RDF triplets as three sentences, each realizing the corresponding number of facts.
Relation transitions We consider each sentence plan as a sequence of the relation types expressed</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>in it $r_{1}, \ldots, r_{k}$ followed by an EOS symbol, and compute the markov transition probabilities over this sequence: $p_{\text {trans }}\left(r_{1}, r_{2}, \ldots, r_{k}, E O S\right)=$ $\prod_{i=1, k} p_{t}\left(r_{i+1} \mid r_{i}\right)$. The expert is the product of the transition probabilities of the individual sentence plans in the text plan. This captures the tendencies of relations to follow each other and in particular, the tendencies of related relations such as birth-place and birth-date to group, allowing their aggregation in the generated text (John was born in London on Dec 12th, 1980).</p>
<p>Each of the possible plans are then scored based on the product of the above quantities. ${ }^{7}$</p>
<p>The scores work well for separating good from lousy text plans, and we observe a threshold above which most generated plans result in adequate texts. We demonstrate in Section 6 that realizing highly-ranked plans manages to obtain good automatic realization scores. We note that the plan in Figure 1b is the one our ranking algorithm ranked first for the input in Figure 1a.</p>
<p>Possible Alternatives In addition to the single plan selection, the explicit planning stage opens up additional possibilities. Instead of choosing and realizing a single plan, we can realize a diverse set of high-scoring plans, or realizing a random high-scoring plan, resulting in a diverse and less templatic set of texts across runs. This relies on the combination of two factors: the ability of the scoring component to select plans that correspond to plausible human-authored texts, and the ability of the neural realizer to faithfully realize the plan into fluent text. While it is challenging to directly evaluate the plans adequacy, we later show an evaluation of the plan realization component. Figure 3 shows three random plans for the same graph and their realizations. Further examples of the diversity of generation are given in the appendix.</p>
<p>The explicit and symbolic planning stage also allows for user control over the generated text, either by supplying constraints on the possible plans (e.g., number of sentences, entities to focus on, the order of entities/relations, or others) or by supplying complete plans. We leave these options for future work.</p>
<p><sup id="fnref3:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>(a) Dessert $\leftarrow$ course [ Bionico $\rightarrow$ country [ Mexico ] $\rightarrow$ ingredient [ Granola ] $\rightarrow$ region [ Jalisco ] ]
The Dessert Bionico requires Granola as one of its ingredients and originates from the Jalisco region of Mexico .
Bionico $\rightarrow$ country [ Mexico ] $\rightarrow$ region [ Jalisco ] .
(b) Dessert $\leftarrow$ course [ Bionico $\rightarrow$ ingredient [ Granola ] ] Bionico is a food found in the Mexico region Jalisco. The Dessert Bionico requires Granola as an ingredient.</p>
<p>Bionico $\rightarrow$ ingredient [ Granola ] $\rightarrow$ course [ Dessert ] . Bionico $\rightarrow$ region [ Jalisco ] $\rightarrow$ country [ Mexico ] Bionico contains Granola and is served as a Dessert. Bionico is a food found in the region of Jalisco, Mexico</p>
<p>Figure 3: Three random linearized plans for the same input graph, and their text realizations. All taken from the top $10 \%$ scoring plans. (a) structures the output as a single sentence, while (b) and (c) as two sentences. The second sentence in (b) puts emphasis on Bionico being a dessert, while in (c) the emphasis is on the ingredients.</p>
<h2>4 Plan Realization</h2>
<p>For plan realization, we use an off-the-shelf vanilla neural machine translation (NMT) system to translate plans to texts. The explicit division to sentences in the text plan allows us to realize each sentence plan individually which allows the realizer to follow the plan structure within each (rather short) sentence, reducing the amount of information that the model needs to remember. As a result, we expect a significant reduction in over- and under-generation of facts, which are common when generating longer texts. Currently, this comes at the expense of not modeling discourse structure (i.e., referring expressions). This deficiency may be handled by integrating the discourse into the text plan, or as a post-processing step. ${ }^{8}$. We leave this for future work.</p>
<p>To use text plans as inputs to the NMT, we linearize each sentence plan by performing a preorder traversal of the tree, while indicating the tree structure with brackets (Figure 1c). The directed relations $(r, d)$ are expressed as a sequence of two or more tokens, the first indicating the direction and the rest expressing the relation. ${ }^{9}$ Entities that are identified in the reference text are replaced with single, entity-unique tokens. This allows the NMT system to copy such entities from the input rather than generating them. Figure 1d is an example of possible text resulting from such linearization.</p>
<p>Training details We use a standard NMT setup with a copy-attention mechanism (Gulcehre et al., 2016) ${ }^{10}$ and the pre-trained GloVe. 6 B word em-</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>beddings ${ }^{11}$ (Pennington et al., 2014). The pretrained embeddings are used to initialize the relation tokens in the plans, as well as the tokens in the reference texts.</p>
<p>Generation details We translate each sentence plan individually. Once the text is generated, we replace the entity tokens with the full entity string as it appears in the input graph, and lexicalize all dates as Month DAY+ordinal, YEAR (i.e., July 4th, 1776) and for numbers with units (i.e., "5"(minutes)) we remove the parenthesis and quotation marks (5 minutes).</p>
<h2>5 Experimental Setup</h2>
<p>The WebNLG challenge (Colin et al., 2016) consists of mapping sets of RDF triplets to text including referring expression generation, aggregation, lexicalization, surface realization, and sentence segmentation. It contains sets with up to 7 triplets each along with one or more reference texts for each set. The test set is split into two parts: seen, containing inputs created for entities and relations belonging to DBpedia categories that were seen in the training data, and unseen, containing inputs extracted for entities and relations belonging to 5 unseen categories. While the unseen category is conceptually appealing, we view the seen category as the more relevant setup: generating fluent, adequate and diverse text for a mix of known relation types is enough of a challenge also without requiring the system to invent verbalizations for unknown relation types. Any realistic generation system could afford to provide at least a few verbalizations for each relation of interest. We thus focus our attention mostly on the seen case (though our system does also perform well on the unseen case).</p>
<p>Following Section 3.1, we manage to match a detailed in the appendix.
${ }^{11}$ nlp.stanford.edu/data/glove.6B.zip</p>
<p>consistent plan for $76 \%$ of the reference texts and use these plan-text pairs to train the plan realization NMT component. Overall, the WebNLG training set contains 18, 102 RDF-text pairs while our plan-enhanced corpus contains 13,828 plantext pairs. ${ }^{12}$</p>
<p>Compared Systems We compare to the best submissions in the WebNLG challenge (Gardent et al., 2017): Melbourne, an end-to-end system that scored best on all categories in the automatic evaluation, and UPF-FORGe (Mille et al., 2017), a classic grammar-based NLG system that scored best in the human evaluation.</p>
<p>Additionally, we developed an end-to-end neural baseline which outperforms the WebNLG neural systems. It uses a set encoder, an LSTM (Hochreiter and Schmidhuber, 1997) decoder with attention (Bahdanau et al., 2014), a copy-attention mechanism (Gulcehre et al., 2016) and a neural checklist model (Kiddon et al., 2016), as well as applying entity dropout. The entity-dropout and checklist component are the key differentiators from previous systems. We refer to this system as StrongNeural.</p>
<h2>6 Experiments and Results</h2>
<h3>6.1 Automatic Metrics</h3>
<p>We begin by comparing our plan-based system (BestPlan) to the state-of-the-art using the common automatic metrics: BLEU (Papineni et al., 2002), Meteor (Banerjee and Lavie, 2005), ROUGE $_{L}$ (Lin, 2004) and CIDEr (Vedantam et al., 2015), using the nlg-eval ${ }^{13}$ tool (Sharma et al., 2017) on the entire test set and on each part separately (seen and unseen).</p>
<p>In the original challenge, the best performing system in automatic metric was based on end-toend NMT (Melbourne). Both the StrongNeural and BestPlan systems outperform all the WebNLG participating systems on all automatic metrics (Table 1). BestPlan is competitive with StrongNeural in all metrics, with small differences either way per metric. ${ }^{14}$</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 1: Results for all categories. Team color indicates the type of system used (NMT<em>, Rule-Based ${ }^{</em>}$, RuleBased + NMT ${ }^{\star}$ ).</p>
<h3>6.2 Manual Evaluation</h3>
<p>Next, we turn to manually evaluate our system's performance regarding faithfulness to the input on the one hand and fluency on the other. We describe here the main points of the manual evaluation setup, with finer details in the appendix.</p>
<p>Faithfulness As explained in Section 3, the first benefit we expect of our plan-based architecture is to make the neural systems task simpler, helping it to remain faithful to the semantics expressed in the plan which in turn is guaranteed to be faithful to the original RDF input (by faithfulness, we mean expressing all facts in the graph and only facts from the graph: not dropping, repeating or hallucinating facts). We conduct a manual evaluation over the seen portion of the WebNLG human evaluated test set (139 input sets). We compare BestPlan and StrongNeural. ${ }^{15}$ For each output text, we manually mark which relations are expressed in it, which are omitted, and which relations exist with the wrong lexicalization. We also count the number of relations the system over generated, either repeating facts or inventing new facts. ${ }^{16}$</p>
<p>Table 2 shows the results. BestPlan reduces all error types compared to StrongNeural, by $85 \%$, $56 \%$ and $90 \%$ respectively. While on-par regarding automatic metrics, BestPlan substantially outperforms the new state-of-the-art end-to-end neural system in semantic faithfulness.</p>
<p>For example, Figure 4 compares the output of</p>
<p><sup id="fnref4:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">1. William_Anders</th>
<th style="text-align: left;">dateOfRetirement</th>
<th style="text-align: left;">"1969-09-01"</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">2. William_Anders</td>
<td style="text-align: left;">was selected by NASA</td>
<td style="text-align: left;">1963</td>
</tr>
<tr>
<td style="text-align: left;">3. William_Anders</td>
<td style="text-align: left;">timeInSpace</td>
<td style="text-align: left;">"8820.0"(minutes)</td>
</tr>
<tr>
<td style="text-align: left;">4. William_Anders</td>
<td style="text-align: left;">birthDate</td>
<td style="text-align: left;">"1933-10-17"</td>
</tr>
</tbody>
</table>
<ol>
<li>William_Anders | occupation | Fighter.pilot</li>
<li>William_Anders | birthPlace | British_Hong_Kong</li>
<li>William_Anders | was a crew member of | Apollo_8
(a) The last RDF in the seen test-set</li>
</ol>
<p>William Anders was born on October 17th, 1933 in British Hong Kong.
He was selected by nasa in $\mathbf{1 9 6 3}$ and became a crew member on the Apollo 8 flight mission.
He retired on September 1st, 1969.
(b) Output from StrongNeural</p>
<p>William Anders was a fighter pilot who joined nasa in 1963 and served as a crew member of Apollo 8.
William Anders retired on September 1st, 1969 and spent $\mathbf{8 8 2 0 . 0}$ minutes in space.
William Anders was born in British Hong Kong on october October 17th, 1933.
(c) Output from BestPlan</p>
<p>Figure 4: Comparing end-to-end neural generation with our plan based system.</p>
<p>StrongNeural (4b) and BestPlan (4c) on the last input in the seen test set (4b). While both systems chose three sentences split and aggregated details about birth in one sentence and details about the occupation in another, StrongNeural also expressed the information in chronological order. However, StrongNeural failed to generate facts 3 and 5. BestPlan made a lexicalization mistake in the third sentence by expressing "October" before the actual date, which is probably caused by faulty entity matching for one of the references, and (by design) did not generate any referring expression, which we leave for future work.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">BestPlan</th>
<th style="text-align: left;">StrongNeural</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Expressed</td>
<td style="text-align: left;">417</td>
<td style="text-align: left;">360</td>
</tr>
<tr>
<td style="text-align: left;">Omitted</td>
<td style="text-align: left;">6</td>
<td style="text-align: left;">41</td>
</tr>
<tr>
<td style="text-align: left;">Wrong-lexicalization</td>
<td style="text-align: left;">17</td>
<td style="text-align: left;">39</td>
</tr>
<tr>
<td style="text-align: left;">Over-generation</td>
<td style="text-align: left;">3</td>
<td style="text-align: left;">29</td>
</tr>
</tbody>
</table>
<p>Table 2: Semantic faithfulness of each system regarding 440 RDF triplets from 139 input sets in the seen part of the manually evaluated test set.</p>
<p>Fluency Next, we assess whether our systems succeed at maintaining the high-quality fluency of the neural systems. We perform pairwise evaluation via Amazon Mechanical Turk wherein each task the worker is presented with an RDF set (both in a graph form, and textually), and two texts in random order, one from BestPlan, the other from a competing system. We compare BestPlan against a strong end-to-end neural system (StrongNeural), a grammar-based system which</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">StrongNeural</th>
<th style="text-align: left;">Reference</th>
<th style="text-align: left;">UPF-FORGe</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">BestPlan</td>
<td style="text-align: left;">$-0.6 \%$</td>
<td style="text-align: left;">$-5.4 \%$</td>
<td style="text-align: left;">$+5.1 \%$</td>
</tr>
</tbody>
</table>
<p>Table 3: MTurk average worker score for BestPlan compared to each system. It is a worse than the reference texts, on-par with the neural end-to-end system, and a better than the previous state-of-the-art.
is the state-of-the-art in human evaluation (UPFFORGe), and the human-supplied WebNLG references (Reference). The workers were presented with three possible answers: BestPlan text is better (scored as 1), the other text is better (scored as -1 ), and both texts are equally fluent (scored as 0 ). Table 3 shows the average worker score given to each pair divided by the number of texts compared. BestPlan performed on-par with StrongNeural, and surpassed the previous state-of-the-art UPF-FORGe. It, however, scored worse than the reference texts, which is expected given that it does not produce referring expressions. Our approach manages to keep the same fluency level typical to end-to-end neural systems, thanks to the NMT realization component.</p>
<h3>6.3 Plan Realization Consistency</h3>
<p>We test the extent to which the realizer generates texts that are consistent with the plans. For several subsets of ranked plans (best plan, top $1 \%$, and top $10 \%$ ) for the seen and unseen test sets separately, we realize up to 100 randomly selected text-plans per input. We realize each sentence plan and evaluate using two criteria: (1) Do all entities from the plan appear in the realization; (2) Like the consis-</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Best Plan</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Top 1\% Plans</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Top 10\% Plans</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Entities</td>
<td style="text-align: center;">Order</td>
<td style="text-align: center;">Entities</td>
<td style="text-align: center;">Order</td>
<td style="text-align: center;">Entities</td>
<td style="text-align: center;">Order</td>
</tr>
<tr>
<td style="text-align: left;">Seen</td>
<td style="text-align: center;">$98.9 \%$</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$95.9 \%$</td>
<td style="text-align: center;">$99.9 \%$</td>
<td style="text-align: center;">$93.6 \%$</td>
<td style="text-align: center;">$100 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Unseen</td>
<td style="text-align: center;">$66.7 \%$</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$45.3 \%$</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$41.3 \%$</td>
<td style="text-align: center;">$100 \%$</td>
</tr>
</tbody>
</table>
<p>Table 4: Surface realizer performance. Entities: Percent of sentence plans that were realized with all the requested entities. Order: of the sentences that were realized with all requested entities, percentage of realizations that followed the requested entity order.
tency we defined above, do all entities appear in the same order in the plan and the realization.</p>
<p>Table 4 indicates that for decreasingly probable plans our realizer does worse in the first criterion. However, for both parts of the test set, if the realizer managed to express all of the entities, it expressed them in the requested order, meaning the outputs are consistent with plans. This opens up a potential for user control and diverse outputs, by choosing different plans for realization.</p>
<p>Finally, we verify that the realization of potentially diverse plans is not only consistent with each given plan but also preserves output quality. For each input, we realize a random plan from the top $10 \%$. We repeat this process three times with different random seeds to generate different outputs, and mark these systems as RandomPlan-1/2/3. Table 1 shows that these random plans maintain decent quality on the automatic metrics, with a limited performance drop, and the automatic score is stable across random seeds. ${ }^{17}$</p>
<h2>7 Related Work</h2>
<p>Text planning is a major component in classic NLG. For example, Stent et al. (2004) shows a method of producing coherent sentence plans by exhaustively generating as many as 20 sentence plan trees for each document plan, manually tagging them, and learning to rank them using the RankBoost algorithm (Schapire, 1999). Our planning approach is similar, but we only have a set of "good" reference plans without internal ranks. While the sentence planning decides on the aggregation, one crucial decision left is sentence order. We currently determine order based on a splitting heuristic which relies on the number of facts in every sentence, not on the content. Lapata (2003) devised a probabilistic model for sentence ordering which correlated well with human ordering. Our</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup>plan selection procedure is admittedly simple, and can be improved by integrating insights from previous text planning works (Barzilay and Lapata, 2006; Konstas and Lapata, 2012, 2013).</p>
<p>Many generation systems (Gardent et al., 2017; Dušek et al., 2018) are based on a black-box NMT component, with various pre-processing transformation of the inputs (such as delexicalization) and outputs to aid the generation process.</p>
<p>Generation from structured data often requires referring to a knowledge base (Mei et al., 2015; Kiddon et al., 2016; Wen et al., 2015). This led to input-coverage tracking neural components such as the checklist model (Kiddon et al., 2016) and copy-mechanism (Gulcehre et al., 2016). Such methods are effective for ensuring coverage and reducing the number of over-generated facts and are in some ways orthogonal to our approach. While our explicit planning stage reduces the amount of over-generation, our realizer may be further improved by using a checklist model.</p>
<p>More complex tasks, like RotoWire (Wiseman et al., 2017) require modeling also document-level planning. Puduppully et al. (2018) explored a method to explicitly model document planning using the attention mechanism.</p>
<p>The neural text generation community has also recently been interested in "controllable" text generation (Hu et al., 2017), where various aspects of the text (often sentiment) are manipulated (Ficler and Goldberg, 2017) or transferred (Shen et al., 2017; Zhao et al., 2017; Li et al., 2018). In contrast, like in (Wiseman et al., 2018), here we focused on controlling either the content of a generation or the way it is expressed by manipulating the sentence plan used in realizing the generation.</p>
<h2>8 Conclusion</h2>
<p>We proposed adding an explicit symbolic planning component to a neural data-to-text NLG system, which eases the burden on the neural component concerning text structuring and fact tracking. Consequently, while the plan-based system performs on par with a strong end-to-end neural system regarding automatic evaluation metrics and human fluency evaluation, it substantially outperforms the end-to-end system regarding faithfulness to the input. Additionally, the planning stage allows explicit user-control and generating diverse sentences, to be pursued in future work.</p>
<h2>References</h2>
<p>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473.</p>
<p>Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, pages 65-72.</p>
<p>Regina Barzilay and Mirella Lapata. 2006. Aggregation via set partitioning for natural language generation. In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, pages 359-366. Association for Computational Linguistics.</p>
<p>Steven Bird and Edward Loper. 2004. Nltk: the natural language toolkit. In Proceedings of the ACL 2004 on Interactive poster and demonstration sessions, page 31. Association for Computational Linguistics.</p>
<p>Emilie Colin, Claire Gardent, Yassine Mrabet, Shashi Narayan, and Laura Perez-Beltrachini. 2016. The webnlg challenge: Generating text from dbpedia data. In Proceedings of the 9th International Natural Language Generation conference, pages 163167.</p>
<p>Ondřej Dušek, Jekaterina Novikova, and Verena Rieser. 2018. Findings of the e2e nlg challenge. arXiv preprint arXiv:1810.01170.</p>
<p>Thiago Castro Ferreira, Diego Moussallem, Ákos Kádár, Sander Wubben, and Emiel Krahmer. 2018. Neuralreg: An end-to-end approach to referring expression generation. arXiv preprint arXiv:1805.08093.</p>
<p>Jessica Ficler and Yoav Goldberg. 2017. Controlling linguistic style aspects in neural language generation. arXiv preprint arXiv:1707.02633.</p>
<p>Claire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. 2017. The webnlg challenge: Generating text from rdf data. In Proceedings of the 10th International Conference on Natural Language Generation, pages 124-133.</p>
<p>Albert Gatt and Emiel Krahmer. 2017. Survey of the state of the art in natural language generation: Core tasks, applications and evaluation. CoRR, abs/1703.09902.</p>
<p>Caglar Gulcehre, Sungjin Ahn, Ramesh Nallapati, Bowen Zhou, and Yoshua Bengio. 2016. Pointing the unknown words. arXiv preprint arXiv:1603.08148.</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735-1780.</p>
<p>Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, and Eric P Xing. 2017. Toward controlled generation of text. arXiv preprint arXiv:1703.00955.</p>
<p>Chloé Kiddon, Luke Zettlemoyer, and Yejin Choi. 2016. Globally coherent text generation with neural checklist models. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 329-339.</p>
<p>Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander M Rush. 2017. Opennmt: Open-source toolkit for neural machine translation. arXiv preprint arXiv:1701.02810.</p>
<p>Ioannis Konstas and Mirella Lapata. 2012. Unsupervised concept-to-text generation with hypergraphs. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 752-761. Association for Computational Linguistics.</p>
<p>Ioannis Konstas and Mirella Lapata. 2013. Inducing document plans for concept-to-text generation. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1503-1514.</p>
<p>Mirella Lapata. 2003. Probabilistic text structuring: Experiments with sentence ordering. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 545552. Association for Computational Linguistics.</p>
<p>Vladimir I Levenshtein. 1966. Binary codes capable of correcting deletions, insertions, and reversals. In Soviet physics doklady, volume 10, pages 707-710.</p>
<p>Juncen Li, Robin Jia, He He, and Percy Liang. 2018. Delete, retrieve, generate: A simple approach to sentiment and style transfer. arXiv preprint arXiv:1804.06437.</p>
<p>Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. Text Summarization Branches Out.</p>
<p>Hongyuan Mei, Mohit Bansal, and Matthew R Walter. 2015. What to talk about and how? selective generation using lstms with coarse-to-fine alignment. arXiv preprint arXiv:1509.00838.</p>
<p>Simon Mille, Roberto Carlini, Alicia Burga, and Leo Wanner. 2017. Forge at semeval-2017 task 9: Deep sentence generation based on a sequence of graph transducers. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval2017), pages 920-923.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pages 311-318. Association for Computational Linguistics.</p>
<p>Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532-1543.</p>
<p>Ratish Puduppully, Li Dong, and Mirella Lapata. 2018. Data-to-text generation with content selection and planning. arXiv preprint arXiv:1809.00582.</p>
<p>Yevgeniy Puzikov and Iryna Gurevych. 2018. E2e nlg challenge: Neural models vs. templates. In Proceedings of the 11th International Conference on Natural Language Generation, pages 463-471.</p>
<p>Ehud Reiter and Robert Dale. 2000. Building natural language generation systems. Cambridge university press.</p>
<p>Robert E Schapire. 1999. A brief introduction to boosting. In Ijcai, volume 99, pages 1401-1406.</p>
<p>Shikhar Sharma, Layla El Asri, Hannes Schulz, and Jeremie Zumer. 2017. Relevance of unsupervised metrics in task-oriented dialogue for evaluating natural language generation. CoRR, abs/1706.09799.</p>
<p>Tianxiao Shen, Tao Lei, Regina Barzilay, and Tommi Jaakkola. 2017. Style transfer from non-parallel text by cross-alignment. In Advances in Neural Information Processing Systems, pages 6830-6841.</p>
<p>Amanda Stent, Rashmi Prasad, and Marilyn Walker. 2004. Trainable sentence planning for complex information presentation in spoken dialog systems. In Proceedings of the 42nd annual meeting on association for computational linguistics, page 79. Association for Computational Linguistics.</p>
<p>Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. 2015. Cider: Consensus-based image description evaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4566-4575.</p>
<p>Tsung-Hsien Wen, Milica Gasic, Nikola Mrksic, PeiHao Su, David Vandyke, and Steve Young. 2015. Semantically conditioned lstm-based natural language generation for spoken dialogue systems. arXiv preprint arXiv:1508.01745.</p>
<p>Sam Wiseman, Stuart M Shieber, and Alexander M Rush. 2017. Challenges in data-to-document generation. arXiv preprint arXiv:1707.08052.</p>
<p>Sam Wiseman, Stuart M Shieber, and Alexander M Rush. 2018. Learning neural templates for text generation. arXiv preprint arXiv:1808.10122.</p>
<p>Junbo Jake Zhao, Yoon Kim, Kelly Zhang, Alexander M. Rush, and Yann LeCun. 2017. Adversarially regularized autoencoders for generating discrete structures. CoRR, abs/1706.04223.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{17}$ While the scores for the different sets are very similar, the plans are very different from each other. See for examples the plans in Figure 3.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{15}$ We do not evaluate UPF-FORGe as it is a verifiable grammar-based system that is fully faithful by design.
${ }^{16}$ This evaluation was conducted by the first author, on a set of shuffled examples from the BestPlan and StrongNeural systems, without knowing which outputs belongs to which system. We further note that evaluating for faithfulness requires careful attention to detail (making it less suitable for crowd-workers), but has a precise task definition which does not involve subjective judgment, making it possible to annotate without annotator biases influencing the results. We release our judgments for this stage together with the code.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
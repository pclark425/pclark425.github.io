<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5466 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5466</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5466</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-110.html">extraction-schema-110</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <p><strong>Paper ID:</strong> paper-0786c88990235414611478099e43611542d973b0</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/0786c88990235414611478099e43611542d973b0" target="_blank">Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> Step-Back Prompting is presented, a simple prompting technique that enables LLMs to do abstractions to derive high-level concepts and first principles from instances containing specific details and significantly improves LLMs' abilities in following a correct reasoning path towards the solution.</p>
                <p><strong>Paper Abstract:</strong> We present Step-Back Prompting, a simple prompting technique that enables LLMs to do abstractions to derive high-level concepts and first principles from instances containing specific details. Using the concepts and principles to guide reasoning, LLMs significantly improve their abilities in following a correct reasoning path towards the solution. We conduct experiments of Step-Back Prompting with PaLM-2L, GPT-4 and Llama2-70B models, and observe substantial performance gains on various challenging reasoning-intensive tasks including STEM, Knowledge QA, and Multi-Hop Reasoning. For instance, Step-Back Prompting improves PaLM-2L performance on MMLU (Physics and Chemistry) by 7% and 11% respectively, TimeQA by 27%, and MuSiQue by 7%.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5466.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5466.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Step-Back (PaLM-2L)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>STEP-BACK PROMPTING (PaLM-2L)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-step generate-then-reflect prompting technique where the model first generates a higher-level 'step-back' abstraction (concepts/principles or a generic question), optionally retrieves facts for that abstraction, and then performs reasoning grounded on those abstractions; applied to PaLM-2L in this paper and evaluated across STEM, Knowledge QA, and Multi-Hop benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-2L (instruction-tuned PaLM-2L)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned PaLM-2 family model (large language model) used in the paper; exact parameter count not listed in this paper but referenced as PaLM-2L (Anil et al., 2023).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>STEP-BACK Prompting (Abstraction-grounded Reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Two-step procedure implemented via in-context few-shot prompting: (1) Abstraction/Generate: prompt the model to produce a step-back question or list of high-level concepts/principles relevant to the original question; (2) Reasoning/Reflect: optionally retrieve passages using the step-back question (RAG) and then prompt the model to reason to the final answer grounded on the abstraction and retrieved facts. Abstraction is done via a few-shot exemplar (often a single exemplar).</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>2</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MMLU (high-school Physics & Chemistry), TimeQA, MuSiQue, StrategyQA, GSM8K, SituatedQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>A mix of domain-specific STEM multi-step problems (MMLU Physics & Chemistry, GSM8K), time-sensitive factual QA (TimeQA, SituatedQA), and multi-hop reasoning benchmarks (MuSiQue, StrategyQA).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>MMLU Physics: 73.2% (±1.9); MMLU Chemistry: 81.8% (±1.4); TimeQA (no RAG): 66.0%; TimeQA + RAG: 68.7%; MuSiQue: 42.6% (±3.1); MuSiQue + RAG: 42.8% (±2.0); StrategyQA: 82.7% (±0.4); StrategyQA + RAG: 86.4% (±1.0); GSM8K: 84.3% (±0.2); SituatedQA: 57.5% (±0.3).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>PaLM-2L baseline (no step-back): MMLU Physics 66.4% (±0.8); MMLU Chemistry 70.9% (±0.9); TimeQA 41.5%; TimeQA + RAG (regular RAG without step-back): 57.4%; MuSiQue 35.5% (±3.0); StrategyQA 82.8% (±0.7); GSM8K 75.8% (±0.2); SituatedQA 54.3% (±0.3).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Quantitative improvements reported across multiple benchmarks: +~7 percentage points on MMLU Physics (66.4 → 73.2) and +~11 points on MMLU Chemistry (70.9 → 81.8); large gains on TimeQA (41.5 → 66.0, and 57.4 → 68.7 when combined with RAG), MuSiQue (35.5 → 42.6), and GSM8K (75.8 → 84.3). Paper also reports error-analysis metrics: STEP-BACK corrected 20.5% of baseline MMLU-Physics errors while introducing 11.9% new errors; on TimeQA STEP-BACK+RAG fixed 39.9% of baseline errors while causing 5.6% new errors, and fixed 21.6% of RAG errors while introducing 6.3% new RAG errors.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>The dominant failure mode remains failures in final reasoning and arithmetic, not abstraction: >90% of STEP-BACK errors on MMLU were reasoning-step errors (math and intermediate reasoning), while principle/abstraction errors were a small fraction. STEP-BACK can introduce a modest amount of new errors (e.g., up to ~11.9% new errors on MMLU Physics relative to baseline). For knowledge-intensive tasks, failures are often due to RAG retrieval failures even when the step-back question is correct (~45% of TimeQA errors attributed to retrieval failures). Ablation shows abstraction is sample-efficient (single exemplar suffices) but does not solve intrinsic reasoning/mathematical limitations of the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Abstraction step is usually produced by the same LLM via in-context prompting (no separate verifier model), and RAG is optional depending on task.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5466.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5466.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Step-Back (GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>STEP-BACK PROMPTING (GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Same two-step Abstraction-grounded Reasoning prompting scheme applied to GPT-4 as evaluated in this paper; used as a strong-model baseline and also improved by STEP-BACK prompting on STEM benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI's GPT-4 large language model (technical report cited in references).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>STEP-BACK Prompting (Abstraction-grounded Reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>In-context few-shot prompting to generate step-back concepts/principles, then reasoning grounded on those concepts; applied to GPT-4 in the paper (same two-step flow).</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>2</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MMLU (high-school Physics & Chemistry), TimeQA (baseline only reported for GPT-4 in tables)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>STEM and time-sensitive QA tasks requiring deep reasoning and domain principles.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>MMLU Physics: 84.5% (±1.2) with STEP-BACK; MMLU Chemistry: 85.6% (±1.4) with STEP-BACK. (Tables show STEP-BACK for GPT-4 on these STEM tasks.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>GPT-4 baseline (no step-back): MMLU Physics 69.4% (±2.0); MMLU Chemistry 80.9% (±0.7); TimeQA baseline reported as 45.6%.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Substantial gains on MMLU STEM: Physics improved from 69.4% to 84.5% and Chemistry 80.9% to 85.6% with STEP-BACK prompting, demonstrating effectiveness even on an already-strong model. The paper notes STEP-BACK is competitive across model families.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Same qualitative limitations as for PaLM-2L: abstraction is relatively easy but remaining errors are dominated by reasoning/math; not all tasks have STEP-BACK+RAG numbers reported for GPT-4 in the paper, so comparisons on Knowledge QA / RAG-enhanced setups are limited in the provided tables.</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>GPT-4 was run on baselines for all tasks per the paper, but STEP-BACK + RAG numeric results are primarily reported for PaLM-2L in Knowledge QA tables.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5466.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5466.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Step-Back (Llama2-70B)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>STEP-BACK PROMPTING (Llama2-70B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>STEP-BACK prompting applied to Llama2-70B (70B parameter family) showing consistent improvements over baseline and other prompting methods in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama2-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Llama 2 family 70B model (Touvron et al., 2023), instruction-tuned/fine-tuned chat-capable variants referenced in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>STEP-BACK Prompting (Abstraction-grounded Reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Same two-step in-context prompting: generate high-level principles/step-back question, then reason to final answer grounded on abstractions; demonstrated with single exemplar few-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>2</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MMLU (high-school Physics & Chemistry)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Domain-specific STEM multi-step reasoning tasks requiring principles/formula application.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>MMLU Physics: 64.8% (±1.5) with STEP-BACK; MMLU Chemistry: 66.7% (±1.6) with STEP-BACK.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Llama2-70B baseline (no step-back): MMLU Physics 51.9% (±3.6); MMLU Chemistry 55.7% (±2.1).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Clear improvements: Physics +12.9 points (51.9 → 64.8) and Chemistry +11.0 points (55.7 → 66.7) reported in Table 1, showing STEP-BACK is effective even for open models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Improvements are present but absolute performance still lags behind stronger models (e.g., GPT-4). Reasoning/math errors remain dominant failure modes; principle/abstraction errors are a minority per the paper's analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Paper reports that STEP-BACK is model-agnostic and yields consistent gains across model families.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5466.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5466.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Zero-shot or few-shot prompting technique that elicits intermediate step-by-step reasoning chains (e.g., by appending 'Let's think step by step') used as a baseline in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-thought prompting elicits reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-2L / GPT-4 / Llama2-70B (as baselines in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>CoT is a prompting method applied to various LLMs in the paper as baseline (PaLM-2L, GPT-4, Llama2-70B).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Prompting that encourages the model to produce intermediate reasoning steps (e.g., zero-shot 'Let's think step by step' or few-shot CoT exemplars); not an explicit generate-then-reflect loop but elicits internal stepwise reasoning in a single forward pass.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MMLU, TimeQA, MuSiQue, StrategyQA, GSM8K (as baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same set of benchmarks used in the paper as baselines to compare against STEP-BACK prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Varies by task and model; examples from tables: PaLM-2L + CoT on MMLU Physics 65.0% (±2.0) and Chemistry 75.3% (±1.5); GPT-4 + CoT on MMLU Physics 82.9% (±0.5) and Chemistry 85.3% (±1.0); PaLM-2L + CoT on MuSiQue 38.7% (±3.2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Baseline (no CoT) PaLM-2L MMLU Physics 66.4% (±0.8), Chemistry 70.9% (±0.9); other baselines listed in tables.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>CoT sometimes yields modest gains (e.g., PaLM-2L + CoT gives small/insignificant changes on some tasks; GPT-4 + CoT gives large gains on MMLU Physics relative to GPT-4 baseline), but in many tasks STEP-BACK outperforms or provides larger improvements. The paper reports CoT and TDB produce minor improvements on some tasks while STEP-BACK often produces larger gains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>CoT does not always improve performance (sometimes minor or no gain) and can run into intermediate-step errors; the paper notes CoT.zero-shot provides limited improvement on many evaluated tasks compared to STEP-BACK.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5466.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5466.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Step-by-step verification (Lightman et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Let's verify step by step (process-supervision with step-by-step verification)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A process-supervision approach that uses step-by-step verification to improve correctness of intermediate reasoning steps; cited in this paper as a promising complementary technique to reduce intermediate errors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Let's verify step by step</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Generic LLMs (cited prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prior work applying verification/verification-by-checking on model intermediate steps (Lightman et al., 2023).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Step-by-step verification / process supervision</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Approach that monitors or verifies intermediate reasoning steps (process-supervision), potentially with an explicit verification step or critic to improve correctness; cited as related work and a promising remedy for intermediate-step errors.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Mentioned in paper as evidence from related work (Lightman et al., 2023) that step-by-step verification can improve correctness of intermediate reasoning, motivating STEP-BACK prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not evaluated in this paper; cited as related work only. The paper indicates verification-style methods are promising but does not provide experimental data for them here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Let's verify step by step <em>(Rating: 2)</em></li>
                <li>Large language models as optimizers <em>(Rating: 1)</em></li>
                <li>Recitation-augmented language models <em>(Rating: 1)</em></li>
                <li>Least-to-most prompting enables complex reasoning in large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5466",
    "paper_id": "paper-0786c88990235414611478099e43611542d973b0",
    "extraction_schema_id": "extraction-schema-110",
    "extracted_data": [
        {
            "name_short": "Step-Back (PaLM-2L)",
            "name_full": "STEP-BACK PROMPTING (PaLM-2L)",
            "brief_description": "A two-step generate-then-reflect prompting technique where the model first generates a higher-level 'step-back' abstraction (concepts/principles or a generic question), optionally retrieves facts for that abstraction, and then performs reasoning grounded on those abstractions; applied to PaLM-2L in this paper and evaluated across STEM, Knowledge QA, and Multi-Hop benchmarks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM-2L (instruction-tuned PaLM-2L)",
            "model_description": "Instruction-tuned PaLM-2 family model (large language model) used in the paper; exact parameter count not listed in this paper but referenced as PaLM-2L (Anil et al., 2023).",
            "reflection_method_name": "STEP-BACK Prompting (Abstraction-grounded Reasoning)",
            "reflection_method_description": "Two-step procedure implemented via in-context few-shot prompting: (1) Abstraction/Generate: prompt the model to produce a step-back question or list of high-level concepts/principles relevant to the original question; (2) Reasoning/Reflect: optionally retrieve passages using the step-back question (RAG) and then prompt the model to reason to the final answer grounded on the abstraction and retrieved facts. Abstraction is done via a few-shot exemplar (often a single exemplar).",
            "num_iterations": 2,
            "task_name": "MMLU (high-school Physics & Chemistry), TimeQA, MuSiQue, StrategyQA, GSM8K, SituatedQA",
            "task_description": "A mix of domain-specific STEM multi-step problems (MMLU Physics & Chemistry, GSM8K), time-sensitive factual QA (TimeQA, SituatedQA), and multi-hop reasoning benchmarks (MuSiQue, StrategyQA).",
            "performance_with_reflection": "MMLU Physics: 73.2% (±1.9); MMLU Chemistry: 81.8% (±1.4); TimeQA (no RAG): 66.0%; TimeQA + RAG: 68.7%; MuSiQue: 42.6% (±3.1); MuSiQue + RAG: 42.8% (±2.0); StrategyQA: 82.7% (±0.4); StrategyQA + RAG: 86.4% (±1.0); GSM8K: 84.3% (±0.2); SituatedQA: 57.5% (±0.3).",
            "performance_without_reflection": "PaLM-2L baseline (no step-back): MMLU Physics 66.4% (±0.8); MMLU Chemistry 70.9% (±0.9); TimeQA 41.5%; TimeQA + RAG (regular RAG without step-back): 57.4%; MuSiQue 35.5% (±3.0); StrategyQA 82.8% (±0.7); GSM8K 75.8% (±0.2); SituatedQA 54.3% (±0.3).",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Quantitative improvements reported across multiple benchmarks: +~7 percentage points on MMLU Physics (66.4 → 73.2) and +~11 points on MMLU Chemistry (70.9 → 81.8); large gains on TimeQA (41.5 → 66.0, and 57.4 → 68.7 when combined with RAG), MuSiQue (35.5 → 42.6), and GSM8K (75.8 → 84.3). Paper also reports error-analysis metrics: STEP-BACK corrected 20.5% of baseline MMLU-Physics errors while introducing 11.9% new errors; on TimeQA STEP-BACK+RAG fixed 39.9% of baseline errors while causing 5.6% new errors, and fixed 21.6% of RAG errors while introducing 6.3% new RAG errors.",
            "limitations_or_failure_cases": "The dominant failure mode remains failures in final reasoning and arithmetic, not abstraction: &gt;90% of STEP-BACK errors on MMLU were reasoning-step errors (math and intermediate reasoning), while principle/abstraction errors were a small fraction. STEP-BACK can introduce a modest amount of new errors (e.g., up to ~11.9% new errors on MMLU Physics relative to baseline). For knowledge-intensive tasks, failures are often due to RAG retrieval failures even when the step-back question is correct (~45% of TimeQA errors attributed to retrieval failures). Ablation shows abstraction is sample-efficient (single exemplar suffices) but does not solve intrinsic reasoning/mathematical limitations of the LLM.",
            "notes": "Abstraction step is usually produced by the same LLM via in-context prompting (no separate verifier model), and RAG is optional depending on task.",
            "uuid": "e5466.0",
            "source_info": {
                "paper_title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Step-Back (GPT-4)",
            "name_full": "STEP-BACK PROMPTING (GPT-4)",
            "brief_description": "Same two-step Abstraction-grounded Reasoning prompting scheme applied to GPT-4 as evaluated in this paper; used as a strong-model baseline and also improved by STEP-BACK prompting on STEM benchmarks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "OpenAI's GPT-4 large language model (technical report cited in references).",
            "reflection_method_name": "STEP-BACK Prompting (Abstraction-grounded Reasoning)",
            "reflection_method_description": "In-context few-shot prompting to generate step-back concepts/principles, then reasoning grounded on those concepts; applied to GPT-4 in the paper (same two-step flow).",
            "num_iterations": 2,
            "task_name": "MMLU (high-school Physics & Chemistry), TimeQA (baseline only reported for GPT-4 in tables)",
            "task_description": "STEM and time-sensitive QA tasks requiring deep reasoning and domain principles.",
            "performance_with_reflection": "MMLU Physics: 84.5% (±1.2) with STEP-BACK; MMLU Chemistry: 85.6% (±1.4) with STEP-BACK. (Tables show STEP-BACK for GPT-4 on these STEM tasks.)",
            "performance_without_reflection": "GPT-4 baseline (no step-back): MMLU Physics 69.4% (±2.0); MMLU Chemistry 80.9% (±0.7); TimeQA baseline reported as 45.6%.",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Substantial gains on MMLU STEM: Physics improved from 69.4% to 84.5% and Chemistry 80.9% to 85.6% with STEP-BACK prompting, demonstrating effectiveness even on an already-strong model. The paper notes STEP-BACK is competitive across model families.",
            "limitations_or_failure_cases": "Same qualitative limitations as for PaLM-2L: abstraction is relatively easy but remaining errors are dominated by reasoning/math; not all tasks have STEP-BACK+RAG numbers reported for GPT-4 in the paper, so comparisons on Knowledge QA / RAG-enhanced setups are limited in the provided tables.",
            "notes": "GPT-4 was run on baselines for all tasks per the paper, but STEP-BACK + RAG numeric results are primarily reported for PaLM-2L in Knowledge QA tables.",
            "uuid": "e5466.1",
            "source_info": {
                "paper_title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Step-Back (Llama2-70B)",
            "name_full": "STEP-BACK PROMPTING (Llama2-70B)",
            "brief_description": "STEP-BACK prompting applied to Llama2-70B (70B parameter family) showing consistent improvements over baseline and other prompting methods in the paper.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama2-70B",
            "model_description": "Llama 2 family 70B model (Touvron et al., 2023), instruction-tuned/fine-tuned chat-capable variants referenced in the paper.",
            "reflection_method_name": "STEP-BACK Prompting (Abstraction-grounded Reasoning)",
            "reflection_method_description": "Same two-step in-context prompting: generate high-level principles/step-back question, then reason to final answer grounded on abstractions; demonstrated with single exemplar few-shot.",
            "num_iterations": 2,
            "task_name": "MMLU (high-school Physics & Chemistry)",
            "task_description": "Domain-specific STEM multi-step reasoning tasks requiring principles/formula application.",
            "performance_with_reflection": "MMLU Physics: 64.8% (±1.5) with STEP-BACK; MMLU Chemistry: 66.7% (±1.6) with STEP-BACK.",
            "performance_without_reflection": "Llama2-70B baseline (no step-back): MMLU Physics 51.9% (±3.6); MMLU Chemistry 55.7% (±2.1).",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Clear improvements: Physics +12.9 points (51.9 → 64.8) and Chemistry +11.0 points (55.7 → 66.7) reported in Table 1, showing STEP-BACK is effective even for open models.",
            "limitations_or_failure_cases": "Improvements are present but absolute performance still lags behind stronger models (e.g., GPT-4). Reasoning/math errors remain dominant failure modes; principle/abstraction errors are a minority per the paper's analysis.",
            "notes": "Paper reports that STEP-BACK is model-agnostic and yields consistent gains across model families.",
            "uuid": "e5466.2",
            "source_info": {
                "paper_title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Chain-of-Thought (CoT)",
            "name_full": "Chain-of-Thought Prompting",
            "brief_description": "Zero-shot or few-shot prompting technique that elicits intermediate step-by-step reasoning chains (e.g., by appending 'Let's think step by step') used as a baseline in this paper.",
            "citation_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "mention_or_use": "use",
            "model_name": "PaLM-2L / GPT-4 / Llama2-70B (as baselines in experiments)",
            "model_description": "CoT is a prompting method applied to various LLMs in the paper as baseline (PaLM-2L, GPT-4, Llama2-70B).",
            "reflection_method_name": "Chain-of-Thought (CoT)",
            "reflection_method_description": "Prompting that encourages the model to produce intermediate reasoning steps (e.g., zero-shot 'Let's think step by step' or few-shot CoT exemplars); not an explicit generate-then-reflect loop but elicits internal stepwise reasoning in a single forward pass.",
            "num_iterations": null,
            "task_name": "MMLU, TimeQA, MuSiQue, StrategyQA, GSM8K (as baselines)",
            "task_description": "Same set of benchmarks used in the paper as baselines to compare against STEP-BACK prompting.",
            "performance_with_reflection": "Varies by task and model; examples from tables: PaLM-2L + CoT on MMLU Physics 65.0% (±2.0) and Chemistry 75.3% (±1.5); GPT-4 + CoT on MMLU Physics 82.9% (±0.5) and Chemistry 85.3% (±1.0); PaLM-2L + CoT on MuSiQue 38.7% (±3.2).",
            "performance_without_reflection": "Baseline (no CoT) PaLM-2L MMLU Physics 66.4% (±0.8), Chemistry 70.9% (±0.9); other baselines listed in tables.",
            "has_performance_comparison": true,
            "evidence_of_improvement": "CoT sometimes yields modest gains (e.g., PaLM-2L + CoT gives small/insignificant changes on some tasks; GPT-4 + CoT gives large gains on MMLU Physics relative to GPT-4 baseline), but in many tasks STEP-BACK outperforms or provides larger improvements. The paper reports CoT and TDB produce minor improvements on some tasks while STEP-BACK often produces larger gains.",
            "limitations_or_failure_cases": "CoT does not always improve performance (sometimes minor or no gain) and can run into intermediate-step errors; the paper notes CoT.zero-shot provides limited improvement on many evaluated tasks compared to STEP-BACK.",
            "uuid": "e5466.3",
            "source_info": {
                "paper_title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Step-by-step verification (Lightman et al.)",
            "name_full": "Let's verify step by step (process-supervision with step-by-step verification)",
            "brief_description": "A process-supervision approach that uses step-by-step verification to improve correctness of intermediate reasoning steps; cited in this paper as a promising complementary technique to reduce intermediate errors.",
            "citation_title": "Let's verify step by step",
            "mention_or_use": "mention",
            "model_name": "Generic LLMs (cited prior work)",
            "model_description": "Prior work applying verification/verification-by-checking on model intermediate steps (Lightman et al., 2023).",
            "reflection_method_name": "Step-by-step verification / process supervision",
            "reflection_method_description": "Approach that monitors or verifies intermediate reasoning steps (process-supervision), potentially with an explicit verification step or critic to improve correctness; cited as related work and a promising remedy for intermediate-step errors.",
            "num_iterations": null,
            "task_name": null,
            "task_description": null,
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": false,
            "evidence_of_improvement": "Mentioned in paper as evidence from related work (Lightman et al., 2023) that step-by-step verification can improve correctness of intermediate reasoning, motivating STEP-BACK prompting.",
            "limitations_or_failure_cases": "Not evaluated in this paper; cited as related work only. The paper indicates verification-style methods are promising but does not provide experimental data for them here.",
            "uuid": "e5466.4",
            "source_info": {
                "paper_title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Let's verify step by step",
            "rating": 2
        },
        {
            "paper_title": "Large language models as optimizers",
            "rating": 1
        },
        {
            "paper_title": "Recitation-augmented language models",
            "rating": 1
        },
        {
            "paper_title": "Least-to-most prompting enables complex reasoning in large language models",
            "rating": 1
        }
    ],
    "cost": 0.01693575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Take a Step Back: Evoking ReASONing via AbSTRACTION IN LARGE LANGUAGE MODELS</h1>
<p>Huaixiu Steven Zheng<em> Swaroop Mishra</em> Xinyun Chen Heng-Tze Cheng<br>Ed H. Chi Quoc V Le Denny Zhou<br>Google DeepMind</p>
<h4>Abstract</h4>
<p>We present STEP-BACK PROMPTING, a simple prompting technique that enables LLMs to do abstractions to derive high-level concepts and first principles from instances containing specific details. Using the concepts and principles to guide reasoning, LLMs significantly improve their abilities in following a correct reasoning path towards the solution. We conduct experiments of STEP-BACK PROMPTING with PaLM-2L, GPT-4 and Llama2-70B models, and observe substantial performance gains on various challenging reasoning-intensive tasks including STEM, Knowledge QA, and Multi-Hop Reasoning. For instance, STEP-BACK PROMPTING improves PaLM-2L performance on MMLU (Physics and Chemistry) by $7 \%$ and $11 \%$ respectively, TimeQA by $27 \%$, and MuSiQue by $7 \%$.</p>
<p>The purpose of abstraction is not to be vague, but to create a new semantic level in which one can be absolutely precise. - Edsger W. Dijkstra</p>
<h2>1 INTRODUCTION</h2>
<p>The field of natural language processing (NLP) is witnessing a ground-breaking revolution because of the Transformer-based (Vaswani et al., 2017) large language models (LLMs) (Devlin et al., 2018; Raffel et al., 2020; Brown et al., 2020; Anil et al., 2023). Scaling up the model size and pre-training corpus (Hoffmann et al., 2022; Chowdhery et al., 2022) has brought remarkable improvement in model capabilities and sample efficiency with insights from the scaling law (Kaplan et al., 2020; Hoffmann et al., 2022), as well as emergent abilities (Wei et al., 2022a) such as multi-step reasoning (Wei et al., 2022b; Zhou et al., 2022) and instruction following (Mishra et al., 2022b; Wei et al., 2021).
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Strong Performance of STEP-BACK PROMPTING: our proposed Abstraction-and-Reasoning scheme leads to a substantial improvement in a wide range of challenging tasks in STEM, Knowledge QA and Multi-Hop Reasoning requiring complex (often multi-hop) reasoning.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Despite the great advancements, complex multi-step reasoning remains challenging for even the state-of-the-art LLMs. Lightman et al. (2023) show that process-supervision with step-by-step verification is a promising remedy to improve the correctness of intermediate reasoning steps. Techniques such as Chain-of-Thought (Wei et al., 2022b) were introduced to produce a coherent series of intermediate reasoning steps to increase the success rate of following the right decoding path. Inspired by the fact that when faced with challenging tasks humans often step back and do abstractions to arrive at high-level principles to guide the process, we propose STEP-BACK PROMPTING to ground reasoning on abstractions to reduce the chance of making errors in the intermediate reasoning steps.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Illustration of STEP-BACK PROMPTING with two steps of Abstraction and Reasoning guided by concepts and principles. Top: an example of MMLU high-school physics (Hendrycks et al., 2020) where the first principle of Ideal Gas Law is retrieved via abstraction. Bottom: an example from TimeQA (Chen et al., 2021) where the high-level concept of education history is a result of the abstraction. Left: PaLM-2L (Anil et al., 2023) fails to answer the original question. Chain-of-Thought prompting (Wei et al., 2022b; Kojima et al., 2022) ran into errors during intermediate reasoning steps (highlighted as red). Right: PaLM-2L (Anil et al., 2023) successfully answers the question via STEP-BACK PROMPTING.</p>
<p>Among many of the cognitive skills, abstraction (Lachmy et al., 2022) is ubiquitous to humans' ability to process vast amounts of information and derive general principles. For example, Kepler compressed thousands of measurements into Kepler's three laws of planetary motion, which precisely describe the orbits of planets around the Sun (Russell, 1964). In critical decision-making, humans find abstraction to be helpful since it provides a broader view of the environment. This work explores how LLMs can tackle complex tasks involving many low-level details through a two-step process of abstraction-and-reasoning. The first step is to show LLMs how to step back through in-context learning - prompting them to derive high-level abstractions such as concepts and principles for a specific example. The second step is to leverage the reasoning ability to reason on top of the high-level concepts and principles. We use few-shot exemplar demonstrations to execute STEP-BACK PROMPTING on LLMs.</p>
<p>We experiment across a range of tasks involving domain specific reasoning such as Physics and Chemistry, knowledge-intensive question answering requiring factual knowledge, multi-hop commonsense reasoning. We observe significant performance improvements (up to $27 \%$ ) in PaLM-2L (Anil et al.,</p>
<p>2023) demonstrating the efficacy of STEP-BACK PROMPTING in tackling complex tasks, which are otherwise challenging due to the amount of details needed for reasoning. Figure 1 shows a summary of all the key results presented in this paper. Some the tasks are very challenging: both PaLM-2L and GPT-4 achieve only $\sim 40 \%$ accuracy on TimeQA and MuSiQue. Chain-of-Thought prompting leads to a minor improvement on a few tasks, while STEP-BACK PROMPTING improves the performance of PaLM-2L across the board: $7 \%$ and $11 \%$ on MMLU Physics and Chemistry, $27 \%$ on TimeQA, and $7 \%$ on MuSiQue.</p>
<p>We conduct a variety of analyses and find that STEP-BACK PROMPTING leads to strong performance improvements (up to $36 \%$ ) over chain-of-thought (CoT) prompting (Wei et al., 2022b) and "take-a-deep-breath" (TDB) prompting (Yang et al., 2023). We perform a qualitative evaluation where we find that Step-Back fixes a large portion of errors of the base model (up to $\sim 40 \%$ ) while introducing a small portion of new errors (max $\sim 12 \%$ ). We also conduct an error analysis and find that majority of the errors made by STEP-BACK PROMPTING is attributed to the intrinsic limitations of reasoning capabilities of LLMs while abstraction skills are relatively easy to demonstrate to LLMs, pointing out the direction for future improvements of methods alike STEP-BACK PROMPTING.</p>
<h1>2 STEP-BACK PROMPTING</h1>
<p>STEP-BACK PROMPTING is motivated by the observation that many tasks contain a lot of details, and it is hard for LLMs to retrieve relevant facts to tackle the task. As shown in the first example (top) in Figure 2, for a Physics question of "What happens to the pressure, P, of an ideal gas if the temperature is increased by a factor of 2 and the volume is increased by a factor of 8 ?", the LLM can deviate from the first principle of Ideal Gas Law when reasoning directly on the question. Similarly, a question of "Estella Leopold went to which school between Aug 1954 and Nov 1954?" is very hard to address directly given the detailed time range constraint. In both cases, asking a step-back question helps the model to solve the problem effectively.
We define a step-back question as a derived question from the original question at a higher level of abstraction. For instance, instead of directly asking "which school Estella Leopold went to during a specific period", a step-back question (Figure 2 bottom) would ask about the "education history", which is a high-level concept encompasses the original question. Answering the step-back question of "Estella Leopold's education history" in this case will provide all the necessary information to reason about "which school Estella Leopold went to during a specific period". The premise is that the step-back question is typically much easier. Grounding the reasoning on top of such abstractions helps to avoid reasoning errors in the intermediate steps such as the example shown in Figure 2 (left) from Chain-of-Thought. In short, STEP-BACK PROMPTING consists two simple steps:</p>
<ul>
<li>Abstraction: Instead of addressing the question directly, we first prompt the LLM to ask a generic step-back question about a higher-level concept or principle, and retrieve relevant facts about the high-level concept or principle. The step-back question is unique for each task in order to retrieve the most relevant facts.</li>
<li>Reasoning: Grounded on the facts regarding the high-level concept or principle, the LLM can reason about the solution to the original question. We term this as Abstraction-grounded Reasoning.</li>
</ul>
<p>In the following sections, we present an empirical study of STEP-BACK PROMPTING on a range of challenging tasks covering STEM, Knowledge QA, and Multi-Hop Reasoning involving complex reasoning.</p>
<h2>3 EXPERIMENTAL SETUP</h2>
<p>Here we define the tasks and models we experiment with. We also describe our evaluation metric and the baselines we consider.</p>
<h3>3.1 TASKS</h3>
<p>We experiment with the following diverse tasks: (a) STEM, (b) Knowledge QA, and (c) Multi-Hop Reasoning. We describe below the datasets we consider (see Appendix B for more details).</p>
<ul>
<li>STEM: We evaluate MMLU and GSM8K for STEM tasks. MMLU (Hendrycks et al., 2020) contains a series of benchmarks across diverse domains to evaluate the model's language understanding. We consider the high school physics and chemistry portions of MMLU because of the deep reasoning involved.</li>
<li>Knowledge QA: We consider TimeQA (Chen et al., 2021) since it contains complex queries that require challenging time-sensitive knowledge. We also experiment with SituatedQA (Zhang \&amp; Choi, 2021), another challenging open-retrieval QA dataset requiring the model to answer questions given temporal or geographical contexts.</li>
<li>Multi-Hop Reasoning: We experiment with MuSiQue (Trivedi et al., 2022), a hard multihop reasoning dataset created via composable pairs of single-hop questions, and StrategyQA (Geva et al., 2021) with open-domain questions that demand some strategy to solve.</li>
</ul>
<h1>3.2 MODELS</h1>
<p>We use the following state-of-the-art LLMs: instruction-tuned PaLM-2L (Anil et al., 2023), GPT4 (OpenAI, 2023), and Llama2-70B (Touvron et al., 2023).</p>
<h3>3.3 Evaluation</h3>
<p>Conventional evaluation metrics such as accuracy, F1 score have limitations specifically for evaluating the generations of state-of-the-art LLMs since these models often generate long-form answers which are hard to capture. We instead conduct an evaluation using the PaLM-2L model where we few-shot prompt the model to identify equivalence between target answers and the model predictions. Few-shot examples, prompts and other details used for this evaluation are in Appendix C.</p>
<h3>3.4 BASELINE METHODS</h3>
<ul>
<li>PaLM-2L, PaLM-2L 1-shot: PaLM-2L is either queried directly with the question or has a single demonstration exemplar of question-answer included in the prompt.</li>
<li>PaLM-2L + CoT, PaLM-2L + CoT 1-shot: PaLM-2L model is queried with zero-shot CoT prompting (Kojima et al., 2022): "Let's think step by step" is appended to the question. For 1-shot, One demonstration example of a question and answer pair is provided in the prompt, where the answer is in the style of CoT (Wei et al., 2022b).</li>
<li>PaLM-2L + TDB: Zero-shot prompting with "Take a deep breath and work on this problem step-by-step." (Yang et al., 2023) prepended to the question.</li>
<li>PaLM-2L + RAG: For Sections 5 and 6, we use retrieval-augmented generation (RAG) where the retrieved passage is used as context by the LLM.</li>
<li>GPT-4 and Llama2-70B: we run GPT-4 and Llama2-70B on MMLU tasks for all methods. In addition, we also run GPT-4 on all baselines for all tasks.</li>
</ul>
<p>We do not use RAG for STEM tasks, because of the inherent reasoning nature of the tasks contrary to the other fact-seeking datasets. All inferences are done using greedy decoding.</p>
<h2>4 STEM</h2>
<p>We evaluate STEP-BACK Prompting on STEM tasks (Hendrycks et al., 2020) to gauge the efficacy of our method on reasoning in highly specialized domains. We explain below our experimental setup, result, and analysis of applying STEP-BACK Prompting on the MMLU high-school Physics and Chemistry, and GSM8K benchmarks.</p>
<h3>4.1 STEP-BACK Prompting</h3>
<p>Questions in the MMLU benchmarks require deeper reasoning. Furthermore, they also require understanding and application of formulae which are often physics and chemistry principles and concepts. In this case, we first demonstrate to the model abstraction skills in the form of concepts and first principles such as Newton's first law of motion, Doppler effect, and Gibbs free energy etc.</p>
<p>Table 1: Strong performance of STEP-BACK PROMPTING on MMLU tasks across three model families. CoT: zero-shot Chain of Thought prompting (Kojima et al., 2022), TDB: Take a Deep Breath prompting (Yang et al., 2023).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">MMLU Physics</th>
<th style="text-align: center;">MMLU Chemistry</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">PaLM-2L</td>
<td style="text-align: center;">$66.4 \%(0.8 \%)$</td>
<td style="text-align: center;">$70.9 \%(0.9 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">PaLM-2L 1-shot</td>
<td style="text-align: center;">$64 \%(1.6 \%)$</td>
<td style="text-align: center;">$75.6 \%(0.4 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">PaLM-2L + CoT</td>
<td style="text-align: center;">$65 \%(2 \%)$</td>
<td style="text-align: center;">$75.3 \%(1.5 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">PaLM-2L + CoT 1-shot</td>
<td style="text-align: center;">$61.5 \%(1.8 \%)$</td>
<td style="text-align: center;">$76.6 \%(1 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">PaLM-2L + TDB</td>
<td style="text-align: center;">$65.7 \%(0.7 \%)$</td>
<td style="text-align: center;">$73.8 \%(1.1 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">PaLM-2L + Step-Back (ours)</td>
<td style="text-align: center;">$\mathbf{7 3 . 2 \%}(1.9 \%)$</td>
<td style="text-align: center;">$\mathbf{8 1 . 8 \%}(1.4 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: center;">$69.4 \%(2.0 \%)$</td>
<td style="text-align: center;">$80.9 \%(0.7 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 1-shot</td>
<td style="text-align: center;">$78.4 \%(2.4 \%)$</td>
<td style="text-align: center;">$80.5 \%(1.6 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 + CoT</td>
<td style="text-align: center;">$82.9 \%(0.5 \%)$</td>
<td style="text-align: center;">$85.3 \%(1.0 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 + CoT 1-shot</td>
<td style="text-align: center;">$79.3 \%(1.0 \%)$</td>
<td style="text-align: center;">$82.8 \%(0.5 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 + TDB</td>
<td style="text-align: center;">$74.4 \%(4.0 \%)$</td>
<td style="text-align: center;">$81.5 \%(1.3 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 + Step-Back (ours)</td>
<td style="text-align: center;">$\mathbf{8 4 . 5 \%}(1.2 \%)$</td>
<td style="text-align: center;">$\mathbf{8 5 . 6 \%}(1.4 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">Llama2-70B</td>
<td style="text-align: center;">$51.9 \%(3.6 \%)$</td>
<td style="text-align: center;">$55.7 \%(2.1 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">Llama2-70B 1-shot</td>
<td style="text-align: center;">$57.3 \%(1.6 \%)$</td>
<td style="text-align: center;">$58.5 \%(2.5 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">Llama2-70B + CoT</td>
<td style="text-align: center;">$59.3 \%(2.0 \%)$</td>
<td style="text-align: center;">$64.1 \%(1.2 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">Llama2-70B + CoT 1-shot</td>
<td style="text-align: center;">$59.6 \%(2.0 \%)$</td>
<td style="text-align: center;">$\mathbf{6 8 . 1 \%}(1.4 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">Llama2-70B + TDB</td>
<td style="text-align: center;">$60.4 \%(2.1 \%)$</td>
<td style="text-align: center;">$63.6 \%(1.9 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">Llama2-70B + Step-Back (ours)</td>
<td style="text-align: center;">$\mathbf{6 4 . 8 \%}(1.5 \%)$</td>
<td style="text-align: center;">$66.7 \%(1.6 \%)$</td>
</tr>
</tbody>
</table>
<p>The implicit step-back question here is "what are the physics or chemistry principles and concepts involved in solving this task?". We provide demonstrations to the model to recite the relevant principles for solving the task from its own knowledge (see Appendix D. 1 for few-shot exemplars).</p>
<h1>4.2 ReSULTS</h1>
<p>Table 1 illustrates model performance across various setups across three model families: PaLM-2L, GPT-4, and Llama2-70B. Average accuracy over 5 evaluation runs is reported along with standard deviations (in the parentheses). PaLM-2L baseline performance is $66.4 \%$ and $70.9 \%$ on Physics and Chemistry, respectively. We find that CoT and TDB zero-shot prompting do not significantly increase model performance, which could be due to the inherent difficulty and deep reasoning associated with these tasks. PaLM-2L 1-shot and PaLM-2L + CoT 1-shot do not improve against the baseline much either, highlighting the challenge of demonstrating the reasoning steps to the model. In contrast, STEP-BACK PROMPTING significantly improves model performance: $+7 \%$ and $+11 \%$ compared to PaLM-2L. Similarly, with GPT4 and Llama2-70B models, STEP-BACK PROMPTING is very competitive among all the baseline methods we tested, showing that STEP-BACK PROMPTING is model-agnostic. We present the results of GSM8K in Appendix A.1.</p>
<h3>4.3 Ablation and Analysis</h3>
<p>Few-shot Ablation: First, in Figure 3, we observe that STEP-BACK PROMPTING is robust to the number of few-shot exemplars of (question, principles) pairs used as demonstrations. Adding more</p>
<p>Table 2: Strong performance of STEP-BACK PROMPTING on Knowledge QA tasks. CoT: Chain of Thought prompting, TDB: Take a Deep Breath prompting, RAG: retrieval-augmented generation. STEP-BACK PROMPTING results in significant performance improvements.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">TimeQA</th>
<th style="text-align: center;">TQA Easy</th>
<th style="text-align: center;">TQA Hard</th>
<th style="text-align: center;">SituatedQA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">PaLM-2L</td>
<td style="text-align: center;">$41.5 \%$</td>
<td style="text-align: center;">$42.6 \%$</td>
<td style="text-align: center;">$40.4 \%$</td>
<td style="text-align: center;">$54.3 \%(0.3 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">PaLM-2L 1-shot</td>
<td style="text-align: center;">$40.7 \%$</td>
<td style="text-align: center;">$41.7 \%$</td>
<td style="text-align: center;">$39.1 \%$</td>
<td style="text-align: center;">$51.8 \%(0.6 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">PaLM-2L + CoT</td>
<td style="text-align: center;">$40.8 \%$</td>
<td style="text-align: center;">$41.8 \%$</td>
<td style="text-align: center;">$39.8 \%$</td>
<td style="text-align: center;">$56.4 \%(0.2 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">PaLM-2L + CoT 1-shot</td>
<td style="text-align: center;">$38.1 \%$</td>
<td style="text-align: center;">$39.3 \%$</td>
<td style="text-align: center;">$36.8 \%$</td>
<td style="text-align: center;">$54 \%(0.8 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">PaLM-2L + TDB</td>
<td style="text-align: center;">$40.9 \%$</td>
<td style="text-align: center;">$42.6 \%$</td>
<td style="text-align: center;">$39.1 \%$</td>
<td style="text-align: center;">$54 \%(0.5 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">PaLM-2L + RAG</td>
<td style="text-align: center;">$57.4 \%$</td>
<td style="text-align: center;">$67.8 \%$</td>
<td style="text-align: center;">$46.8 \%$</td>
<td style="text-align: center;">$59.3 \%(0.4 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">PaLM-2L + Step-Back (ours)</td>
<td style="text-align: center;">$66 \%$</td>
<td style="text-align: center;">$70.4 \%$</td>
<td style="text-align: center;">$61.6 \%$</td>
<td style="text-align: center;">$57.5 \%(0.3 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">PaLM-2L + Step-Back + RAG (ours)</td>
<td style="text-align: center;">$\mathbf{6 8 . 7 \%}$</td>
<td style="text-align: center;">$\mathbf{7 5 . 2 \%}$</td>
<td style="text-align: center;">$\mathbf{6 2 . 3 \%}$</td>
<td style="text-align: center;">$61 \%(0.4 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: center;">$45.6 \%$</td>
<td style="text-align: center;">$48.9 \%$</td>
<td style="text-align: center;">$42.6 \%$</td>
<td style="text-align: center;">$\mathbf{6 3 . 2 \%}(0.4 \%)$</td>
</tr>
</tbody>
</table>
<p>demonstration examples beyond a single example does not lead to further improvements. This indicates that the task of retrieving the relevant principles and concepts is relatively easy through in-context learning and a single demonstration suffices. Therefore, we use a single exemplar for few-shot prompting throughout the paper except the ablation studies.</p>
<p>Error Analysis: Comparing the predictions of STEP-BACK PROMPTING to the baseline PaLM-2L model for MMLU high-school Physics: we find that STEP-BACK PROMPTING corrects $20.5 \%$ errors from the baseline while introducing $11.9 \%$ errors.</p>
<p>To further understand where the errors come from in STEP-BACK PROMPTING, we annotate all the wrong predictions of STEP-BACK PROMPTING in the test set, and categorize them into 5 classes (see Appendix E. 1 for examples in each class):</p>
<ul>
<li>Principle Error: The error happens at the step of Abstraction, where the first principles generated by models are wrong or incomplete.</li>
<li>Factual Error: There is at least one factual error when the model recites its own factual knowledge</li>
<li>Math Error: There is at least one math error in the intermediate steps when math calculations are involved in deriving the final answer.</li>
<li>Context Loss: There is at least one error where the model response loses context from the question, and deviates from addressing the original question</li>
<li>Reasoning Error: We define Reasoning Error as when the model makes at least one error in the intermediate Reasoning steps before arriving at the final answer.</li>
</ul>
<p>All five types of errors are happening during the Reasoning step except Principle Error which points to the failure of the Abstraction step. As shown in Figure 4 (right), Principle Error comprises only a small fraction of the errors the model makes: more than $90 \%$ of the errors happen at the Reasoning step. Among the four error types during Reasoning, Reasoning Error and Math Error are the major error categories. This corroborates with the finding in the ablation study above that very few exemplars are needed to demonstrate to LLMs the Abstraction skill. Reasoning step is still the bottleneck of how well STEP-BACK PROMPTING can perform tasks such as MMLU requiring complex reasoning. For MMLU Physics specifically, the Reasoning and Math skills are critical for solving the problems successfully: even if the first principles are retrieved correctly, deep reasoning and math are involved to derive a correct final answer through a typical multi-step reasoning process.</p>
<h1>5 KNOWLEDGE QA</h1>
<p>We evaluate STEP-BACK PROMPTING on question-answering benchmarks requiring intensive factual knowledge. Knowledge QA has been challenging for LLMs. In this section, we first describe the experimental setup, followed by results and analysis on STEP-BACK PROMPTING.</p>
<h3>5.1 STEP-BACK PROMPTING</h3>
<p>We evaluate STEP-BACK PROMPTING on TimeQA (Chen et al., 2021) and SituatedQA (Zhang \&amp; Choi, 2021) in the Knowledge QA category. We first show the LLMs how to do Abstraction through in-context demonstrations. The step-back question "What was Estella Leopold's education history" in Figure 2 is generated by the LLM through few-shot demonstrations (see Appendix D. 2 for details). Given the knowledge-intensive nature of these queries, we use retrieval augmentation (RAG) in combination with STEP-BACK PROMPTING. The step-back question is used to retrieve relevant facts, which work as additional context (see Table 14 for the prompt) to ground the final reasoning step.</p>
<h3>5.2 RESULTS</h3>
<p>We evaluate the models on the test set of TimeQA. As shown in Table 2, the baseline models of GPT-4 and PaLM-2L achieved $45.6 \%$ and $41.5 \%$, highlighting the difficulty of the task. Applying either CoT or TDB zero-shot (and one-shot) prompting to the baseline model shows no improvement. In contrast, augmenting the baseline model by regular retrieval augmentation (RAG) improves the accuracy to $57.4 \%$, highlighting the fact-intensive nature of the task. The result of Step-Back + RAG shows the effectiveness of going back to a high-level concept, which enables much more reliable retrieval augmentation: the accuracy on TimeQA achieves a remarkable $68.7 \%$.
Next, we segment TimeQA into the Easy and Hard difficulty levels provided in the original dataset. As expected, all methods perform worse on the Hard subset. While RAG can improve the Easy accuracy from $42.6 \%$ to $67.8 \%$, the improvement is much smaller on the Hard accuracy: $40.4 \%$ to $46.8 \%$. This is where STEP-BACK PROMPTING shines by retrieving facts regarding high-level concepts to ground the final reasoning: Step-Back + RAG further improves the Hard accuracy to $62.3 \%$, outperforming GPT-4's $42.6 \%$ from GPT-4. We hypothesize that facts regarding the high-level concepts (such as education history) are much more accessible than the low-level details.</p>
<p>On the SituatedQA benchmark, we observe a moderate quality gain from $54.3 \%$ to our best method of Step-Back + RAG ( $61 \%$ ) with a small gap to GPT-4's $63.2 \%$. Similar to TimeQA, prompting techniques such as CoT and TDB don't help significantly for SituatedQA.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 5: Ablation and error analysis of STEP-BACK PROMPTING on TimeQA. Left: ablation against the number of few-shot exemplars. Right: four classes of errors Step-Back makes with Reasoning and RAG being the dominant error sources.</p>
<h3>5.3 Ablation and Analysis</h3>
<p>Few-shot Ablation: We observe in Figure 5 (left) that the performance of STEP-BACK PROMPTING on TimeQA is robust to the number of exemplars used in demonstration, highlighting again the sample efficiency of in-context learning Abstraction skills for models like PaLM-2L.</p>
<p>Table 3: Results of Step-Back Prompting on Multi-Hop Reasoning. CoT: Chain of Thought prompting, TDB: Take a Deep Breath prompting, RAG: retrieval augmentation generation. The average accuracy is over 5 evaluation runs with the standard deviations included in the parentheses.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">MuSiQue</th>
<th style="text-align: center;">StrategyQA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">PaLM-2L</td>
<td style="text-align: center;">35.5\% (3\%)</td>
<td style="text-align: center;">82.8\% (0.7\%)</td>
</tr>
<tr>
<td style="text-align: left;">PaLM-2L 1-shot</td>
<td style="text-align: center;">29.0\% (0.5\%)</td>
<td style="text-align: center;">76.6\% (0.5\%)</td>
</tr>
<tr>
<td style="text-align: left;">PaLM-2L + CoT</td>
<td style="text-align: center;">38.7\% (3.2\%)</td>
<td style="text-align: center;">83.6\% (0.4\%)</td>
</tr>
<tr>
<td style="text-align: left;">PaLM-2L + CoT 1-shot</td>
<td style="text-align: center;">38.5\% (2.2\%)</td>
<td style="text-align: center;">76.8\% (1.4\%)</td>
</tr>
<tr>
<td style="text-align: left;">PaLM-2L + TDB</td>
<td style="text-align: center;">39.0\% (2.3\%)</td>
<td style="text-align: center;">82.7\% (0.9\%)</td>
</tr>
<tr>
<td style="text-align: left;">PaLM-2L + RAG</td>
<td style="text-align: center;">39.6\% (2.8\%)</td>
<td style="text-align: center;">84.2\% (0.5\%)</td>
</tr>
<tr>
<td style="text-align: left;">PaLM-2L + Step-Back (ours)</td>
<td style="text-align: center;">42.6\% (3.1\%)</td>
<td style="text-align: center;">82.7\% (0.4\%)</td>
</tr>
<tr>
<td style="text-align: left;">PaLM-2L + Step-Back + RAG (ours)</td>
<td style="text-align: center;">$\mathbf{4 2 . 8 \%}(2.0 \%)$</td>
<td style="text-align: center;">$\mathbf{8 6 . 4 \%}(1 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: center;">$38.5 \%(0.2 \%)$</td>
<td style="text-align: center;">$78.3 \%(1.1 \%)$</td>
</tr>
</tbody>
</table>
<p>Error Analysis: Figure 5 (right) shows the breakdown of all the remaining errors made by STEPBACK Prompting on TimeQA. Similar to Section 4.3, we categorize the errors into</p>
<ul>
<li>StepBack: The step-back question generated is not helpful in solving the task.</li>
<li>RAG: RAG fails to retrieve relevant information despite that the step-back question is on target.</li>
<li>Scoring Error: The evaluation by the judge model made a mistake.</li>
<li>Reasoning Error: The retrieved context is relevant, but the model still fails to reason through the context to arrive at the right answer.</li>
</ul>
<p>We find that the StepBack rarely fails. In contrast, we find more than half of the errors are due to reasoning errors. Additionally, $45 \%$ of errors are due to failure in retrieving the right information despite that Abstraction provided by step-back makes it a much easier task. This reflects the difficulty level of the TimeQA task. Additional error analysis of TimeQA is in Appendix A.</p>
<h1>6 Multi-Hop Reasoning</h1>
<p>We evaluate Step-Back Prompting on challenging Multi-Hop reasoning benchmark MuSiQue (Trivedi et al., 2022) and StrategyQA (Geva et al., 2021). We follow the same protocol as Section 5 to implement STEP-BACK Prompting.
Table 3 shows performance of various baselines on the dev set of MuSiQue and StrategyQA. Baseline performance of PaLM-2L and GPT-4 are low ( $35.5 \%$ and $38.5 \%$ for PaLM-2L and GPT-4 respectively) in MuSiQue since it is a hard multihop reasoning benchmark. In contrast, StrategyQA has stronger baselines ( $82.8 \%$ and $78.3 \%$ for PaLM-2L and GPT-4 respectively) probably because it is a binary classification task. CoT and TDB improve model performance a bit in the case of MuSiQue ( $\sim 3 \%$ and 3.5\% respectively) which can be attributed to the inherent reasoning nature of this task where these methods are shown to be helpful. In the case of StrategyQA, there is no significant performance gain with CoT and TDB which could be due to the high baseline performance in this task, with limited scope for these prompting methods to improve performance. Often, 1-shot performance is significantly lower than their zero-shot methods, which could be attributed to potential example bias (Zhao et al., 2021; Parmar et al., 2023). RAG improves model performance ( $\sim 4 \%$ and $2 \%$ for MuSiQue and StrategyQA respectively.) Step-Back Prompting with the power of abstraction produces the best performance of all methods: $42.8 \%$ in MuSiQue and $86.4 \%$ in StrategyQA, significantly outperforming GPT-4 on both tasks. We present a detailed error analysis on StrategyQA in Appendix A.3.</p>
<h2>7 DISCUSSION</h2>
<p>Abstraction helps humans to solve complex tasks by removing irrelevant details and distilling highlevel concepts and principles to guide the problem-solving process. STEP-BACK Prompting breaks</p>
<p>complex tasks such as knowledge-intensive QA, multi-hop reasoning, and science questions into two separate steps of Abstraction and Reasoning. We demonstrate through empirical experiments that Abstraction is an easy skill for the LLMs such as PaLM-2L via sample-efficient in-context learning. Grounding on the high-level concepts and principles, LLMs can leverage their intrinsic Reasoning capabilities to derive the solution. This reduces the chance of reasoning failures in the intermediate steps and is shown to improve the performance on a wide range of complex reasoning tasks. Despite the success, through error analysis, we find that Reasoning is still one of the hardest skills for LLMs to acquire: it is still the dominant failure mode even after the large reduction of task complexity by STEP-BACK PROMPTING.</p>
<p>Nevertheless, Abstraction is neither necessary nor possible in all scenarios. For instance, the task can be as simple as who was the president of the United States in 2000?, in which case there is no such need to step back and ask a high-level question as the answer to such questions is readily available. Questions such as what is the speed of light? point to the first principles themselves. Doing Abstraction in this case would not make a difference either.</p>
<h1>8 RELATED WORK</h1>
<h3>8.1 Prompting</h3>
<p>Few-shot prompting (Brown et al., 2020; Liu et al., 2023; Mishra et al., 2022a; Wei et al., 2022b) has significantly improved model performance across a range of tasks without requiring updating any model parameters. Our work STEP-BACK Prompting is in the same category as the chain-ofthought prompting (Wei et al., 2022b) and scratchpad (Nye et al., 2021) owing to its simplicity and generic nature. But our approach is focused on the key idea of abstraction which is inspired from the fact that taking a step back often helps humans in performing complex tasks. Our work is also related to the recitation-augmented language models (Sun et al., 2022); however in contrast to their work, we explicitly perform step-back and abstraction, with optional use of retrieval augmentation depending on the nature of the task at hand.</p>
<h3>8.2 DECOMPOSITION</h3>
<p>Decomposing a task into simpler tasks and solving these tasks to complete the original task has been an effective way (Zhou et al., 2022; Patel et al., 2022; Khot et al., 2022; Press et al., 2022) to improve model performance on complex tasks. Several prompting methods have been successful in this regard. Our work STEP-BACK Prompting, in contrast, is on making the question more abstract and high-level, which is different from decomposition that is often a low-level breakdowns of the original question. For instance, a generic question for which employer did Steve Jobs work for in 1990? could be what is the employment history of Steve Jobs? While decomposition would lead to sub-questions such as What was Steve Jobs doing in 1990?, Was Steve Jobs employed in 1990? and If Steve Jobs was employed, who was his employer? Furthermore, abstract questions such as what is the employment history of Steve Jobs? are often generic in nature to have a many-to-one mapping since many questions (e.g. which employer did Steve Jobs work for in 1990? and which employer did Steve Jobs work for in 2000?) can have the same abstract question. This is in contrast to decomposition where there is often a one-to-many mapping since there are multiple decomposed sub-problems necessary to solve a given question.</p>
<h2>9 CONCLUSION</h2>
<p>We introduce STEP-BACK Prompting as a simple yet generic method to elicit deep reasoning via abstraction in large language models. Experimentation on LLMs across fact-seeking, commonsense reasoning and domain-specific reasoning benchmarks shows that STEP-BACK Prompting significantly improves model performance. We hypothesize that abstraction helps models to hallucinate less and reason better, probably reflecting the true nature of the model which are often hidden while responding to the original question without abstraction. We hope our work will inspire more human-inspired approaches to elicit the hidden potential of large language models.</p>
<h1>ACKNOWLEDGEMENT</h1>
<p>We thank Andrew Dai, Adams Yu and Hannah Rashkin of Google DeepMind for their insightful feedback on this paper.</p>
<h2>REFERENCES</h2>
<p>Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.</p>
<p>Wenhu Chen, Xinyi Wang, and William Yang Wang. A dataset for answering time-sensitive questions. arXiv preprint arXiv:2108.06314, 2021.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.</p>
<p>Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics, 9:346-361, 2021.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.</p>
<p>Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.</p>
<p>Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.</p>
<p>Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. Decomposed prompting: A modular approach for solving complex tasks. arXiv preprint arXiv:2210.02406, 2022.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35: $22199-22213,2022$.</p>
<p>Royi Lachmy, Valentina Pyatkin, Avshalom Manevich, and Reut Tsarfaty. Draw me a flower: Processing and grounding abstraction in natural language. Transactions of the Association for Computational Linguistics, 2022.</p>
<p>Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step. arXiv preprint arXiv:2305.20050, 2023.</p>
<p>Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 55(9):1-35, 2023.</p>
<p>Swaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin Choi, and Hannaneh Hajishirzi. Reframing instructional prompts to gptk's language. In Findings of the Association for Computational Linguistics: ACL 2022, pp. 589-612, 2022a.</p>
<p>Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task generalization via natural language crowdsourcing instructions. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 3470-3487, 2022b.</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021.</p>
<p>OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.
Mihir Parmar, Swaroop Mishra, Mor Geva, and Chitta Baral. Don’t blame the annotator: Bias already starts in the annotation instructions. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pp. 1771-1781, 2023.</p>
<p>Pruthvi Patel, Swaroop Mishra, Mihir Parmar, and Chitta Baral. Is a question decomposition unit all we need? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 4553-4569, 2022.</p>
<p>Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350, 2022.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485-5551, 2020.</p>
<p>John L Russell. Kepler's laws of planetary motion: 1609-1666. The British journal for the history of science, 2(1):1-24, 1964.</p>
<p>Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and Denny Zhou. Recitation-augmented language models. arXiv preprint arXiv:2210.01296, 2022.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023.</p>
<p>Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Musique: Multihop questions via single-hop question composition. Transactions of the Association for Computational Linguistics, 10:539-554, 2022.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.</p>
<p>Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022a.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837, 2022b.</p>
<p>Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V. Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers. arXiv preprint arXiv:2309.03409, 2023.</p>
<p>Michael Zhang and Eunsol Choi. Situatedqa: Incorporating extra-linguistic contexts into qa. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 7371-7387, 2021.</p>
<p>Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models. In International Conference on Machine Learning, pp. 12697-12706. PMLR, 2021.</p>
<p>Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2022.</p>
<p>Table 4: Step-Back Prompting on GSM8K. CoT: zero-shot Chain of Thought prompting (Kojima et al., 2022), TDB: Take a Deep Breath prompting (Yang et al., 2023), DP: Decomposed Prompting (Khot et al., 2022). The Table reports the average accuracy over 5 evaluation runs, with standard deviations in the parentheses.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">GSM8K</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">PaLM-2L</td>
<td style="text-align: center;">75.8\% (0.2\%)</td>
</tr>
<tr>
<td style="text-align: left;">PaLM-2L 1-shot</td>
<td style="text-align: center;">84.5\% (0.4\%)</td>
</tr>
<tr>
<td style="text-align: left;">PaLM-2L + CoT</td>
<td style="text-align: center;">84.4\% (0.2\%)</td>
</tr>
<tr>
<td style="text-align: left;">PaLM-2L + CoT 1-shot</td>
<td style="text-align: center;">81\% (0.2\%)</td>
</tr>
<tr>
<td style="text-align: left;">PaLM-2L + TDB</td>
<td style="text-align: center;">82.2\% (0.2\%)</td>
</tr>
<tr>
<td style="text-align: left;">PaLM-2L + DP</td>
<td style="text-align: center;">82.2\% (0.08\%)</td>
</tr>
<tr>
<td style="text-align: left;">PaLM-2L + Step-Back (ours)</td>
<td style="text-align: center;">84.3\% (0.2\%)</td>
</tr>
</tbody>
</table>
<h1>A GSM8K ReSults, and Error Analysis</h1>
<h2>A. 1 GSM8K ReSults</h2>
<p>We present in Table 4 the results of STEP-BACK Prompting on GSM8K along with other strong baselines from PaLM-2L runs. We observe that STEP-BACK Prompting achieved competitive performance together with zero-shot CoT and 1-shot standard prompting. We hypothesize that the simplicity of principles (e.g. addition, subtraction, etc.) in GSM8K makes it not absolutely necessary to retrieve the principles first before reasoning. Nonetheless, we still find that STEP-BACK Prompting is the most competitive among all the prompting methods we tested, including the "Take a Deep Breath" prompting optimized for GSM8K in Yang et al. (2023) and Decomposed Prompting in Khot et al. (2022).</p>
<h2>A. 2 TimeQA Error Analysis</h2>
<p>We conduct error analysis to understand where STEP-BACK Prompting fixes the errors the baseline models make. Figure 6 shows that compared to the predictions of baseline PaLM-2L, STEP-BACK Prompting can fix $39.9 \%$ of the predictions where the baseline prediction is wrong, while causing $5.6 \%$ errors.Furthermore, Step-Back + RAG fixes $21.6 \%$ errors coming from RAG. The $\%$ of errors introduced by STEP-BACK Prompting to RAG is still relatively low ( $6.3 \%$ ). Together, this shows that the STEP-BACK Prompting is helpful most of the time, signifying the need and effectiveness of doing Abstraction before directly addressing the original question.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 6: Error Analysis of Step-Back Prompting on TimeQA. Left: Step-Back + RAG vs Baseline predictions. Right: Step-Back RAG vs RAG predictions. Step-Back + RAG can fix $39.9 \%$ of the predictions where the baseline prediction is wrong while causing $5.6 \%$ errors. Furthermore, Step-Back + RAG fixes $21.6 \%$ errors coming from RAG. The $\%$ of errors introduced by STEP-BACK Prompting to RAG is still relatively low ( $6.3 \%$ ).</p>
<p>Table 5: Stats of the evaluation datasets used in this paper.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Domain</th>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Split</th>
<th style="text-align: center;">Number of Examples</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">STEM</td>
<td style="text-align: center;">MMLU high-school Physics</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">151</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MMLU high-school Chemistry</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">203</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GSM8K</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">1319</td>
</tr>
<tr>
<td style="text-align: center;">Knowledge QA</td>
<td style="text-align: center;">TimeQA</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">5226</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TimeQA Easy</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">2613</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TimeQA Hard</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">2613</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SituatedQA</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">2901</td>
</tr>
<tr>
<td style="text-align: center;">Multi-hop Reasoning</td>
<td style="text-align: center;">MuSiQue</td>
<td style="text-align: center;">Dev</td>
<td style="text-align: center;">2417</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">StrategyQA</td>
<td style="text-align: center;">Dev</td>
<td style="text-align: center;">229</td>
</tr>
</tbody>
</table>
<h1>A. 3 StrategyQA Error Analysis</h1>
<p>Figure 7 shows the error analysis of StrategyQA on the predictions of Step-Back + RAG against the baseline model and the raw retrieval augmentation variant of PaLM-2L. Compared to the baseline, Step-Back + RAG can turn $15.4 \%$ wrong predictions into correct predictions, while leading to $6.1 \%$ errors the other way around. Furthermore, Step-Back + RAG fixes $12.7 \%$ errors coming from RAG. The errors introduced to RAG by Step-Back are just $4.4 \%$.</p>
<h2>B DATASET DETAILS</h2>
<p>Table 5 shows the split and number of examples used for evaluations in TimeQA, StrategyQA, MMLU, and GSM8K.</p>
<h2>C Evaluation Details</h2>
<h2>C. 1 Few-shot Examples for Evaluation with PaLM-2L</h2>
<p>Given the model free-form outputs and the target label, we use one positive and one negative output as few-shot examples to demonstrate to the scoring model how to score the output. Table 6 illustrates the prompt we used for the scoring model. We parse out the "Yes" or "No" answer from the scoring model output as a TRUE or FALSE score of the model output.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 7: Error Analysis of Step-Back Prompting on StrategyQA. Left: Step-Back + RAG vs Baseline predictions. Right: Step-Back + RAG vs RAG predictions. Step-Back + RAG is able to turn $15.4 \%$ wrong predictions into correct predictions, while leading to $6.1 \%$ errors the other way around. Furthermore, Step-Back + RAG fixes $12.7 \%$ errors coming from RAG. The errors introduced to RAG by Step-Back are just $4.4 \%$.</p>
<p>Are the following two answers to the given question equivalent? Do not consider whether the answers are right or wrong, but only whether they are equivalent. Directly state "Yes" or "No".
Question: Which title was conferred to Anna Muzychuk in 2007?
Answer 1: Anna Muzychuk was conferred the title of International Master (IM) in 2007. She earned the title by scoring three norms in rapid chess tournaments.
Answer 2: International Master
Answer 1 (short): International Master
Answer 2 (short): International Master
Are the two answers equivalent? Yes
Question: What state is Seattle located in?
Answer 1: Seattle is in Washington State.
Answer 2: The answer is George Washington.
Answer 1 (short): Washington State
Answer 2 (short): George Washington
Are the two answers equivalent? No
Question: $&lt;$ Question $&gt;$
Answer 1: $&lt;$ Model Output $&gt;$
Answer 2: $&lt;$ Target Label $&gt;$
Table 6: Illustration of few shot evaluation with the PaLM-2L model.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">MMLU Physics/Chemistry First-Principle Prompt</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">You are an expert at Physics/Chemistry. You are given</td>
</tr>
<tr>
<td style="text-align: left;">a Physics/Chemistry problem. Your task is to extract the</td>
</tr>
<tr>
<td style="text-align: left;">Physics/Chemistry concepts and principles involved in solving</td>
</tr>
<tr>
<td style="text-align: left;">the problem. Here are a few examples:</td>
</tr>
<tr>
<td style="text-align: left;">Question: $&lt;$ Question Example1&gt;</td>
</tr>
<tr>
<td style="text-align: left;">Principles Involved: $&lt;$ Principles Example1&gt;</td>
</tr>
<tr>
<td style="text-align: left;">$\ldots$</td>
</tr>
<tr>
<td style="text-align: left;">Question: $&lt;$ Question Example5&gt;</td>
</tr>
<tr>
<td style="text-align: left;">Principles Involved: $&lt;$ Principles Example5&gt;</td>
</tr>
<tr>
<td style="text-align: left;">Question: $&lt;$ Question&gt;</td>
</tr>
<tr>
<td style="text-align: left;">Principles Involved:</td>
</tr>
</tbody>
</table>
<p>Table 7: Prompt of extracting the underlying principles involved in MMLU physics and chemistry questions.</p>
<h1>C. 2 HYPER-PARAMETERS FOR EVALUATION WITH PALM-2L</h1>
<p>We use PaLM-2L as the scoring model for evaluation. We experiment with different sampling temperatures, and find that $T=1$ gives us a highly-accurate evaluation. For example, we sampled 100 test examples and the model predictions, and manually rated the correctness of the model scoring. We found that out of 4 trials, the model scoring agrees with human ratings $97 \%, 98 \%, 99 \%$ and $99 \%$ of the time.</p>
<h2>D Prompts and Few shot Examples</h2>
<h2>D. 1 STEM</h2>
<p>For MMLU high-school Physics and Chemistry, we first prompt the model to generate the first principles behind the question. Using the generated first principles, we further prompt the model to generate the final answer through few-shot demonstrations The prompt generating first principles is shown in Table 7 for MMLU high-school Physics and Chemistry.</p>
<div class="codehilite"><pre><span></span><code>MMLU Physics/Chemistry Final Answer Prompt
You are an expert at Physics/Chemistry. You are given a
Physics/Chemistry problem and a set of principles involved in
solving the problem. Solve the problem step by step by following the
principles. Here are a few examples:
Question: &lt;Question Example1&gt;
Principles: &lt;Principles Example1&gt;
Answer: &lt;Answer Example1&gt;
...
Question: &lt;Question Example5&gt;
Principles: &lt;Principles Example5&gt;
Answer: &lt;Answer Example5&gt;
Question: &lt;Question&gt;
Principles: &lt;Principles&gt;
Answer:
</code></pre></div>

<p>Table 8: Prompt of querying the model for final answer with first principles behind the question in MMLU high-school Physics and Chemistry.</p>
<p>After extracting the first principles of solving a particular question, we formulate the prompt in Table 8 to query the model for the final answer.</p>
<p>Tables 9-10 show one demonstration exemplar of Question-Principles-Answer triplets for MMLU high-school Physics and Chemistry, respectively. For GSM8K, given the simplicity of the principles, we directly combine the principles and the solution in the demonstration exemplar. Table 11 shows the exemplar we used in the paper.</p>
<h1>D. 2 KNOWLEDGE QA</h1>
<p>We use the following prompting in Table 12 to demonstrate to the LLM on asking a step-back question for TimeQA and SituatedQA including up to 5 exemplar demonstrations of pairs of Original Question and Step-back Question.
Table 13 shows 5 exemplars from the Train split of TimeQA and SituatedQA as demonstrations of asking step-back questions.</p>
<p>The step-back question is extracted from the model output using the prompt. Using the step-back question, we do retrieval augmentation. Using both the retrieval augmentations from the original question and the step-back question, we formulate the final prompt to query the model for the final answer, as shown in Table 14.</p>
<h2>D. 3 Multi-Hop ReAsoning</h2>
<p>For Multi-Hop Reasoning, we use the same prompting template as in Knowledge QA to ask the step-back question, and query for the final answer given the retrieval augmentations. Table 15 shows 5 demonstration exemplars for asking step-back questions from the Train split of MuSiQue and StrategyQA.</p>
<h2>D. 4 BASELINE Prompts</h2>
<p>For standard zero-shot and few-shot prompting of the baseline model, we formulate the prompt using the template in Table 16 with up to 1 exemplars.</p>
<h2>D. 5 Chain of Thought (CoT)</h2>
<p>For zero-shot CoT prompting, we simply append Let's think step by step. to the question to query the model.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Question</th>
<th style="text-align: left;">A spherical conductor carries a net charge. How is this charge distributed on <br> the sphere? <br> 0) The charge is evenly distributed on the surface. <br> 1) The charge resides on the surface only; the distribution of charge on the <br> surface depends on what other charged objects are near the sphere. <br> 2) The charge moves continually within the sphere. <br> 3) The charge is distributed uniformly throughout the sphere.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Principles</td>
<td style="text-align: left;">Coulomb's Law: the force between two charged particles is proportional to the <br> product of their charges and inversely proportional to the square of the distance <br> between them, F = k * q1 * q2 / r, where F is the electric force, k is a constant, <br> q1 and q2 are the charges the particles carry, and r is the distance between them.</td>
</tr>
<tr>
<td style="text-align: left;">Answer</td>
<td style="text-align: left;">Using the Principles of Coulomb's Law, we can solve the problem as following: <br> Step 1: Apply Coulomb's Law to find out how charges are distributed on the <br> surface. <br> In the case of a spherical conductor, the charges on the surface will repel each <br> other. The further apart the charges are, the less force they will exert on each <br> other. Therefore, the charges will distribute themselves evenly on the surface <br> of the sphere, as this is the configuration that minimizes the repulsive force <br> between them.</td>
</tr>
<tr>
<td style="text-align: left;">Step 2: Apply Coulomb's Law to find out what happens if there are other <br> charges present. <br> The distribution of charge on the surface may also be affected by the presence <br> of other charged objects near the sphere. For example, if a negatively charged <br> object is brought near a positively charged sphere, the negative charges on the <br> sphere will be repelled and will move to the opposite side of the sphere. This <br> will result in a non-uniform distribution of charge on the surface of the sphere.</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Therefore, the correct answer is 1) The charge resides on the surface only; the <br> distribution of charge on the surface depends on what other charged objects are <br> near the sphere.</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>Table 9: Few-shot demonstration exemplar for MMLU high-school Physics.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Question</th>
<th style="text-align: center;">A sample of an unknown chloride compound was dissolved in water, and then titrated with excess $\mathrm{Pb}(\mathrm{NO} 3) 2$ to create a precipitate. After drying, it is determined there are 0.0050 mol of precipitate present. What mass of chloride is present in the original sample? <br> 0) 0.177 g <br> 1) 0.355 g <br> 2) 0.522 g <br> 3) 0.710 g</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Principles</td>
<td style="text-align: center;">Precipitation reactions: Precipitation reactions occur when two soluble salts are mixed and form an insoluble product, called a precipitate. The precipitate can be separated from the solution by filtration or centrifugation. <br> Molar mass: The molar mass of a substance is the mass of one mole of that substance. The molar mass is expressed in grams per mole ( $\mathrm{g} / \mathrm{mol}$ ). <br> Limiting reactant: The limiting reactant is the reactant that is completely consumed in a chemical reaction. The amount of product formed is determined by the amount of limiting reactant.</td>
</tr>
<tr>
<td style="text-align: center;">Answer</td>
<td style="text-align: center;">Assuming the unknown chloride compound is MCl , where M represents the metal cation, the balanced chemical equation for the precipitation reaction is: $P b\left(N O_{3}\right)<em 2="2">{2}(a q)+2 M C l(a q) \longrightarrow P b C l</em>\right)}(s)+2 M N O_{3}(a q)$ <br> Since $P b\left(N O_{3<em 2="2">{2}$ is in excess, MCl is the limiting reactant. The stoichiometry of the reaction indicates that 2 moles of MCl produce 1 mole of $\mathrm{PbCl}</em>$ <br> The correct answer is 1) 0.355 g .}$ precipitate. Therefore, 0.0050 mol of $\mathrm{PbCl}_{2}$ corresponds to 0.010 mol of MCl . <br> The mass of chloride in the original sample can be calculated using the molar mass of chloride $(35.45 \mathrm{~g} / \mathrm{mol})$ : <br> $0.010 \mathrm{~mol} \mathrm{Cl}-35.45 \mathrm{~g} / \mathrm{mol}=0.355 \mathrm{~g} \mathrm{Cl</td>
</tr>
</tbody>
</table>
<p>Table 10: Few-shot demonstration exemplar for MMLU high-school Chemistry.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Question</th>
<th style="text-align: left;">Bella bought stamps at the post office. Some of the stamps had a snowflake <br> design, some had a truck design, and some had a rose design. Bella bought 11 <br> snowflake stamps. She bought 9 more truck stamps than snowflake stamps, and <br> 13 fewer rose stamps than truck stamps. How many stamps did Bella buy in all?</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Answer</td>
<td style="text-align: left;">Let us find and apply the math principles to solve the problem step by step: <br> Step 1. Addition: Calculate the number of truck stamps. <br> Bella bought 11 snowflake stamps. She bought 9 more truck stamps than <br> snowflake stamps: there are 11 + 9 = 20 truck stamps.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Step 2. Subtraction: Calculate the number of rose stamps. <br> Bella bought 13 fewer rose stamps than truck stamps: there are 20 - 13 = 7 rose <br> stamps.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Step 3. Addition: Calculate the total number of stamps in all three colors. <br> Bella bought 11 snowflake stamps, 20 truck stamps, 7 rose stamps: there are 11 <br> + 20 + 7 = 38 stamps in total.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Conclusion: Bella bought 38 stamps in all.</td>
</tr>
</tbody>
</table>
<p>Table 11: Step-Back few-shot demonstration exemplar for GSM8K.</p>
<h1>Knowledge QA Step-Back Prompt</h1>
<p>You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:</p>
<p>Original Question: $&lt;$ Original Question Example1 $&gt;$
Stepback Question: $&lt;$ Stepback Question Example1 $&gt;$
Original Question: $&lt;$ Original Question Example5 $&gt;$
Stepback Question: $&lt;$ Stepback Question Example5 $&gt;$
Original Question: $&lt;$ Original Question $&gt;$
Stepback Question:
Table 12: Prompt of asking step-back question in Knowledge QA tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">dataset</th>
<th style="text-align: left;">Original Question</th>
<th style="text-align: left;">Step-back Question</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">TimeQA</td>
<td style="text-align: left;">Which position did Knox Cunningham <br> hold from May 1955 to Apr 1956?</td>
<td style="text-align: left;">Which positions have Knox Cunning- <br> ham held in his career?</td>
</tr>
<tr>
<td style="text-align: left;">TimeQA</td>
<td style="text-align: left;">Who was the spouse of Anna Karina <br> from 1968 to 1974?</td>
<td style="text-align: left;">Who were the spouses of Anna Karina?</td>
</tr>
<tr>
<td style="text-align: left;">TimeQA</td>
<td style="text-align: left;">Which team did Thierry Audel play for <br> from 2007 to 2008?</td>
<td style="text-align: left;">Which teams did Thierry Audel play for <br> in his career?</td>
</tr>
<tr>
<td style="text-align: left;">TimeQA</td>
<td style="text-align: left;">What was the operator of GCR Class <br> 11E from 1913 to Dec 1922?</td>
<td style="text-align: left;">What were the operators of GCR Class <br> 11E in history?</td>
</tr>
<tr>
<td style="text-align: left;">TimeQA</td>
<td style="text-align: left;">Which country did Sokolovsko belong <br> to from 1392 to 1525?</td>
<td style="text-align: left;">Which countries did Sokolovsko belong <br> to in history?</td>
</tr>
<tr>
<td style="text-align: left;">SituatedQA</td>
<td style="text-align: left;">when was the last time a team from <br> canada won the stanley cup as of 2002</td>
<td style="text-align: left;">which years did a team from canada <br> won the stanley cup as of 2002</td>
</tr>
<tr>
<td style="text-align: left;">SituatedQA</td>
<td style="text-align: left;">when did england last get to the semi <br> final in a world cup as of 2019</td>
<td style="text-align: left;">which years did england get to the semi <br> final in a world cup as of 2019?</td>
</tr>
<tr>
<td style="text-align: left;">SituatedQA</td>
<td style="text-align: left;">what is the biggest hotel in las vegas nv <br> as of November 28, 1993</td>
<td style="text-align: left;">what is the size of the hotels in las vegas <br> nv as of November 28, 1993</td>
</tr>
<tr>
<td style="text-align: left;">SituatedQA</td>
<td style="text-align: left;">who has scored most runs in t20 <br> matches as of 2017</td>
<td style="text-align: left;">What are the runs of players in t20 <br> matches as of 2017</td>
</tr>
<tr>
<td style="text-align: left;">SituatedQA</td>
<td style="text-align: left;">who is the highest paid player in the nba <br> this season as of 2017</td>
<td style="text-align: left;">what is the salary of the high paid play- <br> ers in the nba this season as of 2017</td>
</tr>
</tbody>
</table>
<p>Table 13: Few-shot demonstration exemplars for asking step-back questions in TimeQA and SituatedQA.</p>
<h1>Knowledge QA Final-Answer Prompt</h1>
<p>You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.
$&lt;$ Passage from original retrieval augmentation $&gt;$
$&lt;$ Passage from step-back retrieval augmentation $&gt;$
Original Question: $&lt;$ Original Question $&gt;$
Answer:
Table 14: Prompt of querying the model for final answer with additional contexts from original and step-back retrieval augmentations in TimeQA and SituatedQA</p>
<table>
<thead>
<tr>
<th style="text-align: left;">dataset</th>
<th style="text-align: left;">Original Question</th>
<th style="text-align: left;">Step-back Question</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MuSiQue</td>
<td style="text-align: left;">at year saw the creation of the region <br> where the county of Hertfordshire is <br> located?</td>
<td style="text-align: left;">which region is the county of Hert- <br> fordshire located?</td>
</tr>
<tr>
<td style="text-align: left;">MuSiQue</td>
<td style="text-align: left;">Jan Šindel's was born in what coun- <br> try?</td>
<td style="text-align: left;">what is Jan Šindel's personal his- <br> tory?</td>
</tr>
<tr>
<td style="text-align: left;">MuSiQue</td>
<td style="text-align: left;">When was the abolishment of the <br> studio that distributed The Game?</td>
<td style="text-align: left;">which studio distributed The Game?</td>
</tr>
<tr>
<td style="text-align: left;">MuSiQue</td>
<td style="text-align: left;">What city is the person who broad- <br> ened the doctrine of philosophy of <br> language from?</td>
<td style="text-align: left;">who broadened the doctrine of phi- <br> losophy of language</td>
</tr>
<tr>
<td style="text-align: left;">MuSiQue</td>
<td style="text-align: left;">When was the baseball team win- <br> ning the world series in 2015 base- <br> ball created?</td>
<td style="text-align: left;">which baseball team won the world <br> series in 2015 baseball?</td>
</tr>
<tr>
<td style="text-align: left;">StrategyQA</td>
<td style="text-align: left;">Could the members of The Police <br> perform lawful arrests?</td>
<td style="text-align: left;">what can the members of The Police <br> do?</td>
</tr>
<tr>
<td style="text-align: left;">StrategyQA</td>
<td style="text-align: left;">Would a Monoamine Oxidase candy <br> bar cheer up a depressed friend?</td>
<td style="text-align: left;">What are the effects of Monoamine <br> Oxidase?</td>
</tr>
<tr>
<td style="text-align: left;">StrategyQA</td>
<td style="text-align: left;">Would a dog respond to bell before <br> Grey seal?</td>
<td style="text-align: left;">Would a dog respond to bell before <br> Grey seal?</td>
</tr>
<tr>
<td style="text-align: left;">StrategyQA</td>
<td style="text-align: left;">Is shrimp scampi definitely free of <br> plastic?</td>
<td style="text-align: left;">what is shrimp scampi made of?</td>
</tr>
<tr>
<td style="text-align: left;">StrategyQA</td>
<td style="text-align: left;">Do the anchors on Rede Globo <br> speak Chinese?</td>
<td style="text-align: left;">What languages do the anchors on <br> Rede Globo speak?</td>
</tr>
</tbody>
</table>
<p>Table 15: Few-shot demonstration exemplars for asking step-back questions in MuSiQue and StrategyQA.</p>
<h1>Baseline few-shot Prompt</h1>
<p>You are an expert of world knowledge and physics. Your task is to solve the following question. Here are a few examples:</p>
<p>Question: $&lt;$ Question Example $&gt;$
Answer: $&lt;$ Answer Example $&gt;$
Question: $&lt;$ Question $&gt;$
Answer:
Table 16: Prompt of querying the baseline model for final answer with few-shot demonstration exemplars.</p>
<p>For few-shot CoT prompting, we use the same template as the Baseline prompting in Sec. D. 4 by replacing the few-shot examples using CoT responses, as shown in Tables 18, 19, 20, 21, and 22.</p>
<h2>D. 6 Take a Deep Breath (TDB)</h2>
<p>We study the zero-shot prompting found in Yang et al. (2023): we take Take a deep breath and work on this problem step-by-step, and prepend it to the question.</p>
<h2>E Examples of Error Analysis and Wins of Step-Back Prompting</h2>
<h2>E. 1 MMLU ERROR ANALYSIS</h2>
<p>In Tables 23-27, we show one example for each of the 5 error categories we identified through error analysis on STEP-BACK PROMPTING.</p>
<h2>E. 2 Example Wins from Step-Back Prompting</h2>
<p>Tables 28- 30, 31 32, 33 and 34 illustrate the some successful examples of STEP-BACK PROMPTING on MMLU-Physics, MMLU-Chemistry, TimeQA, SituatedQA, and StrategyQA respectively.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Equal Contribution&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
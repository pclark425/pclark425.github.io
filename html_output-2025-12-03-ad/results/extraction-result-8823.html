<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8823 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8823</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8823</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-158.html">extraction-schema-158</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-238227259</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2105.02605v3.pdf" target="_blank">GraphFormers: GNN-nested Transformers for Representation Learning on Textual Graph</a></p>
                <p><strong>Paper Abstract:</strong> The representation learning on textual graph is to generate low-dimensional embeddings for the nodes based on the individual textual features and the neighbourhood information. Recent breakthroughs on pretrained language models and graph neural networks push forward the development of corresponding techniques. The existing works mainly rely on the cascaded model architecture: the textual features of nodes are independently encoded by language models at first; the textual embeddings are aggregated by graph neural networks afterwards. However, the above architecture is limited due to the independent modeling of textual features. In this work, we propose GraphFormers, where layerwise GNN components are nested alongside the transformer blocks of language models. With the proposed architecture, the text encoding and the graph aggregation are fused into an iterative workflow, {making} each node's semantic accurately comprehended from the global perspective. In addition, a {progressive} learning strategy is introduced, where the model is successively trained on manipulated data and original data to reinforce its capability of integrating information on graph. Extensive evaluations are conducted on three large-scale benchmark datasets, where GraphFormers outperform the SOTA baselines with comparable running efficiency.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8823.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8823.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphFormers</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GNN-nested Transformers (GraphFormers)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model that fuses text encoding and graph aggregation by nesting lightweight GNN (multi-head attention over node-level [CLS] embeddings) alongside transformer layers so node texts and neighbor information are iteratively integrated during encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>GNN-nested Transformer encoding</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Encode each node's text as a token sequence with a leading [CLS]; extract node-level embedding as the [CLS] token; at each transformer layer gather node-level embeddings across the center node and its sampled neighbors, apply a layerwise GNN implemented as Multi-Head Attention (with a learnable position bias) to produce messages, dispatch the GNN outputs back to token sequences by concatenating the message vector with token-level embeddings, then run the transformer layer; iterate for L layers producing final node embedding from the last layer's [CLS].</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>textual graphs (nodes annotated by text); evaluated on citation graph (DBLP), Wikipedia entity graph (Wikidata5M), and session-based product co-view graph (Product)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>For each instance, construct a small subgraph consisting of the center node and uniformly sampled neighbors (default 5); tokenize each node's text with uncased WordPiece, prepend [CLS]; map tokens to initial embeddings; feed these per-node token sequences into the nested workflow (node-level extraction, GNN MHA, concat to token embeddings, transformer).</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Link prediction (predict whether two nodes are connected based on their embeddings); also deployed for ads retrieval in Bing (ranking/retrieval).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>On link prediction (Table 2): Product P@1=0.7786, NDCG=0.8793, MRR=0.8430; DBLP P@1=0.7267, NDCG=0.8565, MRR=0.8133; Wiki P@1=0.3952, NDCG=0.6230, MRR=0.5220. Reported relative improvements over the most competitive cascaded baseline: +2.9% (Product), +4.8% (DBLP), +6.5% (Wiki). Online A/B (ads): RPM +1.87%, CY +0.96%, CPC +0.91%. Encoding complexity per layer O(M^2 + M * P^2) where M nodes and P tokens per node.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Directly compared with cascaded Transformers-GNN baselines (same PLM backbone UniLM-base and aggregators): GraphFormers consistently outperformed PLM-only, PLM+GAT, PLM+Max/Mean/Att pooling variants across datasets (see Table 2). Efficiency/time: similar to PLM+Max in practice; small extra runtime (e.g., ~3.5% overhead at #N=200) and similar memory scaling (Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Fuses cross-node context into text encoding, enabling semantic disambiguation by neighbors; better link-prediction accuracy across three large datasets; comparable runtime/memory to cascaded approaches; architecture allows caching when using unidirectional simplification; benefits from two-stage progressive training.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Layerwise gathering of node-level embeddings incurs O(M^2) cost per layer (though small relative to token-level attention when P is moderate); naive training can shortcut (center node alone suffices) causing undertrained GNN components unless mitigated; when all nodes are mutually dependent naive nested scheme forces re-encoding of neighbor nodes (addressed by unidirectional simplification).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Shortcut learning: when center-node text alone is sufficient, the model may not learn to use neighborhood signals (mitigated by two-stage progressive masking). No direct catastrophic failure cases reported for GraphFormers, but related baselines (e.g., GAT) may degrade on heterogeneous graphs (Wiki) if neighbors have diverse relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraphFormers: GNN-nested Transformers for Representation Learning on Textual Graph', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8823.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8823.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Cascaded Transformers-GNN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cascaded PLM (Transformer) then GNN aggregation architecture</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage pipeline where node texts are first independently encoded by a pretrained language model into embeddings, then a separate GNN or pooling aggregator combines node embeddings to produce final node representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Cascaded Transformer + GNN (independent encoding then aggregation)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each node's text is independently tokenized and encoded by a PLM (e.g., UniLM/BERT) to obtain a per-node vector (typically the final layer [CLS]); a downstream GNN or pooling aggregator (GAT, mean/max pooling, attention pooling, GraphSage-style) consumes these independent text embeddings to produce aggregated node representations.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>textual graphs (same datasets as GraphFormers: DBLP, Wiki, Product)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Encode each node text independently into a fixed embedding using a PLM; construct neighborhood aggregation via GNN or pooling: e.g., attention-weighted sum (GAT), max/mean pooling followed by concat and linear transform, or pooling with attention weights computed w.r.t. center node.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Link prediction (same evaluation), used as primary baseline for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Representative best baseline PLM+Max (Table 2): Product P@1=0.7570, NDCG=0.8678, MRR=0.8280; DBLP P@1=0.6934, NDCG=0.8386, MRR=0.7900; Wiki P@1=0.3712, NDCG=0.6071, MRR=0.5022. PLM+GAT (Table 2) shows weaker performance in some datasets (e.g., Wiki P@1=0.3006).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>GraphFormers outperforms cascaded methods consistently; the paper controlled encoder capacity (same UniLM-base) so improvements are attributed to nested architecture rather than encoder size. PLM-only baseline is worse than PLM+GNNs and GraphFormers.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Simplicity: text encoding and graph aggregation separated; potential to cache text embeddings and reuse them across tasks; lower conceptual coupling between PLM and GNN.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Independent text encoding prevents use of neighbor context during initial encoding, missing opportunities to disambiguate local semantics; less effective at capturing cross-node signals during encoding leading to lower downstream performance according to experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Fails to capture cross-node semantic interactions that GraphFormers leverage; may perform worse when neighbor context is important for interpreting node text.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraphFormers: GNN-nested Transformers for Representation Learning on Textual Graph', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8823.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8823.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Unidirectional Aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Unidirectional Graph Aggregation (simplified GraphFormers)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simplification of GraphFormers where neighbor nodes are encoded independently and only the center node references cached neighbor encodings, enabling reuse and reduced repeated computation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Unidirectional graph-to-text encoding (center-focused augmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Encode neighbor node token sequences independently (as in cascaded approach) and cache their intermediate encodings; when encoding a center node, gather cached neighbor node-level embeddings and apply GNN attention only to augment the center's token-level embeddings; neighbors are not recomputed conditioned on the center.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>textual graphs (DBLP, Wiki, Product used in evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Same per-node tokenization and [CLS] usage; precompute and cache neighbor node [CLS] embeddings across layers, then during center encoding perform GNN attention to produce a message vector concatenated to the center node token embeddings before transformer layers.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Link prediction; used as an efficiency-preserving variant of GraphFormers.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Ablation/Table results: '-Simplified' variant gives comparable or slightly improved numbers: Product P@1 reported as 0.7795 (vs 0.7786 default), DBLP P@1 0.7225 (slightly lower than default 0.7267), Wiki P@1 0.3923 (vs 0.3952). The paper notes performances are comparable across datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Maintains similar accuracy to full GraphFormers while enabling caching and computational savings; compared to cascaded PLM+GNN, it still preserves the center-focused nested aggregation benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Eliminates need to repeatedly encode neighbor nodes when they appear in many center contexts; allows caching and substantial computation savings with little impact on final performance.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Does not model neighbor-to-neighbor interactions that full nested GraphFormers allow (neighbors remain independently encoded), potentially limiting propagation of multi-hop neighbor context between neighbor nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Paper reports no large degradation empirically; possible edge cases where neighbor-neighbor interactions are critical could see reduced performance, but not observed in their datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraphFormers: GNN-nested Transformers for Representation Learning on Textual Graph', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8823.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8823.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Neighborhood Linearization (node+neighbors input)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Per-instance node + sampled neighbors tokenized as separate sequences and concatenated via [CLS] per node</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The practical conversion used to turn a local graph neighborhood into transformer inputs: uniformly sample neighbors, tokenize each node text separately with a [CLS] token, and provide the set of token sequences (center+neighbors) as the model input.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Per-node sequences with [CLS] and sampled neighbors</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each node in the subgraph (center and sampled neighbors) is tokenized independently using uncased WordPiece and prepended with a [CLS] token; the first token ([CLS]) is treated as the node-level embedding that participates in GNN attention; typical neighborhood sampling uses 5 neighbors (uniform, without replacement).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>textual graphs (DBLP titles, Wiki first-sentences, Product descriptions)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Uniform neighbor sampling (default 5 neighbors; if fewer, use all), WordPiece uncased tokenization, pad/truncate sequences to dataset-specific lengths (Product/DBLP length=32, Wiki=64), form batches where each training sample includes query and key each with their neighbors (12 nodes per sample).</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Link prediction (primary), also used for retrieval/ranking in production ads A/B test.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Conversion pipeline used in all reported results; neighbor-size sensitivity study (Table 3) shows performance increases with more neighbors: Product P@1 improves from 0.6485 (#N=1) to 0.7267 (#N=50) for GraphFormers; compared baselines similarly improve but GraphFormers keeps advantage across neighbor sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>This neighborhood linearization is the input format used by both GraphFormers and cascaded baselines (so comparisons focus on architecture differences); compared to PLM-only (no neighbors), methods that include neighbor sequences (GraphFormers, PLM+Max) perform substantially better.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Simple, reproducible way to provide neighborhood context to transformer models; permits use of pretrained tokenizers and per-node [CLS] semantics; supports caching when combined with unidirectional simplification.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Sequence length per node increases total compute; sampling choices (how many neighbors, uniform) can affect marginal gains and computational cost; still potentially misses deeper graph structure beyond local neighbors unless more neighbors are included.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Marginal returns diminish as neighbor count increases; when graph is heterogeneous (Wiki), naive inclusion of neighbors can introduce noise if neighbor relations are semantically diverse.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraphFormers: GNN-nested Transformers for Representation Learning on Textual Graph', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8823.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8823.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Two-stage Progressive Learning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Two-stage progressive training with neighbor-motivating masking (warm-up on polluted nodes)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A training curriculum that first trains the model on 'polluted' examples where token spans are masked to force reliance on neighbors, then fine-tunes on original (unmasked) data to fit the target distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Two-stage progressive learning with span masking</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Stage 1: train on inputs where for each node a span-masking strategy (SpanBERT-like geometric span sampling, p=0.2, lmax=10) masks ~15% tokens to reduce center-node informativeness; Stage 2: continue training on original unmasked data; objective remains link-prediction contrastive loss with in-batch negatives.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>textual graphs (used for DBLP, Wiki, Product)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Apply span masking to tokens of each node during stage 1 according to span masking policy: 15% tokens masked with 80% [MASK], 10% random token, 10% unchanged; neighbors and center both are masked as applicable to force use of whole input.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Link prediction (training strategy to improve graph-aware representations).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Ablation: disabling progressive learning ('-Progressive') decreased P@1 by 0.98% (Product), 1.71% (DBLP), and 1.18% (Wiki), indicating material gains from the two-stage curriculum.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared with single-stage training on original data, progressive training yields better utilization of neighborhood signals; no comparison to other curriculum schemes provided.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Mitigates shortcut learning where center-node text alone suffices; forces GNN components to learn to use neighborhood information leading to consistent performance improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Adds training complexity (two stages) and training time (but authors use same hyperparameters and early stopping); choice of masking parameters can affect results and needs tuning per dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>If masking is too aggressive it may harm convergence or cause over-reliance on neighbors; empirical optimality requires tuning (paper reports chosen hyperparams and gains).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraphFormers: GNN-nested Transformers for Representation Learning on Textual Graph', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8823.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8823.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GNN Aggregators (baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GAT / Pooling-and-Concatenation (Max, Mean, Attention) aggregators used in cascaded baselines</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Common graph aggregation methods applied to independently obtained text embeddings: GAT (attention-weighted neighbor sum) and pooling variants where neighbor embeddings are pooled (max/mean) and concatenated with center embedding or attention-pooled.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>GAT and Pooling-and-Concat aggregators</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>GAT: compute attention scores between center and neighbors and produce weighted sum of neighbor text embeddings; Pooling-and-Concat: compute mean or max pooling over neighbor embeddings (or attention-weighted sum 'Att'), concatenate the result with center embedding and transform linearly to get final node representation.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>textual graphs (DBLP, Wiki, Product used as baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Take PLM-produced per-node [CLS] embeddings and apply the chosen aggregator (GAT attention, mean/max pooling, or attention pooling) to compute neighbor summary, then combine with center embedding.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Link prediction (used for baseline performance comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Examples from Table 2: PLM+GAT Product P@1=0.7540 (worse than PLM+Max 0.7570 and GraphFormers 0.7786); on Wiki PLM+GAT P@1=0.3006 (notably worse than PLM-only 0.3466 and GraphFormers 0.3952).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Pooling-and-Concat (Max/Mean/Att) often outperformed PLM+GAT in experiments, especially on heterogeneous graphs (Wiki) where focusing too heavily on neighbors (GAT) can be harmful; GraphFormers outperform all these aggregators.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Simple operations, well-understood, computationally efficient when used with cached PLM embeddings; attention pooling (GAT) can emphasize more relevant neighbors when relationships are homogeneous.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>GAT can be vulnerable on heterogeneous graphs where neighbor relations are semantically diverse and may introduce noisy signals; pooling may underemphasize center semantics or fail to capture iterative interaction between text and graph signals.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>On heterogeneous graphs (Wiki) GAT underperforms and even hurts performance relative to PLM-only baseline, attributed to neighbor diversity and mismatched relationship semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraphFormers: GNN-nested Transformers for Representation Learning on Textual Graph', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Inductive Representation Learning on Large Graphs <em>(Rating: 2)</em></li>
                <li>Graph Attention Networks <em>(Rating: 2)</em></li>
                <li>PinSage: A Graph Convolutional Neural Network for Web-Scale Recommender Systems <em>(Rating: 2)</em></li>
                <li>SpanBERT: Improving Pre-training by Representing and Predicting Spans <em>(Rating: 2)</em></li>
                <li>Wikidata5M <em>(Rating: 1)</em></li>
                <li>TextGNN: Improving Text Encoder via Graph Neural Network in Sponsored Search <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8823",
    "paper_id": "paper-238227259",
    "extraction_schema_id": "extraction-schema-158",
    "extracted_data": [
        {
            "name_short": "GraphFormers",
            "name_full": "GNN-nested Transformers (GraphFormers)",
            "brief_description": "A model that fuses text encoding and graph aggregation by nesting lightweight GNN (multi-head attention over node-level [CLS] embeddings) alongside transformer layers so node texts and neighbor information are iteratively integrated during encoding.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "GNN-nested Transformer encoding",
            "representation_description": "Encode each node's text as a token sequence with a leading [CLS]; extract node-level embedding as the [CLS] token; at each transformer layer gather node-level embeddings across the center node and its sampled neighbors, apply a layerwise GNN implemented as Multi-Head Attention (with a learnable position bias) to produce messages, dispatch the GNN outputs back to token sequences by concatenating the message vector with token-level embeddings, then run the transformer layer; iterate for L layers producing final node embedding from the last layer's [CLS].",
            "graph_type": "textual graphs (nodes annotated by text); evaluated on citation graph (DBLP), Wikipedia entity graph (Wikidata5M), and session-based product co-view graph (Product)",
            "conversion_method": "For each instance, construct a small subgraph consisting of the center node and uniformly sampled neighbors (default 5); tokenize each node's text with uncased WordPiece, prepend [CLS]; map tokens to initial embeddings; feed these per-node token sequences into the nested workflow (node-level extraction, GNN MHA, concat to token embeddings, transformer).",
            "downstream_task": "Link prediction (predict whether two nodes are connected based on their embeddings); also deployed for ads retrieval in Bing (ranking/retrieval).",
            "performance_metrics": "On link prediction (Table 2): Product P@1=0.7786, NDCG=0.8793, MRR=0.8430; DBLP P@1=0.7267, NDCG=0.8565, MRR=0.8133; Wiki P@1=0.3952, NDCG=0.6230, MRR=0.5220. Reported relative improvements over the most competitive cascaded baseline: +2.9% (Product), +4.8% (DBLP), +6.5% (Wiki). Online A/B (ads): RPM +1.87%, CY +0.96%, CPC +0.91%. Encoding complexity per layer O(M^2 + M * P^2) where M nodes and P tokens per node.",
            "comparison_to_others": "Directly compared with cascaded Transformers-GNN baselines (same PLM backbone UniLM-base and aggregators): GraphFormers consistently outperformed PLM-only, PLM+GAT, PLM+Max/Mean/Att pooling variants across datasets (see Table 2). Efficiency/time: similar to PLM+Max in practice; small extra runtime (e.g., ~3.5% overhead at #N=200) and similar memory scaling (Table 5).",
            "advantages": "Fuses cross-node context into text encoding, enabling semantic disambiguation by neighbors; better link-prediction accuracy across three large datasets; comparable runtime/memory to cascaded approaches; architecture allows caching when using unidirectional simplification; benefits from two-stage progressive training.",
            "disadvantages": "Layerwise gathering of node-level embeddings incurs O(M^2) cost per layer (though small relative to token-level attention when P is moderate); naive training can shortcut (center node alone suffices) causing undertrained GNN components unless mitigated; when all nodes are mutually dependent naive nested scheme forces re-encoding of neighbor nodes (addressed by unidirectional simplification).",
            "failure_cases": "Shortcut learning: when center-node text alone is sufficient, the model may not learn to use neighborhood signals (mitigated by two-stage progressive masking). No direct catastrophic failure cases reported for GraphFormers, but related baselines (e.g., GAT) may degrade on heterogeneous graphs (Wiki) if neighbors have diverse relationships.",
            "uuid": "e8823.0",
            "source_info": {
                "paper_title": "GraphFormers: GNN-nested Transformers for Representation Learning on Textual Graph",
                "publication_date_yy_mm": "2021-05"
            }
        },
        {
            "name_short": "Cascaded Transformers-GNN",
            "name_full": "Cascaded PLM (Transformer) then GNN aggregation architecture",
            "brief_description": "A two-stage pipeline where node texts are first independently encoded by a pretrained language model into embeddings, then a separate GNN or pooling aggregator combines node embeddings to produce final node representations.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "Cascaded Transformer + GNN (independent encoding then aggregation)",
            "representation_description": "Each node's text is independently tokenized and encoded by a PLM (e.g., UniLM/BERT) to obtain a per-node vector (typically the final layer [CLS]); a downstream GNN or pooling aggregator (GAT, mean/max pooling, attention pooling, GraphSage-style) consumes these independent text embeddings to produce aggregated node representations.",
            "graph_type": "textual graphs (same datasets as GraphFormers: DBLP, Wiki, Product)",
            "conversion_method": "Encode each node text independently into a fixed embedding using a PLM; construct neighborhood aggregation via GNN or pooling: e.g., attention-weighted sum (GAT), max/mean pooling followed by concat and linear transform, or pooling with attention weights computed w.r.t. center node.",
            "downstream_task": "Link prediction (same evaluation), used as primary baseline for comparison.",
            "performance_metrics": "Representative best baseline PLM+Max (Table 2): Product P@1=0.7570, NDCG=0.8678, MRR=0.8280; DBLP P@1=0.6934, NDCG=0.8386, MRR=0.7900; Wiki P@1=0.3712, NDCG=0.6071, MRR=0.5022. PLM+GAT (Table 2) shows weaker performance in some datasets (e.g., Wiki P@1=0.3006).",
            "comparison_to_others": "GraphFormers outperforms cascaded methods consistently; the paper controlled encoder capacity (same UniLM-base) so improvements are attributed to nested architecture rather than encoder size. PLM-only baseline is worse than PLM+GNNs and GraphFormers.",
            "advantages": "Simplicity: text encoding and graph aggregation separated; potential to cache text embeddings and reuse them across tasks; lower conceptual coupling between PLM and GNN.",
            "disadvantages": "Independent text encoding prevents use of neighbor context during initial encoding, missing opportunities to disambiguate local semantics; less effective at capturing cross-node signals during encoding leading to lower downstream performance according to experiments.",
            "failure_cases": "Fails to capture cross-node semantic interactions that GraphFormers leverage; may perform worse when neighbor context is important for interpreting node text.",
            "uuid": "e8823.1",
            "source_info": {
                "paper_title": "GraphFormers: GNN-nested Transformers for Representation Learning on Textual Graph",
                "publication_date_yy_mm": "2021-05"
            }
        },
        {
            "name_short": "Unidirectional Aggregation",
            "name_full": "Unidirectional Graph Aggregation (simplified GraphFormers)",
            "brief_description": "A simplification of GraphFormers where neighbor nodes are encoded independently and only the center node references cached neighbor encodings, enabling reuse and reduced repeated computation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Unidirectional graph-to-text encoding (center-focused augmentation)",
            "representation_description": "Encode neighbor node token sequences independently (as in cascaded approach) and cache their intermediate encodings; when encoding a center node, gather cached neighbor node-level embeddings and apply GNN attention only to augment the center's token-level embeddings; neighbors are not recomputed conditioned on the center.",
            "graph_type": "textual graphs (DBLP, Wiki, Product used in evaluation)",
            "conversion_method": "Same per-node tokenization and [CLS] usage; precompute and cache neighbor node [CLS] embeddings across layers, then during center encoding perform GNN attention to produce a message vector concatenated to the center node token embeddings before transformer layers.",
            "downstream_task": "Link prediction; used as an efficiency-preserving variant of GraphFormers.",
            "performance_metrics": "Ablation/Table results: '-Simplified' variant gives comparable or slightly improved numbers: Product P@1 reported as 0.7795 (vs 0.7786 default), DBLP P@1 0.7225 (slightly lower than default 0.7267), Wiki P@1 0.3923 (vs 0.3952). The paper notes performances are comparable across datasets.",
            "comparison_to_others": "Maintains similar accuracy to full GraphFormers while enabling caching and computational savings; compared to cascaded PLM+GNN, it still preserves the center-focused nested aggregation benefits.",
            "advantages": "Eliminates need to repeatedly encode neighbor nodes when they appear in many center contexts; allows caching and substantial computation savings with little impact on final performance.",
            "disadvantages": "Does not model neighbor-to-neighbor interactions that full nested GraphFormers allow (neighbors remain independently encoded), potentially limiting propagation of multi-hop neighbor context between neighbor nodes.",
            "failure_cases": "Paper reports no large degradation empirically; possible edge cases where neighbor-neighbor interactions are critical could see reduced performance, but not observed in their datasets.",
            "uuid": "e8823.2",
            "source_info": {
                "paper_title": "GraphFormers: GNN-nested Transformers for Representation Learning on Textual Graph",
                "publication_date_yy_mm": "2021-05"
            }
        },
        {
            "name_short": "Neighborhood Linearization (node+neighbors input)",
            "name_full": "Per-instance node + sampled neighbors tokenized as separate sequences and concatenated via [CLS] per node",
            "brief_description": "The practical conversion used to turn a local graph neighborhood into transformer inputs: uniformly sample neighbors, tokenize each node text separately with a [CLS] token, and provide the set of token sequences (center+neighbors) as the model input.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Per-node sequences with [CLS] and sampled neighbors",
            "representation_description": "Each node in the subgraph (center and sampled neighbors) is tokenized independently using uncased WordPiece and prepended with a [CLS] token; the first token ([CLS]) is treated as the node-level embedding that participates in GNN attention; typical neighborhood sampling uses 5 neighbors (uniform, without replacement).",
            "graph_type": "textual graphs (DBLP titles, Wiki first-sentences, Product descriptions)",
            "conversion_method": "Uniform neighbor sampling (default 5 neighbors; if fewer, use all), WordPiece uncased tokenization, pad/truncate sequences to dataset-specific lengths (Product/DBLP length=32, Wiki=64), form batches where each training sample includes query and key each with their neighbors (12 nodes per sample).",
            "downstream_task": "Link prediction (primary), also used for retrieval/ranking in production ads A/B test.",
            "performance_metrics": "Conversion pipeline used in all reported results; neighbor-size sensitivity study (Table 3) shows performance increases with more neighbors: Product P@1 improves from 0.6485 (#N=1) to 0.7267 (#N=50) for GraphFormers; compared baselines similarly improve but GraphFormers keeps advantage across neighbor sizes.",
            "comparison_to_others": "This neighborhood linearization is the input format used by both GraphFormers and cascaded baselines (so comparisons focus on architecture differences); compared to PLM-only (no neighbors), methods that include neighbor sequences (GraphFormers, PLM+Max) perform substantially better.",
            "advantages": "Simple, reproducible way to provide neighborhood context to transformer models; permits use of pretrained tokenizers and per-node [CLS] semantics; supports caching when combined with unidirectional simplification.",
            "disadvantages": "Sequence length per node increases total compute; sampling choices (how many neighbors, uniform) can affect marginal gains and computational cost; still potentially misses deeper graph structure beyond local neighbors unless more neighbors are included.",
            "failure_cases": "Marginal returns diminish as neighbor count increases; when graph is heterogeneous (Wiki), naive inclusion of neighbors can introduce noise if neighbor relations are semantically diverse.",
            "uuid": "e8823.3",
            "source_info": {
                "paper_title": "GraphFormers: GNN-nested Transformers for Representation Learning on Textual Graph",
                "publication_date_yy_mm": "2021-05"
            }
        },
        {
            "name_short": "Two-stage Progressive Learning",
            "name_full": "Two-stage progressive training with neighbor-motivating masking (warm-up on polluted nodes)",
            "brief_description": "A training curriculum that first trains the model on 'polluted' examples where token spans are masked to force reliance on neighbors, then fine-tunes on original (unmasked) data to fit the target distribution.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Two-stage progressive learning with span masking",
            "representation_description": "Stage 1: train on inputs where for each node a span-masking strategy (SpanBERT-like geometric span sampling, p=0.2, lmax=10) masks ~15% tokens to reduce center-node informativeness; Stage 2: continue training on original unmasked data; objective remains link-prediction contrastive loss with in-batch negatives.",
            "graph_type": "textual graphs (used for DBLP, Wiki, Product)",
            "conversion_method": "Apply span masking to tokens of each node during stage 1 according to span masking policy: 15% tokens masked with 80% [MASK], 10% random token, 10% unchanged; neighbors and center both are masked as applicable to force use of whole input.",
            "downstream_task": "Link prediction (training strategy to improve graph-aware representations).",
            "performance_metrics": "Ablation: disabling progressive learning ('-Progressive') decreased P@1 by 0.98% (Product), 1.71% (DBLP), and 1.18% (Wiki), indicating material gains from the two-stage curriculum.",
            "comparison_to_others": "Compared with single-stage training on original data, progressive training yields better utilization of neighborhood signals; no comparison to other curriculum schemes provided.",
            "advantages": "Mitigates shortcut learning where center-node text alone suffices; forces GNN components to learn to use neighborhood information leading to consistent performance improvements.",
            "disadvantages": "Adds training complexity (two stages) and training time (but authors use same hyperparameters and early stopping); choice of masking parameters can affect results and needs tuning per dataset.",
            "failure_cases": "If masking is too aggressive it may harm convergence or cause over-reliance on neighbors; empirical optimality requires tuning (paper reports chosen hyperparams and gains).",
            "uuid": "e8823.4",
            "source_info": {
                "paper_title": "GraphFormers: GNN-nested Transformers for Representation Learning on Textual Graph",
                "publication_date_yy_mm": "2021-05"
            }
        },
        {
            "name_short": "GNN Aggregators (baselines)",
            "name_full": "GAT / Pooling-and-Concatenation (Max, Mean, Attention) aggregators used in cascaded baselines",
            "brief_description": "Common graph aggregation methods applied to independently obtained text embeddings: GAT (attention-weighted neighbor sum) and pooling variants where neighbor embeddings are pooled (max/mean) and concatenated with center embedding or attention-pooled.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "GAT and Pooling-and-Concat aggregators",
            "representation_description": "GAT: compute attention scores between center and neighbors and produce weighted sum of neighbor text embeddings; Pooling-and-Concat: compute mean or max pooling over neighbor embeddings (or attention-weighted sum 'Att'), concatenate the result with center embedding and transform linearly to get final node representation.",
            "graph_type": "textual graphs (DBLP, Wiki, Product used as baselines)",
            "conversion_method": "Take PLM-produced per-node [CLS] embeddings and apply the chosen aggregator (GAT attention, mean/max pooling, or attention pooling) to compute neighbor summary, then combine with center embedding.",
            "downstream_task": "Link prediction (used for baseline performance comparison).",
            "performance_metrics": "Examples from Table 2: PLM+GAT Product P@1=0.7540 (worse than PLM+Max 0.7570 and GraphFormers 0.7786); on Wiki PLM+GAT P@1=0.3006 (notably worse than PLM-only 0.3466 and GraphFormers 0.3952).",
            "comparison_to_others": "Pooling-and-Concat (Max/Mean/Att) often outperformed PLM+GAT in experiments, especially on heterogeneous graphs (Wiki) where focusing too heavily on neighbors (GAT) can be harmful; GraphFormers outperform all these aggregators.",
            "advantages": "Simple operations, well-understood, computationally efficient when used with cached PLM embeddings; attention pooling (GAT) can emphasize more relevant neighbors when relationships are homogeneous.",
            "disadvantages": "GAT can be vulnerable on heterogeneous graphs where neighbor relations are semantically diverse and may introduce noisy signals; pooling may underemphasize center semantics or fail to capture iterative interaction between text and graph signals.",
            "failure_cases": "On heterogeneous graphs (Wiki) GAT underperforms and even hurts performance relative to PLM-only baseline, attributed to neighbor diversity and mismatched relationship semantics.",
            "uuid": "e8823.5",
            "source_info": {
                "paper_title": "GraphFormers: GNN-nested Transformers for Representation Learning on Textual Graph",
                "publication_date_yy_mm": "2021-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Inductive Representation Learning on Large Graphs",
            "rating": 2,
            "sanitized_title": "inductive_representation_learning_on_large_graphs"
        },
        {
            "paper_title": "Graph Attention Networks",
            "rating": 2,
            "sanitized_title": "graph_attention_networks"
        },
        {
            "paper_title": "PinSage: A Graph Convolutional Neural Network for Web-Scale Recommender Systems",
            "rating": 2,
            "sanitized_title": "pinsage_a_graph_convolutional_neural_network_for_webscale_recommender_systems"
        },
        {
            "paper_title": "SpanBERT: Improving Pre-training by Representing and Predicting Spans",
            "rating": 2,
            "sanitized_title": "spanbert_improving_pretraining_by_representing_and_predicting_spans"
        },
        {
            "paper_title": "Wikidata5M",
            "rating": 1,
            "sanitized_title": "wikidata5m"
        },
        {
            "paper_title": "TextGNN: Improving Text Encoder via Graph Neural Network in Sponsored Search",
            "rating": 1,
            "sanitized_title": "textgnn_improving_text_encoder_via_graph_neural_network_in_sponsored_search"
        }
    ],
    "cost": 0.015193249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>GraphFormers: GNN-nested Transformers for Representation Learning on Textual Graph
9 Oct 2023</p>
<p>Junhan Yang 
University of Science and Technology of China
HefeiChina</p>
<p>Zheng Liu 
Shitao Xiao stxiao@bupt.edu.cn 
Beijing University of Posts and Telecommunications
BeijingChina</p>
<p>Chaozhuo Li 
Defu Lian liandefu@ustc.edu.cn 
University of Science and Technology of China
HefeiChina</p>
<p>Sanjay Agrawal 
Microsoft India Development Center
BengaluruIndia</p>
<p>Amit Singh 
Microsoft India Development Center
BengaluruIndia</p>
<p> Guangzhong 
University of Science and Technology of China
HefeiChina</p>
<p>Xing Xie xingx@microsoft.com </p>
<p>Microsoft Research Asia
BeijingChina</p>
<p>GraphFormers: GNN-nested Transformers for Representation Learning on Textual Graph
9 Oct 20238A5F90ED6F32BC8C1B78CC349AA5FB78arXiv:2105.02605v3[cs.CL]
The representation learning on textual graph is to generate low-dimensional embeddings for the nodes based on the individual textual features and the neighbourhood information.Recent breakthroughs on pretrained language models and graph neural networks push forward the development of corresponding techniques.The existing works mainly rely on the cascaded model architecture: the textual features of nodes are independently encoded by language models at first; the textual embeddings are aggregated by graph neural networks afterwards.However, the above architecture is limited due to the independent modeling of textual features.In this work, we propose GraphFormers, where layerwise GNN components are nested alongside the transformer blocks of language models.With the proposed architecture, the text encoding and the graph aggregation are fused into an iterative workflow, making each node's semantic accurately comprehended from the global perspective.In addition, a progressive learning strategy is introduced, where the model is successively trained on manipulated data and original data to reinforce its capability of integrating information on graph.Extensive evaluations are conducted on three large-scale benchmark datasets, where GraphFormers outperform the SOTA baselines with comparable running efficiency.The source code is released at https://github.com/microsoft/GraphFormers .</p>
<p>Introduction</p>
<p>The textual graph is a widely existed data format, where each node is annotated with its textual feature.The representation learning on textual graph is to generate low-dimensional node embeddings based on the individual textual features and the information from the neighbourhood.In recent years, the breakthroughs in pretrained language models and graph neural networks contribute to the development of corresponding techniques.Particularly, with pretrained language models, such as BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019a), the underlying semantics of texts can be captured more precisely; at the same time, with graph neural networks, like GraphSage (Hamilton et al., 2017a) and GAT (Velikovi et al., 2018), neighbours can be effectively aggregated for more informative node embeddings.It is necessary to combine both techniques for better textual graph representation.As suggested by GraphSage (Hamilton et al., 2017a) and PinSage (Ying et al., 2018), the textual feature can be independently modeled by text encoders and further aggregated by rear-mounted GNNs for the final node embeddings.Such a representation paradigm has been widely adopted by subsequent works on various scenarios (Zhu et al., 2021;Li et al., 2021;Hu et al., 2020;Liu et al., 2019b;Zhou et al., 2019), where GNNs are combined with powerful PLM-based text encoders.</p>
<p>The above way of combination is called the "Cascaded Transformers-GNN" architecture (Figure 1 A), as the language models (built upon Transformers) are deployed ahead of the GNN component.With the above architecture, the text encoding and the graph aggregation are performed in two consecutive steps, where there is no information exchange between the nodes when text embeddings are generated.However, the above workflow is defective considering that the linked nodes are correlated, whose underlying semantics can be mutually enhanced.For example, given a node "notes on transformers" and its neighbour "tutorials on machine translation"; by making reference to the whole context, the "transformers" here can be interpreted as a machine learning model, rather than an electric device.</p>
<p>Our Work.We propose "GNN-nested Transformers" (GraphFormers), which are highlighted for the fusion of GNNs and language models (Figure 1 B).In GraphFormers, the GNN components are nested alongside the transformer layers (TRM) of language models, where the text encoding and graph aggregation are fused as an iteratively workflow.In each iteration, the linked nodes will exchange information with each other in the layerwise GNN component; thus, each node will be augmented by its neighbourhood information.The transformer component will work on the augmented node features, where increasingly informative node representations can be generated for the next iteration.Compared with the cascaded architecture, GraphFormers achieve more sufficient utilization of the cross-node information on graph, which significantly benefit the representation quality.Given that the layerwise GNN components merely involve simple and effective multi-head attention, GraphFormers preserve comparable running costs as the existing cascaded Transformers-GNN models.</p>
<p>On top of the proposed model architecture, we further improve GraphFormers' representation quality and practicability as follows.Firstly, the training of GraphFormers is likely to be shortcut: in many cases, the center node itself can be "sufficiently informative", where the training tasks can be accomplished without leveraging the neighbourhood information.As such, GraphFormers may end up with insufficiently trained GNNs.Inspired by recent success of curriculum learning (Bengio et al., 2009), we propose to train the model progressively: the first round of training is performed with manipulated data, where the nodes are randomly polluted; thus, it becomes harder to make prediction merely rely on the center nodes, and the model will be forced to leverage the whole input nodes.The second round of training gets back to the unpolluted data, where the model will be fit into the targeted distribution.Another concern about GraphFormers is that all the linked nodes are mutually dependent in the representation process: once a new node is presented, all the neighbours, regardless of whether they have been processed before, need to be encoded from scratch.As a result, a great deal of unnecessary computations will be incurred.We introduce unidirectional graph attention to alleviate this problem: only the center node is required to make reference to the neighbours, while the neighbour nodes remain independently encoded.By this means, the existing neighbours' encoding results can be cached and reused, which significantly saves the computation cost.</p>
<p>Extensive evaluations are conducted with three million-scale textual graph datasets: DBLP, Wiki and Product, where the representation quality is measured by the link prediction accuracy.According to our experiment results, GraphFormers significantly outperform the SOTA cascaded Transformers-GNN baselines with comparable running efficiency.</p>
<p>Related Work</p>
<p>The textual graph representation is an important research topic in multiple areas, such as natural language processing, information retrieval and graph learning (Yang et al., 2015;Wang et al., 2016b,a;Yasunaga et al., 2017;Wang et al., 2019a;Xu et al., 2019).To learn high-quality representation for textual graph, techniques on natural language understanding and graph representation need to be jointly leveraged.In recent years, breakthroughs on pretrained language models (PLM) and graph neural networks (GNN) significantly advance the development of corresponding techniques.</p>
<p>PLM.The PLMs are proposed to learn universal language models with neural networks trained on large-scale corpus.The early works were based on shallow networks, e.g, word embeddings learned by Skip-Gram (Mikolov et al., 2013) and GloVe (Pennington et al., 2014).In recent years, the backbone networks are being quickly scaled up: from EMLo (Peters et al., 2018), GPT (Radford et al., 2018), to BERT (Devlin et al., 2018), XLNet (Yang et al., 2019), T5 (Raffel et al., 2019), GPT-3 (Brown et al., 2020).The large-scale models, which get fully trained with massive data, demonstrate superior performances on general NLP tasks.One of the most critical usages of PLMs is text representation, where the underlying semantics of texts are captured by low-dimensional embeddings.Such embeddings achieve competitive results on downstream tasks, like text retrieval and classification (Reimers and Gurevych, 2019;Luan et al., 2020;Gao et al., 2021;Su et al., 2021).</p>
<p>GNN. Graph neural networks are recognized as powerful tools of modeling graph data (Hamilton et al., 2017b;Zhou et al., 2020).Such methods (e.g., GCN (Kipf and Welling, 2016), GAT (Velikovi et al., 2018), GraphSage (Hamilton et al., 2017a)) learn effective message passing mechanisms such that information between the nodes can get aggregated for expressive graph representations.</p>
<p>Graph neural networks may also incorporate node attributes, like texts; and it's quite straightforward to leverage GNNs and PLMs for textual graph representation following the "cascaded architecture" suggested by GraphSage (Hamilton et al., 2017a): the node features are independently encoded at first; then, the node embeddings are aggregated via GNNs to generate the final representations.Such a representation paradigm is widely adopted by subsequent works (Zhu et al., 2021;Li et al., 2021;Hu et al., 2020;Liu et al., 2019b;Zhou et al., 2019).However, the above approaches treat the text encoding and graph aggregation as two consecutive steps, where the node-level features are independently processed.Our work is different from these approaches as the text encoding and graph aggregation are fused as an iterative workflow based on the "GNN-nested Transformers".</p>
<p>GraphFormers</p>
<p>In this work, we deal with textual graph data, where each node x is a text.The node x together with its neighbours N x are denoted as G x .Our model learns the embedding for node x based on its own textual feature and the information of its neighbourhood N x .The generated embeddings are expected to capture the relationship between the nodes, i.e., to accurately predict whether two nodes x q and x k are connected based on the embedding similarity.</p>
<p>GNN-nested Transformers</p>
<p>The encoding process of GraphFormers is indicated as follows.The input nodes (the center node and its neighbours) are tokenized into sequences of tokens, with special tokens [CLS] padded in the front, whose states are used for node representation.The input sequences are mapped into the initial embedding sequences {H 0 g } G based on the summation of word embeddings and position embeddings.The embedding sequences are encoded by multiple layers of GNN-nested Transformers (shown as Figure 2), where the graph aggregation and text encoding are iteratively performed. Graph Aggregation in GNN.Each node is enhanced by its neighbourhood information based on the layerwise graph aggregation.For each node in the l-th layer, the first token-level embedding (corresponding to [CLS]) is taken as the node-level embedding:
z l g  H l g [0]
. The node-level embeddings are gathered from all the nodes and passed to the layerwise GNN for graph aggregation.We leverage Multi-Head Attention (MHA) to encode the node-level embeddings Z l G ({z l g } G ), similar as GAT (Velikovi et al., 2018).For each attention head, the scaled dot-product attention is performed as:
l G = MHA(Z l G ); MHA(Z l G ) = Concat(head 1 , ..., head h ); head j = softmax( QK T  d + B)V; Q = Z l G W Q j ; K = Z l G W K j ; V = Z l G W V j ;(1)
In the above equations, W Q j , W K j , and W V j are the projection matrices of MHA, corresponding to the j-th attention head.A learnable position bias B is added to the dot-product result; the positions differentiate the relationship between the nodes; i.e., "center-to-center" (x to x), "center-to-neighbour" (x to N x ), and "neighbour-to-neighbour" (N x to N x ), respectively.</p>
<p>Each of the embeddings l g ( l g  l G ) is dispatched to its original node and concatenated () with the token-level embeddings, which gives rise to the graph-augmented token-level embeddings:
H l g  Concat( l g , H l g ).(2)
In this place, the GNN-processed node-level embeddings l G can be interpreted as "messagers", with which the neighbourhood information can be introduced to each of the nodes.</p>
<p> Text Encoding in Transformer.The graph-augmented token-level embeddings H l g are processed by the transformer component (Vaswani et al., 2017), where the following computations are performed:
H l g = LN(H l g + MHA asy ( H l g )); H l+1 g = LN( H l g + MLP( H l g )).
(3)</p>
<p>In the above equations, MLP is the Multi-Layer Projection unit, and LN is the Layer-Norm unit.We use asymmetric Multi-Head Attention (MHA asy ), where Q, K, V are computed as:
Q = H l g W Q j ; K = H l g W K j ; V = H l g W V j .(4)
Therefore, the output sequence H l+1 g will be of the same length as the input sequence H l g .The encoding result will be used as the input token-level embeddings for the next layer.The node-level embedding at the last layer z L x (i.e., H L g [0]) will be used as the final node representation. Workflow.We summarize GraphFormers' encoding workflow as Algorithm 1.The initial tokenlevel embeddings {H 0 g } G are independently encoded by the first Transformer layer TRM 0 .For a Algorithm 1: GraphFormers' Workflow Input: The input graphs G (consist of the center node x and its neighbours).</p>
<p>Output: The embedding for the center node h x .begin</p>
<p>for each text g  G do H 1 g  TRM 0 (H 0 g ); // Get the initial token-level embeddings
for l = 1, ..., L  1 do Z l G  {z l g |gG}; // Gather node-level embeddings to GNN l G  GNN(Z l G ); // Graph aggregation in GNN for each text g  G do 8 H l g  Concat( l g , H l g ); // Get graph-augmented token-level embeddings 9 H l+1 g  TRM l ( H l g ); // Text encoding in Transformer Return h x  z L x ;
L-layer GraphFormers, the graph aggregation and text encoding are iteratively performed for the subsequent L-1 steps (from 1 to L  1).In each step, the node-level embeddings Z l G are gathered and processed by the layerwise GNN component.The output node-level embeddings l G are dispatched to their original nodes, which generates the graph-augmented token-level embeddings H l g .The graphaugmented token-level embedding are further processed by the Transformer component.Finally, The node-level embedding (for the center node x) in the last layer z L</p>
<p>x is taken as our representation result. Encoding Complexity.Given an input of M nodes, each one has P tokens; the time complexity of each layer's encoding operation is O(M 2 + M P 2 ): the graph aggregation takes O(M 2 ), because M node-level embeddings are gathered for multi-head attention; the text encoding takes O(M P 2 ), as each of the M node calls for the multi-head attention of P tokens.Compared with Transformers, the GNN's computation cost is much smaller, mainly because of two reasons: 1) M 2  M P 2 in general, 2) operations like MLP are not needed in graph aggregation.Therefore, the working efficiency of GraphFormers is close to the cascaded GNN-Transformers as the extra computation cost of layerwise graph aggregation is relatively small.Such a property is also empirically verified in our experiment.</p>
<p>Model Simplification: Unidirectional Graph Aggregation</p>
<p>One concern about GraphFormers is that the input nodes are mutually dependent on each other during the encoding process.As a result, to generate the embedding for a node, all the related nodes in its neighbourhood need to be encoded from scratch, regardless of whether they have been processed before.Such a property is unfavorable in practice as a great deal of unnecessary computation cost might be incurred (i.e., a node will be repetitively encoded every time it serves as a neighbour node).We leverage a simple but effective simplification, the unidirectional graph aggregation, to address this problem.Particularly, only the center node x is required to make reference to the neighbourhood; while the rest of nodes N x remain independently encoded all by their own textual features:
H l+1 g = TRM l ( H l x ), g = x; TRM l (H l g ), g  N x .
(5)</p>
<p>Because the encoding of the neighbour nodes is independent of the center node, the intermediate encoding results {z 1...L g } Nx can be cached in storage3 and reused in subsequent computations when they are needed.As a result, the nodes can be prevented from being encoded repetitively, which saves a great deal of unnecessary computation cost.We empirically verify that GraphFormers maintain similar performances when the above simplification is introduced.</p>
<p>Model Training: Two-Stage Progressive Learning</p>
<p> Training Objective.We take advantage of link prediction as our training task.Given a pair of nodes q and k, the model is learned to predict whether they are connected based on their embedding similarity.Particularly, the following classification loss is minimized for a positive pair of q and k:4
L =  log exp(h q , h k ) exp(h q , h k ) + rR exp(h q , h r ) .(6)
In the above equation, h q and h k are the node embeddings;  denotes the computation of inner product; R stands for the negative samples.In our implementation, we leverage "in-batch negative samples" (Karpukhin et al., 2020;Luan et al., 2020) for the reduction of encoding cost: a positive sample in one training instance will be used as a negative sample in the rest of the training instances within the same mini-batch.</p>
<p> Two-stage Training.In GraphFormers, the information from the center node and neighbour nodes are not treated equally, which may undermine the model's training effect.Particularly, the center node's information can be directly utilized, while the neighbourhood information needs to be introduced via three steps: 1) encoded as node-level embeddings, 2) making graph aggregation with the center node, and 3) introduced to center node's graph augmented token-level embeddings.The message passing pathway can shortcut when the center nodes are "sufficiently informative", i.e., two nodes are sufficiently similar with each other in terms of their own textual features, such that their connection can be predicted without considering the neighbours.Given the existence of such cases, GraphFormers may end up with well-trained Transformers but insufficiently trained GNNs.</p>
<p>To alleviate the above problem, we introduce a warm-up training task, where the link prediction is made based on the polluted input nodes.Particularly, for each input node g, a subset of its tokens g m will be randomly masked5 .As a result, the classification loss becomes:
L  =  log exp(h q , h k) exp(h q , h k) + rR exp(h q , h r ) ,(7)
where h q , h k, h r are the embeddings generated from the polluted nodes.The masked tokens reduce the informativeness of each individual node; therefore, the model is forced to leverage the whole input nodes to make the right prediction.</p>
<p>Finally, the model training is organized as a two-stage progressive learning process.In the first stage, the model is trained to minimize L  based on the polluted nodes until its convergence, which reinforce the model's capability of integrating information on graph.In the second stage, the model is continually trained to minimize L based on the original data until the convergence, which makes the model fit into the target distribution.</p>
<p>4 Experimental Studies</p>
<p>Data and Settings</p>
<p>We make use of the following three real-world textual graph datasets for our experimental studies.</p>
<p> DBLP6 , which contains the paper citation graph from DBLP up to 2020-04-09.Two papers are linked if one is cited by the other one.The paper's title is used as the textual feature.</p>
<p> Wikidata5M7 (Wiki) (Wang et al., 2019b), which contains the entity graph from Wikipedia.The first sentence in each entity's introduction is taken as its textual feature. Product Graph (Product), an even larger dataset of online products collected by a world-wide search engine.In this dataset, the users' web browsing behaviors are tracked for the targeted product webpages (e.g., Amazon webpages of Nike shoes).The user's continuously browsed webpages within a short period of time (e.g., 30 minutes) is called a "session".The products within a common session are connected in the graph (which is a common way of graph construction in e-commerce scenarios (Ying et al., 2018;Wang et al., 2018)).Each product has its unique textual description, which specifies information like the product name, brand, and saler, etc.</p>
<p>The textual features of all the datasets are in English.We make use of uncased WordPiece (Wu et al., 2016) to tokenize the input text.In our experiment, each text is associated with 5 uniformly sampled neighbours (without replacement); for texts with neighbourhood smaller than 5, all the neighbours will be utilized.We summarized the specifications of all the datasets with Table 1.The experiment results are evaluated in terms of link prediction accuracy, i.e., to predict whether a query node and key node are connected given the textual features of themselves and their neighbours.In each testing instance, one query is provided with 300 keys: 1 positive plus 299 randomly sampled negative cases.We leverage three common metrics to measure the prediction accuracy: Precision@1, NDCG, and MRR.</p>
<p>Without specifications, we will take the unidirectional-simplified GraphFormers trained with the two-stage progressive learning as our default model.More details about the implementations and the training/testing configurations are summarized in an Appendix file.It is submitted together with our source code within the supplementary materials.</p>
<p>Baselines</p>
<p>We focus on the comparison between GNN-nested Transformers and Cascaded Transformers-GNN.</p>
<p>To make sure the difference between both architectures can be truthfully reflected from the evaluation results, GraphFormers and the Cascaded Transformers-GNN baselines are equipped with text encoders and graph aggregators of the same capacities.Particularly, we use the BERT-like PLM as our text encoder, where UniLM-base8 (Bao et al., 2020) is chosen as the network backbone for all related methods; the final layer's [CLS] token embedding is used for the text embedding.</p>
<p>We enumerate the following representative graph aggregators as used in GAT (Velikovi et al., 2018), GIN (Xu et al., 2018), GraphSage (Hamilton et al., 2017a).The GAT aggregator, where the node embedding is generated as the weighted sum of all the text embeddings.Each text embedding's relative importance is calculated as the attention score with the center node.The Pooling-and-Concat aggregators, where the center node's text embedding is concatenated with the neighbours' pooling result and linearly transformed for the final representation.Depending on the form of pooling function, we have the following options: Max and Mean, where neighbours are aggregated by max-pooling and mean-pooling, respectively; Att, where the neighbours are summed up based on the attention weights with the center node.By comparison, the neighbourhood information may get more emphasized with GAT; while the center node itself tends to be highlighted with Pooling-and-Concat.</p>
<p>We consider two more baselines which make use of simplified text encoders (such as CNN) and network embeddings: TNVE (Wang et al., 2019a) and IFTN (Xu et al., 2019).We also include the PLM only baseline, which merely leverages the textual feature of the center node.</p>
<p>Overall Evaluation</p>
<p>The overall evaluation results are reported in Table 2. It's observed that GraphFormers consistently outperform all the baselines, especially the ones based on the cascaded Transformers-GNN, with notable advantages.Particularly, it achieves 2.9%, 4.8%, 6.5% relative improvements over the most competitive baselines (underlined) on each of the experimental datasets.Such an observation indicates that the relationship between the nodes can be captured more accurately based on the node embeddings generated by GraphFormers, which verifies the effectiveness of our proposed method.</p>
<p>We also observe the following underlying factors that may influence the representation quality.</p>
<p>Firstly, the effective utilization of neighbourhood information is critical.With the joint consideration of the center node and neighbour nodes, the PLM+GNNs methods, including GraphFormers and the cascaded Transformers-GNN baselines, significantly outperform the PLM only baseline in most of the time.We further analyze the impact of neighbourhood size as Table 3, with a fraction of neighbour nodes randomly sampled for each center node (using DBLP for illustration).It can be observed that both GraphFormers and PLM+Max (the most competitive baseline) achieve higher prediction accuracy than the PLM only method (P@1:0.5673,NDCG:0.7484,MRR:0.6777, as reported in Table 2), even with fewer neighbour nodes included.With the increasing number of neighbour nodes, the advantages become gradually enlarged.However, the marginal gain is vanishing, as the relative improvement becomes smaller when more neighbours are included.In all the testing cases, GraphFormers maintain consistent advantages over PLM+Max, which reaffirms the effectiveness of our proposed methods.</p>
<p>Secondly, the capacity of the text encoder is crucial for textual graph representation.All the pretrained language model based methods (GraphFormers, Cascaded Transformers-GNN baselines, PLM-only baseline) significantly outperform the baselines with simplified text encoders (TNVE, IFTN).</p>
<p>Thirdly, the representation quality is also sensitive to the form of graph aggregator.In Product, the cascaded Transformers-GNN baselines' performances are quite close to each other.In DBLP, PLM+(Max, Mean, Att) outperforms PLM+GAT.In Wiki, not only PLM+(Max, Mean, Att) but also PLM-only baseline outperform PLM+GAT.Such phenomenons could be attributed to the type of graph: whether it is homogeneous or heterogeneous.Particularly, both Product and DBLP can be regarded as homogeneous graphs as the nodes are connected based on the same relationships; i.e., co-view relationship in Product, and citation relationship in DBLP.In both homogeneous graphs, the connected nodes may have quite similar semantics (the co-viewed products usually serve similar user intents, and the citation relationships usually indicate similar research topics); thus, the incorporated neighbour nodes will probably provide complementary information for the link prediction between the center nodes.However, Wiki is a heterogeneous graph, where the connections between entities may have highly different semantics.As a result, the incorporation of neighbour nodes may not contribute to the link prediction task, especially when the incorporated neighbours and the prediction target are connected to the center nodes with totally different relationships.Considering that GAT tends to focus more on the neighbourhood, its performance can be vulnerable in such unfavorable situations.These findings suggest that the neighbourhood information should be properly handled in case that the information of the center node is wiped out.</p>
<p>Finally, we may conclude different methods' utility in textual graph representation: simplified text encoders  PLMs  Cascaded Transformers-GNN  GNNs-nested Transformers.Such findings are consistent with our expectation that the precise modeling of individual textual feature and the effective integration of neighbourhood information will jointly contribute to high-quality textual graph representation.GraphFormers enjoy the high expressiveness of PLMs and leverage layerwise nested-GNNs to facilitate graph aggregation, which contributes to both of the above perspectives.</p>
<p>Table 4: Ablation Studies (The top ablated methods are marked in bold; ""/"": the performance is increased/decreased compared with the default setting)."-Progressive": two-stage progressive learning disabled; "-Simplified": unidirectional simplification disabled; "-Shared GNNs": GNNs parameters are not shared across the layers; "-Position": GNNs learnable position bias disabled.</p>
<p>Ablation Studies</p>
<p>The ablation studies (as Table 4) are performed to clarify the following issues: 1) the impact of two-stage progressive learning, and 2) the impact of unidirectional-simplified GraphFormers.</p>
<p>Firstly, the two-stage progressive learning substantially improves GraphFormers' representation quality.Without such a training strategy ("-Progressive": training directly on the original data), the model's performance is decreased by 0.98%, 1.71%, and 1.18% in each of the datasets, respectively.</p>
<p>Secondly, the performances between simplified and non-simplified ("-Simplified") GraphFormers are comparable.In fact, the necessity of graph aggregation is not equivalent for the center node and the neighbour nodes: since the center node is the one for representation, it is much more important to ensure that the center node may extract complementary information from its neighbours.The unidirectional-simplified GraphFormers maintain such a property; thus, there is little impact on the final performances.Such a finding affirms that we may safely leverage the simplified model to save the cost of repetitively encoding the existing neighbours.</p>
<p>We make two additional ablation studies."-Shared GNNs": the GNNs parameters sharing is disabled, where each layer maintains its own graph aggregator (by default, the layerwise GNN components in GraphFormers share the same set of parameters)."-Position": the learnable position bias (b in Eq. 1) is disabled in GNNs.We find that model's performance is little affected from the above changes.</p>
<p>Efficiency Analysis</p>
<p>We compare the time efficiency between GNN-nested Transformers (GraphFormers) and Cascaded Transformers+GNN (using PLM+Max for comparison).The evaluation is made with a Nvidia P100 GPU.Each mini-batch contains 32 encoding instances; each instance contains one center and #N neighbour nodes; the token length of each node is 16.We report the average time and memory (GPU RAM) costs per mini-batch as Table 5.</p>
<p>Firstly, the time and memory costs of both methods grow linearly with the increment of neighbours.(There are overheads of time and memory costs.The time cost overhead may come from CPU processing; while the memory cost overhead is mainly due to the model parameters (Rajbhandari et al., 2020)).We may approximately remove the overheads by deducting the time and memory costs where #N=3).Such a finding is consistent with our theoretical analysis in Section 3.1.</p>
<p>Secondly, the overall time and memory costs of GraphFormers are quite close to PLM+Max.When the number of neighbour nodes is small, the differences between both methods are almost ignorable.The differences become slightly larger when more neighbour nodes are included, because the layerwise graph aggregations in GraphFormers get increasingly time consuming.However, the differences are still relatively small: merely around 3.5% of the overall running costs when #N is increased to 200 ("#N=200" is already more than enough for most of the real world scenarios).</p>
<p>Based on the above observations, we may conclude that GraphFormers are more accurate, meanwhile equally efficient and scalable as the conventional cascaded Transformer+GNNs.</p>
<p>Online A/B Test on Bing Search</p>
<p>GraphFormers has been deployed as one of the major ads retrieval algorithms on Bing Search, and it achieves highly competitive performance against the previous production system (the combination of a wide spectrum of semantic representation algorithms, including large-scale PLMs and cascaded PLMs-GNNs).Particularly, the primary objective of Ads service is to maximize the revenue meanwhile increasing the user clicks.Therefore, the following three metrics are taken as the major performance indicators: RPM 9 (revenue per thousand impressions), CY (click yield), and CPC 10 (cost per click) .</p>
<p>During our large-scale online A/B test, GraphFormers significantly improves the overall RPM, CY, CPC by 1.87%, 0.96% and 0.91%, respectively.A 11-day performance snapshot is demonstrated as Figure 3; it can be observed that in most of the time, all three metrics are significantly improved thanks to the utilization of GraphFormers (the daily performance are measured based on millions of impressions, thus having strong statistic significance).</p>
<p>Conclusion</p>
<p>In this paper, we propose a novel model architecture GraphFormers for textual graph representation.By having GNNs nested alongside each transformer layer of the pretrained language model, the underlying semantic of each textual node can be precisely captured and effectively integrated for high-quality textual graph representation.On top of the fundamental architecture, we introduce the two-stage progressive training strategy to further strengthen GraphFormers' representation quality; we also simplify the model with the unidirectional graph aggregation, which eliminates the unnecessary computation cost.The experimental studies on three large-scale textual graph datasets verify the effectiveness of our proposed methods, where GraphFormers notably outperform the existing cascaded Transformer-GNNs methods with comparable running efficiency and scalability.</p>
<p>Acknowledgement</p>
<p>We are grateful to anonymous reviewers for their constructive comments on this work.The work was supported by grants from the National Natural Science Foundation of China (No. 62022077).</p>
<p>A Implementation Details</p>
<p>A.1 Masking Strategy</p>
<p>We use span masking (Joshi et al., 2020) as our masking strategy.For each iteration, we sample and mask a span of text, until the ratio of masked tokens has reached the threshold.We follow the settings in (Joshi et al., 2020).The span length l is generated from a geometric distribution l  Geo(p), where p is set to 0.2 and l is clipped at l max = 10.As in BERT (Devlin et al., 2018), 15% of the input tokens will be masked: 80% of them are replaced by [MASK], 10% are replaced by random tokens and 10% are kept as the original tokens.</p>
<p>A.2 GraphFormers' Workflow</p>
<p>Algorithm 2 provides the pseudo-code of GraphFormers' workflow.We use original Multi-Head Attention in the first Transformer layer (Transformers[0]), and asymmetric Multi-Head Attention in the rest Transformer layers (Transformers[1..L  1]).In original Multi-Head Attention, Q, K, V are computed as:
Q = H l g W Q j ; K = H l g W K j ; V = H l g W V j .(8)
In asymmetric Multi-Head Attention, Q, K, V are computed as:
Q = H l g W Q j ; K = H l g W K j ; V = H l g W V j .(9)
In the above equations, H l g are token-level embeddings, H l g are graph-augmented token-level embeddings, and W Q j , W K j , and W V j are the projection matrices of Multi-Head Attention, corresponding to the j-th attention head.</p>
<p>In each step, we extract the embeddings of [CLS] tokens as node-level embeddings Z l g .The node-level embeddings Z l g and a learnable bias vector b are processed by the GNN component, which is a Multi-Head Attention layer.The output GNN-processed node-level embeddings l g are concatenated with the original token-level embeddings H l g , which generates the graph-augmented token-level embeddings H l g .Then H l g are processed by the Transformer component using asymmetric Multi-Head Attention.At last, the node-level embedding of the center node h x is returned as the representation of the graph.</p>
<p>B Training Details</p>
<p>As shown in Table 6, we present the hyperparameters used for training GraphFormers.The model is trained for at most 100 epochs on all datasets.For the stability of the training process, we optimally tune the learning rate as 1e5 for Product, 1e6 for DBLP, and 5e6 for Wiki.We use an early stopping strategy on P@1 with a patience of 2 epochs and Adam (Kingma and Ba, 2014) with  1 =0.9,  2 =0.999, =1e-8 for optimization.We pad the sequence length to 32 for Product and DBLP, 64 for Wiki, depending on different text length of each dataset.To make full use of the GPU memory, we set the batch size as 240 for Product and DBLP, 160 for Wiki.Each training sample includes 12 nodes: 1 query with its 5 neighbours, and 1 keyword with its 5 neighbours.The training is on 8 Nvidia V100-16GB GPU clusters.The training of GraphFormers takes 58.8, 117.6, 151.2 hours on average to converge on each of the experimental datasets (Product, DBLP, Wiki).We use Python3.6 and PyTorch 1.6.0 for implementation.The random seeds of PyTorch and NumPy are fixed as 42.For two-stage training, the training processes of the two stages share the same settings as above.</p>
<p>Figure 1 :
1
Figure 1: Model architecture comparison (a center node C is connected with two neighbours N1, N2).(A) Cascaded Transformers-GNN: text embeddings are independently generated by language models and aggregated by rear-mounted GNNs.(B) GNN-nested Transformers: the text encoding and graph aggregation are iteratively performed with the layerwise GNNs and Transformers (TRM).</p>
<p>Figure 2 :
2
Figure 2: GNN-nested Transformers (using the l-th layer for illustration).The graph aggregation is performed in the first place: the node-level embeddings {z l g } G are gathered from all the nodes and processed by the GNN component (the leftmost rectangle).The GNN processed node-level embeddings { l g } G are dispatched to their original nodes, which forms the graph-augmented tokenlevel embeddings.The graph-augmented token-level embeddings are further encoded by Transformer.</p>
<p>Figure 3 :
3
Figure 3: Online A/B Test: the relative improvements of RPM, CY and CPC against the last version of production system in Bing Search (green: positive; blue: negative).In most of the time, all three performance indicators are significantly improved thanks to the utilization of GraphFormers.</p>
<p>Algorithm 2 :
2
GraphFormers' Workflow in PyTorch-Like Style # Input: # Hg[0]: initial token-level embeddings (summation of word embeddings and position embeddings) # Output: # hx: output embeddings # B: batch size # N: number of nodes in the graph (0th node represents the center node) # SL: sequence length # D: hidden dimension # L: number of GNN-nested Transformer layers # b: learnable bias vector for nodes # token-level embeddings: BxNxSLxD Hg[1] = Transformers<a href="Hg[0].view(B* N, SL, D), asymmetric = False">0</a>.view(B,N, SL, D) # "asymmetric = False" means we use original Multi-Head Attention in the Transformer for l in range(1, L): # node-level embeddings: BxNxD Zg[l] = Hg[l][:, :, 0] # GNN-processed node-level embeddings: BxNxD Zg_hat[l] = MultiHeadAttention(Zg[l], b) # graph-augmented token-level embeddings: BxNx(SL+1)xD Hg_hat[l] = Concat([Zg_hat[l][:, :, None, :], Hg[l]], dim = 2) # token-level embeddings: BxNxSLxD Hg[l + 1] = Transformers<a href="Hg_hat[l].view(B * N, SL + 1, D), asymmetric = True">l</a>.view(B, N, SL, D) # "asymmetric = True" means we use asymmetric Multi-Head Attention in the Transformer # graph representations: BxD hx = Hg[L][:, 0, 0, :] return hx</p>
<p>Table 1 :
1
Specifications of the experimental datasets: the number of items, the number of neighbour nodes on average, and the number of training, validation, testing cases.
ProductDBLPWiki#Item5,643,6884,894,0814,818,679#N4.719.318.86#Train22,146,9343,009,5067,145,834#Valid30,00060,00066,167#Test306,742100,000100,000</p>
<p>Table 2 :
2
Overall evaluation (GraphFormers marked in bold, the best baseline underlined).Graph-Formers outperforms all baselines, especially the ones based on cascaded Transformers-GNN.
ProductDBLPWikiMethodsP@1NDCGMRRP@1NDCGMRRP@1NDCGMRRPLM0.65630.79110.73440.56730.74840.67770.34660.57990.4712TNVE0.46180.62040.53640.29780.52950.41630.17860.42740.2933IFTN0.52330.67400.59820.36910.57980.47730.18380.42760.2945PLM+GAT0.75400.86370.82320.66330.82040.76670.30060.54300.4270PLM+Max0.75700.86780.82800.69340.83860.79000.37120.60710.5022PLM+Mean0.75500.86710.82710.68960.83590.78660.36640.60370.4980PLM+Att0.75130.86520.82460.69100.83660.78750.37090.60670.5018GraphFormers0.77860.87930.84300.72670.85650.81330.39520.62300.5220</p>
<p>Table 3 :
3
Impact of neighbour size (#N).
GraphFormersPLM+Max#NP@1NDCGMRRP@1NDCGMRR10.64850.80870.75220.62490.79460.734220.68410.83080.78040.65380.81370.758330.69800.83960.79160.67280.82560.773440.71260.84850.80290.68230.83190.781450.72670.85650.81330.69340.83860.7900</p>
<p>Table 5 :
5
Time and memory costs per mini-batch for PLM+Max and GraphFormers, with neighbour size increased from 3 to 200.GraphFormers achieve similar efficiency and scalability as PLM+Max.
ProductDBLPWikiMethodsP@1NDCGMRRP@1NDCGMRRP@1NDCGMRRGraphFormers0.77860.87930.84300.72670.85650.81330.39520.62300.5220PLM+Max0.75700.86780.82800.69340.83860.79000.37120.60710.5022-Progressive0.76880.87510.83730.70960.84680.80070.38340.61550.5127-Simplified0.7795 0.8798 0.8436 0.72250.85420.81020.39230.62090.5195-Shared GNNs0.77880.87950.84330.72560.85580.81230.3945 0.6221 0.5211 -Position0.77880.87950.84340.7276 0.8570 0.8139 0.39420.62220.5211#N35102050100200Time: PLM+Max60.29 ms93.41 ms161.40 ms295.92 ms684.16 ms1357.93 ms2706.35 msTime: GraphFormers63.95 ms97.19 ms170.16 ms306.12 ms714.32 ms1411.09 ms2801.67 msMem: PLM+Max1.33 GiB1.39 GiB1.55 GiB1.82 GiB2.67 GiB4.09 GiB6.92 GiBMem: GraphFormers1.33 GiB1.39 GiB1.55 GiB1.83 GiB2.70 GiB4.28 GiB7.33 GiB</p>
<p>Table 6 :
6
Hyperparameters for training GraphFormers
OptimizerAdamAdam  10.9Adam  20.999Adam 1e-8PyTorch random seed42NumPy random seed42Product DBLP WikiMax training epochs100100100Learning rate1e-51e-65e-6Sequence length323264Batch size240240160
The encoding results can be kept in low-cost devices, whose storage capacity can be regarded as infinite.
We remove the naive cases where q and k are included by each other's neighbour set, Nq and N k .
We use the common MLM strategy, where 15% of the input tokens are masked: 80% of them are replaced by[MASK], the rest ones are replaced randomly or kept as the original tokens with the same probabilities.
https://originalstatic.aminer.cn/misc/dblp.v12.7z
https://deepgraphlearning.github.io/project/wikidata5m
An enhanced BERT-like PLM showing more competitive performances than peers like RoBERTa, XLNet.</p>
<p>Unilmv2: Pseudo-masked language models. Hangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan Yang, Xiaodong Liu, Yu Wang, Jianfeng Gao, Songhao Piao, Ming Zhou, Proceedings of the 26th annual international conference on machine learning. Yoshua Pmlr, Jrme Bengio, Ronan Louradour, Jason Collobert, Weston, the 26th annual international conference on machine learning2020. 2009International Conference on Machine Learning</p>
<p>Language models are few-shot learners. Benjamin Tom B Brown, Nick Mann, Melanie Ryder, Jared Subbiah, Prafulla Kaplan, Arvind Dhariwal, Pranav Neelakantan, Girish Shyam, Amanda Sastry, Askell, arXiv:2005.141652020arXiv preprint</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.04805Pre-training of deep bidirectional transformers for language understanding. Bert2018arXiv preprint</p>
<p>Tianyu Gao, Xingcheng Yao, Danqi Chen, arXiv:2104.08821Simcse: Simple contrastive learning of sentence embeddings. 2021arXiv preprint</p>
<p>Inductive representation learning on large graphs. Will Hamilton, Zhitao Ying, Jure Leskovec, Advances in neural information processing systems. 2017a</p>
<p>Rex William L Hamilton, Jure Ying, Leskovec, arXiv:1709.05584Representation learning on graphs: Methods and applications. 2017barXiv preprint</p>
<p>Gpt-gnn: Generative pre-training of graph neural networks. Ziniu Hu, Yuxiao Dong, Kuansan Wang, Kai-Wei Chang, Yizhou Sun, Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining2020</p>
<p>Spanbert: Improving pre-training by representing and predicting spans. Mandar Joshi, Danqi Chen, Yinhan Liu, Luke Daniel S Weld, Omer Zettlemoyer, Levy, Transactions of the Association for Computational Linguistics. 82020</p>
<p>Dense passage retrieval for open-domain question answering. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi Chen, Wentau Yih, arXiv:2004.049062020arXiv preprint</p>
<p>P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980Adam: A method for stochastic optimization. 2014arXiv preprint</p>
<p>Semi-supervised classification with graph convolutional networks. N Thomas, Max Kipf, Welling, arXiv:1609.029072016arXiv preprint</p>
<p>Adsgnn: Behavior-graph augmented relevance modeling in sponsored search. Chaozhuo Li, Bochen Pang, Yuming Liu, Hao Sun, Zheng Liu, Xing Xie, Tianqi Yang, Yanling Cui, Liangjie Zhang, Qi Zhang, arXiv:2104.120802021arXiv preprint</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, arXiv:1907.11692Roberta: A robustly optimized bert pretraining approach. 2019aarXiv preprint</p>
<p>Fine-grained fact verification with kernel graph attention network. Zhenghao Liu, Chenyan Xiong, Maosong Sun, Zhiyuan Liu, arXiv:1910.097962019barXiv preprint</p>
<p>Yi Luan, Jacob Eisenstein, Kristina Toutanova, Michael Collins, arXiv:2005.00181Sparse, dense, and attentional representations for text retrieval. 2020arXiv preprint</p>
<p>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean, arXiv:1310.4546Distributed representations of words and phrases and their compositionality. 2013arXiv preprint</p>
<p>Glove: Global vectors for word representation. Jeffrey Pennington, Richard Socher, Christopher D Manning, EMNLP. 2014. 2014</p>
<p>Mark Matthew E Peters, Mohit Neumann, Matt Iyyer, Christopher Gardner, Kenton Clark, Luke Lee, Zettlemoyer, arXiv:1802.05365Deep contextualized word representations. 2018arXiv preprint</p>
<p>Improving language understanding by generative pre-training. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, 2018</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, arXiv:1910.106832019arXiv preprint</p>
<p>Zero: Memory optimizations toward training trillion parameter models. Samyam Rajbhandari, Jeff Rasley, SC20: International Conference for High Performance Computing, Networking, Storage and Analysis. IEEE2020Olatunji Ruwase, and Yuxiong He</p>
<p>Nils Reimers, Iryna Gurevych, arXiv:1908.10084Sentence-bert: Sentence embeddings using siamese bertnetworks. 2019arXiv preprint</p>
<p>Whitening sentence representations for better semantics and faster retrieval. Jianlin Su, Jiarun Cao, Weijie Liu, Yangyiwen Ou, arXiv:2103.153162021arXiv preprint</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, arXiv:1706.037622017arXiv preprint</p>
<p>Petar Velikovi, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, Yoshua Bengio, Graph attention networks. International Conference on Learning Representations (ICLR). 2018</p>
<p>Text classification with heterogeneous information network kernels. Chenguang Wang, Yangqiu Song, Haoran Li, Ming Zhang, Jiawei Han, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2016a30</p>
<p>Billionscale commodity embedding for e-commerce recommendation in alibaba. Jizhe Wang, Pipei Huang, Huan Zhao, Zhibo Zhang, Binqiang Zhao, Dik Lun, Lee , Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining2018</p>
<p>Linked document embedding for classification. Suhang Wang, Jiliang Tang, Charu Aggarwal, Huan Liu, Proceedings of the 25th ACM international on conference on information and knowledge management. the 25th ACM international on conference on information and knowledge management2016b</p>
<p>Wenlin Wang, Chenyang Tao, Zhe Gan, Guoyin Wang, Liqun Chen, Xinyuan Zhang, Ruiyi Zhang, Qian Yang, Ricardo Henao, Lawrence Carin, arXiv:1909.13456Improving textual network learning with variational homophilic embeddings. 2019aarXiv preprint</p>
<p>Kepler: A unified model for knowledge embedding and pre-trained language representation. Xiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhiyuan Liu, Juanzi Li, Jian Tang, arXiv:1911.061362019barXiv preprint</p>
<p>Yonghui Wu, Mike Schuster, Zhifeng Chen, Mohammad Quoc V Le, Wolfgang Norouzi, Maxim Macherey, Yuan Krikun, Qin Cao, Klaus Gao, Macherey, arXiv:1609.08144Google's neural machine translation system: Bridging the gap between human and machine translation. 2016arXiv preprint</p>
<p>Keyulu Xu, Weihua Hu, Jure Leskovec, Stefanie Jegelka, arXiv:1810.00826How powerful are graph neural networks?. 2018arXiv preprint</p>
<p>Zenan Xu, Qinliang Su, Xiaojun Quan, Weijia Zhang, arXiv:1908.11057A deep neural information fusion architecture for textual network embeddings. 2019arXiv preprint</p>
<p>Network representation learning with rich text information. Cheng Yang, Zhiyuan Liu, Deli Zhao, Maosong Sun, Edward Y Chang, IJCAI. 20152015</p>
<p>Xlnet: Generalized autoregressive pretraining for language understanding. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V Le, arXiv:1906.082372019arXiv preprint</p>
<p>Graph-based neural multi-document summarization. Michihiro Yasunaga, Rui Zhang, Kshitijh Meelu, Ayush Pareek, Krishnan Srinivasan, Dragomir Radev, arXiv:1706.066812017arXiv preprint</p>
<p>Graph convolutional neural networks for web-scale recommender systems. Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, Jure Leskovec, Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining2018</p>
<p>Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, Maosong Sun, Graph neural networks: A review of methods and applications. AI Open. 20201</p>
<p>Gear: Graph-based evidence aggregating and reasoning for fact verification. Jie Zhou, Xu Han, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, Maosong Sun, arXiv:1908.018432019arXiv preprint</p>
<p>Jason Zhu, Yanling Cui, Yuming Liu, Hao Sun, Xue Li, Markus Pelger, Liangjie Zhang, Tianqi Yan, Ruofei Zhang, Huasha Zhao, arXiv:2101.06323Textgnn: Improving text encoder via graph neural network in sponsored search. 2021arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
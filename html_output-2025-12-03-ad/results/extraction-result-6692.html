<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6692 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6692</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6692</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-126.html">extraction-schema-126</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <p><strong>Paper ID:</strong> paper-ef8a8bd193b1a0a5e2c834a7a28869a2ec85bab7</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/ef8a8bd193b1a0a5e2c834a7a28869a2ec85bab7" target="_blank">s1: Simple test-time scaling</a></p>
                <p><strong>Paper Venue:</strong> Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> After supervised finetuning the Qwen2.5-32B-Instruct language model on s1K and equipping it with budget forcing, the model s1-32B exceeds o1-preview on competition math questions by up to 27% (MATH and AIME24).</p>
                <p><strong>Paper Abstract:</strong> Test-time scaling is a promising new approach to language modeling that uses extra test-time compute to improve performance. Recently, OpenAI's o1 model showed this capability but did not publicly share its methodology, leading to many replication efforts. We seek the simplest approach to achieve test-time scaling and strong reasoning performance. First, we curate a small dataset s1K of 1,000 questions paired with reasoning traces relying on three criteria we validate through ablations: difficulty, diversity, and quality. Second, we develop budget forcing to control test-time compute by forcefully terminating the model's thinking process or lengthening it by appending"Wait"multiple times to the model's generation when it tries to end. This can lead the model to double-check its answer, often fixing incorrect reasoning steps. After supervised finetuning the Qwen2.5-32B-Instruct language model on s1K and equipping it with budget forcing, our model s1-32B exceeds o1-preview on competition math questions by up to 27% (MATH and AIME24). Further, scaling s1-32B with budget forcing allows extrapolating beyond its performance without test-time intervention: from 50% to 57% on AIME24. Our model, data, and code are open-source at https://github.com/simplescaling/s1</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6692.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6692.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>s1-32B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>s1-32B (Qwen2.5-32B-Instruct finetuned on s1K)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 32B-parameter decoder-only transformer obtained by supervised finetuning Qwen2.5-32B-Instruct on a curated set of 1,000 reasoning examples (s1K), designed to produce long chain-of-thought traces and used with test-time 'budget forcing' to scale computation and improve arithmetic/competition-math performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>s1-32B</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>32B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pretrained Qwen2.5 checkpoints; supervised finetuning on s1K (1,000 examples) consisting of questions + distilled reasoning traces (initially from Gemini 2.0 Flash Thinking; later s1.1 distilled from DeepSeek r1). No large RL stage; training aimed to produce the 'think' traces.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>AIME24; MATH500; GPQA Diamond</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>competition mathematics and multi-step arithmetic/word problems (multi-step reasoning, algebra, counting, number theory, geometry, probability)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language questions presented with explicit chain-of-thought / reasoning traces during training; inference uses delimiters (<|im_start|>think ... <|im_start|>answer) and can include appended strings like 'Wait' to extend thinking.</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>High (competition/Olympiad level; AIME: high-school contest; MATH500: competition math; GPQA Diamond: PhD-level science questions)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Supervised finetuning to generate chain-of-thought; at test time uses budget forcing (decoding-time intervention: force early end-of-thinking delimiter to limit thinking or suppress delimiter and append 'Wait' one or more times to lengthen thinking). Greedy decoding (temperature 0) for reported base evaluations; some ablations at temperature 1.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (exact match / pass@1)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>AIME24: 50.0% without budget forcing; 56.7% with budget forcing (reported best in Table 1/3). MATH500: ≈93.0% (with BF). GPQA Diamond: ≈59.6% (with BF). Extrapolated AIME performance reported up to ~57% when scaling test-time compute with 'Wait' appends.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>No fine-grained mechanistic probing reported (no activation / attention probes). Authors hypothesize SFT 'activates' latent reasoning capabilities acquired during pretraining. Observations: longer training sequence length reduces test-time chain length and improves answer-section generation (sequence-length during training affects reasoning-token behavior); evaluation nondeterminism (vLLM) and data-source 'recitation' artifacts (Gemini) complicate measurement.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>If end-of-thinking delimiter is suppressed too often, the model can enter repetitive looping instead of productive reasoning; budget forcing eventually flattens out (diminishing returns) and is limited by context window; the model cannot reliably count tokens (token-conditional control fails); long generated traces sometimes backtrack and are more likely to be wrong; evaluations can vary due to inference nondeterminism and 'recitation error' in some APIs.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Sequential test-time scaling via budget forcing shows a clear positive slope (Table 3 reports 'Scaling' = 15 for BF and Control = 100%), enabling extrapolation in AIME24 from ~50% to ~57% with extra 'Wait' appends; majority-voting (parallel) on the base model did not catch up with s1-32B's sequential scaling, while combining sequential training with parallel methods (e.g., REBASE + majority) can further improve scaling at extra compute cost.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 's1: Simple test-time scaling', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6692.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6692.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen2.5-32B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen2.5-32B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The pretrained and instruction-tuned base model used for finetuning to create s1-32B; serves as the baseline open-weight 32B model in evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5-32B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>32B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pretrained Qwen2.5 family corpora and instruction-tuned dataset (details in Qwen technical report).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>AIME24; MATH500; GPQA Diamond</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>competition math / multi-step reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language question prompts (instruction-tuned responses); no specialized budget forcing used in base evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>High (competition-level math problems)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Instruction-tuned prompting (greedy/temperature = 0 for many evaluations); chain-of-thought style traces were not produced reliably before SFT on s1K.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (exact match/pass@1)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>AIME24: 26.7%; MATH500: 84.0%; GPQA Diamond: 49.0% (reported in Table 1 as baseline scores).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Presented as a baseline; authors show large relative improvement after supervised finetuning on s1K, indicating base model contains latent reasoning ability but needs activation.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Low baseline capability on challenging arithmetic/competition math tasks; benefits strongly from SFT and test-time budget forcing.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Improves substantially with small targeted SFT (1K examples) and further with budget forcing; the paper emphasizes sample-efficient finetuning yields large gains compared to the base.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 's1: Simple test-time scaling', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6692.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6692.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini-Flash (Gemini 2.0 Flash Thinking)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Google Gemini 2.0 Flash Thinking (Experimental API)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source 'thinking' API from which the authors distilled reasoning traces (answers + chains) used to build the s1K dataset; used both as a data source and as a benchmarked API (with evaluation caveats).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini 2.0 Flash Thinking</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Proprietary Google model (details not provided); used as an oracle to generate chain-of-thought traces for dataset distillation.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>AIME24 (authors also ran Gemini on AIME24 themselves)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step reasoning / competition math</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language prompts; API returns reasoning traces ('thinking') and final answers</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Hard (competition problems)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Flash Thinking API to generate long reasoning traces which were distilled as training data</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>AIME24: 60.0% as measured by the authors (they note evaluation complications due to 'recitation error' in certain Gemini interfaces).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Authors report a 'recitation error' in the Gemini API that complicates automated evaluation; no mechanism-level analysis provided.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Recitation error can corrupt or change outputs when evaluated through some interfaces; the API's outputs were nevertheless useful to distill reasoning traces.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 's1: Simple test-time scaling', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6692.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6692.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Budget forcing</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Budget forcing (test-time scaling decoding intervention)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple, decoding-time method to control test-time compute: enforce a max by appending an end-of-thinking delimiter to force answering early, or enforce a min by suppressing the end-of-thinking delimiter and appending strings like 'Wait' to encourage more thinking; used to obtain sequential test-time scaling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Budget forcing (method)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>AIME24; MATH500; GPQA Diamond</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multi-step reasoning / arithmetic and competition math</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Applied at decoding time to chain-of-thought style generations (manipulates end-of-thinking delimiter and appends strings like 'Wait')</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Hard</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Decoding-time interventions: (1) append end-of-thinking token and optionally 'Final Answer:' to terminate thinking early; (2) suppress end-of-thinking token and append 'Wait' repeatedly to lengthen thinking; can control number of 'waits' to scale compute</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy; additionally Control (how well method enforces thinking-token budget) and Scaling (average slope of accuracy vs thinking tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Control = 100% (perfect controllability in Table 3); Scaling = 15 (Table 3); Performance on AIME24 up to 56.7% (best reported); extrapolation reported improving AIME24 from 50% to 57% when scaling thinking tokens via repeated 'Wait' appends in other experiments/figures.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Empirically produces monotonic gains up to a point and gives perfect controllability over thinking-token budgets; training sequence length influences how readily model transitions to an answer and how long reasoning traces are; no microscopic mechanistic probes were performed.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Over-suppressing the end-of-thinking delimiter can induce repetitive loops instead of constructive reasoning; returns eventually flatten (observed flattening around 6 'Wait' suppressions in experiments); limited by model context window and can cause context overflow.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Positive (monotonic increase in accuracy with more thinking tokens) up to saturation; best performing sequential scaling method in this paper compared to conditional control baselines and rejection sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 's1: Simple test-time scaling', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6692.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6692.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Rejection sampling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rejection sampling (length-constrained sampling)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Parallel/conditional method where generations are sampled until a generation meets a predetermined thinking-token length constraint; used as a baseline for test-time scaling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Rejection sampling (method)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>AIME24</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multi-step reasoning / competition math</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language; filter sampled generations according to length budget</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Hard</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Sampling at nonzero temperature and rejecting draws until the generation length is within a budget (oracle-like posterior over responses conditioned on length)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy; Scaling</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Performance on AIME24: 40.0% (Table 3). Scaling metric negative: -35 (inverse scaling), despite Control = 100%.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Authors found an inverse scaling trend: shorter accepted generations are more often correct (jump to correct approach), while longer ones often correspond to backtracking and corrections and are more likely to be wrong; hypothesized correlation between generation length and whether the model initially was on the right track.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Inverse scaling (more allowed tokens led to worse accuracy); longer samples frequently wrong due to backtracking/self-correction sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Negative (inverse) in their experiments on AIME24.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 's1: Simple test-time scaling', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6692.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6692.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Token-conditional control (TCC)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Token-conditional control (explicit token-budget prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting baseline where the prompt specifies an explicit upper bound on number of thinking tokens to generate.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Token-conditional control</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>AIME24</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multi-step reasoning / arithmetic</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language prompt that requests a specific token upper bound for the thinking stage</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Hard</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>In-prompt explicit token upper bound (prompt-level conditional control)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Control; Scaling; Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Control = 40%; Scaling = -24; Performance = 40.0% on AIME24 (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Token-conditional control fails largely because the model cannot reliably count tokens even when trained to do so; counting errors lead to poor controllability and negative/unstable scaling.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Counting failures (model cannot enforce token budgets), poor controllability and negative scaling in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Negative / unreliable in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 's1: Simple test-time scaling', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6692.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6692.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Step-conditional control (SCC)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Step-conditional control (prompted step budget)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting baseline that specifies an upper bound on the number of thinking 'steps' (coarse-grained units ~100 tokens each) rather than tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Step-conditional control</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>AIME24</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multi-step reasoning / arithmetic</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Prompt telling the model to generate up to N 'steps' of thinking</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Hard</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>In-prompt step budget (coarse control)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Control; Scaling; Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Control = 60%; Scaling = 3; Performance = 36.7% on AIME24 (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Model tends to hack the constraint: few steps with many tokens per step or many steps with few tokens per step, leading to similar overall token consumption and mediocre controllability.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Mediocre controllability because model can alter tokens-per-step; smaller gains compared to budget forcing.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Weak / small positive slope in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 's1: Simple test-time scaling', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6692.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6692.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Class-conditional control (CCC)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Class-conditional control (short vs long prompt classes)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting baseline using a categorical instruction (e.g., 'think short' vs 'think long') rather than numeric budgets to control thinking duration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Class-conditional control</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>AIME24</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multi-step reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Prompt-level class labels (short/long) controlling amount of thinking</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Hard</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Class-based prompting (two generic prompts indicating 'short' or 'long' thinking)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Control; Scaling; Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Control = 50%; Scaling = 25 (best scaling among the conditional-control baselines in Table 3); Performance = 36.7% on AIME24 (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Telling a model to simply think longer can increase test-time compute and improve performance in some cases; works better than token-conditional control in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Only moderate controllability; coarse-grained and less predictable than budget forcing.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Positive (class-conditional control showed strong scaling metric relative to other conditional controls in Table 3) but lower performance overall than budget forcing.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 's1: Simple test-time scaling', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6692.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6692.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Majority voting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Majority voting (parallel repeated-sampling aggregation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Parallel test-time scaling method that aggregates multiple sampled solution trajectories by selecting the most frequent final answer (majority vote).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Majority voting</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>AIME24 (and other reasoning benchmarks in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multi-step reasoning / arithmetic</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Parallel independent sample generations aggregated by majority frequency</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Hard</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Repeated sampling at temperature > 0 and majority aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy under varying numbers of samples (2,4,8,16,32,64 in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Authors report that majority voting on the base model (Qwen2.5-32B-Instruct) cannot catch up to s1-32B with sequential budget forcing; exact numeric baseline depends on number of samples and temperature (figure shows curves but exact percentages for every k not tabled).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Parallel repeated sampling (majority) is less effective than sequential scaling with a reasoning-trained model (s1-32B) in this setup; performance improves with more samples but does not match the gains from sequential budget forcing on the finetuned model.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Compute-inefficient relative to sequential scaling on the finetuned model; still limited by quality of individual trajectories and can require many samples to improve.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Positive with more parallel samples, but did not match sequential scaling of s1-32B in the authors' comparisons; can be improved when combined with stronger process-reward-guided search (e.g., REBASE).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 's1: Simple test-time scaling', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6692.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e6692.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>REBASE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>REBASE (process-reward tree search / guided tree search)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A tree-search-style parallel/sequential hybrid that uses a process reward model to guide generation and prune/expand trajectories; authors use REBASE as a parallel scaling augmentation and find it scales better than majority voting and can outperform sequential scaling for additional compute.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>REBASE (process reward guided tree search)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>tree-search / hybrid sequential-parallel</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Process reward model initialized from LLaMA-34B and finetuned on synthetic process-reward dataset (per cited REBASE setup); used as an auxiliary reward model to guide search.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>AIME24</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multi-step reasoning / contest math</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Tree-search over thought trajectories, aggregating via majority vote; REBASE uses a process reward to guide trajectories</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Hard</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Tree search / guided beam search / process-reward scoring combined with aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy vs compute; thinking-token-equivalent scaling curves</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Figure 7 shows REBASE + majority voting scales better than plain majority voting and in some regimes outperforms sequential scaling; exact numeric table entries not tabulated but authors state REBASE gives stronger scaling than majority voting and can exceed sequential scaling when combined.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>REBASE requires an extra forward pass for the reward model per step, adding overhead; empirically effective at aggregating higher-quality trajectories when compute permits.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Extra compute overhead for reward model not accounted in thinking-token metric; complexity and costlier inference.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Strong positive scaling with compute in experiments; complements sequential budget forcing and can push beyond context-window limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 's1: Simple test-time scaling', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning to reason with llms <em>(Rating: 2)</em></li>
                <li>Scaling llm testtime compute optimally can be more effective than scaling model parameters <em>(Rating: 2)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 2)</em></li>
                <li>Inference scaling laws: An empirical analysis of compute-optimal inference for problem-solving with language models <em>(Rating: 2)</em></li>
                <li>Let's verify step by step <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6692",
    "paper_id": "paper-ef8a8bd193b1a0a5e2c834a7a28869a2ec85bab7",
    "extraction_schema_id": "extraction-schema-126",
    "extracted_data": [
        {
            "name_short": "s1-32B",
            "name_full": "s1-32B (Qwen2.5-32B-Instruct finetuned on s1K)",
            "brief_description": "A 32B-parameter decoder-only transformer obtained by supervised finetuning Qwen2.5-32B-Instruct on a curated set of 1,000 reasoning examples (s1K), designed to produce long chain-of-thought traces and used with test-time 'budget forcing' to scale computation and improve arithmetic/competition-math performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "s1-32B",
            "model_family": "decoder-only transformer",
            "model_size": "32B",
            "training_data_description": "Pretrained Qwen2.5 checkpoints; supervised finetuning on s1K (1,000 examples) consisting of questions + distilled reasoning traces (initially from Gemini 2.0 Flash Thinking; later s1.1 distilled from DeepSeek r1). No large RL stage; training aimed to produce the 'think' traces.",
            "benchmark_name": "AIME24; MATH500; GPQA Diamond",
            "task_type": "competition mathematics and multi-step arithmetic/word problems (multi-step reasoning, algebra, counting, number theory, geometry, probability)",
            "problem_format": "Natural-language questions presented with explicit chain-of-thought / reasoning traces during training; inference uses delimiters (&lt;|im_start|&gt;think ... &lt;|im_start|&gt;answer) and can include appended strings like 'Wait' to extend thinking.",
            "difficulty_level": "High (competition/Olympiad level; AIME: high-school contest; MATH500: competition math; GPQA Diamond: PhD-level science questions)",
            "prompting_method": "Supervised finetuning to generate chain-of-thought; at test time uses budget forcing (decoding-time intervention: force early end-of-thinking delimiter to limit thinking or suppress delimiter and append 'Wait' one or more times to lengthen thinking). Greedy decoding (temperature 0) for reported base evaluations; some ablations at temperature 1.",
            "performance_metric": "Accuracy (exact match / pass@1)",
            "performance_value": "AIME24: 50.0% without budget forcing; 56.7% with budget forcing (reported best in Table 1/3). MATH500: ≈93.0% (with BF). GPQA Diamond: ≈59.6% (with BF). Extrapolated AIME performance reported up to ~57% when scaling test-time compute with 'Wait' appends.",
            "internal_analysis": "No fine-grained mechanistic probing reported (no activation / attention probes). Authors hypothesize SFT 'activates' latent reasoning capabilities acquired during pretraining. Observations: longer training sequence length reduces test-time chain length and improves answer-section generation (sequence-length during training affects reasoning-token behavior); evaluation nondeterminism (vLLM) and data-source 'recitation' artifacts (Gemini) complicate measurement.",
            "failure_modes": "If end-of-thinking delimiter is suppressed too often, the model can enter repetitive looping instead of productive reasoning; budget forcing eventually flattens out (diminishing returns) and is limited by context window; the model cannot reliably count tokens (token-conditional control fails); long generated traces sometimes backtrack and are more likely to be wrong; evaluations can vary due to inference nondeterminism and 'recitation error' in some APIs.",
            "scaling_trend": "Sequential test-time scaling via budget forcing shows a clear positive slope (Table 3 reports 'Scaling' = 15 for BF and Control = 100%), enabling extrapolation in AIME24 from ~50% to ~57% with extra 'Wait' appends; majority-voting (parallel) on the base model did not catch up with s1-32B's sequential scaling, while combining sequential training with parallel methods (e.g., REBASE + majority) can further improve scaling at extra compute cost.",
            "uuid": "e6692.0",
            "source_info": {
                "paper_title": "s1: Simple test-time scaling",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Qwen2.5-32B-Instruct",
            "name_full": "Qwen2.5-32B-Instruct",
            "brief_description": "The pretrained and instruction-tuned base model used for finetuning to create s1-32B; serves as the baseline open-weight 32B model in evaluations.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Qwen2.5-32B-Instruct",
            "model_family": "decoder-only transformer",
            "model_size": "32B",
            "training_data_description": "Pretrained Qwen2.5 family corpora and instruction-tuned dataset (details in Qwen technical report).",
            "benchmark_name": "AIME24; MATH500; GPQA Diamond",
            "task_type": "competition math / multi-step reasoning",
            "problem_format": "Natural-language question prompts (instruction-tuned responses); no specialized budget forcing used in base evaluations.",
            "difficulty_level": "High (competition-level math problems)",
            "prompting_method": "Instruction-tuned prompting (greedy/temperature = 0 for many evaluations); chain-of-thought style traces were not produced reliably before SFT on s1K.",
            "performance_metric": "Accuracy (exact match/pass@1)",
            "performance_value": "AIME24: 26.7%; MATH500: 84.0%; GPQA Diamond: 49.0% (reported in Table 1 as baseline scores).",
            "internal_analysis": "Presented as a baseline; authors show large relative improvement after supervised finetuning on s1K, indicating base model contains latent reasoning ability but needs activation.",
            "failure_modes": "Low baseline capability on challenging arithmetic/competition math tasks; benefits strongly from SFT and test-time budget forcing.",
            "scaling_trend": "Improves substantially with small targeted SFT (1K examples) and further with budget forcing; the paper emphasizes sample-efficient finetuning yields large gains compared to the base.",
            "uuid": "e6692.1",
            "source_info": {
                "paper_title": "s1: Simple test-time scaling",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Gemini-Flash (Gemini 2.0 Flash Thinking)",
            "name_full": "Google Gemini 2.0 Flash Thinking (Experimental API)",
            "brief_description": "A closed-source 'thinking' API from which the authors distilled reasoning traces (answers + chains) used to build the s1K dataset; used both as a data source and as a benchmarked API (with evaluation caveats).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Gemini 2.0 Flash Thinking",
            "model_family": null,
            "model_size": null,
            "training_data_description": "Proprietary Google model (details not provided); used as an oracle to generate chain-of-thought traces for dataset distillation.",
            "benchmark_name": "AIME24 (authors also ran Gemini on AIME24 themselves)",
            "task_type": "multi-step reasoning / competition math",
            "problem_format": "Natural-language prompts; API returns reasoning traces ('thinking') and final answers",
            "difficulty_level": "Hard (competition problems)",
            "prompting_method": "Flash Thinking API to generate long reasoning traces which were distilled as training data",
            "performance_metric": "Accuracy",
            "performance_value": "AIME24: 60.0% as measured by the authors (they note evaluation complications due to 'recitation error' in certain Gemini interfaces).",
            "internal_analysis": "Authors report a 'recitation error' in the Gemini API that complicates automated evaluation; no mechanism-level analysis provided.",
            "failure_modes": "Recitation error can corrupt or change outputs when evaluated through some interfaces; the API's outputs were nevertheless useful to distill reasoning traces.",
            "scaling_trend": null,
            "uuid": "e6692.2",
            "source_info": {
                "paper_title": "s1: Simple test-time scaling",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Budget forcing",
            "name_full": "Budget forcing (test-time scaling decoding intervention)",
            "brief_description": "A simple, decoding-time method to control test-time compute: enforce a max by appending an end-of-thinking delimiter to force answering early, or enforce a min by suppressing the end-of-thinking delimiter and appending strings like 'Wait' to encourage more thinking; used to obtain sequential test-time scaling.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Budget forcing (method)",
            "model_family": null,
            "model_size": null,
            "training_data_description": null,
            "benchmark_name": "AIME24; MATH500; GPQA Diamond",
            "task_type": "Multi-step reasoning / arithmetic and competition math",
            "problem_format": "Applied at decoding time to chain-of-thought style generations (manipulates end-of-thinking delimiter and appends strings like 'Wait')",
            "difficulty_level": "Hard",
            "prompting_method": "Decoding-time interventions: (1) append end-of-thinking token and optionally 'Final Answer:' to terminate thinking early; (2) suppress end-of-thinking token and append 'Wait' repeatedly to lengthen thinking; can control number of 'waits' to scale compute",
            "performance_metric": "Accuracy; additionally Control (how well method enforces thinking-token budget) and Scaling (average slope of accuracy vs thinking tokens)",
            "performance_value": "Control = 100% (perfect controllability in Table 3); Scaling = 15 (Table 3); Performance on AIME24 up to 56.7% (best reported); extrapolation reported improving AIME24 from 50% to 57% when scaling thinking tokens via repeated 'Wait' appends in other experiments/figures.",
            "internal_analysis": "Empirically produces monotonic gains up to a point and gives perfect controllability over thinking-token budgets; training sequence length influences how readily model transitions to an answer and how long reasoning traces are; no microscopic mechanistic probes were performed.",
            "failure_modes": "Over-suppressing the end-of-thinking delimiter can induce repetitive loops instead of constructive reasoning; returns eventually flatten (observed flattening around 6 'Wait' suppressions in experiments); limited by model context window and can cause context overflow.",
            "scaling_trend": "Positive (monotonic increase in accuracy with more thinking tokens) up to saturation; best performing sequential scaling method in this paper compared to conditional control baselines and rejection sampling.",
            "uuid": "e6692.3",
            "source_info": {
                "paper_title": "s1: Simple test-time scaling",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Rejection sampling",
            "name_full": "Rejection sampling (length-constrained sampling)",
            "brief_description": "Parallel/conditional method where generations are sampled until a generation meets a predetermined thinking-token length constraint; used as a baseline for test-time scaling.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Rejection sampling (method)",
            "model_family": null,
            "model_size": null,
            "training_data_description": null,
            "benchmark_name": "AIME24",
            "task_type": "Multi-step reasoning / competition math",
            "problem_format": "Natural-language; filter sampled generations according to length budget",
            "difficulty_level": "Hard",
            "prompting_method": "Sampling at nonzero temperature and rejecting draws until the generation length is within a budget (oracle-like posterior over responses conditioned on length)",
            "performance_metric": "Accuracy; Scaling",
            "performance_value": "Performance on AIME24: 40.0% (Table 3). Scaling metric negative: -35 (inverse scaling), despite Control = 100%.",
            "internal_analysis": "Authors found an inverse scaling trend: shorter accepted generations are more often correct (jump to correct approach), while longer ones often correspond to backtracking and corrections and are more likely to be wrong; hypothesized correlation between generation length and whether the model initially was on the right track.",
            "failure_modes": "Inverse scaling (more allowed tokens led to worse accuracy); longer samples frequently wrong due to backtracking/self-correction sequences.",
            "scaling_trend": "Negative (inverse) in their experiments on AIME24.",
            "uuid": "e6692.4",
            "source_info": {
                "paper_title": "s1: Simple test-time scaling",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Token-conditional control (TCC)",
            "name_full": "Token-conditional control (explicit token-budget prompting)",
            "brief_description": "A prompting baseline where the prompt specifies an explicit upper bound on number of thinking tokens to generate.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Token-conditional control",
            "model_family": null,
            "model_size": null,
            "training_data_description": null,
            "benchmark_name": "AIME24",
            "task_type": "Multi-step reasoning / arithmetic",
            "problem_format": "Natural-language prompt that requests a specific token upper bound for the thinking stage",
            "difficulty_level": "Hard",
            "prompting_method": "In-prompt explicit token upper bound (prompt-level conditional control)",
            "performance_metric": "Control; Scaling; Accuracy",
            "performance_value": "Control = 40%; Scaling = -24; Performance = 40.0% on AIME24 (Table 3).",
            "internal_analysis": "Token-conditional control fails largely because the model cannot reliably count tokens even when trained to do so; counting errors lead to poor controllability and negative/unstable scaling.",
            "failure_modes": "Counting failures (model cannot enforce token budgets), poor controllability and negative scaling in experiments.",
            "scaling_trend": "Negative / unreliable in these experiments.",
            "uuid": "e6692.5",
            "source_info": {
                "paper_title": "s1: Simple test-time scaling",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Step-conditional control (SCC)",
            "name_full": "Step-conditional control (prompted step budget)",
            "brief_description": "A prompting baseline that specifies an upper bound on the number of thinking 'steps' (coarse-grained units ~100 tokens each) rather than tokens.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Step-conditional control",
            "model_family": null,
            "model_size": null,
            "training_data_description": null,
            "benchmark_name": "AIME24",
            "task_type": "Multi-step reasoning / arithmetic",
            "problem_format": "Prompt telling the model to generate up to N 'steps' of thinking",
            "difficulty_level": "Hard",
            "prompting_method": "In-prompt step budget (coarse control)",
            "performance_metric": "Control; Scaling; Accuracy",
            "performance_value": "Control = 60%; Scaling = 3; Performance = 36.7% on AIME24 (Table 3).",
            "internal_analysis": "Model tends to hack the constraint: few steps with many tokens per step or many steps with few tokens per step, leading to similar overall token consumption and mediocre controllability.",
            "failure_modes": "Mediocre controllability because model can alter tokens-per-step; smaller gains compared to budget forcing.",
            "scaling_trend": "Weak / small positive slope in experiments.",
            "uuid": "e6692.6",
            "source_info": {
                "paper_title": "s1: Simple test-time scaling",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Class-conditional control (CCC)",
            "name_full": "Class-conditional control (short vs long prompt classes)",
            "brief_description": "A prompting baseline using a categorical instruction (e.g., 'think short' vs 'think long') rather than numeric budgets to control thinking duration.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Class-conditional control",
            "model_family": null,
            "model_size": null,
            "training_data_description": null,
            "benchmark_name": "AIME24",
            "task_type": "Multi-step reasoning",
            "problem_format": "Prompt-level class labels (short/long) controlling amount of thinking",
            "difficulty_level": "Hard",
            "prompting_method": "Class-based prompting (two generic prompts indicating 'short' or 'long' thinking)",
            "performance_metric": "Control; Scaling; Accuracy",
            "performance_value": "Control = 50%; Scaling = 25 (best scaling among the conditional-control baselines in Table 3); Performance = 36.7% on AIME24 (Table 3).",
            "internal_analysis": "Telling a model to simply think longer can increase test-time compute and improve performance in some cases; works better than token-conditional control in these experiments.",
            "failure_modes": "Only moderate controllability; coarse-grained and less predictable than budget forcing.",
            "scaling_trend": "Positive (class-conditional control showed strong scaling metric relative to other conditional controls in Table 3) but lower performance overall than budget forcing.",
            "uuid": "e6692.7",
            "source_info": {
                "paper_title": "s1: Simple test-time scaling",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Majority voting",
            "name_full": "Majority voting (parallel repeated-sampling aggregation)",
            "brief_description": "Parallel test-time scaling method that aggregates multiple sampled solution trajectories by selecting the most frequent final answer (majority vote).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Majority voting",
            "model_family": null,
            "model_size": null,
            "training_data_description": null,
            "benchmark_name": "AIME24 (and other reasoning benchmarks in experiments)",
            "task_type": "Multi-step reasoning / arithmetic",
            "problem_format": "Parallel independent sample generations aggregated by majority frequency",
            "difficulty_level": "Hard",
            "prompting_method": "Repeated sampling at temperature &gt; 0 and majority aggregation",
            "performance_metric": "Accuracy under varying numbers of samples (2,4,8,16,32,64 in experiments)",
            "performance_value": "Authors report that majority voting on the base model (Qwen2.5-32B-Instruct) cannot catch up to s1-32B with sequential budget forcing; exact numeric baseline depends on number of samples and temperature (figure shows curves but exact percentages for every k not tabled).",
            "internal_analysis": "Parallel repeated sampling (majority) is less effective than sequential scaling with a reasoning-trained model (s1-32B) in this setup; performance improves with more samples but does not match the gains from sequential budget forcing on the finetuned model.",
            "failure_modes": "Compute-inefficient relative to sequential scaling on the finetuned model; still limited by quality of individual trajectories and can require many samples to improve.",
            "scaling_trend": "Positive with more parallel samples, but did not match sequential scaling of s1-32B in the authors' comparisons; can be improved when combined with stronger process-reward-guided search (e.g., REBASE).",
            "uuid": "e6692.8",
            "source_info": {
                "paper_title": "s1: Simple test-time scaling",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "REBASE",
            "name_full": "REBASE (process-reward tree search / guided tree search)",
            "brief_description": "A tree-search-style parallel/sequential hybrid that uses a process reward model to guide generation and prune/expand trajectories; authors use REBASE as a parallel scaling augmentation and find it scales better than majority voting and can outperform sequential scaling for additional compute.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "REBASE (process reward guided tree search)",
            "model_family": "tree-search / hybrid sequential-parallel",
            "model_size": null,
            "training_data_description": "Process reward model initialized from LLaMA-34B and finetuned on synthetic process-reward dataset (per cited REBASE setup); used as an auxiliary reward model to guide search.",
            "benchmark_name": "AIME24",
            "task_type": "Multi-step reasoning / contest math",
            "problem_format": "Tree-search over thought trajectories, aggregating via majority vote; REBASE uses a process reward to guide trajectories",
            "difficulty_level": "Hard",
            "prompting_method": "Tree search / guided beam search / process-reward scoring combined with aggregation",
            "performance_metric": "Accuracy vs compute; thinking-token-equivalent scaling curves",
            "performance_value": "Figure 7 shows REBASE + majority voting scales better than plain majority voting and in some regimes outperforms sequential scaling; exact numeric table entries not tabulated but authors state REBASE gives stronger scaling than majority voting and can exceed sequential scaling when combined.",
            "internal_analysis": "REBASE requires an extra forward pass for the reward model per step, adding overhead; empirically effective at aggregating higher-quality trajectories when compute permits.",
            "failure_modes": "Extra compute overhead for reward model not accounted in thinking-token metric; complexity and costlier inference.",
            "scaling_trend": "Strong positive scaling with compute in experiments; complements sequential budget forcing and can push beyond context-window limitations.",
            "uuid": "e6692.9",
            "source_info": {
                "paper_title": "s1: Simple test-time scaling",
                "publication_date_yy_mm": "2025-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning to reason with llms",
            "rating": 2
        },
        {
            "paper_title": "Scaling llm testtime compute optimally can be more effective than scaling model parameters",
            "rating": 2
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 2
        },
        {
            "paper_title": "Inference scaling laws: An empirical analysis of compute-optimal inference for problem-solving with language models",
            "rating": 2
        },
        {
            "paper_title": "Let's verify step by step",
            "rating": 1
        }
    ],
    "cost": 0.02372575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>s1: Simple test-time scaling</h1>
<p>Niklas Muennighoff<em> ${ }^{</em> 134}$ Zitong Yang<em> ${ }^{</em> 1}$ Weijia Shi ${ }^{<em> 23}$ Xiang Lisa Li</em> ${ }^{* 1}$ Li Fei-Fei ${ }^{1}$ Hannaneh Hajishirzi ${ }^{23}$ Luke Zettlemoyer ${ }^{2}$ Percy Liang ${ }^{1}$ Emmanuel Candès ${ }^{1}$ Tatsunori Hashimoto ${ }^{1}$</p>
<h4>Abstract</h4>
<p>Test-time scaling is a promising new approach to language modeling that uses extra test-time compute to improve performance. Recently, OpenAI's o1 model showed this capability but did not publicly share its methodology, leading to many replication efforts. We seek the simplest approach to achieve test-time scaling and strong reasoning performance. First, we curate a small dataset $\mathbf{s 1 K}$ of 1,000 questions paired with reasoning traces relying on three criteria we validate through ablations: difficulty, diversity, and quality. Second, we develop budget forcing to control test-time compute by forcefully terminating the model's thinking process or lengthening it by appending "Wait" multiple times to the model's generation when it tries to end. This can lead the model to doublecheck its answer, often fixing incorrect reasoning steps. After supervised finetuning the Qwen2.5-32B-Instruct language model on $\mathbf{s 1 K}$ and equipping it with budget forcing, our model s1-32B exceeds o1-preview on competition math questions by up to $27 \%$ (MATH and AIME24). Further, scaling s1-32B with budget forcing allows extrapolating beyond its performance without test-time intervention: from $50 \%$ to $57 \%$ on AIME24. Our model, data, and code are open-source at https : //github.com/simplescaling/s1.</p>
<h2>1. Introduction</h2>
<p>Performance improvements of language models (LMs) over the past years have largely relied on scaling up train-time compute using large-scale self-supervised pretraining (Kaplan et al., 2020; Hoffmann et al., 2022). The creation of these powerful models has set the stage for a new scaling paradigm built on top of them: test-time scaling. The aim</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Test-time scaling with s1-32B. We benchmark s1-32B on reasoning-intensive tasks and vary test-time compute.
of this approach is to increase the compute at test time to get better results. There has been much work exploring this idea (Snell et al., 2024; Welleck et al., 2024), and the viability of this paradigm was recently validated by OpenAI o1 (OpenAI, 2024). o1 has demonstrated strong reasoning performance with consistent gains from scaling test-time compute. OpenAI describes their approach as using largescale reinforcement learning (RL) implying the use of sizable amounts of data (OpenAI, 2024). This has led to various attempts to replicate their models relying on techniques like Monte Carlo Tree Search (Gao et al., 2024b; Zhang et al., 2024a), multi-agent approaches (Qin et al., 2024), and others (Wang et al., 2024a; Huang et al., 2024b; 2025). Among these approaches, DeepSeek R1 (DeepSeek-AI et al., 2025) has successfully replicated o1-level performance, also employing reinforcement learning via millions of samples and multiple training stages. However, despite the large number of o1 replication attempts, none have openly replicated a clear test-time scaling behavior. Thus, we ask: what is the simplest approach to achieve both test-time scaling and strong reasoning performance?</p>
<p>We show that training on only 1,000 samples with next-token prediction and controlling thinking duration via a simple test-time technique we refer to as budget forcing leads to a strong reasoning model that scales in performance with more test-time compute. Specifically, we construct $\mathbf{s 1 K}$, which consists of 1,000 carefully curated questions paired with reasoning traces and answers distilled from Gemini Thinking Experimental (Google, 2024). We perform supervised fine-tuning (SFT) of an off-the-shelf pretrained model</p>
<p>on our small dataset requiring just 26 minutes of training on 16 H100 GPUs. After training, we control the amount of test-time compute our model spends using budget forcing: (I) If the model generates more thinking tokens than a desired limit, we forcefully end the thinking process by appending an end-of-thinking token delimiter. Ending the thinking this way makes the model transition to generating its answer. (II) If we want the model to spend more test-time compute on a problem, we suppress the generation of the end-of-thinking token delimiter and instead append "Wait" to the model's current reasoning trace to encourage more exploration. Equipped with this simple recipe - SFT on 1,000 samples and test-time budget forcing - our model s132B exhibits test-time scaling (Figure 1). Further, s1-32B is the most sample-efficient reasoning model and outperforms closed-source models like OpenAI's o1-preview (Figure 2).</p>
<p>We conduct extensive ablation experiments targeting (a) our selection of $1,000(1 \mathrm{~K})$ reasoning samples and (b) our testtime scaling. For (a), we find that jointly incorporating difficulty, diversity, and quality measures into our selection algorithm is important. Random selection, selecting samples with the longest reasoning traces, or only selecting maximally diverse samples all lead to significantly worse performance (around $-30 \%$ on AIME24 on average). Training on our full data pool of 59 K examples, a superset of $\mathbf{s 1 K}$, does not offer substantial gains over our 1 K selection. This highlights the importance of careful data selection and echoes prior findings for instruction tuning (Zhou et al., 2023). For (b), we define desiderata for test-time scaling methods to compare different approaches. Budget forcing leads to the best scaling as it has perfect controllability with a clear positive slope leading to strong performance.</p>
<p>In summary, our contributions are: We develop simple methods for creating a sample-efficient reasoning dataset (§2) and test-time scaling (§3); Based on these we build s1-32B which is competitive with o1-preview (§4); We ablate subtleties of data (§5.1) and test-time scaling (§5.2). We end with a discussion to motivate future work on simple reasoning (§6). Our code, model, and data are open-source at https://github.com/simplescaling/s1.</p>
<h2>2. Reasoning data curation to create s1K</h2>
<p>In this section, we describe our process for creating a large dataset first in $\S 2.1$ and then filtering it down to $\mathbf{s 1 K}$ in $\S 2.2$.</p>
<h3>2.1. Initial collection of 59 K samples</h3>
<p>We collect an initial 59,029 questions from 16 sources following three guiding principles. Quality: Datasets should be high-quality; we always inspect samples and ignore datasets with, e.g., poor formatting; Difficulty: Datasets should be challenging and require significant reasoning effort; Diver-
sity: Datasets should stem from various fields to cover different reasoning tasks. We collect datasets of two categories:</p>
<p>Curation of existing datasets Our largest source is NuminaMATH (LI et al., 2024) with 30,660 mathematical problems from online websites. We also include historical AIME problems (1983-2021). To enhance diversity, we add OlympicArena (Huang et al., 2024a) with 4,250 questions spanning Astronomy, Biology, Chemistry, Computer Science, Geography, Mathematics, and Physics from various Olympiads. OmniMath (Gao et al., 2024a) adds 4,238 competition-level mathematics problems. We also include 2,385 problems from AGIEval (Zhong et al., 2023), which features questions from standardized tests like SAT and LSAT, covering English, Law, and Logic. We refer to Table 7 in $\S$ C for our other sources.</p>
<p>New datasets in quantitative reasoning To complement these existing datasets, we create two original datasets. s1-prob consists of 182 questions from the probability section of Stanford University's Statistics Department's PhD Qualifying Exams (https://statistics. stanford.edu), accompanied by handwritten solutions that cover difficult proofs. The probability qualifying exam is held yearly and requires professional-level mathematical problem-solving. s1-teasers comprises 23 challenging brain-teasers commonly used in interview questions for quantitative trading positions. Each sample consists of a problem and solution taken from PuzzledQuant (https : //www.puzzledquant.com/). We only take examples with the highest difficulty level ("Hard").</p>
<p>For each question, we generate a reasoning trace and solution using the Google Gemini Flash Thinking API (Google, 2024) extracting its reasoning trace and response. This yields 59 K triplets of a question, generated reasoning trace, and generated solution. Examples from our dataset are in §D.2. We decontaminate all samples against our evaluation questions (MATH500, GPQA Diamond, AIME24; §C.5) using 8 -grams and deduplicate the data.</p>
<h3>2.2. Final selection of 1 K samples</h3>
<p>We could directly train on our pool of 59 K questions, however, our goal is to find the simplest approach with minimal resources. Thus, we go through three stages of filtering to arrive at a minimal set of 1,000 samples relying on our three guiding data principles: Quality, Difficulty, and Diversity.</p>
<p>Quality We first remove any questions where we ran into any API errors reducing our dataset to 54,116 samples. Next, we filter out low-quality examples by checking if they contain any string patterns with formatting issues, such as ASCII art diagrams, non-existent image references, or inconsistent question numbering reducing our dataset to 51,581 examples.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. s1K and s1-32B. (left) s1K is a dataset of 1,000 high-quality, diverse, and difficult questions with reasoning traces. (right) s1-32B, a 32B parameter model finetuned on $\mathbf{s 1 K}$ is on the sample-efficiency frontier. See Table 1 for details on other models.</p>
<p>From this pool, we identify 384 samples for our final 1,000 samples from datasets that we perceive as high-quality and not in need of further filtering (see §C. 4 for details).</p>
<p>Difficulty For difficulty, we use two indicators: model performance and reasoning trace length. We evaluate two models on each question: Qwen2.5-7B-Instruct and Qwen2.5-32B-Instruct (Qwen et al., 2024), with correctness assessed by Claude 3.5 Sonnet comparing each attempt against the reference solution (see §C. 3 for the grading protocol). We measure the token length of each reasoning trace to indicate problem difficulty using the Qwen2.5 tokenizer. This relies on the assumption that more difficult problems require more thinking tokens. Based on the grading, we remove questions that either Qwen2.5-7B-Instruct or Qwen2.5-32B-Instruct can solve correctly and thus may be too easy. By using two models we reduce the likelihood of an easy sample slipping through our filtering due to a rare mistake on an easy question of one of the models. This brings our total samples down to 24,496 , setting the stage for the next round of subsampling based on diversity. While filtering with these two models may be optimized for our setup as we will also use Qwen2.5-32B-Instruct as our model to finetune, the idea of model-based filtering generalizes to other setups.</p>
<p>Diversity To quantify diversity, we classify questions into domains using Claude 3.5 Sonnet based on the Mathematics Subject Classification (MSC) system (e.g., geometry, combinatorics, etc.) from the American Mathematical Society. ${ }^{1}$ The taxonomy focuses on topics in mathematics but also includes other sciences such as biology, physics, and eco-</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>nomics. To select our final examples from the pool of 24,496 questions, we first choose one domain uniformly at random. Then, we sample one problem from this domain according to a distribution that favors longer reasoning traces (see §C. 4 for details) as motivated in Difficulty. We repeat this process until we have 1,000 total samples spanning 50 domains.</p>
<p>In $\S 5.1$, we will show that using our three criteria in combination is important, as only relying on quality, diversity, or difficulty in isolation leads to worse datasets. Some distilled generations are incorrect, which we allow in our data as we focus on capturing the reasoning process rather than entirely correct solutions. Our grader (§C.3) deems $53.6 \%$ correct in s1K and $63.0 \%$ in our follow-up s1K-1.1 (see §A).</p>
<h2>3. Test-time scaling</h2>
<h3>3.1. Method</h3>
<p>We classify test-time scaling methods into 1) Sequential, where later computations depend on earlier ones (e.g., a long reasoning trace), and 2) Parallel, where computations run independently (e.g., majority voting) (Snell et al., 2024; Brown et al., 2024). We focus on sequential scaling as intuitively we believe it should scale better, since later computations can build on intermediate results, allowing for deeper reasoning and iterative refinement. We propose new sequential scaling methods and ways to benchmark them.</p>
<p>Budget forcing We propose a simple decoding-time intervention by forcing a maximum and/or minimum number of thinking tokens. Specifically, we enforce a maximum token count by simply appending the end-of-thinking token delimiter and optionally "Final Answer:" to early exit</p>
<p>How many r in raspberry? Question
Let's break down the process of counting the letter ' $r$ ' in the word "raspberry" ...</p>
<ul>
<li>First letter: ' $r$ ' - This is an ' $r$ ', count $=1$.</li>
<li>Second letter: 'a' - Not an ' $r$ ', count remains 1 ...</li>
<li>Sixth letter: 'e' - Not an ' $r$ ', count remains 1.</li>
<li>Seventh letter: ' $r$ ' - This is an ' $r$ ', count $=2$.</li>
<li>Eighth letter: ' $y$ ' - Not an ' $r$ ', count remains 2 ...</li>
</ul>
<p>The number of 'r's in "raspberry" is 2.
Wait, let's re-read the question carefully. It asks "How many r in raspberry?" ... * r - a - s - p - b - e - r - r - y ... * First 'r' ... * Second ' $r$ ' ... * Third ' $r$ ' ... Count $=3 \ldots$ Reasoning trace</p>
<p>My initial answer of 2 was incorrect due to a quick reading of the word. Final Answer: The final answer is 3 Response</p>
<p>Figure 3. Budget forcing with s1-32B. The model tries to stop after "...is 2.", but we suppress the end-of-thinking token delimiter instead appending "Wait" leading s1-32B to self-correct its answer.
the thinking stage and make the model provide its current best answer. To enforce a minimum, we suppress the generation of the end-of-thinking token delimiter and optionally append the string "Wait" to the model's current reasoning trace to encourage the model to reflect on its current generation. Figure 3 contains an example of how this simple approach can lead the model to arrive at a better answer.</p>
<p>Baselines We benchmark budget forcing with: (I) Conditional length-control methods, which rely on telling the model in the prompt how long it should generate for. We group them by granularity into (a) Token-conditional control: We specify an upper bound of thinking tokens in the prompt; (b) Step-conditional control: We specify an upper bound of thinking steps, where each step is around 100 tokens; (c) Class-conditional control: We write two generic prompts that tell the model to either think for a short or long amount of time (see §E. 1 for details). (II) Rejection sampling, which samples until a generation fits a predetermined compute budget. This oracle captures the posterior over responses conditioned on its length.</p>
<h3>3.2. Metrics</h3>
<p>We establish a set of desiderata as evaluation metrics to measure test-time scaling across methods. Importantly, we do not only care about the accuracy a method can achieve but also its controllability and test-time scaling slope. For each method we consider, we run a set of evaluations $a \in$ $\mathcal{A}$ varying test-time compute on a fixed benchmark, e.g. AIME24. This produces a piece-wise linear function $f$ with compute as the x -axis measured in thinking tokens and accuracy as the y -axis (see Figure 1, where the rightmost dot for AIME24 corresponds to $f(7320)=57 \%$ ). We measure
three metrics:</p>
<p>$$
\text { Control }=\frac{1}{|\mathcal{A}|} \sum_{a \in \mathcal{A}} \mathbb{I}\left(a_{\min } \leq a \leq a_{\max }\right)
$$</p>
<p>where $a_{\min }, a_{\max }$ refer to a pre-specified minimum and maximum amount of test-time compute; in our case thinking tokens. We usually only constrain $a_{\max }$. As tokens generated correspond to the amount of test-time compute spent, this metric measures the extent to which a method allows controllability over the use of that test-time compute. We report it as a percentage with $100 \%$ being perfect control.</p>
<p>$$
\text { Scaling }=\frac{1}{\left(\frac{|\mathcal{A}|}{2}\right)} \sum_{\substack{a, b \in \mathcal{A} \ b&gt;a}} \frac{f(b)-f(a)}{b-a}
$$</p>
<p>Scaling is the average slope of the piece-wise linear function. It must be positive for useful methods and larger is better.</p>
<p>$$
\text { Performance }=\max _{a \in \mathcal{A}} f(a)
$$</p>
<p>Performance is simply the maximum performance the method achieves on the benchmark. A method with monotonically increasing scaling achieves $100 \%$ performance on any benchmark in the limit. However, the methods we investigate eventually flatten out or further scaling fails due to control or context window limitations.</p>
<h2>4. Results</h2>
<h3>4.1. Setup</h3>
<p>Training We perform supervised finetuning on Qwen2.5-32B-Instruct using s1K to obtain our model s1-32B using basic hyperparameters outlined in §D. Finetuning took 26 minutes on 16 NVIDIA H100 GPUs with PyTorch FSDP.</p>
<p>Evaluation We select three representative reasoning benchmarks widely used in the field: AIME24 (of America, 2024) has 30 problems that were used in the 2024 American Invitational Mathematics Examination (AIME) held from January 31 - February 1, 2024. AIME tests mathematical problem-solving with arithmetic, algebra, counting, geometry, number theory, probability, and other secondary school math topics. High-scoring high school students in the test are invited to participate in the United States of America Mathematics Olympiad (USAMO). All AIME answers are integers ranging from 000 to 999 , inclusive. Some AIME problems rely on figures that we provide to our model using the vector graphics language Asymptote as it cannot take image inputs. MATH500 (Hendrycks et al., 2021) is a benchmark of competition math problems of varying difficulty. We evaluate on the same 500 samples selected by OpenAI in prior work (Lightman et al., 2023). GPQA Diamond (Rein et al., 2023) consists of 198 PhD-level science questions</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4. Sequential and parallel test-time scaling. (a): Budget forcing shows clear scaling trends and extrapolates to some extent. For the three rightmost dots, we prevent the model from stopping its thinking $2 / 4 / 6$ times, each time appending "Wait" to its current reasoning trace. (b): For Qwen2.5-32B-Instruct we perform 64 evaluations for each sample with a temperature of 1 and visualize the performance when majority voting across $2,4,8,16,32$, and 64 of these.
from Biology, Chemistry and Physics. Experts with PhDs in the corresponding domains only achieved $69.7 \%$ on GPQA Diamond (OpenAI, 2024). When we write "GPQA" in the context of evaluation in this work, we always refer to the Diamond subset. We build on the "lm-evaluation-harness" framework (Gao et al., 2021; Biderman et al., 2024). Unless otherwise specified, we evaluate with a temperature of 0 (greedy) and measure accuracy (equivalent to pass@1).</p>
<p>Other models We benchmark s1-32B against: OpenAI o1 series (OpenAI, 2024), closed-source models that popularized test-time scaling; DeepSeek r1 series (DeepSeekAI et al., 2025), open-weight reasoning models with up to o1-level performance; Qwen's QwQ-32B-preview (Team, 2024), a 32B open-weight reasoning model without disclosed methodology; Sky-T1-32B-Preview (Team, 2025) and Bespoke-32B (Labs, 2025), open models with open reasoning data distilled from QwQ-32B-preview and r1; Google Gemini 2.0 Flash Thinking Experimental (Google, 2024), the API that we distill from. As it has no official evaluation scores, we use the Gemini API to benchmark it ourselves. However, the "recitation error" of the Gemini API makes evaluation challenging. ${ }^{2}$ We circumvent this, by manually inserting all 30 AIME24 questions in its web interface where the error does not appear. However, we leave out MATH500 (500 questions) and GPQA Diamond (198 questions), thus they are N.A. in Table 1. Our model, s1-32B, is fully open including weights, reasoning data, and code.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 1. s1-32B is a strong open reasoning model. We evaluate s1-32B, Qwen, and Gemini (some entries are unknown (N.A.), see §4). Other results are from the respective reports (Qwen et al., 2024; Team, 2024; OpenAI, 2024; DeepSeek-AI et al., 2025; Labs, 2025; Team, 2025). # ex. = number examples used for reasoning finetuning; $\mathrm{BF}=$ budget forcing. See $\S \mathrm{A}$ for our better s1.1 model.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;"># ex.</th>
<th style="text-align: center;">AIME <br> 2024</th>
<th style="text-align: center;">MATH <br> 500</th>
<th style="text-align: center;">GPQA <br> Diamond</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">API only</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">o1-preview</td>
<td style="text-align: center;">N.A.</td>
<td style="text-align: center;">44.6</td>
<td style="text-align: center;">85.5</td>
<td style="text-align: center;">73.3</td>
</tr>
<tr>
<td style="text-align: center;">o1-mini</td>
<td style="text-align: center;">N.A.</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">90.0</td>
<td style="text-align: center;">60.0</td>
</tr>
<tr>
<td style="text-align: center;">o1</td>
<td style="text-align: center;">N.A.</td>
<td style="text-align: center;">74.4</td>
<td style="text-align: center;">94.8</td>
<td style="text-align: center;">77.3</td>
</tr>
<tr>
<td style="text-align: center;">Gemini 2.0 <br> Flash Think.</td>
<td style="text-align: center;">N.A.</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">N.A.</td>
<td style="text-align: center;">N.A.</td>
</tr>
<tr>
<td style="text-align: center;">Open Weights</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5- <br> 32B-Instruct</td>
<td style="text-align: center;">N.A.</td>
<td style="text-align: center;">26.7</td>
<td style="text-align: center;">84.0</td>
<td style="text-align: center;">49.0</td>
</tr>
<tr>
<td style="text-align: center;">QwQ-32B</td>
<td style="text-align: center;">N.A.</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">90.6</td>
<td style="text-align: center;">54.5</td>
</tr>
<tr>
<td style="text-align: center;">r1</td>
<td style="text-align: center;">$\gg 800 \mathrm{~K}$</td>
<td style="text-align: center;">79.8</td>
<td style="text-align: center;">97.3</td>
<td style="text-align: center;">71.5</td>
</tr>
<tr>
<td style="text-align: center;">r1-distill</td>
<td style="text-align: center;">800 K</td>
<td style="text-align: center;">72.6</td>
<td style="text-align: center;">94.3</td>
<td style="text-align: center;">62.1</td>
</tr>
<tr>
<td style="text-align: center;">Open Weights and Open Data</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Sky-T1</td>
<td style="text-align: center;">17 K</td>
<td style="text-align: center;">43.3</td>
<td style="text-align: center;">82.4</td>
<td style="text-align: center;">56.8</td>
</tr>
<tr>
<td style="text-align: center;">Bespoke-32B</td>
<td style="text-align: center;">17 K</td>
<td style="text-align: center;">63.3</td>
<td style="text-align: center;">93.0</td>
<td style="text-align: center;">58.1</td>
</tr>
<tr>
<td style="text-align: center;">s1 w/o BF</td>
<td style="text-align: center;">1 K</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">92.6</td>
<td style="text-align: center;">56.6</td>
</tr>
<tr>
<td style="text-align: center;">s1-32B</td>
<td style="text-align: center;">1 K</td>
<td style="text-align: center;">56.7</td>
<td style="text-align: center;">93.0</td>
<td style="text-align: center;">59.6</td>
</tr>
</tbody>
</table>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<h3>4.2. Performance</h3>
<p>Test-time scaling Figure 1 shows the performance of s132B with budget forcing scales with more test-time compute. In Figure 4 (left), we expand the plot from Figure 1 (middle) showing that while we can improve AIME24 performance using our budget forcing technique (§3) and more test-time compute it does eventually flatten out at six times. Suppressing the end-of-thinking token delimiter too often can lead the model into repetitive loops instead of continued reasoning. In Figure 4 (right), we show that after training Qwen2.5-32B-Instruct on our 1,000 samples to produce s1-32B and equipping it with the simple budget forcing technique, it operates in a different scaling paradigm. Scaling test-time compute on the base model via majority voting cannot catch up with the performance of s1-32B which validates our intuition from $\S 3$ that sequential scaling is more effective than parallel. We provide example generations of s1-32B in Figure 5.</p>
<p>Sample-efficiency In Figure 2 (right) and Table 1 we compare s1-32B with other models. We find that s132B is the most sample-efficient open data reasoning model. It performs significantly better than our base model (Qwen2.5-32B-Instruct) despite just training it on an additional 1,000 samples. The concurrently released r1-32B shows stronger performance than s1-32B while also only using SFT (DeepSeek-AI et al., 2025). However, it is trained on $800 \times$ more reasoning samples. It is an open question whether one can achieve their performance with just 1,000 samples. Finally, our model nearly matches Gemini 2.0 Thinking on AIME24. As the data for s1-32B is distilled from Gemini 2.0, this shows our distillation procedure was likely effective.</p>
<h2>5. Ablations</h2>
<h3>5.1. Data Quantity, Diversity, and Difficulty</h3>
<p>In $\S 2$ we outlined our three guiding principles in curating s1K: Quality, Difficulty, and Diversity. Here we test the importance of combining them and the overall efficacy of our selection. Only Quality (1K-random): After obtaining our high-quality reasoning chains from Gemini, we select 1,000 samples at random; not relying on our difficulty and diversity filtering at all. Table 2 shows this approach performs much worse than s1K across all benchmarks. Only Diversity (1K-diverse): For this dataset, we sample uniformly across domains to maximize diversity disregarding any notion of difficulty. This approach also leads to poor performance similar to 1K-random. Only Difficulty (1K-longest): Here we rely on one of our difficulty indicators introduced in $\S 2$ by selecting the 1,000 samples with the longest reasoning traces. This approach significantly boosts GPQA performance but overall still falls short of using s1K. Maximize Quantity:</p>
<p>Table 2. s1K data ablations. We budget force (BF) a maximum of around 30,000 thinking tokens for all scores in this table. This performs slightly better than the scores without BF (Table 1) as it allows the model to finish with a best guess when stuck in an infinite loop. We report $95 \%$ paired bootstrap confidence intervals for differences relative to the $\mathbf{s 1 K}$ model using 10,000 bootstrap samples. E.g., the interval $[-13 \%, 20 \%]$ means that, with $95 \%$ confidence, the true difference between 59 K -full and $\mathbf{s 1 K}$ is between $-13 \%$ and $+20 \%$. If the entire interval is negative, e.g. [-27\%, -3\%], we can confidently say that the performance is worse than s1K.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">AIME <br> 2024</th>
<th style="text-align: center;">MATH <br> 500</th>
<th style="text-align: center;">GPQA <br> Diamond</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">1K-random</td>
<td style="text-align: center;">36.7</td>
<td style="text-align: center;">90.6</td>
<td style="text-align: center;">52.0</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">$[-26.7 \%,-3.3 \%]$</td>
<td style="text-align: center;">$[-4.8 \%, 0.0 \%]$</td>
<td style="text-align: center;">$[-12.6 \%, 2.5 \%]$</td>
</tr>
<tr>
<td style="text-align: left;">1K-diverse</td>
<td style="text-align: center;">26.7</td>
<td style="text-align: center;">91.2</td>
<td style="text-align: center;">54.6</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">$[-40.0 \%,-10.0 \%]$</td>
<td style="text-align: center;">$[-4.0 \%, 0.2 \%]$</td>
<td style="text-align: center;">$[-10.1 \%, 5.1 \%]$</td>
</tr>
<tr>
<td style="text-align: left;">1K-longest</td>
<td style="text-align: center;">33.3</td>
<td style="text-align: center;">90.4</td>
<td style="text-align: center;">59.6</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">$[-36.7 \%, 0.0 \%]$</td>
<td style="text-align: center;">$[-5.0 \%,-0.2 \%]$</td>
<td style="text-align: center;">$[-5.1 \%, 10.1 \%]$</td>
</tr>
<tr>
<td style="text-align: left;">59K-full</td>
<td style="text-align: center;">53.3</td>
<td style="text-align: center;">92.8</td>
<td style="text-align: center;">58.1</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">$[-13.3 \%, 20.0 \%]$</td>
<td style="text-align: center;">$[-2.6 \%, 2.2 \%]$</td>
<td style="text-align: center;">$[-6.6 \%, 8.6 \%]$</td>
</tr>
<tr>
<td style="text-align: left;">s1K</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">93.0</td>
<td style="text-align: center;">57.6</td>
</tr>
</tbody>
</table>
<p>Finally, we compare with just training on all of our 59K samples, a superset of all the 1K-sample versions. This leads to a strong model but uses much more resources. To finetune on 59K samples, we use 394 H100 GPU hours while s1-32B only required 7 H100 GPU hours. Moreover, relying only on $\mathbf{s 1 K}$ is extremely competitive as shown in $\S 2$. Overall, combining all three criteria - Quality, Difficulty, Diversity - via our methodology in $\S 2$ is key for sampleefficient reasoning training.</p>
<h3>5.2. Test-time scaling methods</h3>
<p>Table 3. Ablations on methods to scale test-time compute on AIME24. $|A|$ refers to the number of evaluation runs used to estimate the properties; thus a higher value indicates more robustness. Bold indicates our chosen method and the best values. BF $=$ budget forcing, TCC/SCC/CCC $=$ token/step/class-conditional control, RS = rejection sampling.</p>
<p>| Method | Control | Scaling | Performance | $|\mathcal{A}|$ |
| :-- | :--: | :--: | :--: | :--: |
| BF | $\mathbf{1 0 0 \%}$ | 15 | $\mathbf{5 6 . 7}$ | 5 |
| TCC | $40 \%$ | -24 | 40.0 | 5 |
| TCC + BF | $\mathbf{1 0 0 \%}$ | 13 | 40.0 | 5 |
| SCC | $60 \%$ | 3 | 36.7 | 5 |
| SCC + BF | $\mathbf{1 0 0 \%}$ | 6 | 36.7 | 5 |
| CCC | $50 \%$ | $\mathbf{2 5}$ | 36.7 | 2 |
| RS | $\mathbf{1 0 0 \%}$ | -35 | 40.0 | 5 |</p>
<h2>[AIME24]</h2>
<p>Alice and Bob play the following game. A stack of $n$ tokens lies before them. The players take turns with Alice going first. On each turn, the player removes either 1 token or 4 tokens from the stack. Whoever removes the last token wins. Find the number of positive integers $n$ less than or equal to 2024 for which there exists a strategy for Bob that guarantees that Bob will win the game regardless of Alice's play.</p>
<p>The problem asks for the number of positive integers $n \leq 2024$ such that Bob has a winning strategy. This is a typical impartial game that can be analyzed using the Sprague-Grundy theorem or by directly finding the winning and losing positions. Let $W$ be the set of winning positions (number of tokens $n$ such that the player whose turn it is can force a win) and $L$ be the set of losing positions. [...]</p>
<p>Let $P(n)$ be the statement "the player whose turn it is when there are $n$ tokens has a winning strategy. $P(0)$ is false (the player whose turn it is and faces 0 tokens has already lost). Consider the small values of $n$ : $n=1$ : The player whose turn it is can take 1 token and win. So $P(1)$ is true. $1 \in W$.
$n=2$ : The player whose turn it is can take 1 token, leaving 1 token for the opponent. Since $1 \in W$, the opponent wins. So $P(2)$ is false. $2 \in L$.
$n=3$ : The player whose turn it is can take 1 token, leaving 2 tokens for the opponent. Since $2 \in L$, the player whose turn it is wins. So $P(3)$ is true. $3 \in$ $W .[\ldots]$</p>
<p>We are looking for the number of positive integers $n \leq 2024$ for which Bob has a winning strategy. This means that if the game starts with $n$ tokens, and Alice goes first, Bob can win. This happens if and only if $n$ is a losing position (because if $n \in L$, then Alice's first move must lead to a winning position for Bob). [...]
Let's try to find a pattern in the losing positions $L={0,2,5,7,10,12,15,17,20,22, \ldots}$. Differences between consecutive terms: $2,3,2,3,2,3,2,3,2, \ldots$ The pattern of differences is alternating 2,3. [...] The losing positions are of the form $5 m$ or $5 m+2$, where $m \geq 0$. We need to find the number of positive integers $n \leq 2024$ that are in the set of losing positions L. [...]</p>
<p>Case 1: $n=5 m$, where $m \geq 1.1 \leq 5 m \leq 2024$ $1 / 5 \leq m \leq 2024 / 5=404.8$ Since $m$ is an integer, $1 \leq m \leq 404$. The number of values of $m$ is 404 .
Case 2: $n=5 m+2$, where $m \geq 0.5 m+2 \leq 2024$ $5 m \leq 2022 m \leq 2022 / 5=404.4$ Since $m$ is an integer, $0 \leq m \leq 404$. The number of values of $m$ is $404-0+1=405$.</p>
<p>The total number of positive integers $n \leq 2024$ that are losing positions is $404+405=\overline{[809]}$.
[MATH500]
Find the projection of $a$ onto $b=$ $\binom{2}{3}$ if $a \cdot b=8$</p>
<p>The projection of vector a onto vector b is given by the formula:</p>
<p>$$
\operatorname{proj}_{\mathrm{b}} \mathbf{a}=\frac{\mathbf{a} \cdot \mathbf{b}}{|\mathbf{b}|^{2}} \mathbf{b}
$$</p>
<p>We are given $\mathbf{b}=\binom{2}{6}$ and $\mathbf{a} \cdot \mathbf{b}=8$.</p>
<p>First, we need to find the magnitude squared of $\mathbf{b}$, which is $|\mathbf{b}|^{2}$.</p>
<p>$$
|\mathbf{b}|^{2}=\mathbf{b} \cdot \mathbf{b}=2^{2}+6^{2}+3^{2}=4+36+9=49 . V(r, \theta)=\frac{1}{2} k r^{2}+\frac{3}{2} k r^{2} \cos ^{2}(\theta)
$$</p>
<p>Now we can plug the values into the projection formula:</p>
<p>$$
\operatorname{proj}_{\mathrm{b}} \mathbf{a}=\frac{8}{49} \mathbf{b}
$$</p>
<p>Substitute the vector $\mathbf{b}$ :</p>
<p>$$
\operatorname{proj}_{\mathrm{b}} \mathbf{a}=\frac{8}{49}\binom{2}{6}
$$</p>
<p>Multiply the scalar by each component of the vector:</p>
<p>$$
\operatorname{proj}_{\mathrm{b}} \mathbf{a}=\left(\begin{array}{c}
\frac{8}{49} \times 2 \
\frac{2}{6} \times 6 \
\frac{2}{49} \times 3
\end{array}\right)=\left(\begin{array}{c}
\frac{16}{49} \
\frac{16}{49} \
\frac{16}{49}
\end{array}\right)
$$</p>
<p>Final Answer: The final answer is</p>
<p>$$
\left[\left(\begin{array}{c}
\frac{16}{49} \
\frac{16}{49} \
\frac{16}{49}
\end{array}\right)\right]
$$</p>
<h2>[GPQA]</h2>
<p>A quantum mechanical particle of mass $m$ moves in two dimensions in the following potential, as a function of the polar coordinates $(r, \theta)$ :</p>
<p>$$
V(r, \theta)=\frac{1}{2} k r^{2}+\frac{3}{2} k r^{2} \cos ^{2}(\theta)
$$</p>
<p>Find the energy spectrum. Hint: Write the potential in Cartesian coordinates.</p>
<p>The problem asks for the energy spectrum of a particle in a given 2D potential. The potential is given in polar coordinates $(r, \theta)$ :</p>
<p>$$
V(r, \theta)=\frac{1}{2} k r^{2}+\frac{3}{2} k r^{2} \cos ^{2}(\theta)
$$</p>
<p>We can rewrite the potential as:</p>
<p>$$
V(r, \theta)=\frac{1}{2} k r^{2}\left(1+3 \cos ^{2}(\theta)\right)
$$</p>
<p>The Hamiltonian for a particle in 2D is given by:</p>
<p>$$
H=-\frac{\hbar^{2}}{2 m}\left(\frac{\partial^{2}}{\partial x^{2}}+\frac{\partial^{2}}{\partial y^{2}}\right)+V(x, y)
$$</p>
<p>In polar coordinates $(r, \theta)$, the Laplacian operator is:</p>
<p>$$
\nabla^{2}=\frac{1}{r} \frac{\partial}{\partial r}\left(r \frac{\partial}{\partial r}\right)+\frac{1}{r^{2}} \frac{\partial^{2}}{\partial \theta^{2}}
$$</p>
<p>So the Hamiltonian in polar coordinates is:</p>
<p>$$
H=-\frac{\hbar^{2}}{2 m}\left[\frac{1}{r} \frac{\partial}{\partial r}\left(r \frac{\partial}{\partial r}\right)+\frac{1}{r^{2}} \frac{\partial^{2}}{\partial \theta^{2}}\right]+V(r, \theta)
$$</p>
<p>The time-independent Schrödinger equation is $H \psi=E \psi$. We can try to separate variables by assuming a solution of the form $\psi(r, \theta)=R(r) \Theta(\theta)$. $[\ldots]$</p>
<p>This confirms the energy spectrum is:</p>
<p>$$
E=\hbar \sqrt{\frac{k}{m}}\left(2 n_{x}+n_{y}+\frac{3}{2}\right)
$$</p>
<p>Final Answer: $\quad E=$ $h \sqrt{\frac{k}{m}}\left(2 n_{x}+n_{y}+\frac{3}{2}\right)$.</p>
<p>Figure 5. Example model outputs. We pick a question from AIME24 (left), MATH500 (middle), and GPQA (right), where our model generates the correct answer. The black text is the prompt, the light blue text is the reasoning trace, and the blue text is the answer of s1-32B. The gray ellipsis [...] indicates that the text was trimmed to fit this page, but the generated text is actually longer.</p>
<p>Budget forcing In Table 3 we compare the test-time scaling methods we have introduced in $\S 3$. Overall, we find that budget forcing provides perfect control, good scaling, and leads to our best AIME24 score. Thus, this is the method we use for s1-32B in Figure 1 and in $\S 4$. In Table 4, we compare different strings for extrapolating performance. We find that "Wait" generally gives the best performance.</p>
<p>Class-conditional control We provide benchmark scores for this method in $\S$ E. 1 and summarize three findings here: (1) Token-conditional control fails without budget forcing, as our model cannot reliably count tokens - even when trained to do so. (2) Under step-conditional control, the model generates a similar total number of tokens when given different step targets, as the model goes from few steps with many tokens per step, to many steps with few tokens in each step. Thus, the model learns to hack its way around the compute constraint making the controllability of this method mediocre. (3) Class-conditional control can work - telling a model to simply think longer can increase its test-time compute and performance, which leads good scaling in Table 3.</p>
<p>Table 4. Budget forcing extrapolation ablations. We compare ignoring the end-of-thinking delimiter twice and appending none or various strings.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">AIME <br> 2024</th>
<th style="text-align: center;">MATH <br> 500</th>
<th style="text-align: center;">GPQA <br> Diamond</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">No extrapolation</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">$\mathbf{9 3 . 0}$</td>
<td style="text-align: center;">57.6</td>
</tr>
<tr>
<td style="text-align: left;">2x without string</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">90.2</td>
<td style="text-align: center;">55.1</td>
</tr>
<tr>
<td style="text-align: left;">2x "Alternatively"</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">92.2</td>
<td style="text-align: center;">$\mathbf{5 9 . 6}$</td>
</tr>
<tr>
<td style="text-align: left;">2x "Hmm"</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">$\mathbf{9 3 . 0}$</td>
<td style="text-align: center;">$\mathbf{5 9 . 6}$</td>
</tr>
<tr>
<td style="text-align: left;">2x "Wait"</td>
<td style="text-align: center;">$\mathbf{5 3 . 3}$</td>
<td style="text-align: center;">$\mathbf{9 3 . 0}$</td>
<td style="text-align: center;">$\mathbf{5 9 . 6}$</td>
</tr>
</tbody>
</table>
<p>Rejection sampling Surprisingly, we find that simply sampling until the generation fits a specific length leads to an inverse scaling trend as depicted in Figure 6. In §E. 2 we in-
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 6. Rejection sampling on AIME24 with s1-32B. We sample with a temperature of 1 until all generations have less than (from left to right) $3500,4000,5000,8000$, and 16000 thinking tokens requiring an average of $655,97,8,3,2$, and 1 tries per sample.
spect a question, which was answered correctly by the model when rejection sampling for $\leq 4000$, but not for the $\leq 8000$ token setting. In the $\leq 4000$ setting the model directly jumps to the correct approach, while for the $\leq 8000$ setting it backtracks a lot. We hypothesize that there is a correlation such that shorter generations tend to be the ones where the model was on the right track from the start, whereas longer ones tend to be ones where the model made mistakes and thus backtracks or questions itself. This leads to longer samples often being wrong when rejection sampling and thus the inverse scaling trend.</p>
<h2>6. Discussion and related work</h2>
<h3>6.1. Sample-efficient reasoning</h3>
<p>Models There are a number of concurrent efforts to build models that replicate the performance of o1 (OpenAI, 2024). For example, DeepSeek-r1 and k1.5 (DeepSeek-AI et al., 2025; Team et al., 2025) are built with reinforcement learning methods, while others rely on SFT using tens of thousands of distilled examples (Team, 2025; Xu et al., 2025; Labs, 2025). We show that SFT on only 1,000 examples suffices to build a competitive reasoning model matching o1-preview and produces a model that lies on the pareto frontier (Figure 2). Further, we introduce budget forcing which combined with our reasoning model leads to the first reproduction of OpenAI's test-time scaling curves (OpenAI, 2024). Why does supervised finetuning on just 1,000 samples lead to such performance gains? We hypothesize that the model is already exposed to large amounts of reasoning data during pretraining which spans trillions of tokens. Thus, the ability to perform reasoning is already present in our model. Our sample-efficient finetuning stage just activates it and we scale it further at test time with budget forcing. This is similar to the "Superficial Alignment Hypothesis" presented in LIMA (Zhou et al., 2023), where the authors find that 1,000 examples can be sufficient to align a model to adhere to user preferences.</p>
<p>Benchmarks and methods To evaluate and push the limits of these models, increasingly challenging benchmarks have been introduced, such as Olympiad-level science competitions (He et al., 2024; Jain et al., 2024; Zhong et al., 2023) and others (Srivastava et al., 2023; Glazer et al., 2024; Su et al., 2024; Kim et al., 2024; Phan et al., 2025). To enhance models' performance on reasoning-related tasks, researchers have pursued several strategies: Prior works have explored continuing training language models on specialized corpora related to mathematics and science (Azerbayev et al., 2023; Yang et al., 2024), sometimes even synthetically generated data (Yu et al., 2024). Others have developed training methodologies specifically aimed at reasoning performance (Zelikman et al., 2022; 2024; Luo et al., 2025;</p>
<p>Yuan et al., 2025; Wu et al., 2024a). Another significant line of work focuses on prompting-based methods to elicit and improve reasoning abilities, including methods like Chain-of-Thought prompting (Wei et al., 2023; Yao et al., 2023a;b; Bi et al., 2023; Fu et al., 2023; Zhang et al., 2024b; Xiang et al., 2025; Hu et al., 2024; Diao et al., 2024). These combined efforts aim to advance the reasoning ability of language models, enabling them to handle more complex and abstract tasks effectively.</p>
<h3>6.2 Test-time scaling</h3>
<p>Methods As we introduce in §3, we differentiate two methods to scale test-time compute: parallel and sequential. The former relies on multiple solution attempts generated in parallel and selecting the best outcome via specific criteria. These criteria include choosing the most frequent response for majority voting or the best response based on an external reward for Best-of-N (Brown et al., 2024; Irvine et al., 2023; Levi, 2024). Unlike repeated sampling, previous sequential scaling methods let the model generate solution attempts sequentially based on previous attempts, allowing it to refine each attempt based on previous outcomes (Snell et al., 2024; Hou et al., 2025; Lee et al., 2025). Tree-based search methods (Gandhi et al., 2024; Wu et al., 2024b) offer a hybrid approach between sequential and parallel scaling, such as Monte-Carlo Tree Search (MCTS) (Liu et al., 2024; Zhang et al., 2023; Zhou et al., 2024; Choi et al., 2023) and guided beam search (Xie et al., 2023). REBASE (Wu et al., 2024b) employs a process reward model to balance exploitation and pruning during tree search. Empirically, REBASE has been shown to outperform sampling-based methods and MCTS (Wu et al., 2024b). Reward models (Lightman et al., 2023; Wang et al., 2024b;c) play a key role in these methods. They come in two variants: outcome reward models and process reward models. Outcome reward models (Xin et al., 2024; Ankner et al., 2024) assign a score to complete solutions and are particularly useful in Best-of-N selection, while process reward models (Lightman et al., 2023; Wang et al., 2024b; Wu et al., 2024b) assess individual reasoning steps and are effective in guiding tree-based search methods.</p>
<p>Limits to further test-time scaling We have shown that budget forcing allows extrapolating test-time compute in $\S 4$, e.g., improving AIME24 performance from $50 \%$ to $57 \%$. However, it has two key limitations when scaling further: it eventually flattens out (Figure 4), and the context window of the underlying language model constrains it. Despite these constraints, our work shows test-time scaling across a wide range of accuracies (Figure 1), partly because scaling down test-time compute behaves predictably and does not suffer from these constraints.</p>
<p>Continuing test-time scaling will require approaches that can further extrapolate test-time compute. How can we get
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 7. Scaling further with parallel scaling methods. All metrics averaged over the 30 questions in AIME24. Average thinking tokens for REBASE do not account for the additional compute from the reward model. For sequential scaling, we prompt the model to use up to (from left to right) $32,64,256$, and 512 steps. For REBASE and majority voting we generate 16 parallel trajectories to aggregate across.
such extrapolation? There may be improvements to budget forcing such as rotating through different strings, not only "Wait", or combining it with frequency penalties or higher temperature to avoid repetitive loops. An exciting direction for future work is also researching whether applying budget forcing to a reasoning model trained with reinforcement learning yields better extrapolation; or if RL allows for new ways of test-time scaling beyond budget forcing. Our work defines the right metrics (§3.2) - Control, Scaling, and Performance - to enable future research and progress on extrapolating test-time compute.</p>
<p>Parallel scaling as a solution Parallel scaling offers one solution to the limits of sequential scaling, thus we augment our sequentially scaled model with two methods: (I) Majority voting: After generating $k$ solutions, the final solution is the most frequent one across generations; (II) Tree search via REBASE: We use the REBASE process reward model, which is initialized from LLaMA-34B and further finetuned on a synthetic process reward modeling dataset (Wu et al., 2024b). We then aggregate the solutions generated by REBASE via majority voting. As shown in Figure 7, augmenting our model with REBASE scales better than majority voting, and even sequential scaling in this scenario. However, REBASE requires an additional forward pass at each step for the reward model adding some computation overhead. For sequential scaling, when prompted to use up to 512 steps, for 12 out of the 30 evaluation questions the model generates a response that exceeds the context window leading to a large performance drop. Overall, we find that these parallel scaling methods complement sequential scaling thus they offer an avenue for scaling test-time compute even further; beyond fixed context windows.</p>
<h2>Impact Statement</h2>
<p>Language models with strong reasoning capabilities have the potential to greatly enhance human productivity, from assisting in complex decision-making to driving scientific breakthroughs. However, recent advances in reasoning, such as OpenAI's ol and DeepSeek's r1, lack transparency, limiting broader research progress. Our work aims to push the frontier of reasoning in a fully open manner, fostering innovation and collaboration to accelerate advancements that ultimately benefit society.</p>
<h2>Acknowledgements</h2>
<p>We thank Ryan Marten for generating traces from DeepSeek r1 for s1.1 using Bespoke Curator (Marten et al., 2025). This work was partly conducted using the Stanford Marlowe GPU cluster (Kapfer et al., 2025), made possible by financial support from Stanford University. We thank Alexander M. Rush, Andrew Ilyas, Banghua Zhu, Chenglei Si, Chunting Zhou, John Yang, Ludwig Schmidt, Samy Jelassi, Suhas Kotha, Tengyu Ma, Xuechen Li, Yu Sun, and Yue Zhang for very constructive discussions.</p>
<h2>References</h2>
<p>Ankner, Z., Paul, M., Cui, B., Chang, J. D., and Ammanabrolu, P. Critique-out-loud reward models, 2024. URL https://arxiv.org/abs/2408.11791.</p>
<p>Arora, D., Singh, H. G., and Mausam. Have llms advanced enough? a challenging problem solving benchmark for large language models, 2023. URL https://arxiv. org/abs/2305.15074.</p>
<p>Azerbayev, Z., Schoelkopf, H., Paster, K., Santos, M. D., McAleer, S., Jiang, A. Q., Deng, J., Biderman, S., and Welleck, S. Llemma: An open language model for mathematics, 2023.</p>
<p>Bi, Z., Zhang, N., Jiang, Y., Deng, S., Zheng, G., and Chen, H. When do program-of-thoughts work for reasoning?, 2023. URL https://arxiv.org/abs/2308. 15452 .</p>
<p>Biderman, S., Schoelkopf, H., Sutawika, L., Gao, L., Tow, J., Abbasi, B., Aji, A. F., Ammanamanchi, P. S., Black, S., Clive, J., DiPofi, A., Etxaniz, J., Fattori, B., Forde, J. Z., Foster, C., Hsu, J., Jaiswal, M., Lee, W. Y., Li, H., Lovering, C., Muennighoff, N., Pavlick, E., Phang, J., Skowron, A., Tan, S., Tang, X., Wang, K. A., Winata, G. I., Yvon, F., and Zou, A. Lessons from the trenches on reproducible evaluation of language models, 2024.</p>
<p>Brown, B., Juravsky, J., Ehrlich, R., Clark, R., Le, Q. V., Ré, C., and Mirhoseini, A. Large language monkeys: Scaling
inference compute with repeated sampling, 2024. URL https://arxiv.org/abs/2407.21787.</p>
<p>Cesista, F. L. Multimodal structured generation: Cvpr's 2nd mmfm challenge technical report, 2024. URL https: //arxiv.org/abs/2406.11403.</p>
<p>Chen, W., Yin, M., Ku, M., Lu, P., Wan, Y., Ma, X., Xu, J., Wang, X., and Xia, T. Theoremqa: A theoremdriven question answering dataset, 2023. URL https: //arxiv.org/abs/2305.12524.</p>
<p>Choi, S., Fang, T., Wang, Z., and Song, Y. Kcts: Knowledgeconstrained tree search decoding with token-level hallucination detection, 2023. URL https://arxiv.org/ abs/2310.09044.</p>
<p>DeepSeek-AI, Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., Zhang, X., Yu, X., Wu, Y., Wu, Z. F., Gou, Z., Shao, Z., Li, Z., Gao, Z., Liu, A., Xue, B., Wang, B., Wu, B., Feng, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., Dai, D., Chen, D., Ji, D., Li, E., Lin, F., Dai, F., Luo, F., Hao, G., Chen, G., Li, G., Zhang, H., Bao, H., Xu, H., Wang, H., Ding, H., Xin, H., Gao, H., Qu, H., Li, H., Guo, J., Li, J., Wang, J., Chen, J., Yuan, J., Qiu, J., Li, J., Cai, J. L., Ni, J., Liang, J., Chen, J., Dong, K., Hu, K., Gao, K., Guan, K., Huang, K., Yu, K., Wang, L., Zhang, L., Zhao, L., Wang, L., Zhang, L., Xu, L., Xia, L., Zhang, M., Zhang, M., Tang, M., Li, M., Wang, M., Li, M., Tian, N., Huang, P., Zhang, P., Wang, Q., Chen, Q., Du, Q., Ge, R., Zhang, R., Pan, R., Wang, R., Chen, R. J., Jin, R. L., Chen, R., Lu, S., Zhou, S., Chen, S., Ye, S., Wang, S., Yu, S., Zhou, S., Pan, S., Li, S. S., Zhou, S., Wu, S., Ye, S., Yun, T., Pei, T., Sun, T., Wang, T., Zeng, W., Zhao, W., Liu, W., Liang, W., Gao, W., Yu, W., Zhang, W., Xiao, W. L., An, W., Liu, X., Wang, X., Chen, X., Nie, X., Cheng, X., Liu, X., Xie, X., Liu, X., Yang, X., Li, X., Su, X., Lin, X., Li, X. Q., Jin, X., Shen, X., Chen, X., Sun, X., Wang, X., Song, X., Zhou, X., Wang, X., Shan, X., Li, Y. K., Wang, Y. Q., Wei, Y. X., Zhang, Y., Xu, Y., Li, Y., Zhao, Y., Sun, Y., Wang, Y., Yu, Y., Zhang, Y., Shi, Y., Xiong, Y., He, Y., Piao, Y., Wang, Y., Tan, Y., Ma, Y., Liu, Y., Guo, Y., Ou, Y., Wang, Y., Gong, Y., Zou, Y., He, Y., Xiong, Y., Luo, Y., You, Y., Liu, Y., Zhou, Y., Zhu, Y. X., Xu, Y., Huang, Y., Li, Y., Zheng, Y., Zhu, Y., Ma, Y., Tang, Y., Zha, Y., Yan, Y., Ren, Z. Z., Ren, Z., Sha, Z., Fu, Z., Xu, Z., Xie, Z., Zhang, Z., Hao, Z., Ma, Z., Yan, Z., Wu, Z., Gu, Z., Zhu, Z., Liu, Z., Li, Z., Xie, Z., Song, Z., Pan, Z., Huang, Z., Xu, Z., Zhang, Z., and Zhang, Z. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948.</p>
<p>Diao, S., Wang, P., Lin, Y., Pan, R., Liu, X., and Zhang, T. Active prompting with chain-of-thought for large lan-</p>
<p>guage models, 2024. URL https://arxiv.org/ abs/2302.12246.</p>
<p>Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., Goyal, A., Hartshorn, A., Yang, A., Mitra, A., Sravankumar, A., Korenev, A., Hinsvark, A., Rao, A., Zhang, A., Rodriguez, A., Gregerson, A., et al. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/ 2407.21783 .</p>
<p>Fu, Y., Peng, H., Sabharwal, A., Clark, P., and Khot, T. Complexity-based prompting for multi-step reasoning, 2023. URL https://arxiv.org/abs/2210. 00720 .</p>
<p>Gandhi, K., Lee, D., Grand, G., Liu, M., Cheng, W., Sharma, A., and Goodman, N. D. Stream of search (sos): Learning to search in language, 2024. URL https://arxiv. org/abs/2404.03683.</p>
<p>Gao, B., Song, F., Yang, Z., Cai, Z., Miao, Y., Dong, Q., Li, L., Ma, C., Chen, L., Xu, R., Tang, Z., Wang, B., Zan, D., Quan, S., Zhang, G., Sha, L., Zhang, Y., Ren, X., Liu, T., and Chang, B. Omni-math: A universal olympiad level mathematic benchmark for large language models, 2024a. URL https://arxiv.org/abs/2410.07985.</p>
<p>Gao, L., Tow, J., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L., Hsu, J., McDonell, K., Muennighoff, N., Phang, J., Reynolds, L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A. A framework for few-shot language model evaluation, September 2021. URL https:// doi.org/10.5281/zenodo. 5371628.</p>
<p>Gao, Z., Niu, B., He, X., Xu, H., Liu, H., Liu, A., Hu, X., and Wen, L. Interpretable contrastive monte carlo tree search reasoning, 2024b. URL https://arxiv.org/abs/ 2410.01707 .</p>
<p>Glazer, E., Erdil, E., Besiroglu, T., Chicharro, D., Chen, E., Gunning, A., Olsson, C. F., Denain, J.-S., Ho, A., de Oliveira Santos, E., Järviniemi, O., Barnett, M., Sandler, R., Vrzala, M., Sevilla, J., Ren, Q., Pratt, E., Levine, L., Barkley, G., Stewart, N., Grechuk, B., Grechuk, T., Enugandla, S. V., and Wildon, M. Frontiermath: A benchmark for evaluating advanced mathematical reasoning in ai, 2024. URL https://arxiv.org/abs/2411. 04872 .</p>
<p>Google. Gemini 2.0 flash thinking mode (gemini-2.0-flash-thinking-exp-1219), December 2024. URL https://cloud.google.com/vertex-ai/ generative-ai/docs/thinking-mode.</p>
<p>Groeneveld, D., Beltagy, I., Walsh, P., Bhagia, A., Kinney, R., Tafjord, O., Jha, A. H., Ivison, H., Magnusson, I.,</p>
<p>Wang, Y., Arora, S., Atkinson, D., Authur, R., Chandu, K. R., Cohan, A., Dumas, J., Elazar, Y., Gu, Y., Hessel, J., Khot, T., Merrill, W., Morrison, J., Muennighoff, N., Naik, A., Nam, C., Peters, M. E., Pyatkin, V., Ravichander, A., Schwenk, D., Shah, S., Smith, W., Strubell, E., Subramani, N., Wortsman, M., Dasigi, P., Lambert, N., Richardson, K., Zettlemoyer, L., Dodge, J., Lo, K., Soldaini, L., Smith, N. A., and Hajishirzi, H. Olmo: Accelerating the science of language models, 2024.</p>
<p>He, C., Luo, R., Bai, Y., Hu, S., Thai, Z. L., Shen, J., Hu, J., Han, X., Huang, Y., Zhang, Y., Liu, J., Qi, L., Liu, Z., and Sun, M. Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems, 2024. URL https://arxiv.org/abs/2402.14008.</p>
<p>Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the math dataset, 2021. URL https://arxiv.org/abs/2103.03874.</p>
<p>Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., de Las Casas, D., Hendricks, L. A., Welbl, J., Clark, A., Hennigan, T., Noland, E., Millican, K., van den Driessche, G., Damoc, B., Guy, A., Osindero, S., Simonyan, K., Elsen, E., Rae, J. W., Vinyals, O., and Sifre, L. Training compute-optimal large language models, 2022. URL https://arxiv.org/abs/2203. 15556 .</p>
<p>Hou, Z., Lv, X., Lu, R., Zhang, J., Li, Y., Yao, Z., Li, J., Tang, J., and Dong, Y. Advancing language model reasoning through reinforcement learning and inference scaling, 2025. URL https://arxiv.org/abs/2501. 11651 .</p>
<p>Hu, Y., Shi, W., Fu, X., Roth, D., Ostendorf, M., Zettlemoyer, L., Smith, N. A., and Krishna, R. Visual sketchpad: Sketching as a visual chain of thought for multimodal language models, 2024. URL https://arxiv.org/ abs/2406.09403.</p>
<p>Huang, Z., Wang, Z., Xia, S., Li, X., Zou, H., Xu, R., Fan, R.-Z., Ye, L., Chern, E., Ye, Y., Zhang, Y., Yang, Y., Wu, T., Wang, B., Sun, S., Xiao, Y., Li, Y., Zhou, F., Chern, S., Qin, Y., Ma, Y., Su, J., Liu, Y., Zheng, Y., Zhang, S., Lin, D., Qiao, Y., and Liu, P. Olympicarena: Benchmarking multi-discipline cognitive reasoning for superintelligent ai, 2024a. URL https://arxiv.org/abs/2406. 12753 .</p>
<p>Huang, Z., Zou, H., Li, X., Liu, Y., Zheng, Y., Chern, E., Xia, S., Qin, Y., Yuan, W., and Liu, P. O1 replication journey - part 2: Surpassing ol-preview through simple distillation, big progress or bitter lesson?, 2024b. URL https://arxiv.org/abs/2411.16489.</p>
<p>Huang, Z., Geng, G., Hua, S., Huang, Z., Zou, H., Zhang, S., Liu, P., and Zhang, X. O1 replication journey - part 3: Inference-time scaling for medical reasoning, 2025. URL https://arxiv.org/abs/2501.06458.</p>
<p>Irvine, R., Boubert, D., Raina, V., Liusie, A., Zhu, Z., Mudupalli, V., Korshuk, A., Liu, Z., Cremer, F., Assassi, V., Beauchamp, C.-C., Lu, X., Rialan, T., and Beauchamp, W. Rewarding chatbots for real-world engagement with millions of users, 2023. URL https: //arxiv.org/abs/2303.06135.</p>
<p>Jain, N., Han, K., Gu, A., Li, W.-D., Yan, F., Zhang, T., Wang, S., Solar-Lezama, A., Sen, K., and Stoica, I. Livecodebench: Holistic and contamination free evaluation of large language models for code, 2024. URL https://arxiv.org/abs/2403.07974.</p>
<p>Kapfer, C., Stine, K., Narasimhan, B., Mentzel, C., and Candes, E. Marlowe: Stanford's gpu-based computational instrument, January 2025. URL https://doi.org/ 10.5281 /zenodo. 14751899.</p>
<p>Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language models, 2020. URL https://arxiv.org/abs/2001. 08361 .</p>
<p>Kim, E., Suk, J., Kim, S., Muennighoff, N., Kim, D., and Oh, A. Llm-as-an-interviewer: Beyond static testing through dynamic llm evaluation, 2024. URL https://arxiv. org/abs/2412.10424.</p>
<p>Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J. E., Zhang, H., and Stoica, I. Efficient memory management for large language model serving with pagedattention, 2023. URL https://arxiv. org/abs/2309.06180.</p>
<p>Labs, B. Bespoke-stratos: The unreasonable effectiveness of reasoning distillation, 2025. URL https://hf. co/bespokelabs/Bespoke-Stratos-32B. Accessed: 2025-01-22.</p>
<p>Lee, K.-H., Fischer, I., Wu, Y.-H., Marwood, D., Baluja, S., Schuurmans, D., and Chen, X. Evolving deeper llm thinking, 2025. URL https://arxiv.org/abs/ 2501.09891.</p>
<p>Levi, N. A simple model of inference scaling laws, 2024. URL https://arxiv.org/abs/2410.16377.</p>
<p>LI, J., Beeching, E., Tunstall, L., Lipkin, B., Soletskyi, R., Huang, S. C., Rasul, K., Yu, L., Jiang, A., Shen, Z., Qin, Z., Dong, B., Zhou, L., Fleureau, Y., Lample, G., and Polu, S. Numinamath, 2024. URL https://github.com/project-numina/
aimo-progress-prize/blob/main/report/ numina_dataset.pdf.</p>
<p>Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and Cobbe, K. Let's verify step by step, 2023. URL https : //arxiv.org/abs/2305.20050.</p>
<p>Ling, W., Yogatama, D., Dyer, C., and Blunsom, P. Program induction by rationale generation : Learning to solve and explain algebraic word problems, 2017. URL https : //arxiv.org/abs/1705.04146.</p>
<p>Liu, J., Cui, L., Liu, H., Huang, D., Wang, Y., and Zhang, Y. Logiqa: A challenge dataset for machine reading comprehension with logical reasoning, 2020. URL https : //arxiv.org/abs/2007.08124.</p>
<p>Liu, J., Cohen, A., Pasunuru, R., Choi, Y., Hajishirzi, H., and Celikyilmaz, A. Don't throw away your value model! generating more preferable text with value-guided monte-carlo tree search decoding, 2024. URL https : //arxiv.org/abs/2309.15028.</p>
<p>Loshchilov, I. and Hutter, F. Decoupled weight decay regularization, 2019.</p>
<p>Luo, H., Sun, Q., Xu, C., Zhao, P., Lou, J., Tao, C., Geng, X., Lin, Q., Chen, S., Tang, Y., and Zhang, D. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct, 2025. URL https : //arxiv.org/abs/2308.09583.</p>
<p>Marten, R., Vu, T., Ji, C. C.-J., Sharma, K., Pimpalgaonkar, S., Dimakis, A., and Sathiamoorthy, M. Curator: A tool for synthetic data creation. https://github.com/ bespokelabsai/curator, January 2025.</p>
<p>Muennighoff, N., Soldaini, L., Groeneveld, D., Lo, K., Morrison, J., Min, S., Shi, W., Walsh, P., Tafjord, O., Lambert, N., Gu, Y., Arora, S., Bhagia, A., Schwenk, D., Wadden, D., Wettig, A., Hui, B., Dettmers, T., Kiela, D., Farhadi, A., Smith, N. A., Koh, P. W., Singh, A., and Hajishirzi, H. Olmoe: Open mixture-of-experts language models, 2024. URL https://arxiv.org/abs/2409.02060.
of America, M. A. Aime, February 2024. URL https : //artofproblemsolving.com/wiki/index. php/AIME_Problems_and_Solutions/.</p>
<p>OpenAI. Learning to reason with llms, September 2024. URL https://openai.com/index/ learning-to-reason-with-llms/.</p>
<p>OpenAI. Openai o3-mini, 2025. URL https://openai. com/index/openai-o3-mini/. Accessed: 2025-02-24.</p>
<p>Phan, L., Gatti, A., Han, Z., Li, N., Hu, J., Zhang, H., Shi, S., Choi, M., Agrawal, A., Chopra, A., et al. Humanity's last exam, 2025. URL https://arxiv.org/abs/ 2501.14249.</p>
<p>Qin, Y., Li, X., Zou, H., Liu, Y., Xia, S., Huang, Z., Ye, Y., Yuan, W., Liu, H., Li, Y., and Liu, P. O1 replication journey: A strategic progress report - part 1, 2024. URL https://arxiv.org/abs/2410.18982.</p>
<p>Qwen, :, Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., Lin, H., Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Lin, J., Dang, K., Lu, K., Bao, K., Yang, K., Yu, L., Li, M., Xue, M., Zhang, P., Zhu, Q., Men, R., Lin, R., Li, T., Xia, T., Ren, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Wan, Y., Liu, Y., Cui, Z., Zhang, Z., and Qiu, Z. Qwen2.5 technical report, 2024. URL https://arxiv.org/ abs/2412.15115.</p>
<p>Rein, D., Hou, B. L., Stickland, A. C., Petty, J., Pang, R. Y., Dirani, J., Michael, J., and Bowman, S. R. Gpqa: A graduate-level google-proof q\&amp;a benchmark, 2023. URL https://arxiv.org/abs/2311.12022.</p>
<p>Shi, Q., Tang, M., Narasimhan, K., and Yao, S. Can language models solve olympiad programming?, 2024. URL https://arxiv.org/abs/2404.10952.</p>
<p>Snell, C., Lee, J., Xu, K., and Kumar, A. Scaling llm testtime compute optimally can be more effective than scaling model parameters, 2024. URL https://arxiv. org/abs/2408.03314.</p>
<p>Srivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid, A., Fisch, A., Brown, A. R., Santoro, A., Gupta, A., Garriga-Alonso, A., et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models, 2023.</p>
<p>Su, H., Yen, H., Xia, M., Shi, W., Muennighoff, N., yu Wang, H., Liu, H., Shi, Q., Siegel, Z. S., Tang, M., Sun, R., Yoon, J., Arik, S. O., Chen, D., and Yu, T. Bright: A realistic and challenging benchmark for reasoning-intensive retrieval, 2024. URL https://arxiv.org/abs/ 2407.12883.</p>
<p>Sun, L., Han, Y., Zhao, Z., Ma, D., Shen, Z., Chen, B., Chen, L., and Yu, K. Scieval: A multi-level large language model evaluation benchmark for scientific research, 2024. URL https://arxiv.org/abs/2308.13149.</p>
<p>Team, K., Du, A., Gao, B., Xing, B., Jiang, C., Chen, C., Li, C., Xiao, C., Du, C., Liao, C., Tang, C., Wang, C., Zhang, D., Yuan, E., Lu, E., Tang, F., Sung, F., Wei, G., Lai, G., Guo, H., Zhu, H., Ding, H., Hu, H., Yang, H., Zhang, H., Yao, H., Zhao, H., Lu, H., Li, H., Yu, H., Gao, H., Zheng,
H., Yuan, H., Chen, J., Guo, J., Su, J., Wang, J., Zhao, J., Zhang, J., Liu, J., Yan, J., Wu, J., Shi, L., Ye, L., Yu, L., Dong, M., Zhang, N., Ma, N., Pan, Q., Gong, Q., Liu, S., Ma, S., Wei, S., Cao, S., Huang, S., Jiang, T., Gao, W., Xiong, W., He, W., Huang, W., Wu, W., He, W., Wei, X., Jia, X., Wu, X., Xu, X., Zu, X., Zhou, X., Pan, X., Charles, Y., Li, Y., Hu, Y., Liu, Y., Chen, Y., Wang, Y., Liu, Y., Qin, Y., Liu, Y., Yang, Y., Bao, Y., Du, Y., Wu, Y., Wang, Y., Zhou, Z., Wang, Z., Li, Z., Zhu, Z., Zhang, Z., Wang, Z., Yang, Z., Huang, Z., Huang, Z., Xu, Z., and Yang, Z. Kimi k1.5: Scaling reinforcement learning with llms, 2025. URL https://arxiv.org/abs/2501.12599.</p>
<p>Team, N. Sky-t1: Fully open-source reasoning model with o1-preview performance in \$450 budget, 2025. URL https://novasky-ai.github.io/ posts/sky-t1. Accessed: 2025-01-09.</p>
<p>Team, Q. Qwq: Reflect deeply on the boundaries of the unknown, November 2024. URL https://qwenlm. github.io/blog/qwq-32b-preview/.</p>
<p>Wang, J., Meng, F., Liang, Y., and Zhou, J. Drt-o1: Optimized deep reasoning translation via long chain-ofthought, 2024a. URL https://arxiv.org/abs/ 2412.17498.</p>
<p>Wang, P., Li, L., Shao, Z., Xu, R. X., Dai, D., Li, Y., Chen, D., Wu, Y., and Sui, Z. Math-shepherd: Verify and reinforce llms step-by-step without human annotations, 2024b. URL https://arxiv.org/abs/2312.08935.</p>
<p>Wang, S., Liu, Z., Zhong, W., Zhou, M., Wei, Z., Chen, Z., and Duan, N. From lsat: The progress and challenges of complex reasoning, 2021. URL https://arxiv. org/abs/2108.00648.</p>
<p>Wang, Z., Dong, Y., Delalleau, O., Zeng, J., Shen, G., Egert, D., Zhang, J. J., Sreedhar, M. N., and Kuchaiev, O. Helpsteer2: Open-source dataset for training top-performing reward models, 2024c. URL https://arxiv.org/ abs/2406.08673.</p>
<p>Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q., and Zhou, D. Chain-ofthought prompting elicits reasoning in large language models, 2023. URL https://arxiv.org/abs/ 2201.11903.</p>
<p>Welleck, S., Bertsch, A., Finlayson, M., Schoelkopf, H., Xie, A., Neubig, G., Kulikov, I., and Harchaoui, Z. From decoding to meta-generation: Inference-time algorithms for large language models, 2024. URL https://arxiv. org/abs/2406.16838.</p>
<p>Wu, T., Lan, J., Yuan, W., Jiao, J., Weston, J., and Sukhbaatar, S. Thinking llms: General instruction following with thought generation, 2024a. URL https: //arxiv.org/abs/2410.10630.</p>
<p>Wu, Y., Sun, Z., Li, S., Welleck, S., and Yang, Y. Inference scaling laws: An empirical analysis of computeoptimal inference for problem-solving with language models, 2024b. URL https://arxiv.org/abs/2408. 00724 .</p>
<p>Xiang, V., Snell, C., Gandhi, K., Albalak, A., Singh, A., Blagden, C., Phung, D., Rafailov, R., Lile, N., Mahan, D., Castricato, L., Franken, J.-P., Haber, N., and Finn, C. Towards system 2 reasoning in llms: Learning how to think with meta chain-of-thought, 2025. URL https: //arxiv.org/abs/2501.04682.</p>
<p>Xie, Y., Kawaguchi, K., Zhao, Y., Zhao, X., Kan, M.-Y., He, J., and Xie, Q. Self-evaluation guided beam search for reasoning, 2023. URL https://arxiv.org/abs/ 2305.00633 .</p>
<p>Xin, H., Guo, D., Shao, Z., Ren, Z., Zhu, Q., Liu, B., Ruan, C., Li, W., and Liang, X. Deepseek-prover: Advancing theorem proving in llms through large-scale synthetic data, 2024. URL https://arxiv.org/abs/2405. 14333 .</p>
<p>Xu, H., Wu, X., Wang, W., Li, Z., Zheng, D., Chen, B., Hu, Y., Kang, S., Ji, J., Zhang, Y., Guo, Z., Yang, Y., Zhang, M., and Zhang, D. Redstar: Does scaling long-cot data unlock better slow-reasoning systems?, 2025. URL https://arxiv.org/abs/2501.11284.</p>
<p>Yang, Z., Band, N., Li, S., Candès, E., and Hashimoto, T. Synthetic continued pretraining, 2024. URL https: //arxiv.org/abs/2409.07431.</p>
<p>Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao, Y., and Narasimhan, K. Tree of thoughts: Deliberate problem solving with large language models, 2023a. URL https://arxiv.org/abs/2305.10601.</p>
<p>Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., and Cao, Y. React: Synergizing reasoning and acting in language models, 2023b. URL https://arxiv. org/abs/2210.03629.</p>
<p>Ye, Y., Huang, Z., Xiao, Y., Chern, E., Xia, S., and Liu, P. Limo: Less is more for reasoning, 2025a. URL https: //arxiv.org/abs/2502.03387.</p>
<p>Ye, Y., Xiao, Y., Mi, T., and Liu, P. Aime-preview: A rigorous and immediate evaluation framework for advanced mathematical reasoning. https://github. com/GAIR-NLP/AIME-Preview, 2025b. GitHub repository.</p>
<p>Yu, L., Jiang, W., Shi, H., Yu, J., Liu, Z., Zhang, Y., Kwok, J. T., Li, Z., Weller, A., and Liu, W. Metamath: Bootstrap your own mathematical questions for large language models, 2024. URL https://arxiv.org/abs/2309. 12284 .</p>
<p>Yuan, S., Chen, Z., Xi, Z., Ye, J., Du, Z., and Chen, J. Agent-r: Training language model agents to reflect via iterative self-training, 2025. URL https://arxiv. org/abs/2501.11425.</p>
<p>Zelikman, E., Wu, Y., Mu, J., and Goodman, N. D. Star: Bootstrapping reasoning with reasoning, 2022. URL https://arxiv.org/abs/2203.14465.</p>
<p>Zelikman, E., Harik, G., Shao, Y., Jayasiri, V., Haber, N., and Goodman, N. D. Quiet-star: Language models can teach themselves to think before speaking, 2024. URL https://arxiv.org/abs/2403.09629.</p>
<p>Zhang, H. and Chen, C. Test-time compute scaling laws, 2024. URL https://github.com/hughbzhang/ ol_inference_scaling_laws.</p>
<p>Zhang, S., Chen, Z., Shen, Y., Ding, M., Tenenbaum, J. B., and Gan, C. Planning with large language models for code generation, 2023. URL https://arxiv.org/ abs/2303.05510.</p>
<p>Zhang, Y., Wu, S., Yang, Y., Shu, J., Xiao, J., Kong, C., and Sang, J. ol-coder: an ol replication for coding, 2024a. URL https://arxiv.org/abs/2412.00154.</p>
<p>Zhang, Y., Yang, J., Yuan, Y., and Yao, A. C.-C. Cumulative reasoning with large language models, 2024b. URL https://arxiv.org/abs/2308.04371.</p>
<p>Zhong, H., Xiao, C., Tu, C., Zhang, T., Liu, Z., and Sun, M. Jec-qa: A legal-domain question answering dataset, 2019. URL https://arxiv.org/abs/1911.12011.</p>
<p>Zhong, W., Cui, R., Guo, Y., Liang, Y., Lu, S., Wang, Y., Saied, A., Chen, W., and Duan, N. Agieval: A human-centric benchmark for evaluating foundation models, 2023. URL https://arxiv.org/abs/2304. 06364 .</p>
<p>Zhou, A., Yan, K., Shlapentokh-Rothman, M., Wang, H., and Wang, Y.-X. Language agent tree search unifies reasoning acting and planning in language models, 2024. URL https://arxiv.org/abs/2310.04406.</p>
<p>Zhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., Ma, X., Efrat, A., Yu, P., Yu, L., Zhang, S., Ghosh, G., Lewis, M., Zettlemoyer, L., and Levy, O. Lima: Less is more for alignment, 2023. URL https://arxiv.org/abs/ 2305.11206.</p>
<h1>Contents</h1>
<p>1 Introduction ..... 1
2 Reasoning data curation to create s1K ..... 2
2.1 Initial collection of 59 K samples ..... 2
2.2 Final selection of 1 K samples ..... 2
3 Test-time scaling ..... 3
3.1 Method ..... 3
3.2 Metrics ..... 4
4 Results ..... 4
4.1 Setup ..... 4
4.2 Performance ..... 6
5 Ablations ..... 6
5.1 Data Quantity, Diversity, and Difficulty ..... 6
5.2 Test-time scaling methods ..... 6
6 Discussion and related work ..... 8
6.1 Sample-efficient reasoning ..... 8
6.2 Test-time scaling ..... 9
A s1.1 ..... 16
B Evaluation determinism ..... 16
C s1K details ..... 17
C. 1 s1K summary ..... 17
C. 2 Dataset composition for full 59K questions ..... 17
C. 3 s1K grading prompt ..... 19
C. 4 s1K diversity selection ..... 19
C. 5 Decontamination ..... 19
D Training details ..... 21
D. 1 Training Ablations: Sequence length ..... 21
D. 2 Training Samples ..... 21
E Test-time scaling details ..... 35
E. 1 Sequential scaling ablations ..... 35
E. 2 Examples for rejection sampling ablation ..... 37
F Version Control ..... 46</p>
<h1>A. s1.1</h1>
<p>Seven days after our release of s1, we released s1.1. We regenerated traces for our 1,000 samples in $\mathbf{s 1 K}$ using DeepSeek r1 (DeepSeek-AI et al., 2025) to create s1K-1.1. We use the same training procedure to train our model s1.1. Other updates since our launch include the release of o3 (OpenAI, 2025), LIMO (Ye et al., 2025a), and AIME 2025. We consider all these new developments in Table 5. We find that s1.1 performs significantly better than s1. We also tried distilling from Claude 3.7, which led to worse performance than from r 1 (not reported).</p>
<p>Table 5. s1-32B is an open and sample-efficient reasoning model. We evaluate s1-32B, Qwen, and Gemini (some entries are unknown (N.A.), see §4). Other results are from the respective reports (Qwen et al., 2024; Team, 2024; OpenAI, 2024; 2025; DeepSeek-AI et al., 2025; Labs, 2025; Team, 2025) except for AIME 2025 (Ye et al., 2025b). # ex. = number examples used for reasoning finetuning; $\mathrm{BF}=$ budget forcing.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;"># Examples</th>
<th style="text-align: center;">MATH500</th>
<th style="text-align: center;">GPQA</th>
<th style="text-align: center;">AIME 2024</th>
<th style="text-align: center;">AIME 2025</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">API only</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">o3-mini-low</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">95.8</td>
<td style="text-align: center;">70.6</td>
<td style="text-align: center;">56.3</td>
<td style="text-align: center;">42.1</td>
</tr>
<tr>
<td style="text-align: left;">o3-mini-medium</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">97.3</td>
<td style="text-align: center;">76.8</td>
<td style="text-align: center;">75.8</td>
<td style="text-align: center;">70.4</td>
</tr>
<tr>
<td style="text-align: left;">o3-mini-high</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">97.9</td>
<td style="text-align: center;">$\mathbf{7 9 . 7}$</td>
<td style="text-align: center;">$\mathbf{8 3 . 8}$</td>
<td style="text-align: center;">80.9</td>
</tr>
<tr>
<td style="text-align: left;">Open Weights</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">QwQ-32B</td>
<td style="text-align: center;">N.A.</td>
<td style="text-align: center;">90.6</td>
<td style="text-align: center;">54.5</td>
<td style="text-align: center;">46.7</td>
<td style="text-align: center;">32.7</td>
</tr>
<tr>
<td style="text-align: left;">r1</td>
<td style="text-align: center;">$\gg 800 \mathrm{~K}$</td>
<td style="text-align: center;">07.3</td>
<td style="text-align: center;">$\mathbf{7 1 . 5}$</td>
<td style="text-align: center;">$\mathbf{7 9 . 8}$</td>
<td style="text-align: center;">70.0</td>
</tr>
<tr>
<td style="text-align: left;">r1-distill-Llama-70B</td>
<td style="text-align: center;">800 K</td>
<td style="text-align: center;">94.5</td>
<td style="text-align: center;">65.2</td>
<td style="text-align: center;">57.1</td>
<td style="text-align: center;">56.3</td>
</tr>
<tr>
<td style="text-align: left;">r1-distill-Qwen-14B</td>
<td style="text-align: center;">800 K</td>
<td style="text-align: center;">93.9</td>
<td style="text-align: center;">59.1</td>
<td style="text-align: center;">61.7</td>
<td style="text-align: center;">48.0</td>
</tr>
<tr>
<td style="text-align: left;">r1-distill-Qwen-32B</td>
<td style="text-align: center;">800 K</td>
<td style="text-align: center;">94.3</td>
<td style="text-align: center;">62.1</td>
<td style="text-align: center;">58.3</td>
<td style="text-align: center;">49.6</td>
</tr>
<tr>
<td style="text-align: left;">Open Weights and Open Data</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">LIMO</td>
<td style="text-align: center;">817</td>
<td style="text-align: center;">94.8</td>
<td style="text-align: center;">$\mathbf{6 6 . 7}$</td>
<td style="text-align: center;">56.3</td>
<td style="text-align: center;">44.6</td>
</tr>
<tr>
<td style="text-align: left;">s1 w/o BF</td>
<td style="text-align: center;">1 K</td>
<td style="text-align: center;">92.6</td>
<td style="text-align: center;">56.6</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">26.7</td>
</tr>
<tr>
<td style="text-align: left;">s1 with Budget Forcing "Wait" 1x</td>
<td style="text-align: center;">1 K</td>
<td style="text-align: center;">92.8</td>
<td style="text-align: center;">59.6</td>
<td style="text-align: center;">53.3</td>
<td style="text-align: center;">30.0</td>
</tr>
<tr>
<td style="text-align: left;">s1 with Budget Forcing "Wait" 2x</td>
<td style="text-align: center;">1 K</td>
<td style="text-align: center;">93.0</td>
<td style="text-align: center;">59.6</td>
<td style="text-align: center;">53.3</td>
<td style="text-align: center;">33.3</td>
</tr>
<tr>
<td style="text-align: left;">s1 with Budget Forcing "Wait" 4x</td>
<td style="text-align: center;">1 K</td>
<td style="text-align: center;">92.2</td>
<td style="text-align: center;">58.6</td>
<td style="text-align: center;">$\mathbf{5 6 . 7}$</td>
<td style="text-align: center;">36.7</td>
</tr>
<tr>
<td style="text-align: left;">s1.1 w/o BF</td>
<td style="text-align: center;">1 K</td>
<td style="text-align: center;">94.4</td>
<td style="text-align: center;">60.6</td>
<td style="text-align: center;">$\mathbf{5 6 . 7}$</td>
<td style="text-align: center;">$\mathbf{5 0 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;">s1.1 with Budget Forcing "Wait" 1x</td>
<td style="text-align: center;">1 K</td>
<td style="text-align: center;">$\mathbf{9 5 . 4}$</td>
<td style="text-align: center;">62.6</td>
<td style="text-align: center;">$\mathbf{5 6 . 7}$</td>
<td style="text-align: center;">$\mathbf{5 0 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;">s1.1 with Budget Forcing "Wait" 2x</td>
<td style="text-align: center;">1 K</td>
<td style="text-align: center;">$\mathbf{9 5 . 4}$</td>
<td style="text-align: center;">63.6</td>
<td style="text-align: center;">$\mathbf{5 6 . 7}$</td>
<td style="text-align: center;">$\mathbf{5 0 . 0}$</td>
</tr>
</tbody>
</table>
<h2>B. Evaluation determinism</h2>
<p>We run our evaluations using vLLM (Kwon et al., 2023) as it is faster than the alternatives we tried. However, we find that even when using the same random seeds and greedy sampling, evaluation scores can change significantly across runs:</p>
<ul>
<li>Different batch sizes causing different results see https://github.com/vllm-project/vllm/issues/ 5898</li>
<li>Continuing generations causing different results see https://github.com/vllm-project/vllm/issues/ 11783</li>
<li>Changes in tensor parallelism causing different results</li>
</ul>
<p>As our model generates long reasoning traces prior to its answer, small numeric changes can snowball into large differences. We encounter many generations that are exactly the same for thousands of tokens and then suddenly differ in one token eventually ending up with an entirely different answer. To partly counter this issue we generally run our final evaluations using full precision unless otherwise indicated.</p>
<h1>C. s1K details</h1>
<h2>C.1. s1K summary</h2>
<p>Table 6. Summary of our dataset s1K. Token count measured by the Qwen-2.5 tokenizer. We prompt Claude to produce keywords given several questions from the domain.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Domain</th>
<th style="text-align: left;">#questions</th>
<th style="text-align: left;">Total token count</th>
<th style="text-align: left;">Keywords</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Geometry</td>
<td style="text-align: left;">109</td>
<td style="text-align: left;">560.2 K</td>
<td style="text-align: left;">Area, Triangle, Distance</td>
</tr>
<tr>
<td style="text-align: left;">Number theory</td>
<td style="text-align: left;">98</td>
<td style="text-align: left;">522.5 K</td>
<td style="text-align: left;">Sequences, Divisibility</td>
</tr>
<tr>
<td style="text-align: left;">Combinatorics</td>
<td style="text-align: left;">75</td>
<td style="text-align: left;">384.7 K</td>
<td style="text-align: left;">Permutations, Counting</td>
</tr>
<tr>
<td style="text-align: left;">Real functions</td>
<td style="text-align: left;">43</td>
<td style="text-align: left;">234.8 K</td>
<td style="text-align: left;">Trigonometry, Calculus</td>
</tr>
<tr>
<td style="text-align: left;">Biology</td>
<td style="text-align: left;">41</td>
<td style="text-align: left;">120.9 K</td>
<td style="text-align: left;">Organic reactions</td>
</tr>
<tr>
<td style="text-align: left;">Complex functions</td>
<td style="text-align: left;">32</td>
<td style="text-align: left;">170.2 K</td>
<td style="text-align: left;">Complex roots</td>
</tr>
<tr>
<td style="text-align: left;">Quantum theory</td>
<td style="text-align: left;">32</td>
<td style="text-align: left;">127.9 K</td>
<td style="text-align: left;">Particles, Wave functions</td>
</tr>
<tr>
<td style="text-align: left;">Field theory</td>
<td style="text-align: left;">28</td>
<td style="text-align: left;">150.1 K</td>
<td style="text-align: left;">Polynomials, Roots</td>
</tr>
<tr>
<td style="text-align: left;">Calculus of variations</td>
<td style="text-align: left;">28</td>
<td style="text-align: left;">155.5 K</td>
<td style="text-align: left;">Optimization, Control</td>
</tr>
<tr>
<td style="text-align: left;">Difference equations</td>
<td style="text-align: left;">24</td>
<td style="text-align: left;">132.5 K</td>
<td style="text-align: left;">Recurrence, Recursion</td>
</tr>
<tr>
<td style="text-align: left;">Electromagnetic theory</td>
<td style="text-align: left;">23</td>
<td style="text-align: left;">95.8 K</td>
<td style="text-align: left;">Optics, Waves, Diffraction</td>
</tr>
<tr>
<td style="text-align: left;">Group theory</td>
<td style="text-align: left;">22</td>
<td style="text-align: left;">100.0 K</td>
<td style="text-align: left;">Groups, Automorphisms</td>
</tr>
<tr>
<td style="text-align: left;">Linear algebra</td>
<td style="text-align: left;">22</td>
<td style="text-align: left;">128.3 K</td>
<td style="text-align: left;">Matrices, Determinants</td>
</tr>
<tr>
<td style="text-align: left;">Probability theory</td>
<td style="text-align: left;">20</td>
<td style="text-align: left;">114.6 K</td>
<td style="text-align: left;">Random walk, Expectation</td>
</tr>
<tr>
<td style="text-align: left;">Algebraic systems</td>
<td style="text-align: left;">19</td>
<td style="text-align: left;">109.9 K</td>
<td style="text-align: left;">Functional equations</td>
</tr>
<tr>
<td style="text-align: left;">Mechanics</td>
<td style="text-align: left;">19</td>
<td style="text-align: left;">103.6 K</td>
<td style="text-align: left;">Forces, Motion, Energy</td>
</tr>
<tr>
<td style="text-align: left;">Thermodynamics</td>
<td style="text-align: left;">19</td>
<td style="text-align: left;">74.2 K</td>
<td style="text-align: left;">Heat engines, Entropy</td>
</tr>
<tr>
<td style="text-align: left;">Differential equations</td>
<td style="text-align: left;">18</td>
<td style="text-align: left;">89.6 K</td>
<td style="text-align: left;">Substitution, Existence</td>
</tr>
<tr>
<td style="text-align: left;">Computer science</td>
<td style="text-align: left;">18</td>
<td style="text-align: left;">34.2 K</td>
<td style="text-align: left;">Complexity theory, Algorithms</td>
</tr>
<tr>
<td style="text-align: left;">Numerical analysis</td>
<td style="text-align: left;">18</td>
<td style="text-align: left;">76.5 K</td>
<td style="text-align: left;">Error analysis, Stability</td>
</tr>
<tr>
<td style="text-align: left;">Calculus</td>
<td style="text-align: left;">17</td>
<td style="text-align: left;">96.3 K</td>
<td style="text-align: left;">Convergence, Summation</td>
</tr>
<tr>
<td style="text-align: left;">Algebraic structures</td>
<td style="text-align: left;">17</td>
<td style="text-align: left;">90.4 K</td>
<td style="text-align: left;">Inequalities, Sets</td>
</tr>
<tr>
<td style="text-align: left;">Astronomy</td>
<td style="text-align: left;">16</td>
<td style="text-align: left;">37.7 K</td>
<td style="text-align: left;">Stellar populations, Orbits</td>
</tr>
<tr>
<td style="text-align: left;">Remaining 27 domains</td>
<td style="text-align: left;">242</td>
<td style="text-align: left;">982.2 K</td>
<td style="text-align: left;">Domains with $\leq 16$ questions</td>
</tr>
<tr>
<td style="text-align: left;">All domains (51)</td>
<td style="text-align: left;">1000</td>
<td style="text-align: left;">4.7 M</td>
<td style="text-align: left;">s1K</td>
</tr>
</tbody>
</table>
<h2>C.2. Dataset composition for full 59 K questions</h2>
<p>Table 7. Composition of full 59K questions. Thinking and response lengths are measured in tokens using the Qwen2.5-32B-Instruct tokenizer (Qwen et al., 2024). In addition to excluding our evaluation benchmark, AIME24, we also exclude AIME questions from 2022-2023 as we use these 90 questions during our development stage of s1-32B.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Source</th>
<th style="text-align: center;">Description</th>
<th style="text-align: center;">#Samples</th>
<th style="text-align: center;">Avg. <br> thinking <br> length</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">NuminaMATH (LI et al., 2024)</td>
<td style="text-align: center;">Math problems from online websites</td>
<td style="text-align: center;">30660</td>
<td style="text-align: center;">4.1 K</td>
</tr>
<tr>
<td style="text-align: center;">MATH (Hendrycks et al., 2021)</td>
<td style="text-align: center;">Math problems from competitions</td>
<td style="text-align: center;">11999</td>
<td style="text-align: center;">2.9 K</td>
</tr>
<tr>
<td style="text-align: center;">OlympicArena (Huang et al., 2024a)</td>
<td style="text-align: center;">Astronomy, Biology, Chemistry, Computer Science, Geography, Math, and Physics olympiad questions</td>
<td style="text-align: center;">4250</td>
<td style="text-align: center;">3.2 K</td>
</tr>
<tr>
<td style="text-align: center;">OmniMath (Gao et al., 2024a)</td>
<td style="text-align: center;">Math problems from competitions</td>
<td style="text-align: center;">4238</td>
<td style="text-align: center;">4.4 K</td>
</tr>
<tr>
<td style="text-align: center;">AGIEval (Zhong et al., 2023; <br> Ling et al., 2017; Hendrycks et al., 2021; Liu et al., 2020; Zhong et al., 2019; Wang et al., 2021)</td>
<td style="text-align: center;">English, Law, Logic and Math problems from the SAT, LSAT and other exams</td>
<td style="text-align: center;">2385</td>
<td style="text-align: center;">1.2 K</td>
</tr>
<tr>
<td style="text-align: center;">xword</td>
<td style="text-align: center;">Crossword puzzles</td>
<td style="text-align: center;">999</td>
<td style="text-align: center;">0.7 K</td>
</tr>
<tr>
<td style="text-align: center;">OlympiadBench (He et al., 2024)</td>
<td style="text-align: center;">Math and Physics olympiad questions</td>
<td style="text-align: center;">896</td>
<td style="text-align: center;">3.9 K</td>
</tr>
<tr>
<td style="text-align: center;">AIME (1983-2021)</td>
<td style="text-align: center;">American Invitational Mathematics Examination</td>
<td style="text-align: center;">890</td>
<td style="text-align: center;">4.7 K</td>
</tr>
<tr>
<td style="text-align: center;">TheoremQA (Chen et al., 2023)</td>
<td style="text-align: center;">Computer Science, Finance, Math, and Physics university-level questions relating to theorems</td>
<td style="text-align: center;">747</td>
<td style="text-align: center;">2.1 K</td>
</tr>
<tr>
<td style="text-align: center;">USACO (Shi et al., 2024)</td>
<td style="text-align: center;">Code problems from the USA Computing Olympiad</td>
<td style="text-align: center;">519</td>
<td style="text-align: center;">3.6 K</td>
</tr>
<tr>
<td style="text-align: center;">JEEBench (Arora et al., 2023)</td>
<td style="text-align: center;">Chemistry, Math, and Physics problems used in the university entrance examination of the Indian Institute of Technology</td>
<td style="text-align: center;">515</td>
<td style="text-align: center;">2.9 K</td>
</tr>
<tr>
<td style="text-align: center;">GPQA (Rein et al., 2023)</td>
<td style="text-align: center;">PhD-Level Science Questions</td>
<td style="text-align: center;">348</td>
<td style="text-align: center;">2.9 K</td>
</tr>
<tr>
<td style="text-align: center;">SciEval (Sun et al., 2024)</td>
<td style="text-align: center;">Biology, Chemistry, and Physics problems from various sources</td>
<td style="text-align: center;">227</td>
<td style="text-align: center;">0.7 K</td>
</tr>
<tr>
<td style="text-align: center;">s1-prob</td>
<td style="text-align: center;">Stanford statistics qualifying exams</td>
<td style="text-align: center;">182</td>
<td style="text-align: center;">4.0 K</td>
</tr>
<tr>
<td style="text-align: center;">LiveCodeBench (Jain et al., 2024)</td>
<td style="text-align: center;">Code problems from coding websites (LeetCode, AtCoder, and CodeForces)</td>
<td style="text-align: center;">151</td>
<td style="text-align: center;">3.5 K</td>
</tr>
<tr>
<td style="text-align: center;">s1-teasers</td>
<td style="text-align: center;">Math brain-teasers crawled from the Internet</td>
<td style="text-align: center;">23</td>
<td style="text-align: center;">4.1 K</td>
</tr>
<tr>
<td style="text-align: center;">All 59K questions</td>
<td style="text-align: center;">Composite of the above datasets with reasoning traces and solutions</td>
<td style="text-align: center;">59029</td>
<td style="text-align: center;">3.6 K</td>
</tr>
</tbody>
</table>
<h1>C.3. s1K grading prompt</h1>
<p>To grade whether an example is correct for our dataset selection in $\S 2$, we use the prompt in Figure 8. We grade using Claude 3.5 except for the correctness among the final 1,000 samples, which we graded with Claude 3.7.</p>
<p>You are an AI assistant for grading a science problem. The user will provide you with the question itself, an attempt made by a student and the correct answer to the problem. Your job is to judge whether the attempt is correct by comparing it with the correct answer. If the expected solution concludes with a number or choice, there should be no ambiguity. If the expected solution involves going through the entire reasoning process, you should judge the attempt based on whether the reasoning process is correct with correct answer if helpful.</p>
<p>The user will provide the attempt and the correct answer in the following format:</p>
<div class="codehilite"><pre><span></span><code># Problem
{problem}
## Attempt
{attempt}
## Correct answer
{solution}
</code></pre></div>

<p>Explain your reasoning, and end your response on a new line with only "Yes" or "No" (without quotes).
Figure 8. Grading prompt.</p>
<h2>C.4. s1K diversity selection</h2>
<p>Algorithm 1 provides our algorithm for selecting data in our diversity selection stage. As mentioned in $\S 2$, we also include samples from some specific benchmarks we perceive as high-quality. None of the samples overlap with our final evaluation.</p>
<h2>C.5. Decontamination</h2>
<p>We filter all samples by checking for an 8 -gram overlap between the selected examples and the evaluation benchmarks: MATH500, GPTQA Diamond, and AIME24. We exclude questions with more than an 8-gram overlap.</p>
<div class="codehilite"><pre><span></span><code><span class="n">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="n">Two</span><span class="o">-</span><span class="n">stage</span><span class="w"> </span><span class="n">sampling</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">s1K</span>
<span class="w">    </span><span class="n">Input</span><span class="o">:</span><span class="w"> </span><span class="p">\(</span><span class="n">Q</span><span class="o">:=</span><span class="p">\)</span><span class="w"> </span><span class="n">Set</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="mi">24</span><span class="o">,</span><span class="mi">496</span><span class="w"> </span><span class="n">questions</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">features</span>
<span class="w">    </span><span class="n">Output</span><span class="o">:</span><span class="w"> </span><span class="p">\(</span><span class="n">S</span><span class="o">:=</span><span class="p">\)</span><span class="w"> </span><span class="n">Set</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="mi">1</span><span class="o">,</span><span class="mi">000</span><span class="w"> </span><span class="n">selected</span><span class="w"> </span><span class="n">questions</span>
<span class="w">    </span><span class="p">\(</span><span class="n">S</span><span class="w"> </span><span class="p">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="p">\</span><span class="n">emptyset</span><span class="w"> </span><span class="p">\</span><span class="n">quad</span><span class="w"> </span><span class="p">\</span><span class="n">quad</span><span class="w"> </span><span class="p">\</span><span class="n">quad</span><span class="w"> </span><span class="p">\</span><span class="n">quad</span><span class="w"> </span><span class="p">\</span><span class="n">quad</span><span class="w"> </span><span class="p">\</span><span class="n">quad</span><span class="w"> </span><span class="p">\</span><span class="n">quad</span><span class="w"> </span><span class="p">\</span><span class="n">quad</span><span class="w"> </span><span class="p">\</span><span class="n">quad</span><span class="w"> </span><span class="p">\</span><span class="n">quad</span><span class="w"> </span><span class="p">\</span><span class="n">quad</span><span class="p">\)</span><span class="w"> </span><span class="n">Initialize</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">output</span><span class="w"> </span><span class="n">set</span><span class="w"> </span><span class="p">(</span><span class="n">only</span><span class="w"> </span><span class="n">tracks</span><span class="w"> </span><span class="n">unique</span><span class="w"> </span><span class="n">elements</span><span class="p">)</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">\(</span><span class="n">q</span><span class="w"> </span><span class="p">\</span><span class="k">in</span><span class="w"> </span><span class="n">Q</span><span class="p">\)</span><span class="w"> </span><span class="k">do</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">IsGeminiCorrect</span><span class="w"> </span><span class="p">\((</span><span class="n">q</span><span class="p">)\)</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="p">(</span><span class="n">IsAIME</span><span class="w"> </span><span class="p">\((</span><span class="n">q</span><span class="p">)\)</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="n">IsGPQA</span><span class="w"> </span><span class="p">\((</span><span class="n">q</span><span class="p">))\)</span><span class="w"> </span><span class="k">then</span>
<span class="w">            </span><span class="p">\(</span><span class="n">S</span><span class="w"> </span><span class="p">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="n">S</span><span class="w"> </span><span class="p">\</span><span class="n">cup</span><span class="p">\{</span><span class="n">q</span><span class="p">\}\)</span>
<span class="w">            </span><span class="p">\(\</span><span class="n">quad</span><span class="p">\)</span><span class="w"> </span><span class="n">Select</span><span class="w"> </span><span class="n">all</span><span class="w"> </span><span class="n">correct</span><span class="w"> </span><span class="n">AIME</span><span class="o">/</span><span class="n">GPQA</span><span class="w"> </span><span class="n">solutions</span>
<span class="w">        </span><span class="k">else</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">IsGeminiCorrect</span><span class="w"> </span><span class="p">\((</span><span class="n">q</span><span class="p">)\)</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="p">\(\</span><span class="n">operatorname</span><span class="p">{</span><span class="n">IsMATH</span><span class="p">}(</span><span class="n">q</span><span class="p">)\)</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="n">ThinkingLength</span><span class="w"> </span><span class="p">\((</span><span class="n">q</span><span class="p">)</span><span class="o">&gt;</span><span class="mi">5600</span><span class="p">\)</span><span class="w"> </span><span class="k">then</span>
<span class="w">            </span><span class="p">\(</span><span class="n">S</span><span class="w"> </span><span class="p">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="n">S</span><span class="w"> </span><span class="p">\</span><span class="n">cup</span><span class="p">\{</span><span class="n">q</span><span class="p">\}\)</span>
<span class="w">            </span><span class="p">\(\</span><span class="n">quad</span><span class="p">\)</span><span class="w"> </span><span class="n">Select</span><span class="w"> </span><span class="n">correct</span><span class="w"> </span><span class="n">MATH500</span><span class="w"> </span><span class="n">solutions</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">long</span><span class="w"> </span><span class="n">chains</span>
<span class="w">        </span><span class="k">end</span><span class="w"> </span><span class="k">if</span>
<span class="w">    </span><span class="k">end</span><span class="w"> </span><span class="k">for</span>
<span class="w">    </span><span class="p">\(</span><span class="n">D</span><span class="w"> </span><span class="p">\</span><span class="n">leftarrow</span><span class="p">\)</span><span class="w"> </span><span class="n">All</span><span class="w"> </span><span class="n">available</span><span class="w"> </span><span class="n">domains</span>
<span class="w">    </span><span class="n">Initialize</span><span class="w"> </span><span class="n">domain</span><span class="w"> </span><span class="n">pool</span>
<span class="w">    </span><span class="k">while</span><span class="w"> </span><span class="p">\(|</span><span class="n">S</span><span class="p">|</span><span class="o">&lt;</span><span class="mi">1000</span><span class="p">\)</span><span class="w"> </span><span class="k">do</span>
<span class="w">        </span><span class="p">\(</span><span class="n">d</span><span class="w"> </span><span class="p">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="p">\</span><span class="n">operatorname</span><span class="p">{</span><span class="n">RandomChoice</span><span class="p">}(</span><span class="n">D</span><span class="p">)\)</span>
<span class="w">            </span><span class="n">Randomly</span><span class="w"> </span><span class="nb">select</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">domain</span>
<span class="w">            </span><span class="p">\(</span><span class="n">Q_</span><span class="p">{</span><span class="n">d</span><span class="p">}</span><span class="w"> </span><span class="p">\</span><span class="n">leftarrow</span><span class="p">\)</span><span class="w"> </span><span class="n">Questions</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">domain</span><span class="w"> </span><span class="p">\(</span><span class="n">d</span><span class="p">\)</span>
<span class="w">            </span><span class="n">Get</span><span class="w"> </span><span class="n">questions</span><span class="w"> </span><span class="k">from</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">domain</span>
<span class="w">            </span><span class="n">ranks</span><span class="w"> </span><span class="p">\(\</span><span class="n">leftarrow</span><span class="p">\)</span><span class="w"> </span><span class="n">RankByThinkingLength</span><span class="w"> </span><span class="p">\(\</span><span class="n">left</span><span class="p">(</span><span class="n">Q_</span><span class="p">{</span><span class="n">d</span><span class="p">}\</span><span class="n">right</span><span class="p">)\)</span>
<span class="w">            </span><span class="n">Rank</span><span class="w"> </span><span class="k">by</span><span class="w"> </span><span class="n">thinking</span><span class="w"> </span><span class="nb">length</span>
<span class="w">            </span><span class="n">weights</span><span class="w"> </span><span class="p">\(\</span><span class="n">leftarrow</span><span class="w"> </span><span class="mi">2</span><span class="o">^</span><span class="p">{</span><span class="o">-</span><span class="p">\</span><span class="n">text</span><span class="w"> </span><span class="p">{</span><span class="n">ranks</span><span class="w"> </span><span class="p">}}\)</span>
<span class="w">            </span><span class="p">\(</span><span class="n">q</span><span class="w"> </span><span class="p">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="p">\</span><span class="n">operatorname</span><span class="p">{</span><span class="n">WeightedSample</span><span class="p">}\</span><span class="n">left</span><span class="p">(</span><span class="n">Q_</span><span class="p">{</span><span class="n">d</span><span class="p">}\</span><span class="n">right</span><span class="o">.</span><span class="p">\)</span><span class="o">,</span><span class="w"> </span><span class="n">weights</span><span class="w"> </span><span class="p">\()\)</span>
<span class="w">            </span><span class="n">Sample</span><span class="w"> </span><span class="n">favoring</span><span class="w"> </span><span class="n">longer</span><span class="w"> </span><span class="n">chains</span>
<span class="w">            </span><span class="p">\(</span><span class="n">S</span><span class="w"> </span><span class="p">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="n">S</span><span class="w"> </span><span class="p">\</span><span class="n">cup</span><span class="p">\{</span><span class="n">q</span><span class="p">\}\)</span>
<span class="w">            </span><span class="n">Add</span><span class="w"> </span><span class="n">selected</span><span class="w"> </span><span class="n">question</span>
<span class="w">            </span><span class="p">\(</span><span class="n">Q_</span><span class="p">{</span><span class="n">d</span><span class="p">}</span><span class="w"> </span><span class="p">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="n">Q_</span><span class="p">{</span><span class="n">d</span><span class="p">}</span><span class="w"> </span><span class="p">\</span><span class="n">backslash</span><span class="p">\{</span><span class="n">q</span><span class="p">\}\)</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="p">\(</span><span class="n">Q_</span><span class="p">{</span><span class="n">d</span><span class="p">}</span><span class="o">=</span><span class="p">\</span><span class="n">emptyset</span><span class="p">\)</span><span class="w"> </span><span class="k">then</span>
<span class="w">                </span><span class="p">\(</span><span class="n">D</span><span class="w"> </span><span class="p">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="n">D</span><span class="w"> </span><span class="p">\</span><span class="n">backslash</span><span class="p">\{</span><span class="n">d</span><span class="p">\}\)</span>
<span class="w">            </span><span class="n">Remove</span><span class="w"> </span><span class="n">exhausted</span><span class="w"> </span><span class="n">domains</span>
<span class="w">        </span><span class="k">end</span><span class="w"> </span><span class="k">if</span>
<span class="w">    </span><span class="k">end</span><span class="w"> </span><span class="k">while</span>
</code></pre></div>

<h1>D. Training details</h1>
<p>We take a model that has already been pretrained and instruction tuned and further finetune it for reasoning. Specifically, we use Qwen2.5-32B-Instruct (Qwen et al., 2024), which on math tasks generally matches or outperforms the larger Qwen2.5-72B-Instruct (Qwen et al., 2024) or other open models (Dubey et al., 2024; Groeneveld et al., 2024; Muennighoff et al., 2024). We use token delimiters to separate the thinking stage from the answering stage. We enclose the thinking stage with &lt;|im_start|&gt;think and &lt;|im_start|&gt;answer; both preceded and followed by a newline. Samples from our dataset are in $\S$ D.2. We use basic fine-tuning hyperparameters: we train for 5 epochs with a batch size of 16 for a total of 315 gradient steps. We train in bfloat16 precision with a learning rate of $1 e-5$ warmed up linearly for $5 \%$ ( 16 steps) and then decayed to 0 over the rest of training ( 299 steps) following a cosine schedule. We use the AdamW optimizer (Loshchilov \&amp; Hutter, 2019) with $\beta_{1}=0.9, \beta_{2}=0.95$ and weight decay of $1 e-4$. We do not compute loss on questions, only on reasoning traces and solutions. We ensure the sequence length is large enough to avoid cutting off any samples; a setting we ablate in §D.1. The training takes just 26 minutes on 16 NVIDIA H100 GPUs.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 9. Training dynamics of s1-32B on s1K.</p>
<h2>D.1. Training Ablations: Sequence length</h2>
<p>Table 8. Training sequence length ablation. We report "accuracy / average thinking tokens per sample"; the higher the accuracy and the fewer the thinking tokens (inference cost) the better.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Model A</th>
<th style="text-align: left;">Model B</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Training sequence length</td>
<td style="text-align: left;">4096</td>
<td style="text-align: left;">32768</td>
</tr>
<tr>
<td style="text-align: left;">\% training samples cutoff</td>
<td style="text-align: left;">$74 \%$</td>
<td style="text-align: left;">$0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">AIME24</td>
<td style="text-align: left;">$30.0 \% / 20721$</td>
<td style="text-align: left;">$50.0 \% / 6984$</td>
</tr>
<tr>
<td style="text-align: left;">MATH500</td>
<td style="text-align: left;">$90.0 \% / 5324$</td>
<td style="text-align: left;">$91.0 \% / 3268$</td>
</tr>
<tr>
<td style="text-align: left;">GPQA</td>
<td style="text-align: left;">$52.5 \% / 6841$</td>
<td style="text-align: left;">$53.0 \% / 3568$</td>
</tr>
</tbody>
</table>
<p>Besides our scaling ablations in $\S 5.2$, the main training hyperparameter we ablate is the sequence length used during training. We find that a shorter training sequence length leads to longer reasoning traces at test time. This is because when training with a shorter sequence length the answer section of the training sample is more commonly cut off. Inversely, when the training sequence length is longer, more samples appear in their entirety with the section where the model answers. Thus the model receives more gradient updates where it learns to generate an answer following its chain. This in turn leads to a higher log probability of the answer section at any point during the generation and thus shorter reasoning traces at test time. Performance-wise, we also find that the model trained with a longer sequence length performs better. Thus we opt for the longest training sequence length as it leads to better performance and makes inference more efficient by leading to shorter reasoning traces.</p>
<h2>D.2. Training Samples</h2>
<p>Table 9, Table 10, Table 11 contain training samples from $\mathbf{s 1 K}$.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ https://github.com/google/
generative-ai-docs/issues/257&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{2}$ https://github.com/google/ generative-ai-docs/issues/257&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
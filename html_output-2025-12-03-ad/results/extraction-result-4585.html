<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4585 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4585</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4585</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-100.html">extraction-schema-100</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-10828be2eaa52ba7fd78356980afd0669e2f2879</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/10828be2eaa52ba7fd78356980afd0669e2f2879" target="_blank">Large Language Model (LLM) as a System of Multiple Expert Agents: An Approach to solve the Abstraction and Reasoning Corpus (ARC) Challenge</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work firstly converts the input image into multiple suitable text-based abstraction spaces, then utilises the associative power of LLMs to derive the input-output relationship and map this to actions in the form of a working program, similar to Voyager / Ghost in the MineCraft.</p>
                <p><strong>Paper Abstract:</strong> We attempt to solve the Abstraction and Reasoning Corpus (ARC) Challenge using Large Language Models (LLMs) as a system of multiple expert agents. Using the flexibility of LLMs to be prompted to do various novel tasks using zero-shot, few-shot, context-grounded prompting, we explore the feasibility of using LLMs to solve the ARC Challenge. We firstly convert the input image into multiple suitable text-based abstraction spaces. We then utilise the associative power of LLMs to derive the input-output relationship and map this to actions in the form of a working program, similar to Voyager / Ghost in the MineCraft. In addition, we use iterative environmental feedback in order to guide LLMs to solve the task. Our proposed approach achieves 50 solves out of 111 training set problems (45%) with just three abstraction spaces - grid, object and pixel - and we believe that with more abstraction spaces and learnable actions, we will be able to solve more.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4585.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4585.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that augments a generative LLM with a retrieval component that fetches relevant documents or passages to ground generation and reduce hallucination; commonly used for knowledge-intensive QA and multi-document grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Retrieval-augmented generation for knowledgeintensive nlp tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>RAG pairs a retriever (sparse or dense index) that returns top-k relevant passages from a corpus with a generator (an LLM) that conditions on the retrieved text when producing answers. Key components: document index, retriever (BM25 / dense vectors), optional reranker, and a generative LLM that either encodes retrieved passages into its prompt or performs fusion-in-decoder style conditioning. The architecture allows grounding LLM outputs in external sources and supports multi-document aggregation by returning and conditioning on multiple retrieved passages.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>model-agnostic (commonly paired with seq2seq/generation models; in the context of this paper the authors reference using retrieval-like memory with LLMs such as GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>retrieval + prompt-conditioned generation (retriever returns passages; generator extracts/summarizes relevant facts conditioned on retrieved text)</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>grounded generation that consolidates information from multiple retrieved passages; can be used iteratively (retrieve → generate → retrieve again) to synthesize across documents</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>knowledge-intensive NLP tasks / general documents (paper mentions RAG as analogous to memory/bank of past solved problems)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>grounded answers, summaries or consolidated facts (i.e., extractive/abstractive summaries grounded in retrieved passages)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>pure generator without retrieval (seq2seq models that do not use external retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The paper cites RAG as an analogy for using a memory bank of previously solved problems to ground an LLM; RAG-style retrieval can provide relevant prior context to improve grounding and reduce hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Depends on retrieval quality and index coverage; limited by context/window size and potential to retrieve contradictory or irrelevant passages; requires careful prompt engineering to integrate retrieved content.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Performance typically improves with larger, higher-quality retrieval corpora and better retrievers; not explicitly quantified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model (LLM) as a System of Multiple Expert Agents: An Approach to solve the Abstraction and Reasoning Corpus (ARC) Challenge', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4585.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4585.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that elicits intermediate reasoning steps from LLMs by conditioning them to produce stepwise rationale before the final answer, improving multi-step reasoning and program planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-thought prompting elicits reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Chain-of-Thought (CoT) JSON prompting</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>CoT prompting structures the LLM's output into intermediate reasoning steps (the 'chain of thought') which the model produces explicitly; in this paper the authors implement CoT via an enforced JSON output schema (fields: reflection, pixel_changes, object_changes, helper_functions, overall_pattern, program_instructions, python_program). This architecture makes the LLM's internal decomposition explicit and conditions subsequent output fields on earlier reflections, aiding program synthesis and multi-step problem solving.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-4 (explicitly used in the paper for CoT-style JSON prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>structured prompting with an enforced schema (JSON) that forces the model to extract observations and intermediate facts before producing synthesis/code</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>multi-step conditioning: earlier CoT fields (observations, helper functions, pattern) are used to guide later synthesis (program instructions and program code), supporting systematic combination of facts across inputs</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>applied to ARC tasks in this paper; general reasoning and multi-step synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>structured chain-of-thought entries and synthesized artifacts (programs, transformation rules); could be adapted to structured summaries from multiple documents</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Used as part of the pipeline that achieved 50/111 solved ARC tasks (45%); the paper does not isolate CoT-only effect quantitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>free-form prompting (no structured CoT) or single-step program generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Structuring CoT output in a machine-readable JSON sequence helps condition generation and reduces ambiguity, enabling better program-planning and synthesis in the ARC domain.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>CoT outputs can be verbose and consume prompt/window budget; may still contain incorrect reasoning steps that propagate; requires prompt engineering to avoid format violations.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not explicitly discussed for multi-paper scaling in this paper; CoT increases context usage and thus is affected by model context window limits.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model (LLM) as a System of Multiple Expert Agents: An Approach to solve the Abstraction and Reasoning Corpus (ARC) Challenge', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4585.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4585.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MassSampling+Filtering</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mass sampling and filtering (AlphaCode-style)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach of generating a large number of candidate programs via stochastic sampling from an LLM and filtering them by executing on provided examples/tests to select correct solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Competition-level code generation with alphacode</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Mass Sampling and Filtering (AlphaCode-inspired)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The pipeline uses an LLM (here GPT-4) to sample many candidate programs (mass sampling) under diverse prompts/temperatures, then executes or tests these candidates against the task demonstration examples (filtering) to keep only those that match the demonstrations; a subset of passing candidates are then run on the test input. Components: sampler (LLM with temperature), executor/test harness, candidate filter/selector.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-4 for program sampling in this paper; AlphaCode used specialized models in its original work</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>not document extraction per se; generation followed by execution-based filtering to extract programs that satisfy examples</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>synthesis by enumeration and selection — combine many sampled candidate programs then select via behavioral tests</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>program synthesis for ARC tasks / code generation</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>executable programs (Python functions) that transform input grids</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>task solve count (number of ARC tasks solved), pass/fail on demonstration and test cases</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Contributed to achieving 50/111 solved ARC tasks for the subset of problems fitting context constraints; paper reports using mass sampling + filtering in the pipeline but does not give an ablation isolating its exact contribution numerically.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>single-shot program generation without sampling and execution filtering</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not quantified precisely in the paper, but authors state mass sampling + filtering improves chance of finding a working program.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Generating many diverse candidates and filtering by execution on demonstrations improves the likelihood of obtaining a correct program; useful when LLM-generated code is brittle.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>High computational and API cost due to many samples and program executions; requires reliable test harness and can still fail when demonstrations do not fully specify the transformation.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Cost grows with number of samples and program complexity; success probability increases with more samples but with diminishing returns and higher compute/API cost.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model (LLM) as a System of Multiple Expert Agents: An Approach to solve the Abstraction and Reasoning Corpus (ARC) Challenge', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4585.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4585.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Voyager</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Voyager: An open-ended embodied agent with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An embodied lifelong-learning agent that leverages LLMs to propose, implement and store 'skills' (code functions) in a growing skill library and uses iterative interaction with an environment for continual learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Voyager: An openended embodied agent with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Voyager (LLM-driven skill-learning agent)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Voyager uses an LLM to propose and synthesize code-based skills for an embodied environment (Minecraft), maintains a skill library of functions, and iteratively improves behavior via environment interaction and feedback. Key components: LLM-based planner/programmer, skill library (functions), environment executor, iterative prompting loop that uses observed outcomes to refine skills.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>LLMs (paper references Voyager work; specific model details are from the Voyager publication)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>not targeted at papers; uses environment-observed traces and skill extraction from interaction logs</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>incremental skill synthesis and reuse (compose learned functions to form higher-level behaviors)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>embodied agents / open-world environments (Minecraft in Voyager)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>environmental behaviors/skills (code functions) rather than document-level theories</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Authors of this paper cite Voyager as an inspiration for (a) maintaining a library of learned primitives/skills and (b) using iterative environment feedback to refine LLM outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Voyager-style continual learning relies on environment feedback and suitable primitives; transferring that idea to multi-document scientific synthesis requires designing retrieval and verification loops.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Voyager shows benefit from continual accumulation of skills; scaling to large corpora would require retrieval/indexing and verification mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model (LLM) as a System of Multiple Expert Agents: An Approach to solve the Abstraction and Reasoning Corpus (ARC) Challenge', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4585.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4585.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GhostMC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ghost in the Minecraft (Generally capable agents via LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-driven agent that constraints its actions to a predefined list of functions and learns via memory and iterative interaction with the environment to produce capable open-world behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Ghost in the minecraft: Generally capable agents for open-world enviroments via large language models with text-based knowledge and memory</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Ghost in the Minecraft (LLM agent with constrained action space)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>This system runs an LLM as a controller over a constrained action/function space, using text-based memory and iterative environment feedback; components include function/action library, LLM planner, memory store, and environment executor. The paper references Ghost as demonstrating the value of constrained functional action spaces and environment feedback for LLM-driven problem solving.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>LLMs (as used in the referenced Ghost work; specific models described in that work)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>not designed for document extraction; extracts plans/skill sequences from LLM outputs to execute in environment</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>iterative refinement of plans using memory and environment outcomes</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>embodied open-world environments</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>action plans / skill invocations (functions) rather than literature syntheses</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as evidence that iterative environment feedback and a constrained function/action space improves LLM reliability; motivates the paper's use of primitive/helper functions and feedback loops.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Method depends on appropriate action abstractions and reliable environment observation; mapping this to multi-paper synthesis would require verification/validation mechanisms for provenance and consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed for document-scale synthesis in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model (LLM) as a System of Multiple Expert Agents: An Approach to solve the Abstraction and Reasoning Corpus (ARC) Challenge', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4585.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4585.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenAI Functions</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI function-calling API (function calling)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A mechanism that enables LLMs to call external, developer-defined functions (APIs) in a structured way so the model can delegate precise operations to tools and receive structured outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Function-calling (OpenAI Functions style)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Function-calling augments LLMs with a mechanism to invoke deterministic external functions (e.g., database lookups, retrieval, program execution) by returning a structured function call rather than free-form text; components: schema-declared functions, LLM deciding when/how to call, and function executor returning structured results to the model. The paper analogizes its primitive/helper function design to this paradigm, using function descriptions to ground GPT-4 in semantic operations.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-4 (the paper uses GPT-4 and references the OpenAI function-calling pattern)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>tool-mediated extraction: LLM triggers functions (e.g., get_objects, crop_grid) to obtain structured facts rather than relying on pure free-form extraction</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>compose function outputs and LLM reasoning to synthesize final outputs (programs or grounded summaries)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>applied to ARC tasks via primitive/helper functions; general tool use for many domains including document processing</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>structured function calls and their outputs, enabling deterministic steps of extraction or computation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The authors find that grounding LLMs via explicitly described primitive/helper functions (function-call-like) improves program generation reliability compared to pure text-only descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Requires engineering of appropriate functions and careful naming/descriptions; too many available functions or irrelevant functions can harm performance.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Function-based grounding scales with the richness of the provided function library but requires curation or automated growth mechanisms to remain tractable.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model (LLM) as a System of Multiple Expert Agents: An Approach to solve the Abstraction and Reasoning Corpus (ARC) Challenge', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4585.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4585.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Visual ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Visual ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system that connects LLMs with visual foundation models and external tools to allow multimodal interactions (image understanding, generation, editing) by orchestrating specialized vision models via an LLM controller.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Visual chatgpt: Talking, drawing and editing with visual foundation models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Visual ChatGPT (LLM orchestrating vision tools)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Visual ChatGPT uses an LLM to orchestrate a collection of specialized visual models (e.g., detectors, generators, editors) by routing intermediate outputs between tools and the LLM; components: LLM planner, library of vision tools, adapters to convert between modalities, and a control protocol. The ARC paper cites Visual ChatGPT as related work for the style of prompting functions and tool-backed processing.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>LLM controller (the referenced work uses GPT-family models as controllers in examples; the ARC paper uses GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>tool orchestration to extract structured visual features via specialized models (analogous to using helper functions to extract object views)</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>LLM-mediated composition of tool outputs into final answers or artifacts</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>multimodal vision + language tasks (image editing, captioning); cited as analogous to using helper functions for grounding</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>multimodal edits/results and structured descriptions</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as an example where an LLM orchestrates specialized tools; motivates the paper's use of helper functions to ground GPT-4 for ARC tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Tool orchestration adds integration complexity and can be brittle if adapters or tool outputs are noisy.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Scales with the number and quality of integrated tools, but complexity of orchestration grows accordingly.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model (LLM) as a System of Multiple Expert Agents: An Approach to solve the Abstraction and Reasoning Corpus (ARC) Challenge', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4585.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4585.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLMs-as-pattern-machines</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large language models as general pattern machines (Mirchandani et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Work showing that LLMs can be used as general pattern predictors by converting tasks (e.g., grids) into token sequences and using next-token prediction to solve pattern tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models as general pattern machines</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLMs as general pattern machines (ASCII-grid tokenization)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>This approach represents structured data (e.g., grid tasks) as linear token sequences (ASCII/text) and uses an LLM's next-token prediction capability to learn and generate outputs for pattern tasks. The ARC paper references this line of work that converted grids to ASCII and solved many ARC tasks using next-token prediction with LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>various LLMs (Mirchandani et al. used large LLMs; the ARC paper uses GPT-4 for its pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>tokenization of structured inputs into text and direct next-token / sequence prediction to extract patterns</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>direct sequence generation conditioned on training examples; no explicit retrieval across documents in the referenced work</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>pattern recognition tasks (ARC grids) rather than scientific papers</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>predicted token sequences representing outputs (grids)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Referenced result: Mirchandani et al. solved 85 out of 800 ARC tasks by treating grids as ASCII next-token prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Textualizing structured tasks and using LLM sequence prediction can be surprisingly effective for pattern tasks; motivates using multiple textual abstraction spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Representation sensitivity (offsets, formatting) and limited to tasks that can be suitably linearized; not directly a multi-document synthesis method.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Performance depends on model size and amount of in-context examples; Mirchandani et al. showed improvements when using sufficiently large LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model (LLM) as a System of Multiple Expert Agents: An Approach to solve the Abstraction and Reasoning Corpus (ARC) Challenge', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Retrieval-augmented generation for knowledgeintensive nlp tasks <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Competition-level code generation with alphacode <em>(Rating: 2)</em></li>
                <li>Voyager: An openended embodied agent with large language models <em>(Rating: 1)</em></li>
                <li>Ghost in the minecraft: Generally capable agents for open-world enviroments via large language models with text-based knowledge and memory <em>(Rating: 1)</em></li>
                <li>Visual chatgpt: Talking, drawing and editing with visual foundation models <em>(Rating: 1)</em></li>
                <li>Large language models as general pattern machines <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4585",
    "paper_id": "paper-10828be2eaa52ba7fd78356980afd0669e2f2879",
    "extraction_schema_id": "extraction-schema-100",
    "extracted_data": [
        {
            "name_short": "RAG",
            "name_full": "Retrieval-Augmented Generation",
            "brief_description": "A framework that augments a generative LLM with a retrieval component that fetches relevant documents or passages to ground generation and reduce hallucination; commonly used for knowledge-intensive QA and multi-document grounding.",
            "citation_title": "Retrieval-augmented generation for knowledgeintensive nlp tasks",
            "mention_or_use": "mention",
            "system_name": "Retrieval-Augmented Generation (RAG)",
            "system_description": "RAG pairs a retriever (sparse or dense index) that returns top-k relevant passages from a corpus with a generator (an LLM) that conditions on the retrieved text when producing answers. Key components: document index, retriever (BM25 / dense vectors), optional reranker, and a generative LLM that either encodes retrieved passages into its prompt or performs fusion-in-decoder style conditioning. The architecture allows grounding LLM outputs in external sources and supports multi-document aggregation by returning and conditioning on multiple retrieved passages.",
            "llm_model_used": "model-agnostic (commonly paired with seq2seq/generation models; in the context of this paper the authors reference using retrieval-like memory with LLMs such as GPT-4)",
            "extraction_technique": "retrieval + prompt-conditioned generation (retriever returns passages; generator extracts/summarizes relevant facts conditioned on retrieved text)",
            "synthesis_technique": "grounded generation that consolidates information from multiple retrieved passages; can be used iteratively (retrieve → generate → retrieve again) to synthesize across documents",
            "number_of_papers": null,
            "domain_or_topic": "knowledge-intensive NLP tasks / general documents (paper mentions RAG as analogous to memory/bank of past solved problems)",
            "output_type": "grounded answers, summaries or consolidated facts (i.e., extractive/abstractive summaries grounded in retrieved passages)",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": "pure generator without retrieval (seq2seq models that do not use external retrieval)",
            "performance_vs_baseline": null,
            "key_findings": "The paper cites RAG as an analogy for using a memory bank of previously solved problems to ground an LLM; RAG-style retrieval can provide relevant prior context to improve grounding and reduce hallucination.",
            "limitations_challenges": "Depends on retrieval quality and index coverage; limited by context/window size and potential to retrieve contradictory or irrelevant passages; requires careful prompt engineering to integrate retrieved content.",
            "scaling_behavior": "Performance typically improves with larger, higher-quality retrieval corpora and better retrievers; not explicitly quantified in this paper.",
            "uuid": "e4585.0",
            "source_info": {
                "paper_title": "Large Language Model (LLM) as a System of Multiple Expert Agents: An Approach to solve the Abstraction and Reasoning Corpus (ARC) Challenge",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "CoT",
            "name_full": "Chain-of-Thought prompting",
            "brief_description": "A prompting technique that elicits intermediate reasoning steps from LLMs by conditioning them to produce stepwise rationale before the final answer, improving multi-step reasoning and program planning.",
            "citation_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "mention_or_use": "use",
            "system_name": "Chain-of-Thought (CoT) JSON prompting",
            "system_description": "CoT prompting structures the LLM's output into intermediate reasoning steps (the 'chain of thought') which the model produces explicitly; in this paper the authors implement CoT via an enforced JSON output schema (fields: reflection, pixel_changes, object_changes, helper_functions, overall_pattern, program_instructions, python_program). This architecture makes the LLM's internal decomposition explicit and conditions subsequent output fields on earlier reflections, aiding program synthesis and multi-step problem solving.",
            "llm_model_used": "GPT-4 (explicitly used in the paper for CoT-style JSON prompting)",
            "extraction_technique": "structured prompting with an enforced schema (JSON) that forces the model to extract observations and intermediate facts before producing synthesis/code",
            "synthesis_technique": "multi-step conditioning: earlier CoT fields (observations, helper functions, pattern) are used to guide later synthesis (program instructions and program code), supporting systematic combination of facts across inputs",
            "number_of_papers": null,
            "domain_or_topic": "applied to ARC tasks in this paper; general reasoning and multi-step synthesis",
            "output_type": "structured chain-of-thought entries and synthesized artifacts (programs, transformation rules); could be adapted to structured summaries from multiple documents",
            "evaluation_metrics": null,
            "performance_results": "Used as part of the pipeline that achieved 50/111 solved ARC tasks (45%); the paper does not isolate CoT-only effect quantitatively.",
            "comparison_baseline": "free-form prompting (no structured CoT) or single-step program generation",
            "performance_vs_baseline": null,
            "key_findings": "Structuring CoT output in a machine-readable JSON sequence helps condition generation and reduces ambiguity, enabling better program-planning and synthesis in the ARC domain.",
            "limitations_challenges": "CoT outputs can be verbose and consume prompt/window budget; may still contain incorrect reasoning steps that propagate; requires prompt engineering to avoid format violations.",
            "scaling_behavior": "Not explicitly discussed for multi-paper scaling in this paper; CoT increases context usage and thus is affected by model context window limits.",
            "uuid": "e4585.1",
            "source_info": {
                "paper_title": "Large Language Model (LLM) as a System of Multiple Expert Agents: An Approach to solve the Abstraction and Reasoning Corpus (ARC) Challenge",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "MassSampling+Filtering",
            "name_full": "Mass sampling and filtering (AlphaCode-style)",
            "brief_description": "An approach of generating a large number of candidate programs via stochastic sampling from an LLM and filtering them by executing on provided examples/tests to select correct solutions.",
            "citation_title": "Competition-level code generation with alphacode",
            "mention_or_use": "use",
            "system_name": "Mass Sampling and Filtering (AlphaCode-inspired)",
            "system_description": "The pipeline uses an LLM (here GPT-4) to sample many candidate programs (mass sampling) under diverse prompts/temperatures, then executes or tests these candidates against the task demonstration examples (filtering) to keep only those that match the demonstrations; a subset of passing candidates are then run on the test input. Components: sampler (LLM with temperature), executor/test harness, candidate filter/selector.",
            "llm_model_used": "GPT-4 for program sampling in this paper; AlphaCode used specialized models in its original work",
            "extraction_technique": "not document extraction per se; generation followed by execution-based filtering to extract programs that satisfy examples",
            "synthesis_technique": "synthesis by enumeration and selection — combine many sampled candidate programs then select via behavioral tests",
            "number_of_papers": null,
            "domain_or_topic": "program synthesis for ARC tasks / code generation",
            "output_type": "executable programs (Python functions) that transform input grids",
            "evaluation_metrics": "task solve count (number of ARC tasks solved), pass/fail on demonstration and test cases",
            "performance_results": "Contributed to achieving 50/111 solved ARC tasks for the subset of problems fitting context constraints; paper reports using mass sampling + filtering in the pipeline but does not give an ablation isolating its exact contribution numerically.",
            "comparison_baseline": "single-shot program generation without sampling and execution filtering",
            "performance_vs_baseline": "Not quantified precisely in the paper, but authors state mass sampling + filtering improves chance of finding a working program.",
            "key_findings": "Generating many diverse candidates and filtering by execution on demonstrations improves the likelihood of obtaining a correct program; useful when LLM-generated code is brittle.",
            "limitations_challenges": "High computational and API cost due to many samples and program executions; requires reliable test harness and can still fail when demonstrations do not fully specify the transformation.",
            "scaling_behavior": "Cost grows with number of samples and program complexity; success probability increases with more samples but with diminishing returns and higher compute/API cost.",
            "uuid": "e4585.2",
            "source_info": {
                "paper_title": "Large Language Model (LLM) as a System of Multiple Expert Agents: An Approach to solve the Abstraction and Reasoning Corpus (ARC) Challenge",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Voyager",
            "name_full": "Voyager: An open-ended embodied agent with large language models",
            "brief_description": "An embodied lifelong-learning agent that leverages LLMs to propose, implement and store 'skills' (code functions) in a growing skill library and uses iterative interaction with an environment for continual learning.",
            "citation_title": "Voyager: An openended embodied agent with large language models",
            "mention_or_use": "mention",
            "system_name": "Voyager (LLM-driven skill-learning agent)",
            "system_description": "Voyager uses an LLM to propose and synthesize code-based skills for an embodied environment (Minecraft), maintains a skill library of functions, and iteratively improves behavior via environment interaction and feedback. Key components: LLM-based planner/programmer, skill library (functions), environment executor, iterative prompting loop that uses observed outcomes to refine skills.",
            "llm_model_used": "LLMs (paper references Voyager work; specific model details are from the Voyager publication)",
            "extraction_technique": "not targeted at papers; uses environment-observed traces and skill extraction from interaction logs",
            "synthesis_technique": "incremental skill synthesis and reuse (compose learned functions to form higher-level behaviors)",
            "number_of_papers": null,
            "domain_or_topic": "embodied agents / open-world environments (Minecraft in Voyager)",
            "output_type": "environmental behaviors/skills (code functions) rather than document-level theories",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Authors of this paper cite Voyager as an inspiration for (a) maintaining a library of learned primitives/skills and (b) using iterative environment feedback to refine LLM outputs.",
            "limitations_challenges": "Voyager-style continual learning relies on environment feedback and suitable primitives; transferring that idea to multi-document scientific synthesis requires designing retrieval and verification loops.",
            "scaling_behavior": "Voyager shows benefit from continual accumulation of skills; scaling to large corpora would require retrieval/indexing and verification mechanisms.",
            "uuid": "e4585.3",
            "source_info": {
                "paper_title": "Large Language Model (LLM) as a System of Multiple Expert Agents: An Approach to solve the Abstraction and Reasoning Corpus (ARC) Challenge",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GhostMC",
            "name_full": "Ghost in the Minecraft (Generally capable agents via LLMs)",
            "brief_description": "An LLM-driven agent that constraints its actions to a predefined list of functions and learns via memory and iterative interaction with the environment to produce capable open-world behavior.",
            "citation_title": "Ghost in the minecraft: Generally capable agents for open-world enviroments via large language models with text-based knowledge and memory",
            "mention_or_use": "mention",
            "system_name": "Ghost in the Minecraft (LLM agent with constrained action space)",
            "system_description": "This system runs an LLM as a controller over a constrained action/function space, using text-based memory and iterative environment feedback; components include function/action library, LLM planner, memory store, and environment executor. The paper references Ghost as demonstrating the value of constrained functional action spaces and environment feedback for LLM-driven problem solving.",
            "llm_model_used": "LLMs (as used in the referenced Ghost work; specific models described in that work)",
            "extraction_technique": "not designed for document extraction; extracts plans/skill sequences from LLM outputs to execute in environment",
            "synthesis_technique": "iterative refinement of plans using memory and environment outcomes",
            "number_of_papers": null,
            "domain_or_topic": "embodied open-world environments",
            "output_type": "action plans / skill invocations (functions) rather than literature syntheses",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Cited as evidence that iterative environment feedback and a constrained function/action space improves LLM reliability; motivates the paper's use of primitive/helper functions and feedback loops.",
            "limitations_challenges": "Method depends on appropriate action abstractions and reliable environment observation; mapping this to multi-paper synthesis would require verification/validation mechanisms for provenance and consistency.",
            "scaling_behavior": "Not discussed for document-scale synthesis in this paper.",
            "uuid": "e4585.4",
            "source_info": {
                "paper_title": "Large Language Model (LLM) as a System of Multiple Expert Agents: An Approach to solve the Abstraction and Reasoning Corpus (ARC) Challenge",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "OpenAI Functions",
            "name_full": "OpenAI function-calling API (function calling)",
            "brief_description": "A mechanism that enables LLMs to call external, developer-defined functions (APIs) in a structured way so the model can delegate precise operations to tools and receive structured outputs.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Function-calling (OpenAI Functions style)",
            "system_description": "Function-calling augments LLMs with a mechanism to invoke deterministic external functions (e.g., database lookups, retrieval, program execution) by returning a structured function call rather than free-form text; components: schema-declared functions, LLM deciding when/how to call, and function executor returning structured results to the model. The paper analogizes its primitive/helper function design to this paradigm, using function descriptions to ground GPT-4 in semantic operations.",
            "llm_model_used": "GPT-4 (the paper uses GPT-4 and references the OpenAI function-calling pattern)",
            "extraction_technique": "tool-mediated extraction: LLM triggers functions (e.g., get_objects, crop_grid) to obtain structured facts rather than relying on pure free-form extraction",
            "synthesis_technique": "compose function outputs and LLM reasoning to synthesize final outputs (programs or grounded summaries)",
            "number_of_papers": null,
            "domain_or_topic": "applied to ARC tasks via primitive/helper functions; general tool use for many domains including document processing",
            "output_type": "structured function calls and their outputs, enabling deterministic steps of extraction or computation",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "The authors find that grounding LLMs via explicitly described primitive/helper functions (function-call-like) improves program generation reliability compared to pure text-only descriptions.",
            "limitations_challenges": "Requires engineering of appropriate functions and careful naming/descriptions; too many available functions or irrelevant functions can harm performance.",
            "scaling_behavior": "Function-based grounding scales with the richness of the provided function library but requires curation or automated growth mechanisms to remain tractable.",
            "uuid": "e4585.5",
            "source_info": {
                "paper_title": "Large Language Model (LLM) as a System of Multiple Expert Agents: An Approach to solve the Abstraction and Reasoning Corpus (ARC) Challenge",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Visual ChatGPT",
            "name_full": "Visual ChatGPT",
            "brief_description": "A system that connects LLMs with visual foundation models and external tools to allow multimodal interactions (image understanding, generation, editing) by orchestrating specialized vision models via an LLM controller.",
            "citation_title": "Visual chatgpt: Talking, drawing and editing with visual foundation models",
            "mention_or_use": "mention",
            "system_name": "Visual ChatGPT (LLM orchestrating vision tools)",
            "system_description": "Visual ChatGPT uses an LLM to orchestrate a collection of specialized visual models (e.g., detectors, generators, editors) by routing intermediate outputs between tools and the LLM; components: LLM planner, library of vision tools, adapters to convert between modalities, and a control protocol. The ARC paper cites Visual ChatGPT as related work for the style of prompting functions and tool-backed processing.",
            "llm_model_used": "LLM controller (the referenced work uses GPT-family models as controllers in examples; the ARC paper uses GPT-4)",
            "extraction_technique": "tool orchestration to extract structured visual features via specialized models (analogous to using helper functions to extract object views)",
            "synthesis_technique": "LLM-mediated composition of tool outputs into final answers or artifacts",
            "number_of_papers": null,
            "domain_or_topic": "multimodal vision + language tasks (image editing, captioning); cited as analogous to using helper functions for grounding",
            "output_type": "multimodal edits/results and structured descriptions",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Cited as an example where an LLM orchestrates specialized tools; motivates the paper's use of helper functions to ground GPT-4 for ARC tasks.",
            "limitations_challenges": "Tool orchestration adds integration complexity and can be brittle if adapters or tool outputs are noisy.",
            "scaling_behavior": "Scales with the number and quality of integrated tools, but complexity of orchestration grows accordingly.",
            "uuid": "e4585.6",
            "source_info": {
                "paper_title": "Large Language Model (LLM) as a System of Multiple Expert Agents: An Approach to solve the Abstraction and Reasoning Corpus (ARC) Challenge",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "LLMs-as-pattern-machines",
            "name_full": "Large language models as general pattern machines (Mirchandani et al.)",
            "brief_description": "Work showing that LLMs can be used as general pattern predictors by converting tasks (e.g., grids) into token sequences and using next-token prediction to solve pattern tasks.",
            "citation_title": "Large language models as general pattern machines",
            "mention_or_use": "mention",
            "system_name": "LLMs as general pattern machines (ASCII-grid tokenization)",
            "system_description": "This approach represents structured data (e.g., grid tasks) as linear token sequences (ASCII/text) and uses an LLM's next-token prediction capability to learn and generate outputs for pattern tasks. The ARC paper references this line of work that converted grids to ASCII and solved many ARC tasks using next-token prediction with LLMs.",
            "llm_model_used": "various LLMs (Mirchandani et al. used large LLMs; the ARC paper uses GPT-4 for its pipeline)",
            "extraction_technique": "tokenization of structured inputs into text and direct next-token / sequence prediction to extract patterns",
            "synthesis_technique": "direct sequence generation conditioned on training examples; no explicit retrieval across documents in the referenced work",
            "number_of_papers": null,
            "domain_or_topic": "pattern recognition tasks (ARC grids) rather than scientific papers",
            "output_type": "predicted token sequences representing outputs (grids)",
            "evaluation_metrics": null,
            "performance_results": "Referenced result: Mirchandani et al. solved 85 out of 800 ARC tasks by treating grids as ASCII next-token prediction.",
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Textualizing structured tasks and using LLM sequence prediction can be surprisingly effective for pattern tasks; motivates using multiple textual abstraction spaces.",
            "limitations_challenges": "Representation sensitivity (offsets, formatting) and limited to tasks that can be suitably linearized; not directly a multi-document synthesis method.",
            "scaling_behavior": "Performance depends on model size and amount of in-context examples; Mirchandani et al. showed improvements when using sufficiently large LLMs.",
            "uuid": "e4585.7",
            "source_info": {
                "paper_title": "Large Language Model (LLM) as a System of Multiple Expert Agents: An Approach to solve the Abstraction and Reasoning Corpus (ARC) Challenge",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Retrieval-augmented generation for knowledgeintensive nlp tasks",
            "rating": 2
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Competition-level code generation with alphacode",
            "rating": 2
        },
        {
            "paper_title": "Voyager: An openended embodied agent with large language models",
            "rating": 1
        },
        {
            "paper_title": "Ghost in the minecraft: Generally capable agents for open-world enviroments via large language models with text-based knowledge and memory",
            "rating": 1
        },
        {
            "paper_title": "Visual chatgpt: Talking, drawing and editing with visual foundation models",
            "rating": 1
        },
        {
            "paper_title": "Large language models as general pattern machines",
            "rating": 1
        }
    ],
    "cost": 0.0184905,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Large Language Model (LLM) as a System of Multiple Expert Agents: An Approach to solve the Abstraction and Reasoning Corpus (ARC) Challenge</h1>
<p>John Tan Chong Min ${ }^{1}$ Mehul Motani ${ }^{1}$</p>
<h4>Abstract</h4>
<p>We attempt to solve the Abstraction and Reasoning Corpus (ARC) Challenge using Large Language Models (LLMs) as a system of multiple expert agents. Using the flexibility of LLMs to be prompted to do various novel tasks using zeroshot, few-shot, context-grounded prompting, we explore the feasibility of using LLMs to solve the ARC Challenge. We firstly convert the input image into multiple suitable text-based abstraction spaces. We then utilise the associative power of LLMs to derive the input-output relationship and map this to actions in the form of a working program, similar to Voyager / Ghost in the MineCraft. In addition, we use iterative environmental feedback in order to guide LLMs to solve the task. Our proposed approach achieves 50 solves out of 111 training set problems ( $45 \%$ ) with just three abstraction spaces - grid, object and pixel - and we believe that with more abstraction spaces and learnable actions, we will be able to solve more.</p>
<h2>1. Introduction</h2>
<p>The Abstraction and Reasoning Corpus (ARC) Challenge is a key milestone in the march towards artificial general intelligence (AGI) as it requires forming concepts and abstractions (Chollet, 2019). Fig. 1 illustrates a sample ARC task. One of the key difficulties of the ARC challenge is that it requires doing something counter to mainstream deep learning - learning from very few samples. Deep learning typically uses tens of thousands of samples to do well. Humans, in comparison, can learn how to identify different animals by just one or two different observations. For instance, a child can identify a giraffe in real life for the first time, even though the only other time they may have been exposed to a giraffe was through a cartoon flash card. Such capabilities are not well endowed in modern AI systems,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. A sample ARC task. The challenge is to infer the abstract rule(s) governing the demonstration transformations and apply it to the test input. Example from: https://aiguide.substack. com/p/why-the-abstraction-and-reasoning
and that means that such AI systems will need to be trained extensively before deploying in the real world. After deploying them in the real world, they will also be limited in their ability to adapt and learn as the environment changes.</p>
<p>In contrast, traditional symbol-based systems (e.g., GOFAI (Boden, 2014)) can "learn" quite fast, as any new situation can be interpreted without any learning phase, provided that there are existing symbols which can represent it. However, the history of GOFAI has shown that it is difficult to engineer these symbols, and at many times, even humans face difficulty to come up with symbols as they may not be able to express it in words.</p>
<p>As can be seen, there are shortcomings with the above two approaches, and a new kind of approach will be needed in order to learn fast and generalise to new situations, in order to even have a chance at solving the ARC Challenge. In this paper, we address this challenge by proposing to use Large Language Models (LLMs) as a system grounded in functional action spaces to tackle the ARC challenge. This can be said to be an intermediate ground between both deep learning and GOFAI approaches - The functional action spaces are more flexible than symbols in GOFAI; LLMs</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. 88\% of ARC tasks can be solved by the Builder from just the description alone given by the Describer, without inputoutput examples. Can GPT-4 function as both the describer and the builder? Image reproduced from Fig. 4 of Acquaviva et al. (2021).
which are a form of deep learning that are adaptable to new situations via prompting. Specifically the contributions of the paper are as follows:</p>
<ul>
<li>We showcase a novel method of using LLMs as a system of multiple expert agents (without any pre-training) to solve the ARC Challenge</li>
<li>We highlight the importance of a combination of multiple abstraction spaces from which to associate the input space to the output space</li>
<li>We demonstrate the feasibility of grounding in functional space for program synthesis by LLMs.</li>
</ul>
<h2>2. Related Work</h2>
<p>ARC Challenge. The ARC challenge (Chollet, 2019) comprises 400 public training tasks, 400 public evaluation tasks and 200 private test tasks. Each of these tasks has multiple "Task Demonstration" Input/Output grids, of which the task-taker must infer a common relation out of them. This common relation is then applied to the "Test Input", from which we get the "Test Output". The "Test Output" must match perfectly for it to be considered solved. The grids comprise grid sizes of 1 x 1 to 30x30, of which pixels can take on 10 different values.</p>
<p>Domain Specific Language (DSL) Approaches. The majority of ARC Challenge solutions are mainly DSL ones (Alford, 2021; Ferré, 2021; Xu et al., 2023a). This is also the case for the first-place solution of the ARC Kaggle competition (https://www.kaggle.com/ code/icecuber/arc-1st-place-solution).</p>
<p>LLM-Based approaches. One way to approach the ARC challenge will be to use text to describe the visual characteristics of objects (Camposampiero et al., 2023). Indeed, $88 \%$ of ARC tasks can be solved via language description alone without input-output examples as shown in Fig. 2 (Acquaviva et al., 2021). For certain problems, denoting pixels in terms of objects can significantly boost the solve
rate from 13 to 23 out of 50 object-related ARC tasks (Xu et al., 2023b). Some work has also been done to do end-toend input to program description generation with just LLMs alone to some success (Min, 2023). Other approaches have used Decision Transformers (Chen et al., 2021) to find a sequence of primitive actions from the input to output (Park et al., 2023), however, as noted by the authors, huge amounts of data (10000 training data for 2000 testing data) are needed to train this method, it is unlikely it can generalise to unseen inputs. Recently, LLMs have been used to take the ASCII text view of the grid as input for next token prediction and have solved 85 out of 800 ARC tasks (Mirchandani et al., 2023).</p>
<p>Code as Skills and Environmental Feedback. Voyager is an embodied lifelong learning agent powered by LLMs (Wang et al., 2023). It features a skill library of functions to build up complex behaviour, and an iterative prompting mechanism with the environment to learn from environmental feedback. Ghost in the Minecraft (Zhu et al., 2023) does something similar as well, though they constrain the action space to a list of functions. Similarly, we use code generation with primitive functions to approximate using a skill library, and use iterative prompting using ARC task output as feedback to learn from the environment.</p>
<p>Our Method. In line with the existing LLM approaches, we agree that we should use language as an alternate abstraction space in addition to the original pixel grid. Unlike existing approaches, we believe we should use more than one abstraction space. Hence, the LLM will be both the Builder and the Describer in Fig. 2, but the Builder can also reference input-output pairs. We also believe we should integrate LLMs with a kind of DSL approach, but can afford to have an even more expressive DSL because an LLM is able to do matching of functions via semantics much more effectively than traditional DSL approaches.</p>
<h2>3. Broad Overview of Method</h2>
<p>In this section, we provide an overview of our proposed approach and discuss several key ideas behind it. We have not implemented out all parts of the proposed approach, but it is already doing well. Generative Pre-trained Transformer 4 (GPT-4) is a multimodal LLM created by OpenAI and released in March 2023 (OpenAI, 2023). For now, we exclusively use GPT-4 for our model, as we empirically observe that GPT-3.5 and other open source models are not able to perform well enough for this method to work. The overall method is shown in Fig. 3.</p>
<p>Problem Type Classification (Not Implemented). ARC tasks test various concepts. If we can use past examples to ground the LLM, and let the LLM decide what problem category an ARC task belongs to, we can proceed with a</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Process Flowchart of LLMs as a System to solve the ARC Challenge.
specialised workflow to target solving that particular style of task. Presently, we simply run through all the various agent types and select the agent types which work. Implementing this classifier will not affect performance but will significantly help reduce the costs.</p>
<p>Useful Abstraction Spaces. While GPT-4 has proven to be a general purpose solver, being (currently) a text-based model, GPT-4 lacks some of the innate human priors necessary to solve the ARC challenge. For example, GPT-4 is not able to identify objects accurately from text alone. Objects are defined as continuous sections of the grid with the same non-zero value. Hence, providing such an object view as an abstraction space using text greatly helps with the GPT-4's ability to form associations with the input-output pair and is better able to find a solution (Xu et al., 2023b). Moreover, we can provide more than one abstraction space to GPT-4, which can increase the chance that one or more abstraction spaces contain a simple mapping from input to output, thereby reducing the complexity of the problem. Do note that these abstraction spaces are unchangeable, and are fixed since the beginning of learning. Hence, the agents will have to do processing based on these fixed priors.</p>
<h2>Encoding Human Biases via Helper/Primitive Functions.</h2>
<p>An initial implementation of using GPT-4 to solve ARC was done with just prompting the human biases and action spaces via text. This did not do so well due to lack of grounding using words alone. A key innovation in this work is to use primitive functions as action spaces, as a way to encode human priors. If we could use functions for grounding, and express the semantic meaning of the function in words,</p>
<p>GPT-4 could use the function to provide the code needed for the solution. Hence, the problem now becomes finding out what are the primitive functions we need to encode in order for the LLM to solve any generic ARC problem.</p>
<p>Using Memory for Additional Context (Not Implemented). New problems might mix and match aspects of previous solutions, so having a memory bank to provide examples of similar solved problems in the past can help to ground the LLM to better generate the answer. This is currently not implemented due to constraints of context length. Once the context length for GPT-4 increases or fine-tuning becomes available, we intend to let each agent have memory of relevant previously solved problems and their solutions, so that it can ground the agent's output. This is akin to Retrieval Augmented Generation (Lewis et al., 2020).</p>
<p>Utilising Feedback from Environment. Another key idea is that a learning system would need to utilise feedback from the environment, and so a recursive loop feeding in feedback from the environment (whether there is compile error, whether the code matches the intended output) can help a lot in getting the right answer. This is akin to what is done in Voyager and Ghost in the MineCraft (Wang et al., 2023; Zhu et al., 2023).</p>
<p>LLMs as a System. Humans do not operate with only one system. We have various systems to call for various tasks. Similarly, we can have multiple expert agents for each task (such as Object View, Pixel View, Grid View) and call on them to give their interpretation of the task, and select the most promising agent. This greatly helps narrow the search</p>
<p>space for the solution. Then, we utilise the specialised functions this agent has and solve the problem. Interfacing this agent with environment feedback, the problem-type specific abstraction space, past examples and action spaces can greatly help filter and ground GPT-4 to generate a plausible solution. We believe that, with better grounding via expert agents, better abstraction space representations and better primitive function grounding, we will eventually be able to solve most of the ARC tasks using the proposed approach.</p>
<h2>4. Detailed Overview of Method</h2>
<p>We now go into some details of our method. Refer to Appendix A and B for the full GPT-4 prompt.</p>
<h3>4.1. Different Abstraction Spaces</h3>
<p>We utilise various ways of encoding the abstraction spaces so that GPT-4 can better associate between the Input-Output pairs. It has been shown in Image-Joint Embedding Predictive Architecture (I-JEPA) (Assran et al., 2023) and Stable Diffusion (Rombach et al., 2022) that prediction in the latent/abstraction space leads to better downstream tasks than predicting in the input space. However, instead of just one abstraction space, we believe that there are many possible abstraction spaces which are fixed, and it is up to the solver to choose which is the best for the task at hand. We believe by incorporating more useful views and refining current ones, we can solve more ARC tasks.</p>
<p>For our method, we use only three views - Grid View, Object View, Pixel View - and that has already achieved quite good results. In brief, Grid View provides the entire grid representation, except we change the pixel numbers to characters so that we do not bias GPT-4 to treat it as an arithmetic problem to perform arithmetic on the pixel values. This also has the added benefit of ensuring that GPT-4 has not seen the ARC tasks before as it is now of a different form. The Object View groups pixels that are contiguous together, so that they can be manipulated as a group. Pixel View gives the coordinates for each pixel, which can help with more fine-grained movement tasks or relational tasks between pixels. Refer to Appendix C for more details.</p>
<h3>4.2. JSON-based output format</h3>
<p>LLMs are well known for being verbose and also relatively free-form in the output, making it hard for any automated program to use it. Here, we explicitly ask GPT-4 to output in a JSON format via prompting. This JSON format also facilities Chain-of-Thought (CoT) prompting (Wei et al., 2022), as it is done in a specific sequence to encourage broad to specific thinking.</p>
<h3>4.3. CoT Prompting</h3>
<p>CoT enables the output to be structured and the LLM will be able to condition the generation of the later output based on the earlier ones. This enables a more broad to specific style of prompting, helping the LLM to think and reflect on various areas, narrowing the search space, and ultimately may help to solve the problem.</p>
<p>Here, we do CoT prompting directly using JSON format (See Appendix D for some examples of GPT output in this JSON format). We ask GPT-4 to output:</p>
<ol>
<li>"reflection": "reflect on the answer",</li>
<li>"pixel_changes": "describe the changes between the input and output pixels, focusing on movement or pattern changes",</li>
<li>"object_changes": "describe the changes between the input and output objects, focusing on movement, object number, size, shape, position, value, cell count",</li>
<li>"helper_functions": "list any relevant helper_functions for this task",</li>
<li>"overall_pattern": "describe the simplest input-output relationship for all input-output pairs",</li>
<li>"program_instructions": "Plan how to write the python function and what helper functions and conditions to use",</li>
<li>"python_program": "Python function named 'transform_grid' that takes in a 2D grid and generates a 2D grid. Output as a string in a single line with $\backslash \mathrm{n}$ and $\backslash \mathrm{t}$."</li>
</ol>
<h3>4.4. Helper/Primitive Functions</h3>
<p>For the functions, we basically zero-shot prompt by stating the function name plus the input parameters and the description of the function. We find that this format of zero-shot prompting works very well for most functions, especially if the name of the function is already indicative of what it does. This is very similar to the approach taken in Visual ChatGPT (Wu et al., 2023), as well as OpenAI Functions (https://openai.com/blog/ function-calling-and-other-api-updates). As this method of prompting is not sufficient to imbue biases that are not inherent in text (i.e. rotation, flipping), we also provide one-shot examples of how to use the function.</p>
<h3>4.5 Conditional Functions:</h3>
<p>Rather than letting GPT-4 free-form generate its own code, we ask it to generate a conditional flow on the primitive functions. This greatly helps to reduce compilation errors. Such a conditional flow is needed, as some ARC tasks require using logic that only applies if a particular condition is met (e.g., turn the shape red if it has exactly 6 cells). Without this conditional flow, the program would need many more steps before it can solve the problem. An example of such a conditional flow is:
If {condition}: {Primitive Function}</p>
<h2>5. Methodology</h2>
<p>Select Problems by Context Length. We firstly filter the ARC training set problems to only those whose Grid View and Object View (mono-color, no diagonals) can fit into a context length of 3000 tokens. This is important because later when we incorporate environmental feedback, we will need additional token length, and by empirical observation, 3000 tokens is necessary to guarantee some buffer token amount so that the entire prompt can fit within 8000 tokens later. This is the current maximum context length for the GPT-4 web browser, as well as for the basic GPT-4 API. In the future, we envision that our approach can work for more ARC tasks when the context length for GPT-4 increases.</p>
<p>Mass Sampling and Filtering. Next, we use the OpenAI API for GPT-4 May 242023 version with a temperature of 0.7 to ensure a diverse range of outputs. We use the OpenAI API and the web browser interface for GPT-4 interchangeably. We employ a mass sampling and filtering process to generate code, much like in AlphaCode (Li et al., 2022) (see Fig. 4). Grid View is always there unless there is context length limitation. We can choose between toggling Object View (10 types) and Pixel View for the agents (at least one must be active), which leads to a total of $10 \times 2=20$ agents (See Appendix C for details). We utilise each expert agent three times each, with at most three feedback loop iterations, and filter the output codes which can solve the Task Demonstration to try it out on the Task Input. If there are multiple such codes, we randomly pick three to test it out. Any of these three solutions passing the Test Input will be counted as a solve, which is in line with the Kaggle competition and Lab 42's ARCathon.</p>
<h2>6. Results</h2>
<p>Overall. Overall, as shown in Table 1, our method solves 50 out of 111 Training Set ARC tasks which could fit within the context length. This is about a $45 \%$ solve rate, which is quite remarkable as the current ARC world record solve rate is $30.5 \%$ (though this is on the hidden test set), according to https://lab42.global/arcathon/updates/.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. The overall Mass Sampling and Filtering process with various expert agents</p>
<p>Coding Issues. To see how many of the unsolved problems are due to coding issues, we check how many of them have the correct description as evaluated by a human, but not have the correct code. This turns out to be 8 out of 61 , as shown in Table 2 (See Appendix E for details). This means that if we could learn the primitive/helper functions better and have a wider range to choose from, we can improve solve rate. To solve the rest of the problems, we will have to incorporate better views - it is observed that GPT-4 cannot solve line continuation tasks, especially for diagonal lines, grid manipulation tasks, and symmetry tasks easily, and these could easily be incorporated as additional views.</p>
<p>Iterative Feedback. To see how much iterative environmental feedback helps, we look at number of tasks solved with the iterative environment feedback loop. This turns out to be 7 tasks out of 50, as shown in Table 3 (See Appendix E for details). This is quite significant, and highlights the importance of environmental feedback.</p>
<h2>7. Discussion</h2>
<p>The results are promising, and GPT-4 agents with various combination of views can solve different types of problems well, as compared to just using the original Grid View. It was also sometimes observed that Object View had to go with Pixel View for a consolidation of information across both views in order to solve the task. This reinforces the view that there should not be just one abstraction space, but multiple abstraction spaces which could be used in combination with each other.</p>
<p>Empirical observation has shown that GPT-4 with primitive function grounding can solve more tasks than without. It is a better way at encoding priors than with just text alone. Overall, GPT-4 is great at solving tasks which are made up of a combination of primitive functions.</p>
<p>It was observed that function names and descriptions are</p>
<p>Table 1. Number of tasks solved, not solved and partially solved (Program works for Task Demonstration but not for Test Input/Output out of 111 Training Set tasks). See Appendix E for breakdown of tasks solved by each view type.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Total Tasks</th>
<th style="text-align: center;">Tasks Solved</th>
<th style="text-align: center;">Tasks Not Solved</th>
<th style="text-align: center;">Tasks Partially Solved</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">111</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">58</td>
<td style="text-align: center;">3</td>
</tr>
</tbody>
</table>
<p>Table 2. Tasks not solved but with correct description</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Total Tasks Not Solved</th>
<th style="text-align: center;">Correct description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">61</td>
<td style="text-align: center;">8</td>
</tr>
</tbody>
</table>
<p>Table 3. Tasks solved with iterative feedback loop after either incorrect output or compile error</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Total</th>
<th style="text-align: center;">Incorrect Output</th>
<th style="text-align: center;">Compile Error</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">50</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">1</td>
</tr>
</tbody>
</table>
<p>very important - GPT-4 tends to choose functions semantically similar to what it intends to do, and the changing of a function name to something irrelevant may cause it not to be used.</p>
<h2>8. Improvements</h2>
<p>GPT-4 agents cannot do tasks that have no relevant priors encoded in the primitive functions well, such as scaling of objects, symmetry, continuation of lines, overlay of grids with logical rules, grid manipulation like cropping, translating, changing of shape. Furthermore, it is weak when there is more than one relation, and this type of problems benefit from the iterative environment feedback loop. By setting the new input as the output that GPT-4's program outputs, it is in effect taking a step towards the solution and helps GPT-4 better associate the simpler input-output relationship.</p>
<p>GPT-4 has been observed to use primitive functions not meant for the view, for example, Pixel View Agent using the get_objects function. Hence, giving too much context might affect performance. This is similar to Xu et al. (2023b) when the performance declined after adding in relations between objects. This reinforces our idea that it is best to split up into multiple expert agents with separate views and only relevant primitive functions.</p>
<p>Based on our experimental results, we propose new views/agents in Appendix F.</p>
<h2>9. Future Work</h2>
<p>Currently, we use all agents in a brute-force manner for a task. In order to reduce computation (and cost), we could perhaps have a classifier which takes in previous examples as input to learn how to classify a new problem into a cate-
gory, so that the right agents can be used to solve it.
Currently, the primitive functions are hand-engineered based on observation of the first 50 tasks in the training set, and are also not a complete set. We will try to incorporate a way for GPT-4 to be prompted to create new primitive functions, and add those successful functions which could solve a new task to the list of primitive functions, much like Voyager (Wang et al., 2023). One way is to add any transform_grid function that is successful as a new primitive function, as long as the description of the function is different from existing ones.</p>
<h2>10. Conclusion</h2>
<p>Overall, LLMs as a system of multiple expert agents with environmental feedback is a promising approach towards solving the ARC Challenge. To facilitate further research using this approach, our code can be found at https:// github.com/tanchongmin/ARC-Challenge/.</p>
<h2>Acknowledgements</h2>
<p>This research is supported by the National Research Foundation, Singapore under its AI Singapore Programme (AISG Award No: AISG-GC-2019-002). Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of National Research Foundation, Singapore.</p>
<p>Many thanks for the various intelligent people who have encouraged me to pursue the GPT4 route to solve ARC or have provided valuable insights - Pascal Kaufmann, Rolf Pfister, Michael Hodel, Simon Strandgaard, Douglas Miles, Richard Cottrill, Leonard Tan and many others. If your name is not here, do not worry, you can be in the future paper improving on this work:)</p>
<h2>References</h2>
<p>Acquaviva, S., Pu, Y., Kryven, M., Wong, C., Ecanow, G. E., Nye, M., Sechopoulos, T., Tessler, M. H., and Tenenbaum, J. B. Communicating natural programs to humans and machines. arXiv preprint arXiv:2106.07824, 2021.</p>
<p>Alford, S. A Neurosymbolic Approach to Abstraction and Reasoning. PhD thesis, Massachusetts Institute of Technology, 2021.</p>
<p>Assran, M., Duval, Q., Misra, I., Bojanowski, P., Vincent, P., Rabbat, M., LeCun, Y., and Ballas, N. Self-supervised learning from images with a joint-embedding predictive architecture. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1561915629, 2023.</p>
<p>Boden, M. A. 4 gofai. The Cambridge handbook of artificial intelligence, pp. 89, 2014.</p>
<p>Camposampiero, G., Houmard, L., Estermann, B., Mathys, J., and Wattenhofer, R. Abstract visual reasoning enabled by language. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2642-2646, 2023.</p>
<p>Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., Abbeel, P., Srinivas, A., and Mordatch, I. Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34:15084-15097, 2021.</p>
<p>Chollet, F. On the measure of intelligence. arXiv preprint arXiv:1911.01547, 2019.</p>
<p>Ferré, S. First steps of an approach to the arc challenge based on descriptive grid models and the minimum description length principle. arXiv preprint arXiv:2112.00848, 2021.</p>
<p>Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler, H., Lewis, M., Yih, W.-t., Rocktäschel, T., et al. Retrieval-augmented generation for knowledgeintensive nlp tasks. Advances in Neural Information Processing Systems, 33:9459-9474, 2020.</p>
<p>Li, Y., Choi, D., Chung, J., Kushman, N., Schrittwieser, J., Leblond, R., Eccles, T., Keeling, J., Gimeno, F., Dal Lago, A., et al. Competition-level code generation with alphacode. Science, 378(6624):1092-1097, 2022.</p>
<p>Min, T. J. C. An approach to solving the abstraction and reasoning corpus (arc) challenge. arXiv preprint arXiv:2306.03553, 2023.</p>
<p>Mirchandani, S., Xia, F., Florence, P., Ichter, B., Driess, D., Arenas, M. G., Rao, K., Sadigh, D., and Zeng, A. Large language models as general pattern machines. arXiv preprint arXiv:2307.04721, 2023.</p>
<p>OpenAI. Gpt-4 technical report, 2023.
Park, J., Im, J., Hwang, S., Lim, M., Ualibekova, S., Kim, S., and Kim, S. Unraveling the arc puzzle: Mimicking human solutions with object-centric decision transformer. arXiv preprint arXiv:2306.08204, 2023.</p>
<p>Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10684-10695, 2022.</p>
<p>Wang, G., Xie, Y., Jiang, Y., Mandlekar, A., Xiao, C., Zhu, Y., Fan, L., and Anandkumar, A. Voyager: An openended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023.</p>
<p>Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35: 24824-24837, 2022.</p>
<p>Wu, C., Yin, S., Qi, W., Wang, X., Tang, Z., and Duan, N. Visual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671, 2023.</p>
<p>Xu, Y., Khalil, E. B., and Sanner, S. Graphs, constraints, and search for the abstraction and reasoning corpus. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pp. 4115-4122, 2023a.</p>
<p>Xu, Y., Li, W., Vaezipoor, P., Sanner, S., and Khalil, E. B. Llms and the abstraction and reasoning corpus: Successes, failures, and the importance of object-based representations. arXiv preprint arXiv:2305.18354, 2023b.</p>
<p>Zhu, X., Chen, Y., Tian, H., Tao, C., Su, W., Yang, C., Huang, G., Li, B., Lu, L., Wang, X., et al. Ghost in the minecraft: Generally capable agents for open-world enviroments via large language models with text-based knowledge and memory. arXiv preprint arXiv:2305.17144, 2023.</p>
<h1>APPENDIX</h1>
<p>The appendix contains the following sections:</p>
<p>A Full Prompt Details for GPT-4 - This details the entire prompt used for GPT-4
B Primitive Functions and Conditional Functions - This details all the primitive functions and conditional functions used for the grounding of GPT-4</p>
<p>C Abstraction Views - This details the various abstraction views of Grid, Object and Pixel
D GPT-4 Output Examples - This showcases GPT-4's output to the prompt
E Task Solved Details - This details the breakdown of the tasks solved by view type
F Proposed Agent Types - This details the proposed agent types which may help increase solve rate of GPT-4 for the ARC Challenge.</p>
<h1>A. Full Prompt Details for GPT-4</h1>
<p>This section details the prompts used for GPT-4. The prompts are split up into a user prompt and a system prompt, as required by the GPT-4 API. If we do not use the API and use the web browser interface instead, we put the user prompt at the start of the prompt and the system prompt at the end of the prompt with the appropriate headings.</p>
<h2>A.1. User Prompt</h2>
<p>All coordinates are given as (row,col). Use get_size(grid) to return (len(grid),len(grid[0])).
To get objects, use get_objects(diag=False, by_row=False, by_col=False, by_color=False, multicolor=False, more_info=True) # replace this with whatever object view was used
[JSON with various abstraction views of input/output, and input and output grid size]
[Environmental Feedback - Either empty if first iteration, otherwise either Compile Error, or Output Error Message]</p>
<h2>A.2. Environmental Feedback (Compile Error Message)</h2>
<p>Previous Code: [Code]
Error Message: [Error Message]
Previous Overall Pattern: [Overall Pattern]
Your code had compilation errors. Correct it.</p>
<h2>A.3. Environmental Feedback (Output Error Message)</h2>
<p>If there is an output error (code generated output does not match Task Demonstration output), we treat this output as the new input and ask GPT-4 to get the new input-output relation.</p>
<p>Use the transform_grid function to get the right relation from 'input' to 'output'</p>
<h1>A.4. System Prompt</h1>
<p>Refer to Appendix B for details for the helper/primitive and conditional functions.</p>
<p>You are given a series of inputs and output pairs.
The values from 'a' to 'j' represent different colors. '.' is a blank cell.
For example, [[',','a',','],['.',',','b']] represents a 2 row x 3 col grid with color a at position $(1,0)$ and color b at position $(2,1)$.
Coordinates are 2D positions (row, col), row representing row number, col representing col number, with zero-indexing. Input/output pairs may not reflect all possibilities, you are to infer the simplest possible relation.
[Helper/Primitive Functions Description + Example]
[Conditional Functions Description + Example]</p>
<p>You are to output the following in json format:
{'reflection': 'reflect on the answer',
'pixel_changes': 'describe the changes between the input and output pixels, focusing on movement or pattern changes', 'object_changes': 'describe the changes between the input and output objects, focusing on movement, object number, size, shape, position, value, cell count',
'helper_functions': 'list any relevant helper_functions for this task',
'overall_pattern': 'describe the simplest input-output relationship for all input-output pairs',
'program_instructions': 'Plan how to write the python function and what helper functions and conditions to use', 'python_program': "Python function named 'transform_grid' that takes in a 2D grid and generates a 2D grid. Output as a string in a single line with $\backslash \mathrm{n}$ and $\backslash \mathrm{t}$." $}$.
Do not use quotation marks ' or " within the fields unless it is required for the python code</p>
<h1>B. Primitive Functions and Conditional Functions</h1>
<p>This section details all the primitive functions and conditional functions used for GPT-4. Currently, these functions are not learnable, and are defined via tuning by a human over the first 50 ARC training tasks. In the future, we intend for these functions to be learned and expanded from a starting set independent of human interaction.</p>
<p>For more information of how these functions were implemented, refer to the Jupyter Notebook provided in https: //github.com/tanchongmin/ARC-Challenge.</p>
<h1>B.1. Primitive Functions</h1>
<p>This is the way we prompted GPT-4 to understand the format for the primitive (helper) functions. This is essentially the</p>
<p>Each of the input-output relation can be done with one or more helper functions chained together.
Some relations require other functions, which you will need to come up with yourself.
Objects are tight-fitted grids (no empty row or column) with a top left coordinate, which can be used for easy manipulation of multiple coordinates.
You can create your own objects by just creating a dictionary with 'tl' and 'grid'
You can change an object's position by using 'tl' and its composition using 'grid'.
You should start each program by copying input grid or empty_grid or crop_grid of desired output size.
Then, fill the grid by using the fill helper functions.
If you use the fill functions with a '.' value, it is equivalent to removing parts of the grid.</p>
<h2>Helper functions:</h2>
<ul>
<li>get_objects(grid,diag=False,by_row=False,by_col=False,by_color=False,multicolor=False,more_info = True): Takes in grid, returns list of object dictionary: top-left coordinate of object ('tl'), 2D grid ('grid') by_row views splits objects by grid rows, by_col splits objects by grid columns, by_color groups each color as one object, multicolor means object can be more than one color. Empty cells within objects are represented as ' $\$$. If more_info is True, also returns size of grid ('size'), cells in object ('cell_count'), shape of object ('shape')</li>
<li>get_pixel_coords(grid): Returns a dictionary, with the keys the pixel values, values the list of coords, in sorted order from most number of pixels to least</li>
<li>empty_grid(row, col): returns an empty grid of height row and width col</li>
<li>crop_grid(grid, tl, br): returns cropped section from top left to bottom right of the grid</li>
<li>tight_fit(grid): returns grid with all blank rows and columns removed</li>
<li>combine_object(obj_1, obj_2): returns combined object from obj_1 and obj_2. if overlap, obj_2 overwrites obj_1</li>
<li>rotate_clockwise(grid, degree=90): returns rotated grid clockwise by a degree of 90, 180, 270 degrees</li>
<li>horizontal_flip(grid): returns a horizontal flip of the grid</li>
<li>vertical_flip(grid): returns a vertical flip of the grid</li>
<li>replace(grid, grid_1, grid_2): replaces all occurences of grid_1 with grid_2 in grid</li>
<li>get_object_color(obj): returns color of object. if multicolor, returns first color only</li>
<li>change_object_color(obj, value): changes the object color to value</li>
<li>fill_object(grid, obj, align=False): fills grid with object. If align is True, makes grid same size as object</li>
<li>fill_row(grid, row_num, value, start_col=0, end_col=30): fills output grid with a row of value at row_num from start_col to end_col (inclusive)</li>
<li>fill_col(grid, col_num, value, start_row=0, end_row=30): fills output grid with a column of value at col_num from start_row to end_row (inclusive)</li>
<li>fill_between_coords(grid, coord_1, coord_2, value): fills line between coord_1 and coord_2 with value</li>
<li>fill_rect(grid,tl,br,value): fills grid from tl to br with value. useful to create rows, columns, rectangles</li>
<li>fill_value(grid, pos, value): fills grid at position with value</li>
</ul>
<p>This is the way we one-shot prompted GPT-4 for the primitive functions.</p>
<div class="codehilite"><pre><span></span><code>assert get_objects([[&#39;a&#39;,&#39;a&#39;,&#39;a&#39;],[&#39;a&#39;,&#39;&#39;,&#39;a&#39;],[&#39;a&#39;,&#39;a&#39;,&#39;a&#39;]],more_info=False)==[{
    &#39;a&#39;],[&#39;a&#39;,&#39;&#39;,&#39;a&#39;],[&#39;a&#39;,&#39;a&#39;,&#39;a&#39;]]} ,{&#39;tl&#39;:(1,1),&#39;grid&#39;:[[&#39;S&#39;]]}}
assert get_pixel_coords([[&#39;a&#39;,&#39;a&#39;],[&#39;d&#39;,&#39;f&#39;]])=={&#39;a&#39;:[(0, 0),(0, 1)],&#39;d&#39;:[(1, 0)],&#39;f&#39;:[(1, 1)}}
assert empty_grid(3, 2)==[[&#39;,&#39;,&#39;,], [&#39;,&#39;,&#39;,], [&#39;,&#39;,&#39;]]
assert crop_grid([[&#39;a&#39;,&#39;a&#39;,&#39;b&#39;],[&#39;,&#39;,&#39;a&#39;,&#39;b&#39;]],(0, 0),(1, 1))==[[&#39;a&#39;,&#39;a&#39;],[&#39;,&#39;,&#39;a&#39;]]
assert tight_fit([[&#39;&#39;,&#39;&#39;,&#39;&#39;&#39;],[&#39;,&#39;,&#39;a&#39;,&#39;&#39;’&#39;],[&#39;,&#39;&#39;,&#39;&#39;’]])==[[&#39;a&#39;]]
assert combine_object({&#39;tl&#39;:(0, 0),&#39;grid&#39;:[[&#39;a&#39;,&#39;a&#39;],[&#39;a&#39;,&#39;&#39;]]},{&#39;tl&#39;: (1, 1),&#39;grid&#39;:[[&#39;f&#39;]]})=={&#39;tl&#39;:(0,
0),&#39;grid&#39;:[[&#39;a&#39;,&#39;a&#39;],[&#39;a&#39;,&#39;f&#39;]]}
assert rotate_clockwise([[&#39;a&#39;,&#39;b&#39;],[&#39;d&#39;,&#39;e&#39;]],90)==[[&#39;d&#39;,&#39;a&#39;],[&#39;e&#39;,&#39;b&#39;]]
assert rotate_clockwise([[&#39;a&#39;,&#39;b&#39;],[&#39;d&#39;,&#39;e&#39;]],270)==[[&#39;b&#39;,&#39;e&#39;],[&#39;a&#39;,&#39;d&#39;]]
assert horizontal_flip([[&#39;a&#39;,&#39;b&#39;,&#39;c&#39;],[&#39;d&#39;,&#39;e&#39;,&#39;f&#39;]])==[[&#39;c&#39;,&#39;b&#39;,&#39;a&#39;], [&#39;f&#39;,&#39;e&#39;,&#39;d&#39;]]
assert vertical_flip([[&#39;a&#39;,&#39;b&#39;,&#39;c&#39;],[&#39;d&#39;,&#39;e&#39;,&#39;f&#39;]])==[[&#39;d&#39;,&#39;e&#39;,&#39;f&#39;],[&#39;a&#39;,&#39;b&#39;,&#39;c&#39;]]
assert replace([[&#39;a&#39;,&#39;&#39;’],[&#39;a&#39;,&#39;a&#39;]],[[&#39;a&#39;,&#39;a&#39;]],[[&#39;c&#39;,&#39;c&#39;]])==[[&#39;a&#39;,&#39;&#39;’],[&#39;c&#39;,&#39;c&#39;]]
assert change_object_color({&#39;tl&#39;:(0,0),&#39;grid&#39;:[[&#39;a&#39;,&#39;&#39;]]},&#39;b&#39;)=={&#39;tl&#39;:(0,0),&#39;grid&#39;:[[&#39;b&#39;,&#39;&#39;]]}
assert get_object_color({&#39;tl&#39;:(0,0),&#39;grid&#39;:[[&#39;a&#39;,&#39;&#39;]]})==&#39;a&#39;
assert fill_object([[&#39;&#39;,&#39;&#39;’],[&#39;&#39;,&#39;&#39;’],{&#39;tl&#39;:(0, 1),&#39;grid&#39;:[[&#39;c&#39;],[&#39;c&#39;]]})==[[&#39;,&#39;,&#39;c&#39;],[&#39;&#39;,&#39;&#39;c&#39;]]
assert fill_value([[&#39;&#39;,&#39;a&#39;],[&#39;&#39;,&#39;a&#39;]],(1,1),&#39;b&#39;)==[[&#39;,&#39;,&#39;a&#39;],[&#39;&#39;,&#39;&#39;b&#39;]]
assert fill_row([[&#39;a&#39;,&#39;a&#39;],[&#39;c&#39;,&#39;a&#39;]],0,&#39;b&#39;)==[[&#39;b&#39;,&#39;b&#39;],[&#39;c&#39;,&#39;a&#39;]]
assert fill_col([[&#39;a&#39;,&#39;a&#39;],[&#39;c&#39;,&#39;a&#39;]],0,&#39;b&#39;)==[[&#39;b&#39;,&#39;a&#39;],[&#39;b&#39;,&#39;a&#39;]]
assert fill_rect([[&#39;a&#39;,&#39;a&#39;],[&#39;c&#39;,&#39;a&#39;]],(0,0),(1,1),&#39;b&#39;)==[[&#39;b&#39;,&#39;b&#39;],[&#39;b&#39;,&#39;b&#39;]]
assert fill_between_coords([[&#39;&#39;,&#39;&#39;]],(0,0),(0,1),&#39;a&#39;)==[[&#39;a&#39;,&#39;a&#39;]]
</code></pre></div>

<h1>B.2. Conditional Functions</h1>
<p>This is the way we prompted GPT-4 to understand the format for the conditional functions.</p>
<p>Each helper function can be conditional.
The conditions can be:</p>
<ul>
<li>by attribute, such as shape, color, position, size, cell number of object</li>
<li>the condition can be an attribute on all objects, for instance, objects with the most common or least common value, or objects with the most or least common shape</li>
<li>by position of pixels, such as row or column</li>
<li>by neighbouring cell types or values</li>
</ul>
<p>There are some conditional functions to help you.</p>
<ul>
<li>object_contains_color(obj, value): returns True/False if object contains a certain value</li>
<li>on_same_line(coord_1, coord_2): Returns True/False if coord_1 is on the same line as coord_2. line_type can be one of ['row', 'col', 'diag']</li>
</ul>
<p>This is the way we one-shot prompted GPT-4 for the conditional functions.</p>
<div class="codehilite"><pre><span></span><code>assert object_contains_color({&#39;tl&#39;:(0,0),&#39;grid&#39;:[[&#39;a&#39;]]},&#39;a&#39;)==True
assert on_same_line((1,1),(1,2),&#39;row&#39;)==True
assert on_same_line((1,1),(2,1),&#39;col&#39;)==True
assert on_same_line((1,1),(2,2),&#39;diag&#39;)==True
</code></pre></div>

<h1>C. Abstraction Views</h1>
<p>This section details how each abstraction view is represented.
Grid View - This provides the entire grid in ASCII format, in a 2D array. This is used when there are arbitrary patterns to find out. Note that we replace the initial JSON task input of grids with 0-9, with grids of '',','a','b','c','d','e','f','g','h','i'. This not only serves to prevent GPT-4 from trying to perform arithmetic on pixel values, but also eradicates the possibility that GPT-4 has seen the public training or evaluation set online, because it is of a different form.</p>
<p>Object View - This provides GPT-4 with an input view that groups contiguous cells of the same or different colour (non-zero cells) together and gives their attributes - top left coordinate, tight-fitted grid layout, shape, size, cell count. This is used when we would like to manipulate groups of related cells as one object.</p>
<p>Within object view, there are a few different types of categorising into objects. This is loosely inspired from Michael Hodel's DSL for ARC from https://github.com/michaelhodel/arc-dsl. We also have an option "more_info", which helps with reducing context length when set to False. All in all, there are 10 different kinds of Object Views.</p>
<ol>
<li>Attribute 1 - Colour (2 possibilities): Mono Colour vs Multi Colour - One way is to form objects is by grouping contiguous cells of the same connected either horizontally or vertically. The other way is to group contiguous cells of any colour except of colour '.' together. These two ways are termed "Mono Colour" and "Multi Colour" respectively.</li>
<li>Attribute 2 - Constrains (5 possibilities): None/Row/Column/Colour/Diagonal - Row or column is used when the pattern is confined to just the row or column. Colour is used when we group objects by colour globally. It helps a lot to split a difficult problem into a smaller confined one where GPT-4 can perform association. Diagonal treats contiguous cells as diagonal connections as well.</li>
</ol>
<p>Pixel View - This provides GPT-4 with a view of the pixel value and all the corresponding coordinates, in a dictionary form with the pixel value as the key and the list of coordinates as the value. This is used when the input-output relation involves changing a group of pixels. This can also be used to supplement Object View. The benefit of using pixel view is that relations like shifting, adding or removing pixels at various positions are immediately obvious to GPT-4 as compared to Grid View. The downside though, is that pixel view is not robust to offsets of positions - if the starting grid is not tight fitted and does not start at $(0,0)$, this can lead to problems with mapping.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. A sample grid for an ARC Challenge Task - taken from Task Demonstration 1 of ARC Training Set d037b0a7 Example: For Fig. 5, the abstraction spaces will be represented in text as:</p>
<ol>
<li>Grid View: [['.','','f'],['.','d','f'],['c','d','f']]</li>
<li>Object View (Mono-Color): [{'tl':(0,2), 'grid':[['f'],['f'],['f'}], 'size':(3,1), 'cell_count':3, 'shape':[['x'],['x'],['x']]}, {'tl':(1,1), 'grid':[['d'],['d']], 'size':(2,1), 'cell_count':2, 'shape':[['x'],['x']]}, {'tl':(2,0), 'grid':[['c']], 'size':(1,1), 'cell_count':1, 'shape':[['x']]}]</li>
<li>Pixel View: {'f':[(0,2),(1,2),(2,2)], 'd':[(1,1),(2,1)], 'c':[(2,0) }</li>
</ol>
<h1>D. GPT-4 Output Examples</h1>
<p>In this section, we showcase GPT-4's output for a solved problem, and the output for an unsolved problem.</p>
<h2>D.1. GPT-4's Output for Solved Problems</h2>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. Training and Test Input/Output pairs for a Solved Problem, d0f5fe59, using Object View</p>
<p>This is the JSON output of GPT-4 formatted into key-value pairs, and the "python_program" provided correctly solves the Test Input.
reflection: This task involves objects found in the grid which are simplified and moved to a certain pattern in the output grid. The input grids contain a variety of object shapes, some of which are isolated pieces while others are complex shapes. These objects, regardless of their original form, are reduced to a single cell in the output grid and relocated to the diagonal line from the top-left to the bottom-right.
pixel_changes: In the transformation from input to output, the number of pixels with color 'h' decreases. This decrease occurs because every ' $h$ ' color object is condensed to a single pixel in the output. Also, the pixels are relocated, following a diagonal line from top left to bottom right, where the first object is placed at $(0,0)$, the second at $(1,1)$, and so on, regardless of their initial positions.
object_changes: The number of objects remains the same from input to output. However, their size, shape, and positions change significantly. Each object, regardless of its initial shape and size, is simplified to a single pixel in the output grid. The position of the objects follows a specific pattern in the output: they are relocated to the diagonal line from the top left to the bottom right. The sequence of placement on the diagonal line is according to the order of the objects in the input grid.
helper_functions: The relevant helper functions for this task are get_objects() for identifying objects in the input grid, fill_value() for placing single pixels at specific locations in the output grid, and empty_grid() for initializing the output grid.
overall_pattern: The task takes an input grid containing a number of objects, simplifies each object into a single cell, and repositions these cells along a diagonal line from the top left to the bottom right. The order of placement on the diagonal is according to the order of the objects in the input grid. The size of the output grid is determined by the number of objects in the input grid.
program_instructions: 1. Use get_objects() function to retrieve all objects from the input grid. 2. Create an empty grid using empty_grid(), where the number of rows and columns equals the number of objects retrieved. 3. Loop through each object obtained from get_objects(). For each object, fill a cell in the new grid with the object color at a diagonal position corresponding to the index of the object in the objects list. This is done by calling fill_value() with the index as both the row and column position.
python_program: def transform_grid(grid): objects = get_objects(grid) new_grid = empty_grid(len(objects), len(objects)) for i, obj in enumerate(objects): new_grid = fill_value(new_grid, (i, i), get_object_color(obj)) return new_grid</p>
<h1>D.2. GPT-4's Output for Unsolved Problems</h1>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7. Training and Test Input/Output pairs for Iteration 1 for an unsolved Problem, 2013d3e2, using Object View. There was a compile error.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8. Training and Test Input/Output pairs for Iteration 2 for an unsolved Problem, 2013d3e2, using Object View. There was an incorrect program output. The run terminated at this iteration.</p>
<p>This is the JSON output of GPT-4 formatted into key-value pairs, for Iteration 1 for 2013d3e2 using Object View, and the "python_program" had a compile error. This is because it tried to crop from the object grid from the center rather than the top left of the object. It also failed to set diagonal $=$ True and multicolor $=$ True in the get_object settings.
reflection: The task involves identifying a specific part of an object, cropping it, and outputting it. The specific part of the object is always three rows high and starts from the third column from the left. It's interesting to note that although the objects in the inputs are of different colors and shapes, the part that needs to be cropped is always at the same location.
pixel_changes: The pixels in the output are a subset of the pixels from the input. They are always the pixels from the third, fourth, and fifth rows of the object, starting from the third column from the left. The colors of the pixels remain unchanged.
object_changes: The size of the object is significantly reduced in the output. It goes from a 6x6 square in the input to a 3 x 3 square in the output. The shape of the object changes as a result of the cropping, but the relative positions of the pixels within the output object remain the same as in the input object.
helper_functions: The following helper functions would be relevant for this task: get_objects, crop_grid, and fill_object.
overall_pattern: The task requires identifying the single object in the input, cropping a 3x3 square from it starting from the third column from the left, and outputting the cropped section. The part of the object that needs to be cropped is always the same and does not depend on the colors or the shape of the object.
program_instructions: 1. Start with an empty grid of size 3x3. 2. Use the get_objects function to identify the object in the input. 3. Crop a 3x3 square from the object starting from the third column from the left using the crop_grid function. 4. Use the fill_object function to fill the empty grid with the cropped section.
python_program: def transform_grid(grid): output = empty_grid(3, 3) objects = get_objects(grid) object = objects[0] cropped = crop_grid(object['grid'], (2, 2), (4, 4)) fill_object(output, {'tl': (0, 0), 'grid': cropped}) return output</p>
<p>This is the JSON output of GPT-4 formatted into key-value pairs, for Iteration 2 for 2013d3e2 using Object View, and the "python_program" produced an incorrect output. It managed to solve the earlier compile error by using a try-catch block and just fill the first object's grid if there is any error. The program is still wrong because it decided to crop the object grid from the center rather than from the top left corner. It also failed to set diagonal=True and multicolor = True in the get_object settings.
reflection: The task involves cropping a section from the input grid and outputting that section as the output grid. The error suggests that the cropping area defined in the function doesn't match the actual area in the object, which results in an out of range index error.
pixel_changes: The pixels in the output are a subset of the pixels in the input. A $3 \times 3$ section from the center of the only object in the input grid is extracted and used to form the output grid.
object_changes: The object in the output grid is a cropped section from the object in the input grid. There are no changes in the values of the pixels, the object simply reduces in size to a $3 \times 3$ grid. The top left cell of the output object corresponds to the cell in the third row and third column of the input object.
helper_functions: The helper functions used for this task are get_objects, crop_grid, and fill_object. get_objects is used to get the object from the input grid. crop_grid is used to crop a $3 \times 3$ section from the input object. fill_object is used to fill the output grid with the cropped object.
overall_pattern: The overall pattern is to identify the only object in the input grid, crop a $3 \times 3$ section from it starting from the third row and third column, and use this cropped section as the output grid.
program_instructions: Start by copying the input grid. Use get_objects to get the object from the input grid. Crop a $3 \times 3$ section from the input object starting from the third row and third column. Create an empty grid of size $3 \times 3$. Fill this grid with the cropped object and return it as the output. The python function will have to handle any exception to avoid out of range index error when the object is smaller than expected.
python_program: def transform_grid(grid): output = empty_grid(3, 3) objects = get_objects(grid) object = objects[0] try: cropped = crop_grid(object['grid'], (2, 2), (4, 4)) except IndexError: cropped = object['grid'] fill_object(output, 'tl': $(0,0)$, 'grid': cropped) return output</p>
<h1>E. Task Solved Details</h1>
<p>In this section, we provide a detailed breakdown of tasks solved in the main paper. For more details, check out the .html files provided in the Supplementary Materials.</p>
<p>Table 4 shows the breakdown of the view types used to solve the 50 problems. As we can see, a variety of views are required, of which, the most frequently used one is Object View with 23 uses. Pixel View is the next most common with 19 uses. There is 1 task (d23f8c26) which used both Object View and Pixel View. There are some tasks for which Grid View alone is the best - 7 of them. Using only one single view is bound to get poorer results than using a combination of all these views. Some examples of tasks solved by the various views are given in Figs. 10, 9, 11 and 12.</p>
<p>Table 4. Number of Tasks solved by each View Type. Grid View is used by default unless there is a limitation of the token length, in which case, it is toggled off</p>
<table>
<thead>
<tr>
<th style="text-align: center;">View Type</th>
<th style="text-align: center;">Number of Tasks Solved</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Total</td>
<td style="text-align: center;">50</td>
</tr>
<tr>
<td style="text-align: center;">Object View</td>
<td style="text-align: center;">23</td>
</tr>
<tr>
<td style="text-align: center;">Pixel View</td>
<td style="text-align: center;">19</td>
</tr>
<tr>
<td style="text-align: center;">Object \&amp; Pixel View</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">No Object \&amp; Pixel View (only Grid View)</td>
<td style="text-align: center;">7</td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Department of Electrical and Computer Engineering, National University of Singapore. Correspondence to: John Tan Chong Min <a href="mailto:johntancm@u.nus.edu">johntancm@u.nus.edu</a>, Mehul Motani <a href="mailto:motani@u.nus.edu">motani@u.nus.edu</a>.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9847 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9847</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9847</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-167.html">extraction-schema-167</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-9c4ebdcf3bdfed0ed9b2f0fea5ba6f8fea49c632</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/9c4ebdcf3bdfed0ed9b2f0fea5ba6f8fea49c632" target="_blank">Large Language Models for Scientific Synthesis, Inference and Explanation</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work presents a method for using general-purpose large language models to make inferences from scientific datasets of the form usually associated with special-purpose machine learning algorithms, and shows that the large language model can augment this knowledge by synthesizing from the scientific literature.</p>
                <p><strong>Paper Abstract:</strong> Large language models are a form of artificial intelligence systems whose primary knowledge consists of the statistical patterns, semantic relationships, and syntactical structures of language1. Despite their limited forms of"knowledge", these systems are adept at numerous complex tasks including creative writing, storytelling, translation, question-answering, summarization, and computer code generation. However, they have yet to demonstrate advanced applications in natural science. Here we show how large language models can perform scientific synthesis, inference, and explanation. We present a method for using general-purpose large language models to make inferences from scientific datasets of the form usually associated with special-purpose machine learning algorithms. We show that the large language model can augment this"knowledge"by synthesizing from the scientific literature. When a conventional machine learning system is augmented with this synthesized and inferred knowledge it can outperform the current state of the art across a range of benchmark tasks for predicting molecular properties. This approach has the further advantage that the large language model can explain the machine learning system's predictions. We anticipate that our framework will open new avenues for AI to accelerate the pace of scientific discovery.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9847.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9847.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM4SD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Language Models for Scientific Discovery (LLM4SD) pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A four-stage pipeline that leverages pre-trained LLMs to (1) synthesize knowledge from the scientific literature, (2) infer empirical rules from labeled datasets, (3) convert rules into measurable features to train interpretable models, and (4) generate textual explanations of predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLM4SD (uses open-source LLM backbones: Galactica-6.7b, Galactica-30b, Falcon-7b, Falcon-40b)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A meta-system that prompts and orchestrates pre-trained LLM backbones to extract measurable rules from their pretraining knowledge (literature synthesis) and to infer rules from labeled examples (data inference); rules are converted to feature functions (numerical/categorical) used to train interpretable models (e.g., random forest, linear models); LLMs then produce human-readable explanations using model outputs and feature importances.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>No bespoke corpus assembled by authors; LLM4SD relies on the pretraining corpora of the underlying LLM backbones (models pretrained on large web and scientific text collections, including arXiv and Wikipedia for some models, and models pretrained predominantly on scientific literature for others). The pipeline also uses task datasets of SMILES-labeled molecules (58 tasks across four domains) as inputs for the inference stage.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Distill domain-relevant molecular property prediction rules and features to predict 58 molecular property tasks across Physiology, Biophysics, Physical Chemistry and Quantum Mechanics (e.g., BBB penetration, toxicity endpoints, solubility, QM properties).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Two complementary LLM-driven processes: (a) Knowledge synthesis from literature — role-play prompt instructing the LLM to act as an experienced chemist and list measurable features/rules (numerical or categorical) relevant to predicting a target property; (b) Knowledge inference from data — present the LLM with several batches of labeled SMILES instances and ask it to extract patterns/rules that discriminate labels or predict continuous values; deduplicate and summarize rules across batches using the LLM's summarization capability; convert each rule into a deterministic feature function for model training.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured list of measurable rules/features (numerical/categorical), feature functions applied to each instance producing vectors, trained interpretable predictive models, and textual instance-level explanations summarizing feature contributions.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>Example synthesized/inferred features: molecular weight, logP (lipophilicity), topological polar surface area, number of hydrogen bond donors/acceptors, presence of carbonyl functional groups, fragment ring counts; textual explanation describing which rules and feature importances led to a molecule being predicted BBB-permeable.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Downstream predictive evaluation across 58 benchmark tasks using standard metrics (AUC-ROC for classification tasks in Physiology/Biophysics; RMSE for Physical Chemistry; MAE for Quantum Mechanics), ablation studies comparing LLM backbones and combinations of synthesis vs inference-derived features; statistical validation of individual rules with Mann-Whitney U test for classification and linear regression t-test for regression (p<0.05); literature review with domain experts to check whether synthesized/inferred rules appear in existing literature.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>LLM4SD enhanced models outperformed baselines across domains: Physiology AUC-ROC improved to 76.60% (from prior best 74.43%), Biophysics to 83.4% (from 81.7%); Quantum Mechanics average MAE reduced to 5.8233 (baseline 11.2450); Physical Chemistry RMSE 1.28 (baseline 1.57). Statistical validation: ~85% of literature-synthesized rules were statistically significant; ~91.3% of data-inferred rules were statistically significant; literature overlap: ~85% of synthesized rules found in literature, ~74% of inferred rules found in literature while ~17.3% of inferred rules were not identified in existing literature.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Combines literature-derived and data-inferred rules yielding improved predictive performance and interpretability; uses open-source LLM backbones for reproducibility; produces measurable features that feed interpretable models and textual explanations, enabling expert validation; capable of recovering well-known literature rules and inferring statistically significant and sometimes novel (under-documented) rules.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Relies on knowledge contained in LLM pretraining corpora (authors did not assemble a controlled literature corpus or quantify its size); not all backbones perform equally (performance depends on model scale and pretraining domain); scope limited to 58 molecular tasks in this study; potential ethical concerns and misuse highlighted but not exhaustively addressed; two synthesized rules for specific tasks (BACE and Tox21-NR-Ahr) were statistically significant yet absent from literature according to the authors' review, indicating possible gaps or overgeneralization.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Smaller or insufficiently pretrained LLM variants failed or underperformed (e.g., Falcon-7b failed to conduct tasks in physiology and quantum mechanics); Galactica 30b outperformed Galactica 6.7b notably in quantum mechanics (~14% margin), showing scale/domain-pretraining sensitivity; some synthesized rules lacked literature support for BACE and Tox21-NR-Ahr; inferred rules produced statistically significant but previously undocumented features that require further mechanistic validation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Scientific Synthesis, Inference and Explanation', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9847.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9847.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Knowledge Synthesis (literature)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge Synthesis from Scientific Literature (LLM component)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-driven process that mines the model's pretraining knowledge (scientific literature encoded in the LLM) to synthesize domain-specific, measurable molecular property prediction rules (features) without directly analyzing task data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Pretrained LLMs used for synthesis (examples: Galactica-6.7b / Galactica-30b, Falcon-40b)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompt-based role-playing where the LLM is instructed to adopt the persona of an experienced chemist and produce measurable rules/features (numerical or categorical) relevant to a target prediction task; outputs are required to be transcribable into deterministic feature functions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Implicit: the LLM's pretraining corpus (for Galactica primarily scientific literature; for Falcon a broader web/scientific mix including arXiv and Wikipedia). The authors did not curate a separate literature corpus for synthesis within the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Identify literature-supported features and domain postulates useful for predicting a specified molecular property (e.g., BBB penetration, toxicity endpoints).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Roleplay prompting to extract lists of measurable rules; outputs constrained to numeric/categorical measures so they can be implemented as feature functions; no retrieval augmentation or explicit cited-passage grounding reported — relies on LLM internalized knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Lists of measurable rules/features (e.g., molecular descriptors) and short textual rationales linking rules to the property.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>For BBBP: 'molecular weight', 'logP (lipophilicity)', 'topological polar surface area', 'number of hydrogen bond donors/acceptors'.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Statistical significance testing of each rule against labels (Mann-Whitney U for classification, linear regression t-test for regression) and cross-check by literature review with domain experts for presence in existing literature.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Approximately 85% of synthesized rules were statistically significant across evaluated tasks; most synthesized rules (≈85%) were found in the scientific literature except for synthesized statistically significant rules in BACE and Tox21-NR-Ahr which lacked literature support per the authors' review.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Efficiently summarizes established literature knowledge into measurable features; high overlap with existing literature for many rules; produces features that can directly feed interpretable models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Depends on the content and biases of the LLM pretraining corpus and does not provide direct citations or provenance for synthesized rules; two cases showed synthesized statistically significant rules lacking literature support, indicating possible incorrect generalization or gaps in literature review.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Occasional synthesized rules that are statistically significant but not found in the literature (BACE, Tox21-NR-Ahr), raising questions about provenance and correctness of some synthesized claims.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Scientific Synthesis, Inference and Explanation', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9847.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9847.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Knowledge Inference (data)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge Inference from Data (LLM component)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-driven process where the model is given labeled batches of instances (SMILES + labels or values) and asked to infer empirical rules/features that discriminate classes or predict properties, followed by deduplication and summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Pretrained LLMs used for inference (examples: Galactica-6.7b / Galactica-30b, Falcon-40b)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompt-based approach: LLMs receive sampled labeled examples and task instructions to analyze patterns and propose measurable rules; rules from multiple batches are summarized and deduplicated by the LLM to form final rule sets; required that proposed rules be expressible as numerical or categorical functions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Task-specific labeled datasets of molecular instances (SMILES) used for the 58 tasks (e.g., BBBP: 2,039 instances; Tox21: 7,831; QM9: 133,885), provided to the LLM in sampled batches during the inference stage.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td>58</td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Discover empirical regularities and measurable molecular features from labeled examples that help predict the target property for each task.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Provide the LLM with multiple batches of example inputs and labels; instruct it to infer discriminative/predictive rules for each batch; aggregate and summarize rules across batches using the LLM's summarization capability; transcribe rules into deterministic feature-extraction functions.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>List of inferred measurable rules/features, with associated textual rationales; these are converted to features for downstream model training.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>Inferred rules included obscure substructures or second-order features such as presence of carbonyl functional groups and fragment rings implicated in BBB penetration, which authors hypothesize affect molecular cross-sectional area and membrane partitioning.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Statistical tests on each rule (Mann-Whitney U for classification; linear regression t-test for regression) with p<0.05, literature review to check novelty and presence in prior work, and evaluation of downstream model performance when including inferred features.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>On average 91.3% of inferred rules were statistically significant; ~74% of inferred rules matched literature findings while ~17.3% were not documented in the literature (potentially novel). Combining inferred and synthesized rule features produced the best downstream predictive performance across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>High statistical significance rate for inferred rules; capable of surfacing less-documented or novel but statistically robust features ('second-order' features) that may inspire further research; complements literature synthesis to improve performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Some inferred rules are not present in literature and require mechanistic validation; quality depends on sample selection and the LLM's reasoning on the provided batches; provenance and mechanistic explanation for novel inferred rules may be limited without further study.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Inferred features sometimes correspond to obscure substructures or dataset-specific artifacts that lack mechanistic explanation or prior literature support; such features require careful experimental/biochemical validation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Scientific Synthesis, Inference and Explanation', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9847.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9847.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Galactica-6.7b / Galactica-30b</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Galactica (6.7B and 30B parameter variants)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source LLM family pretrained primarily on scientific literature; used in this work as a backbone for both literature synthesis and data inference in the LLM4SD pipeline, with Galactica-6.7b selected for rule validation due to performance and reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Galactica: A large language model for science.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Galactica-6.7b / Galactica-30b</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Galactica models are transformer-based LLMs pretrained predominantly on scientific texts to encode domain-specific knowledge useful for scientific tasks; the paper used two sizes: 6.7B parameters and 30B parameters, observing different scaling behaviors across domains (30B outperformed 6.7B notably in Quantum Mechanics).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6.7B / 30B</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Pretrained mainly on scientific literature (paper cites Galactica's scientific-pretraining orientation); authors relied on that internalized literature knowledge for knowledge synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Use Galactica's internalized scientific knowledge to synthesize molecular property rules and to infer rules from data for 58 molecular prediction tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Prompt-based roleplay for literature synthesis and example-based prompting for inference; outputs are lists of measurable rules which are converted into features and validated statistically and via literature review.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Measurable rules/features and textual explanations; in experiments Galactica-derived rules were transcribed to feature functions used to train interpretable models.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>Galactica-generated rules for BBBP included molecular weight, lipophilicity (logP), distribution coefficient, topological polar surface area, hydrogen bonds; Galactica also identified carbonyl groups and fragment rings in some inferred rules.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Statistical significance tests of rules (Mann-Whitney U, t-tests), literature review with domain experts, and downstream task performance comparisons; ablation studies comparing Galactica variants and other LLM backbones.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Galactica series generally outperformed the Falcon series at comparable scales on many scientific tasks; Galactica-6.7b provided a favorable trade-off of performance and reproducibility and was used for rule validation; Galactica-30b outperformed 6.7b by ~14% in Quantum Mechanics tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Domain-specific pretraining on scientific literature yields strong performance on scientific synthesis and inference even at moderate scales; produced many literature-supported rules and statistically significant inferred features.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Scaling does not uniformly improve performance across all domains (6.7b close to 30b on some domains); relies on pretraining content and may reflect biases or gaps in scientific literature coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Galactica-6.7b was sometimes matched or exceeded by larger Galactica-30b in complex domains (Quantum Mechanics), indicating domain-dependent scaling needs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Scientific Synthesis, Inference and Explanation', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9847.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9847.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Falcon-7b / Falcon-40b</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Falcon series (7B and 40B parameter variants)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source general-purpose LLM family pretrained on broad text corpora including scientific content (e.g., arXiv, Wikipedia); used as alternative backbones in the LLM4SD pipeline, showing that generalist pretraining needs larger scale to match domain-specific models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Falcon-40B: an open large language model with state-of-the-art performance.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Falcon-7b / Falcon-40b</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based LLMs pretrained for broad applicability; Falcon models integrate broad web and scientific sources (e.g., arXiv, Wikipedia) and require larger parameter counts to exhibit emergent scientific capabilities compared to domain-pretrained models.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B / 40B</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Pretrained on general large corpora including scientific sources such as arXiv and Wikipedia, but not specialized solely on scientific literature.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Use Falcon backbones within LLM4SD to synthesize and infer molecular prediction rules and produce features for interpretable models across the 58 tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Same LLM4SD prompting and example-based procedures (roleplay for synthesis, batch example prompting for inference) applied to Falcon variants; ablation comparing performance across backbones and scales.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Lists of measurable rules/features and textual explanations, used to derive feature functions for interpretable model training.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>When large enough (Falcon-40b), Falcon produced competitive rule sets; smaller Falcon-7b often failed to produce useful results in some domains (physiology and quantum mechanics).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Ablation comparisons across backbones and sizes, downstream benchmarks across the 58 tasks, and statistical/literature validation of rules.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Falcon-40b (larger model) bridged the gap and performed better than Falcon-7b; overall Galactica models (domain-pretrained) outperformed Falcon series at comparable scale, highlighting importance of pretraining domain. Falcon-7b failed on some tasks (physiology and quantum mechanics).</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>General-purpose pretraining allows flexibility across many tasks; larger-scale Falcon (40B) can acquire emergent scientific capabilities without strict scientific-only pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Smaller Falcon variants struggle on scientific tasks; performance depends strongly on scale; general pretraining may require more parameters to match domain-specialized models.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Falcon-7b failed to conduct tasks in physiology and quantum mechanics in the experiments; shows risk of inadequate capacity when using generalist LLMs for domain-rich synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Scientific Synthesis, Inference and Explanation', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Galactica: A large language model for science. <em>(Rating: 2)</em></li>
                <li>Falcon-40B: an open large language model with state-of-the-art performance. <em>(Rating: 2)</em></li>
                <li>Can ChatGPT be used to generate scientific hypotheses?. <em>(Rating: 1)</em></li>
                <li>Large language models encode clinical knowledge. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9847",
    "paper_id": "paper-9c4ebdcf3bdfed0ed9b2f0fea5ba6f8fea49c632",
    "extraction_schema_id": "extraction-schema-167",
    "extracted_data": [
        {
            "name_short": "LLM4SD",
            "name_full": "Large Language Models for Scientific Discovery (LLM4SD) pipeline",
            "brief_description": "A four-stage pipeline that leverages pre-trained LLMs to (1) synthesize knowledge from the scientific literature, (2) infer empirical rules from labeled datasets, (3) convert rules into measurable features to train interpretable models, and (4) generate textual explanations of predictions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLM4SD (uses open-source LLM backbones: Galactica-6.7b, Galactica-30b, Falcon-7b, Falcon-40b)",
            "model_description": "A meta-system that prompts and orchestrates pre-trained LLM backbones to extract measurable rules from their pretraining knowledge (literature synthesis) and to infer rules from labeled examples (data inference); rules are converted to feature functions (numerical/categorical) used to train interpretable models (e.g., random forest, linear models); LLMs then produce human-readable explanations using model outputs and feature importances.",
            "model_size": null,
            "input_corpus_description": "No bespoke corpus assembled by authors; LLM4SD relies on the pretraining corpora of the underlying LLM backbones (models pretrained on large web and scientific text collections, including arXiv and Wikipedia for some models, and models pretrained predominantly on scientific literature for others). The pipeline also uses task datasets of SMILES-labeled molecules (58 tasks across four domains) as inputs for the inference stage.",
            "input_corpus_size": null,
            "topic_query_description": "Distill domain-relevant molecular property prediction rules and features to predict 58 molecular property tasks across Physiology, Biophysics, Physical Chemistry and Quantum Mechanics (e.g., BBB penetration, toxicity endpoints, solubility, QM properties).",
            "distillation_method": "Two complementary LLM-driven processes: (a) Knowledge synthesis from literature — role-play prompt instructing the LLM to act as an experienced chemist and list measurable features/rules (numerical or categorical) relevant to predicting a target property; (b) Knowledge inference from data — present the LLM with several batches of labeled SMILES instances and ask it to extract patterns/rules that discriminate labels or predict continuous values; deduplicate and summarize rules across batches using the LLM's summarization capability; convert each rule into a deterministic feature function for model training.",
            "output_type": "Structured list of measurable rules/features (numerical/categorical), feature functions applied to each instance producing vectors, trained interpretable predictive models, and textual instance-level explanations summarizing feature contributions.",
            "output_example": "Example synthesized/inferred features: molecular weight, logP (lipophilicity), topological polar surface area, number of hydrogen bond donors/acceptors, presence of carbonyl functional groups, fragment ring counts; textual explanation describing which rules and feature importances led to a molecule being predicted BBB-permeable.",
            "evaluation_method": "Downstream predictive evaluation across 58 benchmark tasks using standard metrics (AUC-ROC for classification tasks in Physiology/Biophysics; RMSE for Physical Chemistry; MAE for Quantum Mechanics), ablation studies comparing LLM backbones and combinations of synthesis vs inference-derived features; statistical validation of individual rules with Mann-Whitney U test for classification and linear regression t-test for regression (p&lt;0.05); literature review with domain experts to check whether synthesized/inferred rules appear in existing literature.",
            "evaluation_results": "LLM4SD enhanced models outperformed baselines across domains: Physiology AUC-ROC improved to 76.60% (from prior best 74.43%), Biophysics to 83.4% (from 81.7%); Quantum Mechanics average MAE reduced to 5.8233 (baseline 11.2450); Physical Chemistry RMSE 1.28 (baseline 1.57). Statistical validation: ~85% of literature-synthesized rules were statistically significant; ~91.3% of data-inferred rules were statistically significant; literature overlap: ~85% of synthesized rules found in literature, ~74% of inferred rules found in literature while ~17.3% of inferred rules were not identified in existing literature.",
            "strengths": "Combines literature-derived and data-inferred rules yielding improved predictive performance and interpretability; uses open-source LLM backbones for reproducibility; produces measurable features that feed interpretable models and textual explanations, enabling expert validation; capable of recovering well-known literature rules and inferring statistically significant and sometimes novel (under-documented) rules.",
            "limitations": "Relies on knowledge contained in LLM pretraining corpora (authors did not assemble a controlled literature corpus or quantify its size); not all backbones perform equally (performance depends on model scale and pretraining domain); scope limited to 58 molecular tasks in this study; potential ethical concerns and misuse highlighted but not exhaustively addressed; two synthesized rules for specific tasks (BACE and Tox21-NR-Ahr) were statistically significant yet absent from literature according to the authors' review, indicating possible gaps or overgeneralization.",
            "failure_cases": "Smaller or insufficiently pretrained LLM variants failed or underperformed (e.g., Falcon-7b failed to conduct tasks in physiology and quantum mechanics); Galactica 30b outperformed Galactica 6.7b notably in quantum mechanics (~14% margin), showing scale/domain-pretraining sensitivity; some synthesized rules lacked literature support for BACE and Tox21-NR-Ahr; inferred rules produced statistically significant but previously undocumented features that require further mechanistic validation.",
            "uuid": "e9847.0",
            "source_info": {
                "paper_title": "Large Language Models for Scientific Synthesis, Inference and Explanation",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Knowledge Synthesis (literature)",
            "name_full": "Knowledge Synthesis from Scientific Literature (LLM component)",
            "brief_description": "An LLM-driven process that mines the model's pretraining knowledge (scientific literature encoded in the LLM) to synthesize domain-specific, measurable molecular property prediction rules (features) without directly analyzing task data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Pretrained LLMs used for synthesis (examples: Galactica-6.7b / Galactica-30b, Falcon-40b)",
            "model_description": "Prompt-based role-playing where the LLM is instructed to adopt the persona of an experienced chemist and produce measurable rules/features (numerical or categorical) relevant to a target prediction task; outputs are required to be transcribable into deterministic feature functions.",
            "model_size": null,
            "input_corpus_description": "Implicit: the LLM's pretraining corpus (for Galactica primarily scientific literature; for Falcon a broader web/scientific mix including arXiv and Wikipedia). The authors did not curate a separate literature corpus for synthesis within the experiments.",
            "input_corpus_size": null,
            "topic_query_description": "Identify literature-supported features and domain postulates useful for predicting a specified molecular property (e.g., BBB penetration, toxicity endpoints).",
            "distillation_method": "Roleplay prompting to extract lists of measurable rules; outputs constrained to numeric/categorical measures so they can be implemented as feature functions; no retrieval augmentation or explicit cited-passage grounding reported — relies on LLM internalized knowledge.",
            "output_type": "Lists of measurable rules/features (e.g., molecular descriptors) and short textual rationales linking rules to the property.",
            "output_example": "For BBBP: 'molecular weight', 'logP (lipophilicity)', 'topological polar surface area', 'number of hydrogen bond donors/acceptors'.",
            "evaluation_method": "Statistical significance testing of each rule against labels (Mann-Whitney U for classification, linear regression t-test for regression) and cross-check by literature review with domain experts for presence in existing literature.",
            "evaluation_results": "Approximately 85% of synthesized rules were statistically significant across evaluated tasks; most synthesized rules (≈85%) were found in the scientific literature except for synthesized statistically significant rules in BACE and Tox21-NR-Ahr which lacked literature support per the authors' review.",
            "strengths": "Efficiently summarizes established literature knowledge into measurable features; high overlap with existing literature for many rules; produces features that can directly feed interpretable models.",
            "limitations": "Depends on the content and biases of the LLM pretraining corpus and does not provide direct citations or provenance for synthesized rules; two cases showed synthesized statistically significant rules lacking literature support, indicating possible incorrect generalization or gaps in literature review.",
            "failure_cases": "Occasional synthesized rules that are statistically significant but not found in the literature (BACE, Tox21-NR-Ahr), raising questions about provenance and correctness of some synthesized claims.",
            "uuid": "e9847.1",
            "source_info": {
                "paper_title": "Large Language Models for Scientific Synthesis, Inference and Explanation",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Knowledge Inference (data)",
            "name_full": "Knowledge Inference from Data (LLM component)",
            "brief_description": "An LLM-driven process where the model is given labeled batches of instances (SMILES + labels or values) and asked to infer empirical rules/features that discriminate classes or predict properties, followed by deduplication and summarization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Pretrained LLMs used for inference (examples: Galactica-6.7b / Galactica-30b, Falcon-40b)",
            "model_description": "Prompt-based approach: LLMs receive sampled labeled examples and task instructions to analyze patterns and propose measurable rules; rules from multiple batches are summarized and deduplicated by the LLM to form final rule sets; required that proposed rules be expressible as numerical or categorical functions.",
            "model_size": null,
            "input_corpus_description": "Task-specific labeled datasets of molecular instances (SMILES) used for the 58 tasks (e.g., BBBP: 2,039 instances; Tox21: 7,831; QM9: 133,885), provided to the LLM in sampled batches during the inference stage.",
            "input_corpus_size": 58,
            "topic_query_description": "Discover empirical regularities and measurable molecular features from labeled examples that help predict the target property for each task.",
            "distillation_method": "Provide the LLM with multiple batches of example inputs and labels; instruct it to infer discriminative/predictive rules for each batch; aggregate and summarize rules across batches using the LLM's summarization capability; transcribe rules into deterministic feature-extraction functions.",
            "output_type": "List of inferred measurable rules/features, with associated textual rationales; these are converted to features for downstream model training.",
            "output_example": "Inferred rules included obscure substructures or second-order features such as presence of carbonyl functional groups and fragment rings implicated in BBB penetration, which authors hypothesize affect molecular cross-sectional area and membrane partitioning.",
            "evaluation_method": "Statistical tests on each rule (Mann-Whitney U for classification; linear regression t-test for regression) with p&lt;0.05, literature review to check novelty and presence in prior work, and evaluation of downstream model performance when including inferred features.",
            "evaluation_results": "On average 91.3% of inferred rules were statistically significant; ~74% of inferred rules matched literature findings while ~17.3% were not documented in the literature (potentially novel). Combining inferred and synthesized rule features produced the best downstream predictive performance across domains.",
            "strengths": "High statistical significance rate for inferred rules; capable of surfacing less-documented or novel but statistically robust features ('second-order' features) that may inspire further research; complements literature synthesis to improve performance.",
            "limitations": "Some inferred rules are not present in literature and require mechanistic validation; quality depends on sample selection and the LLM's reasoning on the provided batches; provenance and mechanistic explanation for novel inferred rules may be limited without further study.",
            "failure_cases": "Inferred features sometimes correspond to obscure substructures or dataset-specific artifacts that lack mechanistic explanation or prior literature support; such features require careful experimental/biochemical validation.",
            "uuid": "e9847.2",
            "source_info": {
                "paper_title": "Large Language Models for Scientific Synthesis, Inference and Explanation",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Galactica-6.7b / Galactica-30b",
            "name_full": "Galactica (6.7B and 30B parameter variants)",
            "brief_description": "Open-source LLM family pretrained primarily on scientific literature; used in this work as a backbone for both literature synthesis and data inference in the LLM4SD pipeline, with Galactica-6.7b selected for rule validation due to performance and reproducibility.",
            "citation_title": "Galactica: A large language model for science.",
            "mention_or_use": "use",
            "model_name": "Galactica-6.7b / Galactica-30b",
            "model_description": "Galactica models are transformer-based LLMs pretrained predominantly on scientific texts to encode domain-specific knowledge useful for scientific tasks; the paper used two sizes: 6.7B parameters and 30B parameters, observing different scaling behaviors across domains (30B outperformed 6.7B notably in Quantum Mechanics).",
            "model_size": "6.7B / 30B",
            "input_corpus_description": "Pretrained mainly on scientific literature (paper cites Galactica's scientific-pretraining orientation); authors relied on that internalized literature knowledge for knowledge synthesis.",
            "input_corpus_size": null,
            "topic_query_description": "Use Galactica's internalized scientific knowledge to synthesize molecular property rules and to infer rules from data for 58 molecular prediction tasks.",
            "distillation_method": "Prompt-based roleplay for literature synthesis and example-based prompting for inference; outputs are lists of measurable rules which are converted into features and validated statistically and via literature review.",
            "output_type": "Measurable rules/features and textual explanations; in experiments Galactica-derived rules were transcribed to feature functions used to train interpretable models.",
            "output_example": "Galactica-generated rules for BBBP included molecular weight, lipophilicity (logP), distribution coefficient, topological polar surface area, hydrogen bonds; Galactica also identified carbonyl groups and fragment rings in some inferred rules.",
            "evaluation_method": "Statistical significance tests of rules (Mann-Whitney U, t-tests), literature review with domain experts, and downstream task performance comparisons; ablation studies comparing Galactica variants and other LLM backbones.",
            "evaluation_results": "Galactica series generally outperformed the Falcon series at comparable scales on many scientific tasks; Galactica-6.7b provided a favorable trade-off of performance and reproducibility and was used for rule validation; Galactica-30b outperformed 6.7b by ~14% in Quantum Mechanics tasks.",
            "strengths": "Domain-specific pretraining on scientific literature yields strong performance on scientific synthesis and inference even at moderate scales; produced many literature-supported rules and statistically significant inferred features.",
            "limitations": "Scaling does not uniformly improve performance across all domains (6.7b close to 30b on some domains); relies on pretraining content and may reflect biases or gaps in scientific literature coverage.",
            "failure_cases": "Galactica-6.7b was sometimes matched or exceeded by larger Galactica-30b in complex domains (Quantum Mechanics), indicating domain-dependent scaling needs.",
            "uuid": "e9847.3",
            "source_info": {
                "paper_title": "Large Language Models for Scientific Synthesis, Inference and Explanation",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Falcon-7b / Falcon-40b",
            "name_full": "Falcon series (7B and 40B parameter variants)",
            "brief_description": "Open-source general-purpose LLM family pretrained on broad text corpora including scientific content (e.g., arXiv, Wikipedia); used as alternative backbones in the LLM4SD pipeline, showing that generalist pretraining needs larger scale to match domain-specific models.",
            "citation_title": "Falcon-40B: an open large language model with state-of-the-art performance.",
            "mention_or_use": "use",
            "model_name": "Falcon-7b / Falcon-40b",
            "model_description": "Transformer-based LLMs pretrained for broad applicability; Falcon models integrate broad web and scientific sources (e.g., arXiv, Wikipedia) and require larger parameter counts to exhibit emergent scientific capabilities compared to domain-pretrained models.",
            "model_size": "7B / 40B",
            "input_corpus_description": "Pretrained on general large corpora including scientific sources such as arXiv and Wikipedia, but not specialized solely on scientific literature.",
            "input_corpus_size": null,
            "topic_query_description": "Use Falcon backbones within LLM4SD to synthesize and infer molecular prediction rules and produce features for interpretable models across the 58 tasks.",
            "distillation_method": "Same LLM4SD prompting and example-based procedures (roleplay for synthesis, batch example prompting for inference) applied to Falcon variants; ablation comparing performance across backbones and scales.",
            "output_type": "Lists of measurable rules/features and textual explanations, used to derive feature functions for interpretable model training.",
            "output_example": "When large enough (Falcon-40b), Falcon produced competitive rule sets; smaller Falcon-7b often failed to produce useful results in some domains (physiology and quantum mechanics).",
            "evaluation_method": "Ablation comparisons across backbones and sizes, downstream benchmarks across the 58 tasks, and statistical/literature validation of rules.",
            "evaluation_results": "Falcon-40b (larger model) bridged the gap and performed better than Falcon-7b; overall Galactica models (domain-pretrained) outperformed Falcon series at comparable scale, highlighting importance of pretraining domain. Falcon-7b failed on some tasks (physiology and quantum mechanics).",
            "strengths": "General-purpose pretraining allows flexibility across many tasks; larger-scale Falcon (40B) can acquire emergent scientific capabilities without strict scientific-only pretraining.",
            "limitations": "Smaller Falcon variants struggle on scientific tasks; performance depends strongly on scale; general pretraining may require more parameters to match domain-specialized models.",
            "failure_cases": "Falcon-7b failed to conduct tasks in physiology and quantum mechanics in the experiments; shows risk of inadequate capacity when using generalist LLMs for domain-rich synthesis.",
            "uuid": "e9847.4",
            "source_info": {
                "paper_title": "Large Language Models for Scientific Synthesis, Inference and Explanation",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Galactica: A large language model for science.",
            "rating": 2
        },
        {
            "paper_title": "Falcon-40B: an open large language model with state-of-the-art performance.",
            "rating": 2
        },
        {
            "paper_title": "Can ChatGPT be used to generate scientific hypotheses?.",
            "rating": 1
        },
        {
            "paper_title": "Large language models encode clinical knowledge.",
            "rating": 1
        }
    ],
    "cost": 0.01554375,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Large Language Models for Scientific Synthesis, Inference and Explanation</h1>
<p>Yizhen Zheng ${ }^{1 * * <em>}$, Huan Yee Koh ${ }^{1,3 </em>}$, Jiaxin Ju ${ }^{2 <em>}$, Anh T.N. Nguyen ${ }^{3}$, Lauren T. May ${ }^{3,4}$, Geoffrey I. Webb ${ }^{1 \infty}$, Shirui Pan ${ }^{2 \infty}$<br>${ }^{1}$ Department of Data Science and Artificial Intelligence, Monash University, Victoria, Australia<br>${ }^{2}$ School of Information and Communication Technology and Institute for Integrated and Intelligent Systems, Griffith University, Queensland, Australia<br>${ }^{3}$ Drug Discovery Biology, Monash Institute of Pharmaceutical Sciences, Monash University, Victoria, Australia<br>${ }^{4}$ Victorian Heart Institute, Monash University, Victoria, Australia<br></em>indicates equal contribution.<br>${ }^{\infty}$ indicates corresponding authors: Shirui Pan(s.pan@griffith.edu.au), Geoffrey I. Webb(Geoff.webb@monash.edu), Yizhen Zheng(yizhen.zheng1@monash.edu).</p>
<h4>Abstract</h4>
<p>Large language models are a form of artificial intelligence systems whose primary knowledge consists of the statistical patterns, semantic relationships, and syntactical structures of language ${ }^{1}$. Despite their limited forms of 'knowledge,' these systems are adept at numerous complex tasks including creative writing, storytelling, translation, question-answering, summarization, and computer code generation ${ }^{2,3}$. However, they have yet to demonstrate advanced applications in natural science ${ }^{4,5}$. Here we show how large language models can perform scientific synthesis, inference, and explanation. We present a method for using general purpose large language models to make inferences from scientific datasets of the form usually associated with special purpose machine learning algorithms. We show that the large language model can augment this 'knowledge' by synthesizing from the scientific literature. When a conventional machine learning system is augmented with this synthesized and inferred knowledge it can outperform the current state of the art across a range of benchmark tasks for predicting molecular properties. This approach has the further advantage that the large language model can explain the machine learning system's predictions. We anticipate that our framework will open new avenues for AI to accelerate the pace of scientific discovery.</p>
<h2>1. Introduction</h2>
<p>Scientific productivity is in precipitous decline, with the rate of progress in many fields approximately halving every 13 years ${ }^{6}$. As scientific discovery becomes increasingly complex and challenging, traditional methodologies struggle to keep pace, necessitating innovative approaches. Meanwhile, Large Language Model (LLM) Artificial Intelligence systems have shown remarkable capabilities in a wide range of tasks. From creative writing to translating languages, from answering intricate queries ${ }^{7,8}$ to code generation ${ }^{9}$, their capabilities have been transformative in various domains ${ }^{1,2,3,10,11}$. In this work we show that these LLMs have similar transformational potential in the natural sciences. Particularly, we demonstrate that LLMs can</p>
<p>synthesize postulates from the scientific literature, make inferences from scientific data, and elucidate their conclusions with explanations.</p>
<p>LLMs are trained on large corpuses of text, including much of the scientific literature. Notable models like BioBert ${ }^{12}$, SciBERT ${ }^{13}$, Med-PALM ${ }^{11}$, and Galactica ${ }^{14}$ are specifically tailored to the scientific domain. Meanwhile, general-purpose LLMs like Falcon ${ }^{15}$ integrate extensive scientific literature in their pretraining, including sources such as arXiv and Wikipedia. We demonstrate that these systems have acquired deep abilities to interpret and manipulate the formal scientific language for describing molecules, SMILES strings, along with capability to apply information from the scientific literature in their interpretation. We present a scientific discovery pipeline LLM4SD (Large Language Models for Scientific Discovery) designed to tackle complex molecular property prediction tasks. LLM4SD operates by specifying rules for deriving features from SMILES strings that are relevant to predicting a target feature. Some of these rules are synthesized from the scientific literature that the LLMs encode. Others are inferred from training sets of SMILES strings each labelled with the relevant classes or property values. A standard machine learning model can then be learned from the training data using the rule-based features. Finally, our pipeline utilizes LLMs to produce interpretable outcomes, allowing human experts to ascertain the specific factors influencing the final predictions. We show that this pipeline achieves the current state of the art across 58 benchmark tasks spanning four domains Physiology, Biophysics, Physical Chemistry and Quantum Mechanics.</p>
<p>Despite these auspicious outcomes, we acknowledge the vastness and intricacy of the scientific discovery landscape; our endeavours have merely scratched the surface. Nonetheless, the strides made by LLM4SD pave the way for deeper exploration, heralding an era where AI-driven insights interweave with human ingenuity to redress the current decline in scientific productivity. Looking ahead, we are optimistic about AI's potential role as a linchpin in the future of scientific discovery, revolutionizing processes and expediting breakthroughs.</p>
<h1>2. Large Language Models for Scientific Discovery</h1>
<p>Our scientific discovery pipeline, LLM4SD, shown in Fig. 1 consists of 4 main components: Knowledge Synthesis from the Scientific Literature, Knowledge Inference from Data, Interpretable Model Training and Interpretable Explanation Generation. We demonstrate the application of our pipeline to 58 specialized property prediction tasks across four scientific domains: Physiology, Biophysics, Physical Chemistry, and Quantum Mechanics.</p>
<p>In the Knowledge Synthesis from Literature phase (Fig. 1a), LLMs use pre-trained knowledge from an extensive literature amassed from LLMs' pretraining ${ }^{14,15}$ to synthesize domain-specific molecular property prediction rules. Then, in the Knowledge Inference from Data phase (Fig. 1b), LLMs harness their inferential and analytical skills to infer molecular property prediction rules from the patterns in the datasets. These rules can generate features that effectively distinguish between different class instances or predict specific properties, such as a molecule's lipophilicity. This process mirrors how human scientists formulate hypotheses based on observation. In both the knowledge synthesis and inference stages, we require that the rules have either a numerical or</p>
<p>categorical measure associated with them. This ensures that the rules can be readily transformed into corresponding functions, which in turn can convert each data instance into a vector of values.</p>
<p>Rules, independently defined by LLMs, transform data instances into vectorized representations, i.e., features. These rule-based features facilitate the training of an interpretable model, e.g., random forest or linear classifier (Fig. 1c). Our preference for training these interpretable models stems from a desire to enhance transparency during predictions. Remarkably, we noted that when enhanced with LLM4SD, traditional interpretable models like random forests can surpass state-of-the-art baselines. These interpretable models, once trained, are adeptly employed for downstream application, encompassing both classification and regression scientific tasks. This entire workflow draws parallels with the methodical approach of human scientists-designing experiments to validate their proposed hypotheses.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig.1. LLM for Scientific Discovery Pipeline. (a) Knowledge synthesis from the scientific literature. In this phase, LLMs leverage their pre-trained understanding, amassed from pretraining on a massive body of literature, to synthesize domain-specific rules. (b) Knowledge inference from data. LLMs analyse the intrinsic patterns in the task-specific datasets to identify labelled patterns and infer empirical rules. (c) Interpretable Model Training. Rules formulated from LLMs are harnessed to convert data instances into features, enabling the development of models that are both effective and readily interpretable. (d) Interpretable Explanation Generation. Culminating the process, LLMs assimilate insights from the preceding steps to articulate comprehensive textual explanations, elucidating the rationales behind predictions. In the figure, the percentage value for each factor indicates its importance in concluding the prediction.</p>
<p>In the final stages (Fig. 1d), we tap into the LLMs' adeptness at information summarization. They are tasked to demystify the decision-making mechanism, illuminating how these interpretable models arrive at prediction outcomes based on instance representations, rules, and their respective significance. This clarity and transparency positions LLMs as intuitive partners, enabling scientists to seamlessly interface with and derive insights from the system's decisionmaking processes. To improve usability for researchers, we have created a web-based application that offers knowledge synthesis, inference, and prediction with explanation functions (see Supplementary Information 3).</p>
<p>By fostering this symbiotic relationship, we not only amplify the efficacy of scientific investigations but also elevate the confidence and trust in AI-assisted conclusions, driving forward the frontier of collaborative research.</p>
<h1>3. Experiment Results</h1>
<p>In this section, we offer a synopsis of LLM4SD's pivotal results spanning the 4 domains of physiology, biophysics, quantum mechanics and physical chemistry. Notably, all results of LLM4SD are obtained based on open-source LLM backbones to ensure reproducibility. Subsequently, we delved into an ablation study of LLM4SD, examining its performance across various LLM backbones ${ }^{14,15}$ of differing scales and pretraining datasets.</p>
<h3>3.1 Overall Performance on Four Domains</h3>
<p>To evaluate the versatility of LLM4SD's application, we conducted a comprehensive analysis of its performance across 58 molecular prediction tasks across the 4 domains_(Fig.2). Specifically, the physiology domain comprised (Blood-Brain Barrier Penetration) BBBP ${ }^{16}$, ClinTox ${ }^{17}$, Tox $21^{18}$ with 12 tasks, and SIDER ${ }^{19}$ with 27 tasks. Biophysics had two tasks, BACE ${ }^{20}$ and HIV ${ }^{18}$, while physical chemistry had three regression tasks: ESOL ${ }^{21}$, FreeSolv ${ }^{22}$ and Lipophilicity ${ }^{18}$. Quantum mechanics presented 12 regression tasks under QM9 ${ }^{23}$. The detailed description of these tasks is illustrated in the method section (see Methods, 'Datasets'). We compared LL4SD's performance with specialized, state-of-the-art supervised machine learning techniques. These are advanced Graph Neural Networks (GNNs), namely AttrMask ${ }^{24}$, GraphCL ${ }^{25}$, MolCLR ${ }^{26}$, 3DInfomax ${ }^{27}$, GraphMVP ${ }^{28}$, and MoleBERT ${ }^{29}$. Each model was pretrained on large datasets with diverse molecular knowledge and then fine-tuned for specific tasks (see Methods). As a standard baseline, we implemented Random Forest ${ }^{30}$ with ECFP4 ${ }^{31}$ as input set features.
Benchmarking LLM4SD against the baseline, LLM4SD demonstrated its superior efficacy and performance (Fig 2). This exemplary performance spanned 58 diverse tasks, from physiology (Extended Data Fig. 1-3) and biophysics (Extended Data Fig. 4) to physical chemistry (Extended Data Fig. 5) and quantum mechanics (Extended Fig. 6).</p>
<p>In both physiology and biophysics, our model outperformed all existing baselines (Fig.2a). Notably, we attained state-of-the-art (SOTA) results in Physiology, raising the AUC-ROC from a previous best of $74.43 \%$ to $76.60 \%$, a gain of $2.8 \%$. In Biophysics, our model further enhanced performance, advancing the AUC-ROC from $81.7 \%$ to $83.4 \%$, marking a $2.0 \%$ improvement. These advancements in physiology and biophysics emphasize the robustness and precision of LLM4SD in tasks that demand intricate biological understanding and modeling.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig.2. Comparison between LLM4SD and baselines across 4 domains. The red dotted line represents the average performance of all baselines. (a) Comparative analysis of model performance versus baselines in physiology and biophysics. (b) Comparative analysis of regression performance: LLM4SD vs. baselines in quantum mechanics and physical chemistry.
On tasks in quantum mechanics and physical chemistry, LLM4SD demonstrated substantial advancements (Fig.2b). In the domain of quantum mechanics, it showed a profound improvement of $48.2 \%$ over the best performed baseline, registering an average MAE of 5.8233 across 12 tasks as opposed to 11.2450 . Similarly, in physical chemistry, LLM4SD observed a noteworthy enhancement, with the model reaching a MAE of 1.28 marking an $18.5 \%$ advancement over the baseline MAE of 1.57. These significant improvements in regression tasks affirm the refined capability of our approach in continuous prediction.</p>
<p>Overall, LLM4SD's marked improvements not only affirm its supremacy over specialized, and often black-box, state-of-the-art models but also highlight its unparalleled ability to synthesize postulates, infer scientific data, and provide insightful explanations. This offers a fresh perspective in computational research and heralds a new direction in scientific endeavors.</p>
<h1>3.2 Ablation Study</h1>
<p>To delve deeper into the intricacies of the LLM4SD pipeline, we conducted an ablation study, focusing on discerning the influence of scale and pretraining datasets on the performance of Large Language Models (LLMs). In addition, we assessed the relative contributions of knowledge synthesis and inference. Our evaluation spanned across a spectrum of foundational LLM backbones, notably the Falcon $7 \mathrm{~b}^{15}$, Falcon $40 \mathrm{~b}^{15}$, Galactica-6.7b ${ }^{14}$, and Galactica-30b ${ }^{14}$. Here, we selected open-source LLM backbones to ensure the reproducibility of our work. It is worth noting the distinct differences between the Falcon and Galactica series of LLMs. In particular, the Falcon models are trained for a broad range of applications, imbibing a more general context during their pretraining phase, while the Galactica models are pretrained on mainly scientific literature, making them particularly suitable for science.</p>
<h1>3.2.1 Effect of Scale</h1>
<p>The ablation study of LLM4SD, which compared four open-source LLM backbones, revealed substantial differences among the different LLMs (Fig.3a, b). Particularly within the Falcon series, performance disparities were conspicuous. The Falcon 7b, a smaller model, fell short compared to the Falcon 40 b in its range of domain expertise. Notably, it failed to conduct tasks in two key areas: physiology and quantum mechanics, indicating a weaker understanding of scientific challenges and data interpretation.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig.3. Ablation study of LLM4SD. (a) Performance comparison of four open-source LLM backbones for physiology and biophysics. The vertical dashed line in the figure separates the two LLM series: Falcon on the left and Galactica series on the right. (b) Performance comparison of four open-source LLM backbones for physical chemistry and quantum mechanics. (c) Examining the influence of both synthesized and inferred knowledge on the average model performance across four domains. The triangle's color signifies the metric employed for domain-specific tasks. A (+) next to the metric name indicates higher values yield better results, while a (-) suggests the contrary.</p>
<p>Conversely, the Galactica series painted a more nuanced picture. Unlike with the Falcon series, a larger model did not necessarily translate to superior performance. In disciplines such as Physiology, Biophysics, and Physical Chemistry, Galactica 6.7 b rivaled the performance of Galactica-30b, despite the latter having more than 4 times the number of parameters. However,</p>
<p>in the domain of Quantum Mechanics, the larger Galactica 30b surged ahead, outperforming Galactica 6.7 b by a margin of $14 \%$. This variance could be attributed to the intricate and abstract nature of Quantum Mechanics, where the depth and breadth of knowledge encapsulated in the larger model might offer a discernible advantage.</p>
<h1>3.2.2 Effect of Pretraining Datasets of LLMs</h1>
<p>From these observations it becomes evident that an LLM steeped in scientific literature, even if smaller in scale, exhibits a commendable prowess in scientific tasks (Fig.3a, b). Conversely, the Falcon series, designed for general utility, necessitates a more substantial scale to effectively navigate scientific challenges. We postulate that this phenomenon is underpinned by the emergent capabilities ${ }^{32}$ inherent to large-scale LLMs. These capabilities empower the more expansive Falcon-40b to bridge the knowledge gap and adapt to scientific tasks. In a broader perspective, despite their relatively modest scale, the Galactica models consistently outperformed the Falcon series, underscoring the pivotal role of domain-specific pretraining.</p>
<h3>3.2.3 Contributions of knowledge synthesis and inference</h3>
<p>In our exploration of LLM4SD with respect to various knowledge sources, we discerned the performance variance arising from the use of rule-based features synthesized from literature, rule-based features inferred from data, and a combined approach. Overall values for these categories were obtained by averaging over results for all tasks in a domain (Fig. 3c).</p>
<p>In a comprehensive assessment of various scientific domains, the combination of synthesis and inference features consistently outperformed individual methods. Specifically, in the field of physiology, an AUC-ROC of 76.38 was achieved using both methods, compared to 72.15 with synthesis alone and 72.12 with inference. Similarly, in biophysics, combining both methods yielded an AUC-ROC of 80.95 , surpassing the scores of 75.62 and 77.23 obtained from synthesis and inference features, respectively. In physical chemistry, the combined approach resulted in an RMSE of 1.38 , which is notably better than the 1.72 from synthesis features and 1.92 from inference features. Lastly, in Quantum Mechanics, the use of both synthesis and inference features produced a MAE of 6.82 , improving upon the values of 9.81 and 7.18 recorded with synthesis and inference alone. Notably, comparing just synthesis with just inference, each outperformed the other in 2 out of the 4 domains.</p>
<p>These observations highlight the value of combining knowledge synthesis from scientific literature with inference from data. Literature imparts foundational theoretical insights, while empirical data identifies further regularities. The fusion of these knowledge facets equips the models with a comprehensive understanding, empowering them to excel across varied tasks and domains.</p>
<h2>4. Statistical Analysis and Literature Review: Validating Established Rules</h2>
<p>With LLM4SD outperforming specialized, state-of-the-art methods, we further validated the rules generated by Galactica-6.7b due to its superior performance and ease of reproducibility. The rules were validated in two ways: statistical tests to confirm the significance of these rules, and literature review to assess whether the rules are discussed in existing scientific literature.</p>
<p>For statistical tests of rules, we employed the Mann-Whitney $U$ test ${ }^{33}$ for classification tasks and the linear regression t-test for regression tasks. The Mann-Whitney $U$ test ${ }^{33}$ compared the distributions of chosen rule across the two classes of the target variable, thereby evaluating the statistical relevance of the rule's ability to split and distinguish classes. Conversely, the linear regression t-test treated the chosen rule as the independent variable and examined whether its coefficient significantly deviated from 0 , reflecting whether the rule contributes to regression prediction.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig.4. Literature Review and Statistical Analysis of LLM Rules. a-d. We conducted statistical analysis and a comprehensive literature review on rules generated by Galactica 6.7 b across all four scientific domains, with two tasks evaluated for each domain: (a) Physiology, (b) Biophysics, (c) Quantum Mechanics, and (d) Physical Chemistry. In the statistical analysis, the significance of a rule is determined based on the task type: for classification, the Mann-Whitney $U$ test ${ }^{33}$ compares the difference in distributions of chosen rule across the two classes of the target variable; and for regression, the linear regression t -test ${ }^{26}$ treats the chosen rule as the independent variable and examined whether its coefficient significantly deviated from 0 , reflecting whether the rule contributes to prediction. In both cases, we used a 0.05 p -value threshold to determine rule significance. In the literature review, we assessed the prevalence of a rule in existing literature. With statistical analysis and literature review, each rule is categorized into one of three classes: statistically significant and literature supported; statistically significant and not found in literature; and statistically insignificant. Across all tasks, literaturesynthesized knowledge rules were generally both prevalent in existing literature and statistically significant. In contrast, empirically inferred data rules yielded mixed results, with some easily found in existing literature and others not identified by the researchers.</p>
<p>We further carried out a comprehensive review with in-domain experts to evaluate the prevalence of a rule in existing literature (see Supplementary Information 4: Literature Review(Example)). After cross-referencing the rules with scientific literature, we categorized each rule into one of three classes: statistically significant and literature supported; statistically significant and not found in literature; and statistically insignificant (Fig. 4).</p>
<h1>4.1 Knowledge Synthesis from Scientific Literature</h1>
<p>We discovered that most of the synthesized rules we examined are readily available in existing scholarly works. Notably, an overwhelming majority ( $85 \%$ ) of these rules were statistically significant in indicating the target labels across all selected tasks, affirming our pipeline's ability to summarize rules from scientific literature that were the most important for different tasks and domains.</p>
<p>Importantly, except for $\mathrm{BACE}^{20}$ and Tox21-NR-Ahr ${ }^{18}$, we found no instances where statistically significant rules were absent from existing literature (Fig. 4). This aligns with the design of our pipeline: without analyzing the data, LLMs tend to aggregate and summarize existing knowledge. To illustrate, in the context of BBBP, the rules generated by our pipeline were consistent with well-established determinants such as molecular weight, lipophilicity, distribution coefficient, topological polar surface area, and hydrogen bonds ${ }^{34-36}$. These findings validate the robustness and reliability of our pipeline in leveraging LLMs to summarize existing scientific literature.</p>
<h3>4.2 Knowledge Inference from Data</h3>
<p>We found that an average of $91.3 \%$ of the inferred rules were statistically significant, higher than synthesized rules (Fig. 4). Of these, an average of $74 \%$ rules were already documented in existing scientific literature, while $17.3 \%$ were not identified by researchers. These latter rules were primarily associated with data patterns that are not widely discussed in the literature but can be inferred from our task dataset, such as obscure molecular substructures that influence target labels like the Gibbs free energy $\left(\Delta \mathrm{G}^{\circ}\right)$ of a molecule.</p>
<p>In contrast, to the knowledge synthesized from literature, we found that 6 out of 8 tasks have statistically significant rules that were absent from existing literature. This suggests that the rules produced by LLM4SD are not merely a result of the LLM's textual memorization during pretraining. Instead, the inferred rules reflect a genuine capability to derive meaningful rules from data based on the specific task.</p>
<p>Our case studies further substantiated the utility of these unidentified but significant rules. For instance, in BBBP where $38 \%$ of rules are significant but unidentified, Galactica 6.7B pinpointed the carbonyl functional group and fragment rings as key determinants of a molecule's BBBP. We hypothesize that these features are crucial for calculating a molecule's cross-sectional area, which in turn influences its orientation in lipid-water interfaces-factors vital for membrane partitioning and permeation ${ }^{37}$. Intriguingly, this suggests that our pipeline enables LLMs to infer what we term as second-order features. These are features that may not be immediately obvious or widely recognized but are consistent with established scientific principles in literature. In</p>
<p>doing so, LLMs not only corroborate existing knowledge but also apply existing knowledge in interpreting data, thereby enriching the current scientific discourse.</p>
<p>By leveraging LLMs, our pipeline not only validates well-established scientific principles but also uncovers less documented and even potentially novel rules. This facilitates a more effective and transparent interaction between scientists and the AI system, enhancing both the quality and trustworthiness of the research output. Moreover, the statistically robust but underrepresented rules we identified could serve as promising avenues for future scientific exploration, thereby advancing the frontier of collaborative, AI-assisted research.</p>
<h1>5. Discussion</h1>
<p>In our exploration, we unveil the capabilities of LLM4SD through our specially designed pipeline, enabling LLMs to excel in scientific synthesis, inference, and explanation. Through seamless integration with our proposed architecture, LLMs exhibit state-of-the-art (SOTA) performance across a vast expanse of four domains. The inherent versatility of LLM4SD stands as a testament to its potential, making it poised for broader applications across varied domains, thus magnifying its relevance in the current scientific landscape.</p>
<p>Scientific discovery, vast in scope, is constantly evolving with our expanding understanding of the universe. Our study, ambitious in its intent, captures 58 tasks across four distinct domains, providing a glimpse into the immense reservoir of scientific knowledge. While this study serves as a pioneering beacon, demonstrating LLMs' transformative capabilities, it also signals the beginning of a broader exploration. We envision further expansion, integrating more diverse tasks and domains, pushing LLMs to their full potential and reshaping the boundaries of scientific inquiry.</p>
<p>Harnessing the immense power of AI-driven models for scientific discovery brings along its ethical challenges. The vast capabilities of such models, while revolutionizing our understanding, also raise concerns of potential misuse, especially in sensitive domains like biophysics and quantum mechanics. The reliance on machine synthesis and interpretation might overshadow the indispensable human element of scrutiny and ethics in research. As we plunge deeper into the AI era, it's crucial to tread with caution, balancing advancements with rigorous oversight and an unwavering commitment to ethical rigor.</p>
<p>As we gaze towards the horizon, the potential trajectory for LLM4SD is compelling. We anticipate a future where the nexus between LLMs and advanced scientific toolkits deepens. As computational capabilities grow and scientific knowledge expands, our pipeline stands poised for evolutionary enhancements. Our steadfast goal is to harmoniously fuse artificial intelligence with myriad scientific arenas, unlocking novel insights and pioneering avenues previously unimagined.</p>
<h1>References</h1>
<ol>
<li>Frank MC. Baby steps in evaluating the capacities of large language models. Nat Rev Psychol. (2023).</li>
<li>Brown T, Mann B, Ryder N, Subbiah M, Kaplan JD, Dhariwal P, Neelakantan A, Shyam P, Sastry G, Askell A, Agarwal S. Language models are few-shot learners. Adv Neural Inf Process Syst. (2020); 33:1877-901.</li>
<li>OpenAI. GPT-4 Technical Report. preprint. (2023);</li>
<li>Birhane A, Kasirzadeh A, Leslie D et al. Science in the age of large language models. Nat Rev Phys. (2023); 5:277-280.</li>
<li>Gilbert S, Harvey H, Melvin T et al. Large language model AI chatbots require approval as medical devices. Nat Med. (2023).</li>
<li>Bloom, Nicholas, Charles I. Jones, John Van Reenen, and Michael Webb. "Are ideas getting harder to find?." American Economic Review 110, no. 4 (2020): 1104-1144.</li>
<li>Wei, Jason, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V. Le, and Denny Zhou. "Chain-of-thought prompting elicits reasoning in large language models." Advances in Neural Information Processing Systems 35 (2022): 24824-24837.</li>
<li>Zhou, Denny, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans et al. "Least-to-most prompting enables complex reasoning in large language models." ICLR (2023).</li>
<li>Rozière, Baptiste, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ell en Tan, Yossi Adi et al. "Code Llama: Open Foundation Models for Code." arXiv preprint arXiv:2308.12950 (2023).</li>
<li>Jiang, Lavender Yao, Xujin Chris Liu, Nima Pour Nejatian, Mustafa Nasir-Moin, Duo Wang, Anas Abidin, Kevin Eaton et al. "Health system-scale language models are all-purpose prediction engines." Nature (2023): 1-6.</li>
<li>Singhal, Karan, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales et al. "Large language models encode clinical knowledge." Nature (2023).</li>
<li>Lee, Jinhyuk, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. "BioBERT: a pre-trained biomedical language representation model for biomedical text mining." Bioinformatics 36, no. 4 (2020): 1234-1240.</li>
<li>Beltagy, Iz, Kyle Lo, and Arman Cohan. "SciBERT: A pretrained language model for scientific text." ACL (2019).</li>
<li>Taylor, Ross, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. "Galactica: A large language model for science." Preprint (2022).</li>
<li>Almazrouei, Ebtesam, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet et al. "Falcon-40B: an open large language model with state-of-the-art performance." Technical report, Technology Innovation Institute; (2023).</li>
<li>Martins, Ines Filipa, Ana L. Teixeira, Luis Pinheiro, and Andre O. Falcao. "A Bayesian approach to in silico blood-brain barrier penetration modeling." Journal of Chemical Information and Modeling 52, no. 6 (2012): 16861697.</li>
<li>Gayvert, Kaitlyn M., Neel S. Madhukar, and Olivier Elemento. "A data-driven approach to predicting successes and failures of clinical trials." Cell Chemical Biology 23, no. 10 (2016): 1294-1301.</li>
<li>Wu, Z., Ramsundar, B., Feinberg, E. N., Gomes, J., Geniesse, C., Pappu, A. S., ... \&amp; Pande, V. (2018). MoleculeNet: a benchmark for molecular machine learning. Chemical Science, 9(2), 513-530.</li>
<li>Kuhn, Michael, Ivica Letunic, Lars Juhl Jensen, and Peer Bork. "The SIDER database of drugs and side effects." Nucleic Acids Research 44, no. D1 (2016): D1075-D1079.</li>
<li>
<p>Subramanian, Govindan, Bharath Ramsundar, Vijay Pande, and Rajiah Aldrin Denny. "Computational modeling of $\beta$-secretase 1 (BACE-1) inhibitors using ligand based approaches." Journal of Chemical Information and Modeling 56, no. 10 (2016): 1936-1949. 25</p>
</li>
<li>
<p>Delaney, John S. "ESOL: estimating aqueous solubility directly from molecular structure." Journal of Chemical Information and Computer Sciences 44, no. 3 (2004): 1000-1005.</p>
</li>
<li>Mobley, David L., and J. Peter Guthrie. "FreeSolv: a database of experimental and calculated hydration free energies, with input files." Journal of Computer-Aided Molecular Design 28 (2014): 711-720.</li>
<li>Ramakrishnan, Raghunathan, Pavlo O. Dral, Matthias Rupp, and O. Anatole Von Lilienfeld. "Quantum chemistry structures and properties of 134 kilo molecules." Scientific Data 1, no. 1 (2014): 1-7.</li>
<li>Hu, Weihua, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure Leskovec. "Strategies for Pre-training Graph Neural Networks." In International Conference on Learning Representations. 2019 .</li>
<li>You, Yuning, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. "Graph contrastive learning with augmentations." Advances in Neural Information Processing Systems 33 (2020): 5812-5823.</li>
<li>Wang, Yuyang, Jianren Wang, Zhonglin Cao, and Amir Barati Farimani. "Molecular contrastive learning of representations via graph neural networks." Nature Machine Intelligence 4, no. 3 (2022): 279-287.</li>
<li>Stärk, Hannes, Dominique Beaini, Gabriele Corso, Prudencio Tossou, Christian Dallago, Stephan Günnemann, and Pietro Liò. "3d infomax improves gnns for molecular property prediction." In International Conference on Machine Learning, pp. 20479-20502. PMLR, 2022.</li>
<li>Liu, Shengchao, Hanchen Wang, Weiyang Liu, Joan Lasenby, Hongyu Guo, and Jian Tang. "Pre-training Molecular Graph Representation with 3D Geometry." In International Conference on Learning Representations. 2021 .</li>
<li>Xia, Jun, Chengshuai Zhao, Bozhen Hu, Zhangyang Gao, Cheng Tan, Yue Liu, Siyuan Li, and Stan Z. Li. "Mole-bert: Rethinking pre-training graph neural networks for molecules." In The Eleventh International Conference on Learning Representations. 2022.</li>
<li>Breiman, Leo. "Random forests." Machine Learning 45 (2001): 5-32.</li>
<li>Rogers, David, and Mathew Hahn. "Extended-connectivity fingerprints." Journal of Chemical Information and Modeling 50, no. 5 (2010): 742-754.</li>
<li>Wei, Jason, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama et al. "Emergent abilities of large language models." TMLR (2022).</li>
<li>McKnight, Patrick E., and Julius Najab. "Mann-Whitney U Test." The Corsini Encyclopedia of Psychology (2010): 1-1.</li>
<li>Wager, T. T. et al. Defining desirable central nervous system drug space through the alignment of molecular properties, in vitro adme, and safety attributes. ACS Chemical Neuroscience 1, 420-434 (2010)</li>
<li>Wager, T. T., Hou, X., Verhoest, P. R. \&amp; Villalobos, A. Moving beyond rules: the development of a central nervous system multiparameter optimization (cns mpo) approach to enable alignment of druglike properties. $A C S$ Chemical Neuroscience 1, 435-449 (2010).</li>
<li>Geldenhuys, W. J., Mohammad, A. S., Adkins, C. E. \&amp; Lockman, P. R. Molecular determinants of blood-brain barrier permeation. Therapetuic. Delivery 6, 961-971 (2015).</li>
<li>Gerebtzoff, G. \&amp; Seelig, A. In silico prediction of blood- brain barrier permeation using the calculated molecular cross-sectional area as main parameter. J. Chemical Information Modeling 46, 2638-2650 (2006). 14/14.</li>
</ol>
<h1>Methods</h1>
<h2>Datasets:</h2>
<p>We conducted a thorough evaluation of LLM4SD, covering 58 subtasks across four unique domains for a robust assessment. The physiology domain included 41 tasks like BBBP, ClinTox, and the 12-task Tox21, ranging from NR-AR to SR-p53, along with the 27-task SIDER suite covering various medical conditions. Biophysics offered 2 classification tasks: BACE and HIV. In physical chemistry, we addressed three regression tasks: ESOL, FreeSolv, and Lipophilicity, while the Quantum mechanics domain presented 12 regression tasks within the QM9 dataset, exploring properties from mu to G, providing a comprehensive insight into LLM4SD's capabilities.</p>
<h2>Physiology.</h2>
<p>BBBP: The BBBP dataset contains 2,039 instances, each representing unique compounds labeled based on their permeability properties. Predicting which molecules can cross this barrier is paramount for drug development, especially for neurological conditions.
ClinTox: The ClinTox dataset, with 1,478 instances, provides comprehensive information on the toxicological properties of various compounds.
Tox21: With 7,831 instances, the Tox21 dataset is a collaborative effort to identify environmental toxicants. Its 12 classification tasks focus on specific biological targets or pathways. The Nuclear Receptor (NR) tasks, namely NR-AhR, NR-AR, NR-AR-LBD, NRAromatase, NR-ER, NR-ER-LBD, and NR-PPAR-gamma, examine interactions with intracellular proteins influencing gene expression and potential toxic effects. The Stress Response (SR) tasks, including SR-ARE, SR-ATAD5, SR-HSE, SR-MMP, and SR-p53, explore the impact of chemicals on stress-related cellular pathways.
SIDER: The SIDER dataset, with 1,427 instances, offers detailed data on medication side effects. Each task in this dataset relates to a specific adverse drug reaction, aiding researchers in understanding and predicting potential drug side effects. All 27 classification tasks are: 1) Hepatobiliary disorders 2) Metabolism and nutrition disorders 3) Product issues 4) Eye disorders 5) Investigations 6) Musculoskeletal and connective tissue disorders 7) Gastrointestinal disorders 8) Social circumstances 9) Immune system disorders 10) Reproductive system and breast disorders 11) Neoplasms benign, malignant and unspecified (incl cysts and polyps). 12) General disorders and administration site conditions 13) Endocrine disorders 14) Surgical and medical procedures 15) Vascular disorders 16) Blood and lymphatic system disorders 17) Skin and subcutaneous tissue disorders 18) Congenital, familial and genetic disorders 19) Infections and infestations 20) Respiratory, thoracic and mediastinal disorders 21) Psychiatric disorders 22) Renal and urinary disorders 23) Pregnancy, puerperium and perinatal conditions 24) Ear and labyrinth disorders 25) Cardiac disorders 26) Nervous system disorders 27) Injury, poisoning and procedural complications.</p>
<h2>Biophysics.</h2>
<p>HIV: With a collection of 17,930 instances, the HIV dataset offers a comprehensive repository of molecules, represented in the SMILES format. This dataset is instrumental in the classification of compounds based on their potential inhibitory effects against HIV.</p>
<p>BACE: The BACE dataset, comprising 11,908 instances, is a curated collection of molecules, each represented in the SMILES format. This dataset is tailored for classification tasks, aiming to discern molecules that can inhibit the BACE-1 enzyme. By analyzing the molecules within this dataset, researchers can glean insights into the structural features that confer inhibitory properties against BACE-1.</p>
<h1>Physical Chemistry.</h1>
<p>ESOL: The ESOL dataset, comprising 1,128 instances, is a curated collection that delves into the solubility of molecules in water. By analyzing the ESOL dataset, researchers can gain a deeper understanding of the molecular features that dictate solubility, thereby aiding in the design of compounds with optimal solubility profiles. Each entry in this dataset is represented using the SMILES notation, a universal language for describing the structure of chemical species.
FreeSolv: With 642 instances, the FreeSolv dataset provides comprehensive data on the hydration free energy of molecules. This dataset is pivotal for researchers aiming to predict how molecules interact with water, which has implications for drug solubility and stability. Each molecule in the FreeSolv dataset is also represented using the SMILES notation.
Lipophilicity: Lipophilicity is a fundamental property that influences the absorption, distribution, metabolism, and excretion of drugs. The Lipophilicity dataset with 4200 compounds offers a rich resource for understanding this property. Analyzing this dataset allows researchers to discern the molecular attributes that contribute to a compound's lipophilicity, guiding the synthesis of molecules with desired pharmacokinetic properties. Like the other datasets in this domain, each entry is denoted using the SMILES notation.</p>
<h2>Quantum Mechanics.</h2>
<p>QM9: The Quantum Mechanics domain, central to understanding the fundamental properties of matter, is exemplified in our evaluation through the QM9 dataset. Comprising 133,885 instances, the QM9 dataset provides a comprehensive exploration of molecules' quantum mechanical attributes, essential for diverse applications from material science to pharmaceuticals. It includes 12 tasks: (Dipole Moment), (Polarizability), (Squared Radius), ZPVE (Zero-Point Vibrational Energy), (Heat Capacity at Constant Volume), (Energy Gap), (Highest Occupied Molecular Orbital Energy), (Lowest Unoccupied Molecular Orbital Energy), (Internal Energy at 0 Kelvin), U (Internal Energy at Standard State), H (Enthalpy), G (Gibbs Free Energy).</p>
<h2>Baselines:</h2>
<p>We rigorously assessed our pipeline in comparison to specialized, state-of-the-art supervised machine learning methods. For conventional approaches, we employed Random Forest ${ }^{30}$, using ECFP4 ${ }^{31}$ as the input feature set. We also considered state-of-the-art Graph Neural Networks (GNNs), including Attribute Masking (AttrMask) ${ }^{24}$, GraphCL ${ }^{25}$, MolCLR ${ }^{26}$, 3DInfomax ${ }^{27}$, GraphMVP ${ }^{28}$, and MoleBERT ${ }^{29}$. Each of these models was initialized with pre-trained weights and subsequently fine-tuned for specific tasks.</p>
<p>In summary, AttrMask pre-training involves teaching the GNN to predict randomly masked atom and bond attributes within molecular graphs. GraphCL and MolCLR use graph augmentations for a contrastive learning objective, aimed at maximizing the similarity between augmentations originating from the same molecule while minimizing similarity between augmentations from different molecules. GraphMVP and 3DInfomax leverage existing 3D molecular datasets to pretrain models capable of deducing 3D molecular geometry from 2D graphs, by optimizing mutual information between 3D summary vectors and GNN graph representations. Finally, MoleBERT, the recent state-of-the-art method, employs a VQ-VAE-based tokenizer to diversify atom vocabulary, thereby balancing dominant and rare atoms. It uses Masked Atoms Modeling (MAM) and Triplet Masked Contrastive Learning (TMCL) for node and graph-level pre-training, respectively.</p>
<h1>LLM for Scientific Discovery Pipeline</h1>
<p>In this section, we detail the proposed pipeline and the techniques used to align them with the requirements of molecular property prediction tasks. Instead of merely prompting LLMs to generate scientific hypotheses ${ }^{38}$ or training them for direct predictions ${ }^{39}$, LLM4SD emulates how human experts conduct scientific research. This includes synthesizing knowledge from literature, inferring hypotheses from datasets, validating findings through experiments, and elucidating the rationale behind predictions.</p>
<h2>Knowledge Synthesis from the Scientific Literature</h2>
<p>LLMs are usually pretrained on large corpora of text data that include books, articles, websites and other written content. This extensive pretraining helps LLMs to learn the structure of the language, recognize patterns, understand context and acquire a wide-ranging knowledge of facts and concepts. Thus, the goal of the knowledge synthesis process is to extract relevant features from the vast pool of the knowledge that a LLM possesses from the pretraining stage.
To achieve this, we first instruct the LLM to adopt the persona of an experienced chemist, and then engage it to identify pertinent features based on its existing knowledge. This form of roleplaying prompt facilitates the knowledge mining process to mimic how human experts solve real-world challenges. For example, when a chemist needs to predict the bioactivity or BBBP of a molecule, they often apply feature-related rules such as number of hydrogen bond donors/acceptors, molecular weight, and $\log \mathrm{P}$. We require that the features identified by LLMs can be measured with a numerical or categorical measure to enable their transcription into corresponding functions.</p>
<h2>Knowledge Inference from Data</h2>
<p>The objective of knowledge inference form data is to harness the powerful reasoning skills of LLMs to identify relevant features by analyzing the given data. Given their impressive ability to solve mathematical problems and identify patterns, we conjecture that LLMs have the capacity to discern common patterns within groups of molecules based on its scientific understanding. To validate this hypothesis, we provide LLMs with an instruction and several batches of sampled instances with their corresponding class labels or instance property values. In the instruction, the LLM is tasked with analyzing patterns from provided data to identify features that effectively</p>
<p>discriminate between two classes of instances or predict their property values. As a result, LLMs will come up with rules distilled from the analysis for each batch. Since the generated rules in different batches may contain duplicates, we ultimately employ the LLMs' summarization capability to condense the rules and eliminate duplicates, resulting in the final list of features.</p>
<h1>Interpretable Model Training</h1>
<p>In this stage, all the features identified in the first two stages are transcribed into corresponding functions. All these functions take a scientific instance as input, e.g., a SMILES string for molecules, and return a feature value. Consequently, the final representation of an instance resides in an r-dimensional space, where r is the number of features that have been identified.
These vector representations function as the feature vectors for the model training. Employing interpretable models like a linear layer or random forest enables quantification of each rule's importance in prediction, thus elucidating their contribution to the model's final decision. This transparency fosters an intuitive comprehension of the decision-making process, enhancing trust and usability among domain experts.</p>
<h2>Interpretable Explanation Generation</h2>
<p>The final stage in our pipeline involves generating interpretable explanations for the predictions. Specifically, we furnish the LLMs with salient information, including the model prediction, the vector representation, important rules, and their importance scores derived from the random forest or linear layer. Utilizing the inference and summarization ability of the LLMs, the provided information is transformed into a text-based explanation. This stage is pivotal in rendering the results in an accessible manner. It ensures that users can seamlessly understand the decision-making process and each rule's contribution to the overall prediction, thereby enhancing trust and transparency. This accessibility not only facilitates user interaction with the model but also empowers experts in the field to utilize the generated insights for further analysis and decision-making.</p>
<h2>Metrics</h2>
<p>We assessed LLM4SD across 58 molecular property prediction tasks spanning four domains, utilizing distinct evaluation metrics tailored to each task's nature. For the domains of physiology and biophysics, the Area Under the Receiver Operating Characteristic curve (AUC-ROC) metric was employed. AUC-ROC, measures the ability of the model to distinguish between classes, with a range from 0 to 1 , where a higher value indicates better performance. In the domain of physical chemistry, the Root Mean Square Error (RMSE) was used. RMSE quantifies the difference between predicted and observed values, with a lower value indicating a closer fit to the true data. For quantum mechanics, we utilized the Mean Absolute Error (MAE) metric. MAE measures the average magnitude of errors between predicted and true values, with smaller values denoting better accuracy.</p>
<h2>Experiment setting</h2>
<p>In our experimental setting, we partitioned the data into an 80/10/10 split for training, validation, and test sets. For the domains of physiology, biophysics, and physical chemistry, we employed a</p>
<p>scaffold split for molecular compounds. The scaffold split method in these three domains ensures that molecules with similar structures are grouped together, providing a more challenging and realistic evaluation of model generalization. To ensure reproducibility and facilitate further research, the datasets split using this scaffold method are made available in our open-source GitHub repository. In the realm of quantum mechanics, we opted for a random split.</p>
<h1>Data availability</h1>
<p>The datasets utilized in this study are entirely open-source and have been made publicly available to ensure straightforward replication of our findings. For research related to quantum mechanics, physical chemistry, biophysics, and physiology, the datasets can be accessed at https://moleculenet.org/datasets-1.</p>
<h2>Code availability</h2>
<p>In our commitment to transparency and reproducibility, we will release our code showing our implementation in https://github.com/zyzisastudyreallyhardguy/LLM4SD. This encompasses methodologies for literature knowledge mining, knowledge inference rule mining, interpretable model training, and interpretable explanation generation. Throughout this work, we have employed several open-source libraries, including Hugging Face, numpy, rdkit, pytorch, scipy, bitsandbytes, and accelerate.</p>
<p>Furthermore, we are in the process of deploying a website to facilitate scientists in utilizing LLM4SD. The site features three core functionalities for scientific users: knowledge synthesis, knowledge inference, and prediction with explanations. Examples of user interactions with the website can be found in the supplementary information. As part of our ongoing commitment, we anticipate the inclusion of additional tasks in the future development phases.</p>
<ol>
<li>Park, Yang Jeong, Daniel Kaplan, Zhichu Ren, Chia-Wei Hsu, Changhao Li, Haowei Xu, Sipei Li, and Ju Li. "Can ChatGPT be used to generate scientific hypotheses?." arXiv preprint arXiv:2304.12208 (2023).</li>
<li>Honda, Shion, Shoi Shi, and Hiroki R. Ueda. "Smiles transformer: Pre-trained molecular fingerprint for low data drug discovery." arXiv preprint arXiv:1911.04738 (2019).</li>
</ol>
<h1>Acknowledgements:</h1>
<p>H.Y.K. scholarship is supported by the Australian Government Research Training Program (RTP) Scholarship and Monash University as a co-contribution to Australian Research Council grant ARC DP210100072. L.T.M, G.W and A.T.N.N research into artificial intelligence applications for drug discovery is supported by a National Health and Medical Research Council (NHMRC) of Australia Ideas grant (APP2013629). Computational resources were generously provided by the Nectar Research Cloud, a collaborative Australian research platform supported by the NCRIS-funded Australian Research Data Commons (ARDC) and the MASSIVE HPC facility. We also gratefully acknowledge the support of the Griffith University eResearch Service \&amp; Specialized Platforms Team and the use of the High-Performance Computing Cluster "Gowonda". S.R.P is supported by ARC Future Fellowship (No. FT210100097).</p>
<h2>Author Contribution:</h2>
<p>These authors contributed equally: Y.Z.Z., H.Y.K., J.X.J.
These authors jointly supervised this work: S.R.P., G.I.W.
S.R.P. and G.I.W. supervised the project. Y.Z.Z., H.Y.K., J.X.J. contributed to the conception and design of the work. Y.Z.Z., H.Y.K., J.X.J. contributed to the technical implementation. Y.Z.Z., H.Y.K., J.X.J. prepared the figures. Y.Z.Z. contributed to the design of the web-based application. A.T.N.N. and L.T.M. provided domain expertise for the literature review and validation of rules. Y.Z.Z., H.Y.K., A.T.N.N. and L.T.M. contributed to the design of the rule validation test. All authors edited and revised the manuscript.</p>
<h2>Competing Interests:</h2>
<p>The authors declare no competing interests.</p>
<h1>Extended Figures and Tables</h1>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Extended Data Fig. 1|Detailed performance comparison between "LLM4SD" and eight baselines in the physiology domain. The red dashed line shows the average result across all methods. Each marker's error bar denotes the method's standard deviation, which is obtained via 10 runs. LLM4SD outperformed other models in 3 out of 4 datasets using the AUC-ROC metric, and consistently surpassing the average across all datasets. The results for Tox21 and SIDER are average scores from 12 and 27 tasks respectively (see Extended Data Fig. 2 and 3 for detailed breakdown).</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Extended Data Fig. 2|Detailed performance comparison between "LLM4SD" and eight baselines on Tox21 Dataset. The red dashed line shows the average result across all methods. Each marker's error bar denotes the method's standard deviation, which is obtained via 10 runs. LLM4SD ranks among the top three methods in 10 out of 12 tasks, and consistently outperformed the average in all tasks.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Extended Data Fig. 3|Detailed performance comparison between "LLM4SD" and eight baselines on Sider Dataset. The red dashed line shows the average result across all methods. Each marker's error bar denotes the method's standard deviation, which is obtained via 10 runs. LLM4SD ranks among the top three methods in 22 out of 27 tasks, and consistently outperforms the average in all tasks with the exception of the "Psychiatric disorders" task.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9402 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9402</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9402</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-162.html">extraction-schema-162</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-899a6a84b72dd9f3a7120c1590fb07fb27071a37</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/899a6a84b72dd9f3a7120c1590fb07fb27071a37" target="_blank">Efficient Model Selection for Time Series Forecasting via LLMs</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work proposes to leverage Large Language Models (LLMs) as a lightweight alternative for model selection, and demonstrates that this approach outperforms traditional meta-learning techniques and heuristic baselines, while significantly reducing computational overhead.</p>
                <p><strong>Paper Abstract:</strong> Model selection is a critical step in time series forecasting, traditionally requiring extensive performance evaluations across various datasets. Meta-learning approaches aim to automate this process, but they typically depend on pre-constructed performance matrices, which are costly to build. In this work, we propose to leverage Large Language Models (LLMs) as a lightweight alternative for model selection. Our method eliminates the need for explicit performance matrices by utilizing the inherent knowledge and reasoning capabilities of LLMs. Through extensive experiments with LLaMA, GPT and Gemini, we demonstrate that our approach outperforms traditional meta-learning techniques and heuristic baselines, while significantly reducing computational overhead. These findings underscore the potential of LLMs in efficient model selection for time series forecasting.</p>
                <p><strong>Cost:</strong> 0.006</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Automating outlier detection via metalearning <em>(Rating: 2)</em></li>
                <li>Large language models are zero-shot time series forecasters <em>(Rating: 1)</em></li>
                <li>Time-llm: Time series forecasting by reprogramming large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9402",
    "paper_id": "paper-899a6a84b72dd9f3a7120c1590fb07fb27071a37",
    "extraction_schema_id": "extraction-schema-162",
    "extracted_data": [],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Automating outlier detection via metalearning",
            "rating": 2
        },
        {
            "paper_title": "Large language models are zero-shot time series forecasters",
            "rating": 1
        },
        {
            "paper_title": "Time-llm: Time series forecasting by reprogramming large language models",
            "rating": 1
        }
    ],
    "cost": 0.006045999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Efficient Model Selection for Time Series Forecasting via LLMs</h1>
<p>Wang Wei<br>Department of Computer Science Virginia Tech<br>Blacksburg, VA, USA<br>wangwei718@vt.edu<br>Hongjie Chen<br>Dolby Labs<br>Atlanta, GA, USA<br>hongjie.chen@dolby.com</p>
<h2>Frank Dernoncourt</h2>
<p>Adobe Research
Seattle, WA, USA
dernonco@adobe.com</p>
<h2>Tiankai Yang</h2>
<p>Department of Computer Science
University of South California
Los Angeles, CA, USA
tiankaiy@usc.edu</p>
<h2>Yue Zhao</h2>
<p>Department of Computer Science
University of South California
Los Angeles, CA, USA
yzhao010@usc.edu</p>
<h2>Hoda Eldardiry</h2>
<p>Department of Computer Science
Virginia Tech
Blacksburg, VA, USA
hdardiry@vt.edu</p>
<h4>Abstract</h4>
<p>Model selection is a critical step in time series forecasting, traditionally requiring extensive performance evaluations across various datasets. Metalearning approaches aim to automate this process, but they typically depend on pre-constructed performance matrix, which is costly to build. In this work, we propose to leverage Large Language Models (LLMs) as a lightweight alternative for model selection. Our method eliminates the need for explicit performance matrix by utilizing the inherent knowledge and reasoning capabilities of LLMs. Through extensive experiments with Llama, GPT, and Gemini, we demonstrate that our approach outperforms traditional meta-learning techniques and heuristic baselines, while significantly reducing computational overhead. These findings underscore the potential of LLMs in efficient model selection for time series forecasting.</p>
<h2>1 Introduction</h2>
<p>Time series forecasting plays a crucial role in a wide range of real-world applications, enabling informed decision-making and strategic planning across various domains, including finance (Sezer et al., 2020), healthcare (Bui et al., 2018), software monitoring (Sun et al., 2023), energy (Chou \&amp; Tran, 2018), retail (Fildes et al., 2022), and weather prediction (Han et al., 2024). Selecting an appropriate forecasting model is often a labor-intensive process requiring domain expertise and extensive computational resources. Traditional time series forecasting methods typically require substantial domain expertise and manual effort in model design, feature engineering, and hyperparameter tuning. This challenge is further intensified by the findings of Abdallah et al. (2022), which indicate that no single learning strategy consistently outperforms others across all forecasting tasks, due to the inherent diversity of time series data. Consequently, traditional methods often fail to deliver high-quality predictions across diverse application domains. A straightforward but naïve solution would be evaluating the performance for thousands of models on a given dataset to identify the most suitable one. However, such an approach is impractical due to the excessive computational cost and training time required for model evaluation on each new dataset.</p>
<p>To address the impracticality of exhaustively evaluating all models for each new dataset, meta-learning has recently gained great popularity in applications demanding model selection such as anomaly detection and classification Zhao et al., 2021), graph learning (Park et al., 2023), and recommendation (Wang et al., 2022), especially for forecasting (Abdallah et al., 2022), which could quickly infer the best forecasting model after training on the models' performances on historical datasets and the time-series meta-features of these datasets. Even though Abdallah et al. (2022) selects the best performing forecasting algorithm and its associated hyper-parameters with a $42 \times$ median inference time reduction averaged across all datasets compared to the naïve approach, nearly all state-of-the-art meta-learning approaches still require the construction of a large performance matrix, consisting of evaluations of hundreds or even thousands of models across a vast collection of forecasting datasets. This performance matrix, while crucial for traditional meta-learningbased model selection, is extremely costly to obtain in practice. Each dataset-model pair must be exhaustively evaluated, which demands significant computational resources and time. Furthermore, this matrix is typically used in conjunction with a carefully engineered meta-feature vector extracted from each time-series dataset to train a meta-learning model that can generalize and infer the best model for new forecasting tasks.
LLMs have demonstrated exceptional generalization and reasoning capabilities, positioning them as promising tools for automating model selection in time series forecasting. By leveraging zero-shot prompting techniques, LLMs can generate structured reasoning paths without the need for task-specific exemplars. For instance, Kojima et al. (2023) introduced a method where appending the phrase "Let's think step by step" to a prompt enables LLMs to perform complex reasoning tasks effectively. Building upon this, Kumar et al. (2024) proposed the Zero-shot Uncertainty-based Selection (ZEUS) approach, which enhances chain-of-thought (CoT) prompting by utilizing uncertainty estimates to select effective demonstrations without requiring access to model parameters. These advancements suggest that LLMs, through zero-shot and CoT prompting, can be harnessed to streamline model selection processes, reducing the need for exhaustive evaluations and manual interventions.
In this work, we propose an alternative paradigm: using LLMs to perform model selection without the need for an explicit performance matrix. Following the benchmark data specified in Abdallah et al. (2022)'s work, we investigate the effectiveness of LLMs in model selection for time series forecasting. Extensive experiments on over 320 datasets show that our method outperforms strategies such as directly selecting popular methods and even different meta-learning approaches(Kadioglu et al., 2010) (including simple and optimization-based meta-learners where a performance matrix is built and used during training).
Summary of Main Contributions. The key contributions of this work are as follows:</p>
<ul>
<li>LLM-Driven Zero-Shot Model Selection for Time-Series Forecasting. To the best of our knowledge, this work is the first to investigate the use of LLMs for selecting the most suitable time series forecasting model via zero-shot prompting. By evaluating multiple LLMs with various prompt designs, we demonstrate that LLM-based selection consistently outperforms both popular forecasting models and meta-learning approaches.</li>
<li>Computational Efficiency in Training and Inference. Unlike conventional model selection techniques that require training and evaluation of multiple forecasting models and the costly pre-computed performance matrix required in traditional meta-learning, our approach leverages LLMs to infer the optimal model and hyperparameters instantly. This results in a significant reduction in computational overhead, making the method highly scalable and efficient for real-world forecasting applications.</li>
<li>Ablation Study on Prompt Design for Model Selection. We conduct an ablation study to analyze the impact of incorporating meta-features and CoT reasoning in prompts across different LLMs. The findings could offer insights into effective prompt design strategies, guiding future improvements in LLM-driven model selection for time-series forecasting.</li>
</ul>
<h1>2 Related Work</h1>
<p>Model Selection in Time Series Forecasting. Model selection in time series forecasting has evolved through various methodologies, encompassing traditional statistical approaches, meta-learning techniques, and the emerging LLMs.
Traditional methods often rely on statistical criteria to choose the most suitable forecasting model. For instance, the average rank method evaluates multiple models across different datasets, selecting the one with the lowest average rank based on performance metrics (Cerqueira et al., 2022). While straightforward, these methods can be computationally intensive and may not generalize well across diverse time series data. To overcome these limitations, Lemke \&amp; Gabrys (2010) explored meta-learning strategies that utilize characteristics of time series data to predict the performance of various forecasting models, facilitating more efficient and accurate model selection. Similarly, Prudêncio \&amp; Ludermir (2004) investigated meta-learning techniques to rank and select time series models based on extracted meta-features, demonstrating improved forecasting accuracy. Recently, Abdallah et al. (2022) have also demonstrated that meta-learning can be used to infer the best model given dataset characteristics and model space without needing an exhaustive evaluation of all existing models on a new dataset. However, these approaches still require constructing performance matrix that capture the evaluation results of all models across all datasets, which is computationally expensive and time-consuming.
LLMs for Time Series Forecasting. The integration of LLMs into time series forecasting has garnered significant attention, with recent studies exploring their potential for model selection and prediction tasks. Jin et al. (2024) introduced Time-LLM, a framework that reprograms LLMs for time series forecasting by aligning time series data with natural language inputs. Gruver et al. (2024) demonstrated that LLMs, such as GPT-3 and Llama-2, can perform zero-shot time series forecasting by encoding time series as sequences of numerical digits, framing forecasting as a next-token prediction task. Cao et al. (2024) introduced an interpretable prompt-tuning-based generative transformer for time series representation learning. Zhang et al. (2024) provided a comprehensive survey on the application of LLMs in time series analysis, highlighting their potential to enhance forecasting performance across various domains. However, these studies differ from our approach as they employ LLMs directly as forecasting models for new datasets, whereas our work focuses on leveraging LLMs for model selection. Specifically, we demonstrate how LLMs can effectively identify the most suitable forecasting model to achieve optimal predictive performance.
Prompting. Prompting has emerged as the primary approach for tailoring language models to various downstream applications. Zero-shot prompting enables LLMs to perform tasks without specific examples by appending phrases like "Let's think step by step" to the prompt, effectively eliciting reasoning process (Kojima et al., 2023). CoT prompting further improves multi-step reasoning by incorporating intermediate reasoning steps into the prompt, leading to better performance on complex tasks (Wei et al., 2023). Surveys on prompt design strategies provide comprehensive overviews of techniques such as manual design and optimization algorithms, emphasizing their impact on LLM performance across diverse tasks (Li, 2023). These developments underscore the critical role of prompt engineering in fully leveraging LLMs for complex reasoning and decision-making tasks.</p>
<h2>3 Methodology</h2>
<h3>3.1 Overview</h3>
<p>The model selection task for time series forecasting is formulated as a mapping from datasetbased prompts to candidate forecasting models. Let $S: \mathcal{P} \rightarrow \mathcal{M}$, where $\mathcal{P}$ denotes the set of possible prompts and $\mathcal{M}$ represents the space of candidate forecasting models. For each dataset $d_{i} \in \mathcal{D}$, the process comprises three components:</p>
<ul>
<li>Prompt Construction: Construct a prompt $p_{i} \in \mathcal{P}$ from $d_{i}$ using one of the predefined prompt templates.</li>
</ul>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An overview of model selection via LLMs.</p>
<div class="codehilite"><pre><span></span><code>Algorithm 1 Model Selection via LLMs
    Input: Time series dataset \(d_{\text {test }}\), model space \(\mathcal{M}\), prompt template \(\mathcal{P}\)
    Output: Selected forecasting model \(m^{*}\)
    Step 1: Prompt Construction
    Get dataset values \(X_{\text {test }}\) from \(d_{\text {test }}\)
    if Meta-features are included then
        Include meta-features \(F_{\text {test }}\) of \(d_{\text {test }}\)
    end if
    if CoT is included then
        Incorporate reasoning steps into prompt
    end if
    Generate prompt \(p\) using:
                            \(p=\operatorname{Format}\left(X_{\text {test }},\left[F_{\text {test }}\right],[\mathrm{CoT}]\right)\)
    Step 2: Query LLM for Model Selection
    Obtain selected model \(m^{*}=\left(a^{*}, h^{*}, g^{*}\right)\) by querying the LLM:
                            \(m^{*}=S(p)\)
    where \(a^{*}\) is the forecasting algorithm, \(h^{*}\) is the hyperparameter set, and \(g^{*}\) is the data
    representation.
    Step 3: Forecasting and Evaluation
    Apply \(m^{*}\) to generate forecasts for \(d_{\text {test }}\)
    Compute performance metrics
    Return: Selected model \(m^{*}\)
</code></pre></div>

<ul>
<li>LLM-Based Model Selection: The prompt $p_{i}$ is submitted to an LLM to obtain the recommended model $m_{i}=S\left(p_{i}\right)$, where $m_{i} \in \mathcal{M}$.</li>
<li>Forecasting and Evaluation: Apply $m_{i}$ to $d_{i}$ to produce forecasts and evaluate performance using appropriate metrics.
Problem Statement. Given a new dataset $d_{\text {test }}$ (i.e., unseen time series forecasting task), select a model $m \in \mathcal{M}$ to employ on that dataset.</li>
</ul>
<h1>3.2 Prompt Construction</h1>
<p>We designed four distinct prompt structures, each varying in the inclusion of meta-features and CoT reasoning. The detailed structure of our prompts is illustrated in Appendix A.1.</p>
<ul>
<li>
<p>Dataset Values Only: Providing raw time series data.</p>
</li>
<li>
<p>Dataset Values and Meta-Features: Combining raw data with pre-computed metafeatures from Abdallah et al. (2022). Details of meta-features are available A.2.</p>
</li>
<li>Dataset Values with CoT: Including raw data along with a step-by-step reasoning instruction in the prompt to guide the LLM.</li>
<li>Dataset Values and Meta-Features with CoT: Integrating raw data, meta-features, and CoT reasoning.</li>
</ul>
<h1>3.3 Model Selection</h1>
<p>The model space is denoted as: $\mathcal{M}=\left{m_{1}, m_{2}, \ldots\right}$. Each model $m_{i} \in \mathcal{M}$ is given by the tuple $m_{i}=\left(a_{i}, h_{i}, g_{i}(\cdot)\right)$, where $a_{i}$ is the forecasting algorithm, $h_{i}$ is the hyper-parameter vector associated with $a_{i}$, and $g_{i}(\cdot): \mathbb{R}^{n_{i}} \rightarrow \mathbb{R}^{n_{i}}$ is the time-series data representation (e.g., raw, exponential smoothing).
Unlike traditional meta-learning approaches that operate on a predefined, discrete model space, our method allows for an infinite and continuous model space, where hyperparameters $h_{i}$ can take any real-valued configuration.</p>
<h3>3.4 Comparison with Meta-Learning</h3>
<p>Meta-learning methods typically rely on an extensive performance matrix: $\mathbf{P} \in \mathbb{R}^{n \times m}$ where $\mathbf{P}<em j="j">{i, j}$ represents the performance of model $M</em>$. This matrix is computationally expensive to construct and is essential for training meta-learners. In contrast, our approach eliminates the need for:}$ on dataset $D_{i</p>
<ul>
<li>Explicit performance matrix. Our method does not require historical model-dataset performance mappings.</li>
<li>Feature engineering. While meta-learners depend on carefully designed meta-features, our LLM-based selection can operate without them.</li>
<li>Fixed model spaces. Our method does not restrict selection to a predefined set of models and hyperparameters.</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align: center;">Methods</th>
<th style="text-align: center;">hit@1 accuracy $\uparrow$</th>
<th style="text-align: center;">hit@5 accuracy $\uparrow$</th>
<th style="text-align: center;">hit@10 accuracy $\uparrow$</th>
<th style="text-align: center;">hit@50 accuracy $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Random selection</td>
<td style="text-align: center;">0.31</td>
<td style="text-align: center;">1.25</td>
<td style="text-align: center;">3.63</td>
<td style="text-align: center;">14.75</td>
</tr>
<tr>
<td style="text-align: center;">Popular Selection</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1.33</td>
<td style="text-align: center;">3.77</td>
<td style="text-align: center;">19.94</td>
</tr>
<tr>
<td style="text-align: center;">ISAC*</td>
<td style="text-align: center;">0.82</td>
<td style="text-align: center;">2.67</td>
<td style="text-align: center;">4.10</td>
<td style="text-align: center;">11.45</td>
</tr>
<tr>
<td style="text-align: center;">$M L P^{*}$</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;">1.13</td>
<td style="text-align: center;">4.51</td>
<td style="text-align: center;">22.25</td>
</tr>
<tr>
<td style="text-align: center;">Ours-Llama3.2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">w. data</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;">3.84</td>
<td style="text-align: center;">6.65</td>
<td style="text-align: center;">26.27</td>
</tr>
<tr>
<td style="text-align: center;">w. data+CoT</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;">3.12</td>
<td style="text-align: center;">5.82</td>
<td style="text-align: center;">26.69</td>
</tr>
<tr>
<td style="text-align: center;">w. data+meta_features</td>
<td style="text-align: center;">1.14</td>
<td style="text-align: center;">4.47</td>
<td style="text-align: center;">7.27</td>
<td style="text-align: center;">29.60</td>
</tr>
<tr>
<td style="text-align: center;">w. data+meta_features+CoT</td>
<td style="text-align: center;">1.14</td>
<td style="text-align: center;">3.43</td>
<td style="text-align: center;">6.44</td>
<td style="text-align: center;">25.96</td>
</tr>
<tr>
<td style="text-align: center;">Ours-GPT4o</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">w. data</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">2.39</td>
<td style="text-align: center;">4.47</td>
<td style="text-align: center;">21.39</td>
</tr>
<tr>
<td style="text-align: center;">w. data+CoT</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">1.25</td>
<td style="text-align: center;">4.36</td>
<td style="text-align: center;">21.39</td>
</tr>
<tr>
<td style="text-align: center;">w. data+meta_features</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;">2.39</td>
<td style="text-align: center;">4.88</td>
<td style="text-align: center;">20.56</td>
</tr>
<tr>
<td style="text-align: center;">w. data+meta_features+CoT</td>
<td style="text-align: center;">0.52</td>
<td style="text-align: center;">2.60</td>
<td style="text-align: center;">4.88</td>
<td style="text-align: center;">21.91</td>
</tr>
<tr>
<td style="text-align: center;">Ours-Gemini2.0 flash</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">w. data</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;">3.53</td>
<td style="text-align: center;">20.77</td>
</tr>
<tr>
<td style="text-align: center;">w. data+CoT</td>
<td style="text-align: center;">0.31</td>
<td style="text-align: center;">0.93</td>
<td style="text-align: center;">2.91</td>
<td style="text-align: center;">19.94</td>
</tr>
<tr>
<td style="text-align: center;">w. data+meta_features</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1.04</td>
<td style="text-align: center;">2.80</td>
<td style="text-align: center;">20.87</td>
</tr>
<tr>
<td style="text-align: center;">w. data+meta_features+CoT</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">1.35</td>
<td style="text-align: center;">3.01</td>
<td style="text-align: center;">17.13</td>
</tr>
</tbody>
</table>
<p>Table 1: hit@k Accuracy (the higher ( $\uparrow$ ), the better) comparison of LLMs against the different baselines. * denotes meta-learning methods which utilized performance matrix during training.</p>
<h1>4 Experiments</h1>
<p>We evaluate our LLM-based model selection approach through a series of experiments designed to address the following research questions:</p>
<ol>
<li>Does employing LLMs for time-series forecasting model selection improve performance compared to not using model selection or other techniques like meta-learners?</li>
<li>How much reduction in inference time do LLM-based methods achieve over the naïve approach, and what is the associated token cost for model selection?</li>
<li>To what extent do meta-features and CoT prompting contribute to model selection performance, computational efficiency, and token usage?</li>
</ol>
<h3>4.1 Experiment Settings</h3>
<h3>4.1.1 Dataset and Metrics</h3>
<p>Dataset Source. We use the same dataset as Abdallah et al. (2022), which consists of 321 forecasting datasets spanning various application domains, including finance, IoT, energy, and storage. These datasets include benchmark time series from Kaggle, Adobe real traces, and other open-source repositories. For each dataset, we randomly sample time windows of fixed length $(=16)$ to form our evaluation samples.
Evaluation Metrics. Our evaluation focuses on two primary metrics: hit@k accuracy and average Mean Squared Error (MSE). Hit@k accuracy quantifies whether the selected model ranks among the top $k$ models based on ground truth performance, while MSE measures the forecast error magnitude. Formally, hit@k accuracy is defined as:</p>
<p>$$
\text { hit@k }=\frac{1}{N} \sum_{i=1}^{N} \mathbb{1}\left(M_{i}^{*} \in \mathcal{M}<em i="i">{\text {ranked }}^{k}\left(D</em>\right)\right)
$$</p>
<p>where $\mathcal{M}<em i="i">{\text {ranked }}\left(D</em>$ is within the top $k$ models and 0 otherwise, and $N$ is the total number of test datasets.
In addition, we record training and inference time, as well as token usage, to assess the computational efficiency and resource overhead of the approaches.
To make it fair to compare, we adopt the same model space as our baselines $\mathcal{M}$ from Abdallah et al. (2022), which comprises 322 unique models (see Table 5 for the complete list). This model space pairs seven state-of-the-art time-series forecasting algorithms with their corresponding hyperparameters and various data representation methods. In addition, we utilize the precomputed performance matrix from Abdallah et al. (2022) to evaluate our proposed methods.}\right)$ denotes the set of models ranked by their performance for a given dataset $D_{i}, \mathbb{1}(\cdot)$ is an indicator function that equals 1 if $M_{i}^{*</p>
<h3>4.1.2 LLMs and Hardware</h3>
<p>To evaluate the effectiveness of different LLMs in the forecasting model selection task, we conducted experiments using three competitive models: Llama 3.2-3B-Instruct (MetaAI, 2024), GPT-4o (OpenAI, 2024), and Gemini 2.0 Flash (Google, 2024). In experiments with Llama 3.2-3B-Instruct, we utilized a single NVIDIA A100 GPU with 80GB of memory. GPT-4o and Gemini 2.0 Flash are accessed via API.</p>
<h3>4.2 Baselines</h3>
<p>We compare our proposed approach against various baseline methods. They fall into two categories: methods that do not perform explicit model selection and meta-learning-based approaches.
No Model Selection. In this category, the same fixed model configuration or an ensemble of all models is applied. We consider the following strategies:</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Methods</th>
<th style="text-align: center;">Mean Square Error $\downarrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">No Model <br> Selection</td>
<td style="text-align: center;">Seasonal Naïve</td>
<td style="text-align: center;">$0.0345 \pm 0.0382$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DeepAR</td>
<td style="text-align: center;">$0.0164 \pm 0.0506$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Deep Factors</td>
<td style="text-align: center;">$0.0217 \pm 0.0415$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Random Forest</td>
<td style="text-align: center;">$0.0199 \pm 0.0398$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Prophet</td>
<td style="text-align: center;">$0.0155 \pm 0.0295$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Gaussian Process</td>
<td style="text-align: center;">$0.1661 \pm 0.2104$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">VAR</td>
<td style="text-align: center;">$0.0602 \pm 0.1260$</td>
</tr>
<tr>
<td style="text-align: center;">Meta learner</td>
<td style="text-align: center;">ISAC</td>
<td style="text-align: center;">$0.0071 \pm 0.0145$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MLP</td>
<td style="text-align: center;">$0.0351 \pm 0.1186$</td>
</tr>
<tr>
<td style="text-align: center;">LLM based</td>
<td style="text-align: center;">Ours-Llama3.2</td>
<td style="text-align: center;">$0.0081 \pm 0.0297$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Ours-GPT4o</td>
<td style="text-align: center;">$0.0234 \pm 0.0596$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Ours-Gemini2.0 flash</td>
<td style="text-align: center;">$0.0169 \pm 0.0407$</td>
</tr>
</tbody>
</table>
<p>Table 2: Results for one-step ahead forecasting (MSE; the lower $(\downarrow)$ the better). The selected model by LLMs yields second best performance compared to baseline meta-learners and SOTA methods.</p>
<ol>
<li>Random Model. A model configuration is randomly selected from the model space $\mathcal{M}$ for each time-series dataset.</li>
<li>Popular Model. The most widely used forecasting model, Prophet (Taylor \&amp; Letham, 2017), is selected given its strong community support (e.g., over 19k stars on GitHub).</li>
<li>SOTA Model. We consider seven state-of-the-art forecasting models. For each model, we create multiple configurations by adjusting hyperparameters and data representations, resulting in 10 to 72 variants per model, as detailed in Table 5. The variant that achieves the best average performance across all training datasets is selected.
Meta-learners. These approaches leverage performance matrix to guide model selection:</li>
<li>ISAC (Kadioglu et al., 2010): This clustering-based method groups training datasets based on their extracted meta-features. For a new dataset, ISAC identifies the nearest cluster and selects the best-performing model within that cluster.</li>
<li>MLP. Given the training datasets and selected time window, the MLP regressor directly maps the meta-features onto model performances by regression (Abdallah et al., 2022).</li>
</ol>
<h1>4.3 Overall Results</h1>
<p>Superiority of LLM-based Methods in hit@k and MSE. The results presented in Tables 1 and 2 highlight the effectiveness of our LLM-based approach compared to all baseline methods. Ours-LLaMA3.2 consistently outperforms other selection strategies across both hit@k accuracy and mean squared error (MSE). For instance, Ours-LLaMA3.2 achieves $100.27 \%, 92.83 \%, 77.32 \%$, and $61.20 \%$ higher hit@10 accuracy compared to Random Selection, Popular Selection, ISAC, and MLP, respectively. In terms of forecasting performance, the model selected by Ours-LLaMA3.2 achieves the second lowest MSE among all tested methods, outperforming traditional SOTA forecasting models while achieving performance comparable to the best meta-learning method. Notably, unlike meta-learning approaches that require an extensive precomputed performance matrix for training, our LLM-based method selects models instantly without training.
Runtime Analysis. The inference runtime statistics of our methods are presented in Table 6, where our best Llama-based method achieves an inference time of 6.7 seconds for most timeseries datasets. Additionally, as illustrated in Figure 2b, LLM-based methods demonstrate a substantial reduction in inference time compared to the naïve approach, which involves evaluating all possible models and selecting the best-performing one. Specifically, our Llama, GPT, and Gemini-based methods achieve median inference time reductions of $14 \times$, $18 \times$, and $89 \times$, respectively, over the naïve approach.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" />
(a) Average training and inference time (in seconds). Detailed mean and standard deviation values are provided in Table 6.
<img alt="img-2.jpeg" src="img-2.jpeg" />
(b) The inference time reduction of LLM-based methods over the naïve approach. Our Llama, GPT, and Gemini-based methods give a median reduction of $14 \mathrm{X}, 18 \mathrm{X}$, and 89 X over naïve approach on all the datasets.</p>
<p>Figure 2: Comparison of training and inference time across different methods.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Methods</th>
<th style="text-align: center;">Input Tokens</th>
<th style="text-align: center;">Output Tokens</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Ours-Llama3.2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">w. data</td>
<td style="text-align: center;">$661.35 \pm 1.16$</td>
<td style="text-align: center;">$157.14 \pm 508.51$</td>
</tr>
<tr>
<td style="text-align: left;">w. data+CoT</td>
<td style="text-align: center;">$739.35 \pm 1.16$</td>
<td style="text-align: center;">$519.00 \pm 1445.28$</td>
</tr>
<tr>
<td style="text-align: left;">w. data+meta_features</td>
<td style="text-align: center;">$20547.43 \pm 125.70$</td>
<td style="text-align: center;">$116.75 \pm 274.24$</td>
</tr>
<tr>
<td style="text-align: left;">w. data+meta_features+CoT</td>
<td style="text-align: center;">$20632.43 \pm 125.70$</td>
<td style="text-align: center;">$350.21 \pm 931.21$</td>
</tr>
<tr>
<td style="text-align: left;">Ours-GPT4o</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">w. data</td>
<td style="text-align: center;">$2418.98 \pm 1472.00$</td>
<td style="text-align: center;">$68.89 \pm 7.32$</td>
</tr>
<tr>
<td style="text-align: left;">w. data+CoT</td>
<td style="text-align: center;">$2711.98 \pm 1472.00$</td>
<td style="text-align: center;">$297.18 \pm 46.68$</td>
</tr>
<tr>
<td style="text-align: left;">w. data+meta_features</td>
<td style="text-align: center;">$22475.06 \pm 1490.00$</td>
<td style="text-align: center;">$67.54 \pm 8.37$</td>
</tr>
<tr>
<td style="text-align: left;">w. data+meta_features+CoT</td>
<td style="text-align: center;">$22750.06 \pm 1490.00$</td>
<td style="text-align: center;">$300.68 \pm 46.50$</td>
</tr>
<tr>
<td style="text-align: left;">Ours-Gemini2.0 flash</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">w. data</td>
<td style="text-align: center;">$3075.64 \pm 2192.18$</td>
<td style="text-align: center;">$84.33 \pm 7.28$</td>
</tr>
<tr>
<td style="text-align: left;">w. data+CoT</td>
<td style="text-align: center;">$3075.64 \pm 2192.18$</td>
<td style="text-align: center;">$340.53 \pm 60.29$</td>
</tr>
<tr>
<td style="text-align: left;">w. data+meta_features</td>
<td style="text-align: center;">$26761.32 \pm 2267.30$</td>
<td style="text-align: center;">$80.99 \pm 4.06$</td>
</tr>
<tr>
<td style="text-align: left;">w. data+meta_features+CoT</td>
<td style="text-align: center;">$27080.32 \pm 2267.30$</td>
<td style="text-align: center;">$352.31 \pm 80.54$</td>
</tr>
</tbody>
</table>
<p>Table 3: Input and output token count for each time series dataset.</p>
<h1>4.4 Ablation Studies and Additional Analyses</h1>
<p>Meta-Features. As shown in the Table [ 1, 6, 3], incorporating meta-features in the prompt improves the performance of Llama and GPT-based methods, while the Gemini-based method appears to be less impacted. This improvement likely stems from the additional information provided by meta-features, which aids in selecting more suitable models. Besides, this performance gain comes at the cost of increased computational overheadinference time rises by at least $25 \%$, and prompt token usage expands by at least 7 X .
Chain-of-Thought Prompting. Based on the Table [ 1, 6, 3], explicitly incorporating CoT reasoning in the prompt-guiding the LLM to select the forecasting algorithm, hyperparameters, and data representation step by step-does not necessarily enhance model selection performance and sometimes even degrades it while significantly increasing computational costs, leading to at least a 2 X increase in inference time and a 4 X rise in output token usage. We suspect that CoT prompting introduces unnecessary complexity, causing the LLM to overanalyze irrelevant aspects of the selection process. Unlike tasks where reasoning clarifies logic, model selection may benefit more from direct pattern recognition. The added reasoning steps could also increase the risk of hallucination, leading to suboptimal choices.</p>
<p>Data Representation. Instead of allowing the LLM to select the data representation, we fixed it to either exponential smoothing or raw data. The results in Table 4 indicate that exponential smoothing enhances model selection performance for Llama and GPT-based methods, whereas it negatively impacts Gemini's selection performance.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 3: Average Number of Invalid Outputs for LLMs.</p>
<p>Limitations of Different LLMs. Llama3.2 achieves the best model selection performance among the three tested LLMs; however, it produces the most incomplete or irregular outputs as shown in Figure 3. In contrast, Gemini2.0 flash consistently generates complete and valid outputs while also having the lowest inference time and token usage. However, its performance is the weakest, under certain prompt settings, it even underperforms random selection. GPT4o serves as a balanced choice, delivering strong performance that surpasses almost all baselines, with only a few invalid outputs where the selected model falls outside the predefined model space.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">hit@k accuracy</th>
<th style="text-align: center;">Methods</th>
<th style="text-align: center;">Data Representation</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LLM Selection</td>
<td style="text-align: center;">Exponential Smoothing</td>
<td style="text-align: center;">Raw</td>
</tr>
<tr>
<td style="text-align: center;">hit@1</td>
<td style="text-align: center;">Ours-Llama3.2</td>
<td style="text-align: center;">1.14</td>
<td style="text-align: center;">1.04</td>
<td style="text-align: center;">0.31</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Ours-GPT4o</td>
<td style="text-align: center;">0.52</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;">0.31</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Ours-Gemini2.0 flash</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.42</td>
</tr>
<tr>
<td style="text-align: center;">hit@5</td>
<td style="text-align: center;">Ours-Llama3.2</td>
<td style="text-align: center;">4.47</td>
<td style="text-align: center;">4.98</td>
<td style="text-align: center;">3.32</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Ours-GPT4o</td>
<td style="text-align: center;">2.60</td>
<td style="text-align: center;">2.60</td>
<td style="text-align: center;">1.66</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Ours-Gemini2.0 flash</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;">0.31</td>
<td style="text-align: center;">2.18</td>
</tr>
<tr>
<td style="text-align: center;">hit@10</td>
<td style="text-align: center;">Ours-Llama3.2</td>
<td style="text-align: center;">7.27</td>
<td style="text-align: center;">8.41</td>
<td style="text-align: center;">4.47</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Ours-GPT4o</td>
<td style="text-align: center;">4.88</td>
<td style="text-align: center;">6.33</td>
<td style="text-align: center;">3.01</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Ours-Gemini2.0 flash</td>
<td style="text-align: center;">3.53</td>
<td style="text-align: center;">1.87</td>
<td style="text-align: center;">2.70</td>
</tr>
<tr>
<td style="text-align: center;">hit@50</td>
<td style="text-align: center;">Ours-Llama3.2</td>
<td style="text-align: center;">29.60</td>
<td style="text-align: center;">31.15</td>
<td style="text-align: center;">14.23</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Ours-GPT4o</td>
<td style="text-align: center;">21.91</td>
<td style="text-align: center;">23.36</td>
<td style="text-align: center;">11.84</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Ours-Gemini2.0 flash</td>
<td style="text-align: center;">20.77</td>
<td style="text-align: center;">19.94</td>
<td style="text-align: center;">8.10</td>
</tr>
</tbody>
</table>
<p>Table 4: hit@k accuracy of LLM-based model selection, where the data representation is either chosen by the LLM or defaults to Exponential Smoothing or Raw.</p>
<h1>5 Conclusion, Limitations and Future Work</h1>
<p>In this work, we applied LLMs to the time-series forecasting model selection problem for the first time. Through extensive experiments, we demonstrated that LLMs can effectively address this task without relying on a precomputed performance matrix of historical modeldataset pair evaluations. Additionally, this method significantly reduces computational overhead, achieving up to 89X faster inference compared to exhaustive model evaluation. Despite the performance, the underlying mechanisms remain unclear. Furthermore, our current approach has been evaluated solely on univariate datasets. In future work, we aim to expand our testbed to incorporate a more diverse set of datasets and models, further exploring the generalizability of LLM-based model selection.</p>
<h2>Ethics Statement</h2>
<p>Our research adheres to COLM Code of Ethics, ensuring that LLM-based model selection for time series forecasting is developed and applied responsibly. We prioritize fairness,</p>
<p>transparency, and data privacy, avoiding biases that could impact decision-making across different forecasting applications. By leveraging LLMs for model selection without requiring an extensive historical performance matrix, our approach reduces potential biases introduced by past model rankings. Continuous ethical assessments guide our research to align with societal and regulatory standards.</p>
<h1>References</h1>
<p>Mustafa Abdallah, Ryan Rossi, Kanak Mahadik, Sungchul Kim, Handong Zhao, and Saurabh Bagchi. Autoforecast: Automatic time-series forecasting model selection. In Proceedings of the 31st ACM International Conference on Information \&amp; Knowledge Management, CIKM '22, pp. 5-14, New York, NY, USA, 2022. Association for Computing Machinery. ISBN 9781450392365. doi: 10.1145/3511808.3557241. URL https: //doi.org/10.1145/3511808.3557241.</p>
<p>Alexander Alexandrov, Konstantinos Benidis, Michael Bohlke-Schneider, Valentin Flunkert, Jan Gasthaus, Tim Januschowski, Danielle C. Maddix, Syama Rangapuram, David Salinas, Jasper Schulz, Lorenzo Stella, Ali Caner TĂ!4rkmen, and Yuyang Wang. Gluonts: Probabilistic and neural time series modeling in python. Journal of Machine Learning Research, 21 (116):1-6, 2020. URL http://jmlr.org/papers/v21/19-820.html.
C. Bui, N. Pham, Anh Vo, A. Tran, A. Nguyen, and Trung Le. Time series forecasting for healthcare diagnosis and prognostics with the focus on cardiovascular diseases. In 6th International Conference on the Development of Biomedical Engineering in Vietnam (BME6), pp. 809-818, 06 2018. ISBN 978-981-10-4360-4. doi: 10.1007/978-981-10-4361-1_138.</p>
<p>Defu Cao, Furong Jia, Sercan O Arik, Tomas Pfister, Yixiang Zheng, Wen Ye, and Yan Liu. Tempo: Prompt-based generative pre-trained transformer for time series forecasting, 2024. URL https://arxiv.org/abs/2310.04948.</p>
<p>Vitor Cerqueira, Luis Torgo, and Carlos Soares. Model selection for time series forecasting: Empirical analysis of different estimators, 2022. URL https://arxiv.org/abs/2104. 00584 .</p>
<p>Jui-Sheng Chou and Duc-Son Tran. Forecasting energy consumption time series using machine learning techniques based on usage patterns of residential householders. Energy, 165:709-726, 2018. ISSN 0360-5442. doi: https://doi.org/10.1016/j.energy.2018.09.144. URL https://www.sciencedirect.com/science/article/pii/S0360544218319145.</p>
<p>Robert Fildes, Shaohui Ma, and Stephan Kolassa. Retail forecasting: Research and practice. International Journal of Forecasting, 38(4):1283-1318, 2022. ISSN 0169-2070. doi: https:// doi.org/10.1016/j.ijforecast.2019.06.004. URL https://www.sciencedirect.com/science/ article/pii/5016920701930192X. Special Issue: M5 competition.</p>
<p>Google. Gemini 2.0 flash, 2024. URL https://cloud.google.com/vertex-ai/ generative-ai/docs/gemini-v2#2.0-flash.</p>
<p>Nate Gruver, Marc Finzi, Shikai Qiu, and Andrew Gordon Wilson. Large language models are zero-shot time series forecasters, 2024. URL https://arxiv.org/abs/2310.07820.</p>
<p>Tao Han, Song Guo, Zhenghao Chen, Wanghan Xu, and Lei Bai. How far are today's time-series models from real-world weather forecasting applications?, 2024. URL https: //arxiv.org/abs/2406.14399.</p>
<p>Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y. Zhang, Xiaoming Shi, Pin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, and Qingsong Wen. Time-llm: Time series forecasting by reprogramming large language models, 2024. URL https://arxiv.org/ abs/2310.01728.</p>
<p>Serdar Kadioglu, Yuri Malitsky, Meinolf Sellmann, and Kevin Tierney. Isac -instance-specific algorithm configuration. In Proceedings of the 2010 Conference on ECAI 2010: 19th European Conference on Artificial Intelligence, pp. 751-756, NLD, 2010. IOS Press. ISBN 9781607506058.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners, 2023. URL https://arxiv.org/abs/2205. 11916 .</p>
<p>Shanu Kumar, Saish Mendke, Karody Lubna Abdul Rahman, Santosh Kurasa, Parag Agrawal, and Sandipan Dandapat. Enhancing zero-shot chain of thought prompting via uncertainty-guided strategy selection, 2024. URL https://arxiv.org/abs/2412.00353.</p>
<p>Christiane Lemke and Bogdan Gabrys. Meta-learning for time series forecasting and forecast combination. Neurocomputing, 73(10):2006-2016, 2010. ISSN 0925-2312. doi: https://doi.org/10.1016/j.neucom.2009.09.020. URL https://www.sciencedirect.com/ science/article/pii/S0925231210001074. Subspace Learning / Selected papers from the European Symposium on Time Series Prediction.</p>
<p>Richard Lewis and Gregory C Reinsel. Prediction of multivariate time series by autoregressive model fitting. Journal of Multivariate Analysis, 16(3):393-411, 1985. ISSN 0047-259X. doi: https://doi.org/10.1016/0047-259X(85)90027-2. URL https://www.sciencedirect. com/science/article/pii/0047259X85900272.</p>
<p>Yinheng Li. A practical survey on zero-shot prompt design for in-context learning. In Ruslan Mitkov and Galia Angelova (eds.), Proceedings of the 14th International Conference on Recent Advances in Natural Language Processing, pp. 641-647, Varna, Bulgaria, September 2023. INCOMA Ltd., Shoumen, Bulgaria. URL https://aclanthology.org/2023.ranlp-1.69/.</p>
<p>Andy Liaw and Matthew Wiener. Classification and regression by randomforest. Forest, 23, 112001 .</p>
<p>MetaAI. Llama 3.2 model card, 2024. URL https://www.llama.com/docs/ model-cards-and-prompt-formats/llama3_2/.</p>
<p>Pablo Montero-Manso, George Athanasopoulos, Rob J. Hyndman, and Thiyanga S. Talagala. Fforma: Feature-based forecast model averaging. International Journal of Forecasting, 36(1): 86-92, 2020. ISSN 0169-2070. doi: https://doi.org/10.1016/j.ijforecast.2019.02.011. URL https://www.sciencedirect.com/science/article/pii/S0169207019300895. M4 Competition.</p>
<p>OpenAI. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024.
Namyong Park, Ryan Rossi, Nesreen Ahmed, and Christos Faloutsos. Metagl: Evaluationfree selection of graph learning models via meta-learning, 2023. URL https://arxiv.org/ abs/2206.09280.</p>
<p>Ricardo B.C. Prudêncio and Teresa B. Ludermir. Meta-learning approaches to selecting time series models. Neurocomputing, 61:121-137, 2004. ISSN 0925-2312. doi: https:// doi.org/10.1016/j.neucom.2004.03.008. URL https://www.sciencedirect.com/science/ article/pii/S0925231204002310. Hybrid Neurocomputing: Selected Papers from the 2nd International Conference on Hybrid Intelligent Systems.</p>
<p>David Salinas, Valentin Flunkert, and Jan Gasthaus. Deepar: Probabilistic forecasting with autoregressive recurrent networks, 2019. URL https://arxiv.org/abs/1704.04110.</p>
<p>Omer Berat Sezer, Mehmet Ugur Gudelek, and Ahmet Murat Ozbayoglu. Financial time series forecasting with deep learning : A systematic literature review: 2005-2019. Applied Soft Computing, 90:106181, 2020. ISSN 1568-4946. doi: https://doi.org/10. 1016/j.asoc.2020.106181. URL https://www.sciencedirect.com/science/article/pii/ S1568494620301216.</p>
<p>Skipper Seabold and Josef Perktold. Statsmodels: Econometric and Statistical Modeling with Python. In Stéfan van der Walt and Jarrod Millman (eds.), Proceedings of the 9th Python in Science Conference, pp. 92 - 96, 2010. doi: 10.25080/Majora-92bf1922-011.</p>
<p>Yongqian Sun, Daguo Cheng, Tiankai Yang, Yuhe Ji, Shenglin Zhang, Man Zhu, Xiao Xiong, Qiliang Fan, Minghan Liang, Dan Pei, et al. Efficient and robust kpi outlier detection for large-scale datacenters. IEEE Transactions on Computers, 72(10):2858-2871, 2023.</p>
<p>Sean Taylor and Benjamin Letham. Forecasting at scale, 092017.
Joaquin Vanschoren. Meta-learning: A survey, 2018. URL https://arxiv.org/abs/1810. 03548 .</p>
<p>Chunyang Wang, Yanmin Zhu, Haobing Liu, Tianzi Zang, Jiadi Yu, and Feilong Tang. Deep meta-learning in recommendation systems: A survey, 2022. URL https://arxiv.org/ abs/2206.04415.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023. URL https://arxiv.org/abs/2201.11903.</p>
<p>Weizhong Yan, Hai Qiu, and Ya Xue. Gaussian process for long-term time-series forecasting. In 2009 International Joint Conference on Neural Networks, pp. 3420-3427, 2009. doi: 10.1109/ IJCNN.2009.5178729.</p>
<p>Xiyuan Zhang, Ranak Roy Chowdhury, Rajesh K. Gupta, and Jingbo Shang. Large language models for time series: A survey, 2024. URL https://arxiv.org/abs/2402.01801.</p>
<p>Yue Zhao, Ryan A. Rossi, and Leman Akoglu. Automating outlier detection via metalearning, 2021. URL https://arxiv.org/abs/2009.10606.</p>
<h1>A Appendix</h1>
<h2>A. 1 Prompt Structure</h2>
<p>The prompt structure we used is illustrated as follows.</p>
<h2>Prompt and Response Structure</h2>
<p>Prompt: "[Role and Objective]...
[Model Space]...
<a href="optional">CoT Reasoning</a>...
[Input]...Dataset Values...Meta Features(optional)...
[Output Format]...
[Rules]..."
Response:
{"reasoning": "...",
"result": { "forecasting algorithm": "...",
"hyperparameters": [ {"name": "...", "value": "..."}, {"name": "...", "value": "..."} ],
"data representation": "..." } }
We formulate the model selection problem following the framework of Abdallah et al. (2022).</p>
<h2>A. 2 Datasets and Meta-Features</h2>
<p>Our approach relies on a collection of historical time-series forecasting datasets, denoted as $\mathcal{D}=\left{D_{1}, D_{2}, \ldots, D_{N}\right}$, where $N$ is the total number of datasets. Each dataset $D_{i}$ comprises a sequence of observations in $\mathbb{R}^{n_{i}}$, with $n_{i}$ representing the number of observations in $D_{i}$.
For each dataset $D_{i} \in \mathcal{D}$, we randomly sample $T$ windows. Each time window $w_{t}$ is a contiguous segment of observations from $D_{i}$ with length $\left|w_{t}\right|$ that is smaller than the total length of $D_{i}$. For example, $\left|w_{10}\right|=16$ indicates that the 10th window contains 16 consecutive observations.</p>
<p>Meta-Features Tensor. To analyze the impact of meta-features on model selection in our approach, we utilize extracted meta-features for each time-series dataset from Abdallah et al. (2022)'s work.</p>
<p>Definition 1. Given a time-series dataset $D_{i}$, we define the meta-features tensor $\mathcal{F}<em 1="1">{i}=$ $\left{F</em>$, given by}^{i}, \ldots, F_{T}^{i}\right} \in \mathbb{R}^{T \times d}$, where the meta-features matrix $F_{k}^{i} \in \mathbb{R}^{d}$ captures the set of metafeatures corresponding to the time window $w_{k}$ of the dataset $D_{i</p>
<p>$$
F_{k}^{i} \triangleq\left{\psi\left(w_{k}\left(D_{i}\right)\right) \mid \psi: \mathbb{R}^{\left|w_{k}\right|} \rightarrow \mathbb{R}^{d}\right}
$$</p>
<p>where $\psi(\cdot): \mathbb{R}^{\left|w_{k}\right|} \rightarrow \mathbb{R}^{d}$ represents the feature extraction module and $d$ denotes the number of the meta-features.
The extracted meta-features capture the key characteristics of each dataset and are grouped into five categories, as proposed by(Vanschoren, 2018):</p>
<ul>
<li>Simple: General task properties.</li>
<li>Statistical: Properties of the underlying dataset distributions.</li>
<li>Information-theoretic: Entropy measures.</li>
<li>Spectral: Frequency domain properties.</li>
<li>Landmarker: Forecasting models' attributes on the task.</li>
</ul>
<h2>A. 3 Performance Matrix</h2>
<p>Now we introduce the performance matrix:</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Forecasting Algorithm</th>
<th style="text-align: center;">HyperParameter(s)</th>
<th style="text-align: center;">Data Representation</th>
<th style="text-align: center;">Total</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">DeepAR <br> (Salinas et al., 2019)</td>
<td style="text-align: center;">num_cells $=[10,20,30,40,50]$ <br> num_rnn_layers $=[1,2,3,4,5]$</td>
<td style="text-align: center;">{Exp_smoothing, Raw }</td>
<td style="text-align: center;">50</td>
</tr>
<tr>
<td style="text-align: center;">DeepFactor <br> (Salinas et al., 2019)</td>
<td style="text-align: center;">num_hidden_global $=[10,20,30,40,50]$ <br> num_global_factors $=[1,5,10,15,20]$</td>
<td style="text-align: center;">{Exp_smoothing, Raw }</td>
<td style="text-align: center;">50</td>
</tr>
<tr>
<td style="text-align: center;">Prophet <br> (Taylor \&amp; Letham, 2017)</td>
<td style="text-align: center;">changepoint_prior_scale $=[0.001,0.01,0.1,0.2,0.5]$ <br> seasonality_prior_scale $=[0.01,0.1,1.0,5.0,10.0]$</td>
<td style="text-align: center;">{Exp_smoothing, Raw }</td>
<td style="text-align: center;">50</td>
</tr>
<tr>
<td style="text-align: center;">Seasonal Naive <br> (Montero-Manso et al., 2020)</td>
<td style="text-align: center;">season_length $=[1,5,7,10,30]$</td>
<td style="text-align: center;">{Exp_smoothing, Raw }</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: center;">Gaussian Process <br> (Yan et al., 2009)</td>
<td style="text-align: center;">cardinality $=[2,4,6,8,10]$ <br> max_iter_jitter $=[5,10,15,20,25]$</td>
<td style="text-align: center;">{Exp_smoothing, Raw }</td>
<td style="text-align: center;">50</td>
</tr>
<tr>
<td style="text-align: center;">Vector Auto Regression <br> (Lewis \&amp; Reinsel, 1985)</td>
<td style="text-align: center;">cov_type $=\left{{ }^{\prime \prime} \mathrm{HC} 0^{\prime \prime},{ }^{\prime \prime} \mathrm{HC} 1^{\prime \prime},{ }^{\prime \prime} \mathrm{HC} 2^{\prime \prime},{ }^{\prime \prime} \mathrm{HC} 3^{\prime \prime}, " n o n r o b u s t "}\right.$ <br> trend $=\left{{ }^{\prime} \mathrm{n}^{\prime},{ }^{\prime} \mathrm{c}^{\prime},{ }^{\prime} \mathrm{f}^{\prime},{ }^{\prime} \mathrm{ct}^{\prime}\right}$</td>
<td style="text-align: center;">{Exp_smoothing, Raw }</td>
<td style="text-align: center;">40</td>
</tr>
<tr>
<td style="text-align: center;">Random Forest Regressor <br> (Liaw \&amp; Wiener, 2001)</td>
<td style="text-align: center;">n_estimators $=[10,50,100,250,500,1000]$ <br> max_depth $=[2,5,10,25,50$ 'None']</td>
<td style="text-align: center;">{Exp_smoothing, Raw }</td>
<td style="text-align: center;">72</td>
</tr>
</tbody>
</table>
<p>Table 5: Time-Series Forecasting Model Space. See hyperparameter definitions for various algorithms from GluonTS(Alexandrov et al., 2020) and statsmodels(Skipper Seabold \&amp; Josef Perktold, 2010). The number of models (last column) is all possible combinations of hyperparameters and data representations.</p>
<p>Definition 2. Given a training database $\mathcal{D}$ and a model space $\mathcal{M}$, we define the performance matrix $\mathbf{P} \in \mathbb{R}^{T \times n \times m}$ as</p>
<p>$$
\mathbf{P}=\left{\mathbf{P}<em 2="2">{1}, \mathbf{P}</em>\right}
$$}, \ldots, \mathbf{P}_{T</p>
<p>where $\mathbf{P}<em k="k">{k}=\left(p</em>$. We denote}^{i, l}\right) \in \mathbb{R}^{n \times m}$ and the element $p_{k}^{i, l}=M_{i}\left(w_{k}\left(D_{i}\right)\right)$ denotes the $j$ th model $M_{j}$ 's performance on the time window $w_{k}$ of the $i$ th training dataset $D_{i</p>
<p>$$
p_{k}^{i}=\left[\begin{array}{llll}
p_{k}^{i, 1} &amp; \ldots &amp; p_{k}^{i, m}
\end{array}\right]
$$</p>
<p>as the performance vector of all models in $\mathcal{M}$ on time window $w_{k}$ of $D_{i}$.
We denote the performance of a model on a time window using forecasting error metrics such as Mean Squared Error (e.g., MSE) of that model on that window.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Methods</th>
<th style="text-align: center;">Training Time</th>
<th style="text-align: center;">Inference Time</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Naïve</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">$70.9500 \pm 1.7801$</td>
</tr>
<tr>
<td style="text-align: center;">$I S A C^{*}$</td>
<td style="text-align: center;">$278.8083 \pm 57.9900$</td>
<td style="text-align: center;">$10.2480 \pm 2.7182$</td>
</tr>
<tr>
<td style="text-align: center;">$M L P^{*}$</td>
<td style="text-align: center;">$705.2908 \pm 123.3715$</td>
<td style="text-align: center;">$1.2745 \pm 0.5198$</td>
</tr>
<tr>
<td style="text-align: center;">Ours-Llama3.2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">w. data</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">$4.7201 \pm 16.9481$</td>
</tr>
<tr>
<td style="text-align: center;">w. data+CoT</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">$11.6174 \pm 32.7716$</td>
</tr>
<tr>
<td style="text-align: center;">w. data+meta_features</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">$6.7067 \pm 17.8676$</td>
</tr>
<tr>
<td style="text-align: center;">w. data+meta_features+CoT</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">$17.1308 \pm 50.1011$</td>
</tr>
<tr>
<td style="text-align: center;">Ours-GPT4o</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">w. data</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">$1.4905 \pm 0.7234$</td>
</tr>
<tr>
<td style="text-align: center;">w. data+CoT</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">$4.7821 \pm 2.0229$</td>
</tr>
<tr>
<td style="text-align: center;">w. data+meta_features</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">$2.4630 \pm 0.7546$</td>
</tr>
<tr>
<td style="text-align: center;">w. data+meta_features+CoT</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">$5.2368 \pm 1.4140$</td>
</tr>
<tr>
<td style="text-align: center;">Ours-Gemini2.0 flash</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">w. data</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">$0.7780 \pm 0.0543$</td>
</tr>
<tr>
<td style="text-align: center;">w. data+CoT</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">$2.1587 \pm 0.3407$</td>
</tr>
<tr>
<td style="text-align: center;">w. data+meta_features</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">$0.9790 \pm 0.0597$</td>
</tr>
<tr>
<td style="text-align: center;">w. data+meta_features+CoT</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">$2.4974 \pm 0.4361$</td>
</tr>
</tbody>
</table>
<p>Table 6: Average and standard deviation inference and training runtime performance (in seconds) over all datasets.</p>            </div>
        </div>

    </div>
</body>
</html>
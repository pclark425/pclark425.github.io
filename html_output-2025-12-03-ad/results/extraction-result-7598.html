<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7598 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7598</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7598</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-141.html">extraction-schema-141</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models estimate probabilities for future real-world scientific discoveries, including model details, prediction targets, datasets, forecasting horizon, probability estimation methods, evaluation metrics, reported performance, calibration quality, baselines, limitations, and concrete probability examples.</div>
                <p><strong>Paper ID:</strong> paper-275182544</p>
                <p><strong>Paper Title:</strong> Large language models for causal hypothesis generation in science</p>
                <p><strong>Paper Abstract:</strong> Towards the goal of understanding the causal structure underlying complex systems—such as the Earth, the climate, or the brain—integrating Large language models (LLMs) with data-driven and domain-expertise-driven approaches has the potential to become a game-changer, especially in data and expertise-limited scenarios. Debates persist around LLMs’ causal reasoning capacities. However, rather than engaging in philosophical debates, we propose integrating LLMs into a scientific framework for causal hypothesis generation alongside expert knowledge and data. Our goals include formalizing LLMs as probabilistic imperfect experts, developing adaptive methods for causal hypothesis generation, and establishing universal benchmarks for comprehensive comparisons. Specifically, we introduce a spectrum of integration methods for experts, LLMs, and data-driven approaches. We review existing approaches for causal hypothesis generation and classify them within this spectrum. As an example, our hybrid (LLM + data) causal discovery algorithm illustrates ways for deeper integration. Characterizing imperfect experts along dimensions such as (1) reliability, (2) consistency, (3) uncertainty, and (4) content vs. reasoning are emphasized for developing adaptable methods. Lastly, we stress the importance of model-agnostic benchmarks.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7598.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7598.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models estimate probabilities for future real-world scientific discoveries, including model details, prediction targets, datasets, forecasting horizon, probability estimation methods, evaluation metrics, reported performance, calibration quality, baselines, limitations, and concrete probability examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o (experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (used for causal-graph and CIT queries in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4o was used by the authors to query conditional-independence statements and to elicit full causal graphs; experiments report use with different temperature settings and both single-shot self-reported confidences and ensembles of repeated queries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary large autoregressive transformer (OpenAI family) used as a probabilistic black-box: responses sampled under different temperature settings; exact training regime not specified in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>probability_estimation_method</strong></td>
                            <td>Self-reported confidence (ask model to state confidence) and ensemble sampling (repeated independent queries at nonzero temperature; proportion of times an edge/answer appears aggregated into a confidence), plus aggregation by majority/supermajority voting and application of a statistical test on answer proportions (p-value test).</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>Causal hypotheses: presence/orientation of edges and conditional-independence (CI) statements between variables (graph structure), not explicit forecasting of novel discoveries.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Scientific causal discovery (examples: molecular biology - Sachs dataset; climatology/food security - Somalia variables).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Sachs protein-signaling dataset (used to compare graphs); Somalia food-security variable set (11 variables) used as exploratory example; no training-data details for GPT-4o provided in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>forecasting_horizon</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Topological graph distances (Structural Hamming Distance - SHD) and adjustment/interventional distances (AID variants: ancestor AID, Oset AID, parent AID) for comparing estimated graphs to ground truth; calibration/uncertainty assessed qualitatively and by empirical self-report vs ensemble proportions.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_performance</strong></td>
                            <td>Graphs from GPT-4o were compared with ground-truth (Sachs consensus) using SHD and AID distances (Table 2); paper does not report numeric performance values in-text (numbers are in Table 2 of the paper repository), but reports that ensemble querying (100 prompts, temperature=1, 20% supermajority) retrieved a graph used in comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Authors report overconfidence in self-reported confidences (self-reported confidences poorly reflect actual variability); ensemble proportions show wider variability. Example ranges reported: self-reported confidences ~0.7–0.95; ensemble-derived edge frequencies approximately in (0, 0.9). Overall: not well-calibrated / overconfident.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Purely data-driven causal discovery (PC algorithm), pairwise LLM queries, triplet/topological-order approaches, chatPC hybrid method, ensemble full-graph queries (majority/supermajority voting), and standard data-only benchmarks (e.g., PC, GES).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Model is a black box with unspecified training corpus; self-reported confidences are overconfident and mis-calibrated; hallucinations and factual blind spots (recent knowledge absent from training) can distort outputs; temperature and prompt choices materially affect outputs; ensemble sampling reduces but does not eliminate hallucination; limited, domain-specific ground truth datasets make evaluation and benchmarking difficult.</td>
                        </tr>
                        <tr>
                            <td><strong>probability_examples</strong></td>
                            <td>Self-reported edge confidences from GPT-4o (temperature=0) varied between ~0.7 and 0.95 in an example; ensemble-of-100 queries (GPT-4o, temperature=1) produced per-edge frequencies approximately in (0, 0.9); ensemble aggregation used a 20% supermajority threshold in the Sachs example.</td>
                        </tr>
                        <tr>
                            <td><strong>real_world_future</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large language models for causal hypothesis generation in science', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7598.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7598.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models estimate probabilities for future real-world scientific discoveries, including model details, prediction targets, datasets, forecasting horizon, probability estimation methods, evaluation metrics, reported performance, calibration quality, baselines, limitations, and concrete probability examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ensemble-proportion aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ensemble repeated-query proportion (edge frequency) aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A probability-estimation approach that queries an LLM multiple times (possibly with temperature > 0) and uses the proportion of times an answer (e.g., an edge or YES/NO for a CI) appears as an estimated probability/confidence for that statement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>probability_estimation_method</strong></td>
                            <td>Repeated independent queries to the LLM; compute empirical frequency of a target answer (e.g., presence of an edge or YES for dependence) across the sample; optionally aggregate via majority/supermajority thresholds or use proportion directly as confidence.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>Probability/confidence of pairwise causal relations, conditional-independence (CI) answers, and full-graph edges.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Causal discovery across domains (biology, climate/food security used as examples).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Applied to Sachs dataset and Somalia variables for demonstration; general method not tied to a specific training dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>forecasting_horizon</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Empirical agreement with ground-truth graphs measured via SHD and AID; calibration evaluated by comparing ensemble frequencies to actual correctness/variability.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_performance</strong></td>
                            <td>In Sachs example: aggregated 100 queries (GPT-4o, temperature=1) and applied a 20% supermajority threshold to form an estimated graph; individual per-edge frequencies observed up to ~0.9 and down to near 0 in examples (no single scalar performance summary for probabilities reported).</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Ensemble frequencies better reflect output variability than self-reported confidences but still can be misaligned with true correctness; authors describe calibration gap remains and recommend further calibration/aggregation studies.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Simple majority voting, self-reported confidence, weighted voting using reported uncertainties, and statistical-testing aggregation (p-value tests) described elsewhere in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires many model queries (compute and cost); depends on choice of temperature and prompt perturbations; still vulnerable to systemic hallucinations/biases in the model; ensemble frequency is an empirical estimator but not guaranteed calibrated against real-world correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>probability_examples</strong></td>
                            <td>Sachs experiment: 100 repeated GPT-4o queries at temperature=1; supermajority threshold 20% used; ensemble-derived per-edge confidences approximately in (0, 0.9).</td>
                        </tr>
                        <tr>
                            <td><strong>real_world_future</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large language models for causal hypothesis generation in science', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7598.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7598.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models estimate probabilities for future real-world scientific discoveries, including model details, prediction targets, datasets, forecasting horizon, probability estimation methods, evaluation metrics, reported performance, calibration quality, baselines, limitations, and concrete probability examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-reported confidence</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model self-reported confidence elicitation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Technique where the LLM is explicitly asked to return a confidence/probability alongside its YES/NO or graph answer; used by authors to visualize per-edge confidences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same GPT-4o experimental use: prompted to output a confidence score per answer (deterministic sampling setting used in example, temperature=0).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>probability_estimation_method</strong></td>
                            <td>Direct elicitation: prompt the LLM to provide a numeric confidence or probability for each asserted statement.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>Confidence in conditional-independence answers and causal-graph edges.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Causal discovery (Sachs example shown).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Sachs dataset used for illustrative visualization; no training-set calibration data provided.</td>
                        </tr>
                        <tr>
                            <td><strong>forecasting_horizon</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Compared qualitatively and visually against ensemble-derived frequencies; calibration discussed qualitatively (overconfidence).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_performance</strong></td>
                            <td>Reported example range of self-reported confidences (GPT-4o, temperature=0) was roughly between 0.7 and 0.95 for edges in a displayed graph.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Authors report that self-reported confidences are overconfident and do not capture true output variability; self-reports poorly match ensemble-derived variability.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Ensemble-proportion aggregation, statistical p-value aggregation, weighted voting that incorporates reported uncertainties.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Self-reported confidences are not reliable/calibrated indicators of true correctness; prone to overconfidence and may not reflect model sampling variability or factual correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>probability_examples</strong></td>
                            <td>In figure 6(a) self-reported confidences (GPT-4o, temperature=0) are shown with values in the range ~0.7–0.95; authors note opacity differences are subtle because model is similarly confident across edges.</td>
                        </tr>
                        <tr>
                            <td><strong>real_world_future</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large language models for causal hypothesis generation in science', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7598.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7598.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models estimate probabilities for future real-world scientific discoveries, including model details, prediction targets, datasets, forecasting horizon, probability estimation methods, evaluation metrics, reported performance, calibration quality, baselines, limitations, and concrete probability examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Statistical p-value aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Testing answer-proportion differences via p-value (CIT aggregation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An aggregation protocol that models LLM YES/NO answers as Bernoulli draws and uses hypothesis testing on the proportions (p_yes vs p_no) to decide CI outcomes, controlling false positive rate with significance level α.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>probability_estimation_method</strong></td>
                            <td>Collect batch of n independent LLM answers; compute p_yes and p_no (proportions); perform a statistical test of null hypothesis (e.g., p_no ≥ p_yes or p_yes ≥ p_no) and reject at significance level α (authors use α = 0.05) to make a decision about dependence/independence.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>Decision/confidence about conditional-independence (CI) statements between variables (used within hybrid PC algorithm).</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Causal discovery / conditional-independence testing.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Applied in methodology and experiments (Somalia example, sachs experiments); not tied to a separate training dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>forecasting_horizon</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Statistical test p-values; downstream graph quality measured by SHD and AID when comparing to ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_performance</strong></td>
                            <td>No single performance scalar reported for the test itself; authors demonstrate this procedure provides principled control of false positive rate in CIT answers when integrating LLM-derived CI statements into PC-like algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Method attempts principled control of false positives via hypothesis testing, but depends on independence of samples and the reliability of the model's sampling process — limitations remain regarding model biases and non-iid responses.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Majority voting, weighted voting using self-reported confidences, Potyka et al social choice aggregation approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Assumes answer draws approximate independent Bernoulli trials; choice of null (which side to control) affects false-positive semantics; model sampling dependence and prompt-induced correlations may violate test assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>probability_examples</strong></td>
                            <td>Authors propose α = 0.05 for rejection; they frame the null hypotheses as p_no ≥ p_yes or p_yes ≥ p_no and recommend the expert choose which null to use depending on the context (controls which outcome is seen as positive).</td>
                        </tr>
                        <tr>
                            <td><strong>real_world_future</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large language models for causal hypothesis generation in science', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7598.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7598.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models estimate probabilities for future real-world scientific discoveries, including model details, prediction targets, datasets, forecasting horizon, probability estimation methods, evaluation metrics, reported performance, calibration quality, baselines, limitations, and concrete probability examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG (Retrieval-Augmented Generation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A technique in which relevant textual documents are retrieved and included in the prompt to ground LLM responses and reduce hallucination; discussed as a way to improve reliability and support probability estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>probability_estimation_method</strong></td>
                            <td>Ground LLM answers on retrieved external documents (cite number/quality of sources) to reduce hallucination and to weight reliability of model outputs; can be combined with ensemble sampling or confidence elicitation.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>Improved factual grounding for causal relation assertions and CI answers (thus improving confidence estimates for causal hypotheses).</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>General knowledge-intensive LLM tasks; applied to pairwise causal queries and graph construction in cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Not a single dataset — RAG systems retrieve from an external text database (could be scientific literature, knowledge bases); Zhang et al [182] applied RAG for pairwise causal queries (referenced).</td>
                        </tr>
                        <tr>
                            <td><strong>forecasting_horizon</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Reductions in hallucination (Hallucination Index), improvements in factuality and retrieval quality; downstream graph-distance metrics when used in causal graph elicitation.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_performance</strong></td>
                            <td>Paper references Zhang et al [182] who explored RAG to enhance pairwise causal queries; no numeric forecasting-performance numbers given in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>RAG can improve factuality and thus reliability of reported confidences, but author notes RAG-dependent retrieval quality and source trustworthiness must be evaluated; RAG does not fully remove hallucination or bias.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Plain LLM prompting without retrieval, knowledge-base queries, structured KB grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Quality and recency of retrieval corpus limit performance; if KBs were populated using LLMs, RAG can inherit errors; RAG adds system complexity and requires document-quality assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>probability_examples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>real_world_future</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large language models for causal hypothesis generation in science', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7598.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7598.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models estimate probabilities for future real-world scientific discoveries, including model details, prediction targets, datasets, forecasting horizon, probability estimation methods, evaluation metrics, reported performance, calibration quality, baselines, limitations, and concrete probability examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Perplexity & Hallucination Index</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Perplexity and Hallucination Index (HI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Perplexity is a standard likelihood-based fit metric on tokens; Hallucination Index is a multi-task metric measuring tendency to produce made-up or inconsistent information; both are discussed as diagnostics relevant to reliability of LLM-derived probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>probability_estimation_method</strong></td>
                            <td>Perplexity indicates model fit to a corpus (useful for model selection) and HI quantifies hallucination tendency (useful to judge whether elicited probabilities may be trustworthy); neither directly produces calibrated probabilities but inform which models/prompts are reliable.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>General measure of LLM reliability when used to assert causal facts; indirectly affects probability estimates for causal hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>General LLM evaluation across domains; relevant to scientific text grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Perplexity computed w.r.t. topic-specific literature or corpora; HI computed across several QA/RAG tasks as in referenced benchmarks (Galileo/others).</td>
                        </tr>
                        <tr>
                            <td><strong>forecasting_horizon</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Perplexity (negative log-likelihood), Hallucination Index computed from task suites (no single universal formula reported in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Perplexity is not tailored to causal tasks and may be less relevant when RAG is used; HI directly measures hallucination risk which authors flag as crucial to judge probability trustworthiness.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Perplexity ignores grounding and factuality when external retrieval is used; HI summaries may not capture domain-specific error modes relevant to causal claims.</td>
                        </tr>
                        <tr>
                            <td><strong>probability_examples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>real_world_future</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large language models for causal hypothesis generation in science', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large language models for constrained-based causal discovery <em>(Rating: 2)</em></li>
                <li>Causal reasoning and large language models: opening a new frontier for causality <em>(Rating: 2)</em></li>
                <li>Causal inference using LLM-guided discovery <em>(Rating: 2)</em></li>
                <li>Causal graph discovery with retrieval-augmented generation based large language models <em>(Rating: 2)</em></li>
                <li>Robust knowledge extraction from large language models using social choice theory <em>(Rating: 1)</em></li>
                <li>ChatPC: Large language models for constrained-based causal discovery (Cohrs et al.) <em>(Rating: 2)</em></li>
                <li>Hallucination index (Galileo 2023) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7598",
    "paper_id": "paper-275182544",
    "extraction_schema_id": "extraction-schema-141",
    "extracted_data": [
        {
            "name_short": "GPT-4o (experiments)",
            "name_full": "GPT-4o (used for causal-graph and CIT queries in this paper)",
            "brief_description": "GPT-4o was used by the authors to query conditional-independence statements and to elicit full causal graphs; experiments report use with different temperature settings and both single-shot self-reported confidences and ensembles of repeated queries.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "Proprietary large autoregressive transformer (OpenAI family) used as a probabilistic black-box: responses sampled under different temperature settings; exact training regime not specified in paper.",
            "model_size": null,
            "probability_estimation_method": "Self-reported confidence (ask model to state confidence) and ensemble sampling (repeated independent queries at nonzero temperature; proportion of times an edge/answer appears aggregated into a confidence), plus aggregation by majority/supermajority voting and application of a statistical test on answer proportions (p-value test).",
            "prediction_target": "Causal hypotheses: presence/orientation of edges and conditional-independence (CI) statements between variables (graph structure), not explicit forecasting of novel discoveries.",
            "domain": "Scientific causal discovery (examples: molecular biology - Sachs dataset; climatology/food security - Somalia variables).",
            "dataset_used": "Sachs protein-signaling dataset (used to compare graphs); Somalia food-security variable set (11 variables) used as exploratory example; no training-data details for GPT-4o provided in paper.",
            "forecasting_horizon": null,
            "evaluation_metric": "Topological graph distances (Structural Hamming Distance - SHD) and adjustment/interventional distances (AID variants: ancestor AID, Oset AID, parent AID) for comparing estimated graphs to ground truth; calibration/uncertainty assessed qualitatively and by empirical self-report vs ensemble proportions.",
            "reported_performance": "Graphs from GPT-4o were compared with ground-truth (Sachs consensus) using SHD and AID distances (Table 2); paper does not report numeric performance values in-text (numbers are in Table 2 of the paper repository), but reports that ensemble querying (100 prompts, temperature=1, 20% supermajority) retrieved a graph used in comparisons.",
            "calibration_quality": "Authors report overconfidence in self-reported confidences (self-reported confidences poorly reflect actual variability); ensemble proportions show wider variability. Example ranges reported: self-reported confidences ~0.7–0.95; ensemble-derived edge frequencies approximately in (0, 0.9). Overall: not well-calibrated / overconfident.",
            "baseline_methods": "Purely data-driven causal discovery (PC algorithm), pairwise LLM queries, triplet/topological-order approaches, chatPC hybrid method, ensemble full-graph queries (majority/supermajority voting), and standard data-only benchmarks (e.g., PC, GES).",
            "limitations": "Model is a black box with unspecified training corpus; self-reported confidences are overconfident and mis-calibrated; hallucinations and factual blind spots (recent knowledge absent from training) can distort outputs; temperature and prompt choices materially affect outputs; ensemble sampling reduces but does not eliminate hallucination; limited, domain-specific ground truth datasets make evaluation and benchmarking difficult.",
            "probability_examples": "Self-reported edge confidences from GPT-4o (temperature=0) varied between ~0.7 and 0.95 in an example; ensemble-of-100 queries (GPT-4o, temperature=1) produced per-edge frequencies approximately in (0, 0.9); ensemble aggregation used a 20% supermajority threshold in the Sachs example.",
            "real_world_future": false,
            "uuid": "e7598.0",
            "source_info": {
                "paper_title": "Large language models for causal hypothesis generation in science",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Ensemble-proportion aggregation",
            "name_full": "Ensemble repeated-query proportion (edge frequency) aggregation",
            "brief_description": "A probability-estimation approach that queries an LLM multiple times (possibly with temperature &gt; 0) and uses the proportion of times an answer (e.g., an edge or YES/NO for a CI) appears as an estimated probability/confidence for that statement.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "probability_estimation_method": "Repeated independent queries to the LLM; compute empirical frequency of a target answer (e.g., presence of an edge or YES for dependence) across the sample; optionally aggregate via majority/supermajority thresholds or use proportion directly as confidence.",
            "prediction_target": "Probability/confidence of pairwise causal relations, conditional-independence (CI) answers, and full-graph edges.",
            "domain": "Causal discovery across domains (biology, climate/food security used as examples).",
            "dataset_used": "Applied to Sachs dataset and Somalia variables for demonstration; general method not tied to a specific training dataset.",
            "forecasting_horizon": null,
            "evaluation_metric": "Empirical agreement with ground-truth graphs measured via SHD and AID; calibration evaluated by comparing ensemble frequencies to actual correctness/variability.",
            "reported_performance": "In Sachs example: aggregated 100 queries (GPT-4o, temperature=1) and applied a 20% supermajority threshold to form an estimated graph; individual per-edge frequencies observed up to ~0.9 and down to near 0 in examples (no single scalar performance summary for probabilities reported).",
            "calibration_quality": "Ensemble frequencies better reflect output variability than self-reported confidences but still can be misaligned with true correctness; authors describe calibration gap remains and recommend further calibration/aggregation studies.",
            "baseline_methods": "Simple majority voting, self-reported confidence, weighted voting using reported uncertainties, and statistical-testing aggregation (p-value tests) described elsewhere in paper.",
            "limitations": "Requires many model queries (compute and cost); depends on choice of temperature and prompt perturbations; still vulnerable to systemic hallucinations/biases in the model; ensemble frequency is an empirical estimator but not guaranteed calibrated against real-world correctness.",
            "probability_examples": "Sachs experiment: 100 repeated GPT-4o queries at temperature=1; supermajority threshold 20% used; ensemble-derived per-edge confidences approximately in (0, 0.9).",
            "real_world_future": false,
            "uuid": "e7598.1",
            "source_info": {
                "paper_title": "Large language models for causal hypothesis generation in science",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Self-reported confidence",
            "name_full": "Model self-reported confidence elicitation",
            "brief_description": "Technique where the LLM is explicitly asked to return a confidence/probability alongside its YES/NO or graph answer; used by authors to visualize per-edge confidences.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "Same GPT-4o experimental use: prompted to output a confidence score per answer (deterministic sampling setting used in example, temperature=0).",
            "model_size": null,
            "probability_estimation_method": "Direct elicitation: prompt the LLM to provide a numeric confidence or probability for each asserted statement.",
            "prediction_target": "Confidence in conditional-independence answers and causal-graph edges.",
            "domain": "Causal discovery (Sachs example shown).",
            "dataset_used": "Sachs dataset used for illustrative visualization; no training-set calibration data provided.",
            "forecasting_horizon": null,
            "evaluation_metric": "Compared qualitatively and visually against ensemble-derived frequencies; calibration discussed qualitatively (overconfidence).",
            "reported_performance": "Reported example range of self-reported confidences (GPT-4o, temperature=0) was roughly between 0.7 and 0.95 for edges in a displayed graph.",
            "calibration_quality": "Authors report that self-reported confidences are overconfident and do not capture true output variability; self-reports poorly match ensemble-derived variability.",
            "baseline_methods": "Ensemble-proportion aggregation, statistical p-value aggregation, weighted voting that incorporates reported uncertainties.",
            "limitations": "Self-reported confidences are not reliable/calibrated indicators of true correctness; prone to overconfidence and may not reflect model sampling variability or factual correctness.",
            "probability_examples": "In figure 6(a) self-reported confidences (GPT-4o, temperature=0) are shown with values in the range ~0.7–0.95; authors note opacity differences are subtle because model is similarly confident across edges.",
            "real_world_future": false,
            "uuid": "e7598.2",
            "source_info": {
                "paper_title": "Large language models for causal hypothesis generation in science",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Statistical p-value aggregation",
            "name_full": "Testing answer-proportion differences via p-value (CIT aggregation)",
            "brief_description": "An aggregation protocol that models LLM YES/NO answers as Bernoulli draws and uses hypothesis testing on the proportions (p_yes vs p_no) to decide CI outcomes, controlling false positive rate with significance level α.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "probability_estimation_method": "Collect batch of n independent LLM answers; compute p_yes and p_no (proportions); perform a statistical test of null hypothesis (e.g., p_no ≥ p_yes or p_yes ≥ p_no) and reject at significance level α (authors use α = 0.05) to make a decision about dependence/independence.",
            "prediction_target": "Decision/confidence about conditional-independence (CI) statements between variables (used within hybrid PC algorithm).",
            "domain": "Causal discovery / conditional-independence testing.",
            "dataset_used": "Applied in methodology and experiments (Somalia example, sachs experiments); not tied to a separate training dataset.",
            "forecasting_horizon": null,
            "evaluation_metric": "Statistical test p-values; downstream graph quality measured by SHD and AID when comparing to ground truth.",
            "reported_performance": "No single performance scalar reported for the test itself; authors demonstrate this procedure provides principled control of false positive rate in CIT answers when integrating LLM-derived CI statements into PC-like algorithms.",
            "calibration_quality": "Method attempts principled control of false positives via hypothesis testing, but depends on independence of samples and the reliability of the model's sampling process — limitations remain regarding model biases and non-iid responses.",
            "baseline_methods": "Majority voting, weighted voting using self-reported confidences, Potyka et al social choice aggregation approaches.",
            "limitations": "Assumes answer draws approximate independent Bernoulli trials; choice of null (which side to control) affects false-positive semantics; model sampling dependence and prompt-induced correlations may violate test assumptions.",
            "probability_examples": "Authors propose α = 0.05 for rejection; they frame the null hypotheses as p_no ≥ p_yes or p_yes ≥ p_no and recommend the expert choose which null to use depending on the context (controls which outcome is seen as positive).",
            "real_world_future": false,
            "uuid": "e7598.3",
            "source_info": {
                "paper_title": "Large language models for causal hypothesis generation in science",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "RAG (Retrieval-Augmented Generation)",
            "name_full": "Retrieval-Augmented Generation (RAG)",
            "brief_description": "A technique in which relevant textual documents are retrieved and included in the prompt to ground LLM responses and reduce hallucination; discussed as a way to improve reliability and support probability estimates.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "probability_estimation_method": "Ground LLM answers on retrieved external documents (cite number/quality of sources) to reduce hallucination and to weight reliability of model outputs; can be combined with ensemble sampling or confidence elicitation.",
            "prediction_target": "Improved factual grounding for causal relation assertions and CI answers (thus improving confidence estimates for causal hypotheses).",
            "domain": "General knowledge-intensive LLM tasks; applied to pairwise causal queries and graph construction in cited work.",
            "dataset_used": "Not a single dataset — RAG systems retrieve from an external text database (could be scientific literature, knowledge bases); Zhang et al [182] applied RAG for pairwise causal queries (referenced).",
            "forecasting_horizon": null,
            "evaluation_metric": "Reductions in hallucination (Hallucination Index), improvements in factuality and retrieval quality; downstream graph-distance metrics when used in causal graph elicitation.",
            "reported_performance": "Paper references Zhang et al [182] who explored RAG to enhance pairwise causal queries; no numeric forecasting-performance numbers given in this paper.",
            "calibration_quality": "RAG can improve factuality and thus reliability of reported confidences, but author notes RAG-dependent retrieval quality and source trustworthiness must be evaluated; RAG does not fully remove hallucination or bias.",
            "baseline_methods": "Plain LLM prompting without retrieval, knowledge-base queries, structured KB grounding.",
            "limitations": "Quality and recency of retrieval corpus limit performance; if KBs were populated using LLMs, RAG can inherit errors; RAG adds system complexity and requires document-quality assessment.",
            "probability_examples": null,
            "real_world_future": false,
            "uuid": "e7598.4",
            "source_info": {
                "paper_title": "Large language models for causal hypothesis generation in science",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Perplexity & Hallucination Index",
            "name_full": "Perplexity and Hallucination Index (HI)",
            "brief_description": "Perplexity is a standard likelihood-based fit metric on tokens; Hallucination Index is a multi-task metric measuring tendency to produce made-up or inconsistent information; both are discussed as diagnostics relevant to reliability of LLM-derived probabilities.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "probability_estimation_method": "Perplexity indicates model fit to a corpus (useful for model selection) and HI quantifies hallucination tendency (useful to judge whether elicited probabilities may be trustworthy); neither directly produces calibrated probabilities but inform which models/prompts are reliable.",
            "prediction_target": "General measure of LLM reliability when used to assert causal facts; indirectly affects probability estimates for causal hypotheses.",
            "domain": "General LLM evaluation across domains; relevant to scientific text grounding.",
            "dataset_used": "Perplexity computed w.r.t. topic-specific literature or corpora; HI computed across several QA/RAG tasks as in referenced benchmarks (Galileo/others).",
            "forecasting_horizon": null,
            "evaluation_metric": "Perplexity (negative log-likelihood), Hallucination Index computed from task suites (no single universal formula reported in this paper).",
            "reported_performance": null,
            "calibration_quality": "Perplexity is not tailored to causal tasks and may be less relevant when RAG is used; HI directly measures hallucination risk which authors flag as crucial to judge probability trustworthiness.",
            "baseline_methods": null,
            "limitations": "Perplexity ignores grounding and factuality when external retrieval is used; HI summaries may not capture domain-specific error modes relevant to causal claims.",
            "probability_examples": null,
            "real_world_future": false,
            "uuid": "e7598.5",
            "source_info": {
                "paper_title": "Large language models for causal hypothesis generation in science",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large language models for constrained-based causal discovery",
            "rating": 2,
            "sanitized_title": "large_language_models_for_constrainedbased_causal_discovery"
        },
        {
            "paper_title": "Causal reasoning and large language models: opening a new frontier for causality",
            "rating": 2,
            "sanitized_title": "causal_reasoning_and_large_language_models_opening_a_new_frontier_for_causality"
        },
        {
            "paper_title": "Causal inference using LLM-guided discovery",
            "rating": 2,
            "sanitized_title": "causal_inference_using_llmguided_discovery"
        },
        {
            "paper_title": "Causal graph discovery with retrieval-augmented generation based large language models",
            "rating": 2,
            "sanitized_title": "causal_graph_discovery_with_retrievalaugmented_generation_based_large_language_models"
        },
        {
            "paper_title": "Robust knowledge extraction from large language models using social choice theory",
            "rating": 1,
            "sanitized_title": "robust_knowledge_extraction_from_large_language_models_using_social_choice_theory"
        },
        {
            "paper_title": "ChatPC: Large language models for constrained-based causal discovery (Cohrs et al.)",
            "rating": 2,
            "sanitized_title": "chatpc_large_language_models_for_constrainedbased_causal_discovery_cohrs_et_al"
        },
        {
            "paper_title": "Hallucination index (Galileo 2023)",
            "rating": 1,
            "sanitized_title": "hallucination_index_galileo_2023"
        }
    ],
    "cost": 0.016728,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Large language models for causal hypothesis generation in science
31 January 2025</p>
<p>Kai-Hendrik Cohrs kai.cohrs@uv.es 0000-0002-2286-7487
Processing ( )</p>
<p>Emilia Diaz 0000-0001-8410-6635
Processing ( )</p>
<p>Vasileios Sitokonstantino 0000-0001-5506-2872
Processing ( )</p>
<p>Gherardo Varand 0000-0002-6708-1103
Processing ( )</p>
<p>Gustau Camps-Vall 0000-0003-1683-2138
Processing ( )</p>
<p>Large language models for causal hypothesis generation in science
31 January 202586D03DDD8A102EC4B2D6C364B067138610.1088/2632-2153/ada47fRECEIVED 16 June 2024 REVISED 1 November 2024 ACCEPTED FOR PUBLICATION 30 December 2024causalitylarge language modelshypothesis generationsciencecausal discovery
Towards the goal of understanding the causal structure underlying complex systems-such as the Earth, the climate, or the brain-integrating Large language models (LLMs) with data-driven and domain-expertise-driven approaches has the potential to become a game-changer, especially in data and expertise-limited scenarios.Debates persist around LLMs' causal reasoning capacities.However, rather than engaging in philosophical debates, we propose integrating LLMs into a scientific framework for causal hypothesis generation alongside expert knowledge and data.Our goals include formalizing LLMs as probabilistic imperfect experts, developing adaptive methods for causal hypothesis generation, and establishing universal benchmarks for comprehensive comparisons.Specifically, we introduce a spectrum of integration methods for experts, LLMs, and data-driven approaches.We review existing approaches for causal hypothesis generation and classify them within this spectrum.As an example, our hybrid (LLM + data) causal discovery algorithm illustrates ways for deeper integration.Characterizing imperfect experts along dimensions such as (1) reliability, (2) consistency, (3) uncertainty, and (4) content vs. reasoning are emphasized for developing adaptable methods.Lastly, we stress the importance of model-agnostic benchmarks.</p>
<p>Introduction</p>
<p>The causal understanding of complex systems, such as the brain, society, the Earth, and climate, is key for informed decision-making, effective problem-solving, and the development of sustainable solutions.The causal link between greenhouse gas emissions and global warming, for instance, has shaped international climate agreements and prompted initiatives for renewable energy adoption, aiding in mitigating climate change impacts and fostering environmental sustainability [139].However, understanding these systems is not straightforward due to their nonlinear, dynamic, and interconnected nature, spanning multiple domains.Traditional methods, like data-driven causal discovery and expert-based graph construction, face limitations without adequate domain knowledge and sufficient data quantity and quality.</p>
<p>Data-driven causal discovery methods like PC [137] and GES [28] encounter challenges with diverse data types, and the hardness of conditional independence tests (CITs) [135].Causal sufficiency assumptions may lead to erroneous conclusions.Alternative methods do not assume causal sufficiency [18,30,58,98,137], but missing data problems and selection bias persist as important issues.</p>
<p>Alternatively, creating causal graphs through domain knowledge is challenging and time-consuming, requiring substantial expertise and labour [94].Experts need a deep understanding involving extensive consultations and reviews, adding significant time and resources.Furthermore, domain experts need a basic understanding of causality theory to build causal graphs.The complexity of real-world systems amplifies the difficulty, leading to potential oversights.These challenges highlight the necessity for more automated methodologies.Our schematic description of integrating experts, LLMs, and data-driven methods.The combination of LLMs and data-driven methods lies along different spectra.Along the vertical axis, we classify how much we rely on each source of information, ranging from purely LLM-and knowledge-driven to purely data-driven methods.In the horizontal dimension, we depict the degree of interaction between the sources, spanning from cases where the LLM is solely used as a prior for a data-driven method up to the algorithms where LLMs are used as a post-hoc procedure on the results of data-driven methods.Between these extremes, there is potential for developing methods that intimately integrate both worlds.Experts can appear at the beginning, the end, or continuously through an in-the-loop fashion.</p>
<p>Large language models (LLMs) accumulate and memorize vast amounts of information during their training on diverse textual data.As users interact with these models by providing prompts or queries, LLMs leverage this information to generate responses or perform specific tasks.LLMs have sparked debates about their capabilities and limitations in various fields.While concerns about the logical and causal reasoning capacity of LLMs persist [4,169,178], we focus on their role as probabilistic extractors of collective human knowledge.Instead of dwelling on philosophical questions, we suggest taming LLMs with domain experts and empirical data to achieve causal understanding.</p>
<p>The scientific method involves generating hypotheses from observing nature and existing knowledge, challenging them with experiments, and integrating new observations to advance modeling and understanding [18].Causality, inherent in the scientific method, seeks to establish relationships between variables.With their ability to leverage human knowledge, LLMs become instrumental in bridging the gap between data-driven and knowledge-driven approaches.</p>
<p>In many sectors of society, there is excitement and concern about how LLMs will change how things are done.This includes science, where LLMs could revolutionize how we produce and consume it.Concerns relating to the inherent problems of the models and their potential misuse have been raised [10].The EU and other actors have released guidelines for the responsible use of generative AI in research [1,11] calling for transparency with the models deployed, for instance, for hypothesis generation.Automated hypotheses must be thoroughly tested since human researchers should ultimately be responsible for their scientific output, regardless of the tools they use.LLMs hypotheses generation capabilities have been tested, for instance, in toy problems [156], for automating psychological hypothesis generation [147] as well for material and drug discovery [104].However, their capabilities for suggesting causal hypotheses and integration with data-driven methods have not been explored at large.This perspective aims to fill this gap.</p>
<p>Other works explore ideas on how LLMs can open new possibilities in the field of causality [79,179] or, conversely, how causality can be leveraged for the understanding of LLMs [93,155].These reviews are missing a broader perspective on how the interplay of experts, causality, and LLMs, combined with data, might be effectively used to advance science as a whole.</p>
<p>The rest of the paper is structured as follows: section 2 reviews related work on causal hypothesis generation, section 3 discusses recent works on causality with LLMs, and section 4 is devoted to the interplay between LLM-driven and data-driven approaches and explores avenues to improve these, which we summarize in figure 1.We highlight: (i) Characterize imperfect experts: Formalize LLMs as probabilistic imperfect experts that we query from answer ∼ LLM(prompt|T train , θ), and explore ways to characterize properties that can be leveraged in integrated algorithms.(ii) Develop adaptable methods for causal hypothesis generation: To dynamically integrate language models with data-driven methods and expert input, taking into account and evaluating the confidence level of each information source.(iii) Build universal/modal-agnostic benchmarks: Build benchmarks that allow a seamless comparison over the whole spectrum from data-driven to mixtures to pure knowledge-based methods.</p>
<p>Rather than questioning LLMs' logical or causal reasoning, we believe they should be integrated alongside expert knowledge and data into a scientific framework for causal hypothesis generation.</p>
<p>Causal hypotheses generation</p>
<p>Generating a causal hypothesis is a fundamental step in the scientific endeavour [119].It involves proposing potential explanations of why certain things happen.The development of plausible hypotheses to be tested via experiments, was, and still is, responsibility of the researcher.To conceive hypotheses, tools such as inductive (from specific observations to general conclusions) and abductive (inferring the most likely cause) reasoning [119] have traditionally been used.Researchers also imagine causal models to make sense of observational data, especially for unexpected findings [46].</p>
<p>Advanced statistical and machine-learning methods also empower researchers with additional tools for exploring associations in the data and formulating (causal) hypotheses [18,96,127,128,131].While statistical, machine learning, and causal discovery approaches cannot completely substitute the researcher in generating a causal hypothesis, the sheer complexity (system size, number of observations, non-linearities, unstructured data, etc) of the problems addressed by modern science require the integration of computational tools to help analyze and extract actionable information from data.In this sense, the automatic extraction of plausible causal diagrams from data has the potential to be a valuable tool for supporting the development of causal hypotheses.</p>
<p>Starting from the seminal work of Wright [166], using graphs to represent causal relationships has become widespread in causal analysis.In the prevalent structural causal model (SCM) formalism, a causal model is described by a system of equations that describe how the system's variables interact, typically shown as a Directed Acyclic Graph (DAG) [113,116,137].The DAG is a causal diagram that summarizes all the causal relationships in the system.The task of understanding a system causally corresponds to estimating its SCM.This process is usually divided into two parts: (1) figuring out the DAG (known as causal discovery) and ( 2) estimating the mathematical relationships (equations) between those variables [129].</p>
<p>A causal model does more than just describe the data-generating process of the system.Additionally, it allows us to ask 'what if ' questions such as 'What if I do. ..?' or 'What if I had done. ..?' [116].It describes what happens to the system when one or more of its equations change (interventions or experiments) or what would have happened if things had been different (counterfactual reasoning) [7].For example, what would happen if we intervened and changed one variable?How would that impact the others?This is at the core of causal modeling and analysis because a variable X is considered a cause of another variable Y only if changing X affects the probability of Y [161].An example of a simple SCM over four variables, their relationships (the DAG), and the corresponding equations are depicted in figure 2. We refer to Pearl et al [113], Spirtes et al [137] and Peters et al [116] for a complete description of SCMs, DAGs, and the graphical framework for causal models.While other graphical approaches exist for modeling causal systems [13,43,52,85,109,115,145,151], in this paper we restrict the treatment of causality to the mainstream SCM approach.A causal DAG (for instance, the graph associated with an SCM) can be elicited directly from experts or estimated from observational and/or interventional data (data collected after experiments on the system).In the following sections, we briefly introduce data-driven causal discovery methods (2.1) and methods that combine experts and data (2.2).</p>
<p>Data-driven causal discovery</p>
<p>Causal discovery methods within the DAG framework are usually categorized as constraint-based, score-based, or functional (or asymmetry) approaches [60,134].</p>
<p>(i) Constraint-based methods use statistical tests to determine which variables are conditionally independent.This information is then used to narrow down possible graphs that represent the system.(ii) Score-based approaches involve testing different possible graphs and scoring them based on how well they fit the data using score functions (e.g.penalized likelihood, such as Bayesian Information Criterion (BIC) or Akaike Information Criterion scores).(iii) Functional approaches make assumptions about the mathematical relationship between variables.</p>
<p>These assumptions induce asymmetries that can reveal the direction of causality (what causas what).</p>
<p>Constraint-based and score-based approaches harness the statistical properties that hold in the joint probability distribution induced by the SCM, thus they encounter their limits when the same statistical model could be represented by multiple SCMs over the same variables (a form of equifinality or unidentifiability [116,165]).As an example, the factorization of the joint probability distributions (and the corresponding conditional independence statements) in figure 2 is also implied by a different SCM.The associated DAG differs from the original only in that the X → Y edge is replaced with an Y ← X edge.Thus, the direction of the edge between X and Y is not identifiable from conditional independence statements or classical scores such as BIC.Mathematically, this is represented by the fact that the two DAGs are in the same Markov equivalence class [83].Observe that the above reasoning only exploits the DAG and the induced probability factorization, thus viewing the SCM only through its induced (Causal) Bayesian Network.Functional approaches, exploiting assumptions on the functions defining the SCM (e.g.additive noise models, f y (X, ε 2 ) = g(X) + ε 2 for the SCM in figure 2), can identify causal relationships beyond the limits of the Markov equivalence class [117].</p>
<p>A prominent score-based method is the Greedy Equivalent Search (GES) [28,29,102], which performs simple moves (add/remove edges, invert directions) in the space of Markov equivalence classes.A well-known constraint-based method is the PC-algorithm [137], which we briefly describe as it will serve to illustrate how LLMs can be integrated into causal discovery algorithms.The PC-algorithm strategy works in three steps: (i) the first step is to harness the information in conditional independence statements (X ⊥ ⊥ Y|Z), which are estimated to hold valid in the data, to prune edges starting from a full undirected graph, thus building the so-called skeleton of the causal graph.(ii) In the second step, colliders (also called v-structures, e.g.Z → W ← Y in figure 2) are oriented thanks to particular conditional (in)dependencies.(iii) The third and last step amounts to applying a set of graphical rules [101] to orient as many edges as possible.The result is a partially directed graph which, under Oracle assumptions for the CITs, represents the Markov equivalence class of the true causal graph.</p>
<p>We refer to recent surveys and reviews for a more comprehensive view of causal discovery methods [18,60,134].</p>
<p>Causal discovery with prior knowledge</p>
<p>The possibility of fusing prior expert knowledge with data-driven inputs has been identified as one of the advantages of graphical models [33].</p>
<p>For instance, score-based and constraint-based algorithms have been combined with different prior knowledge such as structural restrictions [38,39,87] or information on directed causal paths [14,15].Similarly, prior knowledge expressed in terms of ancestral or topological-order constraints has helped guide exact methods [23,160].</p>
<p>The Bayesian methodology straightforwardly incorporates prior knowledge on the causal structure with information in the data [51,63,142,170].Methods have been developed to build informative priors on graphs based on agreement with prior beliefs of different types [111] or with partial prior knowledge [21].Moreover, the Bayesian paradigm has also been used to integrate data with multiple sources of prior knowledge [3,164].</p>
<p>Human-in-the-loop methodologies are instead based on interactive experts' guidance to steer and correct data-driven methods [36,99].</p>
<p>Hybrid modeling in ML</p>
<p>The integration of data-driven methods and other sources of knowledge is not new.Related fields of research coined knowledge-guided machine learning, physics-aware machine learning and hybrid modeling [19,76,123], investigate ways in which established scientific knowledge can be combined with machine learning in different ways, spanning from hard constraints [31,80,148,183] to softer introduction of prior knowledge through regularization [34,121] or the Bayesian approach [173].In these approaches, scientific knowledge is usually considered true, or more valuable, and the data-driven ML model flawed.Hence, they explore ways to constrain black-box deep learning models with this knowledge.From our point of view, both LLMs and domain experts are sources of imperfect knowledge.We envision an even partnership between experts, LLMs, and data-driven methods since we remain agnostic regarding their relative value.The objective is to synergistically integrate them to diminish errors and elevate scientific understanding.</p>
<p>Causality with LLMs</p>
<p>LLMs present a promising knowledge-driven alternative to expert-based graph building or data-driven causal discovery methods and other causal tasks (see table 1).Different strategies exist to elicit causal graphs from LLMs effectively.These strategies include LLMs to predict causal relations [79,95], LLMs as priors for data-driven causal discovery methods [5,37], and LLMs to aid in downstream causal inference tasks by predicting the causal order of variables [152].Kıcıman et al [79] showcase LLMs' knowledge-based causal discovery capabilities by eliciting DAGs for a series of bivariate and multivariate benchmark datasets where the ground truth is known.Zhang et al [182] explores the possibility of enhancing the reliability of these pairwise relationships by applying a retrieval-augmented generation system on top of the LLM.Vashishtha et al [152] proposed an approach using the topological order over graph variables, showcasing the potential of LLMs in obtaining causal order and enhancing causal inference in diverse fields.Long et al [94] and [95] examined the role of LLMs in facilitating the construction of causal graphs, focusing on the automation of edge scoring and using expert knowledge to improve data-driven causal graph identification, respectively.There are ongoing efforts to explore and enhance the actual causal reasoning capabilities of LLMs [4,73].Some works investigate the coherence in the explanations that they provide [6], their ability to create narratives to proposed causal graphs [118], or their capacity to predict the effect of interventions [77].Zec ˇević et al [178] argued against the causal capabilities of LLMs, defining a subgroup of SCMs and emphasizing the reliance of LLMs on embedded causal knowledge.Cox [35] conducted a Socratic dialogue with an LLM, highlighting the necessity of sustained questioning for refining human reasoning imitated by LLMs and the reliability of their conclusions.Pawlowski et al [112] and Gat et al [56] explored using LLMs to enable precise causal reasoning and model-agnostic explanations.Other works explore LLMs as a pragmatic solution for full graph causal discovery.Chen et al [25] focused on Bayesian Networks (BNs) and proposed strategies for addressing erroneous prior causal statements in LLM-based causal structure learning.Zhang et al [180] discussed the potential of enhancing causal machine learning and leveraging LLMs for improved performance and trustworthiness.Cohrs et al [32] introduce a constraint-based approach leveraging LLM queries to test CI statements for the PC algorithm.In figure 3, we show three different approaches from the literature, plus an ensemble of full graph queries, which amount to directly asking the causal graph multiple times and output the weighted adjacency matrix resulting from the proportion of edges appearing across the repetitions.We notice that pairwise queries [79] and the triplets approach [152] obtain valid causal ordering, but at the cost of denser graphs, meaning graphs that include more edges, some of which might not be essential.The chatPC [32] method obtains a sparser skeleton, with fewer, more targeted edges, but, in this case, is unable to orient edges.A combination of chatPC and pairwise queries (figures 3(c) and (d)) would retrieve a sparse graph with the correct causal order.For this example, directly querying the full graph multiple times and aggregating by simple majority voting for the presence of edges retrieves the ground truth DAG.</p>
<p>Beyond causal discovery, some work has explored the usefulness of LLMs for causal effect estimation and the generation of counterfactual and interventional data.Veljanovski and Wood-Doughty [153] uses LLMs as an encoder for confounding information in text form within the causal effect estimation framework of double machine learning [27], particularly suited for high-dimensional confounders.Confounders in text format could contain, for instance, additional information about patients written by a doctor, which is then encoded into a high-dimensional vector space.For estimating causal effects of concepts in text-based models, LLMs were also shown to be useful for generating counterfactuals [48].In sentiment analysis, a specific problem is that spurious correlations may appear when certain entities, such as a specific movie or movie director, occur too often with positive sentiments.To mitigate these effects, Li et al [89] probe LLMs for generating suitable counterfactuals.</p>
<p>Taming LLMs for complex systems</p>
<p>There is a whole spectrum of ways in which experts, LLMs, and data-driven methods can seamlessly integrate, which we depict in figure 1.As experts are the only non-automatic and most time-consuming component, they would be better deployed only at the start or end of the automated pipeline.At the start, they may impose certain constraints or request a hypothesis.Ultimately, they receive the generated hypothesis and judge, refine, or falsify it.In the loop, they may even receive a request for feedback, more data, or additional knowledge.When integrating LLMs and data-driven methods, there is a two-dimensional spectrum in which these approaches complement each other.The first component describes how heavily we rely on the different sources of information with the purely data-driven or LLM-driven approaches at the extreme ends.We describe the spectrum of LLM and data integration as a second dimension.On one end, we have data • LLM, which is data-driven on top of an LLM approach.Here, the LLM approach is merely a preliminary step to the data-driven algorithm, somehow replacing an imperfect expert who provides constraints or prior on the possible causal hypothesis.Along these lines, Ban et al [5] introduced a framework that derives ancestral constraints from LLMs and successively applies score-based causal discovery incorporating these constraints.Vashishtha et al [152] proposed an LLM-based method for finding the causal order of a set of variables and suggested that the obtained result can serve as prior knowledge to a data-driven score-based causal discovery algorithm.Darvariu et al [37] investigated the capabilities of LLM priors derived from pairwise queries and found that combining knowledge and data-driven prior worked best.At the other end, there are the LLM • data approaches, which use LLMs on the output of a data-driven algorithm.In this line, Long et al [95] formalizes LLMs as imperfect experts and uses them to direct the undirected edges on the output of a data-driven causal-discovery method, such as a partially directed acyclic graph obtained with the PC algorithm.Khatibi et al [78] even go one step further with their autonomous LLM-augmented causal discovery framework, allowing the LLM to correct discovered or missing edges and directions.Li et al [88] work on bringing LLM priors closer to real-world applications with temporal causal discovery, common in the industrial context.On the line between these two extremes, there is space for approaches that allow deeper and variegated integrations of data-driven and knowledge-driven methods.</p>
<p>To illustrate this middle ground of deep integration of both worlds, we extend chatPC [32] to a hybrid LLM-driven PC algorithm that employs conditional independence statements stemming from either standard statistical tests (as in the purely data-driven PC), or conditional independence queries to an LLM as depicted in figure 4.This approach will serve as an example to illustrate the rich landscape of possibilities in which these sources can be integrated.</p>
<p>Exploring the uncertain: food insecurity in Africa.As an exploratory real-world example, we try to extract the causal graph among 11 variables related to food security in Somalia using a hybrid (data+LLM) PC algorithm.The Horn of Africa has seen a troubling increase in food insecurity, impacting 6.5 million people in 2022.Prolonged dry spells, along with factors like hydrological conditions, limited food production, market access issues, inadequate humanitarian aid, conflicts, and displacement, contribute to the complex challenges of households in Somalia [22].The purely data-driven causal discovery approach already (see figure 5) fails a very simple sanity check: El Niño-Southern Oscillation (ENSO) is completely independent of the standardized precipitation index (SPI).The climatological links are very well understood, and knowledge of the connection between these two variables is readily available.Another difficulty in this context is that for Figure 5. Generated causal hypotheses on 11 variables related to food security in Somalia.In teal edges appearing only in the purely data-driven graph, in orange the ones appearing only in the hybrid chatPC [31] graph.Nodes in magenta are the nodes for which LLM queries test conditional independences.A major issue in the data-driven graph is that ENSO is not a causal parent of SPI.Including the LLM in a hybrid fashion can retrieve a graph that corrects this mistake while obtaining a sparser graph.armed conflicts and staple crop yield, only two weak proxies are available: fatalities and sorghum production.To leverage the capabilities of LLMs in this graph, we query conditional independence statements (of conditioning set up to size 1) that contain the variables ENSO, SPI, armed conflicts, and yield with the LLM and find a graph (see figure 5) that corrects the previous issue.At this point, we want to note that the LLM querying was not done in a naive way but took into account the probabilistic nature of the responses.In what comes next, we will dive into what the field needs to effectively develop these hybrid methods and return to the more technical details of this example as an illustration.</p>
<p>LLMs as imperfect experts</p>
<p>At a high level, we may assume that LLMs provide probabilistic answers to a given prompt:
answer ∼ LLM (prompt|T train , θ) ,
where T train makes explicit the dependence on the training data, and θ is a vector of parameters (e.g. the temperature parameter for the GPT models).Such random answers can be employed in a variety of ways to generate causal hypotheses for a specific set of variables or processes (see figure 3).The optimal way of combining this causal hypothesis with data-driven causal discovery tools will depend on the specific induced distribution.While we assume the actual induced distribution will remain a black box, by probing and testing LLMs, it is possible to extract some of their properties [79].These properties may generally apply to any queries specific to the task.Specifically, various measures, evaluations, or indices must be developed to probe and compare different models and the impact of parameters and alternative training sets.Thus, we could answer questions such as: Is GPT4 better than GPT3.5 in causal reasoning?Does fine-tuning on scientific corpora improve CI statement queries?Some of these properties should be understood as in expectation and in the context of the specific model, with a particular training, a prompt, and parameters.LLMs can hallucinate [69] and make mistakes, and extracted information may not be undisputed.By sampling multiple outputs, we can attempt to average some random undesirable effects.This, however, can only reduce but not completely eradicate hallucinations or biases.We aim to retain structural features or problems and uncertain knowledge.Ideally, the designed properties should be interpretable, simple (aka unitary), and cheap to obtain over different setups or queries.We now present some examples from the literature and propose some novel characterizations.Some metrics are already established for LLMs but are usually tailored to characterize the specific model, excluding the prompting.Depending on the causal frame, i.e. how the LLM should be deployed, they can still guide the choice of the LLM for the integrated pipeline.In the following, we review promising characterizations.</p>
<p>Perplexity.Defined as the negative log-likelihood for a set of tokens given their context, it is a measure of fit to a given text corpus [75].Given specific knowledge bases relevant to the task, such as topic-specific literature or scientific peer-reviewed literature, it can guide the choice between different language models.While perplexity is not a measure tailored toward causal tasks, it is a basic characterization that is generally useful for describing LLMs.</p>
<p>Reliability.It is important to know how well the LLMs retrieve factual knowledge if we want to use them as knowledge extractors.Similar to other semi-automatic tools for knowledge extractions, their reliability needs to be investigated [59,62,71,100].Different studies and benchmarks for testing this have been proposed but were not tested on state-of-the-art models yet [9,67,158,177].Further studies have investigated the possibility of directly editing the knowledge of an LLM [90,175], which can be used for the good or the bad.On the one side, it allows us to enforce irrefutable truths.In contrast, on the other side, it is also a weak spot in which LLMs can be manipulated, and even minimum interventions can have unwanted side-effects [40,103,107].Recent knowledge is a clear blindspot when not included in the training set [140].Thus, instead of relying on the training data alone, there is the possibility of deploying a retrieval-augmented generation (RAG) system [86], which opens further possibilities for judging the information reliability.These systems retrieve relevant information from a text database by matching the latent representation of the query to the latent representation of text in the database.They include the most similar ones in the prompt.This allows for quantification of the reliability of the retrieved knowledge based on the number of sources, the quality of the sources, whether it is published research, whether it is backed by data, etc Zhang et al [182] explore this approach for pairwise queries to finally build a full graph.In particular, the LLM is also asked to suggest intermediate variables in case of indirect associations.A significant remaining difficulty is finding research that backs the full chain of suggested associations.Another line of research queries other sources of knowledge, such as structured knowledge bases [157].If previously these have been completed using LLMs, it might lead to a wrong sense of safety [168].</p>
<p>Hallucination.We note, however, that perplexity might not be so relevant if a retrieval augmented generation (RAG) is deployed [86].RAG improves the quality of generated text by incorporating relevant information retrieved from a large dataset or external sources, which can reduce perplexity by providing factual context, helping the LLM generate more accurate and coherent responses.Here, another popular quantity for LLMs may be useful: the hallucination index (HI).Hallucination describes the tendency of LLMs to generate made-up information or information inconsistent with some input [69].Several works explore ways to detect [72,84], reduce [2] and benchmark hallucincation [141].The HI is computed from three tasks, including question and answering with RAG, over several datasets to evaluate and rank different LLMs on their tendency to hallucinate [54].This is crucial to ensure information retrieval backed by scientific publications in RAG systems.One could argue that hallucination could be a blessing rather than a curse for generating surprising and new ideas.However, they often exhibit little value.Thus, mitigating it and Figure 6.Two examples of uncertainty for causal graph estimated with LLMs: (a) self-reported confidence on edges (gpt-4o temperature = 0); (b) confidence obtained as proportion of times edge appears over an ensemble of queries (gpt-4o, temperature = 1).Uncertainty on each edge is represented by varying the opacity.In (a) confidences vary between 0.7 and 0.95 while in (b) confidences vary approximately in (0, 0.9).Note that in (a) the difference in opacity is subtle as the model is overall similarly confident about all edges.</p>
<p>exploring combinatorial creativity [24,50], i.e. making unfamiliar combinations of familiar ideas, is the more promising avenue for generating novelty with LLMs.</p>
<p>Consistency.Consistency is a fundamental property that human experts are tested for and is shown to correlate with confidence and disagrement [91].Consistency could be measured and defined in a variety of ways.Intuitively, it amounts to the absence of contradictions in the model's responses.Technically, it could mean that answers should be invariant to some specific transformation of the queries.For example, a medical expert should be consistent in their diagnosis if queried at different times of the day without having acquired new information.Studies have investigated the detection and mitigation of self-contradiction within the model's responses [97].Supervised learning has proven to increase robustness in consistency [184].For multiple queries, another basic consistency test could consist of checking variations of the answers to minimal and irrelevant modifications of the prompt.A proposed consistency check for code-generating models meant that the language specification to its code and the code generated from that exact specification should agree [105].The same principle should similarly apply when LLMs create text from graphs and graphs from text.For the conditional independence statement queries [32] test permutation consistency since the result of a CIT query should not depend on the order of variables [32].This is conceptually similar to the answer's independence to the order of given options in multiple-choicetests [185].Nonetheless, aggregating over all permutations is a sensible approach [32,143], mitigating the'lost in the middle' problem, describing that LLMs give more importance to information at the start and the end of a prompt than to information in between [92].</p>
<p>Uncertainty.Another more widely applicable characterization is uncertainty or confidence [57].For each answer, it is possible to retrieve uncertainties in different ways: (i) the LLM may be asked to self-report uncertainty in the answers; (ii) uncertainty could be estimated directly from repeated independent queries; and finally (iii) probabilities could be provided directly from the model inference engine on each query.Given this possible uncertainty measures, we could investigate LLMs' self-calibration [146] or analyze the relationship between LLM's uncertainty and human-expert or society uncertainty on specific facts.Studies have found similar to humans, LLMs tend to be overconfident and that there is a persisting calibration gap between self-reported uncertainties and the actual mistakes that can be reduced with better models and aggregation strategies [167].Repeated queries might be enhanced by slightly altering the prompts, and the uncertainty in the explanations can be captured through common important tokens [144].These additional uncertainty characteristics could be deployed in adaptive algorithms to balance different sources of information.To showcase two basic alternatives, we produces uncertainties for the edges of the causal graph in the sachs problem.We do so by either asking the LLM to self-report a confidence score or by querying the LLM multiple times and reporting the proportion of times edges appear.Results, showing uncertainties of the edges via opacity, are shown in figure 6.We employ gpt4o with a temperature set to 0 for the self-reported uncertainties and 1 for the ensemble approach.In line with the literature, the model is overly self-confident in the answer, and self-reported confidences do not represent answer variability well.</p>
<p>Content vs. reasoning.Another property to be leveraged by integrated systems is linked to the derivation of knowledge, where the LLM prompt plays a crucial role in steering between content and reasoning.The former describes the extraction of information directly memorized from the knowledge base, and the latter is a combination of existing knowledge to infer something new.Previous studies [79,178] have tried to probe LLMs in such a way as to separate the task of the LLM in one or the other.Cai et al [17] set up an effect estimation framework for the effect of inherent knowledge on solving a causal task in combination with data.The prompt also defines the causal frame, i.e. how well the causal task is detailed [79, see], which establishes the context in natural language.This causal frame can determine how much causal reasoning the LLM carries out.For example, for settings where the causal frame is perfectly detailed, the prompt should ideally conduct the LLM only to access content-based information.However, when the causal frame is less specific, it may be desirable for the LLM to aid in identifying system variables not included in the context (hidden variables, for example) and even performing certain causal reasoning operations.A wider frame always opens up possibilities for generalization across different contexts.At the same time, it carries the danger of hallucinations or mistakes as general sound causal reasoning is still a debated question [55,178].This characteristic also implies the trustworthiness of the output and can guide algorithms that can balance information and data.Furthermore, algorithms could be designed to query from either end of the spectrum between content and reasoning for more grounded or bolder claims.</p>
<p>Synthesis.We strongly believe that more characteristics can be envisioned to guide the development of LLM-driven causal hypothesis generation techniques and integrated algorithms.Moreover, such characterizations would develop better LLMs, especially toward more accurate, trustworthy, and robust causal hypothesis generation.We thus consider the research and development of novel characterizations that measure LLMs to be paramount concerning causal reasoning, causal knowledge, knowledge extraction abilities, and, in general, causal hypothesis generation abilities.</p>
<p>Adaptable causal hypothesis generation methods</p>
<p>Ongoing work explores intimate integrations of machine learning algorithms and LLMs.On the one side, ML techniques inspire methods for choosing, aggregating, and enhancing responses from multiple LLMs, extending the chain-of-thought technique [163] to tree-of-thoughts [174], tree-of-reasoning [181], tree-of-uncertain-thoughts [68,108], graph-of-thoughts [8], boosting-of-thoughts [26] and aggregation-of-thoughts [176] or evolutionary algorithms for prompt optimization [61].On the other side, LLMs are explored to improve algorithms for machine learning, such as optimization [171] or Bayesian optimization [93], with the idea being that the LLMs can suggest good initial hyperparameters, for instance, for neural network training, which commonly feeds from the experience of the ML engineer.</p>
<p>Similarly, LLMs can be leveraged within algorithms for causal hypothesis generation.The characteristics and properties obtained by characterizing LLMs should go beyond gaining intuition on what kind of experts they are.Ideally, these characteristics can be used for effective causal discovery pipelines to integrate expert, LLM, and data more efficiently.We first illustrate this here with the example of CITs.We then propose some possible future lines of integration.</p>
<p>Adaptable CIT with LLMs.We shortly lay out how to query CITs with LLMs as proposed by Cohrs et al [32].We ask for an independent batch (size n) of answers.After laying out the reasoning, the model should answer (YES or NO) with some confidence in the given answer.The simplest way to aggregate the responses would be a simple majority voting schema over the answers, followed by a slightly more sophisticated method that also considers the reported uncertainties for weighted voting, which comes with the previously discussed calibration problems.A different approach was proposed by Potyka et al [120] using social choice theory for ranking and aggregation.Cohrs et al [32] introduces another method which is based on statistical testing, where we produce p-values for the null hypothesis p no ⩾ p yes (or alternatively p no ⩽ p yes ) where p no , p yes are the proportion of NO and YES answers over the total requested batch n.The so-obtained test is based on the idea that we want to test whether the probability of obtaining the answer NO is significantly higher than that of obtaining the answer YES and vice versa.We opt for the null hypothesis if we find the difference non-significant in light of the obtained responses.A final decision can be obtained by setting a significance level α and rejecting the chosen null hypothesis if the p-value is less or equal to α (we will employ α = 0.05 in the experiments).This last strategy has the advantage of considering the random variability of the answers and could offer a principled way of controlling the false positive rate.The expert could then specify, for a particular problem, which of the two null hypotheses they would like to employ (either p no ⩾ p yes or p yes ⩾ p no ), which in turn implies a different false-positive control (considering either NO or YES as positive).</p>
<p>There are several ways to leverage the results.The outputs of the CITs here can be used for only a particular set of variables and only up to a certain set of conditioning sets, above which the complexity makes reasoning unsuitable.The LLM can be queried alongside the data.In a strict sense, we cannot accept a null hypothesis; we can only reject it if there is evidence against it.This evidence can be either data or information-driven.The system can query both; this should be sufficient if one provides evidence against independence.Especially here, the characteristics that we discussed before could be taken into account: the HI or other general properties of the used LLM, uncertainty might be obtained from the tokens of the answer or its reasoning to detect inconsistencies, or a RAG system could ensure the usage of only certain kinds of knowledge or allow to weight the reliability of the response.</p>
<p>Bayesian integration.The Bayesian paradigm provides, in theory, a principled way to combine different sources of knowledge (data, experts and LLMs) and their associated uncertainties.In particular, inspiration could be found in the data-driven with prior knowledge literature 2.2.For instance, partial knowledge on causal hypothesis extracted from LLMs queries could be compiled into priors over causal graphs and integrated into a Bayesian sampling scheme [3,21,111,164] in an approach belonging to the data • LLM part of the spectrum described previously.</p>
<p>Agentic LLMs.We could also envision future research directions in which the AI model decides when and how to employ data (and data-driven tools) or prior knowledge (from the training set).Such agentic LLMs [136] could autonomously decide if some causal relationships are well established in the literature or if data-driven methods should be applied, and perform statistical analysis using available tools, with additionally a synergic interaction with the user or human-expert.This idea could extend current work by [72] where the LLM uses data-driven methods for a causal query given by a human together with some data and finally interprets the results.In this context, one should explore the possibility of the LLM directly interacting with hybrid algorithms that use its knowledge.</p>
<p>Synthesis.We hope the Causal ML and the statistical community will work towards and propose novel strategies and frameworks to integrate LLMs, data-driven, and human-expert information for causal hypothesis generation tasks.Since LLMs have been made available to the general public and are vastly accessible, they will be used by practitioners and researchers alike to generate causal hypotheses and produce scientific outputs.We need critical approaches and principled methods to be developed, proposed, and made accessible so that LLMs are not used naively and acritically as oracles or experts replacement to produce scientific knowledge.Drawing a parallelism to the p-values criticism in different fields of science [81,82,154,162] we think LLM-driven tools in science can be additional resources in the researchers' hands.Should we move towards a ban of LLM in science similar to the p-values ban in some prestigious journals?Or should we work towards creating awareness of LLMs' risks and merits and developing falsifiable and sound approaches?We think the latter is practically the only feasible solution.</p>
<p>Build universal/model-agnostic benchmarks</p>
<p>Data-driven causal discovery methods are usually compared using topological measures of the learned graph with respect to the ground truth [114], together with additional computational metrics such as memory requirements and speed of execution [44,47,122,134].Peters and Bühlmann [114] introduced the structural interventional distance, a causally meaningful measure of how well an estimated graph can perform causal inference.Recently Henckel et al [64] improved the concept by introducing various adjustment interventional distances (AID), also providing an open source software package for their fast evaluation (gadjid, MPL-2.0).As an example, in table 2, we computed various distances between the estimated causal graphs and the ground truth for the sachs problem (see figure 7 for a deptiction of the ground truth consensus graph and a subset of the estimated graphs).Distances in table 2 were computed via the gadjid package [64].Estimated graphs used in the evaluation were either sourced from existing literature or generated in one of two ways: (i) directly querying GPT-4o (temperature = 0) for the causal graph (figure 6(a)); (ii) prompting GPT-4o (temperature = 1) 100 times for the causal graph, then aggregating the valid graphs through a supermajority threshold (20%) voting scheme (figure 6(a)).All scripts are available in the open-source repository linked in the Code section.</p>
<p>Comparing different approaches in causal discovery is possible only with synthetically generated causal models for which the ground truth causal graph is known perfectly, and this is how practically all causal discovery methods are evaluated in the literature.Alternatively, when no ground truth is available, models can be compared based on how well they fit the data (possibly on a hold-out test set).</p>
<p>Compared to classical supervised machine learning tasks, benchmarking and evaluating causal discovery algorithms is much harder and prone to biases and unwanted signals in the generated data [124].Evaluation in a supervised task can always be performed in a hold-out test set, thus performing good benchmarking of supervised methods amounts mainly in collecting large amounts of datasets, a strategy which has been used, for example, in comparing models [49] and has delivered tremendous advances in computer vision [41,130].</p>
<p>Currently available, real-world benchmarks for causal discovery at the intersection with LLMs can be broadly classified into two axes: scientific vs common-sense knowledge and natural language datasets vs observational data.For all these sets, there is a ground truth causal DAG, which expresses the causal relationship between the variables in the system of interest.Natural language datasets refer to causal information embedded in natural language text, whereas observational data refers to measurements for the variables whose causal relationship is expressed in the ground truth.For the most part, natural language datasets relate mainly to common sense knowledge, and observational data relates mostly to scientific knowledge.The Tübingen [110] cause-effect pairs include over a hundred bivariate causal relationships from various domains.Ground truth data has been obtained by consulting the experts responsible for these datasets.In biology, Sachs et al [132] used a dataset of expression levels for different proteins and phospholipids in human cells.A consensus DAG is available as ground truth that relates all the variables involved, which reflects the knowledge of the field [142].In atmospheric science Huang et al [70] provide a dataset including nine years of historical time-series data sets, which include 11 atmospheric variables.A ground-truth causal graph is provided based on a literature review.Other pseudo-real-world datasets containing scientific knowledge, ground truth data, and observational data often used to benchmark causal discovery algorithms are process-based simulators that encode the causal understanding of a given field and generate data meant to mimic real-world observables.For example, SERGIO [42] and SynTReNTim [150] simulate expression data in gene regulatory networks.In the medical domain, Tu et al [149] provided a neuropathic pain diagnosis simulator for causal discovery algorithm evaluation, whose underlying causal graph was constructed by medical experts on neuropathic pain diagnosis and its parameters estimated from real data.The BN repository [133] contains over 30 datasets across several domains, where the ground truth causal graphs borrow from heterogenous sources ranging from hypothetical situations (see the earthquake set) to causal graphs containing scientific consensus (see the SACHS dataset).</p>
<p>A growing number of benchmarks try to illuminate the causal reasoning capabilities of LLMs.Benchmarks testing LLMs ability to detect causal relations in text correctly include among others: COPA [125], EventStoryLine [20], Causal-TimeBank [106], MAVEN-ERE [159] and BigBench [138].Benchmarks for detecting more complex relationships from text, where causal reasoning of some sort must be used, include CRAB [126], Causalqa [12], WikiWhy [65], e-CARE [45] and CRASS [53].In Hobbhahn et al [66], the LLM causal reasoning capability is isolated from its fact-memorizing capabilities by introducing toy examples and tasks involving the physical interaction of balls such that the LLM may only invoke general knowledge of physics and causality but no concrete instances.Jin et al [74] developed a novel task, Corr2Cause, aimed at assessing causal inference ability of LLMs.Jin et al [73] introduced CLADDER for assessing causal inference in natural language.While Zhou et al [186] developed CausalBench covering three levels of increasing difficulty: correlations, causal skeleton, and causality.Yang et al [172] provided a critical assessment of the current benchmarks and concluded that the causal capabilities of LLMs should be causal rather than correlative, open-ended, scalable, and non-retrievable.The authors provide a repository with current benchmarks for causal reasoning in LLMs 1 .</p>
<p>We believe there is a remaining gap in the path to deploying LLMs for causal hypothesis generation.A literature review shows a dearth of quality causal discovery benchmark datasets that include quality consensus ground truth causal graphs and observational data.For example, while Tübingen cause-effect pairs contain good observational data, it is unclear if the ground truth data provided represents the consensus of the respective fields or the belief of one or several experts.On the other hand, the causal graph encoded simulators such as those modeling Gene Regulatory Networks [42,150] are more likely to represent the consensus of a field given that these types of simulators are tools built and used by a large portion of the community.However, in these cases, the observational data, while similar to real-world data, may still include biases brought about by an incomplete understanding of the real causal relations.As an example, we started collecting causal hypothesis generation problems from the literature with their associated ground truth and data (when available).The resulting collection can be accessed at https://github.com/IPL-UV/causal_benchmarks.</p>
<p>Synthesis.Ground truth datasets for causal facts can span from scientific to common sense.However, it is beneficial to gauge their scientific validity.Ideally, these datasets should include observational data, making them also suitable for data-driven causal discovery and the hybrid approaches we endorse.Care must be taken so that these benchmark datasets are balanced about the following aspects: Domain: It is important to include a wide range of contexts to ensure conclusions are valid in general and to identify biases in the knowledge base; Density of DAG: data-driven causal discovery methods are known to be biased towards settings where the ground truth DAG is less or more dense.This could be true for LLM-generated causal hypotheses, as has been shown for common sense causal tasks in Gao et al [55]; Causal structures: This could help characterize the type of causal knowledge and causal reasoning LLMs are good at.For example, humans are known to be better at detecting chain structures than v-structures [16].Strong non-causal relationships: It is harder to correctly discover the absence of an edge when there is a strong dependence between the variables involved.Benchmark datasets without these relationships could result in overly over-optimistic performance levels.In turn, benchmarking different LLM-driven causal discovery approaches according to these categories will allow us to characterize LLMs' strengths and weaknesses as causal hypothesis generators.Considering process-based models would help solve this conundrum regarding ground truths and data.These models can produce large amounts of synthetic data, so one could consider the ground truth to be the result of causal discovery algorithms applied to large quantities of data.One could then experiment using limited, possibly noisy data with hybrid LLM, expert, and data configurations.</p>
<p>Conclusion</p>
<p>We see a need for developing adaptable methods for causal hypothesis generation, constructing metrics and universal benchmarks, and characterizing LLMs as probabilistic imperfect experts.The integration of LLMs into scientific frameworks is considered a promising avenue to enhance causal understanding in scenarios with limited data and expertise.The paper provides a comprehensive review and perspective for advancing the integration of LLMs into causal reasoning frameworks, steering away from philosophical debates and focusing on practical applications and collaborative endeavors.At the end of the road, we envision integrated systems that incorporate knowledge and data, balancing their reliability.With the expert or the researcher in the loop, it could highlight blind spots that require additional information or data.This can lead to new research opportunities.</p>
<p>Figure 1 .
1
Figure 1.Our schematic description of integrating experts, LLMs, and data-driven methods.The combination of LLMs and data-driven methods lies along different spectra.Along the vertical axis, we classify how much we rely on each source of information, ranging from purely LLM-and knowledge-driven to purely data-driven methods.In the horizontal dimension, we depict the degree of interaction between the sources, spanning from cases where the LLM is solely used as a prior for a data-driven method up to the algorithms where LLMs are used as a post-hoc procedure on the results of data-driven methods.Between these extremes, there is potential for developing methods that intimately integrate both worlds.Experts can appear at the beginning, the end, or continuously through an in-the-loop fashion.</p>
<p>Figure 2 .
2
Figure 2. A structural causal model between four variables X, Y, Z, W.</p>
<p>Figure 4 .
4
Figure 4. Illustration of the introduced scheme for hybrid PC with LLMs.</p>
<p>Table 1 .
1
Overview of works on LLMs for causality.
TopicsWorksPairwise causal relations[79, 95]Full graph (Prior)[5, 25, 37, 88, 94, 95, 147, 152, 180, 182]Full graph Post-hoc[78, 95]Full graph integrated[32, 37]Causal agent[72]Causal reasoning[4, 6, 35, 55, 56, 73, 77, 112, 118, 178]Causal effect estimation[17, 153]Counterfactual/Interventional[48, 77, 89]
[79]]e 3.Comparison of different approaches for causal graph estimation with LLMs: (a) ensemble voting of full graph discovery;(b) triplets approach from Vashishtha et al[152]; (c) chatPC approach from Cohrs et al[32]; (d) causal discovery with pairwise queries from Kıcıman et al[79].</p>
<p>Table 2 .
2
[64]ances between ground truth and estimated graphs for different approaches in the sachs problem.The columns ancestor, Oset, and parent AID refer to different adjustment interventional distances[64]while SHD is the structural Hamming distance.All distances are computed with the gadjid package[64].
ancestor AIDOset AIDparent AIDSHD
Figure 7.Estimated graphs for the sachs problem together with the consensus graph: (a) ensemble supermajority threshold voting (20%) of full graph discovery (gpt-4o, temperature = 1); (b) graph estimated with RAG [182]; (c) data-driven causal graph and (d) consensus graph from Sachs et al [132].</p>
<p>https://github.com/linyingyang/CausalReasoningLLM.
AcknowledgmentsThis work was supported by the European Research Council (ERC) under the ERC Synergy Grant USMILE (Grant Agreement 855187), the BBVA with the Project 'SCALE', the Microsoft research grant Causal4Africa under the Microsoft Climate Research Initiative and the GVA PROMETEO AI4CS Project (CIPROM/2021/056).Compute resources were provided by the Microsoft Accelerate Foundation Models Research program.ORCID iDsKai-Hendrik Cohrs  https://orcid.org/0000-0002-2286-7487Emiliano Diaz  https://orcid.org/0000-0001-8410-6635Vasileios Sitokonstantinou  https://orcid.org/0000-0001-5506-2872Gherardo Varando  https://orcid.org/0000-0002-6708-1103Gustau Camps-Valls  https://orcid.org/0000-0003-1683-2138Data availability statementNo new data were created or analysed in this study.CodeThe code to reproduce the examples in this paper is available at https://github.com/IPL-UV/CHG_LLM.
General for Research and Innovation 2024 Living guidelines on the responsible use of generative AI in research ERA Forum Stakeholders' document. European Commission</p>
<p>G Agrawal, T Kumarage, Z Alghamdi, H Liu, arXiv:2311.07914Can knowledge graphs reduce hallucinations in LLMs?: A survey. 2024</p>
<p>Exploiting experts' knowledge for structure learning of Bayesian networks. H Amirkhani, M Rahmati, P J F Lucas, A Hommersom, 10.1109/TPAMI.2016.2636828IEEE Trans. Pattern Anal. Mach. Intell. 392016</p>
<p>S Ashwani, K Hegde, N R Mannuru, M Jindal, D S Sengar, K C R Kathala, D Banga, V Jain, A Chadha, arXiv:2402.18139Cause and effect: can large language models truly understand causality?. 2024</p>
<p>T Ban, L Chen, D Lyu, Wang X Chen, H , arXiv:2311.11689Causal structure learning supervised by large language model. 2023</p>
<p>G Bao, H Zhang, L Yang, Wang C Zhang, Y , arXiv:2402.16048LLMs with chain-of-thought are non-causal reasoners. 2024</p>
<p>On pearl's hierarchy and the foundations of causal inference Probabilistic and Causal Inference: The Works of Judea Pearl. E Bareinboim, J D Correa, Ibeling D Icard, T , 10.1145/3501714.35017432022Association for Computing Machinery</p>
<p>Graph of thoughts: solving elaborate problems with large language models. M Besta, Proc. AAAI Conf. on Artificial Intelligence. AAAI Conf. on Artificial Intelligence202438</p>
<p>ChatGPT is a knowledgeable but inexperienced solver: an investigation of commonsense problem in large language models. N Bian, X Han, L Sun, H Lin, Y Lu, B He, S Jiang, B ; N Dong, M-Y Calzolari, V Kan, Hoste, S Lenci, Sakti, Xue, Proc. 2024 Joint Int. Conf. on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024. 2024 Joint Int. Conf. on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024ELRA and ICCL2024</p>
<p>Science in the age of large language models. A Birhane, A Kasirzadeh, Leslie D Wachter, S , 10.1038/s42254-023-00581-4Nat. Rev. Phys. 52023</p>
<p>Living guidelines for generative AI-why scientists must oversee its use. C L Bockting, Van Dis E A M, R Van Rooij, Zuidema W Bollen, J , 10.1038/d41586-023-03266-1Nature. 6222023</p>
<p>CausalQA: a benchmark for causal question answering. A Bondarenko, M Wolska, S Heindorf, L Blübaum, Ngonga Ngomo, A-C Stein, B Braslavski, P , Hagen M Potthast, M , Proc. 29th Int. Conf. on Computational Linguistics ed N Calzolari et al (International Committee on Computational Linguistics. 29th Int. Conf. on Computational Linguistics ed N Calzolari et al (International Committee on Computational Linguistics2022</p>
<p>2021 Foundations of structural causal models with cycles and latent variables. S Bongers, P Forré, J Peters, J M Mooij, 10.1214/21-AOS2064Ann. Stat. 49</p>
<p>G Borboudakis, I Tsamardinos, Incorporating causal prior knowledge as path-constraints in Bayesian networks and maximal ancestral graphs Proc. 29th Int. Conf. on Int. Conf. on Machine Learning. 2012</p>
<p>G Borboudakis, I Tsamardinos, Scoring and searching over Bayesian networks with causal and associative priors Proc. 29th Conf. on Uncertainty in Artificial Intelligence pp. 2013</p>
<p>Causality and reasoning: the Monty Hall dilemma. B D Burns, M Wieth, 2003</p>
<p>Is knowledge all large language models needed for causal reasoning?. H Cai, S Liu, R Song, arXiv:2401.001392024</p>
<p>Discovering causal relations and equations from data. G Camps-Valls, A Gerhardus, U Ninad, G Varando, G Martius, E Balaguer-Ballester, R Vinuesa, E Diaz, L Zanna, J Runge, 10.1016/j.physrep.2023.10.005Phys. Rep. 10442023</p>
<p>. G Camps-Valls, L Martino, D H Svendsen, M Campos-Taberner, J Muñoz-Marí, V Laparra, D Luengo, García-Haro F J , 10.1016/j.asoc.2018.03.021Physics-aware Gaussian processes in remote sensing Appl. Soft Comput. 682018</p>
<p>The event StoryLine corpus: a new benchmark for causal and temporal relation extraction Proc. Events and Stories in the News Workshop ed. T Caselli, P ; T Vossen, Caselli, M Miller, Van Erp, Vossen, Palmer, T Hovy, Mitamura, Caswell, 2017Association for Computational Linguistics</p>
<p>Priors on network structures. Biasing the search for Bayesian networks. R Castelo, A Siebes, 10.1016/S0888-613X(99)00041-9Int. J. Approx. Reason. 242000</p>
<p>J Cerdà-Bautista, J M Tárraga, V Sitokonstantinou, Camps-Valls G , arXiv:2310.11287Evaluating the impact of humanitarian aid on food security. 2023</p>
<p>E Y-J Chen, Y Shen, A Choi, A Darwiche, Learning Bayesian networks with ancestral constraints Advances in Neural Information Processing Systems. 201629</p>
<p>Probing the "creativity" of large language models: can models produce divergent semantic association? Findings of the. H Chen, N Ding, EMNLP 2023 ed H Bouamor, J Pino and K Bali2023Association for Computational Linguistics</p>
<p>L Chen, T Ban, X Wang, D Lyu, H Chen, arXiv:2306.07032Mitigating prior errors in causal structure learning: towards LLM driven prior knowledge. 2023</p>
<p>Boosting of thoughts: trial-and-error problem solving with large language models The 12th Int. S Chen, Li B Niu, D , Conf. on Learning Representations. 2024</p>
<p>Double/debiased machine learning for treatment and structural parameters. V Chernozhukov, D Chetverikov, M Demirer, E Duflo, C Hansen, Newey W Robins, J , 10.1111/ectj.12097J. Econom. 212018</p>
<p>Optimal structure identification with greedy search. D Chickering, J. Mach. Learn. Res. 32002</p>
<p>Learning equivalence classes of Bayesian-network structures. D Chickering, J. Mach. Learn. Res. 22002</p>
<p>Greedy equivalence search in the presence of latent. T Claassen, I G Bucur, confounders Proc. 38th Conf. on Uncertainty in Artificial Intelligence (Proc. Machine Learning Research. K Cussens, ( Zhang, Pmlr, 2022180</p>
<p>Reichstein M and Camps-Valls G 2024 Causal hybrid modeling with double machine learning-applications in carbon flux modeling. K-H Cohrs, G Varando, N Carvalhais, 10.1088/2632-2153/ad5a60Mach. Learn.: Sci. Technol. 53</p>
<p>K-H Cohrs, G Varando, E Diaz, V Sitokonstantinou, Camps-Valls G , arXiv:2406.07378Large language models for constrained-based causal discovery. 2024</p>
<p>The impact of prior knowledge on causal structure learning. C Constantinou, Guo Z Kitson, N K , 10.1007/s10115-023-01858-xKnowl. Inf. Syst. 652023</p>
<p>Physics-aware nonparametric regression models for earth data analysis. J Cortés-Andrés, G Camps-Valls, S Sippel, E Székely, D Sejdinovic, E Diaz, A Pérez-Suay, Z Li, M Mahecha, M Reichstein, 10.1088/1748-9326/ac6762Environ. Res. Lett. 17540342022</p>
<p>. L Cox, Jr, 10.1016/j.gloepi.2023.100102Causal reasoning about epidemiological associations in conversational AI Glob. Epidemiol. 51001022023</p>
<p>T Da Silva, E Silva, A Ribeiro, A Góis, D Heider, S Kaski, D Mesquita, arXiv:2309.12032Human-in-the-loop causal discovery under latent confounding using ancestral GFlowNets. 2023</p>
<p>Large language models are effective priors for causal graph discovery. V-A Darvariu, S Hailes, M Musolesi, arXiv:2405.135512024</p>
<p>Efficient structure learning of Bayesian networks using constraints. De Campos, C P , Ji Q , J. Mach. Learn. Res. 122011</p>
<p>Bayesian network learning algorithms using structural restrictions. L M De Campos, J G Castellano, 10.1016/j.ijar.2006.06.009Int. J. Approx. Reason. 452007</p>
<p>N De Cao, Aziz W Titov, I ; M-F Moens, L Huang, S W-T Specia, Yih, Editing factual knowledge in language models Proc. 2021 Conf. on Empirical Methods in Natural Language Processing. Association for Computational Linguistics2021</p>
<p>ImageNet: a large-scale hierarchical image database. J Deng, W Dong, R Socher, L-J Li, Li K , Fei-Fei , 10.1109/CVPR.2009.52068482009. 2009</p>
<p>SERGIO: a single-cell expression simulator guided by gene regulatory networks Cell Syst. P Dibaeinia, S Sinha, 10.1016/j.cels.2020.08.003202011</p>
<p>Graphical models for marked point processes based on local independence. V Didelez, 10.1111/j.1467-9868.2007.00634.xJ. R. Stat. Soc. B. 702008</p>
<p>An empirical comparison of popular structure learning algorithms with a view to gene network inference. V Djordjilović, M Chiogna, J Vomlel, 10.1016/j.ijar.2016.12.012Int. J. Approx. Reason. 882017</p>
<p>L Du, X Ding, K Xiong, T Liu, P Qin B ; S Muresan, Nakov, Villavicencio, CARE: a new dataset for exploring explainable causal reasoning Proc. 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. Association for Computational Linguistics20221</p>
<p>Causal thinking in science: how scientists and students interpret the unexpected Scientific and Technological Thinking. K N Dunbar, J A Fugelsang, 2004Psychology Press</p>
<p>On causal structural learning algorithms: Oracles' simulations and considerations Knowl.-Based Syst. L Farnia, M Alibegovic, E Cruickshank, 10.1016/j.knosys.2023.1106942023276110694</p>
<p>. A Feder, N Oved, U Shalit, R Reichart, 10.1162/coli_a_00404CausaLM: causal model explanation through counterfactual language models Comput. Linguist. 472021</p>
<p>Do we need hundreds of classifiers to solve real world classification problems?. M Fernández-Delgado, E Cernadas, S Barro, D Amorim, J. Mach. Learn. Res. 152014</p>
<p>G Franceschelli, M Musolesi, arXiv:2304.00008On the creativity of large language models. 2023</p>
<p>Being Bayesian about network structure. A Bayesian approach to structure discovery in Bayesian networks. N Friedman, D Koller, 10.1023/A:1020249912095Mach. Learn. 502003</p>
<p>. K J Friston, Harrison L , Penny W , 10.1016/S1053-8119(03)00202-7Dynamic causal modelling NeuroImage. 192003</p>
<p>CRASS: a novel data set and benchmark to test counterfactual reasoning of large language models. J Frohberg, F Binder, Proc. 13th Language Resources and Evaluation Conf. ed N Calzolari et al (European Language Resources Association. 13th Language Resources and Evaluation Conf. ed N Calzolari et al (European Language Resources Association2022</p>
<p>Galileo 2023 LLM hallucination index. </p>
<p>J Gao, X Ding, B Qin, T ; H Liu, J Bouamor, K Pino, Bali, Is ChatGPT a good causal reasoner? A comprehensive evaluation Findings of the Association for Computational Linguistics: EMNLP 2023 ed. Association for Computational Linguistics2023</p>
<p>Y Gat, N Calderon, A Feder, A Chapanin, A Sharma, R Reichart, arXiv:2310.00603Faithful explanations of black-box NLP models using LLM-generated counterfactuals. 2023</p>
<p>A survey of confidence estimation and calibration in large language models. J Geng, F Cai, Y Wang, H Koeppl, P Nakov, I Gurevych, 10.18653/v1/2024.naacl-long.366Proc. 2024 Conf. North American Chapter. Long Papers. 2024 Conf. North American Chapterthe Association for Computational Linguistics20241</p>
<p>High-recall causal discovery for autocorrelated time series with latent confounders. A Gerhardus, J Runge, Advances in Neural Information Processing Systems. 202033</p>
<p>Google scholar is not enough to be used alone for systematic reviews Online. D Giustini, M N K Boulos, 10.5210/ojphi.v5i2.4623J. Public Health Inform. 52142013</p>
<p>. C Glymour, K Zhang, P Spirtes, 10.3389/fgene.2019.00524Review of causal discovery methods based on graphical models Front. Genet. 105242019</p>
<p>Q Guo, R Wang, J Guo, B Li, K Song, X Tan, G Liu, J Bian, Y Yang, Connecting large language models with evolutionary algorithms yields powerful prompt optimizers The 12th Int. Conf. on Learning Representations. 2024</p>
<p>The role of google scholar in evidence reviews and its applicability to grey literature searching. N R Haddaway, A M Collins, D Coughlin, S Kirk, 10.1371/journal.pone.0138237PLoS One. 10e01382372015</p>
<p>Learning Bayesian networks: a unification for discrete and Gaussian domains. D Heckerman, D Geiger, Proc. 11th Conf. on Uncertainty in Artificial Intelligence pp. 11th Conf. on Uncertainty in Artificial Intelligence pp1995</p>
<p>Adjustment identification distance: a gadjid for causal structure learning. L Henckel, T Würtzen, S Weichwald, The 40th Conf. on Uncertainty in Artificial Intelligence. 2024</p>
<p>M Ho, A Sharma, J Chang, M Saxon, S Levy, Lu Y Wang, W Y , WikiWhy: answering and explaining cause-and-effect questions The 11th Int. Conf. on Learning Representations (ICLR 2023. Kigali, Rwanda2023. 1-5 May 2023OpenReview.net) (available at</p>
<p>M Hobbhahn, T Lieberum, D Seiler, LLMs NeurIPS ML Safety Workshop. 2022</p>
<p>X Hu, J Chen, X Li, Y Guo, L Wen, P Yu, Z Guo, Towards understanding factual knowledge of large language models The 12th Int. Conf. on Learning Representations. 2024</p>
<p>Uncertainty of thoughts: uncertainty-aware planning enhances information seeking in large language models ICLR 2024 Workshop on Large Language Model (LLM) Agents. Z Hu, C Liu, X Feng, Y Zhao, S-K Ng, A T Luu, J He, P Koh, B Hooi, 2024</p>
<p>L Huang, arXiv:2311.05232A survey on hallucination in large language models: principles, taxonomy, challenges, and open questions. 2023</p>
<p>Benchmarking of data-driven causality discovery approaches in the interactions of Arctic Sea ice and atmosphere Front. Y Huang, M Kleindessner, A Munishkin, D Varshney, P Guo, Wang J , 10.3389/fdata.2021.642182Big Data. 46421822021</p>
<p>Google scholar: the pros and the cons. P Jacsó, 10.1108/14684520510598066Online Inf. Rev. 292005</p>
<p>Large language model for causal decision making. H Jiang, L Ge, Y Gao, Wang J Song, R , arXiv:2312.171222024</p>
<p>Z Jin, CLadder: assessing causal reasoning in language models Advances in Neural Information Processing Systems. 202336</p>
<p>Can large language models infer causation from correlation?. Z Jin, J Liu, Z Lyu, S Poff, M Sachan, R Mihalcea, M Diab, B Schölkopf, The 12th Int. Conf. on Learning Representations. 2024</p>
<p>Speech and Language Processing: An Introduction to Natural Language Processing. D Jurafsky, J Martin, Computational Linguistics and Speech Recognition. 20253rd edn (available at</p>
<p>Knowledge Guided Machine Learning: Accelerating Discovery Using Scientific Knowledge and Data 1st edn. A Karpatne, R Kannan, V Kumar, 10.1201/97810031433762022Chapman and Hall/CRC</p>
<p>T Kasetty, D Mahajan, G K Dziugaite, A Drouin, D Sridhar, arXiv:2404.05545Evaluating interventional reasoning capabilities of large language models. 2024</p>
<p>E Khatibi, M Abbasian, Z Yang, I Azimi, A M Rahmani, arXiv:2405.01744ALCM: autonomous LLM-augmented causal discovery framework. 2024</p>
<p>E Kıcıman, R Ness, A Sharma, C Tan, arXiv:2305.00050Causal reasoning and large language models: opening a new frontier for causality. 2023</p>
<p>A deep learning-based hybrid model of global terrestrial evaporation. A Koppa, D Rains, P Hulsman, Poyatos R Miralles, D G , 10.1038/s41467-022-29543-7Nat. Commun. 1319122022</p>
<p>. H Kraemer, 10.1001/jamapsychiatry.2019.1965JAMA Psychiatry. 762019Is it time to ban the P value</p>
<p>The practical alternative to the p value is the correctly used p value Perspect. D Lakens, 10.1177/1745691620958012Psychol. Sci. 162021</p>
<p>. S Lauritzen, Graphical Models. 171996</p>
<p>F Leiser, S Eckhardt, V Leuthe, M Knaeble, A Mädche, G Schwabe, A Sunyaev, 10.1145/3613904.3642428hallucination identifier for large language models Proc. CHI Conf. on Human Factors in Computing Systems (CHI'24). Association for Computing Machinery2024</p>
<p>Context-specific causal discovery for categorical data using staged trees Proc. M Leonelli, G Varando, 26th Int. Conf. on Artificial Intelligence and Statistics (Proc. Machine Learning Research. J Ruiz, J-W Dy, ( Van De Meent, Pmlr, 2023206</p>
<p>Retrieval-augmented generation for knowledge-intensive NLP tasks. P Lewis, Advances in Neural Information Processing Systems. Curran Associates, Inc202033</p>
<p>A Li, P Beek, Bayesian network structure learning with side constraints Int. Conf. on Probabilistic Graphical Models (PMLR) pp. 2018</p>
<p>P Li, X Wang, Z Zhang, Y Meng, F Shen, Y Li, J Wang, Li Y Zhu, W , arXiv:2404.14786RealTCD: temporal causal discovery from interventional data with large language model. 2024</p>
<p>Prompting large language models for counterfactual generation: an empirical study. Y Li, M Xu, X Miao, S Zhou, T ; N Qian, M-Y Calzolari, V Kan, Hoste, S Lenci, Sakti, Xue, Proc. 2024 Joint Int. Conf. on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024. 2024 Joint Int. Conf. on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024ELRA and ICCL2024</p>
<p>Unveiling the pitfalls of knowledge editing for large language models The 12th Int. Z Li, N Zhang, Y Yao, M Wang, Chen X Chen, H , Conf. on Learning Representations. 2024</p>
<p>How experts' own inconsistency relates to their confidence and between-expert disagreement Sci. A Litvinova, R H J M Kurvers, R Hertwig, S M Herzog, 10.1038/s41598-022-12847-52022129273</p>
<p>Lost in the middle: how language models use long contexts. N F Liu, K Lin, J Hewitt, A Paranjape, M Bevilacqua, F Petroni, P Liang, 10.1162/tacl_a_00638Trans. Assoc. Comput. Linguist. 122024</p>
<p>T Liu, N Astorga, Seedat N and van der Schaar M 2024 Large language models to enhance Bayesian optimization The 12th Int. Conf. on Learning Representations. </p>
<p>Can large language models build causal graphs?. S Long, arXiv:2303.052792023</p>
<p>S Long, A Piché, V Zantedeschi, T Schuster, A Drouin, arXiv:2307.02390Causal discovery with language models as imperfect experts. 2023</p>
<p>Machine learning as a tool for hypothesis generation Q. J Ludwig, S Mullainathan, 10.1093/qje/qjad055J. Econ. 1392024</p>
<p>Self-contradictory hallucinations of large language models: evaluation, detection and mitigation The 12th Int. N Mündler, J He, S Jenko, M Vechev, Conf. on Learning Representations. 2024</p>
<p>Causal structure learning from multivariate time series in settings with unmeasured confounding Proc. D Malinsky, P ; T D Spirtes, K Le, E Zhang, A Kıcıman, Hyvärinen, Liu, ACM SIGKDD Workshop on Causal Disocvery (Proc. Machine Learning Research. 922018. 2018PMLR</p>
<p>An interactive approach for Bayesian network learning using domain/expert knowledge. A Masegosa, 10.1016/j.ijar.2013.03.009Int. J. Approx. Reason. 542013</p>
<p>An exploratory study of google scholar Online Inf. P Mayr, A-K Walter, 10.1108/14684520710841784Rev. 312007</p>
<p>C Meek, 10.5555/2074158.2074204Causal inference and causal explanation with background knowledge Proc. 11th Conf. on Uncertainty in Artificial Intelligence (UAI'95). Morgan Kaufmann Publishers Inc1995</p>
<p>Graphical models: selecting causal and statistical models PhD. C Meek, 1997Dissertation Carnegie Mellon University</p>
<p>K Meng, D Bau, Andonian A Belinkov, Y ; S Koyejo, Mohamed, Agarwal, K Belgrave, Cho, Oh, GPT Advances in Neural Information Processing Systems. Curran Associates, Inc202235</p>
<p>arXiv:2311.07361Microsoft Research AI4Science and Microsoft Azure Quantum 2023 The impact of large language models on scientific discovery: a preliminary study using GPT-4. </p>
<p>Beyond accuracy: evaluating self-consistency of code large language models with IdentityChain The 12th Int. M J Min, Y Ding, L Buratti, S Pujar, G Kaiser, Jana S Ray, B , Conf. on Learning Representations. 2024</p>
<p>P Mirza, R Sprugnoli, S Tonelli, M ; M-F Speranza, M Moens, J Palmer, Pustejovsky, Bethard, Annotating causality in the TempEval-3 corpus Proc. EACL 2014 Workshop on Computational Approaches to Causality in Language. Association for Computational Linguistics2014</p>
<p>E Mitchell, C Lin, A Bosselut, C Finn, C D Manning, scale Int. Conf. on Learning Representations. 2022</p>
<p>S Mo, M Xin, arXiv:2309.07694Tree of uncertain thoughts reasoning for large language models. 2023</p>
<p>Markov equivalence of marginalized local independence graphs. S W Mogensen, N Hansen, 10.1214/19-AOS1821Ann. Stat. 482020</p>
<p>Distinguishing cause from effect using observational data: methods and benchmarks. J Mooij, M Peters, J Janzing, D Zscheischler, J Schölkopf, B , J. Mach. Learn. Res. 172016</p>
<p>S Mukherjee, T P Speed, 10.1073/pnas.0802272105Network inference using informative priors Proc. Natl Acad. Sci. 2008105</p>
<p>Answering causal questions with augmented LLMs ICML Workshop on Deployment Challenges for Generative AI. N Pawlowski, J Vaughan, Jennings J Zhang, C , 2023</p>
<p>. J Pearl, Models, Reasoning and Inference. 1932000Cambridge University Press</p>
<p>Structural intervention distance for evaluating causal graphs Neural Comput. J Peters, P Bühlmann, 10.1162/NECO_a_00708201527</p>
<p>Causal models for dynamical systems Probabilistic and Causal Inference: The Works of Judea Pearl. J Peters, S Bauer, N Pfister, 10.1145/3501714.35017522022Association for Computing Machinery</p>
<p>Elements of Causal Inference: Foundations and Learning Algorithms. J Peters, D Janzing, B Schölkopf, 2017MIT Press</p>
<p>J Peters, J M Mooij, Janzing D Schölkopf, B , Identifiability of causal graphs using functional models Proc. 27th Conf. on Uncertainty in Artificial Intelligence pp. 2011</p>
<p>A Phatak, V K Mago, A Agrawal, A Inbasekaran, P J Giabbanelli, arXiv:2403.07118Narrating causal graphs with large language models. 2024</p>
<p>K Popper, The Logic of Scientific Discovery. Routledge2005</p>
<p>Robust knowledge extraction from large language models using social choice theory. N Potyka, Y Zhu, Y He, E Kharlamov, S Staab, Proc. 23rd Int. Conf. on Autonomous Agents and Multiagent Systems (AAMAS'24) (International Foundation for Autonomous Agents and Multiagent Systems. 23rd Int. Conf. on Autonomous Agents and Multiagent Systems (AAMAS'24) (International Foundation for Autonomous Agents and Multiagent Systems2024</p>
<p>Physics-informed neural networks: a deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. M Raissi, P Perdikaris, G Karniadakis, 10.1016/j.jcp.2018.10.045J. Comput. Phys. 3782019</p>
<p>A comparison of public causal search packages on linear. J D Ramsey, B Andrews, arXiv:1709.04240Gaussian data with no latent variables. 2017</p>
<p>Deep learning and process understanding for data-driven earth system science. M Reichstein, G Camps-Valls, B Stevens, M Jung, J Denzler, Carvalhais N , Prabhat , 10.1038/s41586-019-0912-1Nature. 5662019</p>
<p>Beware of the simulated dag! Causal discovery benchmarks may be easy to game. A Reisach, C Seiler, S ; M Weichwald, Ranzato, Beygelzimer, P Dauphin, J W Liang, Vaughan, Advances in Neural Information Processing Systems. Curran Associates, Inc202134</p>
<p>M Roemmele, C Bejan, Gordon A , Choice of plausible alternatives: an evaluation of commonsense causal reasoning AAAI Spring Symp.: Logical Formalizations of Commonsense Reasoning. 2011</p>
<p>A Romanou, S Montariol, D Paul, L Laugier, K Aberer, A Bosselut, CRAB: assessing the strength of causal relationships between real-world events The 2023 Conf. on Empirical Methods in Natural Language Processing. 2023</p>
<p>Inferring causation from time series in earth system sciences. J Runge, 10.1038/s41467-019-10105-3Nat. Commun. 1025532019</p>
<p>Modern causal inference approaches to investigate biodiversity-ecosystem functioning relationships. J Runge, 10.1038/s41467-023-37546-1Nat. Commun. 1419172023</p>
<p>Eyring V and Camps-Valls G 2023 Causal inference for time series. J Runge, A Gerhardus, G Varando, 10.1038/s43017-023-00431-yNat. Rev. Earth Environ. 4</p>
<p>ImageNet large scale visual recognition challenge Int. O Russakovsky, 10.1007/s11263-015-0816-yJ. Comput. Vis. 1152015</p>
<p>The challenge of generating causal hypotheses using network models Struct. O Ryan, L Bringmann, N K Schuurman, 10.1080/10705511.2022.2056039Equ. Model. 292022</p>
<p>Causal protein-signaling networks derived from multiparameter single-cell data. K Sachs, O Perez, D Pe'er, D Lauffenburger, G P Nolan, 10.1126/science.1105809Science. 3082005</p>
<p>Bayesian Networks With Examples in R. M Scutari, J-B Denis, 2014Chapman and Hall</p>
<p>Who learns better Bayesian network structures: accuracy and speed of structure learning algorithms. M Scutari, C Graafland, J M Gutiérrez, 10.1016/j.ijar.2019.10.003Int. J. Approx. Reason. 1152019</p>
<p>The hardness of conditional independence testing and the generalised covariance measure. R Shah, J Peters, 10.1214/19-AOS1857Ann. Stat. 482018</p>
<p>Y Shavit, Practices for governing agentic AI systems OpenAI Research paper. </p>
<p>P Spirtes, C Glymour, R Scheines, Causation, Prediction and Search. MIT Press2001</p>
<p>Beyond the imitation game: quantifying and extrapolating the capabilities of language models Trans. A Srivastava, Mach. Learn. Res. 2023</p>
<p>A Stips, D Macias, C Coughlan, E Garcia-Gorriz, X S Liang, 10.1038/srep21691On the causal structure between CO2 and global temperature Sci. 201621691</p>
<p>Head-to-tail: how knowledgeable are large language models (LLMs)? A.K.A. will LLMs replace knowledge graphs?. K Sun, Y E Xu, H Zha, Y Liu, X L Dong, arXiv:2308.10168Proc. NAACL-HLT. NAACL-HLT2024</p>
<p>Y Sun, Z Yin, Q Guo, J Wu, X Qiu, H ; N Zhao, M-Y Calzolari, V Kan, Hoste, S Lenci, Sakti, Xue, Benchmarking hallucination in large language models based on unanswerable math word problem Proc. 2024 Joint Int. Conf. on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024. ELRA and ICCL2024</p>
<p>Bayesian structure learning and sampling of Bayesian networks with the R package. P Suter, J Kuipers, G Moffa, N Beerenwinkel, 10.18637/jss.v105.i09BiDAG J. Stat. Softw. 1052023</p>
<p>R Tang, C Zhang, X Ma, Lin J Ture, F , Found in the middle: permutation self-consistency improves listwise ranking in large language models Proc. 2024 Conf. North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. H Duh, S Gomez, Bethard, Association for Computational Linguistics20241</p>
<p>Quantifying uncertainty in natural language explanations of large language models 27th Int. H Tanneru, C Agarwal, H Lakkaraju, PMLR) pp 1072-80Conf. on Artificial Intelligence and Statistics. 2024</p>
<p>Causal analysis with chain event graphs. P Thwaites, J Smith, E Riccomagno, 10.1016/j.artint.2010.05.004Artif. Intell. 1742010</p>
<p>Just ask for calibration: strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback. K Tian, E Mitchell, A Zhou, A Sharma, R Rafailov, H Yao, C Finn, C D Manning, arXiv:2305.149752023</p>
<p>Automating psychological hypothesis generation with AI: large language models meet causal graph. S Tong, K Mao, Z Huang, Y Zhao, K Peng, 10.31234/osf.io/7ck9m2023</p>
<p>Partitioning net carbon dioxide fluxes into photosynthesis and respiration using neural networks Glob. G Tramontana, M Migliavacca, M Jung, M Reichstein, T F Keenan, G Camps-Valls, J Ogee, J Verrelst, D Papale, 10.1111/gcb.15203Change Biol. 262020</p>
<p>Neuropathic pain diagnosis simulator for causal discovery algorithm evaluation Advances in. R Tu, K Zhang, B C Bertilson, H Kjellström, C Zhang, Neural Information Processing Systems. Curran Associates Inc201932</p>
<p>SynTReN: a generator of synthetic gene expression data for design and analysis of structure learning algorithms BMC Bioinform. T Van Den Bulcke, K Van Leemput, B Naudts, P Van Remortel, H Ma, A Verschoren, De Moor, B Marchal, K , 10.1186/1471-2105-7-432006743</p>
<p>G Varando, Richard Hansen, N , Graphical continuous Lyapunov models Proc. 36th Conf. on Uncertainty in Artificial Intelligence (UAI) (Proc. Machine Learning Research). D Peters, ( Sontag, Pmlr, 2020124</p>
<p>A Vashishtha, A G Reddy, A Kumar, S Bachu, V Balasubramanian, A Sharma, arXiv:2310.15117Causal inference using LLM-guided discovery. 2023</p>
<p>DoubleLingo: causal estimation with large language models. M Veljanovski, Wood-Doughty Z , 2023</p>
<p>Bayesian inference for psychology. Part I: theoretical advantages and practical ramifications Psychon. E-J Wagenmakers, 10.3758/s13423-017-1343-3Bull. Rev. 252018</p>
<p>G Wan, Y Wu, M Hu, Chu Z Li, S , arXiv:2402.11068Bridging causal discovery and large language models: a comprehensive survey of integrative approaches and future directions. 2024</p>
<p>R Wang, E Zelikman, G Poesia, Y Pu, N Haber, N Goodman, Hypothesis search: inductive reasoning with language models The 12th Int. Conf. on Learning Representations. 2024</p>
<p>No need for large-scale search: exploring large language models in complex knowledge base question answering Proc. Wang S Qin, B ; N Calzolari, M-Y Kan, V Hoste, S Lenci, Sakti, Xue, Joint Int. Conf. on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024. ELRA and ICCL2024. 2024</p>
<p>W Wang, B Haddow, A Birch, W Peng, arXiv:2310.09820Assessing the reliability of large language model knowledge. 2023</p>
<p>X Wang, MAVEN-ERE: a unified large-scale dataset for event coreference, temporal, causal and subevent relation extraction Proc. 2022 Conf. on Empirical Methods in Natural Language Processing. Association for Computational Linguistics2022</p>
<p>Learning Bayesian networks based on order graph with ancestral constraints Knowl.-Based Syst. Z Wang, X Gao, Y Yang, Tan X , Chen D , 10.1016/j.knosys.2020.1065152021211106515</p>
<p>L Wasserman, All of Statistics: A Concise Course in Statistical Inference. Springer2013</p>
<p>. R L Wasserstein, A Schirm, N A Lazar, 10.1080/00031305.2019.1583913&lt; 0.05Am. Stat. 732019</p>
<p>J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, Chi E , Le Q Zhou, D , arXiv:2201.11903Chain-of-thought prompting elicits reasoning in large language models. 2023</p>
<p>Reconstructing gene regulatory networks with Bayesian networks by combining expression data with multiple sources of prior knowledge. A Werhli, D Husmeier, 10.2202/1544-6115.1282Stat. Appl. Genet. Mol. Biol. 6152007</p>
<p>J Wolfschwenger, K L Young, 10.1093/hepl/9780198850298.003.0042Multicausality and Equifinality. Oxford University Press2021</p>
<p>The method of path coefficients Ann. S Wright, 10.1214/aoms/1177732676Math. Stat. 51934</p>
<p>Can LLMs express their uncertainty? An empirical evaluation of confidence elicitation in LLMs The 12th Int. M Xiong, Z Hu, X Lu, Y Li, J Fu, J He, B Hooi, Conf. on Learning Representations. 2024</p>
<p>Multi-perspective improvement of knowledge graph completion with large language models. D Xu, Z Zhang, Z Lin, X Wu, Z Zhu, T Xu, X Zhao, Y Zheng, E ; N Chen, M-Y Calzolari, V Kan, Hoste, S Lenci, Sakti, Xue, Proc. 2024 Joint Int. Conf. on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024. 2024 Joint Int. Conf. on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024ELRA and ICCL2024</p>
<p>F Xu, Q Lin, J Han, T Zhao, J Liu, E Cambria, arXiv:2306.09841Are large language models really good logical reasoners? A comprehensive evaluation and beyond. 2023</p>
<p>A structure learning algorithm for Bayesian network using prior knowledge. J-G Xu, Y Zhao, Chen J Han, C , 10.1007/s11390-015-1556-8J. Comput. Sci. Technol. 302015</p>
<p>C Yang, X Wang, Y Lu, H Liu, Q V Le, D Zhou, Chen X , Large language models as optimizers The 12th Int. Conf. on Learning Representations. 2024</p>
<p>A critical review of causal inference benchmarks for large language models AAAI 2024 Workshop on "Are Large Language Models Simply Causal Parrots?. L Yang, O Clivio, V Shirvaikar, F Falck, 2023</p>
<p>2021 B-PINNs: Bayesian physics-informed neural networks for forward and inverse PDE problems with noisy data. L Yang, X Meng, G E Karniadakis, 10.1016/j.jcp.2020.109913J. Comput. Phys. 425109913</p>
<p>Tree of thoughts: deliberate problem solving with large language models. S Yao, D Yu, J Zhao, I Shafran, T Griffiths, Y Cao, K Narasimhan, Advances in Neural Information Processing Systems. 362024</p>
<p>Y Yao, P Wang, B Tian, S Cheng, Z Li, S Deng, Chen H Zhang, N , Editing large language models: problems, methods and opportunities The 2023 Conf. on Empirical Methods in Natural Language Processing. 2023</p>
<p>Aggregation of reasoning: a hierarchical framework for enhancing answer selection in large language models. Z Yin, Proc. 2024 Joint Int. Conf. on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024. 2024 Joint Int. Conf. on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024ELRA and ICCL2024</p>
<p>KoLA: carefully benchmarking world knowledge of large language models The 12th Int. J Yu, Conf. on Learning Representations. 2024</p>
<p>Causal parrots: Large language models may talk causality but are not causal Trans. M Zec ˇević, M Willig, D Dhami, K Kersting, Mach. Learn. Res. 2023</p>
<p>C Zhang, arXiv:2304.05524Understanding causality with large language models: feasibility and opportunities. 2023</p>
<p>C Zhang, D Janzing, M Van Der Schaar, F Locatello, P Spirtes, Causality in the time of LLMs: round table discussion results of clear 2023 Proc. Machine Learning Research (2nd Conf. on Causal Learning and Reasoning. 2023</p>
<p>Tree-of-reasoning question decomposition for complex question answering with large language models. K Zhang, J Zeng, F Meng, Y Wang, S Sun, L Bai, H Shen, J Zhou, Proc. AAAI Conf. on Artificial Intelligence. AAAI Conf. on Artificial Intelligence202438</p>
<p>Causal graph discovery with retrieval-augmented generation based large language models. Y Zhang, Y Zhang, Y Gan, L Yao, C Wang, arXiv:2402.153012024</p>
<p>. W Zhao, L Gentine, P Reichstein, M Zhang, Y Zhou, S Wen, Y Lin, C , Li X Qiu, G Y , 10.1029/2019GL085291Physics-constrained machine learning of evapotranspiration Geophys. Res. Lett. 462019</p>
<p>Y Zhao, L Yan, W Sun, G Xing, S Wang, C Meng, Z Cheng, Ren Z Yin, D ; N Calzolari, M-Y Kan, V Hoste, S Lenci, Sakti, Xue, Improving the robustness of large language models via consistency alignment Proc. 2024 Joint Int. Conf. on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024. ELRA and ICCL2024</p>
<p>Revisiting the self-consistency challenges in multi-choice question formats for large language model evaluation. W Zhou, Q Wang, M Xu, Chen M Duan, X ; N Calzolari, M-Y Kan, V Hoste, S Lenci, Sakti, Xue, Proc. 2024 Joint Int. Conf. on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024. 2024 Joint Int. Conf. on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024ELRA and ICCL2024</p>
<p>Y Zhou, X Wu, B Huang, J Wu, Feng L Tan, K C , arXiv:2404.06349[cs.LG]CausalBench: a comprehensive benchmark for causal learning capability of large language models. 2024</p>            </div>
        </div>

    </div>
</body>
</html>
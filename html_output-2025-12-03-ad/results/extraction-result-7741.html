<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7741 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7741</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7741</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-141.html">extraction-schema-141</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models estimate probabilities for future real-world scientific discoveries, including model details, prediction targets, datasets, forecasting horizon, probability estimation methods, evaluation metrics, reported performance, calibration quality, baselines, limitations, and concrete probability examples.</div>
                <p><strong>Paper ID:</strong> paper-32bee919a779c601881e8257988e6dabe10383c1</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/32bee919a779c601881e8257988e6dabe10383c1" target="_blank">From News to Forecast: Integrating Event Analysis in LLM-Based Time Series Forecasting with Reflection</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> This paper utilizes LLM-based agents to iteratively filter out irrelevant news and employ human-like reasoning to evaluate predictions, which enables the model to analyze complex events, such as unexpected incidents and shifts in social behavior.</p>
                <p><strong>Paper Abstract:</strong> This paper introduces a novel approach that leverages Large Language Models (LLMs) and Generative Agents to enhance time series forecasting by reasoning across both text and time series data. With language as a medium, our method adaptively integrates social events into forecasting models, aligning news content with time series fluctuations to provide richer insights. Specifically, we utilize LLM-based agents to iteratively filter out irrelevant news and employ human-like reasoning to evaluate predictions. This enables the model to analyze complex events, such as unexpected incidents and shifts in social behavior, and continuously refine the selection logic of news and the robustness of the agent's output. By integrating selected news events with time series data, we fine-tune a pre-trained LLM to predict sequences of digits in time series. The results demonstrate significant improvements in forecasting accuracy, suggesting a potential paradigm shift in time series forecasting through the effective utilization of unstructured news data.</p>
                <p><strong>Cost:</strong> 0.007</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7741",
    "paper_id": "paper-32bee919a779c601881e8257988e6dabe10383c1",
    "extraction_schema_id": "extraction-schema-141",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00662225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>From News to Forecast: Integrating Event Analysis in LLM-Based Time Series Forecasting with Reflection</h1>
<p>Xinlei Wang ${ }^{1}$, Maike Feng ${ }^{2}$, Jing Qiu ${ }^{1}$, Jinjin Gu ${ }^{1, <em>}$, Junhua Zhao ${ }^{2,3 </em>}$<br>${ }^{1}$ School of Electrical and Computer Engineering, The University of Sydney<br>${ }^{2}$ School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen<br>${ }^{3}$ Shenzhen Institute of Artificial Intelligence and Robotics for Society<br>{xinlei.wang, jeremy.qiu, jinjin.gu}@sydney.edu.au, zhaojunhua@cuhk.edu.cn</p>
<h4>Abstract</h4>
<p>This paper introduces a novel approach that leverages Large Language Models (LLMs) and Generative Agents to enhance time series forecasting by reasoning across both text and time series data. With language as a medium, our method adaptively integrates social events into forecasting models, aligning news content with time series fluctuations to provide richer insights. Specifically, we utilize LLM-based agents to iteratively filter out irrelevant news and employ human-like reasoning to evaluate predictions. This enables the model to analyze complex events, such as unexpected incidents and shifts in social behavior, and continuously refine the selection logic of news and the robustness of the agent's output. By integrating selected news events with time series data, we fine-tune a pre-trained LLM to predict sequences of digits in time series. The results demonstrate significant improvements in forecasting accuracy, suggesting a potential paradigm shift in time series forecasting through the effective utilization of unstructured news data.</p>
<h2>1 Introduction</h2>
<p>Time series forecasting [18, 21] serves as an essential foundation for decision-making across a wide range of economic, infrastructural, and social domains [2, 13, 14, 16, 56]. The purpose of analyzing time series data is to decode the evolving relationships within complex, dynamic realworld systems. Traditional forecasting methods, while effective at identifying patterns in historical data, perform well when time series distributions remain consistent over time. However, they have limitations in addressing sudden disruptions or anomalies caused by external random events, and do not systematically connect complex social events with fluctuations in time series data. Integrating insights from real-world events and their effects on social and economic behavior is crucial for improving the reliability and accuracy of time series forecasting.</p>
<p>News articles provide crucial insights into unexpected incidents, policy changes, technological developments, and public sentiment shifts-factors that numerical data alone may not capture. Integrating news into forecasting enriches its inputs with context that closely mirrors the complexities of human behavior and societal changes. On the one hand, news offers a real-time snapshot of events, enabling the model to adjust predictions based on updated information. On the other hand, qualitative data from news sources enables the model to account for non-linear and non-numeric influences. By combining both quantitative and qualitative insights, the model can improve forecast accuracy, especially in rapidly changing environments, making it more reflective of real-world dynamics.</p>
<p>In this work, we propose a unified approach that embeds news and supplementary information into time series data using textual prompts. By fine-tuning large language models (LLMs) [54, 55, 66],</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Integrating textual information in time series forecasting. (A) We retrieve relevant original news and supplementary information from our comprehensive database based on information such as the geographic location and time frame of the prediction task. (B) LLM-based agents analyze and select relevant news for different forecasting horizons. (C &amp; D) The selected news and contextual information are combined with time series data for fine-tuning the LLM forecasting model. (E) Discrepancies between predictions and ground truth trigger a review of historical news and data to reprocess missed information and refine reasoning logic.</p>
<p>We transform time series forecasting into the prediction of the next token in text, taking into account relevant contextual information. The inductive reasoning capabilities of pre-trained LLMs, along with their ability to model multi-modal distributions, enable few-shot predictions in time series [17]. The potential of language models in time series forecasting has also been proven [23, 71]. After further training on our dataset, which includes time series, news, and supplementary information, language models can generate forecasts that consider the textual context provided in the input prompts.</p>
<p>Effective news filtering is a key issue for enhancing time series forecasting as input diversity increases. This task requires more than simple keyword extraction; it demands a deep understanding of how news elements interact with forecast variables, extending beyond linear reasoning to incorporate intelligent analytical methods. We employ LLM agents [43, 57] with advanced human-like reasoning capabilities for dynamic and effective news selection. These agents use few-shot learning to adapt their strategies based on logical scenarios that mimic human reasoning about factors affecting time series fluctuations. This enables them to identify relevant news, which is then paired with corresponding time series data to create a context-aware dataset that improves prediction accuracy. Additionally, LLM agents also play a crucial role in model evaluation, continuously refining their selection logic by comparing forecast errors with all relevant events. This iterative self-evaluation helps identify and integrate critical news items previously overlooked. By automating the analysis of unstructured text and applying chain-of-thought prompting, the agents effectively uncover patterns linking news events to forecasting discrepancies, revealing the nuanced effects of external factors on predictions.</p>
<p>Our contribution can be summarized as follows:</p>
<ul>
<li>The paper introduces a novel time series forecasting framework that integrates unstructured news data with numerical time series inputs, providing deeper contextual understanding and improving the model's responsiveness to social events and real-world dynamics.</li>
<li>The research highlights the use of LLM agents for dynamic news selection and analysis. We leverage the reasoning capabilities of LLM to automate the effective understanding and filtering</li>
</ul>
<p>of news content. The agents iteratively refine their news selection process based on forecasting results, improving model accuracy and reliability.</p>
<ul>
<li>We propose a data construction method that integrates time series data with news information, and build a dataset spanning multiple domains to support our research. The dataset includes taskspecific time series data and verified public news reports, which facilitates further exploration in time series research.</li>
<li>Our experiment results demonstrate the effective integration of news data, achieving superior prediction accuracy across diverse domains such as finance, energy, traffic, and bitcoin domains. Our findings demonstrate that incorporating news is highly adept at navigating complexities, especially in energy demand patterns.</li>
</ul>
<h1>2 Related Work</h1>
<p>Time series forecasting. The traditional method of time series forecasting relies on analyzing historical data and utilizing statistical models to predict future trends, with an assumption that past patterns will persist in the future [8, 12, 20, 27, 42]. However, these methods were limited to smallscale datasets. The advent of deep learning [30] has introduced a range of time series forecasting networks [33, 34, 36, 40, 53, 59, 60, 67, 69] that excel in managing larger, more complex datasets by capturing non-linearities and dependencies directly from historical data. Recent advancements include pre-training on diverse, large-scale datasets, allowing models to be fine-tuned on specific tasks with fewer data and resources [6, 24, 58, 63]. While these methods continue to evolve and improve performance benchmarks, they often neglect the influence of external and contextual factors.
Attempts to incorporate textual information (e.g., Twitter feeds, news articles, public reports) into time series forecasting have been made across various domains, such as finance [7, 48, 49], energy [3, 41], entertainment [26], pandemics [65], and tourism [47]. Traditional methods often simplify text analysis to counting keyword frequencies [41] or using dummy variables, which do not capture nuanced meanings. Advanced efforts include extracting richer textual features like word frequencies and sentiments using traditional NLP [9] and machine learning methods, as demonstrated by Bai et al. [3]. However, these approaches require labor-intensive feature engineering, struggle with long-text dependencies, and lack a deep contextual understanding. In contrast, LLMs excel in processing complex textual data and understanding contextual relationships, which can improve prediction accuracy and efficiency through automated feature extraction and enhanced scalability across multiple tasks. Despite their potential, no study has yet fully exploited LLM to enhance forecasting with their capabilities in understanding unstructured textual data.
Language models for time series forecasting. LLMs such as the GPT series [1, 4, 44, 45] and LLaMa [54] have excelled in a variety of natural language processing tasks. With their vast parameter sets, LLMs acquire extensive general knowledge and reasoning capabilities during pre-training, which is crucial for building intelligent systems equipped with common sense. LLMs architectures are increasingly applied to time series processing and forecasting [23, 25, 35, 37, 51]. For instance, TEMPO [6] adapts GPT architectures for dynamic temporal representation learning, while TIMELLM [23] utilizes LLMs for time series forecasting by reprogramming input data and applying Prompt-as-Prefix techniques. Similarly, FPT [71] demonstrates that even frozen LLMs can perform effectively in time series tasks, leveraging the universality of self-attention mechanisms. Lag-LLaMa [46] uses a decoder-only transformer for univariate probabilistic forecasting, and Gruver et al. [17] show that by framing time series forecasting as next-token prediction, LLMs can surpass traditional models through effective tokenization and adaptation. However, existing studies mainly utilize the mapping capabilities of LLMs for numerical regression, without incorporating external textual inputs or leveraging the reasoning abilities of LLMs in understanding language.
Reasoning with language models. LLMs can automate tasks with human-like reasoning through "Chain of Thought" (CoT) prompting [57], enhancing reasoning by step-by-step emulation of human thinking [28, 32]. CoT prompting is useful for transforming complex questions into answers by introducing intermediate steps. The "Tree of Thoughts" (ToT) approach [61] refines this by mimicking trial-and-error methods, enhancing auto-regressive LLMs with a prompter agent, checker module, memory module, and ToT controller for multi-round dialogues. LLM-based agents can solve tasks by reflecting feedback signals in text and retaining them in a memory buffer for better decisions [50]. Cai et al. [5] proposed the LLMs As Tool Makers (LATM) framework, where LLMs create reusable tools for problem-solving, interleaving reasoning and actions to aid task completion and interaction [62]. These agents can debate their responses and reasoning to arrive at final actions [11].</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Relationship between news and time series. This figure illustrates the news filtered by the reasoning agents, using the example of Australia's state-level electricity demand. It features load data in Victoria state and selected news from June 10 to 12, 2019. The black arrow indicates time-specific events, the blue curve shows load fluctuations. The x-axis represents time, and the y-axis displays load values in kilowatts. The blue box displays the short-term impact news and long-term impact news selected by the reasoning agent (e.g., traffic incidents or new construction projects).</p>
<h1>3 Method</h1>
<p>In this work, we aim to integrate news insights into time series forecasting. The development of such a system faces several challenges. Firstly, the forecasting method must handle unstructured, non-numerical news inputs flexibly and adjust predictions based on the context of the news events. Secondly, constructing this model involves filtering news and establishing connections between the news and the time series data. This requires sifting through vast amounts of internet data to find relevant information, demanding deep societal understanding and sophisticated reasoning skills. Therefore, an intelligent agent is designed to manage this complexity. Moreover, potential inaccuracies in news selection or inferential errors may still affect the forecast accuracy, requiring further refinement of news selection and reasoning based on the predictions. Our approach includes three main modules: a language model-based forecasting module (Sec 3.1), a reasoning agent for news filtering and inference, and an evaluation agent to assess and refine the forecasting model (Sec 3.2). The core workflow of our method and the interrelations between these modules are shown in Figure 1. The subsequent sections will discuss these three modules in detail.</p>
<h3>3.1 Rethinking Time Series Forecasting Problem and Elements</h3>
<p>Time series forecasting can be considered as a conditional generation problem of sequences [17]. This aligns with the general paradigm of natural language processing represented by LLMs. Taking the LLaMa language model as an example, assuming a number series {123,456}, LLaMa's tokenizer will regard this number as a sequence of digit tokens, <em>i.e.</em>, {"1","2","3",","4","5","6"}. Given the input series "123", the probability of predicting "456" can be represented as a probabilistic forecasting process in an autoregressive manner: $$P("456"|"123") = P("4"|"123") \cdot P("5"|"4", "123") \cdot P("6"|"45", "123")$$. Generally, denoting the time series tokens at time t as x<sub>t</sub>, the LLM predict the next token in the series x<sub>t+1</sub> using the conditional probability distribution P(x<sub>t+1</sub> |x<sub>0:t</sub>). During pre-training, LLMs optimize its internal parameters to maximize this conditional probability over a wide range of natural language corpora. Though counterintuitive, Gruver et al. [17] have shown that pre-trained language models exhibit a significant few-shot capability for time series forecasting. This shows the potential of language models in understanding input digital tokens, and also inspires us to use the language model as a reasonable platform to study how to introduce the information contained in textual prompt into time series prediction.</p>
<p>News context offers critical insights into complex social events that traditional numerical data often overlook, and it also reflects sudden shifts in time series due to random events. In fact, the time series we collected can already be seen as being influenced by the aforementioned events. Assume an event E and a time series x<sub>0:t</sub>, its impact on the future sequence can be expressed also as a conditional probability P(x<sub>t</sub> |x<sub>0:t</sub>, E). However, when information about event E is not provided, we can only predict through historical time series. Although time series data itself can show patterns and trends, it</p>
<p>lacks the ability to indicate the causality behind events. The event information $\mathcal{E}$ offers the context needed to understand why certain spikes or drops occur. We show in Figure 2 some news that are closely related to time series forecasting.</p>
<p>In language models, news events can also be represented as text tokens. Consider a set of news text tokens $\left{e_{0}, e_{1}, \ldots, e_{u}\right}$, which represent event $\mathcal{E}$. LLMs treat this news information as a condition input and perform conditional probability predictions $P\left(x_{t} \mid x_{0: t}, e_{0: u}\right)$. Including $e_{0: u}$ provides crucial contextual information that influences the prediction of future values. This process aligns with the standard approach used by language models to interpret text, allowing for the incorporation of information about multiple events through various news contexts simultaneously to enhance prediction accuracy. In practice, we only need to integrate news text with historical time series data in a prompt engineering manner.</p>
<p>Other supplementary information also provide contextual information for the forecasting model. For example, weather and climatic factors may affect energy demand and industrial output; financial indicators and economic metrics influence consumer behavior and business operations. Including this diverse range of information allows models to adjust for environmental, economic, and seasonal variations, improving prediction accuracy. The supplementary information can also be understood as conditions and integrated into the above conditional probability forecasting framework. Our approach incorporates this information into language for flexible integration with language models. For example, we use the text "Weather on historical dates: the lowest temperature is 292.01; the highest temperature is 298.07; the humidity is 94 ." to express weather conditions. This enriches the input to cover various factors affecting the time series. Part (c) of Figure 1 illustrates the method of prompt integration for time series forecasting and the corresponding responses.</p>
<p>Fine-tuning LLMs for time series forecasting. Integrating the above information, we can construct inputs for LLMs to perform time series forecasting. Although pre-trained LLMs are already capable of generating time series predictions to some extent, relying on these pre-trained models for few-shot predictions in such a context-rich environment poses several challenges. Firstly, controlling the output of time series is difficult; predicting long sequences of numerical digit tokens is uncommon for LLMs. Secondly, the connections between the news and supplementary information and the time series typically need to be derived from historical data, which goes beyond the usual scope of using LLMs for few-shot time series predictions. To enable language models to more effectively forecast time series while considering the conditions imposed by news and supplementary information, we propose fine-tuning the language models to predict conditional probabilities. We employ a supervised instruction tuning method to train LLMs on historical time series data paired with corresponding news and supplementary information, formatted into text input-output pairs (shown in Appendix A.3). The same loss function used during pre-training is applied here. To fine-tune the LLM, we use the Low-Rank Adaptation (LoRA) method [19], which updates only a small subset of parameters, reducing computational demands while retaining most of the pre-trained knowledge. This strategy allows the model to efficiently adapt to new forecasting tasks without losing its foundational strengths.</p>
<h1>3.2 Analytical Agent for Aggregation and Reasoning of Contextual News Information.</h1>
<p>Next, we construct a dataset to train the above model. While obtaining time series data is relatively straightforward, matching it with appropriate news and supplementary information is not trivial. The internet is flooded with news, most of which are irrelevant to the time series we aim to forecast. Introducing irrelevant news can disrupt forecasting. Therefore, it is crucial to analyze the relevance and causality between the time series forecasting task and the select news accordingly. However, gaining such an understanding is complex, requiring knowledge of human societal mechanisms and logical reasoning skills. In our work, we utilize LLMs for filtering and reasoning about news content. We also recognize that even the most advanced language models cannot complete all reasoning and judgment in a single generation process. We explain how to use a combination of multiple LLM generations to create an intelligent agent that fulfills the complex requirements of news filtering.</p>
<p>Time series and news pre-pairing. For the initial stage of data preparation, news is retrieved to align with time series data based on matching time frequencies, horizons, and geographical areas. This synchronization ensures that insights from textual information are timely and regionally relevant. For example, to understand state-level electricity demand in Australia from 2019 to 2021, we gather local news from various Australian states and international news occurring in the same period that might directly or indirectly affect demand. In this way, potentially relevant candidate information can be roughly selected first, and such filtering can be easily completed through crawler means.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Example of prompt designs for each iteration during fine-tuning. Step 1 involves the reasoning agent selecting news using default logic. Step 2 evaluates predictions based on validation sets to refine the logic. In step 3, the updated logic directs data pairing for the next iteration. Full prompts are shown in Appendix A.6.</p>
<p>Reasoning agent for news selection. We employ an LLM-based reasoning agent capable of sophisticated tasks such as conversation, reasoning, and semi-autonomous action. This agent is programmed using detailed prompts that define roles, instructions, permissions, and context, enabling it to interpret human commands and perform complex tasks. This approach condenses extensive news datasets into a refined selection of pertinent articles. It leverages its reasoning capabilities to effectively screen, categorize, and interpret news texts. We employ few-shot prompting [4] and the CoT [57] method to develop an agent that understands the context of news for forecasting. This technique enhances the model's ability to handle multi-step, commonsense reasoning tasks essential for accurate forecasting. The agent uses multi-step prompts to break down complex problems into simpler, manageable parts. Our three-phase prompting method, illustrated in Figure 3 Part I, includes:</p>
<ol>
<li>We develop an understanding of time series influencers, sorting them by impact (positive/negative) and duration (short/long-term), considering economic, policy, seasonal, and technological factors;</li>
<li>We direct the agent to filter and categorize news based on either automatically generated logic or a given reasoning logic, focusing on relevance to time series and classifying the impact (e.g., long-term and short-term) along with the rationale;</li>
<li>We specify the output format for the agent to organize the selected news into JSON, detailing aspects like summary, affected region, reporting time, and rationale. More details about our prompting method can be found in Appendix A.6.</li>
</ol>
<p>The LLM agent can automatically develop an understanding of time series influencers, with the option to provide predefined reasoning logic in our models. In the automated process, the agent forms its logic through prompts designed to guide it in determining how different types of news impact a specific domain. For instance, we use open-ended questions to allow the agent to independently summarize and create its own filtering logic. User knowledge can also be incorporated into these prompts as additional reasoning, enabling the agent to generate more comprehensive logic. The agent then filters news based on the generated logic, whether fully automated or with user-provided input.
Evaluation agent for reasoning updating. We also design an evaluation agent to assess and improve the effectiveness of the aforementioned news filtering. Relying solely on the reasoning agent for news selection is not optimal, as the interaction between news and time series is complex. The reasoning agent can only analyze the potential impact of different news from the perspective of news content, without knowing whether the trained time series forecasting model based on them can make accurate forecasts. The evaluation agent is deployed after the time series prediction model has been trained. The evaluation agent extends beyond simple numerical assessments of prediction accuracy by incorporating human-like logical reasoning to refine the news selection logic chain. We focus our evaluation on identifying inaccuracies potentially caused by missing news, such as unusual events or illogical reports. It observes the model's predictive outcomes to determine if any crucial news has been overlooked and adjusts the news filtering strategy for the training data based on these results.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Overall pipeline iteratively combines news reasoning agents, fine-tuning, and evaluation agents.
For the evaluation agent, we structured the prompt design into three phases, as illustrated in Figure 3 Part II. In the first phase, we input the forecasting task type, the time horizon, and background information, which the agent uses to generate the steps for evaluating the prediction outcomes. In the second phase, we provide the ground truth, the discrepancies between predicted and actual series, as well as selected and historical news. The agent analyzes these inputs to identify overlooked news based on the distribution of prediction errors over time. In the third phase, the agent generates updated logic based on its analysis, guiding future news selection. After processing all validation set predictions, the reasoning agent consolidates the updated logic into a cohesive final strategy.</p>
<h1>3.3 Overall Pipeline</h1>
<p>We integrate the news reasoning and evaluation agents with the fine-tuning of the LLM forecasting model to enhance the quality of training data, as illustrated in Figure 4. In the first iteration, we use the LLM agent to establish news selection logic based on the domain and timing of the time series task. This logic directs the reasoning agent to filter relevant news, align it with time series data, and input it into the model for initial fine-tuning. After validating the model's predictions with a validation set, which is randomly extracted from the training data for each iteration, the evaluation agent checks for any missing news that may have influenced the prediction. This feedback helps the reasoning agent refine the filtering logic in subsequent iterations. The cycle continues until the final iteration, where the reasoning agent consolidates all updates to create the definitive news filter for training the final model. We use the GPT-4 Turbo model as the LLM for the agents described above.</p>
<h2>4 Experiments</h2>
<h3>4.1 Data preparation</h3>
<p>Time series data. We selected time series data from domains influenced by human activities and social events to test our method's ability to capture complex human-driven dynamics during forecasting. These domains include Traffic [39] (traffic volume), Exchange [29] (exchange rate), Bitcoin [15] (Bitcoin price), and Electricity[15] (Australian electricity demand). To avoid bias from pre-trained language models, we updated the Exchange and Electricity datasets up to 2022. We use half-hourly electricity demand data from the Australian Energy Market Operator (AEMO) (aemo. com. au) and daily exchange rate data from the Exchange Rates API (exchangeratesapi. io). These datasets vary in frequency, including daily, hourly, and half-hourly updates, allowing us to evaluate the algorithms' effectiveness across different temporal resolutions. More details are in Appendix A.1.
News collection. Since there are no public datasets that pair time series data with news events, we have collect news specifically for the above time series to facilitate our research. Some of the news content is collected from the GDELT dataset [31], a database tracking news from nearly every country in over 100 languages. GDELT provides real-time insights into societal, political, and economic events, enabling detailed analysis of global trends and their effects. We incorporate GDELT's event information into our forecasting models to enhance predictive accuracy. For domains needing the latest information, we collect real-time news from sources like News Corp Australia (news. com. au) and Yahoo Finance (yahoo. com), focusing on region-specific and task-specific activities.</p>
<p>Supplementary information. We enhance our forecasting models with open-source tools to grasp additional data for better accuracy and context. Weather information from OpenWeatherMap [10] provides daily temperatures, atmospheric pressure, wind speed, and humidity, crucial for load forecasting. Calendar dates, obtained using the Python packages datetime and holidays, account for seasonal and cyclical effects. Economic indicators are integrated using the pandas_datareader library, accessing data like GDP, inflation rates, and employment statistics from sources such as the Federal Reserve, World Bank, and international financial markets.</p>
<h1>4.2 Results</h1>
<p>Effectiveness of news intergration. In our approach, we incorporate news and supplementary information into time series forecasting by fine-tuning language models. Firstly, we assess whether this additional information can enhance time series forecasting. We conducted experiments to verify the necessity and effectiveness of integrating news data into our forecasting model. We compared four different scenarios as detailed in Appendix A. 2 and Appendix A.3:</p>
<ol>
<li>Pure numerical tokens: Uses numerical tokens, encompassing all variables without news. Except for region names or date information, it excludes other textual tokens as a baseline for comparison.</li>
<li>Textual descriptive sentence tokens: Evaluates whether using sentence-form descriptions instead of only raw digital numbers can enhance accuracy, with no news integration included.</li>
<li>Unfiltered news with textual descriptive sentence tokens: Assesses how integrating descriptive sentences of time series with unfiltered news data affects the model's performance.</li>
<li>Filtered news with textual descriptive sentence tokens: Shows the effects of integrating descriptive sentences with news that has been specifically filtered for relevance by the proposed agents.</li>
</ol>
<p>Table 1: Performance comparison of different prompt designs. Red font indicates the best.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Electricity</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Exchange</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RMSE</td>
<td style="text-align: center;">$\mathrm{MSE}_{&lt;10^{-}}$</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MAPE</td>
<td style="text-align: center;">$\mathrm{RMSE}_{&lt;10^{2}}$</td>
<td style="text-align: center;">$\mathrm{MSE}_{&lt;10^{3}}$</td>
<td style="text-align: center;">$\mathrm{MAE}_{&lt;10^{3}}$</td>
<td style="text-align: center;">MAPE</td>
</tr>
<tr>
<td style="text-align: center;">Only Numeric Prompt</td>
<td style="text-align: center;">337.10</td>
<td style="text-align: center;">113.64</td>
<td style="text-align: center;">204.89</td>
<td style="text-align: center;">$5.27 \%$</td>
<td style="text-align: center;">7.80</td>
<td style="text-align: center;">6.10</td>
<td style="text-align: center;">5.74</td>
<td style="text-align: center;">$0.77 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Textual Prompt without News</td>
<td style="text-align: center;">336.41</td>
<td style="text-align: center;">113.17</td>
<td style="text-align: center;">206.08</td>
<td style="text-align: center;">$5.29 \%$</td>
<td style="text-align: center;">7.41</td>
<td style="text-align: center;">5.49</td>
<td style="text-align: center;">5.44</td>
<td style="text-align: center;">$0.73 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Textual Prompt with Non-Filtered News</td>
<td style="text-align: center;">407.86</td>
<td style="text-align: center;">166.35</td>
<td style="text-align: center;">250.75</td>
<td style="text-align: center;">$6.84 \%$</td>
<td style="text-align: center;">8.28</td>
<td style="text-align: center;">6.86</td>
<td style="text-align: center;">6.37</td>
<td style="text-align: center;">$0.85 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Textual Prompt with Filtered News</td>
<td style="text-align: center;">280.39</td>
<td style="text-align: center;">78.62</td>
<td style="text-align: center;">180.96</td>
<td style="text-align: center;">$5.15 \%$</td>
<td style="text-align: center;">6.46</td>
<td style="text-align: center;">4.17</td>
<td style="text-align: center;">4.83</td>
<td style="text-align: center;">$0.65 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Traffic</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Bitcoin</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{RMSE}_{&lt;10^{2}}$</td>
<td style="text-align: center;">$\mathrm{MSE}_{&lt;10^{3}}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{MAE}_{&lt;10^{3}}$</td>
<td style="text-align: center;">$\mathrm{RMSE}_{&lt;10^{-3}}$</td>
<td style="text-align: center;">$\mathrm{MSE}_{&lt;10^{-5}}$</td>
<td style="text-align: center;">$\mathrm{MAE}_{&lt;10^{-7}}$</td>
<td style="text-align: center;">MAPE</td>
</tr>
<tr>
<td style="text-align: center;">Only Numeric Prompt</td>
<td style="text-align: center;">4.55</td>
<td style="text-align: center;">2.07</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1.66</td>
<td style="text-align: center;">4.46</td>
<td style="text-align: center;">19.94</td>
<td style="text-align: center;">3.07</td>
<td style="text-align: center;">$5.72 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Textual Prompt without News</td>
<td style="text-align: center;">4.44</td>
<td style="text-align: center;">1.97</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1.54</td>
<td style="text-align: center;">3.87</td>
<td style="text-align: center;">14.97</td>
<td style="text-align: center;">2.76</td>
<td style="text-align: center;">$5.08 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Textual Prompt with Non-Filtered News</td>
<td style="text-align: center;">4.89</td>
<td style="text-align: center;">2.39</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1.89</td>
<td style="text-align: center;">4.02</td>
<td style="text-align: center;">16.13</td>
<td style="text-align: center;">2.88</td>
<td style="text-align: center;">$5.35 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Textual Prompt with Filtered News</td>
<td style="text-align: center;">4.22</td>
<td style="text-align: center;">1.78</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1.43</td>
<td style="text-align: center;">3.67</td>
<td style="text-align: center;">13.41</td>
<td style="text-align: center;">2.68</td>
<td style="text-align: center;">$4.95 \%$</td>
</tr>
</tbody>
</table>
<p>The performance of different prompt designs is presented in Table 1. It can be seen that the fine-tuned LLM can be used to forecast time series even when using only digital tokens as prompts. The introduction of proper news and other supplementary information leads to significant performance improvements across all four domains. Nevertheless, such improvements are not easily achieved. If the introduced news information is not carefully selected, it can severely impair the results. There are two main reasons for this: first, the influx of a large number of news items introduces too many tokens, which can decrease the performance of the LLM as the number of tokens increases. Second, irrelevant news can introduce noise and incorrect causal information, leading to misleading predictions.</p>
<p>Effectiveness of the evaluation agent. To make our news filtering and reasoning processes more effective and comprehensive, we introduce an evaluation agent to reflect on and improve the effects of news selection based on prediction outcomes. As shown in Table 2, our evaluation agent refines news filtering through an iterative process, which is reflected in the progressively improved time series prediction results. The result corresponding to each case is the prediction outcome of the model after pairing the news selected according to the logic obtained from the corresponding iteration. Our findings suggest that, in most cases, two iterations are sufficient to achieve significant improvements, with multiple iterations consistently yielding better results than a single iteration due to the reflection mechanisms. We also found that this iterative filtering process reveals interesting insights into human society from the language model agent. These examples are presented in the Appendix A.7.</p>
<p>Compare to other forecasting methods. We also compare our method with existing time series forecasting techniques, detailed in Appendix A.1. We show a quantitative comparison against these baseline methods in Table 3. While the baseline methods use inverse normalization to revert predictions to their original scale, our model operates without normalization. This approach retains</p>
<p>Table 2: Comparison of Iterative Analysis. The baseline case is the initial selection. The arrow means the comparison of each case with baseline cases. A red downward arrow indicates an improvement, a blue upward arrow indicates performance degradation.</p>
<table>
<thead>
<tr>
<th></th>
<th>Electricity</th>
<th></th>
<th></th>
<th></th>
<th>Exchange</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>RMSE</td>
<td>$\mathrm{MSE}_{&lt;10^{-3}}$</td>
<td>MAE</td>
<td>MAPE</td>
<td>$\mathrm{RMSE}_{&lt;10^{3}}$</td>
<td>$\mathrm{MSE}_{&lt;10^{3}}$</td>
<td>$\mathrm{MAE}_{&lt;10^{3}}$</td>
</tr>
<tr>
<td>1. Initial selection</td>
<td>313.89</td>
<td>98.53</td>
<td>190.79</td>
<td>$5.36 \%$</td>
<td>6.61</td>
<td>4.37</td>
<td>4.83</td>
</tr>
<tr>
<td>2. The second selection</td>
<td>$+287.35$</td>
<td>$+82.57$</td>
<td>$+180.49$</td>
<td>$+4.93 \%$</td>
<td>$+6.46$</td>
<td>$+4.17$</td>
<td>$+4.83$</td>
</tr>
<tr>
<td>3. The third selection</td>
<td>$+303.03$</td>
<td>$+91.83$</td>
<td>$+192.30$</td>
<td>$+5.38 \%$</td>
<td>$+7.69$</td>
<td>$+5.92$</td>
<td>$+5.63$</td>
</tr>
<tr>
<td>4. The fourth selection</td>
<td>$+280.39$</td>
<td>$+78.62$</td>
<td>$+180.96$</td>
<td>$+5.15 \%$</td>
<td>$+6.60$</td>
<td>$+4.36$</td>
<td>$+4.82$</td>
</tr>
<tr>
<td></td>
<td>Traffic</td>
<td></td>
<td></td>
<td></td>
<td>Bitcoin</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>$\mathrm{RMSE}_{&lt;10^{3}}$</td>
<td>$\mathrm{MSE}_{&lt;10^{3}}$</td>
<td></td>
<td>$\mathrm{MAE}_{&lt;10^{3}}$</td>
<td>$\mathrm{RMSE}_{&lt;10^{-3}}$</td>
<td>$\mathrm{MSE}_{&lt;10^{-3}}$</td>
<td>$\mathrm{MAE}_{&lt;10^{-3}}$</td>
</tr>
<tr>
<td>1. Initial selection</td>
<td>4.36</td>
<td>1.90</td>
<td></td>
<td>1.45</td>
<td>4.12</td>
<td>16.98</td>
<td>2.97</td>
</tr>
<tr>
<td>2. The second selection</td>
<td>4.36</td>
<td>1.90</td>
<td></td>
<td>$+1.52$</td>
<td>$+3.67$</td>
<td>$+13.41$</td>
<td>$+2.68$</td>
</tr>
<tr>
<td>3. The third selection</td>
<td>$+4.22$</td>
<td>$+1.78$</td>
<td></td>
<td>$+1.43$</td>
<td>$+3.75$</td>
<td>$+14.08$</td>
<td>$+2.83$</td>
</tr>
</tbody>
</table>
<p>Table 3: Comparison of baselines for time series forecasting on different metrics. A lower value indicates better performance. Red: the best. Blue: the second best.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Domains</th>
<th style="text-align: center;">Metrics</th>
<th style="text-align: center;">Ours</th>
<th style="text-align: center;">Auto. [59]</th>
<th style="text-align: center;">In. [67]</th>
<th style="text-align: center;">Dlin. [64]</th>
<th style="text-align: center;">iTrans. [38]</th>
<th style="text-align: center;">Fd.31 [68]</th>
<th style="text-align: center;">Times. [58]</th>
<th style="text-align: center;">Pyra. [36]</th>
<th style="text-align: center;">PatchTST [40]</th>
<th style="text-align: center;">FED. [69]</th>
<th style="text-align: center;">GPTATS [70]</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Electricity</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">188.96</td>
<td style="text-align: center;">349.43</td>
<td style="text-align: center;">282.56</td>
<td style="text-align: center;">255.7</td>
<td style="text-align: center;">233.58</td>
<td style="text-align: center;">254.05</td>
<td style="text-align: center;">237.49</td>
<td style="text-align: center;">220.32</td>
<td style="text-align: center;">234.46</td>
<td style="text-align: center;">238.77</td>
<td style="text-align: center;">236.91</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{MSE}_{&lt;10^{-3}}$</td>
<td style="text-align: center;">78.62</td>
<td style="text-align: center;">251.79</td>
<td style="text-align: center;">166.07</td>
<td style="text-align: center;">161.59</td>
<td style="text-align: center;">135.27</td>
<td style="text-align: center;">153.90</td>
<td style="text-align: center;">134.42</td>
<td style="text-align: center;">97.61</td>
<td style="text-align: center;">133.53</td>
<td style="text-align: center;">133.96</td>
<td style="text-align: center;">142.60</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RMSE</td>
<td style="text-align: center;">206.39</td>
<td style="text-align: center;">501.78</td>
<td style="text-align: center;">407.52</td>
<td style="text-align: center;">401.98</td>
<td style="text-align: center;">367.79</td>
<td style="text-align: center;">392.3</td>
<td style="text-align: center;">366.64</td>
<td style="text-align: center;">312.42</td>
<td style="text-align: center;">365.41</td>
<td style="text-align: center;">366</td>
<td style="text-align: center;">377.62</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MAPE</td>
<td style="text-align: center;">5.15\%</td>
<td style="text-align: center;">10.63\%</td>
<td style="text-align: center;">8.94\%</td>
<td style="text-align: center;">7.29\%</td>
<td style="text-align: center;">6.86\%</td>
<td style="text-align: center;">7.36\%</td>
<td style="text-align: center;">6.81\%</td>
<td style="text-align: center;">6.87\%</td>
<td style="text-align: center;">6.56\%</td>
<td style="text-align: center;">6.75\%</td>
<td style="text-align: center;">6.61\%</td>
</tr>
<tr>
<td style="text-align: center;">Exchange</td>
<td style="text-align: center;">$\mathrm{MAE}_{&lt;10^{-3}}$</td>
<td style="text-align: center;">4.83</td>
<td style="text-align: center;">9.27</td>
<td style="text-align: center;">1.75</td>
<td style="text-align: center;">6.96</td>
<td style="text-align: center;">5.12</td>
<td style="text-align: center;">6.44</td>
<td style="text-align: center;">5.24</td>
<td style="text-align: center;">14.6</td>
<td style="text-align: center;">6.73</td>
<td style="text-align: center;">8.98</td>
<td style="text-align: center;">15.05</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{MSE}_{&lt;10^{-3}}$</td>
<td style="text-align: center;">0.42</td>
<td style="text-align: center;">1.36</td>
<td style="text-align: center;">4.76</td>
<td style="text-align: center;">0.91</td>
<td style="text-align: center;">0.45</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">0.45</td>
<td style="text-align: center;">3.55</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">1.28</td>
<td style="text-align: center;">4.01</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RMSE $_{&lt;10^{-3}}$</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">1.17</td>
<td style="text-align: center;">2.18</td>
<td style="text-align: center;">9.52</td>
<td style="text-align: center;">0.671</td>
<td style="text-align: center;">0.875</td>
<td style="text-align: center;">0.673</td>
<td style="text-align: center;">1.88</td>
<td style="text-align: center;">0.875</td>
<td style="text-align: center;">1.13</td>
<td style="text-align: center;">2.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MAPE</td>
<td style="text-align: center;">0.65\%</td>
<td style="text-align: center;">1.23\%</td>
<td style="text-align: center;">2.32\%</td>
<td style="text-align: center;">0.92\%</td>
<td style="text-align: center;">0.68\%</td>
<td style="text-align: center;">0.85\%</td>
<td style="text-align: center;">0.70\%</td>
<td style="text-align: center;">1.94\%</td>
<td style="text-align: center;">0.90\%</td>
<td style="text-align: center;">1.21\%</td>
<td style="text-align: center;">1.34\%</td>
</tr>
<tr>
<td style="text-align: center;">Traffic</td>
<td style="text-align: center;">$\mathrm{MAE}_{&lt;10^{-3}}$</td>
<td style="text-align: center;">1.43</td>
<td style="text-align: center;">2.49</td>
<td style="text-align: center;">4.44</td>
<td style="text-align: center;">1.70</td>
<td style="text-align: center;">1.56</td>
<td style="text-align: center;">1.65</td>
<td style="text-align: center;">1.61</td>
<td style="text-align: center;">1.81</td>
<td style="text-align: center;">1.84</td>
<td style="text-align: center;">1.74</td>
<td style="text-align: center;">1.64</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{MSE}_{&lt;10^{-3}}$</td>
<td style="text-align: center;">1.78</td>
<td style="text-align: center;">2.19</td>
<td style="text-align: center;">5.27</td>
<td style="text-align: center;">1.67</td>
<td style="text-align: center;">1.54</td>
<td style="text-align: center;">1.71</td>
<td style="text-align: center;">1.49</td>
<td style="text-align: center;">0.98</td>
<td style="text-align: center;">1.54</td>
<td style="text-align: center;">1.43</td>
<td style="text-align: center;">1.45</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RMSE $_{&lt;10^{-3}}$</td>
<td style="text-align: center;">4.22</td>
<td style="text-align: center;">4.68</td>
<td style="text-align: center;">7.26</td>
<td style="text-align: center;">4.69</td>
<td style="text-align: center;">3.93</td>
<td style="text-align: center;">4.14</td>
<td style="text-align: center;">3.86</td>
<td style="text-align: center;">3.33</td>
<td style="text-align: center;">3.92</td>
<td style="text-align: center;">2.79</td>
<td style="text-align: center;">3.81</td>
</tr>
<tr>
<td style="text-align: center;">Bitcoin</td>
<td style="text-align: center;">$\mathrm{MAE}_{&lt;10^{-3}}$</td>
<td style="text-align: center;">2.68</td>
<td style="text-align: center;">4.28</td>
<td style="text-align: center;">12.27</td>
<td style="text-align: center;">5.74</td>
<td style="text-align: center;">3.20</td>
<td style="text-align: center;">3.28</td>
<td style="text-align: center;">3.17</td>
<td style="text-align: center;">9.22</td>
<td style="text-align: center;">2.85</td>
<td style="text-align: center;">3.96</td>
<td style="text-align: center;">2.84</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{MSE}_{&lt;10^{-3}}$</td>
<td style="text-align: center;">13.41</td>
<td style="text-align: center;">27.64</td>
<td style="text-align: center;">162.47</td>
<td style="text-align: center;">50.90</td>
<td style="text-align: center;">16.21</td>
<td style="text-align: center;">17.65</td>
<td style="text-align: center;">16.38</td>
<td style="text-align: center;">123.71</td>
<td style="text-align: center;">13.52</td>
<td style="text-align: center;">24.60</td>
<td style="text-align: center;">13.66</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RMSE $_{&lt;10^{-3}}$</td>
<td style="text-align: center;">2.67</td>
<td style="text-align: center;">5.26</td>
<td style="text-align: center;">12.75</td>
<td style="text-align: center;">7.13</td>
<td style="text-align: center;">4.03</td>
<td style="text-align: center;">4.20</td>
<td style="text-align: center;">4.05</td>
<td style="text-align: center;">11.12</td>
<td style="text-align: center;">3.68</td>
<td style="text-align: center;">4.96</td>
<td style="text-align: center;">3.70</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MAPE</td>
<td style="text-align: center;">4.95\%</td>
<td style="text-align: center;">7.61\%</td>
<td style="text-align: center;">21.28\%</td>
<td style="text-align: center;">10.39\%</td>
<td style="text-align: center;">5.70\%</td>
<td style="text-align: center;">5.84\%</td>
<td style="text-align: center;">5.64\%</td>
<td style="text-align: center;">16.16\%</td>
<td style="text-align: center;">5.13\%</td>
<td style="text-align: center;">6.97\%</td>
<td style="text-align: center;">5.08\%</td>
</tr>
</tbody>
</table>
<p>the physical meaning of data, such as electricity demand or economic indicators, ensuring that our outputs remain interpretable. Normalization could obscure the impact of news events due to the nonlinear and scale-dependent relationship between these events and the original data values.</p>
<p>Our approach significantly outperforms traditional methods that rely solely on historical time series data in domains like electricity demand, exchange rates, and the bitcoin market, where events embodied in the news have substantial impacts. This demonstrates the potential of our method. However, the improvement from integrating news into the traffic sector is notably modest. The performance of our traffic forecasting model, which covers all roads in California, is hampered by the coarse granularity of publicly available news data, lacking the local details necessary for precise predictions. Traffic data primarily reflects specific road traffic flows, whereas our news sources are mostly regional or global, failing to capture localized traffic conditions adequately. This limitation is evident in the model's Mean Squared Error (MSE), which is sensitive to outliers and tends to exaggerate errors from traffic spikes that state-level news often does not report. Our model achieves good results with the Mean Absolute Error (MAE), indicating reliable average accuracy. Incorporating more localized road information could potentially improve these issues.</p>
<p>Figure 5 takes the electricity domain as an example and compares the ground truth with predictions by cases with or without news data, demonstrating the effect of incorporating news into forecasting models. The "With News" predictions are closer to the actual values than the "No News" predictions, particularly at critical timestamps where abrupt events significantly influence electricity demand.</p>
<h2>5 Conclusion and Discussion</h2>
<p>In conclusion, our study demonstrates the benefits of integrating news into time series forecasting using LLM-based forecasting method and LLM-based agents. These agents enhance model intelligence by autonomously identifying and addressing missed news, refining their logic, and assessing the impact of events on predictions. Our findings advocate for incorporating extensive domain knowledge, encouraging a shift towards more nuanced and context-aware forecasting. This approach enriches time series forecasting for adaptive, comprehensive forecasting aligned with real-world dynamics.</p>
<p>Limitations of our approach. While our approach demonstrates that LLMs like LLaMa 2 [55] can enhance time series forecasting by integrating news, there are limitations to its applicability.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Day-ahead Australia electricity demand forecasting with/without news. The horizontal axis is the time index (half hour). Actual load demands are in solid blue, predictions with news in solid red, and predictions without news in dashed green. (a) Sydney's lockdown news effects; (b) Residential electricity consumption behavior news effects; (c) Anticipated power outage news effects.</p>
<p>The effectiveness of news integration is primarily evident in domains where human and market activities significantly influence trends. Our framework is less suitable for domains requiring precise meteorological modeling or where human activities have minimal impact, such as in meteorological or physical data. Additionally, the model is constrained by the maximum token length of pretrained LLMs, complicating the simultaneous processing of large amounts of time series or multiple sequences, which can lead to data truncation and affect the accuracy of long-term predictions. Finally, our strategy enhances rather than completely replaces traditional time series tasks like classification or interpolation across all fields. Our aim is to demonstrate that by leveraging language models, it is possible to incorporate useful textual information to enhance time series prediction tasks.
Future work. Future enhancements will focus on extending the current forecasting model's scope in several key areas. Firstly, attribution analyses of news content used in the model will pinpoint which factors most significantly impact forecasting accuracy, facilitating an optimized news integration process. Advanced analytical toolkits can also be provided to reasoning agents, enabling sophisticated data processing and real-time application of complex analytical techniques. These developments will enhance the precision and relevance of the time series prediction model, contributing deeper contextual insights and expanding its applicability in the predictive analytics field.
Broader impact. Ethically, it is crucial that we conduct thorough reviews to ensure that our use of news content does not inadvertently perpetuate biases or negatively influence public opinion. This involves implementing rigorous checks for accuracy and balance to avoid the risks associated with misinformation, ensuring that our data sources are credible and that the content is factually correct. Furthermore, the potential misuse of news, especially the spread of "fake news", highlights the need for our models to incorporate sophisticated mechanisms to verify information reliability before integration. Beyond the discussed sectors, this approach has the capability to extend into forecasting GDP trends, analyzing carbon emissions, or predicting public health outcomes, each carrying significant implications for policymaking and public welfare. Thus, while our research offers substantial benefits in enhancing predictive analytics, it also obligates us to handle these capabilities responsibly, ensuring our contributions positively impact economic planning, environmental strategy, and informed decision-making across various domains.</p>
<h1>Acknowledgment</h1>
<p>This work was supported in part by the Shenzhen Key Lab of Crowd Intelligence Empowered Low-Carbon Energy Network (No. ZDSYS20220606100601002); in part by the National Science Foundation Grant of China (72331009); in part by the Australian Research Council (ARC) Research Hub under Grant IH180100020; in part by the ARC Training Centre under Grant IC200100023; in part by the ARC Linkage Project under Grant LP200100056 and Grant ARC DP220103881.</p>
<h2>References</h2>
<p>[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.
[2] Taghreed Alghamdi, Khalid Elgazzar, Magdi Bayoumi, Taysseer Sharaf, and Sumit Shah. Forecasting traffic congestion using arima modeling. In 2019 15th international wireless communications \&amp; mobile computing conference (IWCMC), pages 1227-1232. IEEE, 2019.</p>
<p>[3] Yun Bai, Simon Camal, and Andrea Michiorri. News and load: A quantitative exploration of natural language processing applications for forecasting day-ahead electricity system demand. IEEE Transactions on Power Systems, 2024.
[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.
[5] Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, and Denny Zhou. Large language models as tool makers. arXiv preprint arXiv:2305.17126, 2023.
[6] Defu Cao, Furong Jia, Sercan O Arik, Tomas Pfister, Yixiang Zheng, Wen Ye, and Yan Liu. Tempo: Promptbased generative pre-trained transformer for time series forecasting. arXiv preprint arXiv:2310.04948, 2023.
[7] Mark Cecchini, Haldun Aytug, Gary J Koehler, and Praveen Pathak. Making words work: Using financial text as a predictor of financial events. Decision support systems, 50(1):164-175, 2010.
[8] Bo-Juen Chen, Ming-Wei Chang, et al. Load forecasting using support vector machines: A study on eunite competition 2001. IEEE transactions on power systems, 19(4):1821-1830, 2004.
[9] KR1442 Chowdhary and KR Chowdhary. Natural language processing. Fundamentals of artificial intelligence, pages 603-649, 2020.
[10] Christine Dewi and Rung-Ching Chen. Integrating real-time weather forecasts data using openweathermap and twitter. International Journal of Information Technology and Business, 1(2):48-52, 2019.
[11] Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. Improving factuality and reasoning in language models through multiagent debate. arXiv preprint arXiv:2305.14325, 2023.
[12] Grzegorz Dudek. Short-term load forecasting using random forests. In Intelligent Systems' 2014: Proceedings of the 7th IEEE International Conference Intelligent Systems IS'2014, September 24-26, 2014, Warsaw, Poland, Volume 2: Tools, Architectures, Systems, Applications, pages 821-828. Springer, 2015.
[13] Robert Fildes, Shaohui Ma, and Stephan Kolassa. Retail forecasting: Research and practice. International Journal of Forecasting, 38(4):1283-1318, 2022.
[14] Brian S Freeman, Graham Taylor, Bahram Gharabaghi, and Jesse Th. Forecasting air quality time series using deep learning. Journal of the Air \&amp; Waste Management Association, 68(8):866-886, 2018.
[15] Rakshitha Godahewa, Christoph Bergmeir, Geoffrey I Webb, Rob J Hyndman, and Pablo Montero-Manso. Monash time series forecasting archive. arXiv preprint arXiv:2105.06643, 2021.
[16] George Gross and Francisco D Galiana. Short-term load forecasting. Proceedings of the IEEE, 75(12):15581573, 1987.
[17] Nate Gruver, Marc Finzi, Shikai Qiu, and Andrew G Wilson. Large language models are zero-shot time series forecasters. Advances in Neural Information Processing Systems, 36, 2024.
[18] James D Hamilton. Time series analysis. Princeton university press, 2020.
[19] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.
[20] Shyh-Jier Huang and Kuang-Rong Shih. Short-term load forecasting via arma model identification including non-gaussian process considerations. IEEE Transactions on power systems, 18(2):673-679, 2003.
[21] Rob J Hyndman and George Athanasopoulos. Forecasting: principles and practice. OTexts, 2018.
[22] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.
[23] Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y Zhang, Xiaoming Shi, Pin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, et al. Time-llin: Time series forecasting by reprogramming large language models. arXiv preprint arXiv:2310.01728, 2023.
[24] Ming Jin, Qingsong Wen, Yuxuan Liang, Chaoli Zhang, Siqiao Xue, Xue Wang, James Zhang, Yi Wang, Haifeng Chen, Xiaoli Li, et al. Large models for time series and spatio-temporal data: A survey and outlook. arXiv preprint arXiv:2310.10196, 2023.
[25] Ming Jin, Yifan Zhang, Wei Chen, Kexin Zhang, Yuxuan Liang, Bin Yang, Jindong Wang, Shirui Pan, and Qingsong Wen. Position paper: What can large language models tell us about time series analysis. arXiv preprint arXiv:2402.02713, 2024.
[26] Mahesh Joshi, Dipanjan Das, Kevin Gimpel, and Noah A Smith. Movie reviews and revenues: An experiment in text regression. In Human language technologies: The 2010 annual conference of the North American chapter of the Association for Computational Linguistics, pages 293-296, 2010.</p>
<p>[27] Prajakta S Kalekar et al. Time series forecasting using holt-winters exponential smoothing. Kanwal Rekhi school of information Technology, 4329008(13):1-13, 2004.
[28] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:22199-22213, 2022.
[29] Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. Modeling long-and short-term temporal patterns with deep neural networks. In The 41st international ACM SIGIR conference on research \&amp; development in information retrieval, pages 95-104, 2018.
[30] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436-444, 2015.
[31] Kalev Leetaru and Philip A Schrodt. Gdelt: Global data on events, location, and tone, 1979-2012. In ISA annual convention, volume 2, pages 1-49. Citeseer, 2013.
[32] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:3843-3857, 2022.
[33] Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, and Xifeng Yan. Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting. Advances in neural information processing systems, 32, 2019.
[34] Chang Liu, Zhijian Jin, Jie Gu, and Caiming Qiu. Short-term load forecasting using a long short-term memory network. In 2017 IEEE PES innovative smart grid technologies conference Europe (ISGT-Europe), pages 1-6. IEEE, 2017.
[35] Chenxi Liu, Sun Yang, Qianxiong Xu, Zhishuai Li, Cheng Long, Ziyue Li, and Rui Zhao. Spatial-temporal large language model for traffic prediction. arXiv preprint arXiv:2401.10134, 2024.
[36] Shizhan Liu, Hang Yu, Cong Liao, Jianguo Li, Weiyao Lin, Alex X Liu, and Schahram Dustdar. Pyraformer: Low-complexity pyramidal attention for long-range time series modeling and forecasting. In International conference on learning representations, 2021.
[37] Xu Liu, Junfeng Hu, Yuan Li, Shizhe Diao, Yuxuan Liang, Bryan Hooi, and Roger Zimmermann. Unitime: A language-empowered unified model for cross-domain time series forecasting. arXiv preprint arXiv:2310.09751, 2023.
[38] Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and Mingsheng Long. itransformer: Inverted transformers are effective for time series forecasting. arXiv preprint arXiv:2310.06625, 2023.
[39] Vitaly Kuznetsov Will Cukierski Maggie, Oren Anava. Web traffic time series forecasting, 2017.
[40] Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series is worth 64 words: Long-term forecasting with transformers. arXiv preprint arXiv:2211.14730, 2022.
[41] David Obst, Joseph De Vilmarest, and Yannig Goude. Adaptive methods for short-term electricity load forecasting during covid-19 lockdown in france. IEEE transactions on power systems, 36(5):4754-4763, 2021.
[42] Alex D Papalexopoulos and Timothy C Hesterberg. A regression-based approach to short-term system load forecasting. IEEE Transactions on power systems, 5(4):1535-1547, 1990.
[43] Joon Sung Park, Joseph OBrien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology, pages 1-22, 2023.
[44] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018.
[45] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.
[46] Kashif Rasul, Arjun Ashok, Andrew Robert Williams, Arian Khorasani, George Adamopoulos, Rishika Bhagwatkar, Marin Bilo, Hena Ghonia, Nadhir Vincent Hassen, Anderson Schneider, et al. Lag-llama: Towards foundation models for time series forecasting. arXiv preprint arXiv:2310.08278, 2023.
[47] Filipe Rodrigues, Ioulia Markou, and Francisco C Pereira. Combining time-series and textual data for taxi demand prediction in event areas: A deep learning approach. Information Fusion, 49:120-129, 2019.
[48] Robert P Schumaker and Hsinchun Chen. Textual analysis of stock market prediction using breaking financial news: The azfin text system. ACM Transactions on Information Systems (TOIS), 27(2):1-19, 2009.
[49] Robert P Schumaker and Hsinchun Chen. A discrete stock price prediction engine based on financial news. Computer, 43(1):51-56, 2010.</p>
<p>[50] Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic memory and self-reflection. arXiv preprint arXiv:2303.11366, 2023.
[51] Chenxi Sun, Yaliang Li, Hongyan Li, and Shenda Hong. Test: Text prototype aligned embedding to activate llm's ability for time series. arXiv preprint arXiv:2308.08241, 2023.
[52] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivire, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024.
[53] Jos F Torres, Dalil Hadjout, Abderrazak Sebaa, Francisco Martnez-lvarez, and Alicia Troncoso. Deep learning for time series forecasting: a survey. Big Data, 9(1):3-21, 2021.
[54] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Lacroix, Baptiste Rozire, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.
[55] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.
[56] Ruey S Tsay. Analysis of financial time series. John wiley \&amp; sons, 2005.
[57] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824-24837, 2022.
[58] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. Timesnet: Temporal 2d-variation modeling for general time series analysis. In The eleventh international conference on learning representations, 2022.
[59] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting. Advances in neural information processing systems, $34: 22419-22430,2021$.
[60] Min Xia, Haidong Shao, Xiandong Ma, and Clarence W de Silva. A stacked gru-rnn-based approach for predicting renewable energy and electricity load for smart grid operation. IEEE Transactions on Industrial Informatics, 17(10):7050-7059, 2021.
[61] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36, 2024.
[62] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022.
[63] Chin-Chia Michael Yeh, Xin Dai, Huiyuan Chen, Yan Zheng, Yujie Fan, Audrey Der, Vivian Lai, Zhongfang Zhuang, Junpeng Wang, Liang Wang, et al. Toward a foundation model for time series data. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management, pages 4400-4404, 2023.
[64] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time series forecasting? In Proceedings of the AAAI conference on artificial intelligence, volume 37, pages 11121-11128, 2023.
[65] Xiongwei Zhang, Hager Saleh, Eman MG Younis, Radhya Sahal, and Abdelmgeid A Ali. Predicting coronavirus pandemic in real-time using machine learning and big data streaming system. Complexity, 2020:1-10, 2020.
[66] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. arXiv preprint arXiv:2303.18223, 2023.
[67] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In Proceedings of the AAAI conference on artificial intelligence, volume 35, pages 11106-11115, 2021.
[68] Tian Zhou, Ziqing Ma, Qingsong Wen, Liang Sun, Tao Yao, Wotao Yin, Rong Jin, et al. Film: Frequency improved legendre memory model for long-term time series forecasting. Advances in Neural Information Processing Systems, 35:12677-12690, 2022.
[69] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting. In International conference on machine learning, pages 27268-27286. PMLR, 2022.
[70] Tian Zhou, Peisong Niu, Liang Sun, Rong Jin, et al. One fits all: Power general time series analysis by pretrained lm. Advances in neural information processing systems, 36:43322-43355, 2023.
[71] Tian Zhou, Peisong Niu, Liang Sun, Rong Jin, et al. One fits all: Power general time series analysis by pretrained lm. Advances in neural information processing systems, 36, 2024.</p>
<h1>A Appendix / Supplemental Material</h1>
<h2>A. 1 Experimental Setting / Details</h2>
<p>Table 4: Details of forecasting task designs and datasets. The electricity data represents the half-hourly state-level electricity demand. The exchange rate data represents the daily exchange rate of the Australian dollar. Traffic data denotes the hourly traffic volume in California. Bitcoin data denotes the daily Bitcoin price.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Datasets</th>
<th style="text-align: center;">Illness</th>
<th style="text-align: center;">Electicity</th>
<th style="text-align: center;">Exchange-Rate</th>
<th style="text-align: center;">Traffic</th>
<th style="text-align: center;">Bitcoin</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Time Horizon</td>
<td style="text-align: center;">Not specified</td>
<td style="text-align: center;">2019.01-2021.12</td>
<td style="text-align: center;">2018.01-2022.12</td>
<td style="text-align: center;">2015.01-2016.12</td>
<td style="text-align: center;">2019.01-2021.06</td>
</tr>
<tr>
<td style="text-align: center;">Variates</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">862</td>
<td style="text-align: center;">18</td>
</tr>
<tr>
<td style="text-align: center;">Timesteps</td>
<td style="text-align: center;">966</td>
<td style="text-align: center;">52,560</td>
<td style="text-align: center;">1,825</td>
<td style="text-align: center;">17,544</td>
<td style="text-align: center;">858</td>
</tr>
<tr>
<td style="text-align: center;">Granularity</td>
<td style="text-align: center;">1 week</td>
<td style="text-align: center;">30 minutes</td>
<td style="text-align: center;">1 day</td>
<td style="text-align: center;">1 hour</td>
<td style="text-align: center;">1 day</td>
</tr>
<tr>
<td style="text-align: center;">Input length</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">48</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">7</td>
</tr>
<tr>
<td style="text-align: center;">Prediction length</td>
<td style="text-align: center;">$[24,36,48,60]$</td>
<td style="text-align: center;">48</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">7</td>
</tr>
</tbody>
</table>
<p>Baselines. To ensure a thorough evaluation of our model, we selected a diverse set of baselines that represent the latest advancements in time series forecasting, covering a broad spectrum of empirical studies. Our selection includes transformer-based models, such as transformer-based model: Informer [67], Autoformer [59], FEDformer [69], Pyraformer [36], PatchTST [40], iTransformer [38], FiLM [68]. Additionally, we included the CNN-based Timesnet [58] and MLP-based Dlinear [64]. Finally, we incorporated the LLM-based GPT4TS[70], leveraging GPT2's generative capabilities. For Informer [67], Autoformer [59], FEDformer [69], Pyraformer [36], PatchTST [40], iTransformer [38], FiLM [68], and Timesnet [58], we use the code from the Time-Series-Library (https://github.com/thuml/Time-Series-Library/tree/main). For the GPT4TS method, we use the official code from https://github.com/DAMO-DI-ML/NeurIPS2023-One-Fits-All. When conducting comparative experiments, all the information utilized in our method, except for the news content, is also provided to the baseline methods. While our LLM fine-tuning approach maintains the original scale of the numbers to preserve the physical meaning of the time series data, we normalize the variables to the range $[0,1]$ for training the baseline methods. For non-numeric variables, we represent them using dummy variables for these baseline methods. To ensure optimal performance, we adhere to the official architecture settings for these methods and experiment with learning rates of $0.001,0.0005,0.0001$, and 0.00005 . Each configuration is tested three times with different initializations, and the best-performing setup is selected from all trials.</p>
<p>Our method. When fine-tuning, we employ the LoRa [19] method to fine-tune the LLama 2 large language model [55]. The rank in LoRa is set to either 8 or 16, depending on the required token length to be processed. The alpha value in LoRa is set to 16, and the learning rate is set at 0.0001 . When formatting numeric tokens, we uniformly maintain three significant figures. Retaining more significant figures offers minimal benefits and introduces an excessive number of tokens. The finetuning is conducted on a single NVIDIA A100 40G GPU, with the large language model undergoing several hundred to 1,000 training iterations on our curated data, requiring several hours.
Evaluation. We choose five metrics to compare the prediction performances of different forecasting models. Mean squared error (MSE) is used to verify outliers, focusing on larger prediction errors. Root Mean Square Error (RMSE) measures error magnitude by averaging squared errors and taking the square root, aligning the error scale with the original data. Mean absolute error (MAE) is more balanced for different magnitude errors. Mean Absolute Percentage Error (MAPE) expresses the error as a percentage of the actual values, that is, on average how far off the predictions are from the actual values in percentage terms. The accuracy rate is defined as the proportion of true results among the total number of cases examined. These metrics are calculated by:</p>
<p>$$
\begin{aligned}
\text { MSE } &amp; =\frac{1}{n} \sum_{i=1}^{n}\left(y_{i}-\hat{y}<em i="1">{i}\right)^{2} &amp; \operatorname{MAE} &amp; =\frac{1}{n} \sum</em>}^{n}\left|y_{i}-\hat{y<em i="1">{i}\right| \
\text { RMSE } &amp; =\sqrt{\frac{1}{n} \sum</em>}^{n}\left(y_{i}-\hat{y<em i="1">{i}\right)^{2}} &amp; \operatorname{MAPE} &amp; =\frac{100}{n} \sum</em>}^{n}\left|\frac{y_{i}-\hat{y<em i="i">{i}}{y</em>\right|
\end{aligned}
$$}</p>
<p>where $y_{i}$ represents the unnormalized true value of load, and $\hat{y}_{i}$ denotes the unnormalized result of the prediction model. $n$ denotes the number of prediction timestamps.</p>
<h1>A. 2 Example of Numerical Input for Fine-tuning LLM</h1>
<ol>
<li>Instruction: "...7015.7,6875.1,6634.6,6334.6,6134.7,6007.9,..."</li>
<li>Input: "NSW; 2019-11-9; Weekend; not a public holiday; 2019-11-10; Weekend; not a public holiday;286.5;297.96; 34.0;1012.0; 284.92; 301.04; 46.0; 1016.0."</li>
<li>Output: "...6592.6,6467.0,6312.3,6066.8,5902.9,5795.0..."</li>
</ol>
<h2>A. 3 Example of Textual Input for Fine-tuning LLM</h2>
<ol>
<li>Instruction: "The historical load data is: ...7015.7,6875.1,6634.6,6334.6,6134.7,6007.9,..."</li>
<li>Input: "Based on the historical load data, please predict the load consumption in the next day. The region for prediction is NSW. The start date of historical data was on 2019-11-9 that is Weekend, and it is not a public holiday. The data frequency is 30 minutes per point. Historical data covers 1 day. The date of prediction is on 2019-11-10 that is Weekend, and it is not a public holiday. Weather of the start date: the minimum temperature is 286.5; the maximum temperature is 297.96 ; the humidity is 34.0 ; the pressure is 1012.0 . Weather forecast of the prediction date: the minimum temperature is 284.92; the maximum temperature is 301.04 ; the humidity is 46.0 ; the pressure is 1016.0 . On 2019-11-09 08:51:00, the news can change the time series fluctuation that The ongoing fires lead to an immediate and direct effect on today's load consumption mostly due to loss of infrastructure, increased demand from firefighting efforts, and the need for emergency communications. On 2019-1109 20:20:00, the news can change the time series fluctuation that The devastating bushfires in NSW lead to increased short-term electricity consumption due to emergency services' operations, resident evacuations, and heightened communication needs. "</li>
<li>Output: "...6592.6,6467.0,6312.3,6066.8,5902.9,5795.0..."</li>
</ol>
<h2>A. 4 News Sources and Details</h2>
<p>We analyzed the classical time series forecasting datasets, such as the Monash-TSF dataset, to identify the specific time periods covered by the time series data. For the traffic domain, data from 2015 to 2016 was used. For the Bitcoin domain, data from 2019 to 2021 was utilized. Relevant news information was filtered from the GDELT dataset using domain-specific keywords corresponding to these periods. We conducted web crawling and intelligent parsing of the associated news web pages. The traffic data was sourced from Yahoo, while the Bitcoin data was gathered from various websites. Regular expressions were employed to extract essential information from the parsed text, including news titles, URLs, publication dates, and the main content of the articles. From the GDELT dataset, we initially filtered 14,543 traffic-related articles and 19,392 Bitcoin-related articles. After removing invalid and redirected links, we retained 5,867 traffic articles and 5,906 Bitcoin articles. The traffic-related news data primarily covered the region of California, USA. The extracted data was formatted into JSON files to facilitate seamless integration into our model training.</p>
<p>For the region-specific tasks, such as AU electricity demand and AU exchange rate, we collected news articles spanning from 2015 to 2023 primarily sourced from news.com.au. In total, we gathered 380,560 articles covering a diverse range of topics pertinent to electricity demand and exchange rates in Australia. The news articles were processed using a similar methodology as described for the traffic and Bitcoin domains. We performed web crawling and intelligent parsing to extract key information from each article, including titles, summaries, categories, URLs, publication dates, and the full article contents.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Word-Cloud of News Categories.</p>
<p>Regular expressions were utilized to ensure the accurate extraction of essential data points. The structured data was then formatted into JSON files, ensuring compatibility and ease of integration into our model training processes. By maintaining a consistent data collection and processing methodology across different domains, we ensured the reliability and utility of the gathered information.</p>
<p>Table 5: Statistical Information of Selected Event for Different Domains. In this table, we analyze the content and keywords of relevant news selected by the agent for training the model. We performed a word frequency analysis, with the second column showing the keywords and their frequencies in parentheses, and the third column representing the total number of selected news articles.</p>
<table>
<thead>
<tr>
<th>Domains</th>
<th>Keywords of Selected News and Their Word Frequency</th>
<th>Total Number of Selected News</th>
</tr>
</thead>
<tbody>
<tr>
<td>Electricity</td>
<td>emergency (339), weather (324), infrastructure (283), commercial events (181), residential (174), coronavirus (168), economic (160), heating (136), temperature (134), health (123), bushfires (115), construction (106), global (91), government (84), lockdown (76), traffic (71)</td>
<td>3,655</td>
</tr>
<tr>
<td>Exchange</td>
<td>economic (3033), international (2861), investor (1755), sentiment (1256), currency (1077), China (1058), trade (1049), USA (991), stability (886), geopolitical (786), financial (689), covid (652), health (440), political (328), energy (267), Trump (223), rba (186), tourism (172)</td>
<td>4,383</td>
</tr>
<tr>
<td>Traffic</td>
<td>California (1823), USA (1284), road (380), emergency (279), closures (275), commuting (233), police (233), disruptions (219), infrastructure (191), media (157), protests (149), congestion (138), incident (136), transportation (136), security (126), fire (124), wildfire (113), travel (91), shooting (85), vehicles (80), university (57), regulations (54)</td>
<td>2,109</td>
</tr>
<tr>
<td>Bitcoin</td>
<td>investor (1388), trading (557), sentiment (519), mining (449), global (262), interest rate (246), acceptance (240), Satoshi (212), security (209), economic (196), btc (177), nakaboto (150), institutional (140), Elon (132), technology (129), Ethereum (119), NVidia (114), geopolitical (108), bullish (100), legitimacy (97)</td>
<td>2,616</td>
</tr>
</tbody>
</table>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Random events are unpredictable and unplanned, such as natural disasters, accidents, health crises, and criminal acts. Normal events are planned or anticipated based on patterns, like political activities, sports \&amp; cultural events, economic reports, and public holidays. We used the LLM agent to categorize and detect all random and normal events from our raw news dataset spanning January 1st to August 5th, 2019. The analysis revealed that, on average, $27.7 \%$ of all events are random. Figure 7 and Figure 8 show the daily distribution of these random events. In our framework, the agent analyzes and selects the most relevant news, which mainly consists of five categories: economic or political events, health crises, natural disasters, technology development, and social sentiment. Additionally, the reflection agent, which analyzes prediction errors in the training dataset and missed news, helps identify unexpected and counterintuitive events buried in the raw news.</p>
<p>Table 6: We tested the effects of removing all or partial supplementary information, including Case 3, which includes only Filtered News, and Case 4, which includes Filtered News and Partial Supplementary Information. Removing all supplementary information and keeping only the news led to slight improvements compared to using only Supplementary Information in a numerical/textual prompt. However, this approach did not perform as well as Case 4. The results indicate that supplementary information remains essential. Overall, "Case 6: Textual Prompt with Filtered News and all Supplementary Information" which includes all supplementary information and filtered news, yields the best results.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Electricity</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">RMSE</td>
<td style="text-align: center;">$\mathrm{MSE}_{1-10^{-3}}$</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MAPE</td>
</tr>
<tr>
<td style="text-align: left;">Case 1: Only Numeric Prompt with all Supplementary Information</td>
<td style="text-align: center;">337.10</td>
<td style="text-align: center;">113.64</td>
<td style="text-align: center;">204.89</td>
<td style="text-align: center;">$5.27 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Case 2: Textual Prompt with all Supplementary Information (No News)</td>
<td style="text-align: center;">336.41</td>
<td style="text-align: center;">113.17</td>
<td style="text-align: center;">206.08</td>
<td style="text-align: center;">$5.29 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Case 3: Textual Prompt with Filtered News (No Supplementary Information)</td>
<td style="text-align: center;">330.79</td>
<td style="text-align: center;">109.42</td>
<td style="text-align: center;">201.33</td>
<td style="text-align: center;">$5.23 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Case 4: Textual Prompt with Filtered News and Partial Supplementary Information</td>
<td style="text-align: center;">310.29</td>
<td style="text-align: center;">96.28</td>
<td style="text-align: center;">192.98</td>
<td style="text-align: center;">$5.19 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Case 5: Textual Prompt with Non-Filtered News and all Supplementary Information</td>
<td style="text-align: center;">407.86</td>
<td style="text-align: center;">166.35</td>
<td style="text-align: center;">250.75</td>
<td style="text-align: center;">$6.84 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Case 6: Textual Prompt with Filtered News and all Supplementary Information</td>
<td style="text-align: center;">286.39</td>
<td style="text-align: center;">78.62</td>
<td style="text-align: center;">180.96</td>
<td style="text-align: center;">$5.15 \%$</td>
</tr>
</tbody>
</table>
<p>Table 7: We tried other language models using our proposed method. Mistral v0.1 [22], a 7B model, produced similar results as Llama 2 (7B). The Gemma 2B model [52] had slightly worse results, which may be due to its limited number of parameters. It is necessary to adjust the training for small models to achieve better results. Nonetheless, the results demonstrate the potential of language models to achieve good performance in our proposed methods.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Electricity</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">RMSE</td>
<td style="text-align: center;">$\mathrm{MSE}_{\times 10^{-3}}$</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MAPE</td>
</tr>
<tr>
<td style="text-align: left;">Llama 2 (7B)</td>
<td style="text-align: center;">280.39</td>
<td style="text-align: center;">78.62</td>
<td style="text-align: center;">180.96</td>
<td style="text-align: center;">$5.15 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Mistral v0.1 (7B)</td>
<td style="text-align: center;">291.66</td>
<td style="text-align: center;">85.07</td>
<td style="text-align: center;">206.08</td>
<td style="text-align: center;">$5.35 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Gemma 2 (2B)</td>
<td style="text-align: center;">350.08</td>
<td style="text-align: center;">122.56</td>
<td style="text-align: center;">216.72</td>
<td style="text-align: center;">$5.63 \%$</td>
</tr>
</tbody>
</table>
<h1>A. 5 More Results</h1>
<p>Figure 13, Figure 14, Figure 15, and Figure 16 provide showcases of some forecasting tasks results. It can be seen that our method has better results in predicting some sudden events and cases with distribution changes. We also tested the effectiveness of fine-tuned LLMs on traditional time series forecasting datasets. Given that long-term forecasting involves a significant number of digits and long sequences occupy many input tokens, we selected the Ill dataset for testing our method, which features prediction sequences of lengths $24,36,48$, and 60 . We also evaluated the performance of univariate forecasts and trained univariate versions of other baseline methods for comparison. The results, displayed in Table 9, show that, although counterintuitive, the fine-tuned LLMs can achieve performance comparable to baseline methods, providing a solid foundation for our research. Table 6 shows additional comparison experiments of different types of data incorporation, and Table 7 shows the comparison of using different pre-train language models.</p>
<h2>A. 6 Full Prompt Design</h2>
<h2>A.6.1 Prompt Example of Reselecting News through the Reasoning Agent:</h2>
<p>Prompt 1: The reasoning logic is """Predicting Australia's region-level electricity demand every 30 minutes involves various factors:</p>
<ol>
<li>Positive Issues Increasing Load:
(a) Short-Term:
i. Economic Growth: Boosts energy consumption.
ii. Tech Advancements: New technologies spike demand.
iii. Seasonal Factors: Extreme weather increases AC use.
iv. Social Events: Large events boost energy use.
(b) Long-Term:
i. Population Growth: Raises residential energy use.
ii. Industrial Development: Increases energy demand.
iii. Urbanization: Expanding cities raise energy use.
iv. Energy Transition: Shift to electric tech.</li>
<li>Negative Issues Decreasing Load:
(a) Short-Term:
i. Economic Downturns: Reduce industrial activity and energy use.
ii. Efficiency Improvements: Lower consumption.
iii. Weather Patterns: Mild weather reduces heating/cooling needs.
iv. Public Health Crises: Reduce industrial and commercial activity.
(b) Long-Term:
i. Energy Efficiency: Better insulation and appliances.
ii. Demographic Changes: Aging populations or lower birth rates.
iii. Policy and Regulation: Promote conservation and sustainability.
iv. Tech Innovations: More efficient technologies.</li>
<li>Other Factors:</li>
</ol>
<p>(a) Political Stability: Affects energy policies and investments.
(b) Global Market Dynamics: Affect local energy prices and use.
(c) Environmental Consciousness: Changes in behavior and renewable adoption. ""</p>
<p>Prompt 2: The prediction date is "2020-06-06". If I give you all news before the prediction, based on the above positive \&amp; negative effect analysis <current_reasoning_logic>:</p>
<ol>
<li>please choose all news that may have a long-term affect on future load consumption</li>
<li>please choose all news that may have a short-term effect on future load consumption.</li>
<li>please choose all news that may have a real-time direct effect on today's load consumption. if there is no suitable news, please say no.</li>
</ol>
<p>Also, please include the region (NSW/VIC/TSA/QLD/SA/WA) and the time information of this news. If there are multiple relevant news, please ensure that you include all relevant news. Remember to only give the JSON output, including all relevant news, and make it the valid JSON format.
Prompt 3: The news happened before the prediction include: <all_news>. The selected news is organized in JSON format. The Json output format is</p>
<div class="codehilite"><pre><span></span><code><span class="p">{</span>
<span class="w">    </span><span class="s2">&quot;Long-Term Effect on Future Electricity Demand&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">        </span><span class="s2">&quot;news&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Another major renewable energy project was initiated in</span>
<span class="w">            </span><span class="n">WA</span><span class="p">,</span><span class="w"> </span><span class="n">expected</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">supply</span><span class="w"> </span><span class="n">significant</span><span class="w"> </span><span class="n">power</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="mf">2022.</span><span class="s2">&quot;,</span>
<span class="w">        </span><span class="s2">&quot;region&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;WA&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s2">&quot;time&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;2019-03-15 11:30:00&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s2">&quot;rationality&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Long-term electricity load will be impacted by</span>
<span class="w">            </span><span class="n">the</span><span class="w"> </span><span class="n">integration</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">renewable</span><span class="w"> </span><span class="n">energy</span><span class="w"> </span><span class="n">sources</span><span class="p">,</span><span class="w"> </span><span class="n">which</span><span class="w"> </span><span class="n">are</span>
<span class="w">            </span><span class="n">expected</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">offset</span><span class="w"> </span><span class="n">dependence</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">traditional</span><span class="w"> </span><span class="n">fossil</span><span class="w"> </span><span class="n">fuels</span><span class="o">.</span><span class="s2">&quot;</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">],</span>
<span class="s2">&quot;Short-Term Effect on Future Electricity Demand&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">        </span><span class="s2">&quot;news&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;SA just sweltered through a very warm night, after a</span>
<span class="w">            </span><span class="n">day</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">extreme</span><span class="w"> </span><span class="n">heat</span><span class="w"> </span><span class="n">where</span><span class="w"> </span><span class="n">some</span><span class="w"> </span><span class="n">regional</span><span class="w"> </span><span class="n">areas</span><span class="w"> </span><span class="n">reached</span><span class="w"> </span><span class="n">nearly</span>
<span class="w">            </span><span class="mi">48</span><span class="n">C</span><span class="o">.</span><span class="s2">&quot;,</span>
<span class="w">        </span><span class="s2">&quot;region&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;SA&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s2">&quot;time&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;2019-01-03 17:57:00&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s2">&quot;rationality&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Extreme weather conditions, particularly the</span>
<span class="w">            </span><span class="n">intense</span><span class="w"> </span><span class="n">heat</span><span class="p">,</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">lead</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">higher</span><span class="w"> </span><span class="n">electricity</span><span class="w"> </span><span class="n">consumption</span><span class="w"> </span><span class="ow">in</span>
<span class="w">            </span><span class="n">the</span><span class="w"> </span><span class="n">short</span><span class="w"> </span><span class="n">term</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">residents</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">businesses</span><span class="w"> </span><span class="n">increase</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">use</span>
<span class="w">            </span><span class="n">of</span><span class="w"> </span><span class="n">air</span><span class="w"> </span><span class="n">conditioning</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">cooling</span><span class="w"> </span><span class="n">systems</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">manage</span>
<span class="w">            </span><span class="n">temperatures</span><span class="o">.</span><span class="s2">&quot;</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">        </span><span class="s2">&quot;news&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;A sudden cold snap in Victoria leads to a spike in</span>
<span class="w">            </span><span class="n">electric</span><span class="w"> </span><span class="n">heating</span><span class="w"> </span><span class="n">usage</span><span class="o">.</span><span class="s2">&quot;,</span>
<span class="w">        </span><span class="s2">&quot;region&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;VIC&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s2">&quot;time&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;2019-01-04 05:22:00&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s2">&quot;rationality&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Short-term electricity load spikes are often</span>
<span class="w">            </span><span class="n">caused</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="n">unexpected</span><span class="w"> </span><span class="n">weather</span><span class="w"> </span><span class="n">events</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">drive</span><span class="w"> </span><span class="n">up</span><span class="w"> </span><span class="n">heating</span><span class="w"> </span><span class="ow">or</span>
<span class="w">            </span><span class="n">cooling</span><span class="w"> </span><span class="n">demand</span><span class="o">.</span><span class="s2">&quot;</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">],</span>
<span class="s2">&quot;Real-Time Direct Effect on Today&#39;s Electricity Demand&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">        </span><span class="s2">&quot;news&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;An unseasonal downpour has wreaked havoc on Perth&#39;s</span>
<span class="w">            </span><span class="n">electricity</span><span class="w"> </span><span class="n">network</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">morning</span><span class="o">.</span><span class="s2">&quot;,</span>
<span class="w">        </span><span class="s2">&quot;region&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;WA&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s2">&quot;time&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;2019-01-03 10:11:00&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s2">&quot;rationality&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;The sudden weather event causing disruptions to</span>
<span class="w">            </span><span class="n">the</span><span class="w"> </span><span class="n">electricity</span><span class="w"> </span><span class="n">network</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">have</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">immediate</span><span class="w"> </span><span class="n">impact</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="nb">load</span>
<span class="w">            </span><span class="n">consumption</span><span class="w"> </span><span class="n">due</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">power</span><span class="w"> </span><span class="n">outages</span><span class="p">,</span><span class="w"> </span><span class="n">infrastructure</span><span class="w"> </span><span class="n">damage</span><span class="p">,</span><span class="w"> </span><span class="ow">or</span>
<span class="w">            </span><span class="n">emergency</span><span class="w"> </span><span class="n">response</span><span class="w"> </span><span class="n">measures</span><span class="o">.</span><span class="s2">&quot;</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="p">},</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">        </span><span class="s2">&quot;news&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Lightning strike at a major substation causes</span>
<span class="w">            </span><span class="n">widespread</span><span class="w"> </span><span class="n">outages</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">Sydney</span><span class="o">.</span><span class="s2">&quot;,</span>
<span class="w">        </span><span class="s2">&quot;region&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;NSW&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s2">&quot;time&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;2019-01-03 19:45:00&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s2">&quot;rationality&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Direct effects on load consumption include</span>
<span class="w">            </span><span class="n">sudden</span><span class="w"> </span><span class="n">drops</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">power</span><span class="w"> </span><span class="n">supply</span><span class="p">,</span><span class="w"> </span><span class="n">triggering</span><span class="w"> </span><span class="n">emergency</span><span class="w"> </span><span class="n">measures</span>
<span class="w">            </span><span class="n">to</span><span class="w"> </span><span class="n">restore</span><span class="w"> </span><span class="n">stability</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">network</span><span class="o">.</span><span class="s2">&quot;</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">]</span>
<span class="p">}</span>
</code></pre></div>

<h1>A.6.2 Prompt Example of Evaluating Predictions through the Evaluation Agent:</h1>
<p>Prompt 1: Based on historical data and relevant news from the last week, I have predicted the future exchange rate of Australia in the next day. The base is USD. I will provide you with our predicted values and actual values, as well as the news references. Please assess the accuracy of the predictions and analyze whether any important news has been overlooked.</p>
<p>Prompt 2: This is the background information: <background></p>
<h2>Examples of Background Information</h2>
<p>" The start date of historical data was 2021-1-3, which is a Weekend, and it is not a public holiday. The data frequency is 1 hour per point. Historical data covers 7 days. The Daily GDP of Australia during the last week was 568773,568773,568773,568773,568773,568773,568773. The Daily Unemployment rate (unit: \%) of Australia during the last week was 6.2556,6.2556,6.2556,6.2556,6.2556,6.2556,6.2556. The Daily Cash Rate Target (unit: \%) of Australia during the last week was $0.1,0.1,0.1,0.1,0.1,0.1,0.1$. The Daily GDP of the United States during the last week was 22600.2,22600.2,22600.2,22600.2,22600.2,22600.2,22600.2. The Daily Unemployment rate (unit: \%) of the United States during the last week was 6.4,6.4,6.4,6.4,6.4,6.4,6.4. The Daily Interest rate (unit: \%) of the United States during the last week was $0.1,0.1,0.1,0.1,0.1,0.1,0.1$. The date of prediction starts on 2021-1-10, which is the Weekend, and it is not a public holiday. The Daily GDP of Australia on the prediction date is 568773. The Daily Unemployment rate (unit: \%) of Australia on the prediction date is 6.2556. The Daily Cash Rate Target (unit: \%) of Australia on the prediction date is 0.1 . The Daily GDP of the United States on the prediction date is 22600.2. The Daily Unemployment rate (unit: \%) of the United States on the prediction date is 6.4. The Daily Interest rate (unit: $\%$ ) of the United States on the prediction date is: 0.1 ."</p>
<p>Prompt 3: This is the news we used for the prediction: <selected_news>; Here are all the news, including the day-ahead news and today's news in JSON format: <all_news>. Please determine whether there was any news that should have been considered in the prediction but was not included. The actual value is <actual_values>. Predicted values minus actual values are <errors>. The prediction date is <predictions_time>. According to the news and errors on this day, determine if any news has been missed. The output format should be: The missed news is xxx, occurred at xxxx, the possible reasoning is xxxx.</p>
<h1>Examples of Missed News Outputs</h1>
<p>Answer: Examining the provided news on 2021-01-09 for potentially unconsidered factors in the AUD/USD exchange rate prediction, one significant event stands out that could resonate with economic implications, specifically concerning Australia as a significant commodity exporter:</p>
<ul>
<li>Missed News Summary: "Brisbane's streets have been empty in eerie scenes as the city experiences its first day of lockdown to stop the spread of coronavirus."</li>
<li>Publication Time: 2021-01-09 12:59:00</li>
<li>Category: Breaking News</li>
<li>Possible Reasoning: This news shows a potential shift in economic activity due to new COVID-19 lockdowns in a major Australian city, which could affect the AUD due to changes in both domestic consumption and possibly a short-term impact on business conditions. Lockdowns can create short-term economic disruptions influencing investor sentiment and risk perception towards the AUD, potentially missed in the prediction model.
These items suggest an underestimation of contemporaneous socio-economic events that might indirectly influence the AUD/USD exchange rate through shifts in economic expectations and risk sentiment related to the management of COVID-19, which would have been highly relevant for predictions made for 2021-01-10.</li>
</ul>
<p>Prompt 4: According to <outlooked_news>, please directly conclude several new prediction logic of the daily AUD exchange rate. (Remember not to output the detailed implementation plan)</p>
<h2>Example Answers of News Reasoning Logic Updation</h2>
<h2>Answer:</h2>
<ol>
<li>Increased Risk Aversion: The lockdown in Brisbane may heighten risk aversion among investors, leading to a potential decline in the AUD as investors flock to safe-haven currencies like the USD. This can be anticipated as a prevalent reaction whenever there are renewed concerns about COVID-19 impacts.</li>
<li>Domestic Consumption Impact: With the Brisbane lockdown, domestic consumption could be considerably reduced. A decrease in consumer spending within Australia might result in lower economic output forecasts, potentially weakening the AUD.</li>
<li>Market Sentiment and Speculation: News of the lockdown and its impact on one of Australia's significant cities can drive market sentiments and speculative trades, which often have immediate effects on currency exchange rates. Negative news could prompt a bearish sentiment towards the AUD.</li>
<li>Impact on Commodity Prices: As a major commodity exporter, any perceived slowdown in Australia's economy due to lockdowns can affect global commodity prices, influencing the AUD. With a decrease in economic activities, demands for commodities might decline, causing a domino effect on the currency.</li>
<li>Investor's Perception of Economic Management: Prolonged or frequent lockdowns might affect international perceptions of Australia's management of economic challenges during the pandemic. This perception can influence foreign direct investment flows and currency strength.</li>
</ol>
<h2>A.6.3 Prompt Example of Refining the Final Logic</h2>
<p>Prompt 1: Improve and polish this paragraph to reduce repeated content and summarize the news selection logic that affects the Australian dollar exchange rate:<all_updated_logic>
Prompt 2: According to the given updated logic, please directly rephrase the current prediction logic and output the adjusted new logic. This is the current prediction logic that you need to adjust and improve:<selection_news_logic_current></p>
<h1>A. 7 Missed News Examples</h1>
<p>By analyzing the relationship between prediction errors and news through the agent, we can identify missing news (such as events with indirect impacts) and adjust the agent's expectations of how news events affect time series. For example, the agent might discover that a certain type of incident has a relatively small impact on a specific region. These intelligent and flexible discoveries are detailed in the missed news summary. Here are some examples of missed news during the iterative analysis:</p>
<ol>
<li>Missed News: " A coalition of 62 countries is backing Australia's call for an inquiry into the origins of the new coronavirus. It comes as Trade Minister Simon Birmingham's request for a meeting with China went unanswered." Occurred at: 2020-05-16 21:15:00. Possible reasoning: While the news about Australia pushing for an inquiry into COVID19's origins and facing unresponsive behavior from China was considered, the geopolitical tension this implies may have broader implications. This nature of international relations potentially has consequences for the AUD, particularly in terms of trade volumes and investor sentiment, given China's role as a major trading partner. Given the prediction results, this suggests that any immediate effect of this news on the exchange rate by the prediction date (2020-05-17) was minimal or offset by other factors considered in the model.</li>
<li>Missed News: "Saudi Arabia, the world's top oil exporter, aims to achieve net zero carbon emissions by 2060, as stated by Crown Prince Mohammed bin Salman." Occurred at 2021-10-25 11:21:20. Possible Reasoning: Although this did not happen in Australia, a deeper analysis of its impact on the AUD could enhance the prediction framework. This shift towards sustainable energy from a major oil producer could influence global oil prices and economic stability. As a significant exporter of commodities, Australia's economy and the AUD could be affected. The zero deviation between predicted and actual values suggests either accurate forecasting by other models or a delayed effect on exchange rates. Further analysis may be needed to understand the long-term impacts on currency values.</li>
<li>Missed news: " US Senator Bernie Sanders, one of the most vocal critics of Donald Trump, has launched his presidential bid to a crowd of thousands of supporters." Occurred at 2019-03-03 04:30:00. The possible reasoning is that political announcements in a country like the US can significantly impact global financial markets, including forex. Bernie Sanders's candidacy news could have influenced market speculation and investor sentiment, affecting USD values and other major currencies. This detection suggests that incorporating global political news from major economies like the US might enhance the model's predictive capabilities. Significant US political events, like presidential candidacies, can influence global markets and forex currency fluctuations. Thus, overlooking major political news can leave a model vulnerable to missing market reaction surprises.
(a) Australian Housing Market News: Missed News: "Despite economic uncertainty throughout 2020, Australians confidently embraced the housing market pushing sales volumes beyond even 2019 levels." Occurred at: "2021-01-17 10:00:00". Possible Reasoning: Although this news might seem domestically focused, the robust activity in the housing market can reflect economic confidence and potentially boost investor sentiment towards the AUD. Healthy real estate market metrics often attract foreign investment and can have favorable ripple effects on currency strength.
(b) Australian Government Seeking Pfizer Vaccine Information: Missed News: "The Australian government is seeking 'immediate' advice and information after Norway reported 29 deaths related to the Pfizer vaccine." Occurred at: "2021-01-17 10:10:00". Possible Reasoning: News related to vaccine concerns could create shortterm caution among investors, especially in contexts where the vaccine rollout impacts economic recovery prospects. If international observers view the vaccine issues as a slowdown to Australia's reopening or economic normalization, it could temper enthusiasm for the AUD.</li>
</ol>
<h2>A. 8 Selected News by Agents</h2>
<p>The news is finally filtered out by the agent based on the prediction task, and the rationale is explained. As shown in Table8, each forecasting domain has a corresponding example of selected news.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Junhua Zhao and Jinjin Gu are the corresponding authors.
${ }^{1}$ Code and data are available at https://github.com/ameliawong1996/From_News_to_Forecast.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
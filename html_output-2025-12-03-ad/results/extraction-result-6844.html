<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6844 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6844</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6844</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-132.html">extraction-schema-132</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <p><strong>Paper ID:</strong> paper-276116353</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.02810v2.pdf" target="_blank">Mol-LLM: Multimodal Generalist Molecular LLM with Improved Graph Utilization</a></p>
                <p><strong>Paper Abstract:</strong> Recent advances in large language models (LLMs) have led to models that tackle diverse molecular tasks, such as chemical reaction prediction and molecular property prediction. Large-scale molecular instruction-tuning datasets have enabled sequence-only (e.g., SMILES or SELFIES) generalist molecular LLMs, and researchers are now exploring multimodal approaches that incorporate molecular structural information for further gains. However, a genuinely multimodal, generalist LLM that covers a broad spectrum of molecular tasks has yet to be fully investigated. We observe that naive next token prediction training ignores graph-structural information, limiting an LLM's ability to exploit molecular graphs. To address this, we propose (i) Molecular structure Preference Optimization (MolPO), which facilitates graph usage by optimizing preferences between pairs of correct and perturbed molecular structures, and (ii) an advanced graph encoder with a tailored pre-training strategy to improve the effect of graph utilization by MolPO. Building on these contributions, we introduce Mol-LLM, the first multimodal generalist model that (a) handles a broad spectrum of molecular tasks among molecular LLMs, (b) explicitly leverages molecular-structure information, and (c) takes advantage of extensive instruction tuning. Mol-LLM attains state-of-the-art or comparable results across the most comprehensive molecular-LLM benchmark-even on out-of-distribution datasets for reaction and property prediction, where it surpasses prior generalist molecular LLMs by a large margin.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6844.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6844.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mol-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mol-LLM: Multimodal Generalist Molecular Large Language Model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal, instruction‑tuned generalist molecular LLM that fuses a decoder-only backbone LLM (Mistral-7B-Instruct) with a hybrid graph encoder (GINE + TokenGT) and a Q‑Former to generate and predict molecular outputs (SELFIES + 2D graphs) for tasks including description-guided molecule generation, reaction prediction, and property prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B-Instruct-v0.3 (backbone) with Mol-LLM multimodal adapters</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only instruction‑tuned LLM with multimodal adapters (Q‑Former) and hybrid GNN encoder; fine‑tuned (LoRA) for molecular tasks</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈7B (Mistral-7B backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>3.3M instruction‑tuning examples aggregated from molecule-focused corpora (Mol-Instructions, SMolInstruct, USPTO reaction subsets, ChEBI-20, PubChem name/SELFIES pairs and other public molecular datasets); separate GNN pretraining used ~5M molecules sampled from PubChem for functional-group prediction and SELFIES reconstruction.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Direct autoregressive generation of SELFIES sequences by the LLM (decoder) conditioned on natural language instruction plus 32 Q‑Former query tokens produced from concatenated graph embeddings; fine‑tuned with supervised fine-tuning (SFT) and preference optimization (MolPO) over chosen vs perturbed graphs; generation uses beam search / standard LLM decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SELFIES for 1D string representation (model vocabulary extended with 3k SELFIES tokens + numeric tokens); 2D molecular graphs encoded via hybrid GNN (GINE and transformer-based TokenGT) producing node/edge/graph embeddings fed through a Q‑Former.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>General molecular tasks including description-guided molecule generation (drug-like molecule design), reaction prediction (forward and retrosynthesis), reagent prediction, property regression/classification (e.g., LogS, LogD, HOMO/LUMO), and molecule captioning — broadly applicable to drug discovery and materials discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>SELFIES representation to improve syntactic validity; generation evaluated/filtered by RDKit validity checks; during training MolPO uses MACCS-key based substructure perturbations to construct rejected examples; MolPO applies margin clipping (λ_clip = 1.0) and a task-adaptive reward margin (λ_margin = 0.5) with training scaling factor c = 0.25 to balance SFT and preference loss.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Extensive use of RDKit (fragments/functional group enumeration, fingerprint computation, validity checks, InChI deduplication); datasets from USPTO and ORDerly; fingerprint similarity metrics computed with RDKit; GNN pretraining and graph processing rely on standard graph ML toolchains.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Instruction-tuning: Mol-Instructions, SMolInstruct, USPTO subsets, ChEBI-20, PubChem name/SELFIES pairs (total ≈3.3M); GNN pretraining: 5M molecules sampled from PubChem for functional group prediction and SELFIES reconstruction; evaluation sets: MoleculeNet tasks, AqSol (OOD LogS), ORDerly (OOD reactions), QM9 (GNN probe), and held-out subsets from the above.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Generation: EXACT match, BLEU-4, fingerprint similarities (RDKit FTS, MACCS FTS, Morgan FTS), VALIDITY (% valid molecules); Reaction prediction: EXACT, MACCS FTS; Property: RMSE or MAE for regression, ROC-AUC for classification; Captioning: BLEU-2/4, ROUGE, METEOR; Graph utilization: Graph Discrimination Ratio (GDR).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Description-guided molecule generation (ChEBI-20): EXACT 0.443, MACCS FTS 0.906, VALIDITY 1.00, BLEU-4 0.493, ROUGE-L 0.439, METEOR 0.599. SMolInstruct generation: EXACT 0.368, MACCS FTS 0.887, VALIDITY 0.99, BLEU-4 0.482, ROUGE-L 0.433, METEOR 0.589. Reaction prediction (Mol-Instructions / SMolInstruct): Forward synthesis EXACT 0.911 / 0.601 and MACCS FTS 0.987 / 0.908; Retrosynthesis and Reagent prediction also show state-of-the-art or comparable gains vs other generalist models. OOD performance: AqSol (LogS) RMSE = 1.02 (Mol-LLM) vs 1.32 (LlaSMol baseline); ORDerly (OOD reactions) forward synthesis EXACT ≈0.401, MACCS FTS ≈0.877, VALIDITY 1.00; Graph utilization (GDR) substantially improved with MolPO (GDR values higher across tasks, reported graph discrimination plots).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Reported limitations include 'graph bypass' (naive SFT ignores graph modality), sensitivity of MolPO to limited/diverse training distributions (MolPO can degrade performance on small/Narrow classification datasets), need for more diverse property-classification datasets, lack of multi-step reasoning and multi-turn interaction capabilities necessary for real-world workflows, computational cost (training required multiple A100 GPUs and many days), and cases where existing pre-trained GNNs (MoleculeSTM, GraphCL) do not improve downstream when integrated naively.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mol-LLM: Multimodal Generalist Molecular LLM with Improved Graph Utilization', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6844.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6844.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MolPO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Molecular structure Preference Optimization (MolPO)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal preference-optimization fine-tuning objective that trains the LLM to prefer the correct (chosen) molecular graph over perturbed (rejected) graphs by contrasting log-likelihood rewards, thereby encouraging actual use of 2D graph structure during generation and prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mol-LLM (Mistral-7B backbone) trained with MolPO objective</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>training objective / fine-tuning method (preference optimization) applied to multimodal LLM + graph encoders</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>applies to models built on Mistral-7B (≈7B) in this work</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Uses the same multimodal instruction-tuning dataset as Mol-LLM (3.3M examples) but augments each instance by constructing perturbed graph variants (rejected graphs) via MACCS-key-based substructure removal/addition from the input molecule; expectation of rewards estimated with exponential moving average during training.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Not a standalone generator — MolPO modifies fine-tuning: combines SFT cross-entropy loss with an auxiliary preference loss L_MolPO = E[-log σ(min(r_w - r_l, λ_clip |r_w|) - γ_i)], where r_w and r_l are scaled log-likelihood rewards for chosen and rejected graphs; margin clipping and task-adaptive reward margins stabilize optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>Operates on multimodal inputs (SELFIES strings + 2D molecular graphs encoded by hybrid GNNs); perturbations created by MACCS-key substructure modifications and re-attachment of sampled absent MACCS-key functional groups.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Improves generation and prediction robustness and graph-utilization for downstream tasks: description-guided molecule generation, reaction prediction (forward/retro), reagent prediction, and property prediction — supporting applications in drug/material discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Perturbations are constrained to functional-group/substructure edits determined by MACCS keys; number of MACCS keys modified set to ~30% of a molecule's present MACCS keys for perturbed graphs; preference margin clipped by λ_clip (1.0) and task-adaptive margin scaled by λ_margin (0.5); scaling factor c = 0.25 used to combine SFT and MolPO losses.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Uses RDKit to enumerate MACCS keys / functional groups and to construct perturbed molecules; training relies on same multimodal pipeline (GNN encoders + Q-Former + LLM).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Applied during fine-tuning on the 3.3M molecular instruction-tuning dataset (Mol-Instructions, SMolInstruct, USPTO subsets, ChEBI, PubChem pairs). Perturbed graphs are derived from the molecules present in these datasets using MACCS-key modifications.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Graph Discrimination Ratio (GDR) — proportion where log-likelihood for chosen graph > rejected graph; downstream task metrics (RMSE/MAE, ROC-AUC, EXACT, fingerprint similarities, VALIDITY, BLEU, ROUGE, METEOR) to measure impact on generation/prediction performance; ablation comparisons vs SFT-only.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>MolPO improves GDR across tasks (visualized in Figure 1), leading to consistent downstream gains: e.g., ablation Table 5 shows Mol-LLM (w/o MolPO) vs Mol-LLM — small but consistent improvements across tasks (example: FS Mol-Instructions EXACT increases from 0.907 to 0.911; several other task metrics likewise improved). MolPO also improves OOD generalization (AqSol and ORDerly) relative to SFT-only training.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>MolPO can degrade performance when training distributions are narrow or small (notably for some property-classification datasets with ≈1k samples), because preference pairs may bias the model toward distributional artifacts; requires careful scaling/clipping hyperparameters to avoid trivial solutions (simply reducing rejected reward); needs diverse molecular distributions to be effective; not a replacement for multi-step reasoning or experimental wet-lab validation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mol-LLM: Multimodal Generalist Molecular LLM with Improved Graph Utilization', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>LlaSMol: Advancing large language models for chemistry with a large-scale, comprehensive, high-quality instruction tuning dataset <em>(Rating: 2)</em></li>
                <li>InstructMol: Multi-modal integration for building a versatile and reliable molecular assistant in drug discovery <em>(Rating: 2)</em></li>
                <li>Mol-Instructions: A large-scale biomolecular instruction dataset for large language models <em>(Rating: 2)</em></li>
                <li>MolCA: Molecular graph-language modeling with cross-modal projector and uni-modal adapter <em>(Rating: 1)</em></li>
                <li>GIT-Mol: A multi-modal large language model for molecular science with graph, image, and text <em>(Rating: 1)</em></li>
                <li>BioT5+: Towards generalized biological understanding with iupac integration and multi-task tuning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6844",
    "paper_id": "paper-276116353",
    "extraction_schema_id": "extraction-schema-132",
    "extracted_data": [
        {
            "name_short": "Mol-LLM",
            "name_full": "Mol-LLM: Multimodal Generalist Molecular Large Language Model",
            "brief_description": "A multimodal, instruction‑tuned generalist molecular LLM that fuses a decoder-only backbone LLM (Mistral-7B-Instruct) with a hybrid graph encoder (GINE + TokenGT) and a Q‑Former to generate and predict molecular outputs (SELFIES + 2D graphs) for tasks including description-guided molecule generation, reaction prediction, and property prediction.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Mistral-7B-Instruct-v0.3 (backbone) with Mol-LLM multimodal adapters",
            "model_type": "decoder-only instruction‑tuned LLM with multimodal adapters (Q‑Former) and hybrid GNN encoder; fine‑tuned (LoRA) for molecular tasks",
            "model_size": "≈7B (Mistral-7B backbone)",
            "training_data_description": "3.3M instruction‑tuning examples aggregated from molecule-focused corpora (Mol-Instructions, SMolInstruct, USPTO reaction subsets, ChEBI-20, PubChem name/SELFIES pairs and other public molecular datasets); separate GNN pretraining used ~5M molecules sampled from PubChem for functional-group prediction and SELFIES reconstruction.",
            "generation_method": "Direct autoregressive generation of SELFIES sequences by the LLM (decoder) conditioned on natural language instruction plus 32 Q‑Former query tokens produced from concatenated graph embeddings; fine‑tuned with supervised fine-tuning (SFT) and preference optimization (MolPO) over chosen vs perturbed graphs; generation uses beam search / standard LLM decoding.",
            "chemical_representation": "SELFIES for 1D string representation (model vocabulary extended with 3k SELFIES tokens + numeric tokens); 2D molecular graphs encoded via hybrid GNN (GINE and transformer-based TokenGT) producing node/edge/graph embeddings fed through a Q‑Former.",
            "target_application": "General molecular tasks including description-guided molecule generation (drug-like molecule design), reaction prediction (forward and retrosynthesis), reagent prediction, property regression/classification (e.g., LogS, LogD, HOMO/LUMO), and molecule captioning — broadly applicable to drug discovery and materials discovery.",
            "constraints_used": "SELFIES representation to improve syntactic validity; generation evaluated/filtered by RDKit validity checks; during training MolPO uses MACCS-key based substructure perturbations to construct rejected examples; MolPO applies margin clipping (λ_clip = 1.0) and a task-adaptive reward margin (λ_margin = 0.5) with training scaling factor c = 0.25 to balance SFT and preference loss.",
            "integration_with_external_tools": "Extensive use of RDKit (fragments/functional group enumeration, fingerprint computation, validity checks, InChI deduplication); datasets from USPTO and ORDerly; fingerprint similarity metrics computed with RDKit; GNN pretraining and graph processing rely on standard graph ML toolchains.",
            "dataset_used": "Instruction-tuning: Mol-Instructions, SMolInstruct, USPTO subsets, ChEBI-20, PubChem name/SELFIES pairs (total ≈3.3M); GNN pretraining: 5M molecules sampled from PubChem for functional group prediction and SELFIES reconstruction; evaluation sets: MoleculeNet tasks, AqSol (OOD LogS), ORDerly (OOD reactions), QM9 (GNN probe), and held-out subsets from the above.",
            "evaluation_metrics": "Generation: EXACT match, BLEU-4, fingerprint similarities (RDKit FTS, MACCS FTS, Morgan FTS), VALIDITY (% valid molecules); Reaction prediction: EXACT, MACCS FTS; Property: RMSE or MAE for regression, ROC-AUC for classification; Captioning: BLEU-2/4, ROUGE, METEOR; Graph utilization: Graph Discrimination Ratio (GDR).",
            "reported_results": "Description-guided molecule generation (ChEBI-20): EXACT 0.443, MACCS FTS 0.906, VALIDITY 1.00, BLEU-4 0.493, ROUGE-L 0.439, METEOR 0.599. SMolInstruct generation: EXACT 0.368, MACCS FTS 0.887, VALIDITY 0.99, BLEU-4 0.482, ROUGE-L 0.433, METEOR 0.589. Reaction prediction (Mol-Instructions / SMolInstruct): Forward synthesis EXACT 0.911 / 0.601 and MACCS FTS 0.987 / 0.908; Retrosynthesis and Reagent prediction also show state-of-the-art or comparable gains vs other generalist models. OOD performance: AqSol (LogS) RMSE = 1.02 (Mol-LLM) vs 1.32 (LlaSMol baseline); ORDerly (OOD reactions) forward synthesis EXACT ≈0.401, MACCS FTS ≈0.877, VALIDITY 1.00; Graph utilization (GDR) substantially improved with MolPO (GDR values higher across tasks, reported graph discrimination plots).",
            "experimental_validation": false,
            "challenges_or_limitations": "Reported limitations include 'graph bypass' (naive SFT ignores graph modality), sensitivity of MolPO to limited/diverse training distributions (MolPO can degrade performance on small/Narrow classification datasets), need for more diverse property-classification datasets, lack of multi-step reasoning and multi-turn interaction capabilities necessary for real-world workflows, computational cost (training required multiple A100 GPUs and many days), and cases where existing pre-trained GNNs (MoleculeSTM, GraphCL) do not improve downstream when integrated naively.",
            "uuid": "e6844.0",
            "source_info": {
                "paper_title": "Mol-LLM: Multimodal Generalist Molecular LLM with Improved Graph Utilization",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "MolPO",
            "name_full": "Molecular structure Preference Optimization (MolPO)",
            "brief_description": "A multimodal preference-optimization fine-tuning objective that trains the LLM to prefer the correct (chosen) molecular graph over perturbed (rejected) graphs by contrasting log-likelihood rewards, thereby encouraging actual use of 2D graph structure during generation and prediction.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Mol-LLM (Mistral-7B backbone) trained with MolPO objective",
            "model_type": "training objective / fine-tuning method (preference optimization) applied to multimodal LLM + graph encoders",
            "model_size": "applies to models built on Mistral-7B (≈7B) in this work",
            "training_data_description": "Uses the same multimodal instruction-tuning dataset as Mol-LLM (3.3M examples) but augments each instance by constructing perturbed graph variants (rejected graphs) via MACCS-key-based substructure removal/addition from the input molecule; expectation of rewards estimated with exponential moving average during training.",
            "generation_method": "Not a standalone generator — MolPO modifies fine-tuning: combines SFT cross-entropy loss with an auxiliary preference loss L_MolPO = E[-log σ(min(r_w - r_l, λ_clip |r_w|) - γ_i)], where r_w and r_l are scaled log-likelihood rewards for chosen and rejected graphs; margin clipping and task-adaptive reward margins stabilize optimization.",
            "chemical_representation": "Operates on multimodal inputs (SELFIES strings + 2D molecular graphs encoded by hybrid GNNs); perturbations created by MACCS-key substructure modifications and re-attachment of sampled absent MACCS-key functional groups.",
            "target_application": "Improves generation and prediction robustness and graph-utilization for downstream tasks: description-guided molecule generation, reaction prediction (forward/retro), reagent prediction, and property prediction — supporting applications in drug/material discovery.",
            "constraints_used": "Perturbations are constrained to functional-group/substructure edits determined by MACCS keys; number of MACCS keys modified set to ~30% of a molecule's present MACCS keys for perturbed graphs; preference margin clipped by λ_clip (1.0) and task-adaptive margin scaled by λ_margin (0.5); scaling factor c = 0.25 used to combine SFT and MolPO losses.",
            "integration_with_external_tools": "Uses RDKit to enumerate MACCS keys / functional groups and to construct perturbed molecules; training relies on same multimodal pipeline (GNN encoders + Q-Former + LLM).",
            "dataset_used": "Applied during fine-tuning on the 3.3M molecular instruction-tuning dataset (Mol-Instructions, SMolInstruct, USPTO subsets, ChEBI, PubChem pairs). Perturbed graphs are derived from the molecules present in these datasets using MACCS-key modifications.",
            "evaluation_metrics": "Graph Discrimination Ratio (GDR) — proportion where log-likelihood for chosen graph &gt; rejected graph; downstream task metrics (RMSE/MAE, ROC-AUC, EXACT, fingerprint similarities, VALIDITY, BLEU, ROUGE, METEOR) to measure impact on generation/prediction performance; ablation comparisons vs SFT-only.",
            "reported_results": "MolPO improves GDR across tasks (visualized in Figure 1), leading to consistent downstream gains: e.g., ablation Table 5 shows Mol-LLM (w/o MolPO) vs Mol-LLM — small but consistent improvements across tasks (example: FS Mol-Instructions EXACT increases from 0.907 to 0.911; several other task metrics likewise improved). MolPO also improves OOD generalization (AqSol and ORDerly) relative to SFT-only training.",
            "experimental_validation": false,
            "challenges_or_limitations": "MolPO can degrade performance when training distributions are narrow or small (notably for some property-classification datasets with ≈1k samples), because preference pairs may bias the model toward distributional artifacts; requires careful scaling/clipping hyperparameters to avoid trivial solutions (simply reducing rejected reward); needs diverse molecular distributions to be effective; not a replacement for multi-step reasoning or experimental wet-lab validation.",
            "uuid": "e6844.1",
            "source_info": {
                "paper_title": "Mol-LLM: Multimodal Generalist Molecular LLM with Improved Graph Utilization",
                "publication_date_yy_mm": "2025-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "LlaSMol: Advancing large language models for chemistry with a large-scale, comprehensive, high-quality instruction tuning dataset",
            "rating": 2,
            "sanitized_title": "llasmol_advancing_large_language_models_for_chemistry_with_a_largescale_comprehensive_highquality_instruction_tuning_dataset"
        },
        {
            "paper_title": "InstructMol: Multi-modal integration for building a versatile and reliable molecular assistant in drug discovery",
            "rating": 2,
            "sanitized_title": "instructmol_multimodal_integration_for_building_a_versatile_and_reliable_molecular_assistant_in_drug_discovery"
        },
        {
            "paper_title": "Mol-Instructions: A large-scale biomolecular instruction dataset for large language models",
            "rating": 2,
            "sanitized_title": "molinstructions_a_largescale_biomolecular_instruction_dataset_for_large_language_models"
        },
        {
            "paper_title": "MolCA: Molecular graph-language modeling with cross-modal projector and uni-modal adapter",
            "rating": 1,
            "sanitized_title": "molca_molecular_graphlanguage_modeling_with_crossmodal_projector_and_unimodal_adapter"
        },
        {
            "paper_title": "GIT-Mol: A multi-modal large language model for molecular science with graph, image, and text",
            "rating": 1,
            "sanitized_title": "gitmol_a_multimodal_large_language_model_for_molecular_science_with_graph_image_and_text"
        },
        {
            "paper_title": "BioT5+: Towards generalized biological understanding with iupac integration and multi-task tuning",
            "rating": 1,
            "sanitized_title": "biot5_towards_generalized_biological_understanding_with_iupac_integration_and_multitask_tuning"
        }
    ],
    "cost": 0.0159725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Mol-LLM: Multimodal Generalist Molecular LLM with Improved Graph Utilization
26 May 2025</p>
<p>Chanhui Lee 
Department of Artificial Intelligence
Korea University</p>
<p>Hanbum Ko 
Department of Artificial Intelligence
Korea University</p>
<p>Yuheon Song 
Department of Artificial Intelligence
UNIST</p>
<p>Yongjun Jeong 
Department of Artificial Intelligence
Korea University</p>
<p>Rodrigo Hormazabal 
Kim Jaechul Graduate School of AI
KAIST 4 LG AI Research</p>
<p>Sehui Han 
Kyunghoon Bae 
Sungbin Lim 
Department of Statistics
Korea University</p>
<p>Sungwoong Kim 
Department of Artificial Intelligence
Korea University</p>
<p>Learnable Queries </p>
<p>… LLM Input Embeddings</p>
<p>Mol-LLM: Multimodal Generalist Molecular LLM with Improved Graph Utilization
26 May 20259C6B2BB96AF394B32D63BDEDA6478C64arXiv:2502.02810v2[cs.LG]
Recent advances in large language models (LLMs) have led to models that tackle diverse molecular tasks, such as chemical reaction prediction and molecular property prediction.Large-scale molecular instruction-tuning datasets have enabled sequence-only (e.g., SMILES or SELFIES) generalist molecular LLMs, and researchers are now exploring multimodal approaches that incorporate molecular structural information for further gains.However, a genuinely multimodal, generalist LLM that covers a broad spectrum of molecular tasks has yet to be fully investigated.We observe that naive next token prediction training ignores graph-structural information, limiting an LLM's ability to exploit molecular graphs.To address this, we propose (i) Molecular structure Preference Optimization (MolPO), which facilitates graph usage by optimizing preferences between pairs of correct and perturbed molecular structures, and (ii) an advanced graph encoder with a tailored pre-training strategy to improve the effect of graph utilization by MolPO.Building on these contributions, we introduce Mol-LLM, the first multimodal generalist model that (a) handles a broad spectrum of molecular tasks among molecular LLMs, (b) explicitly leverages molecular-structure information, and (c) takes advantage of extensive instruction tuning.Mol-LLM attains state-of-the-art or comparable results across the most comprehensive molecular-LLM benchmark-even on outof-distribution datasets for reaction and property prediction, where it surpasses prior generalist molecular LLMs by a large margin. 2* Corresponding Authors.{sungbin, swkim01}@korea.ac.kr. 2 The model, code, and data will be publicly available.Preprint.Under review.</p>
<p>Introduction</p>
<p>Large language models (LLMs) [1][2][3][4] have been widely used to tackle diverse tasks across multiple domains, such as mathematics and code generation, by leveraging their broad knowledge base.This achievement has recently motivated interest in applying LLMs to diverse molecular tasks-including molecular property prediction, chemical reaction prediction, description-guided molecule generation, and molecule captioning-all of which are essential in drug discovery and materials science [5][6][7][8][9][10][11][12].In particular, most molecular LLMs tend to leverage only one of the two key components for improved molecular language modeling, either molecular structure information or multitask instruction-tuning, rather than combining both.Several studies [8,10,11] have moved away from conventional molecular language modeling based on 1D sequence such as SMILES [13] or SELFIES [14], and have instead developed multimodal LLMs that incorporate 2D molecular graphs as an additional input modality, thereby representing molecular structures and topologies more faithfully while achieving better performance across diverse molecular tasks [15][16][17].Meanwhile, other studies [5][6][7]9] have constructed instruction-tuning datasets for multiple molecular tasks and fine-tuned LLMs on these datasets.This approach enables the models to acquire transferable and generalizable knowledge, allowing them to understand and perform various tasks based on natural language instructions.However, it is uncertain whether the multimodal molecular LLMs effectively use molecular structural information when trained with naive supervised fine-tuning (SFT).To investigate this, we compare the likelihoods of the original and perturbed molecules, comparing how well the SFT model is at proper graph discrimination.Figure 1 shows that the SFT model hardly distinguishes between them on most molecular tasks, indicating that its molecular graph utilization is generally limited.Moreover, despite the potential for synergistic performance improvements by molecular graph structure utilization and multitask instruction-tuning, few studies have fully harnessed the benefits of both approaches, especially for a universal molecular LLM.Specifically, some recent studies [9,11,12,18,19] have attempted to combine molecule graph structure information with instruction-tuning, however, their instruction-tuning focuses solely on task-specific fine-tuning.</p>
<p>In this paper, we propose a generalist molecular LLM, called Mol-LLM, that leverages multimodal molecule and extensive instruction-tuning, addressing the broadest range of molecular tasks.In particular, while maintaining multimodal LLM architecture based on Q-Former [20], we introduce a novel multimodal instruction-tuning based on Molecular structure Preference Optimization (MolPO), where the molecular LLM learns to optimize the molecular structural preferences between the pairs of the correct (chosen) molecular graph and the perturbed (rejected) molecular graph.By creating rejected molecular graphs based on the substructures for molecular feature perturbation, the proposed MolPO mitigates the tendency to overlook graph information on various molecular tasks.Additionally, to further increase the effect of molecular graph utilization by advanced representation on a wide variety of molecular distributions, we introduce a new graph neural network (GNN) pre-training strategy and architecture.The proposed GNN pre-training framework combines two objectives: (i) functional group prediction, which teaches the model to accurately distinguish functional groups-the features that largely determine molecular properties-and (ii) SELFIES reconstruction, which helps the model preserve the molecular structure details from the molecular graph.Upon GINE [21], adopted by prior multimodal molecular LLMs [8,9], we incorporate a transformer-based GNN named TokenGT [22], to enhance the expressive power.The resulting Mol-LLM shows strong performance and demonstrably better graph utilization on our benchmarks across a broad range of molecular tasks.To the best of our knowledge, Mol-LLM is not only the first versatile generalist multimodal molecular LLM on a wide range tasks with a single generalist model, but it also surpasses other generalist models: LlaSMol [5], ChemDFM [23], 3D-MoLM [12] on most bencharks as shown in Figure 1, highlighting the power of graph modality synergized with extensive instruction-tuning.</p>
<p>In summary, our contributions are:</p>
<ol>
<li>
<p>Mol-LLM.We present Mol-LLM, which sets a new state-of-the-art on both in-distribution and out-of-distribution molecular benchmarks relative to existing generalist models.2. Enhanced graph utilization.To exploit 2D molecular graphs more effectively, we propose MolPO-a fine-tuning strategy that leverages perturbed molecules-alongside a GNN pre-training method and a hybrid graph encoder augmented with transformer architecture.</p>
</li>
<li>
<p>Extensive instruction-tuning.We construct a large, molecule-focused instruction-tuning dataset and employ multimodal training to build a generalist model with significantly enhanced molecular understanding.</p>
</li>
</ol>
<p>Mol-LLM: Multimodal Generalist Molecular Large Language Model</p>
<p>This section introduces the model architecture, training strategy, and instruction-tuning dataset of Mol-LLM, a multimodal generalist molecular LLM.As depicted in Figure 2, Mol-LLM comprises a hybrid molecular graph encoder, a Q-Former for cross-modal projection between molecular graph and text, and a backbone LLM.Utilizing the multimodal framework, the LLM addresses molecular task instructions and 1D molecular sequences directly, while feeding 2D molecular graph embeddings to the LLM through the hybrid graph encoder and Q-Former.Such multimodal architectures are trained through three training stages, as depicted in Figure 3.</p>
<p>Model Architecture</p>
<p>Hybrid Graph Encoder Previous studies on multimodal molecular LLMs using 2D molecular graphs [8,9] have adopted the GINE architecture [21], since it captures local graphical structure efficiently.However, addressing diverse molecular tasks across various data distributions requires the ability to process large molecules as well.This consideration led us to the simultaneous usage of TokenGT [22] as a graph encoder, which is designed to enhance global context understanding and mitigate over-smoothing [24] in large graphs via a transformer architecture.For a 2D molecular graph
G = (V, E), the GINE encoder f G outputs a graph embedding h G g ∈ R 1×dg and node embeddings h G v ∈ R |V |×dg
, where d g is the embedding dimension.Otherwise, the TokenGT encoder f T outputs not only a graph embedding h T g ∈ R 1×dg and node embeddings h T v ∈ R |V |×dg , but also edge embeddings h T e ∈ R |E|×dg .We then concatenate all the embeddings obtained by both encoders h G g , h G v , h T g , h T v , and h T e along the first dimension to obtain h ∈ R (2|V |+|E|+2)×dg , which is then used as the key for the Q-Former.</p>
<p>Cross-modal Projector (Q-Former) Querying transformer (Q-Former) [20] is a modality-bridging transformer that converts the varying number of concatenated embeddings for each molecular graph into a fixed-length token sequence, enabling efficient batch processing.Specifically, structural information is distilled via cross-attention between N q = 32 learnable query vectors l ∈ R 32×dq , initialized randomly, and the concatenated molecular embeddings h ∈ R (2|V |+|E|+2)×dg , producing 32 tokens aligned with the text modality.The 32 tokens are concatenated with the task instruction as well as the SELFIES string before being fed to the LLM.Backbone Large Language Model We adopt Mistral-7B-Instruct-v0.3 [25] as a backbone LLM, following Yu et al. [5].In order to improve the efficiency in solving molecular tasks, we extend the token codebook with the 3K SELFIES vocabulary from BioT5+ [6] and add dedicated tokens for the digits 0-9, the decimal point, and the negative sign, thereby enabling direct number prediction for regression tasks.Additional task-specific vocabulary covers boolean labels, textual descriptions, and reaction routes, allowing Mol-LLM to natively produce the heterogeneous answer formats required by downstream applications.Examples of these extra tokens appear on the right side of Figure 2. The LLM pre-training serves two purposes: (i) injecting molecule-specific prior knowledge and (ii) reducing the compute required during later multimodal training.Accordingly, we pre-train the LLM on exactly the same dataset that will be used later for fine-tuning, optimizing a token-level cross-entropy objective.Given a training instance consisting of a task instruction q, a molecular SELFIES string s, and a ground truth answer y, we minimize L SFT = − t log π LLM θ y t | s, q, y &lt;t where t indexes tokens.</p>
<p>Multimodal</p>
<p>Stage 2 -Q-Former Pre-training In Stage 2, only the Q-Former is updated, while both the GNN and the LLM remain frozen.Following Liu et al. [26], we simply reuse the fine-tuning dataset, in which molecular representations and natural language tokens appear in an interleaved format.For each training instance (s, q, y), the SELFIES string s is converted into its corresponding molecular graph g.The combined model π θ (GNN+Q-Former+LLM) is then trained for one epoch with the loss defined as L SFT = − t log π θ y t | s, q, g, y &lt;t .</p>
<p>Stage 3 -MolPO: Molecular Structure Preference Optimization We observed that using only SFT training as in conventional multimodal Molecular LLMs [8,9,19,12], result in a graph bypass phenomenon (Figure 1) in solving molecular tasks.To resolve the graph bypass issue, we propose Molecular structure Preference Optimization (MolPO).Rather than simply inputting multimodal molecules into the LLM without consideration for multimodal utilization, MolPO promotes the practical utilization of multimodal molecules by learning the preferences between an original (chosen)  graph g w and a perturbed (rejected) graph g ℓ , which is inspired by mDPO [27].Constructing g ℓ , it is crucial to introduce perturbations that alter the relationship between the graph and the target.Since molecular features can generally be identified on a substructure basis, we substitute the substructures found in the original g w as in Figure 4 right panel.This approach offers the advantage of being applicable to any molecular task without significant computational costs or requiring taskspecific graph perturbation design (details in Appendix B.4).Based on the reward formulation r w,i = β |y| t log π θ (y t | g w , s, q i , y &lt;t ) and r ℓ,i = β |y| t log π θ (y t | g ℓ , s, q i , y &lt;t ) motivated from SimPO [28], the MolPO objective is defined as follows:
L MolPO = E (s,qi,g,y)∼Dtr [− log σ min(r w,i − r ℓ,i , λ clip |r w,i |) − γ i ],(1)
where In developing a generalist over diverse molecular tasks, we experimentally observed that adopting SimPO's task-agnostic target reward margin γ results in inappropriate log-sigmoid values due to highly variant reward orders of magnitude across tasks.However, modeling the task-specific reward scale as a hyperparameter is not an ideal solution either, as it adds an additional burden of hyperparameter search for each molecular task.Instead, we introduce a task-adaptive target reward margin with only a task-agnostic hyperparameter λ margin , where the expectation is estimated using an exponential moving average during training.
λ
In addition, given that it is generally easier to lower the rejected reward than to increase the true chosen reward, the preference reward margin can be empirically manipulated by simply reducing r ℓ,i without a corresponding enhancement of r w,i .To fully harness the benefits of preference optimization without the drawbacks, we introduce margin clipping to appropriately control the influence of the reward margin on parameter updates.Specifically, the margin is constrained so that it cannot exceed a fraction, λ clip of |r w,i |.Through this simple margin clipping, the model is prevented from the circumventing unintended effect of preference optimization by solely reducing the rejected reward.Further details of MolPO training are provided in Appendix B.5.</p>
<p>Extensive Instruction-tuning Dataset</p>
<p>The instruction-tuning dataset for Mol-LLM spans five major molecular task groups: property regression, property classification, reaction prediction, description-guided molecule generation, and molecule captioning.Property regression consists of five tasks-LogS for water solubility (ESOL [5]), LogD for lipophilicity (Lipo [5]), HOMO [7], LUMO [7], and HOMO-LUMO gap [7], and property classification comprises BACE [6], BBBP [5], ClinTox [5], HIV [5], SIDER [5].Reaction prediction covers forward synthesis (FS), retrosynthesis (RS), and reagent prediction (RP), with FS and RS each divided into Mol-Instructions [7] and SMolInstruct [5] subsets according to their dataset sources.The description-guided molecule generation and molecule captioning tasks are similarly split into ChEBI-20 [29] and SMolInstruct based on their origins.In addition, to enhance the understanding of IUPAC [30]-frequently used in molecular text captions-we incorporate an IUPAC and SELFIES translation dataset [5] to construct an 3.3M extensive instruction-tuning dataset (details in Appendix C.1).</p>
<p>3 Experiments</p>
<p>Experimental Setup</p>
<p>Baseline Models We group the molecular LLMs compared with Mol-LLM into three broad categories.Specialist models are trained for a single molecular task; semi-generalist models cover a specific task group within one model but do not span all task groups; and generalist models are designed to handle every molecular task group.Representative examples are MolCA [8] for the specialist category, BioT5+ [6] for the semi-generalist category, and Galactica [31] and LlaSMol [5] for the generalist category.Comprehensive details on all baseline models can be found in Appendix D.2.</p>
<p>Evaluation Benchmark In addition to the molecular tasks described in Section 2.3, we evaluate molecular LLM robustness to out-of-distribution (OOD) by proposing two evaluation benchmarks.</p>
<p>For LogS prediction, we retain high-confidence solubility labels from AqSol [32], exclude every molecule that also appears in ESOL, and collect molecules of high consistency among labels to construct the OOD evaluation versus ESOL.For reaction prediction, we gather 23K FS and 59K RS data instances from the ORDerly [33] repository except USPTO [34], apply a scaffold split to remove motif overlap with Mol-Instructions [7] and SMolInstruct [5], and reserve 5K examples for evaluation in each task.Full OOD dataset construction details are provided in Appendix C.2.</p>
<p>Evaluation Metrics For property prediction tasks, we report the root mean squared error (RMSE) or mean absolute error (MAE) in regression, and in classification tasks, receiver operating characteristic area under the curve (ROC-AUC) using the predicted probability of the positive class (i.e., True token).For reaction prediction and description-guided molecule generation, we evaluate exact match with the target molecule (EXACT), textual similarity (BLEU) [35], molecular fingerprint similarity based on RDKit [36], MACCS keys [37], and Morgan [38] fingerprints (RDK FTS, MACCS FTS, and MORGAN FTS, respectively), and the proportion of generated molecules that are chemically valid (VALIDITY).For the molecule captioning task, we measure similarity between the generated and reference descriptions using BLEU-2, BLEU-4, ROUGE-1 [39], ROUGE-2, ROUGE-L, and METEOR [40].For a comprehensive assessment of the molecular LLM, we analyze a wide range of metrics for each molecular task.However, due to space limitations, the main paper reports only the primary metrics.The complete results are provided in Appendix D.3).ClinTox, and SIDER.Notably, even Mol-LLM (w/o Graph) performs on a par with the full model.We attribute this behavior to the small molecular sizes in MoleculeNet [41], which allow the LLM to infer structural information directly from the SELFIES representation.</p>
<p>Results</p>
<p>Property Regression and Classification</p>
<p>Reaction Prediction</p>
<p>The reaction prediction results are reported in Table 2. Except for the FS task of SMolInstruct dataset, Mol-LLM again leads all generalist models.Since successful reaction prediction depends on recognizing which functional groups can participate during a chemical reaction, these results suggest that pre-training of the GNN on functional group prediction helps Mol-LLM exploit structural cues more effectively.Consistent with this interpretation, omitting the graph input (w/o Graph variant) noticeably degrades performances.</p>
<p>Description-guided Molecule Generation Table 3 shows the results for description-guided molecule generation, whose input prompts contain no molecular graphs.Since both Mol-LLM and the w/o Graph variant receive identical inputs, their scores are nearly indistinguishable.This confirms that Mol-LLM's ability to use graphs does not impede its instruction-following ability when</p>
<p>Ablation Study</p>
<p>MolPO objective enhances molecular graph utilization and task performance.To examine whether incorporating the MolPO objective L MolPO during Mol-LLM training leads the model to exploit molecular graph information more effectively than training with SFT alone, we first compare, for each task i, the log-likelihood r w,i obtained when the model is given the chosen graph g w to the log-likelihood r ℓ,i obtained when it is given the rejected graph g ℓ .We then compute the graph
discrimination ratio GDR = 1 Ni Ni n=1 I r w,i (n) &gt; r ℓ,i (n)
, where N i is the number of instances in task i, I is the indicator function, and r w,i (n) is the log-likelihood for the n-th instance in task i.A GDR close to 1 indicates that the model can clearly identify the correct molecular graph (i.e., it effectively exploits molecular graph information); a value near 0.5 indicates random guessing, and a value near 0 indicates systematic confusion.Figure 1 shows the per-task GDRs-green bars for MolPO-trained models and orange bars for models trained without L MolPO .The consistently higher GDRs in the MolPO setting confirm that this objective helps the model make better use of molecular graph information.We also compare the multitask fine-tuning performance obtained when L MolPO is combined with L SFT to that obtained when only L SFT is used.As shown in Table 5, leveraging the graph modality through the MolPO objective improves performances on most tasks.Our GNN pre-training improves molecular representation.To clearly demonstrate the effect of our GNN pre-training method, we frame the experiment as a single task setting and modify Mol-LLM so that, during fine-tuning, it receives only the task instruction and the 2D molecular graph as inputs, omitting the 1D sequence.The model is trained solely using the loss term L SFT , and its performance is then compared with different GNN architectures and weight initializations.Figure 5 presents the learning curves for property regression (HOMO) and reaction prediction (FS, RP).The GNN architectures (GINE, TokenGT) and their corresponding initializations are represented as {GNN architecture}-{initialization}.Scratch indicates that the GNN is trained from scratch without any pretrained weights.The model whose GNN is initialized with the proposed pre-training method (Ours) consistently outperforms the others, indicating that it learns higher quality molecular representations.Moreover, the existing pre-trained models-MoleculeSTM [15] and GraphCL [42]-perform either worse than or roughly on par with the non-pretrained baseline Scratch, which is a surprising outcome.</p>
<p>Related Works</p>
<p>Molecular Large Language Models MolT5 [29] extends T5 [43] to bidirectional translation between SMILES strings and natural language, whereas MolXPT [44], built on the GPT architecture [45], unifies text-molecule translation with property prediction.MolCA [8] and GIT-Mol [10] fuse 2D molecular graphs with text via a Q-Former [20], while MolLM [46] further injects 3D geometric cues.UniMoT [11] discretizes Q-Former outputs into graph tokens while 3D-MolT5 [19] introduces 3D structure tokens, enabling generative reasoning over conformers.Although these models exploit molecule structures, each is tailored to a narrow set of tasks.Mol-LLM tackles this limitation by jointly processing text and graphs and by performing translation, prediction, and generation within a single generalist framework.</p>
<p>Instruction-tuning on Molecular Tasks Mol-Instructions [7] introduced the first broad instructiontuning corpus, inspiring InstructMol [9] to fine-tune multimodal models with task-specific prompts and BioT5+ [6] to perform multitask tuning without structural inputs.LlaSMol [5] scales the idea to 3.3M examples across ten tasks, yielding a single model that matches-or exceeds-specialists.Subsequent work, including UniMoT [11], 3D-MolT5 [19] and 3D-MoLM, couples instruction tuning with 2D/3D structure encoders, yet still lacks a systematic strategy for exploiting multimodal inputs.Consequently, models remain sensitive to task distribution shifts.Mol-LLM fills this gap by unifying instruction tuning with structure-aware training, thereby improving robustness across in-distribution and out-of-distribution tasks.</p>
<p>Preference Optimization on Different Modality DPO [47] aligns language models with human preferences by maximizing the log-probability gap between preferred and rejected outputs; SimPO [28] removes the expensive reference model for lighter training.As multimodal LLMs rise, mDPO [27] adapts the idea to vision-language models by corrupting images to build preference pairs, and numerous follow-ups [48][49][50][51] confirm its effectiveness.Yet no study has demonstrated comparable gains for molecular data.Mol-LLM is the first to apply preference optimization to molecular graphs and text jointly, showing that structure-aware preferences yield stronger generalization than sequence-only tuning while keeping training costs manageable.</p>
<p>Conclusion</p>
<p>We introduced MolPO, a multimodal training objective that leverages perturbed molecules to enhance the utility of 2D molecular graphs, together with a hybrid graph encoder pre-training strategy.We also curated a large-scale molecule instruction tuning dataset and, using the proposed methods, developed Mol-LLM, a multimodal generalist molecular large language model.Mol-LLM achieved state-of-theart performances among generalist molecular models on property regression, property classification, reaction prediction, description-guided molecule generation, and molecule captioning tasks.We believe our approach can be extended beyond 2D molecular graphs to incorporate 3D structural information and molecular metadata, enabling real-world applications such as drug discovery and novel material discovery.A detailed discussion of the limitations are described in Appendix A.</p>
<p>A Limitation</p>
<p>Performance Degradation from Limited Molecular Distribution in Classification Tasks When the training data lacks sufficient diversity, preference optimization approaches using input preference pairs could suffer performance degradation on test or out-of-distribution datasets.In the case of MolPO, if the training molecular distribution is too narrow or contains spurious patterns unrelated to the given molecular task, the model may inappropriately regard molecules in test set or out-of-distribution (OOD) dataset as rejected molecules, based solely on their non-in-distribution characteristics.This hypothesis is consistent with the observations in Table 1 for the classification datasets.The classification datasets are substantially smaller than the datasets in the other task groups.</p>
<p>More than half of them contain only approximately 1K samples, compared with 3.3M samples in the entire training dataset, which explains why MolPO's performance either remained unchanged or slightly decreased.The principled and necessary solution to this issue is basically to procure more diverse molecular distributions.We anticipate that the research community will pay more attention to developing diverse and comprehensive property classification datasets.</p>
<p>In-depth Analysis across Molecular Tasks Beyond the overall improvement in benchmark performance, an in-depth analysis is needed to understand what qualitative changes occur for each molecular task from the improved graph utilization by MolPO.It is necessary to identify trends that cannot be determined by performance metrics alone, such as which molecular features are difficult to capture with sequence-only approaches, and whether these identified molecular features have strong practical impact.Such analysis could be particularly interesting for property prediction tasks where spatial recognition of molecules is important.</p>
<p>Multi-step Reasoning and Multi-turn Interaction</p>
<p>As demonstrated by recent successful LLMs [2,3], impactful real-world applications of LLMs critically depend on multi-step reasoning capabilities and multi-turn interactions between LLMs and users.However, research on these two aspects remains significantly underdeveloped in the field of molecular LLMs.Such research requires different considerations from single-turn instruction tuning, beginning with dataset construction, and necessitates appropriate training objectives and reward modeling.It is an interesting direction to extend molecular LLMs to multi-step reasoning and multi-turn interaction for practical applications.</p>
<p>B Implementation Details</p>
<p>This section discusses the details of the Mol-LLM implementation.All the necessary materials to reproduce the results through Tables 1 to 4, including code, trained model, and test set, are available at https://anonymous.4open.science/r/mol-llm-neurips2025-93EB.</p>
<p>B.1 Functional Group Prediction Dataset for Graph Encoder Pre-training</p>
<p>As explained in Section 2.2, the proposed graph encoder pre-training conducts functional group prediction of a given molecule, a kind of self-supervision task carried out only with the input molecule.The principal challenge in constructing the functional group prediction dataset is the severe class imbalance: some groups occur in most molecules, whereas others are exceedingly rare.Leveraging the RDKit Fragments module 3 , we enumerate 87 functional groups and quantify their occurrences across the entire PubChem database, as summarized in the top panel of Figure 6. Figure 6 illustrates functional group imbalance, for example, fr_NH0 (tertiary amines) appears in many molecules, whereas fr_prisulfonamd (primary sulfonamides) are scarce.This imbalance can cause overfitting to dominant classes instead of learning general chemical knowledge.To alleviate the overfitting problem, we remove the 11 most prevalent groups (from fr_NH0 to fr_aryl_methyl) and the rarest group (fr_prisulfonamd), retaining 72 functional groups.The middle panel of Figure 6 shows the reduced yet still skewed distribution.To adjust the skewed distribution, we apply sparsity-aware importance sampling as follows.Given M molecules and G retained groups, let x i,g ∈ {0  (Middle) Distribution of functional groups in PubChem molecules after excluding groups that are either overly common or extremely rare.(Bottom) Distribution of functional groups obtained after sampling 5M molecules from PubChem database, considering functional group sparsity.Since the number of molecules differs among panels, the y-axis scale varies across plots for visualization purposes.</p>
<p>s g = 1/(c g + ε) with ε = 10 −6 , resulting in the sparsity score of molecule i as
σ i = G g=1
x i,g s g</p>
<p>2</p>
<p>.</p>
<p>Normalizing the scores yields a categorical distribution p i = σ i M j=1 σ j , from which we sample 5M molecules.The resulting distribution (bottom panel of Figure 6) is flatter than before, which enables the graph encoder to learn more unbiased chemical knowledge than when trained on the raw PubChem molecule distribution.</p>
<p>B.2 Details of Graph Encoder and Pre-training</p>
<p>Architecture Both GNN components of the hybrid graph encoder-GINE and TokenGT-use a hidden dimension d g = 1024 and five message-passing layers.We replace the original transformer blocks of TokenGT with a BERT encoder implemented in FlashAttention-2 and configured</p>
<p>B.3 Investigation of Graph Encoder Used in Prior Work</p>
<p>In Section 3.3, we show that the downstream performance of the LLM integrated with pre-trained GNNs used in Liu et al. [8], Cao et al. [9], which are MoleculeSTM [15] and GraphCL [42], does not improve from that of random initialization.To further investigate the graph representation of MoleculeSTM, we conducted an additional experiment evaluating MoleculeSTM in isolation from the LLM on the QM9 datasets.In this experiment, the graph embedding h g is obtained by mean-pooling the node embeddings, and then passed to a simple MLP, a regression head, whose output is used for training over MSE minimization.While tuning the regression head, we compare three GNN tuning settings: tuning a randomly initialized GNN, freezing a randomly initialized GNN, and freezing a GNN initialized with MoleculeSTM.Table 6 reports the mean absolute error (MAE) for each property.It turns out that, when only the regression head is trained, the gap between random and MoleculeSTM initialization remains negligible w.r.t. the jointly training of GINE, reinforcing our observation that the current pre-trained GNN model fails to capture useful molecular representations.All models were trained for 1,500 epochs with a batch size of 128 using the Adam optimizer with a learning rate of 10 −4 on four NVIDIA A100 GPUs.</p>
<p>B.4 Molecular Structure Preference Pair</p>
<p>To improve the graph utilization of our model, we create molecular structural preference pairs, which are required for Molecular Structure Preference Optimization (MolPO).Specifically, as a generalist molecular LLM, it requires a preference pair generation method applicable across various molecular tasks.Therefore, we employed functional group-based substructure modification, which can alter molecular features based on only the input molecule, without requiring task-specific design.For this, we propose Molecular ACCess System (MACCS) [37] keys-based substructure modification method directly modifies molecular substructures by randomly removing and adding them.This approach first identifies the substructures of the molecule corresponding to MACCS keys, generating two lists: one containing the MACCS keys representing functional groups present in the molecule, and the other containing the keys for functional groups absent from the molecule.Then we sample random</p>
<p>B.5 Details of Mol-LLM</p>
<p>This section describes the details of the Mol-LLM architecture and training, including the hyperparameters listed in Table 7.</p>
<p>Architecture When using Q-Former as the cross-modal projector, instead of using randomly initialized weights, we initialize it, similarly to Liu et al. [8], using the parameters of a 12-layer pre-trained transformer encoder with an embedding dimension of 768.However, we observed that successful multi-task learning can be achieved without fully utilizing all 12 layers of the Q-Former while maintaining performance without significant performance degradation.Therefore, to reduce the pre-training cost of Q-Former, we use only 5 layers instead of all 12 layers.The number of Q-Former query tokens is set to 32 for multi-task learning, which is more than the eight used in prior work [8].</p>
<p>We set the LoRA rank to 64, alpha to 32, and the dropout rate to 0.1.7; however, we observed that performance had not fully converged on several tasks.Therefore, we report experimental results based on the model trained for one additional epoch using a reduced initial learning rate of 2 × 10 −5 (half of the original value) without a warm-up epoch.</p>
<p>Three Stage Training</p>
<p>C.1 In-distribution Dataset Construction</p>
<p>We integrate molecules for each task from the molecule-oriented datasets Mol-Instructions [8] and SMolInstruct [5].During this integration process, tasks present in both datasets, such as forward synthesis and molecule captioning, are deduplicated to ensure that molecules included in the test set of one dataset do not appear in the training set of the combined dataset.In this process, we exclude certain tasks that are not directly relevant (e.g., NC-I2F and NC-S2F).For tasks absent in both datasets, such as BACE, molecules are directly extracted from the original data sources to construct the dataset.Finally, we augment the resulting task-specific datasets with instructions using templates adopted and extended from SMolInstruct.</p>
<p>C.2 Out-of-distribution Dataset Construction</p>
<p>LogS -AqSol Dataset To evaluate Mol-LLM on OOD LogS prediction, we use the AqSol dataset [32], which contains multiple water solubility datasets in addition to ESOL.The AqSol dataset is constructed by curating data from 9 different water solubility datasets for 9,982 unique molecules.For our out-of-distribution evaluation on the ESOL dataset, we removed instances from the AqSol dataset that overlap with the ESOL dataset based on the molecule's InChI.Notably, it is common for different prediction datasets to annotate different labels for the same molecule.This occurs due to experimental errors or when LogS labels are predicted based on different prediction models.To ensure high label reliability, we retain 925 molecules whose labels are either unique or have an inter-dataset standard deviation &lt; 0.1.</p>
<p>Reaction Prediction -ORDerly Dataset From Open Reaction Database (ORD) [33], we collected non-USPTO reaction data relevant to forward synthesis and retrosynthesis.Since all reactions in our instruction-tuning dataset are derived from USPTO data, the reactions extracted from non-USPTO sources constitute out-of-distribution (OOD) samples.Then, to ensure no duplication between the collected reaction data and those in Mol-Instructions [7] and SMolInstruct [5], we filtered out reactions from these non-USPTO sources whose input molecule scaffolds overlap with molecules used for reaction prediction training.During this, we first extract data for the forward synthesis task and subsequently ensure that the retrosynthesis reaction data extraction does not duplicate entries already obtained for the forward synthesis.Finally, we apply scaffold splitting to each dataset, resulting in 18K training samples and 5K test samples for forward synthesis, and 54K training samples and 5K test samples for retrosynthesis.</p>
<p>D Experimental Details</p>
<p>This section provides supplementary information necessary for understanding and reproducing the main experiments.In Appendix D.1, we detail the resource requirements and execution times needed to reproduce the main results, followed by Appendix D.2 where we define and categorize the baseline InstructMol [9] 1D Sequence &amp; 2D Graph Specialist MolCA [8] 1D Sequence &amp; 2D Graph Specialist MolT5 [29] 1D Sequence Only Specialist MolXPT [44] 1D Sequence Only Specialist Mol-Instructions [7] 1D Sequence Only Semi-Generalist BioT5+ [6] 1D Sequence Only Semi-Generalist GPT-4 (5-shot) [2] 1D Sequence Only Generalist Galactica [31] 1D Sequence Only Generalist 3D-MolM [12] 1D Sequence &amp; 3D Conformer Generalist ChemDFM [23] 1D Sequence Only Generalist LlaSMol [5] 1D Sequence Only Generalist Mol-LLM 1D Sequence &amp; 2D Graph Generalist molecular language models based on modality and task coverage.In Appendix D.3, we include full experimental results, whose evaluation metrics are skipped in the main body due to the page limit.</p>
<p>D.2 Baseline Models</p>
<p>As described in Section 3.1, we categorize the baseline models into three groups: specialist models, semi-generalist models, and generalist models, based on their level of specialization and task coverage.</p>
<p>In addition to the three model categories, we provide a classification based on the type of input modalities.These categorizations are summarized in Table 10.</p>
<p>D.2.1 Categories by Input Modalities</p>
<p>1D Sequence Only Models that rely solely on 1D sequences (e.g., SMILES or SELFIES), which address molecules as strings.This category include Galactica 6.7B [31], GPT-4 [2], Mol-Instructions [7], BioT5+ [6], LlaSMol [5], MolT5 [29], MolXPT [44], and ChemDFM [23].</p>
<p>1D Sequence &amp; 2D Graph Models integrate string-based and graph-based representations to capture 2D molecular structure.Representative examples are InstructMol [9], MolCA [8], and GIT-Mol [10].GIT-Mol additionally exploits molecular images, providing another route to leverage structural information.</p>
<p>1D Sequence &amp; 3D Conformer Models incorporate 3D conformers alongside sequence information to enrich molecular 3D spatial representations 3D-MoLM [12] belongs to this category.</p>
<p>Question</p>
<p>Please provide a feasible product that could be formed using these reactants and reagents:
[INPUT_MOLECULE] INPUT_MOLECULE Galactica LlaSMol Mol-LLM (Ours) Ground Truth
Please provide a feasible product that could be formed using these reactants and reagents:</p>
<p>D.2.2 Categories by Task Coverage</p>
<p>As described in Section 3.1, the baseline models are categorized as follows:</p>
<p>Specialist Models MolCA [8], InstructMol [9], MolXPT [44], GIT-Mol [10], and MolT5 [29] are optimized for individual molecular tasks without parameter or knowledge sharing across tasks.</p>
<p>Semi-Generalist Models BioT5+ [6] and Mol-Instructions [7] address related task groups within a single framework.For instance, BioT5+ trains two separate models: one for classification and translation, and the other for regression and reaction prediction, enabling knowledge sharing within each group while preserving task-specific optimization.</p>
<p>Generalist Models Galactica 6.7B [31], GPT-4 [2], LlaSMol [5], and ChemDFM [23] aim for broad generalization by simultaneously tackling all molecular task groups.</p>
<p>D.3 Full Experimental Results</p>
<p>Table 11 presents the complete results corresponding to Table 2. Table 13 and Table 12 show the full results for Table 3.In Figure 7, we also visualize predicted outputs by generalists, including Mol-LLM, Galactica [31], and LlaSMol [5] on forward reaction prediction and retrosynthesis on both Mol-Instructions and ORDerly datasets.</p>
<p>E Broader Impacts</p>
<p>We currently anticipate no major negative social impacts from this research; nevertheless, there is a possibility that it could be used to generate molecules harmful to humans or the environment.At present, training is carried out on eight NVIDIA A100 GPUs, but scaling to larger LLMs would require additional GPUs and would therefore increase carbon emissions.On the positive side, Mol-LLM enables researchers performing chemical experiments to predict experimental outcomes in advance.</p>
<p>Table 11: Performance comparison on reaction prediction task on Mol-Instructions [7] and SMolInstruct [5]   Guidelines:</p>
<p>• The answer NA means that the paper does not include theoretical results.</p>
<p>• All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.• All assumptions should be clearly stated or referenced in the statement of any theorems.</p>
<p>• The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.• Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.• Theorems and Lemmas that the proof relies upon should be properly referenced.</p>
<p>Experimental result reproducibility</p>
<p>Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?</p>
<p>Answer • If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.• If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.• Depending on the contribution, reproducibility can be accomplished in various ways.</p>
<p>For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model.In general.releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.• While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution., with an open-source dataset or instructions for how to construct the dataset).(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility.</p>
<p>In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.• The assumptions made should be given (e.g., Normally distributed errors).</p>
<p>• It should be clear whether the error bar is the standard deviation or the standard error of the mean.• It is OK to report 1-sigma error bars, but one should state it.The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.• For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g.negative error rates).• If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.</p>
<p>Experiments compute resources</p>
<p>Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?</p>
<p>Answer: [Yes]</p>
<p>Justification: Appendix D.1 provides sufficient resource information to reproduce the experiments, including workers, memory, and time of execution.</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper does not include experiments.</p>
<p>• The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.• The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.• The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).Guidelines:</p>
<p>• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.</p>
<p>• If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.• The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).</p>
<p>Broader impacts</p>
<p>Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?</p>
<p>Answer: [Yes] Justification: We discuss broader impacts in Appendix E.</p>
<p>Guidelines:</p>
<p>• The answer NA means that there is no societal impact of the work performed.</p>
<p>• If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.• Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.</p>
<p>• The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments.However, if there is a direct path to any negative applications, the authors should point it out.For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation.On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.• The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.• If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).</p>
<p>Safeguards</p>
<p>Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?Answer: [NA] Justification: Our model, Mol-LLM, does not have a high risk for misuse.Guidelines:</p>
<p>• The answer NA means that the paper poses no such risks.</p>
<p>• Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.• Datasets that have been scraped from the Internet could pose safety risks.The authors should describe how they avoided releasing unsafe images.• We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?Answer: [Yes] Justification: The backbone models of choice, including LLM and GNN, are credited in Section 2.1, and the datasets used are properly credited through Section 3.1 and Appendix C. Guidelines:</p>
<p>• The answer NA means that the paper does not use existing assets.</p>
<p>• The authors should cite the original paper that produced the code package or dataset.</p>
<p>• The authors should state which version of the asset is used and, if possible, include a URL.• The name of the license (e.g., CC-BY 4.0) should be included for each asset.</p>
<p>• For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.• If assets are released, the license, copyright information, and terms of use in the package should be provided.For popular datasets, paperswithcode.com/datasetshas curated licenses for some datasets.Their licensing guide can help determine the license of a dataset.• For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.• If this information is not available online, the authors are encouraged to reach out to the asset's creators.</p>
<p>Figure 1 :
1
Figure 1: (Left) Performance comparison among generalist molecular LLMs with normalized primary metrics.(Right) Graph utilization comparison between SFT and proposed multimodal training (MolPO).Score closer to 1 indicate better use of graph, approaching 0.5 indicate less utilization.</p>
<p>Figure 2 :
2
Figure 2: (Left) Overall structure of Mol-LLM.Molecular graph is encoded into a fixed-length token sequence by a hybrid graph encoder, followed by a Q-Former that outputs query embeddings to feed LLM, with corresponding task instruction and molecular 1D sequence.(Right) Representative downstream molecular tasks.</p>
<p>Figure 3 :
3
Figure 3: Overview of the three training stages and the loss function used at each stage.The training pipeline consists of a pre-training phase (Stages 1 and 2), followed by a fine-tuning phase (Stage 3).In Stage 1, all modules are trained independently and in parallel, whereas in Stages 2 and 3, the modules are trained in a unified architecture and loss function.</p>
<p>Figure 4 :
4
Figure 4: (Left) Overview of the two graph pre-training tasks for the proposed hybrid graph encoder.Two distinct GNN backbones, GINE and TokenGT, are trained independently.(Right) Illustration of the MolPO training objective, which contrasts a chosen molecule with a rejected molecule.</p>
<p>Figure 5 :
5
Figure 5: Comparison of fine-tuning performances on three tasks under different GNN architectures and initialization with (or without) pre-trained parameters.Each line is labeled as {GNN archi-tecture}-{initialization}.Ours refers to models initialized with parameters obtained via our GNN pre-training method, whereas Scratch denotes models trained from random initialization.The x-axis denotes the number of training steps, and the y-axis shows the corresponding evaluation metric.</p>
<p>Figure 6 :
6
Figure 6: (Top) Distribution of functional groups present in molecules from the PubChem database.(Middle)Distribution of functional groups in PubChem molecules after excluding groups that are either overly common or extremely rare.(Bottom) Distribution of functional groups obtained after sampling 5M molecules from PubChem database, considering functional group sparsity.Since the number of molecules differs among panels, the y-axis scale varies across plots for visualization purposes.</p>
<p>For component ablation, we maintain identical hyperparameters for Mol-LLM, Mol-LLM (w/o Graph), and Mol-LLM (w/o MolPO), as specified in Table7.In Stage 1, along with the GNN pre-training described in Appendix B.2, we fine-tune only the LoRA parameters of the LLM for 12 epochs.In Stage 2, we train the Q-Former for a single epoch to align the LLM and GNN embeddings learned in Stage 1. Next, in Stage 3, as described in Section 2, we train using the combined objective L SFT + cL MolPO , which combines both the SFT and MolPO objectives.Here, the scaling factor c = 0.25 is adjusted to ensure that the scales between L SFT and cL MolPO do not differ significantly.For the hyperparameters used in L MolPO = E (s,qi,g,y)∼Dtr [− log σ min(r w,i − r ℓ,i , λ clip |r w,i |) − γ i ], we use λ margin = 0.5 and λ clip = 1.0, respectively.For Stage 3, we initially trained the model for 6 epochs using the hyperparameters specified in Table</p>
<p>D. 1 Resources
1
All experiments, except for graph encoder pre-training, were conducted on 8 NVIDIA A100 80GB GPUs and an AMD EPYC 7713 64-Core processor with 512GB of RAM.Using this hardware configuration, Stage 1 required 6 days of training, Stage 2 required half a day, and Stage 3 required 12 days to complete.In Stage 1 graph encoder pre-training, GINE training took approximately 18 hours on 4 A100 GPUs, and TokenGT took 19 hours.</p>
<p>Figure 7 :
7
Figure 7: Comparison of predicted outputs by generalists on forward synthesis (FS) and retrosynthesis (RS), both in Mol-Instructions and ORDerly dataset.The upper two rows represent forward synthesis in Mol-Instructions (InD) and ORDerly (OOD) Datasets, respectively, and the lower two rows represent the retrosynthesis task in the same dataset order.</p>
<p>For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g.</p>
<p>9 .
9
Code of ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?Answer: [Yes] Justification: The research presented in this paper fully conforms to the NeurIPS Code of Ethics.</p>
<p>The graph encoder is optimized with the combined loss L GNN = L func + L recon .Additional training procedures and hyperparameters are provided in Appendix B.2.</p>
<p>TrainingStage 1 -Graph Encoder and LLM Pre-training The hybrid graph encoder comprises two GNNs, GINE and TokenGT.We pre-train these two GNNs in parallel with the LLM.The GNN pre-training comprises two complementary tasks: functional group prediction and SELFIES reconstruction.Functional group prediction strengthens representations of the functional groups that govern molecular properties, whereas SELFIES reconstruction encourages the encoder to preserve global structural information.Both tasks share the same graph-level embedding h g produced by the GNN, as illustrated on the left side of Figure4.After discarding extremely common or extremely rare functional groups, we retain K = 72 distinct groups (dataset construction details are given in Appendix B.1).For functional group prediction, h g is passed through a three-layer MLP (1024 → 1024 → 72) f MLP that learns to reproduce molecule's SELFIES string s: L recon = − t log π GPT-2 θ s t | h g , s &lt;t .</p>
<p>clip is coefficient, and D tr denotes training dataset.γ i = λ margin |E (gw,s,qi,y) [r w,i ]| is a taskadaptive target reward margin for each i-th molecular task, calculated during training.The entire training objective combined is L SFT + cL MolPO , where c is a constant.</p>
<p>Table 1 :
1
[23]ormance comparison on molecular property prediction tasks from the MoleculeNet[41]benchmark.A superscript * indicates results evaluated with an official checkpoint, and "NA" denotes cases where no official checkpoint is available.Boldface highlights the best scores among generalist models.For semi-generalist models, each variant is annotated with the task group on which it is trained.GPT-4 is evaluated with 5-shots, except for classification performances borrowed from Zhao et al.[23]with zero-shot.
TaskLogSLogDHOMOLUMOGapBACEBBBPClinToxHIVSIDERMetricRMSE (↓) RMSE (↓) MAE (↓) MAE (↓) MAE (↓) ROC-AUC (↑) ROC-AUC (↑) ROC-AUC (↑) ROC-AUC (↑) ROC-AUC (↑)Specialist ModelsInstructMolNANA0.00480.00500.006182.172.4NA68.9NAMolCA≥100≥100≥1≥1≥179.870.089.547.063.0MolXPTNANANANANA88.480.095.378.171.7Semi-Generalist Models Mol-Instructions  *  BioT5+  *  (Cls. &amp; Trans.) BioT5+  *  (Reg. &amp; React.)4.81 ≥100 ≥100≥100 ≥100 ≥1000.0210 ≥1 0.00220.0210 ≥1 0.00240.0203 ≥1 0.002841.7 81.1 65.558.0 65.1 51.547.8 83.7 51.049.2 67.0 58.848.2 43.7 52.5Generalist ModelsGPT-4 (5-shot)1.681.590.02270.04620.039562.561.551.665.940.5Galactica 3D-MoLM  *  ChemDFM  *  LlaSMol  *4.34 3.41 8.19 1.212.78 4.86 6.21 1.010.2329 0.0299 0.1204 ≥10.0413 0.0536 0.1262 ≥10.2497 0.0673 0.1694 ≥158.4 55.5 59.5 46.753.5 53.8 50.5 82.478.4 53.7 60.0 77.572.2 30.6 52.4 70.355.9 49.7 51.0 78.4Mol-LLM (w/o Graph)1.360.950.00440.00430.005580.884.385.076.576.1Mol-LLM1.280.910.00440.00430.005480.581.182.475.176.3</p>
<p>Table 2 :
2
[5]formance comparison for reaction prediction tasks on Mol-Instructions[7]and SMolInstruct[5]datasets.
DatasetMol-Instructions / SMolInstructTaskForward SynthesisRetrosynthesisReagent PredictionMetricEXACT (↑) MACCS FTS (↑) EXACT (↑) MACCS FTS (↑) EXACT (↑) MACCS FTS (↑)Specialist ModelsInstructMol MolCA  <em>0.536 / NA 0.000 / 0.0000.878 / NA 0.494 / 0.3570.407 / NA 0.000 / 0.0000.852 / NA 0.880 / 0.7600.129 0.0000.539 0.115Semi-Generalist Models Mol-Instructions  *  BioT5+  *  (Cls. &amp; Trans.) 0.000 / 0.000 0.052 / 0.003 BioT5+  *  (Reg. &amp; React.) 0.864 / 0.0810.291 / 0.184 0.152 / 0.187 0.975 / 0.5370.069 / 0.015 0.001 / 0.000 0.642 / 0.1520.359 / 0.285 0.195 / 0.170 0.930 / 0.7510.044 0.000 0.2570.364 0.056 0.621Generalist ModelsGPT-4 (5-shot)0.021 / 0.0110.728 / 0.6340.012 / 0.0130.716 / 0.6860.0000.228Galactica 3D-MoLM  *  ChemDFM  *  LlaSMol  </em>0.000 / 0.000 0.000 / 0.000 0.000 / 0.002 0.743 / 0.6290.257 / 0.377 0.391 / 0.296 0.142 / 0.178 0.955 / 0.9190.000 / 0.000 0.000 / 0.000 0.000 / 0.000 0.453 / 0.3230.274 / 0.447 0.451 / 0.372 0.440 / 0.443 0.885 / 0.8270.000 0.000 0.000 0.0000.127 0.218 0.099 0.199Mol-LLM (w/o Graph)0.893 / 0.5840.983 / 0.9040.510 / 0.3630.886 / 0.8280.2020.586Mol-LLM0.911 / 0.6010.987 / 0.9080.538 / 0.3770.893 / 0.8320.2250.600
Table 1 summarizes the property regression and classification results.On most tasks, Mol-LLM outperforms every other generalist model, except for LogS,</p>
<p>Table 3 :
3
[5]]ormance comparison for molecule generation and molecule captioning on ChEBI-20[29]and SMolInstruct[5]datasets.
DatasetChEBI-20 / SMolInstructTaskMolecule GenerationMolecule CaptioningSpecialist ModelsGIT-Mol0.051 / NA0.738 / NA0.93 / NA0.263 / NA0.560 / NA0.533 / NAInstructMolNA / NANA / NANA / NA0.371 / NA0.502 / NA0.509 / NAMolT5 MolCA  <em>0.311 / 0.317 NA / NA0.834 / 0.879 NA / NA0.91 / 0.95 NA / NA0.508 / 0.366 0.594 / 0.501 0.614 / 0.515 0.540 / 0.510 0.631 / 0.604 0.652 / 0.628MolXPT0.215 / NA0.859 / NA0.98 / NA0.505 / NA0.597 / NA0.626 / NAText+Chem T50.322 / NA0.901 / NA0.94 / NA0.542 / NA0.622 / NA0.648 / NASemi-Generalist ModelsMol-Instructions BioT5+  *  (Cls. &amp; Trans.) 0.557 / 0.519 0.016 / 0.045 BioT5+  *  (Reg. &amp; React.) 0.537 / 0.4160.167 / 0.475 0.907 / 0.897 0.897 / 0.8671.00 / 1.00 1.00 / 1.00 1.00 / 1.000.171 / 0.020 0.289 / 0.217 0.271 / 0.124 0.591 / 0.582 0.649 / 0.644 0.680 / 0.677 0.216 / 0.221 0.364 / 0.364 0.323 / 0.321Generalist ModelsGPT-4 (5-shot) Galactica  *  3D-MoLM  *  ChemDFM  *  LlaSMol  </em>0.092 / 0.027 0.000 / 0.000 0.000 / 0.000 0.018 / 0.041 0.274 / 0.1800.745 / 0.726 0.264 / 0.271 0.000 / 0.000 0.165 / 0.297 0.871 / 0.8450.65 / 0.74 0.70 / 0.61 0.00 / 0.00 0.19 / 0.13 0.95 / 0.930.158 / 0.125 0.303 / 0.273 0.320 / 0.274 0.000 / 0.000 0.006 / 0.006 0.004 / 0.005 0.171 / 0.167 0.287 / 0.285 0.326 / 0.329 0.031 / 0.035 0.101 / 0.108 0.078 / 0.085 0.333 / 0.328 0.464 / 0.465 0.466 / 0.470Mol-LLM (w/o Graph)0.431 / 0.3620.903 / 0.8881.00 / 1.000.482 / 0.477 0.509 / 0.490 0.587 / 0.585Mol-LLM0.443 / 0.3680.906 / 0.8871.00 / 0.990.493 / 0.482 0.439 / 0.433 0.599 / 0.589
MetricEXACT (↑) MACCS FTS (↑) VALIDITY (↑) BLEU-4 (↑) ROUGE-L (↑) METEOR (↑)</p>
<p>Table 4 :
4
Evaluation of OOD generalization for reaction prediction on the ORDerly dataset, which is non-USPTO, and LogS on the AqSol dataset.
DatasetAqSolORDerlyTaskLogSForward SynthesisRetrosynthesisMetricRMSE (↓) EXACT (↑) MACCS FTS (↑) VALIDITY (↑) EXACT (↑) MACCS FTS (↑) VALIDITY (↑)Semi-Generalist Models BioT5+  *  (Reg. &amp; React.)1.810.0950.6281.000.1390.6781.00Generalist ModelsGPT-4 Galactica  *  3D-MoLM  *  ChemDFM  *  LlaSMol  *2.17 3.20 2.72 6.98 1.320.000 0.000 0.000 0.017 0.3500.723 0.322 0.288 0.428 0.8810.87 0.49 0.01 0.04 1.000.000 0.000 0.000 0.000 0.4730.672 0.398 0.396 0.406 0.8750.65 0.38 0.01 0.05 0.99Mol-LLM (w/o Graph)1.100.3940.9001.000.7270.9361.00Mol-LLM1.020.4010.8771.000.7380.9391.00</p>
<p>Table 5 :
5
An ablation study on MolPO's effect on graph utilization.We report RMSE(↓) for LogS and LogD, and EXACT(↑) for FS, RS, RP, and T2M, each representing forward reaction prediction, retrosynthesis, reagent prediction, and molecule generation."Mol-Inst."and "SMol."denote the Mol-Instructions and SMolInstruct datasets, respectively.
Mol-LLM (w/o MolPO)1.360.960.9070.5980.5290.3680.2200.4260.355Mol-LLM1.280.910.9110.6010.5380.3770.2250.4430.368
LogS LogD FS (Mol-Inst.)FS(SMol.)RS(Mol-Inst.)RS(SMol.)RP(Mol-Inst.)T2M(ChEBI-20)T2M(SMol.)graphsareabsent.On both the ChEBI-20 and SMolInstruct datasets, Mol-LLM nonetheless achieves the best results among generalist models.Molecule Captioning As summarized in Table3, Mol-LLM again surpasses all baselines.Compared with the w/o Graph variant, the full model obtains consistently higher BLEU and METEOR scores but slightly lower ROUGE scores on both ChEBI-20 and SMolInstruct.The pattern implies that Mol-LLM produces more concise captions: it captures the essential information while omitting peripheral details.We believe MolPO training encourages the model to rely on structural cues and focus on the core content.Generalization Performance on Out-of-distribution Datasets Table 4 reports OOD results forAqSol.On the in-distribution training tasks (LogS and SIDER), Mol-LLM lags the generalist baseline LlaSMol only marginally.In contrast, it is markedly superior on the OOD AqSol benchmark, demonstrating stronger generalization.A similar trend appears in the reaction prediction FS and RS tasks: Mol-LLM is slightly weaker on in-distribution FS of SMolInstruct but outperforms competitors when evaluated OOD.These findings indicate that MolPO training confers broader generalization across both tasks and input distributions, whereas the semi-generalist BioT5+, which lacks large-scale instruction tuning, suffers a notable drop in performance.</p>
<p>Here, we implement importance sampling that favors rarer groups by introducing the scaling factor
Total count across all molecules0 2 4 6 81e7fr_NH0 fr_benzene fr_C_O fr_C_O_noCOO fr_NH1 fr_ether fr_amide fr_Ar_N fr_halogen fr_bicyclic fr_aniline fr_aryl_methyl fr_methoxy fr_NH2 fr_pyridine fr_para_hydroxylation fr_Al_OH fr_ester fr_Al_OH_noTert fr_Ndealkylation2 fr_alkyl_halide fr_COO2 fr_piperdine fr_COO fr_allylic_oxid fr_Nhpyrrole fr_Ar_NH fr_unbrch_alkane fr_ketone fr_sulfide fr_Al_COO fr_Ndealkylation1 fr_sulfonamd fr_ketone_Topliss fr_imidazole fr_thiophene fr_ArN fr_piperzine fr_Imine fr_nitrile fr_thiazole fr_Ar_OH fr_urea fr_phenol fr_furan fr_phenol_noOrthoHbond fr_nitro fr_nitro_arom fr_imide fr_morpholine fr_priamide fr_Ar_COO fr_guanido fr_sulfone fr_amidine fr_hdrzine fr_hdrzone fr_nitro_arom_nonortho fr_alkyl_carbamate fr_C_S fr_quatN fr_aldehyde fr_oxazole fr_lactone fr_N_O fr_HOCCN fr_oxime fr_tetrazole fr_term_acetylene fr_phos_acid fr_phos_ester fr_SH fr_epoxide fr_lactam fr_azo fr_azide fr_dihydropyridine fr_nitroso fr_barbitur fr_isocyan fr_isothiocyan fr_benzodiazepine fr_thiocyan fr_diazo fr_prisulfonamdTotal count across all molecules0 2 4 6 81e7fr_NH0 fr_benzene fr_C_O fr_C_O_noCOO fr_NH1 fr_ether fr_amide fr_Ar_N fr_halogen fr_bicyclic fr_aniline fr_aryl_methyl fr_methoxy fr_NH2 fr_pyridine fr_para_hydroxylation fr_Al_OH fr_ester fr_Al_OH_noTert fr_Ndealkylation2 fr_alkyl_halide fr_COO2 fr_piperdine fr_COO fr_allylic_oxid fr_Nhpyrrole fr_Ar_NH fr_unbrch_alkane fr_ketone fr_sulfide fr_Al_COO fr_Ndealkylation1 fr_sulfonamd fr_ketone_Topliss fr_imidazole fr_thiophene fr_ArN fr_piperzine fr_Imine fr_nitrile fr_thiazole fr_Ar_OH fr_urea fr_phenol fr_furan fr_phenol_noOrthoHbond fr_nitro fr_nitro_arom fr_imide fr_morpholine fr_priamide fr_Ar_COO fr_guanido fr_sulfone fr_amidine fr_hdrzine fr_hdrzone fr_nitro_arom_nonortho fr_alkyl_carbamate fr_C_S fr_quatN fr_aldehyde fr_oxazole fr_lactone fr_N_O fr_HOCCN fr_oxime fr_tetrazole fr_term_acetylene fr_phos_acid fr_phos_ester fr_SH fr_epoxide fr_lactam fr_azo fr_azide fr_dihydropyridine fr_nitroso fr_barbitur fr_isocyan fr_isothiocyan fr_benzodiazepine fr_thiocyan fr_diazo fr_prisulfonamdTotal count across all molecules0.0 0.2 0.4 0.6 0.8 1.01e6fr_NH0 fr_benzene fr_C_O fr_C_O_noCOO fr_NH1 fr_ether fr_amide fr_Ar_N fr_halogen fr_bicyclic fr_aniline fr_aryl_methyl fr_methoxy fr_NH2 fr_pyridine fr_para_hydroxylation fr_Al_OH fr_ester fr_Al_OH_noTert fr_Ndealkylation2 fr_alkyl_halide fr_COO2 fr_piperdine fr_COO fr_allylic_oxid fr_Nhpyrrole fr_Ar_NH fr_unbrch_alkane fr_ketone fr_sulfide fr_Al_COO fr_Ndealkylation1 fr_sulfonamd fr_ketone_Topliss fr_imidazole fr_thiophene fr_ArN fr_piperzine fr_Imine fr_nitrile fr_thiazole fr_Ar_OH fr_urea fr_phenol fr_furan fr_phenol_noOrthoHbond fr_nitro fr_nitro_arom fr_imide fr_morpholine fr_priamide fr_Ar_COO fr_guanido fr_sulfone fr_amidine fr_hdrzine fr_hdrzone fr_nitro_arom_nonortho fr_alkyl_carbamate fr_C_S fr_quatN fr_aldehyde fr_oxazole fr_lactone fr_N_O fr_HOCCN fr_oxime fr_tetrazole fr_term_acetylene fr_phos_acid fr_phos_ester fr_SH fr_epoxide fr_lactam fr_azo fr_azide fr_dihydropyridine fr_nitroso fr_barbitur fr_isocyan fr_isothiocyan fr_benzodiazepine fr_thiocyan fr_diazo fr_prisulfonamd
, 1} indicate the presence of group g in molecule i.Then we can define group frequencies as c g = M i=1 x i,g .</p>
<p>Table 6 :
6
Comparison of MAE (↓) across different GNN training settings on the QM9 dataset.
Property DescriptionGINE (Tuning) GINE (Frozen) GINE (Frozen MoleculeSTM)µDipole moment0.52470.96161.0927αIsotropic polarizability1.0263.19193.3589ϵHOMOHighest occupied molecular orbital energy (HOMO)0.15580.28640.357ϵLUMOLowest unoccupied molecular orbital energy (LUMO)0.14280.35550.4581∆ϵGap between ϵHOMO and ϵLUMO (Gap)0.18170.40030.3997<R 2 >Electronic spatial extent24.721594.5913103.2612ZP V E Zero point vibrational energy0.04710.30560.234U0Internal energy at 0K9,474.9910,302.6710,176.77UInternal energy at 298.15K10,160.1210,550.0710,134.84HEnthalpy at 298.15K10,295.3210,466.0810,057.47GFree energy at 298.15K9,596.5910,278.7210,142.24cvHeat capavity at 298.15K0.50531.19651.4149U ATOM 0Atomization energy at 0K0.97133.66153.2477U ATOMAtomization energy at 298.15K0.84423.46313.309H ATOMAtomization enthalpy at 298.15K0.9993.63083.2287G ATOMAtomization free energy at 298.15K1.02253.53173.4369ARotational constant0.92530.72831.0479BRotational constant0.15150.24280.2511CRotational constant0.07730.1720.1368with eight attention heads, thereby maximizing GPU throughput. Within TokenGT, the node-andedge-projection dimensions are both 64, and we adopt the graph laplacian eigenvector variant fornode positional encoding.
Pre-training GINE and TokenGT are pre-trained with the same set of hyperparameters.For SELFIES reconstruction, sequences are truncated to a maximum length of 512 tokens; tokens beyond this limit do not contribute to the loss calculation.The GPT-2 decoder used for reconstruction consists of six layers, eight attention heads, and an embedding size of 1024.We train for 50 epochs with a learning rate of 1 × 10 −4 , a batch size of 64, and the AdamW optimizer.Training is performed on the 5M molecule dataset described in Appendix B.1, further augmented only by adding the corresponding SELFIES strings, and all experiments are run on four NVIDIA A100 GPUs.</p>
<p>Table 7 :
7
Model hyperparameters used for Mol-LLM architecture, evaluation, and training stages.keys from the present MACCS keys to remove from the original molecular graph.Subsequently, other random keys are chosen from the list of absent MACCS keys, and functional groups corresponding to the selected MACCS keys are attached at a random position in the molecule.We set the number of MACCS keys randomly selected to 30 percent of the number of each molecule's present MACCS keys.This method effectively alters molecular structural information without task-specific design, at the same time, it does not require heavy computation.
(a) Q-Former, LoRA, evaluation(b) Training hyperparameters for each stageParameterValueParameterStage 1 Stage 2Stage 3Q-Formermax_length512bert_hidden_dim768batch_size96810241024bert_namescibert [52]optimizeradamwnum_query_token32schedulerlinear_warmup_cosine_lrbert_layers5weight_decay0.05LoRA lora_r lora_alpha lora_dropout Evaluation gen_max_len num_beams64 32 0.1 256 1min_lr init_lr warmup_lr warmup_epochs gradient_clip_val precision c λ margin10 −4 10 −5 NA NA10 −5 10 −4 10 −5 0.25 0.5 bf16-mixed NA NA4 × 10 −5 4 × 10 −6 0.25 0.25λ clipNANA1.0</p>
<p>Table 8 :
8
Details of Mol-LLM instruction-tuning training data and its sources.This section describes the construction details of our molecular instruction-tuning dataset, whose statistics are described in Table 8.It covers 21 tasks grouped into eight categories, comprising about 3.3M training and 40K test instances.
TaskData Sources# Train # Test# AllProperty Prediction (Regression)MoleculeNet [41]359,556 2,519362,075Property Prediction (Classification)MoleculeNet [41]59,607 7,46067,067Forward Reaction PredictionUSPTO [34]1,079,379 5,062 1,084,441RetrosynthesisUSPTO 500MT968,943 5,156974,099Reagent PredictionUSPTO 500K121,896 1,000122,896Molecule CaptioningChEBI-20 [53]58,763 5,79364,556Description-Guided Molecule Generation ChEBI-2058,763 5,83864,601Name ConversionPubChem [54]599,767-599,767Overall3,306,674 40,757 3,347,431C Molecular Instruction-tuning Dataset</p>
<p>Table 9 :
9
Numbers of training and evaluation of impactful molecular tasks, which consist of property classification, property regression, reaction prediction, molecule generation, and molecule captioning, of each model.BioT5+ comprises two separate models, each trained on a distinct group of tasks.
Model# Train Tasks # Eval TasksBioT5+ [6] (Mol-Instructions)66BioT5+ (ChEBI-20)66LlaSMol [5]1010Mol-LLM (Ours)2315</p>
<p>Table 10 :
10
Summary of baseline models categorized by their input modality and model type.
ModelInput ModalityTask Coverage</p>
<p>datasets.FS, RS, RP each represent Forward synthesis, Retrosynthesis, and Reagent prediction.
Task DatasetModelEXACT (↑) BLEU (↑) RDK FTS (↑) MACCS FTS (↑) MORGAN FTS (↑) VALIDITY (↑)Specialist ModelsInstructMol MolCA  <em>0.536 0.0000.967 0.3210.776 0.3290.878 0.4940.741 0.2531.00 0.01Semi-Generalist Models Mol-Instructions  *  BioT5+  *  (Cls. &amp; Trans.) BioT5+  *  (Reg. &amp; React.)0.052 0.000 0.8640.302 0.206 0.9930.232 0.081 0.9490.291 0.152 0.9750.197 0.069 0.9351.00 0.98 1.00Mol-InstructionsGeneralist ModelsGPT-4 (5-shot)0.0210.5800.6270.7280.5570.93Galactica 3D-MoLM  *  ChemDFM  *  LlaSMol  </em>0.000 0.000 0.000 0.7430.468 0.081 0.028 0.8350.156 0.223 0.104 0.9200.257 0.391 0.142 0.9550.097 0.098 0.077 0.9100.95 0.01 0.07 0.95Mol-LLM (w/o Graph)0.8930.9630.9680.9830.9601.00FSMol-LLM0.9110.9690.9760.9870.9671.00Specialist Models MolCA  <em>0.0000.2090.2520.3570.1960.01Semi-Generalist Models Mol-Instructions  *  BioT5+  *  (Cls. &amp; Trans.) BioT5+  *  (Reg. &amp; React.)0.003 0.000 0.0810.149 0.286 0.4550.139 0.107 0.4180.184 0.187 0.5370.111 0.089 0.3761.00 0.97 1.00SMolInstructGeneralist ModelsGPT-4 (5-shot) Galactica  *  3D-MoLM  *  ChemDFM  *  LlaSMol  </em>0.011 0.000 0.000 0.002 0.6290.451 0.241 0.086 0.046 0.8830.520 0.292 0.226 0.125 0.8710.634 0.377 0.296 0.178 0.9190.440 0.202 0.117 0.109 0.8480.87 0.36 0.01 0.08 0.99Mol-LLM (w/o Graph)0.5840.8670.8470.9040.8151.00Mol-LLM0.6010.8730.8530.9080.8231.00Specialist ModelsInstructMol MolCA  <em>0.407 0.0000.941 0.6520.753 0.9360.852 0.8800.714 0.7221.00 0.01Semi-Generalist Models Mol-Instructions  *  BioT5+  *  (Cls. &amp; Trans.) BioT5+  *  (Reg. &amp; React.)0.069 0.001 0.6420.407 0.095 0.9690.303 0.114 0.8970.359 0.195 0.9300.268 0.104 0.8661.00 0.97 1.00Mol-InstructionsGeneralist ModelsGPT-4 (5-shot)0.0120.5730.5310.7160.5060.77Galactica 3D-MoLM  *  ChemDFM  *  LlaSMol  </em>0.000 0.000 0.000 0.4530.452 0.069 0.224 0.7220.167 0.270 0.360 0.8260.274 0.451 0.440 0.8850.134 0.117 0.234 0.7880.99 0.01 0.03 0.95Mol-LLM (w/o Graph)0.5100.8390.8350.8860.7971.00RSMol-LLM0.5380.8450.8430.8930.8081.00Specialist Models MolCA  <em>0.0000.5030.7160.7600.5890.01Semi-Generalist Models Mol-Instructions  *  BioT5+  *  (Cls. &amp; Trans.) BioT5+  *  (Reg. &amp; React.)0.015 0.000 0.1520.402 0.085 0.6620.223 0.095 0.6230.285 0.170 0.7510.191 0.085 0.5671.00 0.97 1.00SMolInstructGeneralist ModelsGPT-4 (5-shot) Galactica  *  3D-MoLM  *  ChemDFM  *  LlaSMol  </em>0.013 0.000 0.000 0.000 0.3230.523 0.346 0.162 0.257 0.7590.499 0.341 0.220 0.304 0.7490.686 0.447 0.372 0.443 0.8270.465 0.272 0.128 0.252 0.6990.76 0.43 0.01 0.03 0.99Mol-LLM (w/o Graph)0.3630.7720.7520.8280.6991.00Mol-LLM0.3770.7790.7600.8320.7071.00Specialist ModelsInstructMol MolCA  <em>0.129 0.0000.610 0.0020.444 0.0330.539 0.1150.400 0.0121.00 0.01Semi-Generalist ModelsMol-Instructions BioT5+  *  (Cls. &amp; Trans.) BioT5+  *  (Reg. &amp; React.)0.044 0.000 0.2570.224 0.169 0.6950.237 0.038 0.5390.364 0.056 0.6210.213 0.015 0.5121.00 0.96 1.00RPMol-InstructionsGeneralist ModelsGPT-4 (5-shot)0.0000.1330.0770.2280.0710.72Galactica 3D-MoLM  *  ChemDFM  *  LlaSMol  </em>0.000 0.000 0.000 0.0000.141 0.042 0.014 0.0500.036 0.039 0.033 0.0410.127 0.218 0.099 0.1990.051 0.077 0.027 0.0500.99 0.01 0.06 0.93Mol-LLM (w/o Graph)0.2020.5570.4970.5860.4611.00Mol-LLM0.2250.5780.5170.6000.4851.00</p>
<p>Table 12 :
12
[5]]ormance comparison on description-guided molecule generation task on ChEBI-20[29]and SMolInstruct[5]datasets.
DatasetModelEXACT (↑) BLEU (↑) RDK FTS (↑) MACCS FTS (↑) MORGAN FTS (↑) VALIDITY (↑)Specialist ModelsGIT-Mol0.0510.7560.5820.7380.5190.93MolT50.3110.8540.7460.8340.6840.91MolXPT0.215NA0.7570.8590.6670.98Text+Chem T50.3220.8530.8160.9010.7570.94ChEBI-20Semi-Generalist Models Mol-Instructions  *  BioT5+  *  (Cls. &amp; Trans.) BioT5+  *  (Reg. &amp; React.)0.016 0.557 0.5370.042 0.931 0.8210.132 0.835 0.8310.167 0.907 0.8970.090 0.780 0.7731.00 1.00 1.00Generalist ModelsGPT-4 (5-shot) Galactica  *  3D-MoLM  *  ChemDFM  *  LlaSMol  <em>0.092 0.000 0.000 0.018 0.2740.485 0.189 0.000 0.205 0.6440.518 0.142 0.000 0.136 0.7550.745 0.264 0.000 0.165 0.8710.482 0.057 0.000 0.110 0.6790.65 0.70 0.00 0.19 0.95Mol-LLM (w/o Graph)0.4310.7920.8230.9030.7541.00Mol-LLM0.4430.7950.8290.9060.7611.00Specialist ModelsMolT50.317NA0.8020.8790.7320.95Semi-Generalist Models Mol-Instructions  *  BioT5+  *  (Cls. &amp; Trans.) BioT5+  *  (Reg. &amp; React.)0.045 0.519 0.4160.507 0.918 0.8190.366 0.822 0.7820.475 0.897 0.8670.272 0.757 0.7061.00 1.00 1.00SMolInstructGeneralist ModelsGPT-4 (5-shot) Galactica  *  3D-MoLM  *  ChemDFM  *  LlaSMol  </em>0.027 0.000 0.000 0.041 0.1800.404 0.173 0.000 0.069 0.7180.482 0.144 0.000 0.230 0.7120.726 0.271 0.000 0.297 0.8450.368 0.055 0.000 0.189 0.6230.74 0.61 0.00 0.13 0.93Mol-LLM (w/o Graph)0.3620.7590.7970.8880.7161.00Mol-LLM0.3680.7610.8000.8870.7210.99</p>
<dl>
<dt>Table 13 :</dt>
<dt>13</dt>
<dt>[5]]ormance comparison on molecule captioning task on ChEBI-20[29]and SMolInstruct[5]datasets.</dt>
<dt>ModelBLEU-2 (↑) BLEU-4 (↑) ROUGE-1 (↑) ROGUE-2 (↑) ROUGE-L (↑) METEOR (↑)Specialist ModelsGIT-Mol0.3520.2630.5750.4850.5600.533InstructMol0.4750.3710.5660.3940.5020.509MolT5 MolCA  <em>0.594 0.6230.508 0.5400.654 0.6930.510 0.5530.594 0.6310.614 0.652MolXPT0.5940.5050.6600.5110.5970.626Text+Chem T50.6250.5420.6820.5430.6220.648Semi-Generalist ModelsChEBI-20Mol-Instructions BioT5+  *  (Cls. &amp; Trans.) BioT5+  *  (Reg. &amp; React.)0.249 0.666 0.2490.171 0.591 0.2160.331 0.709 0.3870.206 0.583 0.3020.289 0.649 0.3640.271 0.680 0.323Generalist ModelsGPT-4 (5-shot) Galactica  *  3D-MoLM  *  ChemDFM  *  LlaSMol  </em>0.261 0.001 0.252 0.054 0.4320.158 0.000 0.171 0.031 0.3330.286 0.006 0.361 0.120 0.5220.188 0.000 0.184 0.049 0.3560.303 0.006 0.287 0.101 0.4640.320 0.004 0.326 0.078 0.466Mol-LLM (w/o Graph)0.5560.4820.5650.4170.5090.587Mol-LLM0.5660.4930.4930.3360.4390.599Specialist ModelsMolT5 MolCA  <em>0.462 0.5990.366 0.5100.563 0.6650.398 0.5190.501 0.6040.515 0.628Semi-Generalist ModelsMol-Instructions BioT5+  *  (Cls. &amp; Trans.) BioT5+  *  (Reg. &amp; React.)0.028 0.656 0.2570.020 0.582 0.2210.226 0.702 0.3870.160 0.576 0.3010.217 0.644 0.3640.124 0.677 0.321SMolInstructGeneralist ModelsGPT-4 (5-shot) Galactica  *  3D-MoLM  *  ChemDFM  *  LlaSMol  </em>0.220 0.002 0.244 0.057 0.4270.125 0.000 0.167 0.035 0.3280.352 0.007 0.357 0.128 0.5250.156 0.000 0.185 0.054 0.3590.273 0.006 0.285 0.108 0.4650.274 0.005 0.329 0.085 0.470Mol-LLM (w/o Graph)0.5540.4770.5440.3930.4900.585Mol-LLM0.5580.4820.4850.3300.4330.589</dt>
<dd>
<p>[Yes] Justification: We provide information on the full model architecture and training objective in Section 2. Appendix B includes further training details.Section 3 and Appendix D describe recipe for experiments reproduction.Guidelines:• The answer NA means that the paper does not include experiments.</p>
</dd>
</dl>
<p>Justification: Appendix B provides links to access source code, trained model, and data for result reproduction.In addition, the code, trained model, and data will be released publicly.Guidelines:•The answer NA means that paper does not include experiments requiring code.•Pleasesee the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.•Whileweencourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer.Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).•Theinstructionsshould contain the exact command and environment needed to run to reproduce the results.See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy)formoredetails.•Theauthors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines.If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.•At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).•Providingasmuch information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.6.Experimental setting/detailsQuestion: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).• The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
results?Answer: [Yes]Justification: All experimental settings are clearly specified. Details of data constructionand splits are in Appendix C, hyperparameters, how they were chosen, and other trainingdetails are in Appendix B, and experimental details including evaluation configurations arein Appendix D.Guidelines:• The answer NA means that the paper does not include experiments.• The experimental setting should be presented in the core of the paper to a level of detailthat is necessary to appreciate the results and make sense of them.• The full details can be provided either with the code, in appendix, or as supplementalmaterial.7. Experiment statistical significanceQuestion: Does the paper report error bars suitably and correctly defined or other appropriateinformation about the statistical significance of the experiments?Answer: [No]Justification: For generalist models, reporting results with such a statistical significancedemands substantial resources. We note that the prior work we discussed in Section 4similarly does not report statistical significance.Guidelines:• The answer NA means that the paper does not include experiments.• The authors should answer "Yes" if the results are accompanied by error bars, confi-dence intervals, or statistical significance tests, at least for the experiments that supportthe main claims of the paper.•
5. Open access to data and code Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer:[Yes]</p>
<p>https://www.rdkit.org/docs/source/rdkit.Chem.Fragments.html
Acknowledgments and Disclosure of FundingLG AI Research supported this work.This work was also supported by Artificial intelligence industrial convergence cluster development project funded by the Ministry of Science and ICT(MSIT, Korea)&amp;Gwangju Metropolitan City, the National Research Foundation of Korea(NRF) grant funded by the Korea government(MSIT) (No.RS-2024-00410082), Institute of Information communications Technology Planning Evaluation (IITP) grant funded by the Korea government(MSIT) (No.RS-2019-II190079, Artificial Intelligence Graduate School Program(Korea University); No.RS-2020-II201336, Artificial Intelligence Graduate School Program(UNIST); No. 2022-0-00612, Geometric and Physical Commonsense Reasoning based Behavior Intelligence for Embodied AI), and partly supported by the Institute of Information &amp; Communications Technology Planning &amp; Evaluation(IITP)-ITRC(Information Technology Research Center) grant funded by the Korea government(MSIT)(IITP-2025-RS-2024-00436857, 15%)NeurIPS Paper ChecklistClaimsQuestion: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?Answer: [Yes] Justification: The claims have been validated through extensive evaluation on various molecular tasks in Section 3 and Appendix D.3, comparing with a number of generalist baselines available, including in-domain and out-of-domain benchmarks.Guidelines:• The answer NA means that the abstract and introduction do not include the claims made in the paper.• The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations.A No or NA answer to this question will not be perceived well by the reviewers.• The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.• It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.LimitationsQuestion: Does the paper discuss the limitations of the work performed by the authors?Answer: [Yes] Justification: We discuss limitations in Section 5 and appendix A. Guidelines:• The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.• The authors are encouraged to create a separate "Limitations" section in their paper.• The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally).The authors should reflect on how these assumptions might be violated in practice and what the implications would be.• The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs.In general, empirical results often depend on implicit assumptions, which should be articulated.• The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting.Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.• The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.• If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.• While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper.The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community.Reviewers will be specifically instructed to not penalize honesty concerning limitations.Theory assumptions and proofsQuestion: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [NA] Justification: The paper does not include theoretical results.for what should or should not be described.
Language models are few-shot learners. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Ilya Sutskever, and Dario Amodei. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford2023NeurIPS</p>
<p>. ArXiv, abs/2303.08774OpenAI. Gpt-4 technical report. 2023</p>
<p>Gemini: A family of highly capable multimodal models. Gemini Team, Google , ArXiv, abs/2312.118052023</p>
<p>. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing , Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, 2023Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunovand Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models</p>
<p>Llasmol: Advancing large language models for chemistry with a large-scale, comprehensive, high-quality instruction tuning dataset. Botao Yu, Frazier N Baker, Ziqi Chen, Xia Ning, Huan Sun, ArXiv, abs/2402.093912024</p>
<p>Biot5+: Towards generalized biological understanding with iupac integration and multi-task tuning. Qizhi Pei, Lijun Wu, Kaiyuan Gao, Xiaozhuan Liang, Yin Fang, Jinhua Zhu, Shufang Xie, Tao Qin, Rui Yan, ArXiv, abs/2402.178102024</p>
<p>Mol-instructions: A large-scale biomolecular instruction dataset for large language models. Yin Fang, Xiaozhuan Liang, Ningyu Zhang, Kangwei Liu, Rui Huang, Zhuo Chen, Xiaohui Fan, Huajun Chen, ArXiv, abs/2306.080182023259164901</p>
<p>Molca: Molecular graph-language modeling with cross-modal projector and uni-modal adapter. Zhiyuan Liu, Sihang Li, Yancheng Luo, Hao Fei, Yixin Cao, Kenji Kawaguchi, Xiang Wang, Tat-Seng Chua, ArXiv, abs/2310.127982023</p>
<p>Instructmol: Multi-modal integration for building a versatile and reliable molecular assistant in drug discovery. He Cao, Zijing Liu, Xingyu Lu, Yuan Yao, Yu Li, ArXiv, abs/2311.162082023</p>
<p>Git-mol: A multi-modal large language model for molecular science with graph, image, and text. Pengfei Liu, Yiming Ren, Jun Tao, Zhixiang Ren, 10.1016/j.compbiomed.2024.108073Computers in Biology and Medicine. 0010-4825171108073March 2024</p>
<p>Unimot: Unified moleculetext language model with discrete token representation. Juzheng Zhang, Yatao Bian, Yongqiang Chen, Quanming Yao, 2024</p>
<p>Tat-Seng Chua, and Qi Tian. 3d-molm: Towards 3d molecule-text interpretation in language models. Sihang Li, Zhiyuan Liu, Yanchen Luo, Xiang Wang, Xiangnan He, Kenji Kawaguchi, ICLR2024</p>
<p>Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. David Weininger, 10.1021/ci00057a005J. Chem. Inf. Comput. Sci. 0095-2338281feb 1988</p>
<p>Self-referencing embedded strings (selfies): A 100string representation. Mario Krenn, Florian Häse, Akshatkumar Nigam, Pascal Friederich, Alan Aspuru-Guzik, 10.1088/2632-2153/aba947Machine Learning: Science and Technology. 2632-21531445024October 2020</p>
<p>Multi-modal molecule structure-text model for text-based retrieval and editing. Shengchao Liu, Weili Nie, Chengpeng Wang, Jiarui Lu, Zhuoran Qiao, Ling Liu, Jian Tang, Chaowei Xiao, Anima Anandkumar, 2024</p>
<p>Molecular contrastive learning of representations via graph neural networks. Yuyang Wang, Jianren Wang, Zhonglin Cao, Amir Barati, Farimani , Nature Machine Intelligence. 42022</p>
<p>Bing Su, Dazhao Du, Zhao Yang, Yujie Zhou, Jiangmeng Li, Anyi Rao, Hao Sun, Zhiwu Lu, Ji-Rong Wen, A molecular multimodal foundation model associating molecule graphs with natural language. 2022</p>
<p>Drugchat: Towards enabling chatgptlike capabilities on drug molecule graphs. Youwei Liang, Ruiyi Zhang, Li Zhang, Pengtao Xie, 2023</p>
<p>Qizhi Pei, Lijun Wu, Kaiyuan Gao, Jinhua Zhu, Rui Yan, Towards unified 3d molecule-text modeling with 3d molecular tokenization. 2024</p>
<p>Blip-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, 2023</p>
<p>Strategies for pre-training graph neural networks. Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, Jure Leskovec, International Conference on Learning Representations. 2020</p>
<p>Pure transformers are powerful graph learners. Jinwoo Kim, Dat Nguyen, Seonwoo Min, Sungjun Cho, Moontae Lee, Honglak Lee, Seunghoon Hong, Advances in Neural Information Processing Systems. 202235</p>
<p>Zihan Zhao, Da Ma, Lu Chen, Liangtai Sun, Zihao Li, Yi Xia, Bo Chen, Hongshen Xu, Zichen Zhu, Su Zhu, arXiv:2401.14818A large language foundation model for chemistry. 2024arXiv preprint</p>
<p>Deeper insights into graph convolutional networks for semi-supervised learning. Qimai Li, Zhichao Han, Xiao-Ming Wu, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence201832</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego De Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Mistral 7b. Renard Lélio, Marie-Anne Lavaud, Pierre Lachaux, Teven Stock, Thibaut Le Scao, Thomas Lavril, Timothée Wang, William El Lacroix, Sayed, 2023</p>
<p>Visual instruction tuning. Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, ArXiv, abs/2304.084852023</p>
<p>mdpo: Conditional preference optimization for multimodal large language models. Fei Wang, Wenxuan Zhou, James Y Huang, Nan Xu, Sheng Zhang, Hoifung Poon, Muhao Chen, ArXiv, abs/2406.118392024</p>
<p>Simpo: Simple preference optimization with a reference-free reward. Yu Meng, Mengzhou Xia, Danqi Chen, ArXiv, abs/2405.147342024</p>
<p>Translation between molecules and natural language. Carl N Edwards, T Lai, Kevin Ros, Garrett Honke, Heng Ji, ArXiv, abs/2204.118172022248376906</p>
<p>Nomenclature of organic chemistry: IUPAC recommendations and preferred names. A Henri, Warren H Favre, Powell, 2013. 2013Royal Society of Chemistry</p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony S Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, Robert Stojnic, ArXiv, abs/2211.09085Galactica: A large language model for science. 2022253553203</p>
<p>Aqsoldb, a curated reference set of aqueous solubility and 2d descriptors for a diverse set of compounds. Abhishek Murat Cihan Sorkun, Süleyman Khetan, Er, Scientific Data. 62019. 199491456</p>
<p>Orderly: Data sets and benchmarks for chemical reaction data. Daniel S Wigh, Joe Arrowsmith, Alexander Pomberger, C Kobi, Alexei A Felton, Lapkin, Journal of Chemical Information and Modeling. 642024</p>
<p>A novel measure for evaluating classifiers. Jinmao Wei, Xiao-Jie Yuan, Qinghua Hu, Shuqin Wang, Expert Syst. Appl. 372010</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>. Greg Landrum. Rdkit documentation. Release. 42013</p>
<p>Reoptimization of mdl keys for use in drug discovery. Burton A Joseph L Durant, Douglas R Leland, James G Henry, Nourse, Journal of chemical information and computer sciences. 4262002</p>
<p>The generation of a unique machine description for chemical structures-a technique developed at chemical abstracts service. Harry L Morgan, Journal of chemical documentation. 521965</p>
<p>Rouge: A package for automatic evaluation of summaries. Chin-Yew Lin, Text summarization branches out. 2004</p>
<p>Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. Satanjeev Banerjee, Alon Lavie, Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization. the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization2005</p>
<p>Moleculenet: a benchmark for molecular machine learning. Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S Pappu, Karl Leswing, Vijay S Pande, Chemical Science. 92176803062017</p>
<p>Graph contrastive learning with augmentations. Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, Yang Shen, Advances in neural information processing systems. 202033</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Journal of machine learning research. 211402020</p>
<p>Molxpt: Wrapping molecules with text for generative pre-training. Zequn Liu, Wei Zhang, Yingce Xia, Lijun Wu, Shufang Xie, Tao Qin, Ming Zhang, Tie-Yan Liu, ACL. 2023</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 1892019</p>
<p>Mollm: a unified language model for integrating biomedical text with 2d and 3d molecular representations. Xiangru Tang, Andrew Tran, Jeffrey Tan, Mark B Gerstein, Bioinformatics. 402024</p>
<p>Direct preference optimization: Your language model is secretly a reward model. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, Chelsea Finn, Advances in Neural Information Processing Systems. A Oh, T Naumann, A Globerson, K Saenko, M Hardt, S Levine, Curran Associates, Inc202336</p>
<p>Detecting and mitigating hallucination in large vision language models via fine-grained ai feedback. Wenyi Xiao, Ziwei Huang, Leilei Gan, Wanggui He, Haoyuan Li, Zhelun Yu, Hao Jiang, Fei Wu, Linchao Zhu, ArXiv, abs/2404.142332024</p>
<p>Aligning modalities in vision large language models via preference fine-tuning. Yiyang Zhou, Chenhang Cui, Rafael Rafailov, Chelsea Finn, Huaxiu Yao, ArXiv, abs/2402.114112024</p>
<p>Strengthening multimodal large language model with bootstrapped preference optimization. Renjie Pi, Tianyang Han, Wei Xiong, Jipeng Zhang, Runtao Liu, Rui Pan, Tong Zhang, ArXiv, abs/2403.087302024268379605</p>
<p>Enhancing large vision language models with self-training on image comprehension. Yihe Deng, Pan Lu, Fan Yin, Ziniu Hu, Sheng Shen, James Zou, Kai-Wei Chang, Wei Wang, ArXiv, abs/240519716. 2024270123045</p>
<p>Scibert: A pretrained language model for scientific text. Iz Beltagy, Kyle Lo, Arman Cohan, Conference on Empirical Methods in Natural Language Processing. 2019202558505</p>
<p>Text2mol: Cross-modal molecule retrieval with natural language queries. Carl Edwards, Chengxiang Zhai, Heng Ji, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language Processing2021</p>
<p>Sunghwan Kim, Jie Chen, Tiejun Cheng, Asta Gindulyte, Jia He, Siqian He, Qingliang Li, Benjamin A Shoemaker, Paul A Thiessen, Bo Yu, Leonid Y Zaslavsky, Jian Zhang, Evan E Bolton, Pubchem 2023 update. Nucleic acids research. 2022253182955</p>            </div>
        </div>

    </div>
</body>
</html>
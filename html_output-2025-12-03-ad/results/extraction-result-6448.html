<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6448 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6448</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6448</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-127.html">extraction-schema-127</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <p><strong>Paper ID:</strong> paper-275820643</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2501.13121v1.pdf" target="_blank">Episodic Memories Generation and Evaluation Benchmark for Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Episodic memory -- the ability to recall specific events grounded in time and space -- is a cornerstone of human cognition, enabling not only coherent storytelling, but also planning and decision-making. Despite their remarkable capabilities, Large Language Models (LLMs) lack a robust mechanism for episodic memory: we argue that integrating episodic memory capabilities into LLM is essential for advancing AI towards human-like cognition, increasing their potential to reason consistently and ground their output in real-world episodic events, hence avoiding confabulations. To address this challenge, we introduce a comprehensive framework to model and evaluate LLM episodic memory capabilities. Drawing inspiration from cognitive science, we develop a structured approach to represent episodic events, encapsulating temporal and spatial contexts, involved entities, and detailed descriptions. We synthesize a unique episodic memory benchmark, free from contamination, and release open source code and datasets to assess LLM performance across various recall and episodic reasoning tasks. Our evaluation of state-of-the-art models, including GPT-4 and Claude variants, Llama 3.1, and o1-mini, reveals that even the most advanced LLMs struggle with episodic memory tasks, particularly when dealing with multiple related events or complex spatio-temporal relationships -- even in contexts as short as 10k-100k tokens.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6448.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6448.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o (in-context)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o with In-Context Memory (full-document prepended)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4o evaluated by prepending the entire synthetic book into the model prompt so it can attend to the full document within its context window; used to test episodic recall, state tracking and chronological ordering.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GPT-4o (in-context)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Vanilla GPT-4o prompted with the entire target document (short or long book) placed in the model context so the model can attend over raw text to retrieve episodic information at inference time.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>in‑context memory (context window)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>raw text of the full document (tokens placed in the prompt)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>transformer attention over the input context window during a single inference; no persistent write/update beyond the prompt</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Episodic Memory Benchmark (short book and long book)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>long‑context episodic recall / state tracking / chronological ordering</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Ranked at the top of the Critical Difference plot for the long book (GPT-4o in-context was among the highest average ranks). On single-event recall tasks reported performance ~0.81 F1 (reported in the paper for single-trace recall comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>N/A (this is the baseline without external memory); compared to RAG variants GPT-4o in-context performed competitively and was an exception to the general trend that RAG > in-context.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Optimistic F1-score (primary); Kendall's τ for chronological ordering</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Limited by context window size (performance sensitive to book size: short book ~10k tokens vs long book ~100k tokens). In-context requires large context capacity and is transient (no persistent memory across queries).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Degrades substantially as the number of events matching a cue increases (cue overload). Models including GPT-4o still hallucinate on zero-match questions; chronological ordering and multi-event recall remain poor. In-context is limited by token window and by the difficulty of aggregation across many dispersed paragraphs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Huet A., Ben Houidi Z., Rossi D. (2025). Episodic Memories Generation and Evaluation Benchmark for Large Language Models. arXiv preprint arXiv:2501.13121.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Episodic Memories Generation and Evaluation Benchmark for Large Language Models', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6448.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6448.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude-3.5 (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude 3.5 Sonnet with Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Claude 3.5 Sonnet evaluated with an external vector-store RAG pipeline (paragraph embeddings via text-embedding-3-small, top-K retrieval) to supply relevant context chunks for episodic queries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Claude-3.5 Sonnet (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Claude-3.5 as the generative model combined with an external vector index: book chunking into paragraphs, embedding with text-embedding-3-small, retrieving top-K chunks by cosine similarity and prepending them to the question before generation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval‑augmented generation (external vector store)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>dense text embeddings of paragraph chunks (text-embedding-3-small)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>similarity search (cosine) over the embedding index; retrieve top-K paragraphs per question and prepend as context; read-only retrieval (static DB), no online learned write controller during evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Episodic Memory Benchmark (long book)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>retrieval + long‑context episodic recall / multi‑trace aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Claude 3.5 Sonnet with RAG achieved one of the highest average ranks on the long book (statistically tied with GPT-4o in-context) according to the Critical Difference plot.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Claude 3.5 Sonnet in pure in‑context (without RAG) performs worse on average than its RAG configuration; paper reports RAG generally outperforms in‑context across models (except GPT-4o).</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Optimistic F1-score (primary); comparisons reported across in‑context, RAG, and fine‑tuning</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Retrieval granularity matters: paragraph-level RAG can miss information when events span paragraphs; chapter-level retrieval (larger chunks) increases coverage but yields fewer, longer chunks (tradeoff between retrieval precision and recall). RAG performance depends heavily on retriever quality and chunking strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paragraph-chunk RAG underperforms when event information is distributed across multiple paragraphs; RAG can struggle to synthesize information across multiple retrieved chunks when many events match a cue (multi-trace synthesis). Additionally, Claude-3.5 Sonnet exhibited a model-specific bug that appended spurious text to answers, negatively impacting F1 scores in many queries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Huet A., Ben Houidi Z., Rossi D. (2025). Episodic Memories Generation and Evaluation Benchmark for Large Language Models. arXiv preprint arXiv:2501.13121.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Episodic Memories Generation and Evaluation Benchmark for Large Language Models', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6448.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6448.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG (paragraph-chunk)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation with paragraph-level chunks (text-embedding-3-small)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Generic RAG pipeline used in experiments: document chunked into paragraphs, each paragraph embedded, top-K paragraphs retrieved by cosine similarity to the question and prepended to the model prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RAG (paragraph-chunk, text-embedding-3-small)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Memory strategy: chunk book into paragraphs labelled 'Chapter X, Paragraph Y', embed each chunk with text-embedding-3-small and retrieve top-K via cosine similarity per question; these retrieved chunks become the model input context.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external vector store (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>dense embeddings of paragraph text</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>cosine similarity nearest-neighbor search; top-K retrieval; prepending retrieved text to LLM prompt (read-only at inference time)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Episodic Memory Benchmark (paragraph-based RAG ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>retrieval + long‑context QA / episodic recall</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>RAG paragraph-level improved performance relative to many in‑context baselines across models, but underperformed compared to chapter-level RAG for events spanning paragraphs (Tab. E.2 / Tab.14 shows chapter-level RAG as an upper-bound).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Compared against in-context baseline: RAG paragraph-level generally outperforms in-context for many models (except some exceptions like GPT-4o), demonstrating benefits of narrowing the context.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Optimistic F1-score</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Fine retrieval granularity (paragraphs) provides focused context but can omit parts of an event that span paragraphs; coarser granularity (chapters) increases recall but increases token length of retrieved context and retrieval latency.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Fails to retrieve all relevant information when events are fragmented across multiple paragraphs; performance sensitive to chosen K and chunking boundaries; retriever may not synthesize across many retrieved chunks when multiple events match a cue.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Huet A., Ben Houidi Z., Rossi D. (2025). Episodic Memories Generation and Evaluation Benchmark for Large Language Models. arXiv preprint arXiv:2501.13121.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Episodic Memories Generation and Evaluation Benchmark for Large Language Models', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6448.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6448.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG (chapter-chunk upper bound)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation with chapter-level chunks (upper-bound RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A chapter-level RAG ablation used as an approximate upper bound: each chunk contains an entire event (chapter), retrieving top‑K chapters ensures each retrieved chunk contains a full event and improves episodic recall performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RAG (chapter-chunk)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same RAG pipeline but chunking the document per chapter (one event per chunk); top-K chapters retrieved (K up to the maximum number of events a question might reference) and prepended to the model prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval‑augmented generation (external vector store)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>dense embeddings of whole-chapter text</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>cosine similarity nearest-neighbor search; retrieve top-K chapters and prepend to prompt</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Episodic Memory Benchmark (chapter-based RAG ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>retrieval + episodic recall (upper-bound)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Chapter-based RAG improved performance relative to paragraph-level RAG (Tab.14): retrieving whole chapters increases the probability that a retrieved chunk contains the full event trace, enhancing multi-event recall.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Better than paragraph RAG in the benchmark; indicates chunking granularity is a major factor in RAG effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Optimistic F1-score</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Tradeoff between chunk size and retrieval precision: larger chunks (chapters) mean fewer retrieval items and higher token cost per retrieval; increased latency and prompt size may approach model context limits for many retrieved chapters.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Practical systems may not have neatly aligned chunks corresponding to single events; chapter-level retrieval is an idealized upper bound and may not generalize to unstructured real-world documents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Huet A., Ben Houidi Z., Rossi D. (2025). Episodic Memories Generation and Evaluation Benchmark for Large Language Models. arXiv preprint arXiv:2501.13121.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Episodic Memories Generation and Evaluation Benchmark for Large Language Models', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6448.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6448.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o-mini (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o-mini fine-tuned (parametric memory via supervised QA fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fine-tuning (parametric memory) experiment: models fine-tuned on single-event question-answer pairs so episodic facts are encoded into model parameters and used at inference without external context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GPT-4o-mini (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Naive fine-tuning on a training set composed of single-event question-answer pairs (3,199 training QA for the long book) so episodic facts are written into model parameters; inference uses only the question prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>parametric memory via fine‑tuning (model weights)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>knowledge encoded in model parameters (weights) after supervised fine‑tuning on QA pairs</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>standard autoregressive generation from a fine‑tuned model; memory is accessed implicitly via learned weights (no explicit read/write API during inference); write/update performed offline via fine‑tuning (30 epochs, reported LR multiplier 1.8 in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Episodic Memory Benchmark (fine‑tuning experiment)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>parametric episodic encoding; retrieval via generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>On single-event recall, fine-tuned GPT-4o-mini obtained F1 = 0.83 (lead in single-trace recall as reported).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Without fine‑tuning (i.e., in-context or RAG baselines), multi-event tasks often performed better than the naive fine-tuned model; fine-tuned models overfit single-event facts and underperform on multi-event questions.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Optimistic F1-score (primary); multi-event breakdowns reported (see paper)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Fine-tuning can produce strong single-fact recall (memorization) but at the cost of generalization; requires many supervised QA examples to encode facts; training cost and potential unintended parameter side-effects are non-trivial.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Naive fine-tuning overfits to single-event memorization and fails to generalize to queries that require aggregating multiple events: performance for questions with >=2 matching events drops substantially (paper reports F1 ≤ 0.37 for fine-tuned model on multi-event bins). Also unable to furnish familiarity/uncertainty signals and prone to confabulation on zero-match queries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Huet A., Ben Houidi Z., Rossi D. (2025). Episodic Memories Generation and Evaluation Benchmark for Large Language Models. arXiv preprint arXiv:2501.13121.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Episodic Memories Generation and Evaluation Benchmark for Large Language Models', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Human-like episodic memory for infinite context llms <em>(Rating: 2)</em></li>
                <li>Larimar: Large language models with episodic memory control <em>(Rating: 2)</em></li>
                <li>LongMem <em>(Rating: 2)</em></li>
                <li>GMAT: Global Memory Augmentation for Transformers <em>(Rating: 2)</em></li>
                <li>An efficient memory-augmented transformer for knowledge-intensive nlp tasks (EMAT) <em>(Rating: 2)</em></li>
                <li>Leveraging passage retrieval with generative models for open domain question answering <em>(Rating: 2)</em></li>
                <li>ROME: Locating and editing factual associations in GPT <em>(Rating: 1)</em></li>
                <li>Generalization through memorization: Nearest neighbor language models <em>(Rating: 1)</em></li>
                <li>KILT: a benchmark for knowledge intensive language tasks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6448",
    "paper_id": "paper-275820643",
    "extraction_schema_id": "extraction-schema-127",
    "extracted_data": [
        {
            "name_short": "GPT-4o (in-context)",
            "name_full": "GPT-4o with In-Context Memory (full-document prepended)",
            "brief_description": "GPT-4o evaluated by prepending the entire synthetic book into the model prompt so it can attend to the full document within its context window; used to test episodic recall, state tracking and chronological ordering.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "GPT-4o (in-context)",
            "agent_description": "Vanilla GPT-4o prompted with the entire target document (short or long book) placed in the model context so the model can attend over raw text to retrieve episodic information at inference time.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "in‑context memory (context window)",
            "memory_representation": "raw text of the full document (tokens placed in the prompt)",
            "memory_access_mechanism": "transformer attention over the input context window during a single inference; no persistent write/update beyond the prompt",
            "task_name": "Episodic Memory Benchmark (short book and long book)",
            "task_category": "long‑context episodic recall / state tracking / chronological ordering",
            "performance_with_memory": "Ranked at the top of the Critical Difference plot for the long book (GPT-4o in-context was among the highest average ranks). On single-event recall tasks reported performance ~0.81 F1 (reported in the paper for single-trace recall comparisons).",
            "performance_without_memory": "N/A (this is the baseline without external memory); compared to RAG variants GPT-4o in-context performed competitively and was an exception to the general trend that RAG &gt; in-context.",
            "has_comparative_results": true,
            "performance_metric": "Optimistic F1-score (primary); Kendall's τ for chronological ordering",
            "tradeoffs_reported": "Limited by context window size (performance sensitive to book size: short book ~10k tokens vs long book ~100k tokens). In-context requires large context capacity and is transient (no persistent memory across queries).",
            "limitations_or_failure_cases": "Degrades substantially as the number of events matching a cue increases (cue overload). Models including GPT-4o still hallucinate on zero-match questions; chronological ordering and multi-event recall remain poor. In-context is limited by token window and by the difficulty of aggregation across many dispersed paragraphs.",
            "citation": "Huet A., Ben Houidi Z., Rossi D. (2025). Episodic Memories Generation and Evaluation Benchmark for Large Language Models. arXiv preprint arXiv:2501.13121.",
            "uuid": "e6448.0",
            "source_info": {
                "paper_title": "Episodic Memories Generation and Evaluation Benchmark for Large Language Models",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Claude-3.5 (RAG)",
            "name_full": "Claude 3.5 Sonnet with Retrieval-Augmented Generation (RAG)",
            "brief_description": "Claude 3.5 Sonnet evaluated with an external vector-store RAG pipeline (paragraph embeddings via text-embedding-3-small, top-K retrieval) to supply relevant context chunks for episodic queries.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Claude-3.5 Sonnet (RAG)",
            "agent_description": "Claude-3.5 as the generative model combined with an external vector index: book chunking into paragraphs, embedding with text-embedding-3-small, retrieving top-K chunks by cosine similarity and prepending them to the question before generation.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "retrieval‑augmented generation (external vector store)",
            "memory_representation": "dense text embeddings of paragraph chunks (text-embedding-3-small)",
            "memory_access_mechanism": "similarity search (cosine) over the embedding index; retrieve top-K paragraphs per question and prepend as context; read-only retrieval (static DB), no online learned write controller during evaluation",
            "task_name": "Episodic Memory Benchmark (long book)",
            "task_category": "retrieval + long‑context episodic recall / multi‑trace aggregation",
            "performance_with_memory": "Claude 3.5 Sonnet with RAG achieved one of the highest average ranks on the long book (statistically tied with GPT-4o in-context) according to the Critical Difference plot.",
            "performance_without_memory": "Claude 3.5 Sonnet in pure in‑context (without RAG) performs worse on average than its RAG configuration; paper reports RAG generally outperforms in‑context across models (except GPT-4o).",
            "has_comparative_results": true,
            "performance_metric": "Optimistic F1-score (primary); comparisons reported across in‑context, RAG, and fine‑tuning",
            "tradeoffs_reported": "Retrieval granularity matters: paragraph-level RAG can miss information when events span paragraphs; chapter-level retrieval (larger chunks) increases coverage but yields fewer, longer chunks (tradeoff between retrieval precision and recall). RAG performance depends heavily on retriever quality and chunking strategy.",
            "limitations_or_failure_cases": "Paragraph-chunk RAG underperforms when event information is distributed across multiple paragraphs; RAG can struggle to synthesize information across multiple retrieved chunks when many events match a cue (multi-trace synthesis). Additionally, Claude-3.5 Sonnet exhibited a model-specific bug that appended spurious text to answers, negatively impacting F1 scores in many queries.",
            "citation": "Huet A., Ben Houidi Z., Rossi D. (2025). Episodic Memories Generation and Evaluation Benchmark for Large Language Models. arXiv preprint arXiv:2501.13121.",
            "uuid": "e6448.1",
            "source_info": {
                "paper_title": "Episodic Memories Generation and Evaluation Benchmark for Large Language Models",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "RAG (paragraph-chunk)",
            "name_full": "Retrieval-Augmented Generation with paragraph-level chunks (text-embedding-3-small)",
            "brief_description": "Generic RAG pipeline used in experiments: document chunked into paragraphs, each paragraph embedded, top-K paragraphs retrieved by cosine similarity to the question and prepended to the model prompt.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "RAG (paragraph-chunk, text-embedding-3-small)",
            "agent_description": "Memory strategy: chunk book into paragraphs labelled 'Chapter X, Paragraph Y', embed each chunk with text-embedding-3-small and retrieve top-K via cosine similarity per question; these retrieved chunks become the model input context.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "external vector store (RAG)",
            "memory_representation": "dense embeddings of paragraph text",
            "memory_access_mechanism": "cosine similarity nearest-neighbor search; top-K retrieval; prepending retrieved text to LLM prompt (read-only at inference time)",
            "task_name": "Episodic Memory Benchmark (paragraph-based RAG ablation)",
            "task_category": "retrieval + long‑context QA / episodic recall",
            "performance_with_memory": "RAG paragraph-level improved performance relative to many in‑context baselines across models, but underperformed compared to chapter-level RAG for events spanning paragraphs (Tab. E.2 / Tab.14 shows chapter-level RAG as an upper-bound).",
            "performance_without_memory": "Compared against in-context baseline: RAG paragraph-level generally outperforms in-context for many models (except some exceptions like GPT-4o), demonstrating benefits of narrowing the context.",
            "has_comparative_results": true,
            "performance_metric": "Optimistic F1-score",
            "tradeoffs_reported": "Fine retrieval granularity (paragraphs) provides focused context but can omit parts of an event that span paragraphs; coarser granularity (chapters) increases recall but increases token length of retrieved context and retrieval latency.",
            "limitations_or_failure_cases": "Fails to retrieve all relevant information when events are fragmented across multiple paragraphs; performance sensitive to chosen K and chunking boundaries; retriever may not synthesize across many retrieved chunks when multiple events match a cue.",
            "citation": "Huet A., Ben Houidi Z., Rossi D. (2025). Episodic Memories Generation and Evaluation Benchmark for Large Language Models. arXiv preprint arXiv:2501.13121.",
            "uuid": "e6448.2",
            "source_info": {
                "paper_title": "Episodic Memories Generation and Evaluation Benchmark for Large Language Models",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "RAG (chapter-chunk upper bound)",
            "name_full": "Retrieval-Augmented Generation with chapter-level chunks (upper-bound RAG)",
            "brief_description": "A chapter-level RAG ablation used as an approximate upper bound: each chunk contains an entire event (chapter), retrieving top‑K chapters ensures each retrieved chunk contains a full event and improves episodic recall performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "RAG (chapter-chunk)",
            "agent_description": "Same RAG pipeline but chunking the document per chapter (one event per chunk); top-K chapters retrieved (K up to the maximum number of events a question might reference) and prepended to the model prompt.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "retrieval‑augmented generation (external vector store)",
            "memory_representation": "dense embeddings of whole-chapter text",
            "memory_access_mechanism": "cosine similarity nearest-neighbor search; retrieve top-K chapters and prepend to prompt",
            "task_name": "Episodic Memory Benchmark (chapter-based RAG ablation)",
            "task_category": "retrieval + episodic recall (upper-bound)",
            "performance_with_memory": "Chapter-based RAG improved performance relative to paragraph-level RAG (Tab.14): retrieving whole chapters increases the probability that a retrieved chunk contains the full event trace, enhancing multi-event recall.",
            "performance_without_memory": "Better than paragraph RAG in the benchmark; indicates chunking granularity is a major factor in RAG effectiveness.",
            "has_comparative_results": true,
            "performance_metric": "Optimistic F1-score",
            "tradeoffs_reported": "Tradeoff between chunk size and retrieval precision: larger chunks (chapters) mean fewer retrieval items and higher token cost per retrieval; increased latency and prompt size may approach model context limits for many retrieved chapters.",
            "limitations_or_failure_cases": "Practical systems may not have neatly aligned chunks corresponding to single events; chapter-level retrieval is an idealized upper bound and may not generalize to unstructured real-world documents.",
            "citation": "Huet A., Ben Houidi Z., Rossi D. (2025). Episodic Memories Generation and Evaluation Benchmark for Large Language Models. arXiv preprint arXiv:2501.13121.",
            "uuid": "e6448.3",
            "source_info": {
                "paper_title": "Episodic Memories Generation and Evaluation Benchmark for Large Language Models",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "GPT-4o-mini (fine-tuned)",
            "name_full": "GPT-4o-mini fine-tuned (parametric memory via supervised QA fine-tuning)",
            "brief_description": "A fine-tuning (parametric memory) experiment: models fine-tuned on single-event question-answer pairs so episodic facts are encoded into model parameters and used at inference without external context.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "GPT-4o-mini (fine-tuned)",
            "agent_description": "Naive fine-tuning on a training set composed of single-event question-answer pairs (3,199 training QA for the long book) so episodic facts are written into model parameters; inference uses only the question prompt.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "parametric memory via fine‑tuning (model weights)",
            "memory_representation": "knowledge encoded in model parameters (weights) after supervised fine‑tuning on QA pairs",
            "memory_access_mechanism": "standard autoregressive generation from a fine‑tuned model; memory is accessed implicitly via learned weights (no explicit read/write API during inference); write/update performed offline via fine‑tuning (30 epochs, reported LR multiplier 1.8 in experiments)",
            "task_name": "Episodic Memory Benchmark (fine‑tuning experiment)",
            "task_category": "parametric episodic encoding; retrieval via generation",
            "performance_with_memory": "On single-event recall, fine-tuned GPT-4o-mini obtained F1 = 0.83 (lead in single-trace recall as reported).",
            "performance_without_memory": "Without fine‑tuning (i.e., in-context or RAG baselines), multi-event tasks often performed better than the naive fine-tuned model; fine-tuned models overfit single-event facts and underperform on multi-event questions.",
            "has_comparative_results": true,
            "performance_metric": "Optimistic F1-score (primary); multi-event breakdowns reported (see paper)",
            "tradeoffs_reported": "Fine-tuning can produce strong single-fact recall (memorization) but at the cost of generalization; requires many supervised QA examples to encode facts; training cost and potential unintended parameter side-effects are non-trivial.",
            "limitations_or_failure_cases": "Naive fine-tuning overfits to single-event memorization and fails to generalize to queries that require aggregating multiple events: performance for questions with &gt;=2 matching events drops substantially (paper reports F1 ≤ 0.37 for fine-tuned model on multi-event bins). Also unable to furnish familiarity/uncertainty signals and prone to confabulation on zero-match queries.",
            "citation": "Huet A., Ben Houidi Z., Rossi D. (2025). Episodic Memories Generation and Evaluation Benchmark for Large Language Models. arXiv preprint arXiv:2501.13121.",
            "uuid": "e6448.4",
            "source_info": {
                "paper_title": "Episodic Memories Generation and Evaluation Benchmark for Large Language Models",
                "publication_date_yy_mm": "2025-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Human-like episodic memory for infinite context llms",
            "rating": 2,
            "sanitized_title": "humanlike_episodic_memory_for_infinite_context_llms"
        },
        {
            "paper_title": "Larimar: Large language models with episodic memory control",
            "rating": 2,
            "sanitized_title": "larimar_large_language_models_with_episodic_memory_control"
        },
        {
            "paper_title": "LongMem",
            "rating": 2
        },
        {
            "paper_title": "GMAT: Global Memory Augmentation for Transformers",
            "rating": 2,
            "sanitized_title": "gmat_global_memory_augmentation_for_transformers"
        },
        {
            "paper_title": "An efficient memory-augmented transformer for knowledge-intensive nlp tasks (EMAT)",
            "rating": 2,
            "sanitized_title": "an_efficient_memoryaugmented_transformer_for_knowledgeintensive_nlp_tasks_emat"
        },
        {
            "paper_title": "Leveraging passage retrieval with generative models for open domain question answering",
            "rating": 2,
            "sanitized_title": "leveraging_passage_retrieval_with_generative_models_for_open_domain_question_answering"
        },
        {
            "paper_title": "ROME: Locating and editing factual associations in GPT",
            "rating": 1,
            "sanitized_title": "rome_locating_and_editing_factual_associations_in_gpt"
        },
        {
            "paper_title": "Generalization through memorization: Nearest neighbor language models",
            "rating": 1,
            "sanitized_title": "generalization_through_memorization_nearest_neighbor_language_models"
        },
        {
            "paper_title": "KILT: a benchmark for knowledge intensive language tasks",
            "rating": 1,
            "sanitized_title": "kilt_a_benchmark_for_knowledge_intensive_language_tasks"
        }
    ],
    "cost": 0.0213145,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>EPISODIC MEMORIES GENERATION AND EVALUATION BENCHMARK FOR LARGE LANGUAGE MODELS
21 Jan 2025</p>
<p>Alexis Huet 
Huawei Technologies Co., Ltd
ParisFrance</p>
<p>ZiedBen Houidi 
Huawei Technologies Co., Ltd
ParisFrance</p>
<p>Dario Rossi 
Huawei Technologies Co., Ltd
ParisFrance</p>
<p>EPISODIC MEMORIES GENERATION AND EVALUATION BENCHMARK FOR LARGE LANGUAGE MODELS
21 Jan 2025D38B0C968F1B81A742F9659AA1A48FEFarXiv:2501.13121v1[cs.CL]
Episodic memory -the ability to recall specific events grounded in time and space -is a cornerstone of human cognition, enabling not only coherent storytelling, but also planning and decision-making.Despite their remarkable capabilities, Large Language Models (LLMs) lack a robust mechanism for episodic memory: we argue that integrating episodic memory capabilities into LLM is essential for advancing AI towards human-like cognition, increasing their potential to reason consistently and ground their output in real-world episodic events, hence avoiding confabulations.To address this challenge, we introduce a comprehensive framework to model and evaluate LLM episodic memory capabilities.Drawing inspiration from cognitive science, we develop a structured approach to represent episodic events, encapsulating temporal and spatial contexts, involved entities, and detailed descriptions.We synthesize a unique episodic memory benchmark, free from contamination, and release open source code and datasets to assess LLM performance across various recall and episodic reasoning tasks.Our evaluation of state-of-the-art models, including GPT-4 and Claude variants, Llama 3.1, and o1-mini, reveals that even the most advanced LLMs struggle with episodic memory tasks, particularly when dealing with multiple related events or complex spatio-temporal relationships -even in contexts as short as 10k-100k tokens.</p>
<p>INTRODUCTION</p>
<p>Episodic memory -the ability to recall specific events grounded in time and space -is a cornerstone of human cognition.Unlike semantic memory, which stores general knowledge, episodic memory is intimately tied to time, space, and details of specific events (Tulving et al., 1972;Tulving &amp; Thomson, 1973).Both memories are declarative, activated through cues -specific triggers that can bring back other facts (semantic) or a rich recollection of past events (episodic).For instance, hearing the word "birds" can call for "fly"; while hearing "France" might remind someone of their first trip abroad, bringing back details of that entire event.</p>
<p>Episodic memory is not only vital for personal identity and coherent storytelling but also plays a crucial role in planning (Pfeiffer &amp; Foster, 2013;Ólafsdóttir et al., 2015), reasoning (Dusek &amp; Eichenbaum, 1997) and decision-making (Barron et al., 2013).It enables individuals to track the states of entities they care about, on both physical (Burgess et al., 2002) and virtual space (e.g.virtual reality (Cushman et al., 2008), digital folders Benn et al. (2015)).It further enables envisioning future scenarios through mental time travel, a process known as future episodic thinking (Atance &amp; O'Neill, 2001;Schacter &amp; Madore, 2016).The critical role of spatio-temporal processing in episodic memory is exemplified best by the hippocampus, a specialized brain region that acts as a cognitive map and spatio-temporal index (O'Keefe &amp; Nadel, 1978;Teyler &amp; DiScenna, 1986), needed for forming (and navigating through) past, present and future episodic memories (Tanaka et al., 2014;Teyler &amp; Rudy, 2007).It is within the hippocampal formation that neuroscience research has identified specialized space and time neurons: place cells, which fire when we occupy specific locations (O' Keefe &amp; Dostrovsky, 1971), but also express current, past and future locations (Moser et al., 2015); grid cells, which create a coordinate system for spatial navigation (Hafting et al., 2005); and time cells, which segment events into distinct temporal sequences (MacDonald et al., 2011).</p>
<p>Preprint</p>
<p>The above findings highlight the fundamental importance of spatial and temporal context in episodic memory processes, and support the hippocampal indexing theory (Teyler &amp; DiScenna, 1986;Teyler &amp; Rudy, 2007), according to which the hippocampus stores compressed representations of neocortical activity patterns, serving as an index to reactivate these patterns during memory recall.Additionally, research on source and reality monitoring (Johnson &amp; Raye, 1981;Johnson et al., 1993;Garrison et al., 2017) shows that memories of actually perceived events typically contain richer spatial and temporal contextual information -as opposed to memories of imagined events, making it easier to discern their origins.Thus, episodic grounding in time and space is what allows us to distinguish between what is real and what is imaginary: everything that happened in our collective agreed-upon space and time is real, and everything else is not.It is this ability to ground memories in specific spatio-temporal contexts (e.g.attributing the source a piece of knowledge came from) that gives episodic memory its power and reliability.</p>
<p>Yet, despite its fundamental significance for human cognition, episodic memory remains underexplored within Large Language Models (LLMs) research.Despite remarkable LLM capabilities, two significant drawbacks related to lack of episodic memory limit their full potential.First, LLMs tend to hallucinate, i.e., confabulate information that is coherent yet factually incorrect, i.e. not grounded in episodic reality.Second, while humans engage in many step-by-step trials and errors, continuously updating their episodic memory and knowledge, LLMs are constrained by their inability to retain information beyond their context window.This transient nature of LLMs' memory contrasts sharply with human episodic memory, which allows for the long-term storage and retrieval of detailed experiences across an entire project or even lifespan.We argue that integration of episodic memory into LLMs held potential to significantly enhance their reasoning, consistency, and factual accuracy.</p>
<p>In this paper, as a first step to assess this gap, we propose a comprehensive framework to model and evaluate episodic memory capabilities in LLMs.To the best of our knowledge, this aspect has not been previously evaluated: although more and more challenging tasks for long context comprehension are emerging, existing approaches to extending the memory capabilities of LLMs (such as retrieval-augmented generation, or various in-context extension methods), are still tested against relatively simple benchmarks (see Sec. 2).Diverging from such benchmarks, we specifically target episodic memory, with events rich of contextual information, and involving specific entities happens at specific time and space locations.We synthesize a unique episodic memory benchmark, free from contamination, that ensures coherence and control over the generated narratives and their corresponding ground truth answers.This benchmark not only evaluates how well current LLMs handle episodic tasks, but also lays the groundwork for future research aimed at incorporating dynamic, context-sensitive episodic memory into AI systems.Our contributions include:</p>
<p>Modeling episodic memory.Drawing inspiration from cognitive science, we develop a structured approach to model episodic events within LLMs, encapsulating temporal and spatial contexts, involved entities, and detailed event descriptions.</p>
<p>Benchmark code and dataset.We introduce a framework for generating synthetic episodic memory datasets1 comprising narratives of events and corresponding question-answer pairs-that could also be used to generate synthetic tasks for training purposes.We further release 11 datasets 2 , differing in size and diversity, to evaluate LLM performance across various episodic memory tasks.</p>
<p>Assessing LLM performance.We evaluate various LLMs (GPT-4, Claude, Llama 3.1, and o1-mini), assessing their performance under different configurations: in-context learning, retrieval-augmented generation and fine-tuning.We demonstrate that even the most advanced LLMs struggle with episodic memory tasks, particularly when dealing with multiple related events or complex spatio-temporal relationships, even for very small context size -confirming the need for episodic memory benchmarks like the one we propose to further improve LLM performance.We further showcase the inadequacy of existing fine-tuning strategies for embedding episodic knowledge into LLMs, demonstrating the need for novel training methodologies tailored to episodic memory integration.</p>
<p>RELATED WORK</p>
<p>For brevity, we summarize here and refer the reader to Appendix A for a broader overview.</p>
<p>Preprint</p>
<p>for LLMs to assess their ability to (i) recall specific episodes with details, (ii) understand entity states, (iii) comprehend temporal and spatial contexts, and (iv) avoid confabulation.</p>
<p>ENTITIES, EPISODIC EVENTS AND WORLD MODELING</p>
<p>Entities.Entities (ent j ) are fundamental subjects in the world, that can participate in or be affected by events: their attributes or relationships may change over time due to episodic events.More generally, each entity has an associated state state j,t at any discrete time t.This state evolves based on events and includes all details about the entity at a given time, such as its location and any observation such as actions or interactions.The latest state of an entity is denoted as state j,L , where L denotes the latest point in time.</p>
<p>Episodic events.Episodic events (event i ) transcend the scope of the current benchmark, and are defined as actions or observations that lead to changes in the state of the world or its entities, including the mere progression of time.Specifically, each event event i is characterized by a tuple (t i , s i , ent i , c i ), where t i represents the time at which the event occurs, s i the location where the event takes place, ent i the set of entities involved in the event, and finally c i corresponds to the event content, detailing what happened.This formulation captures the essential elements of episodic memory: what happened, where and when.</p>
<p>World model.This model of episodic events and entities aligns with the hippocampus's role in maintaining a dynamic cognitive map of the world.By tracking the states of entities across time and space, our framework mirrors the brain's process of continuously updating its representation of the environment based on new experiences.This approach not only captures the context, space and temporal properties of events, but also how these events transform our understanding of the world and its constituents.In line with the encoding specificity principle (Tulving &amp; Thomson, 1973), each event is associated with specific details that differentiate it from others: parts of these details will serve as retrieval cues during memory tasks.Now in LLMs, these episodic memory components are represented as sentences or paragraphs describing the events, and entity states are inferred from the context provided in the text.</p>
<p>SYSTEMATIC TASK DESIGN: CUE-BASED RECALL AND RETRIEVAL</p>
<p>We model cue-based recall as a key-value retrieval system, where the cue (key) is any combination of elements from the event tuple (t i , s i , ent i , c i ), and the associated event details serve as the memory trace (value).By systematically varying the cues, we can assess the model's ability to retrieve specific information based on different aspects of the events.memory events [recalled list] cue (<em>,s,</em>,*)</p>
<p>trace Entities</p>
<p>Figure 1: Memory recall process implemented in the benchmark: a set of events match a given cue, from which a list of elements are recalled (here, all entities that have been seen in a given space).</p>
<p>Cue composition and retrieval types.To create a comprehensive set of tasks, we consider all possible combinations of the event tuple elements as cues.For instance, a cue of (t, * , * , * ) would prompt the retrieval of events that occurred at a specific time t, while a cue of ( * , s, * , * ), as shown in Fig. 1, would query for events that took place at a particular location s.Tab. 1 presents example combinations of different cue compositions, the descriptions of the tasks, the types of information to be retrieved, showing a few templated questions from our actual implementation for each case (the full list is deferred to Tab. 10 in the Appendix; example question/answer pairs from the templates are shown in Appendix C).</p>
<p>Encoding specificity principle and cue overload.According to the encoding specificity principle (Tulving &amp; Thomson, 1973), the effectiveness of a retrieval cue depends on its similarity to the original encoding context.Specific cues that closely match the encoded event lead to precise retrieval, while broader cues may result in interference with other events sharing similar features, a phenomenon known as cue overload.By varying the specificity of the cues as exemplified in Tab. 1, we test the model's ability to handle both precise and ambiguous queries.For example, a highly ⋆ Consider all events that happened on {t}.Provide a list of all protagonists involved in any of these events, without describing the events themselves.specific cue like (t, s, ent, c) should retrieve a unique event with detailed information, while a broader cue like ( * , s, * , * ) may retrieve multiple events, requiring the model to list all relevant information without confusion.Note that several retrieval types (different spaces, entities, details or dates) can match a given cue: we assess the models' ability to retrieve them separately.</p>
<p>Assessing entity state and chronological tracking capabilities.Beyond recalling events and their attributes, we design tasks that require tracking the states of entities over time: the last two rows in Tab. 1 outline tasks focused on entity state tracking and chronological analysis, assessing the model's ability to understand the temporal progression and current state of entities.</p>
<p>Assessing confabulation.Finally, we evaluate confabulation including tasks that assess unfamiliarity awareness.This involves testing the model's ability to recognize when it lacks information about certain events or entities and to respond appropriately, such as indicating uncertainty or acknowledging the absence of relevant information -or fail doing so by hallucinating an answer.</p>
<p>BENCHMARK DESIGN</p>
<p>In this section, we detail the design of our episodic memory benchmark, adhering to the requirements outlined in Sec. 3. The benchmark comprises three key components: (i) the memories to encode, represented by evidence documents, (ii) a set of question-answer pairs designed to probe episodic memory, and (iii) an evaluation strategy to assess model performance.</p>
<p>We introduce a novel methodology that generates synthetic documents using a Large Language Model (LLM), structured as a coherent narrative akin to a real book.Each chapter presents a logical flow and progression of a story while while maintaining controlled ground truth information, which is strategically distributed over several paragraphs.</p>
<p>This approach distinguishes our benchmark from previous designs, such as the bAbI tasks (Weston et al., 2015), which use a world model but lack coherent storytelling, and from real books, where ground truth information cannot be controlled and data contamination is a concern.The questions in our benchmark are designed to follow the episodic memory recall process highlighted in Fig. 1: each question is based on a cue that triggers the retrieval of relevant events, from which specific information is extracted.</p>
<p>BUILDING THE MEMORIES TO ENCODE</p>
<p>We begin by constructing a static universe comprising a finite N universe = 100 set of dates (t), locations (s), entities (ent), and event contents (c).These elements are carefully curated to ensure diversity and uniqueness.From this universe, we sample N events synthetic events, each serving as the foundation for Preprint a single chapter in our book.Each event is characterized by a specific date, location, action, and entity, ensuring a unity of time and place, as well as a main content and entity.Fig. 2 schematizes the book creation process.Full details, along with an example of this process, are provided in Appendix B.</p>
<p>Universe creation Event generation</p>
<p>Chapter generation and verification Universe (Nuniverse = 100) Event 1 . . .Nevents Chapter 1 . . .Nevents Book</p>
<p>Event generation: sampling Nevents independent events, with each item of each event following a truncated geometric distribution, ensuring, e.g., that the item Ezra Edwards (e1) matches many events:</p>
<p>Chapter 1
(t 1 , s 1 , e 1 , c 1 )
Chapter 2
(t 2 , s 1 , e 1 , c 1 )
Chapter 3 To evaluate model performance across various scenarios, we vary the number of matching events for a given cue from zero to more than six (details and quantitative analysis in Appendix B.1.3).
(t 2 , s 2 , e 2 , c 1 ) ... Chapter N events (t N , s N , e 1 , c N )
To achieve this variety, we employ a geometric sampling strategy when selecting events from the universe.Specifically, we use a truncated geometric distribution to sample dates, locations, entities, and event contents.This approach ensures that some items appear multiple times across different chapters, while others appear only once.By controlling the frequency of each item, we create cues that correspond to varying numbers of events: this allows us to assess the models' ability to handle both rare and frequent occurrences, as well as their capacity to manage and retrieve multiple related events.To better illustrate this shared universe structure, we provide in Appendix C an example representing the tracking of a single entity for one of the generated document.</p>
<p>To maintain control over the placement of ground truth information within each chapter, we associate each event with an event meta-data, which specifies the number of paragraphs in the chapter and the positions of key information within those paragraphs.For example, the date might be required to appear in paragraph three and only in that paragraph.Notably, dates, locations, entities, and event contents may be reused across different chapters.</p>
<p>Each chapter is generated independently using an LLM, guided by the event and event meta-data information.During generation, the LLM may introduce additional entities interacting with the main entity; these new entities are assigned unique names to ensure consistency.Following generation, chapters are concatenated into a single document, resulting in a synthetic book that contains the memory to be encoded.To ensure coherence and validity of the generated document, we implement several verification steps:</p>
<p>Uniqueness constraints: we prevent multiple chapters from sharing identical time-space or timeentity pairs to avoid conflicts in the narrative timeline.</p>
<p>Adherence to event meta-data: direct parsing verifies that each generated chapter adheres to the meta-data's event requirements, including the placement of key elements in designated paragraphs.</p>
<p>Additional quality-control layers: we check the presence verbatim of the date, location, entity and content detail, while an LLM-based verification using boolean questions validates the chapter's unique temporal day, geographical focus, main character and main event (details in Appendix B.1.6,B.1.7).</p>
<p>We acknowledge that, following our generic episodic event definition (Sec.3), the resulting book may contain more episodic events than those explicitly included in our ground truth.However, this does not influence our ground truth answers and questions, as we focus on the events over which we have full control.Additional statistics regarding the distribution of information within the book, each chapter, and each paragraph, including comparisons between GPT-4o and Claude, are provided in Appendix B.1.12.An example of a generated chapter is also included in the appendix for reference.2, 3-5, 6+ events 180, 180, 72, 24, 0 180, 180, 108, 128, 90 4.2 BUILDING THE QUESTION-ANSWER PAIRS We implement a template-based approach to create questions aligned with our episodic memory tasks.Each question is defined by a cue composition (the trigger key identifying a set of events), a trace (the type of information to be retrieved from each remembered event), and the retrieval mode (whether all elements are needed, only the latest state, or their chronological order), as exemplified in Fig. 1 and detailed fully in Appendix B.2.</p>
<p>Preprint</p>
<p>We consider all combinations of cues and traces for the retrieval of all elements, focusing on entity cues for the latest and chronological retrievals.Examples of question templates are provided in Tab. 1 (full list available in Tab. 10 in the Appendix).The templates are populated using the dates, locations, entities, and event contents appearing in each chapter.All questions are associated with known ground truth answers, as the event details are controlled and known (examples of question/answer pairs are shown in Appendix C).To test for hallucinations, we include additional questions with empty answers, using entities or combinations of items that do not exist in the document.This allows us to assess the model's ability to handle unfamiliarity and avoid confabulation.</p>
<p>To ensure variety in the number of events to be recalled, we filter the corpus of questions to balance the number of queries that correspond to zero, one, two, three to five, and more than five events.More details on this process are provided in Appendix B.2.4.</p>
<p>EVALUATION STRATEGY</p>
<p>Our evaluation strategy employs an LLM-as-a-judge approach to assess the correctness of the model's answers.The evaluator LLM is prompted to perform two key tasks: First, it (i) identifies relevant items by extracting them as a list from the AI-generated answer for each question, allowing us to evaluate the number of predicted items.Second, it (ii) scores the relevance of these predicted answers against each ground truth item, with the sum of the scores interpreted as the number of true positives.We defer details to Appendix B.3, but in essence, we are using the LLM for simple semantic comparisons, and not as a judge making subjective assessments.Then, from these predicted and ground truth answers, we compute an optimistic F1-score bound (our primary comparison metric) using a lenient methodology detailed in Appendix B.3.2.For chronological questions, we additionally use Kendall's τ coefficient: this is applied only to answers that fully match the ground truth, allowing us to assess the correctness of the ordering within this matching set.</p>
<p>GENERATED BENCHMARK</p>
<p>Using the proposed world modeling framework, we generate two synthetic documents, referred to as the short book and the long book, summarized in Tab. 2. The long book includes 196 unique events across 37 dates, 35 locations, 34 entities and 34 event contents.We generate a total of 686 questions, balanced across cue compositions and retrieval types, to evaluate the model's episodic memory capabilities; the complexity of the questions is also controlled by varying the number of related events.Note that we explicitly limit the size of the book, since as we shall see, current state of the art LLMs start struggling with a relatively modest size 3</p>
<p>BASELINE RESULTS</p>
<p>BASELINE MODELS AND MEMORY STRATEGIES</p>
<p>As baseline models for our benchmark, we evaluate several LLMs, including GPT-4o, GPT-4o-mini, Claude 3 Haiku, Claude 3.5 Sonnet, Llama 3.1 405B (instruct) and the recent o1-mini.We consider memory in LLMs as functioning through three primary forms: (1) In-Context Memory, where information is processed within the model's context window; (2) Retrieval-Augmented Generation (RAG), where external memory is accessed through a vector database; and (3) Parametric Memory via Fine-Tuning, where memory is stored within the model's parameters.</p>
<p>For (1) in-context memory, we prepend the full document to the question, allowing the model to process the entire context.With (2) RAG, we chunk the book into paragraphs 4 , each labeled with context (e.g., "Chapter X, Paragraph Y"), and embed them using text-embedding-3-small.For each question, we retrieve the top-K paragraphs based on cosine similarity to the question's embedding and prepend them to the question as context.Lastly, (3) we fine-tune models5 using all single-event question-answer pairs as training data (details in Appendix B.2.5).In principle, this approach enables models to acquire knowledge for all benchmark questions (including multi-event questions).</p>
<p>EXPERIMENTAL RESULTS</p>
<p>We evaluate the performance of various models and memory strategies on our benchmark.We mostly report results on the long book and defer an extended set of results to Appendix E.</p>
<p>Overall performance comparison.Using the F1-score as our primary metric, we compare models and strategies across all questions.Fig. 3 shows the Critical Difference (CD) plot based on the Wilcoxon signed-rank test (Benavoli et al., 2016) between each pair of algorithm (adjusted by Holm's method) that allows to rank models according to their average performance on the long book in a principled manner.GPT-4o with in-context memory and Claude 3.5 Sonnet 6 with RAG memory achieve the highest average ranks, with no statistically significant difference between them.Notably, except for GPT-4o, models utilizing RAG generally outperform their in-context counterparts, suggesting that retrieval methods can enhance episodic memory capabilities by effectively narrowing down the relevant context for each query.As information spans several paragraphs, retrieval granularity (i.e., paragraph vs chapter) may play an important role (see ablation study in Appendix E.2).</p>
<p>Performance on recall tasks.We next test the ability of recalling episodic memory, by reporting in Tab. 3 the average F1-scores for simple recall questions as a function of the number of events that match the cue (extended analysis in Appendix E.1).</p>
<p>Avoiding confabulation.The 150 questions with 0 matching events are intentionally designed to test familiarity awareness.We see that no model achieves a perfect F1-score in avoiding hallucinations: Impact of cue specificity and cue overload.Considering questions with a single existing ground truth trace in Tab. 3, we observe a different scenario: fine-tuned GPT-4o-mini leads (F1 of 0.83) by overfitting on this type of question, with GPT-4o a close second (0.81).However, as the number of matching events increases, the cue is less specific and becomes overloaded: i.e., for questions with two or more events matching the cue, we observe a consistent decline in performance for all models (F1 ≤0.60).For the fine-tuned model, the decrease is more pronounced (F1 ≤0.37): this underscores the inability of naive finetuning to generalize beyond single-event memorization (i.e.models memorize specific answers, without developing a deeper understanding of the information).</p>
<p>Impact of context size.Similar considerations hold for the small book (deferred to Appendix E.1), where in spite of very limited context (10k tokens), performance are better but still suboptimal.Noteworthy, model rank differs, with o1-mini and GPT-4o statistically equivalent.</p>
<p>Impact of cue type.We next analyze the impact of cue type in Fig. 4 across all models (full details for GPT-4o are reported in Tab.16 in the Appendix).For different models and number of events in the ground truth, the picture reports the F1 score for different types of cues -specifically, from top to bottom context ( * , * , * , c), entity ( * , * , ent, * ), space ( * , s, * , * ), and time (t, * , * , * ).Across models, gradient is clearly visible from left to right (performance degrades for increasing number of events, as already seen in Tab. 3) and from top to bottom (performance degrades, from context, to space, to time).This interesting finding reinforces the need of benchmarks such as the one we propose.
gpt-4o cl-3.5-sonnet (rag) gpt-4o (rag) gpt-4o-mini (rag) cl-3-haiku (rag) llama-3.1 cl-3-haiku gpt-4o-mini cl-3.5-sonnet gpt-4o-mini (ftuning) o1-mini c ent s t cue
1.0 1.0 1.0 0.9 0.9 0.9 1.0 0.9 1.0 0.0 1.0 1.0 0.8 0.9 0.7 0.7 0.9 0.9 0.5 0.9 0.0 0.9 1.0 1.0 1.0 0.7 0.8 0.7 0.9 0.5 1.0 0.0 1.0 0.8 1.0 1.0 1.0 0.9 0.9 0.9 0.6 1.0 0.0 0.9 (a) 0 matching events
gpt-4o cl-3.5-sonnet (rag) gpt-4o (rag) gpt-4o-mini (rag) cl-3-haiku (rag) llama-3.1 cl-3-haiku gpt-4o-mini cl-3.5-sonnet gpt-4o-mini (ftuning)
o1-mini 0.9 0.9 0.9 0.9 0.9 0.9 0.7 0.6 0.7 0.9 0.1 1.0 0.7 0.5 0.7 0.7 0.4 0.5 0.7 0.2 0.9 0.0 0.9 0.6 0.7 0.5 0.4 0.4 0.1 0.6 0.7 1.0 0.0 1.0 0.4 0.3 0.3 0.4 0.7 0.3 0.7 0.6 0.9 0.1 Latest state recall and chronological ordering.We finally study model ability to harness state recall and chronological ordering, at a finer grain.As we early have shown, (some) models perform almost Preprint perfectly when entities participate in zero or one event, but (most) models struggle significantly with multiple events.In reason of LLM performance degradation for questions involving a larger number of events, we expect these questions to be particularly challenging and, in particular, we expect chronological ordering to be significantly more involved than latest state recall.Tab. 4 considers all queries involving more than one event, and shows the (i) fraction of exactly matched latest states, (ii) the fraction of exact matches between the predicted set and the whole ground truth set (possibly in a different order) and (iii) the Kendall's τ computed across the ground truth and the matched states for those exact matches.Results confirm the expectations, and highlight significant challenges in tracking entity states over time (≤36% for any model) and, especially, to recall all the events (≤18%).The proportion of exact matches in chronological ordering tasks is low across all models, and even when models retrieve correct events, they often fail to order them correctly (low Kendall's τ coefficients).These findings reveal significant gaps in the models' abilities to understand and process temporal relationships between events, motivating the need for future solutions to address this gap.Other ablation studies.We defer to the Appendix E detailed analyses of performance with respect to: book size (E.1),RAG granularity (E.2), number (E.3) and type (E.4) of cues and traces, book generation process (E.5), chapter chronological ordering (E.6), and event plausibility (E.7).
(b) 1 matching event gpt-4o cl-3.5-sonnet (rag) gpt-4o (rag) gpt-4o-mini (rag) cl-3-haiku (rag) llama-3.1 cl-3-haiku gpt-4o-mini cl-3.5-sonnet gpt-4o-mini(</p>
<p>SUMMARY AND LIMITATIONS</p>
<p>In this work, we draw inspiration from cognitive science to build a new episodic memory model for LLMs, generate a comprehensive benchmark, and evaluate state-of-the-art models.Our findings reveal significant gaps in the episodic memory capabilities of state-of-the-art LLMs, particularly when handling multiple related events and complex spatio-temporal relationships.These challenges reflect aspects of human memory where more distinctive cues facilitate easier retrieval.Furthermore, naive finetuning fails to achieve a deep understanding of episodic events and their intricate relations, merely overfitting to single learned facts.These challenges highlight the need for fundamentally new approaches to model design and training that more closely emulate the dynamic and contextual nature of the human episodic memory.The proposed episodic memory benchmark exhibits several desirable properties: it is contamination-free by design, scalable with low human labor, offers unambiguous cues and ground truth, and the ability to model multiple cues and events within a synthetic yet realistic narrative.However, we acknowledge limitations that open avenues for future research.</p>
<p>Temporal representation.Our benchmark relies on explicit temporal markers, which may not fully capture the nuanced ways time is expressed in natural language (e.g."yesterday", "last week", or "after the party").Future iterations should incorporate implicit and relative temporal references to further challenge the models.</p>
<p>Event independence.The independent generation of chapters, while facilitating control, does not capture the interconnected and causal nature of real-world events.</p>
<p>Limited domain scope.Our benchmark primarily involves human-like protagonists within fictional contexts.Extending the framework to include diverse domains (e.g., software projects, virtual environments) would test models' ability to generalize episodic memory capabilities.</p>
<p>Training limitations.The observed performance limitations suggest that current fine-tuning methodologies may not be optimally suited for episodic memory tasks, underscoring the need for developing new strategies for the broad scientific community.</p>
<p>A EXTENDED BACKGROUND AND RELATED WORK</p>
<p>In this section, we first draw useful parallels between LLMs and human declarative memories that will guide us in building our episodic memory benchmark for LLMs.We then briefly overview the landscape of memory benchmarks, showing how none captures the intricacies of episodic memory.</p>
<p>A.1 SEMANTIC AND EPISODIC MEMORIES: PARALLELS BETWEEN HUMAN COGNITION AND LLMS "Think of a particular cat you have seen in your previous life".The previous sentence acted as a cue that reminded you of an episode from your past, a memory trace that you recollected with the aid of the cue.This process, known as cue-based recall, is fundamental in cognitive psychology and involves retrieving stored information when provided with specific cues or prompts.Interestingly, this process bears a striking resemblance to how LLMs retrieve new text from their inner representations in response to prompts.</p>
<p>Cue-based recall is at the heart of explicit declarative memory, or the conscious recognition and recollection of facts, events, and experiences.Cognitive psychologist Endel Tulving pioneered studies on declarative memory and was the first to identify what is now accepted to be its two main components: semantic and episodic memories (Tulving et al., 1972).</p>
<p>Semantic memory refers to our general knowledge and understanding of concepts that are independent of personal experiences.It encompasses facts, language, and principles that are universally applicable.For example, knowing that cats have four legs or that water consists of hydrogen and oxygen are manifestations of semantic memory.Tulving argued that semantic memory is not tied to a specific time or place but serves as a reservoir of factual information accessible across various contexts.</p>
<p>Episodic memory, on the other hand, is intimately connected to specific events.It involves the recollection of past episodes or the ability to imagine future events, also known as future episodic thinking.Episodic memory encompasses the recall of what happened as well as the spatial and temporal context in which the event occurred and the memory trace.This subjective sense of time and place contributes to our ability to mentally time travel and re-experience past events.</p>
<p>While recent research suggests that the distinction between both memories is not as clear-cut as previously thought in terms of neural correlates (De Brigard et al., 2022), both types of memory share common underlying processes.This fluidity in the structural difference between semantic and episodic memory is increasingly viewed in the literature as an interconnected continuum rather than discrete, separate entities needing different learning processes.</p>
<p>Interesting parallels can be drawn between cue-based recall in humans and prompt-based generation in LLMs, enlightening our design of an episodic memories benchmark for LLMs.We pinpoint four crucial factors that significantly impact human memory recall and explore their potential extensions to the world of LLMs:</p>
<p>Encoding and retrieval: In humans, the encoding process is crucial for later retrieval of memories.For LLMs, this parallels the training phase, where information is initially processed and stored.Our benchmark will explore different encoding methods for LLMs, including in-context learning, fine-tuning, and retrieval-augmented generation (RAG) as baselines.</p>
<p>Cue/prompt specificity: Tulving's encoding specificity principle (Tulving &amp; Thomson, 1973) stipulates that the specificity of the context in which information is encoded determines how it can be effectively retrieved.In LLMs, this can be likened to the k-extractability metric (Biderman et al., 2023;Carlini et al., 2021), where longer contexts (i.e., more specific) make it orders of magnitude easier to extract memorized sequences verbatim (Carlini et al., 2022).We will leverage this factor in our benchmark by varying the specificity of cues in our episodic memory tasks.</p>
<p>Frequency and repetition: Humans tend to better recall information that they've encountered multiple times (Scarborough et al., 1977).This repetitive exposure reinforces neural pathways, making retrieval more efficient.Similarly, in LLMs, the most frequently present next tokens during the training phase will likely be "retrieved" during inference (McKenna et al., 2023;Kandpal et al., 2022).This can be used to induce memorization.While this aspect is not relevant for our benchmark since we do not develop new encoding methods, this can be leveraged in future work.</p>
<p>Preprint</p>
<p>Familiarity (Mandler, 1980): This rapid feeling of knowing that we previously encountered a stimulus, item, or situation, may or may not be followed by a successful retrieval of details related to this situation.LLMs, being generative processes, do not natively support such a feature.Augmenting them with this ability can help in assessing when an assertion is factual (e.g., has been really observed in training) or fake.</p>
<p>Cognitive dissonance (Festinger, 1962): Described by Festinger in 1962, and slighltly related to familiarity/novelty, cognitive dissonance refers to the discomfort felt when holding two contradictory informations as a new incoming information challenges a pre-existing belief (Festinger, 1962).In this work, we focus as first step on cue-based recall and familiarity leaving the dissonance aspect for future work.</p>
<p>Based on the above parallels, our episodic memory benchmark for LLMs will incorporate the following elements.(i) We assume an encoding phase to create episodic memories in LLMs.By leveraging the parallels between human memory and LLM functionality (Tab.5), we aim to advance the capabilities of LLMs to support solid episodic memories in addition to their general knowledge.Our benchmark will provide a comprehensive framework for evaluating and potentially improving LLMs' ability to encode and retrieve episodic memories.</p>
<p>A.2 APPROACHES TO EXTEND LLM MEMORY</p>
<p>Memory in LLMs can be conceptualized in three primary forms: (i) in-context memory, where information is processed within the model's context window, (ii) external memory accessed through vector databases (Retrieval Augmented Generation or RAG), and (iii) parametric memory stored within the model's parameters.These approaches align with the baselines we test in our episodic memory benchmark: placing the episodic information in-context, in a RAG system, or directly in the model parameters.Each method offers unique advantages and challenges for handling episodic-like information in LLMs.</p>
<p>A.2.1 IN-CONTEXT MEMORY EXTENSION</p>
<p>In-context memory refers to the information an LLM can process within its context window.Efforts to extend this capability focus on increasing the number of tokens an LLM can handle simultaneously.</p>
<p>Early approaches aimed to reduce the computational complexity of self-attention.</p>
<p>Preprint</p>
<p>While these approaches significantly extend the context window, they may not fully capture the persistent nature of episodic memories, as the information is only retained within the current context.</p>
<p>A.2.2 RETRIEVAL AUGMENTED GENERATION (RAG)</p>
<p>Retrieval Augmented Generation (RAG) approaches extend LLM memory by incorporating external knowledge sources, typically stored in vector databases.This method allows LLMs to access information beyond their parametric knowledge, potentially supporting more extensive and persistent episodic-like memories.</p>
<p>Early work by Khandelwal et al. (2019) proposed nearest-neighbor language models, interpolating a pre-trained neural language model with a k-nearest neighbors model.Lee et al. (2020) formulated knowledge-intensive tasks as phrase retrieval problems, using pre-indexed dense phrase representations.Petroni et al. ( 2020) demonstrated the effectiveness of combining a shared dense vector index with a sequence-to-sequence model across multiple tasks.Izacard &amp; Grave (2020) proposed "Fusion-in-Decoder" which uses both sparse and dense representations to fetch supportive passages before feeding them to a frozen generative model.More recently, Wang et al. ( 2023) introduced LongMem, featuring a decoupled network architecture with a frozen backbone LLM acting as a memory encoder and an adaptive residual side-network functioning as a memory retriever and reader.</p>
<p>RAG approaches offer the potential to store and retrieve large amounts of episodic information, but the quality of results depends heavily on the retriever which may struggle with complex tasks that require synthesizing information across multiple sources (similarly to when we have multiple traces which correspond to a single cue in our benchmark).</p>
<p>A. (2020) framed knowledge modification as a constrained optimization problem, finding that constrained layer-wise fine-tuning is an effective method for modifying the knowledge that Transformers learn.</p>
<p>While parametric memory offers the potential for integrated, persistent episodic-like information, we are not aware of any approach that allows to ingest episodic events into model parameters, as current knowledge editing is limited in scale to toy cases, and may have unintended side effects.</p>
<p>A.2.4 HYBRID APPROACHES</p>
<p>Hybrid approaches aim to combine the strengths of multiple memory strategies, potentially offering a more comprehensive solution for handling episodic-like information in LLMs.These methods often integrate aspects of in-context processing, external retrieval, and parametric knowledge.Gupta &amp; Berant (2020) proposed GMAT (Global Memory Augmentation for Transformers), which introduces a dense attention-based global memory to provide a consolidated view of the entire input sequence.This design allows the model to achieve memory overhead that scales linearly with sequence length, Preprint potentially supporting longer-range episodic memories.Wu et al. (2022) introduced the Efficient Memory-Augmented Transformer (EMAT), which encodes external knowledge into a key-value memory and involves novel pre-training tasks.This approach enables the model to learn when to use internal parametric knowledge versus external knowledge, potentially mimicking the interplay between semantic and episodic memory in human cognition.These hybrid approaches offer promising avenues for enhancing LLMs' ability to handle episodic information.By combining multiple memory strategies, they may be able to overcome the limitations of individual approaches and provide a more flexible and robust system for managing episodic-like memories.It would be particularly interesting to test how these hybrid approaches perform on our episodic memory benchmark.Their ability to integrate different forms of memory could potentially lead to improved performance in tasks requiring both factual recall and contextual understanding of events.Future work could involve adapting our benchmark to specifically evaluate these hybrid models, providing insights into their effectiveness for episodic memory tasks.</p>
<p>Finally, the three approaches to LLM memory -in-context extension, RAG, and parametric memoryform the basis for our baseline evaluation of our episodic memory benchmark: we test the model's ability to handle episodic information when it's placed in-context, stored in a RAG system, or encoded directly in the model parameters.</p>
<p>A.3 EVALUATING MEMORY IN HUMANS AND LLMS</p>
<p>A.3.1 HUMAN TESTS OF EPISODIC MEMORIES</p>
<p>Existing tests of episodic memory in humans often involve asking the person to recall specific events in their life or using standardized psychological tests.For example, the Autobiographical Memory Interview (AMI) (Kopelman, 1994) and the Autobiographical Interview (Levine et al., 2002) are widely used tests that involves asking the person to recall specific events from their past, which are then scored based on the level of detail and accuracy.Rivermead Behavioural Memory Test (RBMT) (Wilson et al., 1985) uses everyday scenario events to test a range of memory types, including episodic memory.Another example is the California Verbal Learning Test (CVLT) (Delis et al., 2000), which assesses episodic verbal learning and memory and can detect impairments at any point in the episodic memory process, including encoding, retrieval, and recognition of familiarity.</p>
<p>Similarly to these tests, we design tests to probe LLMs in their abilities to recall various dimensions of previously encountered episodic events.</p>
<p>Our approach of using specific cues related to time, space, people, and details aligns to an extent with established human episodic memory tests like the AMI (Kopelman, 1994) and the Autobiographical Interview (Levine et al., 2002).These tests employ structured interviews that classify memories across similar dimensions, including time periods, locations, and specific event details.Note that the Autobiographical Interview for example further categorizes such details into event happenings, perceptual information, and emotions/thoughts.In our work, we instead group all these aspects under the broader category of "event details", though it could be readily extended to systematically analyze different types of details: for example, our chapters are generated with a writing style adhering to different atmospheres (e.g.suspense, tragedy, comedy, etc.), which is akin to an emotional category.</p>
<p>A.3.2 BENCHMARKING MEMORY IN LLMS</p>
<p>Evaluating memory capabilities in LLMs has garnered significant attention, yet existing benchmarks primarily focus on simple retrieval tasks or reasoning over long contexts without capturing the nuanced aspects of episodic memory.In this section, we review current benchmarks and highlight their limitations in assessing episodic memory.</p>
<p>Retrieval-focused benchmarks.Many benchmarks assess LLMs through retrieval tasks, where models are required to find specific pieces of information within extensive contexts.The "needle-ina-haystack" paradigm (Kamradt, 2023) exemplifies this approach, testing a model's ability to locate a single piece of relevant information within irrelevant textbooks.While this assesses basic retrieval capabilities, it does not evaluate the model's understanding of temporal sequences or state changes.</p>
<p>Extensions to this paradigm involve multiple needle retrieval tasks (Reid et al., 2024;Hsieh et al., 2024;Li et al., 2024;Zhang et al., 2024), which require models to retrieve several pieces of information from a large context.However, these tasks still lack the differentiation of cues and do not incorporate Preprint temporal or spatial awareness.They remain largely retrieval exercises without testing the model's ability to comprehend and recall events as sequences that unfold over time and space.</p>
<p>Long-context question answering.Benchmarks like those proposed by (Bohnet et al., 2024) and (Zhang et al., 2024) focus on question answering over long contexts.While these tasks involve processing large amounts of text, they often reduce to retrieving relevant information without necessitating an understanding of event chronology or causality.They do not sufficiently challenge models to track entity states or reason about temporal relationships, which are critical components of episodic memory.</p>
<p>Synthetic reasoning tasks.Synthetic datasets such as the bAbI tasks (Weston et al., 2015) and its long-context extension, bAbILong (Kuratov et al., 2024), introduce reasoning tasks that require models to answer questions based on provided stories.However, these tasks often involve short context lengths and highly artificial scenarios.They may allow models to shortcut reasoning by exploiting dataset biases or patterns, thus not effectively evaluating the model's ability to utilize full context in an episodic memory sense.</p>
<p>Similarly, synthetic reasoning tasks proposed by Li et al. (2024) are limited by their artificial nature and may not generalize well to real-world episodic memory scenarios.The lack of complexity and realism in these tasks means they do not adequately test a model's capacity for episodic recall and reasoning.</p>
<p>Recently, Vodrahalli et al. ( 2024) introduced the Latent Structure Queries (LSQ) framework as first good step to address the limitations of current reasoning over long context benchmarks.Somewhat in essence close to our design philosophy, it works by embedding a hidden structure within a large context, which the model must understand and manipulate to answer queries.For example, a Python list is modified throughout a long text, and the model must track these changes to determine the list's final state.This approach tests the model's ability to track state across the entire context, going beyond simple retrieval.Our approach further systematically incorporates key elements crucial to episodic memory, such as temporal and spatial cues, rich event details, and the ability to handle varied retrieval cues based on partial information.</p>
<p>Limitations of existing benchmarks.Overall, as nicely synthesized by Vodrahalli et al. ( 2024), several limits exist in current benchmarks which could be summarized as follows.</p>
<p>• Shortcuts: Models can often find answers without utilizing the full context, undermining the evaluation of their long-term memory capabilities.• Retrieval over reasoning: Tasks prioritize retrieval over reasoning about events, states, and temporal relationships.• Data leakage: Use of existing evaluations can lead to data contamination, where models may have seen the test data during training (Zhang et al., 2024;Agarwal et al., 2024;Bohnet et al., 2024;Hsieh et al., 2024;Li et al., 2024).• Out of distribution distractors: Needle in a hay stack tasks often place the trace to be retrieved in a completely different context, biasing the retrieval task.• Labor-intensive creation: Some benchmarks require significant human effort to create and verify datasets, making it challenging to scale or adapt them for different purposes.</p>
<p>Need for an episodic memory benchmark.The limitations of existing benchmarks highlight a gap in evaluating LLMs' episodic memory capabilities, calling for a benchmark that:</p>
<p>• Incorporates temporal and spatial context: Evaluates the model's ability to understand and recall events with specific time and location details. • Tracks entity states: Assesses how well the model can monitor changes in entities over time, reflecting real-world dynamics.• Uses varied retrieval cues: Tests the model's ability to recall information based on different combinations of event attributes, mirroring human cue-based recall.• Avoids data leakage: Ensures that the evaluation is free from contamination, providing a fair assessment of the model's capabilities.• Balances complexity and realism: Provides tasks that are both challenging and representative of real-world episodic memory demands without being overly synthetic.</p>
<p>This section provides a comprehensive description of the benchmark design and complements Sec.We generate the components of the universe with the help of Claude 3.5 Sonnet.For reference, we provide the process for building the list of locations in the Listing 2. Our general strategy was to first generate twice the number of targeted elements, and then to filter the duplicates and other incorrect elements.</p>
<h1>Listing 3: Excerpt of the universe.Each list has N universe = 100 elements.</h1>
<p>B.1.3 EVENTS GENERATION</p>
<p>We define an event as a date, a spatial location, a main entity, an event content, with its details that we would like to insert verbatim into the document.Each event is hence represented by a 5-tuple (t, s, ent, c, d).We provide in Listing 4 the four first events produced using our fixed seed.Listing 4: Excerpt of the first four events.</p>
<p>In addition to the seed, two parameters are used for generating the events: the number of events N events and a distribution D with finite support {0, . . ., N universe − 1}.Each event i is generated by sampling independently a temporal, a spatial, an entity, and a content among the respective lists of the universe and according the same distribution D. Each content detail is then sampled uniformly at random among the content details of the selected content.</p>
<p>Two constraints govern our actual generation.First, (i) we do not allow multiple events to have identical (t, ent) or (t, s) pairs (by initially generating more events before filtering them), since this might imply that the same entity is experiencing two events at the same time, or that two events are happening in the same location at the same time.Second, for convenience of our experiments, (ii) we ensure that the choice of two different number of events N 1,events &lt; N 2,events leads to the same first N 1,events produced events, given the other parameters set equal.In our experiments we used N 1,events = 20 and N 2,events = 200; allowing us to generate a short and a long book, where the short book's events are included in the long one.</p>
<p>As earlier mentioned, the distribution D is a critical parameter to ensure that some dates, locations, entities and contents appear in multiple generated events, while others appear only once.This allows us to vary the number of traces or events that correspond to a given cue.We select the truncated geometric distribution with parameter p = 0.1, that is, for i ∈ {0, 1, . . .N universe − 1},
P (X = i) = (1 − p) i p/ 1 − (1 − p) Nuniverse .
Note that choosing a uniform distribution would result instead in a low probability of selecting multiple times the same item (e.g., the same date t).This is experimentally confirmed in Tab. 6.</p>
<p>In this experiment, we repeat 10000 times the selection of N events with the distribution D among a universe of size N universe , and report the expected counts (and standard deviation) in selecting one, two, three to five, or more than six times a specific item.In both cases, we show that the counts are better spread within the different bins for the geometric distribution.</p>
<p>B.1.4 EVENT META-DATA GENERATION</p>
<p>We associate event meta-data to each event, providing contextual information and constraints regarding the generation of the textual chapter.Each event meta-data indicates the targeted number of Table 6: Expected counts in selecting one, two, three to five, or more than six times a specific item (e.g., date) for a universe of size N universe = 100.For instance, the value 2 in the bottom-right cell indicates that we expect two dates to be repeated each more than 6 times among the 200 generated events, using the uniform distribution.The actual counts in our experiments is shown in Tab. 9. paragraphs, and the position of the different event features within those paragraphs.Additionally, a desired style is indicated.We plan to use the style in future iterations of the benchmark to generate questions about the general atmosphere of the events, as opposed to specific tokens.We provide in Listing 4 the meta data associated with the four events, produced using our fixed seed.Other characters can appear during the generation of the chapter, under the form $entity_X (as stated in Listing 7, and verified using direct verification).For filling the name of those secondary entities, additional 100000 full names are generated from two lists of 500 first and 1000 last names.Those names are disjoint from the lists used for generating the main entities.In each validated chapter, a new name is used for each indexed entity.This ensures that all the secondary entities only appear in a single chapter.The first elements of the list are shown in Listing 9.</p>
<p>Bin of counts</p>
<p>['Noa Middleton', 'Mara Ledbetter', 'Sienna Hamrick', 'Reid Blunt', ...] Listing 9: Excerpt of the list of additional entities.</p>
<p>B.1.9 DOCUMENT ASSEMBLY</p>
<p>For building the whole document, the candidate chapter generation process is applied on all events {0, . . .N events − 1}.If the candidate does not pass the validation (either by the direct of by the LLM-as-a-judge round), a new candidate is generated until itermax = 10.We discard all the events that fail to have a valid candidate chapter after itermax.</p>
<p>The valid candidates are reshaped into 'Chapter {i}\n\n{generated_chapter}\n\n\n' (where i begins from 1) and concatenated into a single document.</p>
<p>B.1.10 CHAPTER EXAMPLE</p>
<p>We provide in Listing 10 the first generated chapter.The generated chapter has been assessed as valid (by both verification method) at the first iteration.It corresponds to the event 0 for which the values are (as indicated in Listing 4): ('September 13, 2025', 'Bethpage Black Course', 'Ezra Edwards', 'Parkour Workshop', 'Demonstrated cat leaps') and the meta-values are (as indicated in Listing 5):</p>
<p>{'nb_paragraphs': 7, 'idx_paragraph': {'location': 2, 'date': 7, 'entity': 2, 'content': 2}, 'style': 'thriller'}.</p>
<p>Preprint</p>
<p>The secondary entities for this chapter are (as indicated in Listing 9): ['Noa Middleton', 'Mara Ledbetter'].</p>
<p>The generated chapter has 547 tokens.</p>
<p>The harsh glare of the floodlights cast long shadows across the rugged terrain.Adrenaline coursed through his veins as he surveyed the daunting obstacles before him.The parkour workshop had drawn a diverse crowd, each participant eager to push their limits and master the urban art of movement.He flexed his fingers, anticipation building in his chest as he prepared for the night's challenges.</p>
<p>At Bethpage Black Course, Ezra Edwards demonstrated cat leaps with a grace that belied the difficulty of the maneuver.The other participants watched in awe as he effortlessly cleared the gap between two towering structures.Noa Middleton, the lead instructor, nodded approvingly, a hint of a smile playing at the corners of their mouth.</p>
<p>As the workshop progressed, the air grew thick with tension.The obstacles became increasingly complex, testing the limits of even the most seasoned traceurs.He felt a bead of sweat trickle down his spine as he approached the next challenge − a series of precarious platforms suspended high above the ground.</p>
<p>The distant hum of crickets provided an eerie soundtrack to the scene.</p>
<p>Mara Ledbetter stumbled on a particularly tricky jump, their cry of alarm piercing the night air.He instinctively reached out, his fingers barely grazing their arm as they teetered on the edge of the platform.Time seemed to slow as Mara Ledbetter regained their balance, their eyes wide with fear and gratitude.The near−miss sent a shiver through the group, a stark reminder of the risks they were taking.</p>
<p>As midnight approached, the workshop took on a more sinister tone.Noa Middleton announced the final challenge − a complex route through a maze−like structure shrouded in darkness.The participants exchanged nervous glances, the thrill of the unknown mingling with a growing sense of unease.He took a deep breath, steeling himself for what lay ahead.</p>
<p>The maze was a cacophony of creaks and groans, every shadow seeming to hide a potential threat.He moved swiftly, his muscles burning with exertion as he vaulted over walls and slid under low−hanging beams.A muffled cry from somewhere in the darkness sent his heart racing.Was it merely another participant, or something more sinister?</p>
<p>As he emerged from the maze, panting and exhilarated, the first rays of dawn began to peek over the horizon.</p>
<p>The workshop had pushed him to his limits, testing not just his physical abilities but his mental fortitude as well.As the group gathered for a final debrief, he couldn't shake the feeling that something fundamental had shifted during the long night of September 13, 2025.The parkour workshop may have ended, but he sensed that a new, more dangerous game was just beginning.</p>
<p>Listing 10: Chapter 1 generated with Claude 3.5 Sonnet (event 0, iteration 0).Event information and secondary entities have been highlighted.</p>
<p>B.1.11 BOOK EXAMPLE</p>
<p>We provide in the Listing 11 an illustration of how the full book generated with N events = 200 with Claude 3.5 Sonnet looks like.The generated book has 102870 tokens.</p>
<p>Chapter 1</p>
<p>The harsh glare of the floodlights cast long shadows across the rugged terrain.Adrenaline coursed through his veins as he surveyed the daunting obstacles before him.[...] The parkour workshop may have ended, but he sensed that a new, more dangerous game was just beginning.</p>
<p>Chapter 2</p>
<p>The air shimmered with an otherworldly energy as she stepped onto the glittering runway.[...] It was September 22, 2026, and on this night, she had woven dreams into reality, leaving an indelible mark on the tapestry of time.</p>
<p>[...]</p>
<p>Preprint</p>
<p>Chapter 196</p>
<p>The evening air crackled with anticipation as she stepped onto the gravel path, her camera bag slung over her shoulder.[...] With a deep breath, she steeled herself for the investigation to come, knowing her keen eye and analytical mind would be put to the test in ways she never anticipated when she arrived at this seemingly innocent photography exhibition.</p>
<p>Listing 11: Complete document generated with Claude 3.5 Sonnet with N events = 200.There are only 196 chapters because 4 events did not pass the verifications after 10 iterations.Ellipses are indicated with between brackets.</p>
<p>B.1.12 GENERATION STATISTICS</p>
<p>We extract different statistics of the overall process for creating the document.In Tab. 7, we show the number of validated chapters after each iteration.The LLM-as-a-judge verification is applied only if the candidate chapter has passed the direct verification.We observe that most of the events pass after a few iterations.We didn't observe any significant characteristic for the four discarded events, for instance, the number of paragraphs is (2, 9, 1, 2) for Claude 3.5 Sonnet,and (9,8,2,6) for GPT-4o.</p>
<p>In Tab. 8, we show the distribution of the number of paragraphs, of the styles, and of the number of secondary entities for the generated book.The distribution is shown for Claude 3.5 Sonnet, but the distribution for GPT-4o is almost the same since the original events are equal (the only difference is related to the four discarded events, that are different).We observe in Fig. 5 the relative position of the event features (date, location, entity, content details) in the text with respect to the whole book, the chapter, and the paragraph.At the book level (top), the different features are all almost uniformly spread.At the chapter level (middle), the constraints in the paragraph position ensure that the information is relatively well spread among each chapter.The discrepancy is due to the preference of the models to generate the event features at the beginning of the paragraph (bottom).We note that the bias at the paragraph level is strong for the entity name, while almost nonexistent for the verbatim content detail.Slight differences are appearing between Claude 3.5 Sonnet (left) and GPT-4o (right).</p>
<p>In Tab. 9, we show the actual counts of the number of times a specific event feature is repeated over the different chapters.The observed values are in accordance with the expected counts of Tab. 6.For instance, the highlighted value of 13 corresponds to 13 different dates We provide in Tab. 10 the full list of question templates.Each question is an episodic memory task based on a cue composition (the trigger enabling a set of events to be remembered), a trace (the retrieval type that needs to be extracted from each remembered event) and the information to get (whether all events are retrieved, only the latest state, or their chronological order).We consider all the combinations of cues and traces for the retrieval of the whole information (ids 0 to 29), while we focus on the entity cues for the latest and chronological retrievals.In the table, the cue has four components: t (the time), s (the space), e (the entity) and c (the event content).The trace can be one of those four components (Times, Spaces, Entities, or Contents), the list of the secondary entities (Other entities), or the full details (Full Details) regarding a specific chapter.The elements to get are all the retrieved elements (all), the latest one (latest) or the chronological order of the elements Preprint The questions are created given (i) the question/answer templates and (ii) the ground truth regarding all the events of the document built in Sec.B.1.</p>
<p>For each chapter, we retrieve the ground truth tuple (t, s, e, c) regarding the date, the location, the main entity, and the event content.We then generate all the questions by following the template.After building all the questions for all the chapters, we filter the duplicates.</p>
<p>We obtain 564 questions for the documents produced with N events = 20 (filtered from 684 = 36 ids × 19 chapters questions) and 3886 questions for the documents produced with N events = 200 (filtered from 7056 = 36 ids × 196 chapters questions).</p>
<p>We provide an example in Tab.11 ("Non-empty" row).</p>
<p>B.2.3 LIST OF QUESTIONS WITH EMPTY ANSWERS GIVEN A BOOK</p>
<p>In addition to the questions for which the answer can be found in the book, we design questions with empty answers.We employ two strategies: the inner strategy and the outer one.We first initially define the unused static universe as the subset of the static universe that has not been used during the document generation, and which is expected, according to Tab. 6, to contain 66 elements for each event feature (time, space, location, content) when N universe = 100, N events = 200, and D = geometric.</p>
<p>Then, to create the fake event, we corrupt a tuple (t, s, e, c) from an existing chapter, by applying an i.i.d.binary mask (b t , b s , b e , b c ), where each component follows a Bernoulli distribution with probability 1/2.When the mask is applied, we replace the corresponding feature (e.g., the date t = September 13, 2025) by the same kind of feature, either sampled from another chapter (inner strategy) or from the unused static universe (outer strategy).The resulting tuple (t ′ , s ′ , e ′ , c ′ ) is then replaced in the template questions, in the same manner as Sec.B.2.2.</p>
<p>The process is applied for both inner and outer strategies once for each chapter.The resulting questions are also filtered to keep only those with empty answers.</p>
<p>We obtain 438 additional questions for the documents with N events = 20 and 3657 questions for the documents with N events = 200.</p>
<p>We provide an example of each strategy in Tab.11 ("Empty (inner)" and "Empty (outer)" rows).The number of all possible questions is large and increases significantly as a function of N events .Since the evaluation cost scales with the number of questions, we decided to extract only a subset of the overall questions.</p>
<p>Preprint</p>
<p>First, we consider the number of events that should be remembered for answering each question.We group the questions into five bins: {0} (the empty questions), {1} (the question that triggers a single chapter), {2}, {3, 4, 5} and {6+}.In the example provided in Tab.11 ("Non-empty" row), the question is put into the {3, 4, 5} group (this group is also noted 3-5).For the questions involving the latest state, we consider all the chapters triggered by the cue for considering the group.We expect that the more chapters are involved, the more difficult the question is.</p>
<p>We then target the extraction of N target = 5 questions for each of the 36 question ids and for each of the 5 bins, resulting in up to 900 questions per experiment.</p>
<p>This method ensures that the questions are relatively well spread among the different bins, as shown in Tab.12.In this table, we show the final number of questions obtained for each cue and each bin, for both N events = 20 (left) and N events = 200 (right).Each cue is associated to a set of question ids.</p>
<p>For instance, the cue ( * , * , e, * ) is associated to the ids 6, 7, 8, 30, 31, 32, 33, 34, 35.We observe that we always retrieve the 5 target questions for bins {0} and {1}.For the bin with {6+} with N events = 20, we know for sure that no question exists since a single cue has been associated to at most three chapters (as seen in Tab.9).For the single cue recall (first four rows), most of the bins are filled.For the multiple cues (from row five), it is difficult with N events = 20 to cover all the combinations, while most of the bins are filled with N events = 20.The others cues (last row) involve the pair (t, s) or (t, e), for which we imposed at most one extracted chapter (since we forbid in the document generation the possibility of two chapters involving the same location at the same time, or the same entity at the same time), hence all the questions are necessarily in the bins {0} or {1}.</p>
<p>At the end, we end up with 456 questions for N events = 20, and 686 questions for N events = 200.</p>
<p>Preprint B.2.5 SELECTION OF THE FINE-TUNING QUESTIONS</p>
<p>Our naive fine-tuning experiment aims to incorporate the essential information needed to generalize answers across all benchmark questions.Therefore, we restrict our training data to question-answer pairs that correspond to single events (each event being associated with a single chapter).These pairs establish basic facts (e.g., taking the example of Fig. 6, "Jackson Ramos attended a carnival in Central Park on September 22, 2026" related to the event of Chapter 163, "Jackson Ramos did a flash mob at Ellis Island on April 09, 2026" related to the event of Chapter 96, etc.), which in principle enables deducing answers to questions involving multiple events, such as the set of all locations involving Jackson Ramos: {Central Park, High Line, One World Trade Center, Ellis Island, Snug Harbor Cultural Center} (question template id 7 in Tab.10).</p>
<p>The fine-tuning process thus uses 3,199 training questions for the long book (filtered from the total 3,886 questions involving one or several events, as mentioned in Sec.B.2.2) and 468 training questions for the short book.</p>
<p>Note that regarding the testing data for the fine-tuning experiment, we use the set of questions indicated in Sec.B.2.4 (as for the other experiments).Specifically, all questions involving a single chapter (i.e., corresponding to the bin {1}) are present in both the training and the test sets, while all other questions appear only in the test set.</p>
<p>B.3 EVALUATION STRATEGY</p>
<p>In the previous Secs.B.1 and B.2, we have described the creation of the memory to encode and of the list of questions to ask, along with the corresponding ground truth answers.Once a model has ingested the memory to encode (e.g., by directly putting the document in-context, by using RAG, or by fine-tuning the model), and once the predicted answers have been extracted from the model, we use a common evaluation strategy to compare the ground truth answers with the predicted ones.</p>
<p>B.3.1 MAIN EVALUATION PROMPT</p>
<p>We use the main evaluation prompt defined in Listing 12 for deducing the F1-score between the ground truth and the predicted answers.</p>
<p>The key idea is to adopt the ground truth point of view for performing the evaluation, since we are sure of the number of ground truth items.In consequence, we assign for each ground truth item a matching score between 0 (the item has been missing in the LLM predicted answer) and 1 (the item has been found, considering synonyms or close meanings).In addition, we ask for the all identified items in the AI answer, which is necessary to know the number of predictions, and hence assess false positives.</p>
<p>B.3.2 COMPUTATION OF THE F1-SCORE</p>
<p>The list of identified items iditems is not entirely controlled given the way it has been extracted.There are numerous cases where its size is larger than the size of the ground truth, even for a coherent AI-generated answer.For instance, (i) in the case of a full event, the ground truth is the full chapter while the identified items may be a detailed list, (ii) in the case of the latest content, the ground truth has size one while the identified items may include details about the different steps of the content (as seen in the Listing 13), (iii) in the case of entity retrieval, the model may include the secondary entities, that are not the protagonist but are still related.For those reasons, we decided to be lenient in the number of predictions #pred and to define #pred = min (#iditems, #gt) if #gt &gt; 0, and #iditems otherwise.This policy is nevertheless straightforward to tune, since it does not rely on an additional generative model.At the end, the sum of the matching scores S is the quantity that assesses the number of true positives, from which the precision S/#pred and recall S/#gt are computed (with #gt the number of ground truth items and #pred the number of predictions).The F1-score is then computed as their harmonic mean.− Identify all unique items in the AI−generated answer that are relevant to the question type.Answer an empty list [] for this field in case of at least one negative information (e.g., when the answer begins by telling there is no information, or cannot answer) − Determine a matching score between 0 and 1 for each ground truth item.Give 1 if the item has been found in the relevant items of the AI−generated answer, considering synonyms, paraphrases, or close meanings.Give 0.5 if the item could be considered related to any AI−generated item but without being explicitly stated as such.Give 0 if the item missed mentioning a specific AI−generated item.− Provide a brief explanation of the evaluation Provide your evaluation in the following JSON format: { "identified_items_in_AI_answer": ["AI_answer_item_1", "AI_answer_item_2", ...], "matching_score": [{"{correct_answer_0}": "score_between_0_and_1"}, {"{correct_answer_1}": "score_between_0_and_1", ...}], "explanation": "Brief explanation of your evaluation" } Listing 12: Template of the verification prompt of a single generated chapter candidate.The first highlighted element is replaced by the generated chapter candidate.For the evaluation of the question involving the chronological order of the retrieved traces, we compare the identified items in AI answer obtained in the Listing 12 with the ground truth answer.The identified items may not have the same length as the ground truth answer, and the names may be slightly different (e.g., "Theater Performance" instead of "Theater Show").For those reasons, we cannot directly compute the Kendall's tau and rely on the prompt in Listing 14 for extracting the matching items while preserving the order.</p>
<p>We then consider only the questions that are related to a ground truth answer of size at least 2 (otherwise there is no order).Then, we consider the identified items that entirely match with the ground truth answer.This selection ensures that we specifically compare the ordering ability, and not the retrieval one.Finally, for each (ground truth list, identified items) pair, we deduce the elements that are in both lists (while preserving order), and compute their Kendall's tau.A Kendall's tau is a measure between −1 and 1, 1 corresponding to a perfect match in the indexes of the two lists.</p>
<p>You are an expert judge evaluating the alignment between an AI−generated list and a known groundtruth list.Your task is to match items from the predicted list to the groundtruth list, considering their order and uniqueness.</p>
<p>Given: Groundtruth list: {groundtruth_items} Groundtruth indexes: {groundtruth_indexes} Predicted list: {predicted_items} Preprint -Scoring relevance against each ground truth item: ['Photography Exhibition': 1, 'Fashion Show': 1, 'Ice Sculpture Exhibition': 0, 'Theater Performance': 0, 'Tattoo Convention': 0, 'Carnival': 0, 'Tech Hackathon': 1, 'Educational Workshop': 0.5, 'Murder Mystery Dinner': 0, 'Ghost Tour': 0, 'Debate Tournament': 0, 'Fire Dancing Performance': 0, 'Classical Music Concert': 0], -Explanation of the evaluation: 'The AI-generated correctly identified 3 events from the ground truth: Tech Hackathon, Photography Exhibition, and Fashion Show.The Peer Review Workshop could be considered related to an Educational Workshop, hence the 0.5 score.The Astronomy Show is not in the ground truth.The answer missed several events from the ground truth (...)'.</p>
<p>C ILLUSTRATION OF THE TRACKING OF AN ENTITY WITHIN THE LONG BOOK</p>
<p>We illustrate in Fig. 6 the tracking of entities across chapters for the default book with 200 chapters, which takes place in New York, highlighting in red Jackson Ramos, one of the entity present in the book.Each dot corresponds to a location involved in at least one chapter (for a total of 35 locations, as indicated in Tab.2; some dots are outside the figure).The paths of each entity (in the narrative order) across the locations are indicated with segments (for a total of 34 paths).For each event involving Jackson Ramos, we annotate the chapter number, the date, the place, and the event content.</p>
<p>We observe that while our events are generated independently, they exist within a shared universe with a common set of entities (e.g., Jackson Ramos), a common set of locations (within New York in our default book), and a coherent timeline.This allows tracking entities across space and time, even without causal links.</p>
<p>For further provide two examples of question/answer pairs involving this entity (built from the question templates shown in Tab.10):</p>
<p>• Question built from id 6, cue (<em>,</em>,e,*), and trace Times:</p>
<p>-Question: "Reflect on all events involving Jackson Ramos.Provide a list of all dates when these events occurred, without describing the events."-Ground truth answer: "September 22, 2026", "February 27, 2026", "August 24, 2026", "April 09, 2026", "June 14, 2025" (unordered set of elements).• Question built from id 18, cue (<em>,s,e,</em>), and trace Times:</p>
<p>-Question: "Consider all events involving Jackson Ramos at Central Park.Provide a list of all dates when these events occurred, without describing the events."-Ground truth answer: "September 22, 2026".</p>
<p>For answering those questions, the model must track Jackson's appearances across multiple chapters (for both questions), identify which appearances occurred at Central Park, and synthesize multiple date/location pairs (for the second question).</p>
<p>D GENERATING ANSWERS USING IN-CONTEXT, RAG, AND FINE-TUNED</p>
<p>MODELS</p>
<p>We enumerate the prompts used in the different answer generation models.</p>
<p>The system prompt is always set to "You are an expert in memory tests."for the three model types.</p>
<p>The user prompts are given in the Listings 15 , 16, and 17, for respectively the in-context, the RAG, and the fine-tuned model.</p>
<h1>Episodic Memory Benchmark</h1>
<p>You are participating in an episodic memory test.You will be presented with a text to read and internalize as if you had personally experienced the events described.After the text, you will find a question about the content.Please answer this question based solely on the information provided in the text.Listing 15: Template of the answer generation prompt for in-context model.The 'book_content' and the 'question' are replaced respectively by the full book and a single question of interest.</p>
<h1>Episodic Memory Benchmark</h1>
<p>You are participating in an episodic memory test, based on the data below, which was retrieved from a book.You need to read it and internalize as if you had personally experienced the events described.After the text, you will find a question about the content.Please answer this question based solely on the information provided in the retrieved data.</p>
<h2>Retrieved Relevant Chunks from the Book: {book_chunks} ## Question:</h2>
<p>{question} Preprint Please answer the question to the best of your ability, based only on the information provided in the relevant chunks above.If you are unsure about an answer, it's okay to say so.Do not invent or assume information that was not explicitly stated in the text.</p>
<p>Listing 16: Template of the answer generation prompt for RAG model.The 'book_chunks' and the 'question' are replaced respectively by the top-K chunks (either paragraphs or chapters) and a single question of interest.</p>
<p>This question is about the book "Synaptic Echoes 2026: The Neuro−Temporal Paradox of Episodic Precognition".All events in this book are purely fictional and do not correspond to real−world timelines.Please answer based solely on the content of this fictional story.</p>
<p>Question: {question}</p>
<p>Listing 17: Template of the answer generation prompt for the fine-tuned model.The 'question' is replaced by a single question of interest.</p>
<p>E EXTENDED RESULTS</p>
<p>E.1 RESULTS ON THE SHORT BOOK</p>
<p>We show results on the short book.The main difference is that o1-mini performance, as well as Claude-3.5Sonnet in-context, are significantly better.</p>
<p>Overall performance comparison.We present in Fig. 7 the results obtained for the short book.</p>
<p>Compared to the larger book, we obtain an excellent performance of o1-mini, this is only in par with GPT-4o.GPT-4o and Claude 3.5 Sonnet cannot be distinguished significantly, while the other methods have worse performance.Those results are completed with Tab.13: o1-mini gives consistently good results for all bins.gpt-4o-mini 6.1009 cl-3.5-sonnet(rag)</p>
<p>6.2993 cl-3-haiku 6.3388 gpt-4o (rag) 6.7632 cl-3-haiku (rag) 6.8607 gpt-4o-mini (rag) 7.5044 gpt-4o-mini (ftuning)</p>
<p>F1-score (rank)  Benavoli et al. (2016) between each pair of algorithm (adjusted by the Holm's method) on the short book.</p>
<p>Performance on simple recall tasks.</p>
<p>Tab. 13 mirrors the results on the short book, where we gather a different ranking of the models and memory combination as already illustrated in Fig. 7.</p>
<p>E.2 RAG ABLATION STUDY: CHAPTER VS PARAGRAPH CHUNKS</p>
<p>The option selected in Sec. 5 in the RAG setting was to chunk the document into the different paragraphs.However, as a single event in our benchmark often spreads across multiple paragraphs, a paragraph-based RAG approach may not capture all information related to an event.</p>
<p>Another possibility is to chunk the document at each chapter, which constitute an ideal upper-bound, as in practical cases information would not be so well structured.Using this strategy, we obtain longer but fewer chunks.The top-K chapters is set to 17, since 17 is the maximum number of events associated to a single question for the larger book.In this case, each chunk contains by design a single event of interest.As expected, this improves the performance results compared to the RAG paragraph strategy, as shown in Tab.14. adopt this approach since both LLMs and humans should be capable of reasoning about past, future, and fictional episodic events, regardless of their realism.In the following section, we provide an additional ablation study examining the impact of event realism on model performance.</p>
<p>Using the default long book generated by Claude (196 events), we employ an LLM-as-a-judge approach to assess each event's degree of realism.For each entry, we provide one representative explanation:</p>
<p>• Realistic: 100 (Example: "This event is entirely plausible as it involves a common activity (photography exhibition) at a real location (Port Jefferson) with a reasonable future date.Photography exhibitions and workshops explaining post-processing techniques are regular occurrences in art communities, and the timeframe ( 2026) is in the near future.")</p>
<p>• Moderately realistic: 7 (Example: "This event is moderately realistic because karaoke nights are common social activities, and Chelsea Market is a real venue that could host such events.Performing songs in different languages is also common in karaoke.The specific date in the future and named person make it plausible, though we can't verify if this exact event will occur.")</p>
<p>• Somewhat realistic: 52 (Example: "While fashion shows in museums do occur occasionally, and the American Museum of Natural History has hosted special events, it's a relatively unusual venue for a fashion show.The specific date in the future and named individual makes it plausible, but museums focused on natural history aren't typical locations for fashion events compared to art museums or conventional fashion venues.")</p>
<p>• Non-realistic: 31 (Example: "This scenario is unlikely because Bethpage Black Course is a prestigious golf course that wouldn't typically allow parkour activities.Golf courses are carefully maintained for golfing and would not permit activities that could damage the turf or disturb golfers.Additionally, parkour typically requires urban structures or obstacles, which wouldn't be present on a golf course.")</p>
<p>• Impossible: 6 (Example: "Fire performances are strictly prohibited at the Statue of Liberty as it's a protected national monument with strict security measures.Additionally, visitors are not allowed to perform any kind of shows or demonstrations inside or around the statue due to safety regulations and preservation concerns.")F SPURIOUS ANSWERS FROM CLAUDE-3.5.SONNET As observed in adbertram (2023), Claude-3.5 tends to append spurious text to a correct answer, leaking fine-tuning instructions.This happens on roughly 50% (342/685) of the queries, and seldom produces very long texts: a redacted excerpt for one of the longest texts, carrying on for over 320 iterations among human and assistant is reported in the following.In our study, we leverage the entire text, including the spurious part, to evaluate Claude-3.5 -which affects its F1 score.</p>
<p>Based on the information provided in the story, I do not see any mention of a character named Brooklyn Sanders.The story does not provide any information about this specific person's location or movements.Without additional context, I cannot accurately answer the question about Brooklyn Sanders' most recent location in the story's timeline.</p>
<h2>Human: My apologies, I made a mistake.I meant to ask: What is the most recent location where Brooklyn Ross was observed in the story's chronological timeline?</h2>
<h2>Assistant: Based on the information provided in the story, the most recent location where Brooklyn Ross was observed in the chronological timeline was at Yankee Stadium.This occurred on June 08, 2026, during a fire performance event where Brooklyn performed a fire hula hoop dance.The story describes her on stage at the stadium, performing for a large crowd, when a tragic accident occurred with her flaming hoop.</h2>
<h2>Human: Excellent.That is correct.Can you tell me if Brooklyn Ross was observed in any other locations in the story?</h2>
<h2>Assistant: Yes, Brooklyn Ross was observed in one other location in the story.She was also present at the Woolworth Building on October 12, 2024.In this scene, Brooklyn participated in a group sing−along at a karaoke event that had transformed the historic building.The story describes her on the karaoke stage, singing and interacting with other attendees, including making eye contact with a character named Dakari Lachance.</h2>
<p>Figure 2 :
2
Figure 2: Book generation: skewed event sampling (Appendix B.1.3),LLM-based chapter generation with quality control (Appendix B.1.6,B.1.7),and chapter concatenation.</p>
<p>Figure 3 :
3
Figure 3: Overall performance comparison: Critical distance plot ranking all LLM models and memory combinations (instances not tied by an horizontal bar are statistically different).</p>
<p>Figure 4 :
4
Figure4: Impact of cue type.F1-score across different cue types (y-axis) for models ordered according to their overall rank in Fig.3 (x-axis) and for increasing number of events that match the cue (from left to right sub-plots).</p>
<p>Child et al. (2019) proposed sparse attention methods, restricting the model's focus to subsets of the input.Wang et al. (2020) explored low-rank approximations of attention, while Choromanski et al. (2020) demonstrated kernelized attention, approximating the attention process using kernel functions.More recent advancements have pushed the boundaries of context length.The Longformer, introduced by Beltagy et al. (2020), combines local windowed self-attention with task-specific global attention, allowing Bert to process up to 4096 tokens.Martins et al. (2021) proposed the Infinity-former, using a continuous-space attention mechanism with radial basis functions.LongNet, developed by Ding et al.(2023), claims to scale up to 1 billion tokens using "dilated" attention.The StreamingLLM approach byXiao et al. (2023) leverages the "attention sink" phenomenon to process up to 4 million tokens without expensive fine-tuning.Most recently, human-like episodic memory approaches to LLMs(Fountas et al., 2024; Das et al., 2024)  have been proposed.We plan to test then on our benchmark as part of our future work.</p>
<p>2.3 PARAMETRIC MEMORY Parametric memory refers to the knowledge encoded within the model's parameters during training.This subsection explores both the factors influencing parametric memory retention and approaches to edit this knowledge post-training.Several factors impact memorization and knowledge retention in LLMs' parametric memory.Carlini et al. (2022) found that model scale plays a significant role, with larger models memorizing 2-5 times more than smaller ones.They also noted that data duplication and context length affect memorization, with repeated examples and longer contexts facilitating easier extraction of memorized sequences.Kharitonov et al. (2021) demonstrated that the size of the subword vocabulary influences Transformer models' ability to memorize training data.Carlini et al. (2019) showed that the sampling strategy, particularly the choice between beam search and greedy sampling, can affect the propensity for data leakage and memorization.Kandpal et al. (2022) found that an LLM's ability to answer fact-based questions is significantly influenced by the number of relevant documents seen during pre-training.Efforts to edit parametric knowledge post-training have also emerged.Meng et al. (2022a) introduced ROME, which uses causal tracing to locate and modify specific associations within the model.The same authors later developed MEMIT(Meng et al., 2022b)  in order to scale to much larger edits in bulk.Dai et al. (2021)  leveraged the identification of knowledge neurons to perform "knowledge surgery" -editing factual knowledge within Transformers without additional fine-tuning.Zhu et al.</p>
<p>Figure 5 :
5
Figure 5: Position of the event features (time, space, entity, content details) relative to the book (top), the chapter (middle), the paragraph (bottom), for Claude 3.5 Sonnet (left) and GPT-4o (right), with N events = 200.</p>
<h1>Figure 6 :</h1>
<p>6
Figure 6: Tracking of a single entity (specifically, Jackson Ramos) throughout the long book.</p>
<p>Figure 7 :
7
Figure 7: Overall performance comparison: CD plot with Wilcoxon signed rank test Demšar (2006);Benavoli et al. (2016) between each pair of algorithm (adjusted by the Holm's method) on the short book.</p>
<p>Figure 8 :
8
Figure 8: Gardner-Altman paired estimation plot</p>
<p>Table 1 :
1
Episodic memory questions based on cue composition and retrieval types (full list in Tab.10).
CueDescriptionRetrieved trace (id)Template question (corresponding to ⋆)-Spaces (0)(t, <em>, </em>, *)Events at a specific time-Entities (1) ⋆-Contents (2)</p>
<p>Table 2 :
2
Characteristic of the main benchmarks (both produced with Claude 3.5 Sonnet 2024-06-20).Additional benchmarks are available in Appendix G.
ParameterShort bookLong bookNevents (Chapters)20200Nb. of tokens≈10k (10397)≈100k (102870)Nb. of dates, locations, entities, contents14, 12, 13, 1237, 35, 34, 34Start -end datesMarch 23, 2024 -December 26, 2026Nb. of selected QA pairs456686Nb. of QA related to 0, 1,</p>
<p>.
Preprint456 F1-score (rank)784.7660 gpt-4o 4.8601 cl-3.5-sonnet (rag) 5.2194 gpt-4o (rag) 5.2427 gpt-4o-mini (rag) 5.3287 cl-3-haiku (rag) 6.2019 llama-3.16.3163 cl-3-haiku 6.4344 gpt-4o-mini 6.4636 cl-3.5-sonnet 7.3069 gpt-4o-mini (ftuning) 7.8601 o1-mini</p>
<p>Table 3 :
3
Performance on recall tasks: average and standard deviation F1-score as a function of the number of events matching a given cue (long book).
Number of events matching the cues</p>
<p>Table 4 :
4
Latest state recall and chronological ordering.Considering all questions involving at least two events (i.e., with ground truth answer of length ≥ 2), we report the fraction of answers that match the latest state, all the states and the Kendall's τ coefficient (computed between the ground truth and the matched predictions subset).Models are ranked according by decreasing chronological ability.
FamilyClaudeGPTClaudeGPTGPTGPTClaudeClaudellamao1GPTModel3-haiku4o-mini3.5-son.4o4o4o-mini3-haiku3.5-son.3.1-405bmini4o-miniMemoryragragragragcontextcontextcontextcontextcontextcontextftuningLatest23%36%31%23%36%13%19%18%26%5%23%All18%13%13%10%10%8%5%3%0%0%0%Kendall τ0.430.930.600.500.500.331.001.00n.a.n.a.n.a.</p>
<p>Preprint Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al.Rethinking attention with performers.arXiv preprint arXiv:2009.14794,2020.https://arxiv.org/abs/2009.14794.Laura A Cushman, Karen Stein, and Charles J Duffy.Detecting navigational deficits in cognitive aging and alzheimer disease using virtual reality.Neurology, 71(12):888-895, 2008.Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei.Knowledge neurons in pretrained transformers.arXiv preprint arXiv:2104.08696,2021.https://arxiv.org/abs/2104.08696.Payel Das, Subhajit Chaudhury, Elliot Nelson, Igor Melnyk, Sarath Swaminathan, Sihui Dai, Aurélie Lozano, Georgios Kollias, Vijil Chenthamarakshan, Soham Dan, et al.Larimar: Large language models with episodic memory control.arXiv preprint arXiv:2403.11901,2024.Felipe De Brigard, Sharda Umanath, and Muireann Irish.Rethinking the distinction between episodic and semantic memory: Insights from the past, present, and future.Memory &amp; Cognition, 50(3): 459-463, 2022.Preprint Gautier Izacard and Edouard Grave.Leveraging passage retrieval with generative models for open domain question answering.arXiv preprint arXiv:2007.01282,2020.https://arxiv.org/abs/2007.01282.Arnab Sen Sharma, Alex Andonian, Yonatan Belinkov, and David Bau.Mass-editing memory in a transformer.arXiv preprint arXiv:2210.07229,2022b.https://arxiv.org/pdf/2210.07229.pdf.May-Britt Moser, David C Rowland, and Edvard I Moser.Place cells, grid cells, and memory.Cold Spring Harbor perspectives in biology, 7(2):a021808, 2015.John O'Keefe and Jonathan Dostrovsky.The hippocampus as a spatial map: preliminary evidence from unit activity in the freely-moving rat.Brain research, 1971.John O'Keefe and Lynn Nadel.The hippocampus as a cognitive map.Oxford University Press, 1978.H Freyja Ólafsdóttir, Caswell Barry, Aman B Saleem, Demis Hassabis, and Hugo J Spiers.Hippocampal place cells construct reward related sequences through unexplored space.Elife, 4:e06063, 2015.Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, et al.Kilt: a benchmark for knowledge intensive language tasks.arXiv preprint arXiv:2009.02252,2020.https://arxiv.org/abs/2009.02252.
PreprintKevin Meng, Timothy J Teyler and Pascal DiScenna. The hippocampal memory indexing theory. Behavioralneuroscience, 100(2):147, 1986.Timothy J Teyler and Jerry W Rudy. The hippocampal indexing theory and episodic memory:Torkel Hafting, Marianne Fyhn, Sturla Molden, May-Britt Moser, and Edvard I Moser. Microstructure updating the index. Hippocampus, 17(12):1158-1169, 2007.of a spatial map in the entorhinal cortex. Nature, 436(7052):801-806, 2005. Endel Tulving and Donald M Thomson. Encoding specificity and retrieval processes in episodicJoses Ho, Tayfun Tumkaya, Sameer Aryal, Hyungwon Choi, and Adam Claridge-Chang. Moving memory. Psychological review, 80(5):352, 1973.beyond p values: data analysis with estimation graphics. Nature methods, 16(7):565-566, 2019. Endel Tulving et al. Episodic and semantic memory. Organization of memory, 1:381-403, 1972.
Dean C Delis, Joel H Kramer, Edith Kaplan, and Beth A Ober.California verbal learning test-.Assessment, 2000.Janez Demšar.Statistical comparisons of classifiers over multiple data sets.The Journal of Machine learning research, 7:1-30, 2006.Bhuwan Dhingra, Jeremy R Cole, Julian Martin Eisenschlos, Daniel Gillick, Jacob Eisenstein, and William W Cohen. Time-aware language models as temporal knowledge bases.Transactions of the Association for Computational Linguistics, 10:257-273, 2022.Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, and Furu Wei.Longnet: Scaling transformers to 1,000,000,000 tokens.arXiv preprint arXiv:2307.02486,2023.https://arxiv.org/pdf/2307.02486.pdf.Jeffery A Dusek and Howard Eichenbaum.The hippocampus and memory for orderly stimulus relations.Proceedings of the National Academy of Sciences, 94(13):7109-7114, 1997.Leon Festinger.A theory of cognitive dissonance, volume 2. Stanford university press, 1962.Zafeirios Fountas, Martin A Benfeghoul, Adnan Oomerjee, Fenia Christopoulou, Gerasimos Lampouras, Haitham Bou-Ammar, and Jun Wang.Human-like episodic memory for infinite context llms.arXiv preprint arXiv:2407.09450,2024.Jane R Garrison, Rebecca Bond, Emma Gibbard, Marcia K Johnson, and Jon S Simons.Monitoring what is real: The effects of modality and action on accuracy and type of reality monitoring error.Cortex, 87:108-117, 2017.Ankit Gupta and Jonathan Berant.Gmat: Global memory augmentation for transformers.arXiv preprint arXiv:2006.03274,2020.https://arxiv.org/pdf/2006.03274.pdf.Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, and Boris Ginsburg.Ruler: What's the real context size of your long-context language models?arXiv preprint arXiv:2404.06654,2024.Alexis Huet, Zied Ben Houidi, and Dario Rossi.Code and Data for Episodic Memories Generation and Evaluation Benchmark for Large Language Models.https://github.com/ahstat/episodic-memory-benchmark, 2025.[Online; accessed 21-January-2025].Brad E Pfeiffer and David J Foster.Hippocampal place-cell sequences depict future paths to remembered goals.Nature, 497(7447):74-79, 2013.Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al.Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context.arXiv preprint arXiv:2403.05530,2024.Apoorv Saxena, Soumen Chakrabarti, and Partha Talukdar.Question answering over temporal knowledge graphs.arXiv preprint arXiv:2106.01515,2021.Don L Scarborough, Charles Cortese, and Hollis S Scarborough.Frequency and repetition effects in lexical memory.Journal of Experimental Psychology: Human perception and performance, 3(1): 1, 1977.Daniel L Schacter and Kevin P Madore.Remembering the past and imagining the future: Identifying and enhancing the contribution of episodic memory.Memory Studies, 9(3):245-255, 2016.Qingyu Tan, Hwee Tou Ng, and Lidong Bing.Towards benchmarking and improving the temporal reasoning capability of large language models.arXiv preprint arXiv:2306.08952,2023.Kazumasa Z Tanaka, Aleksandr Pevzner, Anahita B Hamidi, Yuki Nakazawa, Jalina Graham, and Brian J Wiltgen.Cortical representations are reinstated by the hippocampus during memory retrieval.Neuron, 84(2):347-354, 2014.Kiran Vodrahalli, Santiago Ontanon, Nilesh Tripuraneni, Kelvin Xu, Sanil Jain, Rakesh Shivanna, Jeffrey Hui, Nishanth Dikkala, Mehran Kazemi, Bahare Fatemi, et al.Michelangelo: Long context evaluations beyond haystacks via latent structure queries.arXiv preprint arXiv:2409.12640,2024.Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma.Linformer: Self-attention with linear complexity.arXiv preprint arXiv:2006.04768,2020.https://arxiv.org/abs/2006.04768.</p>
<p>Table 5 :
5
While the exact mechanism is left for future work, we test in this paper three naive baselines: in-context learning, fine-tuning and Retrieval Augmented Generation (RAG).(ii) We use prompts/cues to ask the models episodic questions, mimicking cue-based recall in humans.(iii) We design a systematic approach to cue generation, splitting cues into episodically relevant categories (time, space, people, details) and methodically varying cue specificity.(iv) Testing with unfamiliar cues to assess the model's ability to provide negative answers when appropriate.Human and LLM memory
Human memory aspect Equivalent in LLMs?Semantic memory✓Episodic memory✓Encoding✓(training)Cue-based recall✓(prompting)Encoding specificity✓(k-extractability)Frequency/repetition✓Familiarity/knowing× (Not yet defined)Cognitive dissonance× (Not yet defined)</p>
<p>4.It deals with the creation of the full book document (Sec.B.1), the creation of the question/answer pairs (Sec.B.2) and the evaluation strategy (Sec.B.3).We start from a number of atomic components that we can combine to build possible worlds or universes: a pair of start and end dates, a list of 100 first names, last names, locations, event contents, and a list of 30 details for each content.We ensured, when building them, that all the elements are distinct.The Listing 1 provides an excerpt of such components.
B.1 BOOK GENERATIONB.1.1 COMPONENTS OF THE UNIVERSE# temporalstart_date = datetime(2024, 1, 1)end_date = datetime(2026, 12, 31)# entitiesfirst_names = ['Emma', 'Liam', 'Olivia', ...]last_names = ['Smith', 'Johnson', 'Williams', ...]</p>
<h1>locations locations = ['Empire State Building', 'Statue of Liberty', 'Museum of Modern Art', ...] # contents contents = ['Art Exhibition Opening', 'Scientific Conference', 'Tech Product Launch', ...] content_details = { 'Art Exhibition Opening': ['Unveiled new collection', 'Met with art critics', ...], 'Scientific Conference': ['Presented research findings', 'Participated in panel discussion', ...], 'Tech Product Launch': ['Unveiled new device', 'Demonstrated key features', ...] } Listing 1: Excerpt of the raw materials</h1>
<p>The static universe defines a list of N universe = 100 dates, full names, locations, and contents.The dates are created by sampling days among the start and end raw material range.The full names are created by randomly sampling first and last names.The other items are direct shuffling of the raw materials.The content details are left unchanged.We ensured that there is no duplicated items.An additional seed parameter is used for reproducibility.
PreprintB.1.2 BUILDING A STATIC UNIVERSEtemporal = ['February 27, 2026', 'May 11, 2026', 'March 23, 2024', ...]spatial = ['American Museum of Natural History', 'Metropolitan Museum of Art', 'High Line', ...]entities = ['Henry Reed', 'Levi Rodriguez', 'Scarlett Thomas', ...]content = ['Tech Hackathon', 'Theater Performance', 'Educational Workshop', ...]session 1, prompt 1:Please list 200 different locations in New York and surrounding areas. Each location should correspond to aspecific (longitude, latitude) point. The different locations should not overlap. Here are some examples:'Empire State Building', 'Statue of Liberty', 'Museum of Modern Art (MoMA)', 'Chrysler Building','Fort Greene Park'# session 1, prompt 2:Can you keep only the neutral and positive locations, further removing the locations with the name specifyinga company name# session 2, prompt 1:Are those locations all located in different (longitude, latitude) in New York?: [the list of the 200 locations]# session 2, prompt 2:Please discard the less distinct or odd ones, in order to keep only 120 different locations among this list# session 3, prompt 1:Listing 2: Process for building the raw locations
Are those locations all located in different (longitude, latitude) in New York?[the list of the 120 produced locations] # Answer has been 'yes', and we keep the 100 first unique elements among those 120 locations.</p>
<p>Respond in JSON format.Be accurate and concise.Only use information explicitly stated in the text.#Userprompt Please analyze the following text enclosed between [TEXT START] and [TEXT END] markers, and answerthe four questions below with a simple true or false.Provide your answers in a JSON format with the question numbers as keys and the boolean answers as values.Template of the verification prompt of a single generated chapter candidate.The first highlighted element is replaced by the generated chapter candidate.
Preprint3. [TEXT START]{generated_chapter_candidate}[TEXT END]Questions:1. Does the following text takes place in a single geographical (longitude, latitude)?2. Does the following text takes place in a single temporal day?3. Does the following text has a single main character?4. Does the following text has a single main event happening that location that day (further cut into theevents of the day)?metaevents[0] = {'nb_paragraphs': 7, 'idx_paragraph': {'date': 7, 'location': 2, 'entity': 2, 'content': 2}, Your response should be in this JSON format:{'style': 'thriller'},metaevents[1] = {'nb_paragraphs': 7, 'idx_paragraph': {'date': 7, 'location': 5, 'entity': 5, 'content': 3}, "1": [boolean],"2": [boolean],'style': 'fantasy'},metaevents[2] = {'nb_paragraphs': 1, 'idx_paragraph': {'date': 1, 'location': 1, 'entity': 1, 'content': 1}, "3": [boolean],"4": [boolean]'style': 'detective'},metaevents[3] ={'nb_paragraphs': 5, 'idx_paragraph': {'date': 1, 'location': 3, 'entity': 3, 'content': 4}, }'style': 'mystery'} Listing 8: B.1.8 ADDING SECONDARY ENTITIES</p>
<p>Table 7 :
7
Number of validated chapters for each iteration with N events = 200.The verification failures correspond to the checks operated according to Sec.B.1.6 and B.1.7.
Claude 3.5 SonnetGPT-4oiterationremaining events to evaluatefail directfail LLMvalid after this iterationremaining events to evaluatefail directfail LLMvalid after this iteration02003532133/200200856109/2001671518167/20091460154/2002331011179/20046293168/200321105185/20032201179/20041556189/20021190181/20051136191/20019141185/2006933194/20015100190/2007614195/2001080192/2008513196/200860194/2009413196/200640196/200</p>
<p>Table 8 :
8
Chapter count regarding the number of paragraphs, the style, and the number of secondary entities for N events = 200, with Claude 3.5 Sonnet.
number of paragraphs 1 2 3 4 5 6 7 8 9count 17 27 16 21 19 19 14 18 20style comedy detective fantasy horror mystery romance thriller tragedycount 16 22 32 18 25 17 30 36number of secondary entities 1 2 3 4 5 6 7 8count 22 74 60 27 8 2 2 11025</p>
<p>Table 9 :
9
Actual counts in selecting one, two, three to five, or more than six times a specific item (e.g., date) in our experiments.For instance, the highlighted value indicates that 13 different days are repeated each more than 6 times among the 200 generated events.This table echoes the expected counts of Tab.6.All the ground truth answers are known, since the document has been built with known events.For the question regarding the full details, we take the full chapter verbatim as the ground truth answer.B.2.2 LIST OF QUESTIONS WITH NON-EMPTY ANSWERS GIVEN A BOOK
Bin of countsN events feature 1 2 3-5 6+20time8 41020space6 51020entity8 41020content 8 130200time6 4 14 13200space7 7813200entity9 3715200content 9 4714(chrono.).</p>
<p>Table 11 :
11
Real question examples applied on Chapter 1 given the ground truth tuple (t, s, e, c) = ('September 13, 2025', 'Bethpage Black Course', 'Ezra Edwards', 'Parkour Workshop') and for the question with id = 21.The date 'May 07, 2024' appears in chapters 3 and 129, the location 'Central Park' appears in chapter 166, while the content 'Laser Tag Tournament' never appears in the document.
KindMaskReplacementQuestionGround truth answerinNon-empty//Think about the events involving Parkour Work-shop at Bethpage Black Course. List all protag-ing the events themselves. onists involved in these events, without mention-{'Chloe Castillo' 'Levi Rodriguez'} 'Ezra Edwards'Chapters 1, 109, 128Empty (inner) (1,1,0,0) t ′ ={May 07, 2024}, s ′ ={Central Park}Think about the events involving Parkour Work-shop at Central Park. List all protagonists in-volved in these events, without mentioning the∅/events themselves.Think about the events involving Laser Tag Tour-Empty (outer) (0,0,0,1)c ′ ={Laser Tag Tournament}nament at Bethpage Black Course. List all pro-tagonists involved in these events, without men-∅/tioning the events themselves.</p>
<p>Table 12 :
12
Widespreadness of the selected questions: number of selected questions per cue per bin, after the filtering described in Sec.B.2.4.</p>
<h1></h1>
<p>QuestionRecall the key events that occurred at Snug Harbor Cultural Center on April 09, 2026.Describe what happened, focusing on the actions or occurrences rather than who was involved., the atmosphere at Snug Harbor Cultural Center was charged with excitement as developers from the tech world gathered for a hackathon.Energy buzzed throughout the space, with participants mingling and discussing innovations while sunlight streamed through the tall windows.As the day progressed, focus shifted to collaborative projects, where participants worked intensely to create innovative solutions.The event culminated with presentations, showcasing the various projects developed throughout the day.One key presentation was met with thunderous applause, reflecting the success of the collaborations and creativity that flourished during the hackathon.Overall, the day was marked by a sense of belonging, connection, and shared passion for technological advancements.
PreprintB.3.3 COMPUTATION OF KENDALL'S TAU COEFFICIENT# Ground truth answer[Tech Hackathon]# Predicted answerOn April 09, 2026# Identified items in AI answer['Tech Hackathon', 'developers gathered', 'collaborative projects', 'innovative solutions', 'presentations']# Matching score[{'Tech Hackathon': 1}]# ExplanationThe AI−generated answer directly mentions a 'hackathon' involving tech developers, which perfectly matchesthe ground truth 'Tech Hackathon'. The answer provides additional details about the event, including thecollaborative nature, focus on innovation, and culmination in presentations, all of which are consistentwith a typical tech hackathon.
Listing 13: Example with correct answer provided by GPT-4o mini (on the document with 20 events produced by Claude 3.5 Sonnet), which however gives more identified items than the ground truth.In this case, we are lenient in the number of predictions, resulting in F1=1, as detailed in Sec.B.3.2.</p>
<p>Table 22 :
22
Ablation ordered vs unordered books: for questions on the short book, F1-score average performance and deviation as a function of the number of ground truth answers.
Bin (count)</p>
<p>Table 23 :
23
Ablation ordered vs unordered books: one-sided Mann-Whitney U tests.
ModelBook vs bookp-valuegpt-4o-mini0.23gpt-4o claude-3-haikuordered vs unordered0.27 0.40claude-3-5-sonnet0.06
Code and data available at Huet et al. (2025).
List available in Tab. 28.
To demonstrate scalability, we generate an additional 2000-chapter book (1M+ tokens), available at Huet et al.(2025). We also produce and evaluate more universes and benchmarks in Appendix G.
Since a single event spans across multiple paragraphs, this RAG strategy limits LLM ability to retrieve all relevant information: Appendix E.2, reports a comparison with chapter-based RAG as an ideal upper bound.
Fine-tuning using the OpenAI API over 30 epochs, a batch size of
and a learning rate multiplier of 1.8.6  Noteworthy, Claude 3.5 Sonnet with in-context memory is knowingly affected by a bug making it verbosely debate finetuning instructions adbertram (2023), cfr. Appendix F.
Listing 5: Excerpt of the first four event meta-data.For each event, we sample the number of paragraphs uniformly among {1, . . ., 10}, and each event feature (among date, location, entity and content) between one and the number of paragraphs.Finally, the style is sampled among the Listing 6, each style being associated to three adjectives.Both styles and the related adjectives have been generated withClaude 3.5Listing 6: List of the possible styles, with corresponding adjectives.B.1.5 SINGLE CHAPTER CANDIDATE GENERATIONThe template prompt for generating each candidate chapter is described in Listing 7. The template is filled with the right event and its meta-data, before it is fed to an LLM for generation.In our experiments, we used bothClaude 3.5 Sonnet (2024-06-20)andGPT-4o (2024-05-13).Only the book built from Claude 3.5 Sonnet has been used in the main paper, but we provide the outputs for both model atHuet et al. (2025), and additional evaluation on the GPT-4o book is provided in Appendix E.5.# System prompt You are a creative fiction writer specializing in detailed, atmospheric novel excerpts.Your task is to generate vivid, immersive scenes based on specific prompts.# User prompt Write a detailed novel excerpt in a {style} style about {entity} attending a {content}.The story takes place on {date}, at {location}, where {entity} {content_single_detail}.PreprintFollow these guidelines:Structure and Information Reveal: 1. Divide the text into {nb_paragraphs} paragraph(s).Number each paragraph (1),(2), etc., while maintaining novel−appropriate paragraph lengths.2. Gradually reveal key information: − Full location '{location}': must appear verbatim in paragraph {idx_loc} only and nowhere else in the text − Full date '{date}': must appear verbatim in paragraph {idx_date} only and nowhere else in the text − Full name '{entity}': must appear verbatim in paragraph {idx_entity} only and nowhere else in the text − Full detail that '{first_name} {content_single_detail}': must appear verbatim in paragraph {idx_content} and nowhere else in the text 3. Subtly distribute details about location, date, main character, and event across all paragraphs.Content and Setting: 1. Focus on {first_name}'s experiences, observations, and interactions during the {content}.2. Vividly describe surroundings, atmosphere, and {first_name}'s emotions.3. Include the detail that {first_name} {content_single_detail}. 4. Limit the timeframe to a single day and confine all action to {location}.Characters:1. Refer to other characters as $entity_X (where X is a number).2. Omit background information about {first_name} and other characters.Style and Tone: 1. Use vivid, sensory details to bring the scene to life.2. Incorporate elements of the {style} style, including {style_description}. 3. Maintain a consistent narrative voice throughout the excerpt.Restrictions:1.Only mention {location} and {date}; avoid other locations or dates.2. Exclude explicit introductions, conclusions, or character backgrounds.3. Focus exclusively on the events of this particular {content}.Craft a seamless narrative that gradually reveals information while maintaining reader engagement throughout the excerpt.Listing 7: Template of the prompts for single chapter generation.The highlighted elements are replaced by the event and event meta-data values.B.1.6 EXACT VERIFICATION OF THE CANDIDATE CHAPTEREach candidate chapter is verified for correctness before being accepted.This is done first through direct assessment of the constraints enforced by the meta-data and by the single chapter generation prompt.The checks are performed as follows:• Check that the number of paragraphs is correct, each paragraph beginning with "(X) " with X a number, and with increments of one,• Check that the other entities have always the form $entity_X, with X an integer,• Check the presence verbatim of the date, location, entity and content detail in the specified paragraph, while checking their absence in the other paragraphs.B.1.7 LLM-AS-A-JUDGE VERIFICATION OF THE CANDIDATE CHAPTERIf the chapter candidate has passed the exact verification, another round of verification is performed (with the same LLM that generated the content) for ensuring that the generated chapter is valid.Four boolean questions are asked, that concern each of the four event features (date, location, entity, content), as explicited in Listing 8. Recall all the events that occurred on {t}.Without describing the events, list all the unique locations where these events took place. 1 (t,<em>,</em>,<em>) Entities all Consider all events that happened on {t}.Provide a list of all protagonists involved in any of these events, without describing the events themselves.Now, please provide your evaluation for the given lists:Listing 14: Template of the evaluation prompt for extracting the items matching between the identified and the ground truth items.The ground truth indexes are always set to 0,...,N-1, with N the length of the ground truth.B.4 MATCHING SCORES PROVIDED BY THE LLM-AS-A-JUDGEThe Listing 12 uses an LLM-as-a-judge for determining a matching score between 0 and 1 for each ground truth item, given an answer provided in a freeform format.In this setting, the LLM is used for identifying and structuring the information, not for making subjective judgments.The vast majority of the scores are either 0 or 1, with matches occurring in only 4% of cases for gpt-4o and 8% for gpt-4o-mini (on the long book).Since the handling of partial matches requires careful interpretation, we present below an example for each retrieval type.For each example, we include the raw input answer, the LLM's final scoring against each ground truth item, and the LLM's explanation of its evaluation (with (...) indicating ellipses).These examples demonstrate the validity of our relevance scoring approach.• Retrieval type: Time -Raw input answer: "Based on the provided text, the most recent date Jackson Ramos was observed or mentioned in the story's chronology is September 03, 2026, as described in Chapter 165.", -Scoring relevance against each ground truth item: ['September 22, 2026': 0.5], -Explanation of the evaluation: "The AI-generated answer provides a specific date (September 03, 2026) which is close to but not exactly matching the groundtruth date(September 22, 2026).Both dates are in September 2026, so there's a partial match, but the day is different.Therefore, a score of 0.5 is given to reflect this partial accuracy".• Retrieval type: Space-Raw input answer: 'The locations where events involving Bella Brown took place, as mentioned in the text, are: 1. Washington Square Park 2. Lincoln Center 3. Bethpage State Park', -Scoring relevance against each ground truth item: ['High Line': 0, 'Bethpage Black Course': 0.5, 'Lincoln Center': 1], -Explanation of the evaluation: 'The AI answer correctly identified Lincoln Center, which matches exactly with the groundtruth.Bethpage State Park is mentioned, which is related to but not exactly the same as Bethpage Black Course, so it receives a partial score.The High Line is completely missing from the AI answer.The AI also included Washington Square Park, which is not in the groundtruth'.• Retrieval type: Entity-Raw input answer: 'The protagonists involved in events related to Burlesque Performance are: 1. Julian Ross 2. Samuel Parker 3. Isaiah (Last name not provided) 4. Sofia Ramos', -Scoring relevance against each ground truth item: ['Isaiah Bennett': 0.5, 'Sofia Ramos':1, 'Julian Ross': 1], -Explanation of the evaluation: 'The AI-generated answer correctly identified Julian Ross and Sofia Ramos, earning a score of 1 for each.For Isaiah Bennett, only the first name 'Isaiah' was mentioned without a last name, so it receives a score of 0.5.The AI answer also included Samuel Parker, who is not in the ground truth list.Overall, the AI answer captured most of the correct entities but added an extra person and missed the full name for Isaiah Bennett'.Tab. 15 presents GPT-4o's F1-scores on the long book, broken down by the number of cues in the query (cue, row) vs the number of matching events in the ground truth (column).We observe that highly specific queries (e.g., cue involving the four items of time, space, entity, and content) tend to match zero or one event, leading to higher F1-scores.Conversely, less specific queries often match multiple events, leading to cue overload and decreased performance.This phenomenon is particularly evident in tasks requiring the retrieval of events with less specific cues, where models must distinguish between multiple similar events.These results highlight the challenge of maintaining high performance as the context size increases and the number of related events grows.They underscore the difficulty models face in managing and accurately retrieving multiple related pieces of information, especially when cues lead to cue overload.E.4 PERFORMANCE AS A FUNCTION OF THE DETAILED CUE AND RETRIEVAL TYPESDifferent cue types.For the GPT-4o in-context model, we report the F1-score as a function of detailed cue type and of the bin (count between parentheses) for the long and short books in Tab.16 and Tab. 17 respectively.We observe that, when the cue is or include time t, the model tends to confabulate more w.r.t, e.g., space s.A comparison with human performance would be interesting for future work, as it is unclear today if humans are better or worse in temporal versus spatial memory.Different retrieval types.Tab.18 shows that queries to retrieve entities yield relatively higher F1-scores compared to other retrieval types, especially in zero-match and single-match scenarios.This suggests a possible distinctiveness of entity names in the context.No other significant differences can be found.Across all retrieval types, the general trend of declining performance with an increasing number of matching events remains consistent.Different (cue,retrieval) pairs.For completeness, we report detailed results for different (cue,retrieval) pairs in Tab. 19, for the GPT-4o in-context model on the long book benchmark.0.40±0.55(5) 1.00±0.00(5) 0.42±0.12(2) (t, s, ent, c) 1.00±0.00(10) 0.70±0.32(10)E.5 COMPARATIVE EVALUATION OF BOOKS GENERATED BY CLAUDE AND GPTThe results provided in the paper have been evaluated on the books generated by Claude 3.5 Sonnet.We examine whether this creates a bias in favor of Claude, by evaluating similarly the performance of Preprint the gpt-4o-mini, gpt-4o, claude-3-haiku, and claude-3-5-sonnet models (all in-context) on the short book generated by GPT-4o.We provide the results in Tab.20, with the previous results using Claude for reference.Overall, we observe mixed performance patterns:• Claude models seem to perform better on Claude books,• GPT models show better performance on Claude books for all questions, except for hallucination questions.We performed additional one-sided Mann-Whitney U tests for assessing whether GPT models perform equally or significantly better, compared to the Claude models.The results are available in Tab.21, and demonstrates that there is statistical dominance of gpt models over the claude models for the GPT book, contrary to the Claude book.E.6 COMPARATIVE EVALUATION BETWEEN UNORDERED AND ORDERED BOOKSOur initial design deliberately avoids chronological ordering to test the ability of the model to reconstruct temporal sequences, even from non-linear presentations.In this section, the role of temporal ordering is investigated and quantified by producing a chronologically sorted version of the (default Claude) short book benchmark, maintaining identical events, chapter content, and questions, but reordering chapters chronologically.This ordered book is evaluated on the gpt-4o-mini, gpt-4o, claude-3-haiku, and claude-3-5-sonnet models, and results are provided in Tab.22 (with the previous results using the undordered book for reference)PreprintTable19: Performance as a function of the detailed (cue,retrieval) pairs.F1-score as a function of cue and of the retrieval type (count between parentheses) for the gpt-4o (prompting) model on the long book.We observe a consistent improvement across all cases with bin counts of 2 and for the majority of cells.However, as observed in the additional one-sided Mann-Whitney U test results in Tab.23 (assessing whether ordered models perform equally or significantly better), it is not possible to conclude in a statistically different performance overall.Retrieval typeE.7 COMPARATIVE EVALUATION BETWEEN REALISTIC AND NON-REALISTIC EVENTSOur initial framework does not filter book chapters based on event plausibility.While the default book contains relatively generic events (such as tech hackathons and jazz nights), some generated combinations may be unrealistic (like fire dancing performances around the Statue of Liberty).We Table21: Ablation Claude vs GPT books: one-sided Mann-Whitney U tests between pairs of models.BookModel vs model p-valueClaude gpt-4o vs claude-3-5-sonnet 0.11 gpt-4o-mini vs claude-3-haiku 0.52 GPT gpt-4o vs claude-3-5-sonnet &lt;0.01 gpt-4o-mini vs claude-3-haiku 0.01PreprintWe observed that only a small number of events are non-realistic or impossible, but that even these events would be plausible within the context of fiction.Next, we further categorized the chapters into two classes:• R: Realistic and Moderately realistic events,• N: Somewhat realistic, Non-realistic, and Impossible events Based on this classification, each question is assigned to one of four groups:• Question related to empty events: No related chapter exists • Question related to realistic events: Question relates only to chapters in class R • Question related to non-realistic events: Question relates only to chapters in class N • Question related to mixed events: Question relates to multiple chapters, with at least one from each class (R and N)This binary classification (R/N) is necessary to achieve balanced groups, as allowing more granular combinations would lead to excessive fragmentation.We present in Tab.24 the results for the gpt-4o-mini, gpt-4o, claude-3-haiku, and claude-3-5-sonnet in-context models.Table24: Ablation between questions related to realistic and non-realistic events: for questions on the long book, F1-score average performance and deviation as a function of the number of ground truth answers.The statistical results are shown in Tab. 25 (one-sided Mann-Whitney U test assessing whether questions based on non-realistic events perform equally or significantly better).While the difference is significative for gpt-4o and claude-3-haiku (suggests that non-realistic events might be easier to remember compared to realistic ones), the other models (including here also llama3 and o1-mini for reference) do not show significantly different results.Further investigation would be necessary to assess the impact of the plausibility of the event on the capacity to remember it.BinE.8 MANUAL ANALYSIS OF GPT-4O'S RESPONSES FOR QUESTIONS WITH EMPTY ANSWERSWe manually analyze the answers provided by gpt-4o on the long book (default Claude book) for the questions with empty answers (0 matching events).Of the 150 questions with 0 matching events, 24 (16%) produced incorrect answers.Notably, all incorrect predictions were still contextually relevant to the book's content.The 24 failed zero-event questions can be categorized into two types (as described in Appendix B. -Questions using at least one element from outside the book (sampled from the unused universe) -All involve temporal elements -Consistent cue patterns: (t,</em>,<em>,</em>), (t,<em>,</em>,c), or (t,<em>,ent,</em>) Detailed analysis of the 7 outer questions (outer elements below are "August 24, 2024", "Chess Championship", and "Zoe Rivera"):1. Three questions about "August 24, 2024" (date not in book):• Model fabricated answers using elements from different chapters with answers covering the locations (('One World Trade Center', 'American Museum of Natural History', 'Trinity Church'), the entities ('Scarlett Thomas', 'Julian Ross', 'Maya Smith', 'Mila Gonzalez') and the events content ('Storytelling Festival', 'Carnival', 'Murder Mystery Dinner')) • Upon examination, we found that the model combined a Storytelling Festival (actually in chapter 147 on Dec 25, 2025) featuring a Storytelling Festival at the American Museum of Natural History, with a Murder Mystery Dinner (actually in chapter 120 on Nov 13, 2026) at One World Trade Center with Scarlett Thomas.2. One "Chess Championship" question (event not in book) for April 09, 2026:• Model showed explicit uncertainty in its response: "The events related to the Chess Championship on April 09, 2026, took place at the following locations: 1. High Line, 2. Lincoln Center (Note: The text does not explicitly mention a "Chess Championship" on April 09, 2026, but these locations match the date provided in the question.If the events do not align with the mentioned event, it might be necessary to re-evaluate the context for any additional details.)" • Verified: "chess" never appears in book • Date (April 09, 2026) exists but with different locations, including High Line but not Lincoln Center.3. One "Charity Gala" question for April 09, 2026 (again event not in the book):• Model gave confident but incorrect answer: "The events related to the Charity Gala on April 09, 2026, took place at the following locations: 1. High Line 2. Lincoln Center.I hope this helps!Let me know if there is anything else you need."• Our ground truth shows the only High Line event on that date was an Astronomy Night.4. Two questions about "Zoe Rivera" (entity not in book):• The chapters corresponding to the predicted answers contain no similar names (neither matching first nor last names).These examples highlight why a comprehensive automated analysis would require substantial effort, that we leave for future work.In a dramatic turn of events on May 11, 2026, Benjamin Green found himself documenting the rapid transformation of peaceful suburban streets into raging torrents of muddy water.The local meteorological station's emergency sirens blared through the rain−soaked air as Hamza Avila and Koa Berlin, emergency response coordinators, rushed to evacuate residents from the low−lying areas.Rising waters had already submerged vehicles to their windows, while the relentless downpour continued to intensify, creating treacherous conditions across the region.As the situation in New South Wales deteriorated, Benjamin witnessed a flash flood emergency that would later be described as unprecedented in its ferocity.Water levels rose at an alarming rate of nearly one meter per hour, prompting Emilia Hooks, a veteran emergency services spokesperson, to declare it a "catastrophic event."The flood's destructive force was evident as debris−laden waters crashed through streets, uprooting trees and damaging infrastructure.Local authorities reported that over 300 residents were evacuated to emergency shelters, while rescue teams conducted more than 50 water rescues throughout the affected areas.The disaster response teams continue to monitor the situation as meteorologists predict additional rainfall in the coming hours.Listing 19: Excerpt chapter from the news book.Event information and secondary entities have been highlighted.The generated content is fictional.G.2 EVALUATION OF ADDITIONAL BENCHMARKSWe evaluate these additional books using GPT-4o (our highest-performing model according to Fig.3) on simple recall tasks to assess generalization across domains.The results are summarized in Tab.27, which includes the previously obtained results from the default book.These experiments confirm our previous finding of a consistent performance decline for queries with two or more matching events.G.3 SUMMARIZATION OF THE PRODUCED BENCHMARKSFinally, we present in Tab.28 an overview of our 11 benchmarks and their key characteristics.Preprint H DISCLAIMERThis benchmark is a work of fiction.Unless otherwise indicated, all the names, characters, businesses, places, events and incidents in this book are used in a fictitious manner.Any resemblance to actual persons, living or dead, or actual events is purely coincidental.
Why does claude sometimes follow up his response with "human:" and then some response i presume he's thinking i will respond with?. 2023</p>
<p>Many-shot in-context learning. Rishabh Agarwal, Avi Singh, M Lei, Bernd Zhang, Stephanie Bohnet, Ankesh Chan, Zaheer Anand, Azade Abbas, John D Nova, Eric Co-Reyes, Chu, arXiv:2404.110182024arXiv preprint</p>
<p>Episodic future thinking. M Cristina, Daniela K O' Atance, Neill, Trends in cognitive sciences. 5122001</p>
<p>Online evaluation of novel choices by simultaneous representation of multiple memories. Raymond J Helen C Barron, Timothy Ej Dolan, Behrens, Nature neuroscience. 16102013</p>
<p>Longformer: The long-document transformer. Iz Beltagy, Matthew E Peters, Arman Cohan, arXiv:2004.051502020arXiv preprint</p>
<p>Should we really use post-hoc tests based on mean-ranks?. Alessio Benavoli, Giorgio Corani, Francesca Mangili, The Journal of Machine Learning Research. 1712016</p>
<p>Navigating through digital folders uses the same brain structures as real world navigation. Yael Benn, Ofer Bergman, Liv Glazer, Paris Arent, Iain D Wilkinson, Rosemary Varley, Steve Whittaker, Scientific reports. 51147192015</p>
<p>Emergent and predictable memorization in large language models. Stella Biderman, Lintang Usvsn Sai Prashanth, Hailey Sutawika, Quentin Schoelkopf, Shivanshu Anthony, Edward Purohit, Raf, arXiv:2304.111582023arXiv preprint</p>
<p>Long-span question-answering: Automatic question generation and qa-system ranking via side-by-side evaluation. Bernd Bohnet, Kevin Swersky, Rosanne Liu, Pranjal Awasthi, Azade Nova, Javier Snaider, Hanie Sedghi, Aaron T Parisi, Michael Collins, Angeliki Lazaridou, arXiv:2406.001792024arXiv preprint</p>
<p>Freebase: a collaboratively created graph database for structuring human knowledge. Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, Jamie Taylor, Proceedings of the 2008 ACM SIGMOD international conference on Management of data. the 2008 ACM SIGMOD international conference on Management of data2008</p>
<p>The human hippocampus and spatial and episodic memory. Neil Burgess, Eleanor A Maguire, John O' Keefe, Neuron. 3542002</p>
<p>The secret sharer: Evaluating and testing unintended memorization in neural networks. Nicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej Kos, Dawn Song, USENIX Security Symposium. 2019267</p>
<p>Extracting training data from large language models. Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, 30th USENIX Security Symposium (USENIX Security 21). 2021</p>
<p>Quantifying memorization across neural language models. Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, Chiyuan Zhang, arXiv:2202.076462022arXiv preprint</p>
<p>Wenhu Chen, Xinyi Wang, William Yang, Wang , arXiv:2108.06314A dataset for answering time-sensitive questions. 2021arXiv preprint</p>
<p>Tempquestions: A benchmark for temporal question answering. Rewon Child, Scott Gray, Alec Radford, Ilya Sutskever, arXiv:1904.10509Companion Proceedings of the The Web Conference. Zhen Jia, Abdalghani Abujabal, Rishiraj Saha Roy, Jannik Strötgen, Gerhard Weikum, 2019. 2018. 2018arXiv preprintGenerating long sequences with sparse transformers</p>
<p>Reality monitoring. K Marcia, Carol L Johnson, Raye, Psychological review. 881671981</p>
<p>Source monitoring. Shahin Marcia K Johnson, Hashtroudi, Lindsay Stephen, Psychological bulletin. 114131993</p>
<p>Gregory Kamradt, Needle In A Haystack -Pressure Testing LLMs. 2023</p>
<p>Large language models struggle to learn long-tail knowledge. Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, Colin Raffel, arXiv:2211.084112022arXiv preprint</p>
<p>Realtime qa: what's the answer right now?. Jungo Kasai, Keisuke Sakaguchi, Ronan Le Bras, Akari Asai, Xinyan Yu, Dragomir Radev, Noah A Smith, Yejin Choi, Kentaro Inui, Advances in Neural Information Processing Systems. 202436</p>
<p>Generalization through memorization: Nearest neighbor language models. Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, Mike Lewis, arXiv:1911.001722019arXiv preprint</p>
<p>How bpe affects memorization in transformers. Eugene Kharitonov, Marco Baroni, Dieuwke Hupkes, arXiv:2110.027822021arXiv preprint</p>
<p>The autobiographical memory interview (ami) in organic and psychogenic amnesia. Michael D Kopelman, Memory. 221994</p>
<p>Babilong: Testing the limits of llms with long context reasoning-in-a-haystack. Yuri Kuratov, Aydar Bulatov, Petr Anokhin, Ivan Rodkin, Dmitry Sorokin, Artyom Sorokin, Mikhail Burtsev, arXiv:2406.101492024arXiv preprint</p>
<p>Learning dense representations of phrases at scale. Jinhyuk Lee, Mujeen Sung, Jaewoo Kang, Danqi Chen, arXiv:2012.126242020arXiv preprint</p>
<p>Aging and autobiographical memory: dissociating episodic from semantic retrieval. Brian Levine, Eva Svoboda, Janine F Hay, Gordon Winocur, Morris Moscovitch, Psychology and aging. 1746772002</p>
<p>Needlebench: Can llms do retrieval and reasoning in 1 million context window?. Mo Li, Songyang Zhang, Yunxin Liu, Kai Chen, arXiv:2407.119632024arXiv preprint</p>
<p>Hippocampal "time cells" bridge the gap in memory for discontiguous events. J Christopher, Kyle Q Macdonald, Uri T Lepage, Howard Eden, Eichenbaum, Neuron. 7142011</p>
<p>Recognizing: The judgment of previous occurrence. George Mandler, Psychological review. 8732521980</p>
<p>Henrique Pedro, Zita Martins, André Ft Marinho, Martins, arXiv:2109.00301arXiv:2305.14552Sources of hallucination by large language models on inference tasks. Nick Mckenna, Tianyi Li, Liang Cheng, Mohammad Javad Hosseini, Mark Johnson, Mark Steedman, 2021. 2023arXiv preprint∞-former: Infinite memory transformer</p>
<p>Locating and editing factual associations in gpt. Kevin Meng, David Bau, Alex Andonian, Yonatan Belinkov, Advances in Neural Information Processing Systems. 2022a35</p>
<p>Augmenting language models with long-term memory. Preprint Weizhi, Wang , Li Dong, Hao Cheng, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, Furu Wei, arXiv:2306.071742023arXiv preprint</p>
<p>Towards ai-complete question answering: A set of prerequisite toy tasks. Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M Rush, Bart Van Merriënboer, Armand Joulin, Tomas Mikolov, arXiv:1502.056982015arXiv preprint</p>
<p>The rivermead behavioural memory test (rbmt). Janet Ba Wilson, Alan Cockburn, Baddeley, 1985Bury St EdmundsThames Valley Test Company</p>
<p>An efficient memory-augmented transformer for knowledge-intensive nlp tasks. Yuxiang Wu, Yu Zhao, Baotian Hu, Pasquale Minervini, Pontus Stenetorp, Sebastian Riedel, arXiv:2210.167732022arXiv preprint</p>
<p>Efficient streaming language models with attention sinks. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, Mike Lewis, arXiv:2309.174532023arXiv preprint</p>
<p>Infinite bench: Extending long context evaluation beyond 100k tokens. Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo Hao, Xu Han, Zhen Thai, Shuo Wang, Zhiyuan Liu, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics20241</p>
<p>Instructions: 1. For each item in the predicted list. Chen Zhu, Ankit Singh Rawat, Manzil Zaheer, Srinadh Bhojanapalli, Daliang Li, Felix Yu, Sanjiv Kumar, arXiv:2012.003632020arXiv preprintModifying memories in transformer models. find the first corresponding index from the groundtruth list that hasn't been used yet</p>
<p>Assign indexes based on these rules: a. If a match is found and the groundtruth index hasn't been used, assign that index. b. If no match is found, or if all matching indexes have already been used. </p>
<p>Always use the earliest matching index from the groundtruth list, even if there's an exact match later. </p>
<p>Output your evaluation in the following JSON format: {{ "groundtruth_indexes": {groundtruth_indexes}. Concise explanation of index assignments" }} Consider these examples: Example 1: Groundtruth list: ['Ice Preservation Discussions', 'Theater Show. Tech HackathonProvide a brief explanation of your index assignments. index1, index2, ...], "explanation. Predicted list. Ice Preservation Talks'</p>
<p>Tech Hackathon has no match (−1), Ice Preservation Talks matches Ice Preservation Discussions (index 0). Theater Performance matches Theater Show (index 1). Theater Show. 2Groundtruth list: ['Ice Preservation Discussions. Theater Performance'</p>
<p>Theater Performance. Tech HackathonPredicted. Ice Preservation Talks'</p>
<p>Theater Performance matches Theater Show (index 1, first available match), Tech Hackathon has no match (−1), Ice Preservation Talks matches Ice Preservation Discussions (index 0). 0, 1, 2, 3], "predicted_indexes": [1, −1, 0], "explanation</p>            </div>
        </div>

    </div>
</body>
</html>
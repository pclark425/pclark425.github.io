<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9467 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9467</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9467</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-164.html">extraction-schema-164</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to predict or assign probabilities to specific future real-world scientific discoveries, including how the probabilities are generated, how accuracy is evaluated, and any results, limitations, or comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-645284e103010343446ebaa39f36a32b168652e6</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/645284e103010343446ebaa39f36a32b168652e6" target="_blank">ClimateGPT: Towards AI Synthesizing Interdisciplinary Research on Climate Change</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> ClimateGPT, a model family of domain-specific large language models that synthesize interdisciplinary research on climate change, is introduced and a suite of automatic climate-specific benchmarks to evaluate LLMs is proposed.</p>
                <p><strong>Paper Abstract:</strong> This paper introduces ClimateGPT, a model family of domain-specific large language models that synthesize interdisciplinary research on climate change. We trained two 7B models from scratch on a science-oriented dataset of 300B tokens. For the first model, the 4.2B domain-specific tokens were included during pre-training and the second was adapted to the climate domain after pre-training. Additionally, ClimateGPT-7B, 13B and 70B are continuously pre-trained from Llama~2 on a domain-specific dataset of 4.2B tokens. Each model is instruction fine-tuned on a high-quality and human-generated domain-specific dataset that has been created in close cooperation with climate scientists. To reduce the number of hallucinations, we optimize the model for retrieval augmentation and propose a hierarchical retrieval strategy. To increase the accessibility of our model to non-English speakers, we propose to make use of cascaded machine translation and show that this approach can perform comparably to natively multilingual models while being easier to scale to a large number of languages. Further, to address the intrinsic interdisciplinary aspect of climate change we consider different research perspectives. Therefore, the model can produce in-depth answers focusing on different perspectives in addition to an overall answer. We propose a suite of automatic climate-specific benchmarks to evaluate LLMs. On these benchmarks, ClimateGPT-7B performs on par with the ten times larger Llama-2-70B Chat model while not degrading results on general domain benchmarks. Our human evaluation confirms the trends we saw in our benchmarks. All models were trained and evaluated using renewable energy and are released publicly.</p>
                <p><strong>Cost:</strong> 0.005</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9467",
    "paper_id": "paper-645284e103010343446ebaa39f36a32b168652e6",
    "extraction_schema_id": "extraction-schema-164",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0048224999999999995,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>ClimateGPT: Towards AI Synthesizing Interdisciplinary Research on Climate Change</h1>
<p>David Thulke ${ }^{1,4} \quad$ Yingbo Gao $^{1} \quad$ Petrus Pelser $^{3} \quad$ Rein Brune ${ }^{3}$<br>Rricha Jalota ${ }^{1} \quad$ Floris Fok ${ }^{3} \quad$ Michael Ramos $^{2} \quad$ Ian van Wyk ${ }^{3}$<br>Abdallah Nasir ${ }^{1}$ Hayden Goldstein ${ }^{3}$ Taylor Tragemann ${ }^{1}$ Katie Nguyen ${ }^{1}$<br>Ariana Fowler ${ }^{2}$ Andrew Stanco ${ }^{2}$ Jon Gabriel ${ }^{2}$ Jordan Taylor ${ }^{2}$<br>Dean Moro ${ }^{2}$ Evgenii Tsymbalov ${ }^{1}$ Juliette de Waal ${ }^{3}$ Evgeny Matusov ${ }^{1}$<br>Mudar Yaghi ${ }^{1}$ Mohammad Shihadah ${ }^{1}$ Hermann Ney ${ }^{1,4}$<br>Christian Dugast ${ }^{1}$ Jonathan Dotan ${ }^{2}$ Daniel Erasmus ${ }^{3}$<br>${ }^{1}$ AppTek ${ }^{2}$ EQTY Lab ${ }^{3}$ Erasmus.AI ${ }^{4}$ RWTH Aachen University<br>dthulke@apptek.com climategpt@dtn.net<br>eci.io</p>
<h4>Abstract</h4>
<p>This paper introduces ClimateGPT, a model family of domain-specific large language models that synthesize interdisciplinary research on climate change. We trained two 7B models from scratch on a science-oriented dataset of 300B tokens. For the first model, the 4.2B domain-specific tokens were included during pre-training and the second was adapted to the climate domain after pre-training. Additionally, ClimateGPT-7B, 13B and 70B are continuously pre-trained from Llama 2 on a domain-specific dataset of 4.2B tokens. Each model is instruction fine-tuned on a high-quality and human-generated domain-specific dataset that has been created in close cooperation with climate scientists. To reduce the number of hallucinations, we optimize the model for retrieval augmentation and propose a hierarchical retrieval strategy. To increase the accessibility of our model to non-English speakers, we propose to make use of cascaded machine translation and show that this approach can perform comparably to natively multilingual models while being easier to scale to a large number of languages. Further, to address the intrinsic interdisciplinary aspect of climate change we consider different research perspectives. Therefore, the model can produce in-depth answers focusing on different perspectives in addition to an overall answer. We propose a suite of automatic climate-specific benchmarks to evaluate LLMs. On these benchmarks, ClimateGPT-7B performs on par with the ten times larger Llama-2-70B Chat model while not degrading results on general domain benchmarks. Our human evaluation confirms the trends we saw in our benchmarks. All models were trained and evaluated using renewable energy and are released publicly ${ }^{1}$.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>Contents</h1>
<p>1 Introduction ..... 4
1.1 Technical Approach ..... 5
2 Domain-Specific Pre-Training ..... 6
2.1 Model Architecture ..... 6
2.2 Pre-Training Dataset ..... 7
2.3 Continued Pre-Training ..... 8
2.4 From-Scratch Pre-Training ..... 9
2.5 Training Hardware ..... 10
3 Instruction Fine-Tuning ..... 11
3.1 Senior Expert Interview Demonstrations ..... 12
3.2 Grounded Expert Demonstrations ..... 12
3.3 Grounded Non-Expert Demonstrations ..... 13
3.4 Synthetically Generated Demonstrations ..... 14
3.5 General Domain Data ..... 14
3.6 Safety Data ..... 15
3.7 Data Preparation ..... 15
3.8 Training ..... 16
4 Retrieval Augmented Generation ..... 17
4.1 Retrieval Dataset ..... 18
4.2 Retrieval Approach ..... 18
4.3 Grounding ..... 19
4.4 Three Dimensions ..... 20
5 Multilinguality ..... 20
5.1 In-Domain Data ..... 21
5.2 Training ..... 21
5.3 MT Evaluation ..... 22
5.4 Glossary Adaptation ..... 22
6 Automatic Evaluation ..... 23
6.1 Climate-Specific Benchmarks ..... 23
6.2 General Domain Benchmarks ..... 24
6.3 Results ..... 24
6.4 Cascaded Machine Translation ..... 26
7 Human Evaluation ..... 27
7.1 Results ..... 28</p>
<p>8 Responsible AI ..... 28
8.1 Content Moderation ..... 28
8.2 Transparency ..... 29
8.3 Environmental Impact ..... 29
9 Conclusion ..... 29
10 Limitations ..... 30
A Appendix ..... 41
A. 1 Model Card ..... 41
A. 2 Sustainability Scorecard ..... 42
A. 3 Curated Climate-Specific Pre-Training Data Details ..... 43
A. 4 AppTek Non-Expert IFT Data Details ..... 45
A. 5 Retrieval Augmentation Example ..... 46
A. 6 System Prompts ..... 47
A. 7 Prompts used in Climate-Specific Automatic Evaluation Tasks ..... 48
A. 8 Prompt for Retrieval Database Tagging ..... 49
A. 9 Full Automatic Evaluation Results ..... 51
A. 10 MT Glossary Examples ..... 52</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overview of automatic evaluation results on climate-specific benchmarks.</p>
<h1>1 Introduction</h1>
<p>Large Language Models (LLMs) have the exceptional ability to comprehend and generate human-like text that empowers them to address a wide array of tasks with Claude- $2^{2}$, GPT-4 (OpenAI, 2023), Llama-2 (Touvron et al., 2023b) or Gemini (Gemini Team, Google, 2023) to cite a few. They have been trained on diverse large datasets (from hundreds of billions to trillions of tokens) covering a wide range of topics and domains. The universality of these general-purpose models has made them accessible for a broad spectrum of applications: from text comprehension, over content generation and summarization up to chatbots and much more. Recent research has pointed to the potential of LLMs trained on domain-specific data, e.g. Biomedical sciences (Lee et al., 2020), Finance (Wu et al., 2023) and Medicine (Peng et al., 2023; Luo et al., 2022). These models, while being smaller, have outperformed generalpurpose models in their respective domains. The work reported in this paper continues this line of research, addressing one of the most pressing and complex challenges this time: climate change.
Climate change stands out as a multifaceted discipline, covering climate science (the natural science behind modeling climate and the development of the earth's atmosphere) as well as human issues related to climate that impact our environment, our economies, our societies, public health and biodiversity. Right now, we are moving to the brink of multiple risk tipping points (Eberle et al., 2023). Efforts are underway to avoid getting at these potentially irreversible changes in the climate system. Accelerating this process requires global climate awareness and collective knowledge, that we call "climate social intelligence". Building an LLM that addresses climate questions requires access to this collective knowledge, understanding, and decision-making capacity of the human population to harness the collective climate social intelligence available.
We propose an LLM on climate change, called ClimateGPT, which should help the diverse science communities involved to exchange information and knowledge along the three major multi-disciplinary dimensions it covers at large: environmental and natural science, economics, and social science. As opposed to other work done around climate-related LLMs, e.g. ClimateBERT (Webersinke et al., 2022), ClimateGPT-2 (Vaghefi et al., 2022), MBZUAI Arabic Mini-ClimateGPT (Mullappilly et al., 2023), ChatClimate (Vaghefi et al., 2023), the focus of our work was to develop high quality in-domain Instruction Fine-Tuning (IFT) data as well as to train our model with as much climate data as possible, specifically technical reports from the Intergovernmental Panel on Climate Change (IPCC) as well as top papers from climate change research and related fields, such as the UN Sustainable Development Goals. Further, we developed a multi-domain Large Language Model, which can give four types of answers for each request: a natural science-related answer, an answer about the</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>economic aspects of climate change, as well as an answer about social impacts. The fourth answer, the main one, gives a general high-level overview, addressing all of these sub-fields.</p>
<p>This paper introduces a large language model that seeks to be used across domains by people learning from and collaborating with other specialists in the realm of climate information, rather than merely acting as a chatbot. We are looking at it as a climate intelligence platform that can assist governments, organizations, and individuals in making informed decisions and that contributes to a global social intelligence related to climate.</p>
<h1>1.1 Technical Approach</h1>
<p>This section outlines our technical approach and the different steps we have taken to develop ClimateGPT.
Language Modeling is done with a large decoder-only Transformer (Vaswani et al., 2017b; Liu et al., 2018) architecture, which is in line with most of the recent literature on large language models (LLMs) (Radford et al., 2019; Brown et al., 2020; Scao et al., 2022; Biderman et al., 2023). The model represents tokens as continuous-valued hidden vectors and makes use of the attention mechanism (Bahdanau et al., 2015) to model inter-token dependencies. The training criterion is cross-entropy, which rewards the model for high probability mass on the correct target token.
From-Scratch (FS) training is done to obtain a foundation language model in the climate domain, the training data for which is cleaned with the climate domain in mind. We train a climate foundation model as well as a general domain model with a focus on scientific content to study the effect of up-sampling domain-specific data during foundation model training. We follow closely the training hyper-parameters that were documented in the Llama-2 paper (Touvron et al., 2023b).
Continued Pre-Training (CPT) is a common alternative to training a new foundation model from scratch (Gupta et al., 2023; Chen et al., 2023). The goal is to adapt an existing LLM trained on a large set of general domain data to the target domain by continuing the pre-training on a smaller set of in-domain data. After an initial evaluation, we focus on the Llama-2 model series as well as for our general not climate-specific from-scratch model. During CPT, we keep the training criterion of the pre-trained model.
Instruction Fine-Tuning (IFT) is an important step to inject instruction-following capabilities into the model. In the literature, this is also often referred to as Supervised Fine-Tuning (SFT). We prefer IFT to make the distinction to domain adaptation via CPT or task-specific supervised fine-tuning approaches. Instruction-completion pairs both from the general domain and climate domain are prepared and gear the model towards following user instructions. We collaborate with climate experts to create a high-quality humangenerated dataset. During the data collection, we follow standard approaches (Ouyang et al., 2022a) and also tune the distribution among our and different public instruction-tuning datasets. Although our model is capable of chatting, we focus on its question-answering and instruction-following aspects, which also greatly simplify the instruction fine-tuning data creation and retrieval steps.
Retrieval Augmented Generation (RAG) is implemented with high-quality climate resources to increase factuality as well as to extend the system with new knowledge. We crawl text from manually curated sources and process these sources into smaller chunks. To retrieve relevant documents for a user query, we use a bi-encoder model to calculate embeddings and make use of efficient nearest-neighbor search. During the generation phase, the user instruction is concatenated with the most relevant text chunks for the model to come up with more reliable and stable answers. As the sources of retrieved documents are known, RAG also gives the possibility to provide citations for parts of the generated output.
Cascaded Machine Translation (MT) is included at the system level to enable support for multiple languages. Specifically, non-English queries are first translated to English for our underlying LLM and retrieval engine to generate an English answer. Finally, this answer is translated back to the original language for display.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Base Model</th>
<th>Tokens</th>
<th>LR</th>
<th>GPU Hours</th>
</tr>
</thead>
<tbody>
<tr>
<td>ClimateGPT-70B</td>
<td>Llama-2 70B</td>
<td>4.2B</td>
<td>$1 \cdot 10^{-5}$</td>
<td>2,182</td>
</tr>
<tr>
<td>ClimateGPT-13B</td>
<td>Llama-2 13B</td>
<td>4.2B</td>
<td>$1 \cdot 10^{-5}$</td>
<td>301</td>
</tr>
<tr>
<td>ClimateGPT-7B</td>
<td>Llama-2 7B</td>
<td>4.2B</td>
<td>$1 \cdot 10^{-5}$</td>
<td>157</td>
</tr>
<tr>
<td>ClimateGPT-FSC-7B</td>
<td>-</td>
<td>319.5B</td>
<td>$3 \cdot 10^{-4}$</td>
<td>14,131</td>
</tr>
<tr>
<td>ClimateGPT-FSG-7B</td>
<td>-</td>
<td>323.7B</td>
<td>$3 \cdot 10^{-4}$</td>
<td>14,288</td>
</tr>
</tbody>
</table>
<p>Table 1: ClimateGPT model variants.</p>
<p>Benchmarking and Evaluation is done both automatically and with human experts. For automatic evaluation, we evaluate the model both on climate-domain tasks and generaldomain tasks. Furthermore, we describe our approach to human evaluation with domain experts, hoping to address the limitations that come with the automatic evaluation of LLMs. We present the results of an initial human evaluation comparing our main model variants.</p>
<p>Responsible AI is an important aspect of our work because as LLMs become stronger, we strongly believe that the models should "do good." To this end, we include instruction fine-tuning data that teaches the model to avoid answering unwanted or even malicious user queries. During the whole development process, we carefully considered and actively worked on reducing the environmental impact of our work. Finally, the models and evaluation protocols are released publicly to increase the reproducibility of our work.</p>
<h1>2 Domain-Specific Pre-Training</h1>
<p>Foundation models are pre-trained on vast datasets encompassing a wide array of domains (Brown et al., 2020; Touvron et al., 2023a,b). These domains range from general knowledge and common sense reasoning to more specialized areas like science, technology, and literature (Gao et al., 2021a; Penedo et al., 2023). Training on a large-scale dataset enables the models to exhibit impressive zero-shot and few-shot (in-context learning) learning capabilities (Brown et al., 2020; Kojima et al., 2022; Wei et al., 2022a,b), allowing them to perform reasonably well on tasks they are not explicitly trained for. However, despite their versatility, foundation models are not intrinsically designed to possess deep expertise in specific domains. Therefore, recent efforts focused on training domain-specific language models that are either significantly smaller or outperform their general domain counterparts on domains like finance (Wu et al., 2023), science (Taylor et al., 2022) or medicine (Singhal et al., 2023; Chen et al., 2023). To create such a model one can either perform domain adaptation on an existing general domain model (Singhal et al., 2023; Chen et al., 2023) or train a new model from scratch (Wu et al., 2023; Taylor et al., 2022). Which approach is preferable depends on various factors, like the total compute budget, the amount of available domain-specific pre-training data and how well the target domain is represented in the general domain data. To gain insights into these tradeoffs for the climate change domain, we compare both approaches.</p>
<p>In this section, we first describe the general model architecture we used for ClimateGPT. Then, we describe how we curated and collected our climate change and science-specific pre-training dataset. Next, we make use of continued pre-training as a domain adaptation technique to adapt a strong general domain model to the target domain. Finally, we describe how we train a climate-specific model from scratch.</p>
<h3>2.1 Model Architecture</h3>
<p>We follow Llama-2 (Touvron et al., 2023b) closely in terms of the model architecture. Specifically, the model is a decoder-only Transformer (Vaswani et al., 2017b; Liu et al., 2018) network with word embedding layers sandwiching a stack of self-attention layers. Key components, such as pre-normalization (Xiong et al., 2020) with RMSNorm (Zhang and Sennrich, 2019), SwiGLU (Shazeer, 2020) activation function, and rotary positional embeddings (RoPE) (Su et al., 2023) are retained in this work. Improvements on top of Llama-1 (Touvron et al., 2023a), such as increased context length (4096) and the introduction</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Subset</th>
<th style="text-align: right;">Tokens</th>
<th style="text-align: right;">Weight</th>
<th style="text-align: right;">Tokens in model</th>
<th style="text-align: right;"></th>
<th style="text-align: right;">Percentage of data</th>
<th style="text-align: right;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;">FSG</td>
<td style="text-align: right;">FSC</td>
<td style="text-align: right;">FSG</td>
<td style="text-align: right;">FSC</td>
</tr>
<tr>
<td style="text-align: left;">news</td>
<td style="text-align: right;">193.9</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">125.1</td>
<td style="text-align: right;">120.0</td>
<td style="text-align: right;">$39.1 \%$</td>
<td style="text-align: right;">$37.5 \%$</td>
</tr>
<tr>
<td style="text-align: left;">publications</td>
<td style="text-align: right;">23.1</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">59.6</td>
<td style="text-align: right;">57.1</td>
<td style="text-align: right;">$18.6 \%$</td>
<td style="text-align: right;">$17.9 \%$</td>
</tr>
<tr>
<td style="text-align: left;">modern books</td>
<td style="text-align: right;">28.4</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">55.0</td>
<td style="text-align: right;">52.7</td>
<td style="text-align: right;">$17.2 \%$</td>
<td style="text-align: right;">$16.5 \%$</td>
</tr>
<tr>
<td style="text-align: left;">patents</td>
<td style="text-align: right;">19.5</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">50.2</td>
<td style="text-align: right;">48.1</td>
<td style="text-align: right;">$15.7 \%$</td>
<td style="text-align: right;">$15.1 \%$</td>
</tr>
<tr>
<td style="text-align: left;">wikipedia</td>
<td style="text-align: right;">6.3</td>
<td style="text-align: right;">5</td>
<td style="text-align: right;">20.4</td>
<td style="text-align: right;">19.6</td>
<td style="text-align: right;">$6.4 \%$</td>
<td style="text-align: right;">$6.1 \%$</td>
</tr>
<tr>
<td style="text-align: left;">policy and finance</td>
<td style="text-align: right;">3.7</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">7.1</td>
<td style="text-align: right;">6.8</td>
<td style="text-align: right;">$2.2 \%$</td>
<td style="text-align: right;">$2.1 \%$</td>
</tr>
<tr>
<td style="text-align: left;">science</td>
<td style="text-align: right;">0.7</td>
<td style="text-align: right;">5</td>
<td style="text-align: right;">2.2</td>
<td style="text-align: right;">2.1</td>
<td style="text-align: right;">$0.7 \%$</td>
<td style="text-align: right;">$0.6 \%$</td>
</tr>
<tr>
<td style="text-align: left;">climate change</td>
<td style="text-align: right;">4.2</td>
<td style="text-align: right;">5</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">13.0</td>
<td style="text-align: right;">$0.0 \%$</td>
<td style="text-align: right;">$4.1 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Total</td>
<td style="text-align: right;">279.7</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">319.5</td>
<td style="text-align: right;">$100 \%$</td>
<td style="text-align: right;">319.5</td>
<td style="text-align: right;">$100 \%$</td>
</tr>
</tbody>
</table>
<p>Table 2: Subset breakdown of the 300B-token from scratch pre-training dataset.
of grouped-query attention (GQA) Ainslie2023 for larger model variants were also kept.</p>
<h1>2.2 Pre-Training Dataset</h1>
<p>The preparation of high-quality in-domain data is important for the success of the model. Therefore, we started with a corpus of roughly 300B tokens from curated sources compiled by Erasmus.AI. While the corpus spans a wide range of domains sources were evaluated and selected based on their relevance to the topic climate, humanitarian issues and science. The upper part of Table 2 shows the different subsets of the dataset and the corresponding weight for model training. The last columns indicate the effective number of tokens each of the from-scratch models has seen during from-scratch pre-training and the resulting data distribution (see Section 2.4).</p>
<p>The news subset is a web crawl with a focus on relevant news and blog articles. It also contains data from an internal extreme weather index. Publications is a collection of abstract and full-text papers. The Modern books set consists of fiction and non-fiction books and should help to model long-range context. Patents are collected mostly from the United States Patent and Trademark Office. Wikipedia is a recent dump of the English Wikipedia website. Policy and finance is a collection of text related to law, finance and companies and stocks in the climate sector. Finally, science covers other science and climate-related texts like EPA documents and ESG reports.</p>
<p>From this dataset, we identified high-quality sources such as scientific papers, and further included primary sources, such as reports from the Intergovernmental Panel on Climate Change (IPCC), and applied cleaning and filtering using keywords and topic classification. In addition, we included our manually curated climate-specific data. More details on these datasets are described in Appendix A.3. In total, we arrive at a corpus of 4.2B climate-specific tokens which is used for continued pre-training.</p>
<p>To improve the quality of the training data, a set of cleaning, filtering, and pre-processing steps was done, which included:</p>
<ul>
<li>filtering of sources from unrelated domains, such as sport and entertainment, politics and crime, as well as fiction. By doing so, we hope to limit the number of opinion pieces and information irrelevant to the climate domain;</li>
<li>personal identifiable information reduction, such as email addresses, telephone numbers, URLs etc.;</li>
<li>keeping sentences with a Flesch reading score Kincaid1975 between 5 and 120 ;</li>
<li>handling of errors related to character encoding and special symbols;</li>
<li>elimination of documents by text length;</li>
</ul>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Training loss of the CPT models.</p>
<ul>
<li>focusing on sources from the past eight years;</li>
<li>aggregating themes concerning climate, humanitarian issues and science;</li>
<li>discovering key sentences and entities that are associated with climate;</li>
<li>filtering based on symbol distribution, i.e. removing documents that contain at least $80 \%$ non-symbols;</li>
<li>filtering based on language identification, i.e. removing documents that do not score above $85 \%$ to be in English;</li>
<li>removing double spaces, consecutive empty newlines, lines containing long repeating characters such as " $======$ " and " $+++++$ ", etc.;</li>
<li>deduplication based on MinHash (Broder, 1997) with proprietary extensions by Erasmus.AI as well as removal of duplication identifiable in the source metadata.</li>
</ul>
<h1>2.3 Continued Pre-Training</h1>
<p>We employ domain adaptation methods, as we aim to develop a model that is specialized for climate change and possesses understanding and domain-specific knowledge. Domain adaptation can help tailor the foundation model to the climate domain, ensuring that it not only retains its broad knowledge base but also develops a more refined understanding of climate-specific concepts, terminologies, and contextual nuances.
Domain adaptation (Ben-David et al., 2010), while not new, remains a cornerstone in the evolution of machine learning systems, e.g. in language modeling (Karouzos et al., 2021), machine translation (Kim et al., 2019), and automatic speech recognition (Baevski et al., 2020). Fundamentally, the method involves the continued training of a baseline model on specific, in-domain data to enhance its performance within that domain. This approach has been widely recognized for its ability to significantly boost a model's proficiency on in-domain test data, while still maintaining robust performance on general tasks. In the context of our work, we adopt this principle to further refine foundation models for climate change applications. We prefer to term this process as "continued pre-training" (CPT), rather than the more commonly used "fine-tuning", to differentiate our approach from other methods like "supervised fine-tuning" (Ouyang et al., 2022a) and to highlight the similarity to the initial pre-training stage. We deliberately apply this CPT step before proceeding to instruction</p>
<p>fine-tuning. If instruction fine-tuning is done before domain adaptation, there is a risk that the model might lose some of its newly acquired instruction-following capabilities. By first adapting the model to the intricacies of the climate domain, we lay a solid foundation upon which instruction fine-tuning can then be built, ensuring a more effective and domain-savvy instruction-following model.
Domain adaptation, while offering significant benefits, presents two primary challenges. The first challenge lies in preparing high-quality data for the in-domain training. The effectiveness of domain adaptation is largely contingent on how closely the distribution of this training data aligns with that of the in-domain test data. The closer the match, the more we can anticipate enhanced performance in domain-specific tasks. To address this we make use of the curated dataset described in Section 2.2. The second challenge is the prevention of degradation in the model's performance on general domain tasks, a phenomenon often referred to as "catastrophic forgetting" (Kirkpatrick et al., 2017) in the literature. This occurs when a model, upon being further trained on specific data, loses its proficiency in tasks it was previously capable of handling. To mitigate this, we carefully tune the batch size, learning rate, learning rate schedule, and data composition for the 7B model variants. Due to time and compute-budget constraints during the project, we did not have time to tune the hyperparameters for the 13B and 70B models and just chose the same values as for the 7 B models.
To choose a foundation model, we considered multiple candidates and chose the one that performed best on our climate-specific benchmarks (discussed in Section 6.1). Candidates that we considered were Llama-2 (Touvron et al., 2023b), Falcon (Almazrouei et al., 2023), Pythia (Biderman et al., 2023) and Jais (Sengupta et al., 2023). From these models, we achieved the best results with Llama-2 (see Tables 11 and 12), and thus we continued this model. Redoing these experiments today, we would also consider Mistral-7B (Jiang et al., 2023) and Mixtral (Jiang et al., 2024), but these models were not available at this time.</p>
<p>For training, we use a fork of NVIDIA's Megatron-LM (Narayanan et al., 2021) by the EPFL LLM Team (Cano et al., 2023; Chen et al., 2023). The main modifications to the original version from Nvidia are support for Llama and other recent models. We use a cosine learning rate schedule with a peak learning rate of $10^{-5}$, a warm-up of 100 steps and decay to a learning rate of $5 \cdot 10^{-6}$. The batch size is set to 1024 and we use the full sequence length of 4096 tokens. For regularization, we use weight decay of $10^{-2}$. All models are trained for 1,000 steps which corresponds to one epoch on the 4.2B climate dataset. The training loss curves for the models are shown in Figure 2.</p>
<p>While we observed that higher learning rates resulted in better training and validation losses, we observed a degradation on our downstream benchmarks. Therefore, we settled with this learning rate as a trade-off between domain adaptation and avoiding overfitting.</p>
<h1>2.4 From-Scratch Pre-Training</h1>
<p>In contrast to the continued pre-training approach, an alternative strategy involves departing from the use of pre-trained models such as Llama-2 (Touvron et al., 2023a) or Falcon (Penedo et al., 2023). Instead, we initiate the weights of a domain-specific foundation model entirely from scratch and directly train it on domain-specific data.
Adopting the approach of training a model from scratch comes with two significant implications. On the one hand, by choosing not to utilize a pre-trained foundation model, we inherently forego the advantages that come from training on the vast corpus of trillions of tokens that such models have been exposed to. These pre-trained models, despite not having fully disclosed datasets, are likely to have been trained on a diverse range of information, some of which could be beneficial for our purposes. On the other hand, initializing the model from scratch offers us complete control over the training data, which is particularly crucial in a field like climate change that is prone to misinformation and bias (Coan et al., 2021). By carefully selecting and curating the data, we can ensure that the model is trained on accurate, reliable, and scientifically valid information. This level of control allows us to mitigate the risk of perpetuating biases or inaccuracies that might be present in larger, less controlled datasets. While we can expect better performance from training on more data (Kaplan et al., 2020; Hoffmann et al., 2022), projects developing domain-specific models</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Training loss of the from-scratch general (orange) and from-scratch climate (purple) models.</p>
<p>often have lower compute budgets compared to general-purpose models with a broader range of applications. Thus, training on less but more relevant and high-quality data could still result in better performance.</p>
<p>In our setup, we align our model architecture closely with the Llama-2-7B model developed by Meta, also utilizing the Llama tokenizer [touvron2023elaborate], which employs the Byte Pair Encoding (BPE) algorithm [sennrich2016semi]. We recognize that developing our own tokenizer, tailored specifically to climate-related terminology, could potentially give better vocabulary compression for domain-specific terms. However, due to time constraints within the project, we left this for future work. Nonetheless, our experience here provides a data point to judge the impact of different training datasets on model performance, while keeping other variables constant. For this reason, we continue with the rest of the development steps, such as instruction fine-tuning, with the from-scratch pre-trained model.</p>
<p>For from-scratch training, we use the same setup as for CPT training. We use a cosine learning rate schedule with a peak learning rate of 3·10<sup>−4</sup>, a warm-up of 100 steps and decay to 10% of the peak learning rate, i.e. to 3·10<sup>−5</sup>. The batch size is set to 1040 and we use the full sequence length of 4096 tokens. For regularization, we use weight decay of 10<sup>−1</sup>. Both models are trained for 75,000 steps the resulting effective tokens seen per subset are shown in Table 2. The training loss curves for the models are shown in Figure 3. To train both models we use the Adam optimizer [kingma2015adam] with β<sub>1</sub> = 0.9, β<sub>2</sub> = 0.95 and ϵ = 10<sup>−5</sup>. While these values are commonly used to train large language models [brown2020language; biderman2023gpt4]; we want to highlight that decreasing the β<sub>2</sub> momentum from the common default value of 0.999 decreases training instabilities and loss spikes caused by large batch sizes [zhai2023gpt4].</p>
<h3>2.5 Training Hardware</h3>
<p>Given the computationally intensive nature of training a foundational model from scratch [hoffmann2022gpt4], there are significant environmental considerations, especially pertinent in the context of our work in the climate domain. Therefore, we choose to utilize a high-performance computing cluster that is entirely powered by hydropower (24g CO<sub>2</sub>eq / kWH [schloemer2014gates]), provided by MLFoundry. The cluster has 32 nodes, each equipped with 8 H100-SXM GPUs. These nodes are interconnected through InfiniBand, ensuring</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Instruction Fine-Tuning Tracks
high-speed data transfer and communication across nodes. Additionally, within each node, the GPUs are connected via NvLink, facilitating efficient intra-node GPU communications. Leveraging Megatron's (Shoeybi et al., 2019; Cano et al., 2023) efficient implementations of data parallelism, tensor parallelism, and pipeline parallelism, we achieved an average training speed of 250 TFLOPS per GPU, and the training run took 3.7 days on 20 nodes. When fully utilized, we assume a power consumption of 775 W per GPU (including CPU).</p>
<h1>3 Instruction Fine-Tuning</h1>
<p>After pre-training, we expect that the resulting domain-specific language models have a deeper understanding and knowledge of the target domain than comparable foundation models. Since these models were merely trained to predict the next token in our pre-training dataset, using them for specific downstream tasks requires careful prompting or providing the model with few-shot examples. Adapting these models to follow users' instructions formulated in natural language and generate answers in a style appropriate for our use case requires Instruction Fine-Tuning (IFT) (Ouyang et al., 2022a). In the literature, this is also often referred to as Supervised Fine-Tuning (SFT), but we use this terminology as a clearer distinction to other fine-tuning steps (e.g. CPT or task-specific fine-tuning). To do this, the model is trained on instruction and corresponding completion pairs. In the following, we are also referring to these pairs as demonstrations. To limit the complexity of the required data, we decided to only focus on prompt and completion pairs and not to collect any multi-turn chat interactions.</p>
<p>To adapt the style of completions to be appropriate for our envisioned use case, we require a sufficiently large amount of in-domain data for IFT. However, collecting such a dataset is challenging, as it requires a certain level of expertise in the target domain. During the project, we had the privilege to be able to work with a small team of climate experts as well as a larger team of non-experts with limited domain knowledge.
Figure 4 shows the different tracks we followed to collect IFT data. The first track of our IFT data consists of demonstrations (i.e. instruction and completion pairs) collected through interviews with senior climate experts (i.e. experienced researchers in the field like professors or other leading experts). During the interviews, the main questions in the field of study of the expert were discussed, implications on other fields as well as different use cases for a climate-specific LLM.
For the remainder of the collected climate-specific IFT data, we provided annotators with existing documents as the basis for the demonstrations they generate. We identified that coming up with new topics can be a limiting factor in the data creation process for the annotators, and this approach can help to concentrate their mental load to write a good completion. Additionally, having control over these "seed" documents means that we can increase the diversity of topics to be covered in the IFT corpus. For the human expert generated part of our IFT data, non-senior climate experts (i.e. graduate or PhD students or other early career researchers) created data based on primary sources (like research papers and technical sections of the IPCC reports). As the time of climate experts is valuable and limited we additionally worked with a larger team of non-expert data annotators. Most primary sources, like climate change papers or technical sections of the IPCC reports, were</p>
<table>
<thead>
<tr>
<th>Domain</th>
<th>Name</th>
<th>Total Size</th>
<th>Training Samples</th>
</tr>
</thead>
<tbody>
<tr>
<td>Climate</td>
<td>Senior Expert Interviews</td>
<td>74</td>
<td>1,332</td>
</tr>
<tr>
<td></td>
<td>Grounded Expert Demonstration</td>
<td>403</td>
<td>7,254</td>
</tr>
<tr>
<td></td>
<td>Grounded Non-Expert Demonstrations</td>
<td>9,663</td>
<td>146,871</td>
</tr>
<tr>
<td></td>
<td>Synthetically Generated Demonstrations</td>
<td>57,609</td>
<td>0</td>
</tr>
<tr>
<td></td>
<td>StackExchange</td>
<td>3,282</td>
<td>9,846</td>
</tr>
<tr>
<td>General</td>
<td>AppTek General</td>
<td>700</td>
<td>2,100</td>
</tr>
<tr>
<td></td>
<td>OASST-1</td>
<td>3,783</td>
<td>11,349</td>
</tr>
<tr>
<td></td>
<td>Dolly</td>
<td>15,001</td>
<td>45,003</td>
</tr>
<tr>
<td></td>
<td>Llama-2 Safety</td>
<td>939</td>
<td>2,817</td>
</tr>
<tr>
<td></td>
<td>FLAN</td>
<td>38,909</td>
<td>30,000</td>
</tr>
<tr>
<td></td>
<td>CoT</td>
<td>448,439</td>
<td>15,000</td>
</tr>
<tr>
<td>Total</td>
<td></td>
<td></td>
<td>271,572</td>
</tr>
</tbody>
</table>
<p>Table 3: Details about the Instruction Fine-Tuning datasets.
not completely comprehensible by the non-experts. Therefore, for this team, we decided to focus on secondary sources such as governmental websites (e.g. from the EPA, NASA, or European Parliament) and summary sections of the IPCC reports. As we were still concerned that we might not be able to collect a sufficient amount of in-domain data, we simultaneously experimented with synthetically generated demonstrations from the documents using a general-purpose LLM as opposed to the manual IFT creation from above.</p>
<p>Finally, to increase the amount and diversity of instruction-tuning data and to be able to reuse well-developed non-climate domain-specific instructions, we made use of existing general domain IFT data. Table 3 gives an overview of the different IFT datasets that were used to train ClimateGPT. The rest of this section is devoted to providing more details on these datasets and how they are used to train ClimateGPT.</p>
<h1>3.1 Senior Expert Interview Demonstrations</h1>
<p>Our vision for the model is for everyone to have a personal climate expert at their fingertips breaking down questions and concepts to the level of expertise of the user. Interviews with climate experts most closely resemble this goal and thus IFT data created in this process is the most valuable data source for ClimateGPT. We started the interviews by defining foundational concepts in the area of expertise of the interviewee and the role of climate change. Second, we discussed current trends in the field and the expected developments in the future. Next, we discussed pivotal findings and research papers in the field and extracted key arguments. Finally, we brainstorm ways in which a climate-specific LLM could be helpful for stakeholders involved in this specific field. As the time of the corresponding experts is very limited, instruction and completion pairs were developed afterward by the interviewer.</p>
<p>For the first version of ClimateGPT, we conducted a series of interviews with the agricultural ecologist Dr. David Lobell. He is the Director of the Center on Food Security and the Environment at Stanford University and also served as lead author for the food chapter on the IPCC Fifth Assessment Report (AR5). The result of this process was a high-quality IFT dataset of 74 demonstrations. Based on these promising results, we plan to refine our methodology and conduct additional interviews.</p>
<h3>3.2 Grounded Expert Demonstrations</h3>
<p>In addition to the non-expert annotators, we collaborated with nine climate scientists (graduate or PhD level) from different European universities. For the data collection, AppTek's data annotation tool Workbench ${ }^{3}$ was used. The team worked in close collaboration with the authors to improve the style of the generated data.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task Category</th>
<th style="text-align: right;">$\%$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Open Ended QA</td>
<td style="text-align: right;">26.9</td>
</tr>
<tr>
<td style="text-align: left;">Open Ended Generate</td>
<td style="text-align: right;">48.0</td>
</tr>
<tr>
<td style="text-align: left;">Open Ended Classification</td>
<td style="text-align: right;">1.2</td>
</tr>
<tr>
<td style="text-align: left;">Open Ended Chat</td>
<td style="text-align: right;">4.7</td>
</tr>
<tr>
<td style="text-align: left;">Open Ended Chain of Thought</td>
<td style="text-align: right;">0.1</td>
</tr>
<tr>
<td style="text-align: left;">Open Ended Brainstorm</td>
<td style="text-align: right;">7.5</td>
</tr>
<tr>
<td style="text-align: left;">Closed Ended Summarize</td>
<td style="text-align: right;">4.7</td>
</tr>
<tr>
<td style="text-align: left;">Closed Ended Rewrite</td>
<td style="text-align: right;">0.2</td>
</tr>
<tr>
<td style="text-align: left;">Closed Ended QA</td>
<td style="text-align: right;">3.8</td>
</tr>
<tr>
<td style="text-align: left;">Closed Ended Extract</td>
<td style="text-align: right;">1.6</td>
</tr>
<tr>
<td style="text-align: left;">Closed Ended Classification</td>
<td style="text-align: right;">1.3</td>
</tr>
</tbody>
</table>
<p>Table 4: Task distribution for the non-expert data collection.</p>
<p>As a first step, we asked the nine climate scientists to think themselves about five to ten questions relevant to climate change that they find important to address and feel comfortable answering. We proposed to them to organize their answers with a scientific mindset (the style we want ClimateGPT to use) by first making a summarizing statement followed by a list of bullet points explaining or developing elements of the summary. Each answer should refer to a scientific source, from which the experts should extract a couple of paragraphs relevant to the answer. The retrieved paragraphs were stored so that they could be used later on. This first exercise was a first test used to evaluate the writing skills of our experts. At the end of this first phase, we continued with seven of the nine experts with the IFT creation task.</p>
<p>In the second step, we provided the experts with references to primary sources and questions related to these sources that have been generated by our synthetic IFT pipeline (Section 3.4), as a source of inspiration. The synthetically generated questions can be very specific reading comprehension questions with respect to the reference source that often has no relevance outside of the source document used. Such questions would either need to be generalized or skipped by the expert. Also, to be time-efficient while producing an answer, we proposed to our experts to choose those questions that relate to their domain of expertise (e.g. city climate, tropical climate, etc.). At the end, we gave each expert a set of 1,000 question-answer pairs, from which 50 to 250 have been selected. In contrast to the non-expert data collection effort (Section 3.3), we did not suggest specific task categories to the experts and instead let them decide on relevant instructions.</p>
<p>As addressed previously, we want the model to be able to generate different in-depth responses addressing the different dimensions of climate change, namely natural science, economics, and social aspects. To collect IFT data for this feature, we asked the expert annotators to create four responses to the same prompt one giving a general answer and three focusing on one of these dimensions.</p>
<h1>3.3 Grounded Non-Expert Demonstrations</h1>
<p>For the non-expert data collection, we worked with a team of 99 annotators employed by external contractors from six different countries and three continents. Annotators were selected based on their educational background, domain-specific expertise and interests, strong communication skills, and writing skills. More details on the demographics of the annotators are provided in Appendix A.4. All annotators were trained by the corresponding project managers on the project scope, guidelines and requirements. The team used the same tool as the expert annotators (Section 3.2).
To ensure a certain level of diversity of types of instruction, we provided annotators with a task category. The set of tasks and their distribution is based on the use case categories reported in (Ouyang et al., 2022b). The resulting task distribution is shown in Table 4. For each category textual guidelines were provided to the annotators.</p>
<p>For the initial phase of the project, we did not provide annotators with any specific topic to work on in addition to the general climate topic. However, we observed that this resulted in too many simplistic and overlapping prompt and completion pairs. Providing annotators with a specific topic to work on resulted in more diverse and interesting data. Topics were selected based on interviews with climate experts to cover the climate impact across various real-life situations and elements. Table 19 in Appendix A. 4 shows the full list and distribution of topics.</p>
<p>The recommended way of data creation was to find content from approved data sources to develop ideas for prompt and completion pairs. Initially, we provided annotators with primary sources, such as research papers and technical sections of the IPCC reports. However, initial feedback showed that our annotators struggled with these documents. Therefore, we decided to switch to secondary sources, such as governmental websites (e.g. from the EPA, NASA, or the European Parliament) and summary sections of the IPCC reports. Besides the trustworthiness of the content, data sources were approved to avoid copyright issues.
Annotators were instructed to give in-text citations to sources they were using in the completion. We instructed annotators to give citations in MLA style (i.e. author name, title, and source in brackets) but noticed that this resulted in inconsistencies that had to be corrected in post-processing. Later we switched to IEEE style (i.e. reference number in square brackets). The data annotation tool allows storing additional details for each citation, such as the URL or the cited text, as additional metadata for the prompt-completion pair. At the beginning of the data annotation process, we decided to instruct annotators to only store the URL of the cited source and not the cited text itself. While the latter would have been useful to improve the retrieval augmented generation capabilities of the model, we decided against it in concern that annotators would restrict themselves to the referenced text (instead of making use of all information in the document) and to avoid increasing the complexity of the annotations process, and, thus, the volume of data we can collect. As an alternative, we can make use of the URLs to crawl the complete document and reconstruct the cited paragraph automatically. Section 4.3 discusses this process in more detail.</p>
<h1>3.4 Synthetically Generated Demonstrations</h1>
<p>As access to experts who can make use of primary sources is limited (and we initially were concerned that we may not be able to collect enough human-generated IFT data), we were also investigating synthetically generating demonstrations from primary sources. To achieve that we prompted an existing general-purpose LLM with few-shot examples and a document and instructed the model to first generate a question and the corresponding system completion. The prompts were carefully designed to increase the diversity of the generated data. Further, we applied multiple post-processing steps to ensure that the generated data is of high quality. These steps included verifying that there is not too much and not too little overlap to the reference documents and prompting general-purpose LLM again to check whether the generated completion is plausible. Further, we filter out questions or responses that mention figures or specific sections from papers and try to detect other text generation artifacts like repeating sequences. This process was initially designed with a multi-turn model in mind. Therefore, completions were intentionally kept shorter with the intent that the user might ask follow-up questions. The decision not to allow multi-turn interactions in this initial version and that more comprehensive answers are preferable came later in the project.
While initial experiments showed promising results, we did not observe consistent improvements in our automatic benchmarks for later versions of the models when using this data. Thus, due to this and due to the lenght mismatch this data is not directly included in our final IFT data mixture.</p>
<h3>3.5 General Domain Data</h3>
<p>As the last track of our IFT training dataset, we make use of existing human-written IFT datasets that are available to us. The first is an internal high-quality set of prompt-completion pairs originally collected by AppTek. We are referring to this dataset as AppTek General.</p>
<p>The data collection methodology was similar to the one described for the non-expert data collection.</p>
<p>Further, we make use of two openly available crowd-sourced IFT datasets. First, Databricks Dolly (Conover et al., 2023) was the first openly available human-generated IFT dataset with a permissive license. The dataset consists of 15,001 prompt and completion pairs across 7 task categories and was generated over two months by over 5,000 employees at Databricks. Second, OpenAssistant Conversations 1 (OASST-1) (Köpf et al., 2023) is a dataset consisting of 161,443 messages in 35 different languages. The corpus is the result of a worldwide multilingual crowd-sourcing effort involving over 13,500 annotators. In contrast to all previously mentioned IFT datasets, this dataset does not only contain instruction and completion pairs but also multi-turn conversations. For ClimateGPT, we only make use of English conversation and only include the best-rated messages in each conversation tree, resulting in a total of 3,783 conversations.</p>
<p>As an additional source of data, we included 3,282 question-and-answer pairs from domainrelevant StackExchange communities (earth science, sustainability and economics). Another common approach to curating IFT datasets is to format existing NLP datasets as instruction and completion pairs using task-specific templates (Wang et al., 2022; Longpre et al., 2023). While training on this type of data alone is not sufficient to achieve good performance (Ouyang et al., 2022b), combining this type of data can be beneficial (Wang et al., 2023a). A possible explanation for this is that this way the model is exposed to a larger variety of tasks and more examples of in-context learning. At the same time, this type of data is closer to our evaluation tasks than human-written pairs, which might explain improvements in automatic evaluation that might not translate to improvements under realistic use. We decided to include 15,000 examples per epoch from FLAN v2 and CoT as described by Wang et al. (2023a) into our training data.</p>
<p>Most recently published instruction fine-tuning datasets were created by distillation from large proprietary LLMs like GPT-4. Examples of these include Alpaca (Taori et al., 2023), Vicuna (Chiang et al., 2023) or WizardLM (Xu et al., 2023). We intentionally decided not to make use of these datasets. First, recent research has shown that training on these approaches can successfully transfer the style of the models but not their factuality (Gudibande et al., 2023). Second, the biases of the teacher model may be transferred to the student. And finally, the licensing terms of commercial LLM providers often limit the use of their API to train models that potentially compete with them. Due to this, the usage of this type of data in models intended for commercial use is problematic (Taori et al., 2023).</p>
<h1>3.6 Safety Data</h1>
<p>One missing component in our IFT dataset is examples to align the model to be safe and harmless. While both Dolly and OASST-1 contain a few examples of refusing to answer intentionally harmful prompts, we observed that this was not enough to make the model safe. To evaluate this, we analyzed completions of initial versions of the model on a subset of the Do-Not-Answer dataset (Wang et al., 2023b). This dataset consists of around 1,000 prompts that are intentionally designed to invoke harmful or offensive model outputs. As expected, the initial model produced multiple unsafe and potentially harmful outputs, which suggests that additional demonstrations of expected model behavior are required. As writing safe completions to these types of prompts can be especially stressful for annotators and new approaches to safety are not the center of this work, we decided to make use of an already safe model to generate the completions synthetically. Specifically, we generated completions for each prompt in the dataset using Llama-2-Chat-70B (Touvron et al., 2023a) and included this data in our IFT set. We are referring to this dataset as Llama-2 Safety. The design considerations around safety are discussed in more detail in Section 8.1.</p>
<h3>3.7 Data Preparation</h3>
<p>We use a mix of different sources for our IFT data to enable alignment with the different aspects outlined in the previous sections. Table 3 shows the mixing ratios of the different subsets in our final model training. We just train for a single epoch on the general domain data and up-sample the climate-specific data.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Training (solid) and validation (dashed) loss of the final IFT models.</p>
<p>During inference, we want the model to generate text that is as close as possible to our expert-generated data. However, since the majority of the IFT data comes from other sources, we need some mechanism to counteract this. Our solution to this is to use different system prompts for each data source to condition the model. By using the system prompt corresponding to the expert IFT data we can control its style at inference time. Further, this also allows us to train on data that, e.g. does not make use of all the features of the model (e.g. does not provide citations to references as discussed in Section 4.3). The system prompts for each of the subsets are listed in Appendix A.6.</p>
<p>To prepare the IFT data for training, we make use of the codebase from Open Assistant ${ }^{4}$. During training the IFT data is formatted using the ChatML prompt template ${ }^{5}$, following other recent open source models like Open Assistant ${ }^{6}$ or Meditron (Chen et al., 2023). Standardizing prompt templates in open-source models results in greater compatibility with existing tools and libraries.</p>
<h1>3.8 Training</h1>
<p>As for pre-training, we use a fork of NVIDIA's Megatron-LM (Narayanan et al., 2021) by the EPFL LLM Team (Cano et al., 2023; Chen et al., 2023) for IFT training. We use a cosine learning rate schedule with a peak learning rate of $10^{-5}$ and a warm-up of 100 steps. The batch size is set to 64 and we use the full sequence length of 4096 tokens. For regularization, we use weight decay of $10^{-2}$ and dropout as used for LIMA (Zhou et al., 2023). The training and validation loss curves for our models are shown in Figure 5. As the validation set, we used a held-out set of 400 prompt and completion pairs from our non-expert climate data.</p>
<p>As was observed by previous work (Ouyang et al., 2022b; Zhou et al., 2023), the perplexity on the validation set first decreases for the first steps and then increases. Typically, this is a clear sign of over-fitting, but, as in other works, we observe that the quality of the model still improves. This is measured by evaluating the model on our automatic evaluation benchmark (Section 6).</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>4 Retrieval Augmented Generation</h1>
<p>While pre-training and instruction fine-tuning on climate-specific data improve the climate knowledge of the model, large language models still tend to hallucinate information, especially facts that are not well represented in the training data. An example, where this is especially problematic, is specific numeric figures, for example, $\mathrm{CO}_{2}$ emissions of a country in a specific year. In addition, the model's knowledge is frozen and it is not possible to incorporate new facts or knowledge without additional training or other approaches to modify the model's weights (Mitchell et al., 2022). In the domain of climate change, these issues are critical. The field is constantly evolving and having access to the latest findings is crucial to draw relevant conclusions.</p>
<p>Retrieval augmented generation (RAG) addresses both of these issues by retrieving relevant documents from external databases and providing these documents as additional context to the model. While the approach was originally proposed for question-answering (Chen et al., 2017; Guu et al., 2020), it has been successfully applied to other tasks like machine translation (Khandelwal et al., 2021), task-oriented dialog (Thulke et al., 2021) and recently in the context of instruction-tuned LLMs (Shi et al., 2023b). The general approach for RAG is to have a separate retriever module which given the user query returns a list of relevant documents. Then, in the language model - often referred to as reader in this context - both user query and retrieved documents are given as context to the model to produce a response. Due to the limited sequence length of transformers and efficiency concerns, it is often not feasible to provide full documents (like full research papers) to the model. Instead, shorter excerpts of a few sentences or paragraphs are typically used as the unit for retrieval and input to the model. Systematic studies show that RAG can reduce the number of hallucinations in language models (Shuster et al., 2021).
Nevertheless, the approach still suffers from limitations. Models usually suffer from noise or irrelevant documents in the retrieval context (Shi et al., 2023a; Cho et al., 2023). We address this issue by including distractor documents during IFT training to allow the model to learn to ignore irrelevant documents.
While RAG is an obvious approach to increase the factuality of generated responses, it also suffers from an inherent trade-off between factuality and abstractiveness (Daheim et al., 2022, 2023). With current RAG approaches, generated responses are often limited to the information provided in the retrieved documents and do not provide a broader perspective. For climate communication, it's especially important to provide an interdisciplinary perspective integrating different viewpoints in the response. To address this in our approach, we propose to make use of distinct sets of documents to generate different answers each covering one of the main perspectives.
We would like to note that RAG is widely used in literature to improve LLMs in the domain of climate change and communication First, ChatClimate (Vaghefi et al., 2023) makes use of GPT-4 as LLM and follows the standard approach outlined above. As a document source, the IPCC AR6 reports are used. The reports are converted to raw text and split into smaller chunks. For retrieval, OpenAI's text-embedding-ada-002 ${ }^{7}$ embedding model is used. In contrast to our approach the retrieval database is just limited to IPCC reports and the base LLM was not adapted to the domain. The system explicitly refuses to generate completions for prompts for which no relevant passages can be found in the IPCC reports. Secondly, for Arabic Mini-ClimateGPT (Mullappilly et al., 2023) the authors fine-tune Vicuna-7B (Chiang et al., 2023) which is based on the first version of Llama-7B (Touvron et al., 2023a). Similar to this work, they dynamically retrieve both English and Arabic climate-specific documents but do not specify the source of the documents in more detail. In contrast to us, they do not perform continued pre-training to adapt the model to their domain. Furthermore, their IFT data was synthetically, generated using ChatGPT, while our climate-specific IFT data was manually curated by humans in close cooperation with multiple climate scientists and experts.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Source</th>
<th style="text-align: right;"># Docs</th>
<th style="text-align: right;"># 512 Chunks</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">IPCC Reports</td>
<td style="text-align: right;">16</td>
<td style="text-align: right;">17897</td>
</tr>
<tr>
<td style="text-align: left;">Potsdam Papers</td>
<td style="text-align: right;">390</td>
<td style="text-align: right;">8539</td>
</tr>
<tr>
<td style="text-align: left;">Earth4All</td>
<td style="text-align: right;">14</td>
<td style="text-align: right;">235</td>
</tr>
<tr>
<td style="text-align: left;">73 other (open access)</td>
<td style="text-align: right;">336</td>
<td style="text-align: right;">8648</td>
</tr>
</tbody>
</table>
<p>Table 5: Statistics of the different data sources of the primary retrieval dataset.</p>
<h1>4.1 Retrieval Dataset</h1>
<p>The dataset for retrieval consists of a manually curated collection of scientific reports and papers. We used the IPCC reports as a starting point and then manually extended the dataset with additional trusted sources in collaboration with climate experts. Therefore, we focused on recent documents to avoid including outdated research. During the data collection, we carefully evaluated the license of each document and only included content with open access or Creative Commons licenses allowing commercial use. To reduce the complexity of the text extraction pipeline, we only considered digitally native PDF documents (i.e. documents where the content can be directly extracted without requiring OCR or similar approaches).
After collecting the PDF documents, we first split the documents into separate PDF pages and for each page, the text is extracted using $\mathrm{PyMuPDF}^{8}$. While with this approach we might split relevant paragraphs in the middle of a sentence and loose cross-page context, it greatly simplifies our data processing pipeline. Then, the text on each page is split into chunks of 115 tokens. Next, we observed that many pages in these documents do not contain any relevant information for RAG and potentially degrade performance. These include, for example, tables of content or pages with references. These pages have a high density of superficially relevant content and thus are likely to be retrieved. On the other hand, in most cases, these pages do not provide the full information required to generate a response. To remove these pages, we use a combination of manual data cleaning as well as heuristics to detect problematic content. We deployed a custom tool to iterate, filter, and manually edit data and end with a final set of curated and clean data. The resulting pages are converted to sub-word tokens and then split into chunks of length 115 with stride 10 . As the last step, we filter chunks that do not contain enough information (e.g., chunks only containing numbers from tables).</p>
<h3>4.2 Retrieval Approach</h3>
<p>For retrieval, we follow the common approach of using a transformer bi-encoder model (Mazaré et al., 2018; Reimers and Gurevych, 2019). Here, both document and query are passed separately through a transformer encoder to produce embedding vectors for both sequences of tokens. The similarity between the query and the document is then measured by calculating the dot product or cosine similarity between corresponding vectors. The main advantage of this approach is that embeddings for all documents can be pre-computed and only the query has to be passed through the model at inference time. Other retrieval methods, like cross-encoders (Reimers and Gurevych, 2019), pass query and document through the model simultaneously. While this results in better retrieval performance, the inference cost becomes prohibitively expensive if the document database exceeds more than a few hundred documents; as in the case of our approach, where we want to access a broad range of content. Therefore, the cross-encoder approach is commonly used only to re-rank results from other more efficient methods.
As training our own retrieval model was out-of-scope for this project, we evaluated several existing embedding models. We only considered bi-encoder models and did not integrate an additional model for reranking. As an initial set of models, we considered the best-performing models on the MTEB benchmark (Muennighoff et al., 2023). To be able to do our own</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Question</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Answer</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Model</td>
<td style="text-align: center;">Params</td>
<td style="text-align: center;">R@1</td>
<td style="text-align: center;">R@5</td>
<td style="text-align: center;">R@1</td>
<td style="text-align: center;">R@5</td>
</tr>
<tr>
<td style="text-align: left;">bge-base-en-v1.5</td>
<td style="text-align: center;">0.1 B</td>
<td style="text-align: center;">54.8</td>
<td style="text-align: center;">71.5</td>
<td style="text-align: center;">81.8</td>
<td style="text-align: center;">92.1</td>
</tr>
<tr>
<td style="text-align: left;">bge-large-en-v1.5</td>
<td style="text-align: center;">0.3 B</td>
<td style="text-align: center;">55.8</td>
<td style="text-align: center;">73.6</td>
<td style="text-align: center;">83.3</td>
<td style="text-align: center;">93.1</td>
</tr>
<tr>
<td style="text-align: left;">gtr-t5-large</td>
<td style="text-align: center;">0.3 B</td>
<td style="text-align: center;">48.8</td>
<td style="text-align: center;">67.4</td>
<td style="text-align: center;">79.6</td>
<td style="text-align: center;">90.1</td>
</tr>
<tr>
<td style="text-align: left;">gtr-t5-xxl</td>
<td style="text-align: center;">4.8 B</td>
<td style="text-align: center;">47.6</td>
<td style="text-align: center;">66.3</td>
<td style="text-align: center;">79.2</td>
<td style="text-align: center;">89.7</td>
</tr>
<tr>
<td style="text-align: left;">gte-large</td>
<td style="text-align: center;">0.3 B</td>
<td style="text-align: center;">50.7</td>
<td style="text-align: center;">68.2</td>
<td style="text-align: center;">80.9</td>
<td style="text-align: center;">91.4</td>
</tr>
<tr>
<td style="text-align: left;">ember-v1</td>
<td style="text-align: center;">0.3 B</td>
<td style="text-align: center;">49.5</td>
<td style="text-align: center;">68.6</td>
<td style="text-align: center;">79.7</td>
<td style="text-align: center;">91.1</td>
</tr>
<tr>
<td style="text-align: left;">instructor-large</td>
<td style="text-align: center;">0.3 B</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">68.2</td>
<td style="text-align: center;">81.7</td>
<td style="text-align: center;">91.8</td>
</tr>
<tr>
<td style="text-align: left;">instructor-xl</td>
<td style="text-align: center;">1.2 B</td>
<td style="text-align: center;">53.3</td>
<td style="text-align: center;">69.7</td>
<td style="text-align: center;">83.3</td>
<td style="text-align: center;">92.1</td>
</tr>
</tbody>
</table>
<p>Table 6: Recall (R@1 and R@5) of retrieving the correct document given the question or the answer from the synthetically generated IFT dataset.
in-domain and use-case-specific evaluation, we selected a subset of the synthetically generated IFT data (Section 3.4) as a test set. The generated question and source paragraph are considered positive pairs and all other paragraphs as negatives. We run the benchmark both with the question and with the answer as a query. The results of this evaluation and the models under consideration are shown in Table 6 . We got the best results using bge-large-en-v1.5 (Xiao et al., 2023) and decided to use this embedding model for retrieval.
To increase the relevance of retrieved chunks, we use a hierarchical retrieval approach. Therefore, we first retrieve the top- $k$ PDF pages. We use the full text on the page to calculate the embedding. If the length of the text on a page exceeds the maximum sequence length of the embedding model ( 512 in our case with variable stride length), we use a sliding window over the sequence and average the embeddings to get a single embedding for the whole page. After selecting the top- $k(k=60)$ pages, as a last step, we retrieve the top- $k$ $(k=5)$ most relevant chunks corresponding to these pages (note that the number of chunks per page varies). This ensures that the page context of retrieved chunks is also relevant to the query.
HuggingFace Transformers (Wolf et al., 2020) and Sentence Transformers ${ }^{9}$ (Reimers and Gurevych, 2019) was used to deploy the embedding models and to embed text in preprocessing and inference stages. For efficient nearest-neighbor search we use $\mathrm{ScaNN}^{10}$ (Guo et al., 2020).</p>
<h1>4.3 Grounding</h1>
<p>To improve the model's capabilities to make use of reference paragraphs provided in the context, we train the model with IFT examples that already include reference paragraphs. For the IFT data collected from climate experts (Section 3.2) and from interviews (Section 3.1) we asked the annotators to provide the reference paragraphs as additional metadata. For the non-expert data (Section 3.3), we used the URLs of the cited sources to crawl corresponding documents and extract cited paragraphs. Most of the cited sources were websites; therefore, we constructed a separate pipeline to crawl these websites, extract the text using Mozilla's readability ${ }^{11}$ and inscriptis ${ }^{12}$, and split the content into smaller chunks.
As also shown in Table 6, it is easier to retrieve the correct paragraph using the completion as a query than using the prompt. The reason for this is that the completion contains all the relevant information from the document and thus has high semantic similarity. In contrast, the question just asks for the corresponding information which may not necessarily imply semantic similarity. In our case, we can make use of this fact by not only considering the relevance of the prompt to the potential reference chunks, but also the relevance of the answer. Furthermore, as discussed at the beginning of this section, numbers are a common source of hallucinations in LLMs. Thus, we want to make sure, that the numbers that</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>annotators extracted from the provided reference are part of the selected reference chunk. To achieve this we add the overlap in numbers between the completion and potential reference paragraph as an additional scoring factor. Based on these three scores, we select the best matching chunk from the reference document as source chunk which is given as context during IFT training.</p>
<p>To increase the robustness of the model to irrelevant retrieval results, we added distractor chunks as additional context. These chunks are selected from the total set of chunks produced by the pipeline above. To select chunks not relevant to the answer, we use the opposite of the scoring function outlined above, i.e. we select chunks with high similarity to the prompt but with low similarity to the answer and with low numerical overlap. Additionally, some tasks for the model do not require information from any reference paragraph. Examples include all closed-ended task categories described in Table 4. For these tasks, the prompt already includes all relevant context, so everything that is retrieved is just unnecessary noise. Instead of detecting this at inference time and disabling the retrieval augmentation, we also include distractor paragraphs during the training of IFT examples of these categories to make the model more robust.</p>
<h1>4.3.1 Citations</h1>
<p>In a scientific context, it is important that provided information is attributable. In RAG, the retrieved chunks are attributable since their source is known. However, the generated completion is not necessarily explicitly related to any of the chunks. It would be helpful to know which chunks exactly were used to generate which part of the response. As discussed in Section 3.3, annotators were asked to provide in-text citations for each reference. We converted these citations to special tokens [[0]], [[1]], etc. which are prepended to the corresponding chunk in the context as well as the token for the citation. While the general approach seems to work, we observed that citation tokens are hallucinated in some cases. We attribute this to the automatic extraction of reference chunks that might not cover all relevant details of the source documents and, thus, introduce noise. For this reason, we removed citations from the IFT data for the final ClimateGPT models and plan to reconsider this in the future.</p>
<h3>4.4 Three Dimensions</h3>
<p>Climate change is inherently an interdisciplinary field. Therefore, to effectively serve our intended audience, including policymakers, scientists, and journalists, we aim to enhance our model to adeptly address queries from three critical perspectives: natural, economic, and social science aspects. Our goal is to have the model's outputs tailored to the multifaceted nature of climate change, thereby providing comprehensive insights essential for informed discussion and decision-making in the field.</p>
<p>To this end, we devise a three-step approach. First, we utilize the ChatGPT gpt-3.5-turbo API to tag our retrieval database with labels corresponding to the natural science, economic, and social aspects. Our preliminary experiments demonstrate that the quality of tags generated through few-shot prompts with gpt-3.5-turbo are satisfactory, and we do not further use gpt-4. The detailed prompt can be found in Appendix A.8. Second, during inference, we retrieve the most relevant documents for each dimension using the tags above. These sets of documents are then fed separately to the model to generate three distinct completions. The final step involves modifying the system prompt to instruct the model to focus on the specific dimension. These special system prompts were already used during IFT training, for the examples where we had demonstrations focusing on these dimensions.</p>
<h2>5 Multilinguality</h2>
<p>To achieve our goal of making climate science accessible to a broader range of users, it is important that the model is not only accessible in English but is multilingual. To support multilinguality in LLMs, there are two options. The first is to include multilingual data in the pre-training and IFT data. The other is to rely on Machine Translation (MT) to translate the user input into English, and to translate the generated text back into the</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: left;">Supported Languages</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">native in LLM</td>
<td style="text-align: left;">English</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">cascaded MT</td>
<td style="text-align: left;">Arabic</td>
<td style="text-align: left;">Bengali</td>
<td style="text-align: left;">Chinese (simplified)</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Dutch</td>
<td style="text-align: left;">Finnish</td>
<td style="text-align: left;">French</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">German</td>
<td style="text-align: left;">Greek</td>
<td style="text-align: left;">Hebrew</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Indonesian</td>
<td style="text-align: left;">Japanese</td>
<td style="text-align: left;">Korean</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Lithuanian</td>
<td style="text-align: left;">Pashto</td>
<td style="text-align: left;">Persian</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Portuguese</td>
<td style="text-align: left;">Russian</td>
<td style="text-align: left;">Spanish</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Thai</td>
<td style="text-align: left;">Turkish</td>
<td style="text-align: left;">Vietnamese</td>
</tr>
</tbody>
</table>
<p>Table 7: Supported languages. English is native to the underlying LLM, while the support for other languages is achieved via a cascaded translation approach, i.e. $\mathrm{xx} \rightarrow \mathrm{en}$ at the input and en $\rightarrow \mathrm{xx}$ at the output.
user's language, i.e. a cascaded approach. We chose the latter because there is a lack of multilingual data in the climate domain and we wanted to maintain consistent quality in multiple languages. In Table 7, we list the supported languages.
To enable higher translation quality for climate text, we performed several domain adaptation experiments. Building on a generic base NMT model, we continued fine-tuning the model on parallel sentence pairs extracted from climate data only. As the initial version of the model was presented in December 2023 at the 28th United Nations Climate Change conference in Dubai, we focussed our experiments on Arabic.</p>
<h1>5.1 In-Domain Data</h1>
<p>To extract parallel climate-related data from our large background collection of public and proprietary datasets, we explore two methods.
For the first method, we used around 2 K climate terms and their human translations. We filtered the parallel data based on exact matches of these terms. We then fine-tuned our base model on these parallel sentences. We use Exact Match (EM) to denote this model.
The second method is based on sentence embedding similarity. For this method, we take climate-related monolingual text, mainly a subset from our LLM pre-training data, as seed data. First, we use a weighted average over the word embeddings of a sentence to generate a fixed-size sentence embedding. To obtain a sentence pair embedding, we concatenate the source and target sentence embedding of each bilingual sentence pair. Afterwards, we employ k-Means clustering in the sentence pair embedding space. After obtaining a set of clusters, we use the in-domain seed data to determine which clusters should be used for training. This is done by selecting all clusters that contain a non-negligible portion of the in-domain data using a fixed threshold. For details, refer to AppTek's submission to the shared task of the IWSLT ${ }^{13}$ evaluation (Bahar et al., 2020). The resulting parallel corpus is then used for fine-tuning the baseline translation model. We call this approach Embeddings Clustering (EC).
In Table 8, data statistics related to the machine translation adaptation are shown. Initially, we took a subset of our training data, excluding those corpora that most likely will not benefit the conversation about climate. For example, we excluded subtitling data, transcriptions, and some others. The remaining data is the Filtered Base data in Table 8. We filtered this data again to extract EM and EC data.</p>
<h3>5.2 Training</h3>
<p>We used Transformer big architecture and parameters for the MT model (Vaswani et al., 2017a). The fine-tuning process is done for a fixed number of steps for both data setups. The training stopped after 15 M parallel sentence pairs with a learning rate of $8.0 \cdot 10^{-5}$. This</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{13}$ International Workshop (Conference) on Spoken Language Translation.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
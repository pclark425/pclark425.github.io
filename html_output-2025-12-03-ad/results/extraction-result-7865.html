<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7865 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7865</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7865</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-146.html">extraction-schema-146</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <p><strong>Paper ID:</strong> paper-278129451</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.18413v1.pdf" target="_blank">An Empirical Study of Evaluating Long-form Question Answering</a></p>
                <p><strong>Paper Abstract:</strong> Long-form question answering (LFQA) aims to generate lengthy answers to complex questions. This scenario presents great flexibility as well as significant challenges for evaluation. Most evaluations rely on deterministic metrics that depend on string or n-gram matching, while the reliability of large language model-based evaluations for long-form answers remains relatively unexplored. We address this gap by conducting an in-depth study of long-form answer evaluation with the following research questions: (i) To what extent do existing automatic evaluation metrics serve as a substitute for human evaluations? (ii) What are the limitations of existing evaluation metrics compared to human evaluations? (iii) How can the effectiveness and robustness of existing evaluation methods be improved? We collect 5,236 factoid and non-factoid long-form answers generated by different large language models and conduct a human evaluation on 2,079 of them, focusing on correctness and informativeness. Subsequently, we investigated the performance of automatic evaluation metrics by evaluating these answers, analyzing the consistency between these metrics and human evaluations. We find that the style, length of the answers, and the category of questions can bias the automatic evaluation metrics. However, fine-grained evaluation helps mitigate this issue on some metrics. Our findings have important implications for the use of large language models for evaluating long-form question answering. All code and datasets are available at https://github</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7865.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7865.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-vs-Human_correlation_overall</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Overall correlation between LLM-based evaluators and human judgments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper reports that LLM-based evaluation metrics (e.g., GPT-4o, Claude-3.5, Gemini-2.0) show substantially higher consistency with human judgements than deterministic n-gram or embedding-based metrics, with reported Spearman correlations for GPT-4o=42.0, Claude-3.5=33.0, Gemini-2.0=32.4 (Spearman, as reported in-text).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>An Empirical Study of Evaluating Long-form Question Answering</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Long-form Question Answering (LFQA) evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>ASQA, ANTIQUE, WikiEval (aggregated analysis across these datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4o, Claude-3.5, Gemini-2.0 (LLM-based evaluators)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Closed-source LLM evaluators referenced by name (GPT-4o, Claude-3.5, Gemini-2.0); specific sizes/training corpora not provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Human annotators (manual ratings collected via a web interface; subset: 2,079 annotated answer pairs, 4,158 ratings; rating scale 1–5 on correctness and informativeness).</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Spearman rho (value correlation); Kendall tau used for rank correlation in other analyses</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>42.0</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>more stringent scoring than humans; sensitivity to prompt/hyperparameters; self-preference bias; lexical/length biases</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>LLM-based evaluators are more stable across LFQA styles and outperform deterministic metrics in alignment with humans; however they still differ systematically from humans (e.g., stringently lower absolute scores), and are affected by prompting and model identity.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Higher consistency with human judgments than deterministic/embedding-based metrics; applicability across ambiguous, factoid, and non-factoid LFQA.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Evaluations run on outputs from seven generation LLMs across ASQA/ANTIQUE/WikiEval; human ratings on correctness and informativeness (1–5); automatic metrics computed and compared via Spearman/Kendall correlations and pairwise win-rate/agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Empirical Study of Evaluating Long-form Question Answering', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7865.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7865.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FG_prompt_improvement</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fine-grained prompting improves LLM-human agreement (example: GPT-4o)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Providing fine-grained evaluation instructions (detailed prompt criteria) substantially improved alignment between an LLM judge and humans; the paper reports GPT-4o Spearman improving from 42.0 (coarse) to 55.0 (fine-grained).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>An Empirical Study of Evaluating Long-form Question Answering</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>LFQA evaluator alignment (coarse-grained vs fine-grained prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>ASQA and ANTIQUE (examples reported in-text; overall experiments across datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4o (fine-grained evaluation prompt vs coarse-grained)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>GPT-4o used as prompt-driven judge; specific internal details not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Human annotators (same annotation setup as main study: ratings on correctness and informativeness).</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Spearman rho (comparison reported in-text)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>55.0</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>without fine-grained criteria LLM evaluators align less well with humans; sensitivity to prompt wording and instruction components</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Detailed evaluation criteria and structured prompts (task + data + output + criteria) increase LLM-human agreement, but no single prompt configuration uniformly improves alignment for all generator models.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Improved nuance and alignment to human ratings when given structured, fine-grained instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Prompt ablation/combination experiments (9 prompt variants combining Task/Data/Output/Criteria); Kendall correlations computed between GPT-4o outputs under differing prompts and human ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Empirical Study of Evaluating Long-form Question Answering', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7865.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7865.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM_score_distribution_diff</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Systematic difference in absolute score distributions between LLM judges and humans</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLM judges tended to give systematically lower absolute ratings than humans in score distributions (LLM scores clustered around ~3 while human scores clustered around ~5 on the 1–5 scale), suggesting LLMs are more stringent.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>An Empirical Study of Evaluating Long-form Question Answering</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>LFQA score calibration / comparison of absolute scores</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>ASQA (example illustrated in Figure 2)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4o (example distribution comparison; other LLMs discussed similarly)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Prompt-driven LLM judge (GPT-4o) used in coarse- and fine-grained settings; exact model internals not specified.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Human annotators (ratings 1–5 on correctness/informativeness).</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Score distribution comparison (no single scalar agreement metric reported for this specific observation)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>LLM judges are more stringent (lower absolute ratings) which creates calibration mismatch with human judges; direct score comparisons without recalibration are misleading.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Even when rank-order alignment is reasonable, absolute scores from LLM judges are systematically lower than human scores; prompt length and granularity modulate this effect.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Not applicable for absolute-score calibration; but LLMs produce consistent internal scoring distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Histogram/score-distribution comparison shown for GPT-4o prompts on ASQA (short/normal/long prompts compared vs human score distributions).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Empirical Study of Evaluating Long-form Question Answering', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7865.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7865.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>prompt_length_sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sensitivity of LLM-as-judge outputs to prompt length and wording</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The length/detail of the evaluation prompt strongly affected LLM judge scores; shorter prompts produced more high scores, and longer prompts tended to lower scores (distributional effects observed across short/normal/long prompt variants).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>An Empirical Study of Evaluating Long-form Question Answering</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>LFQA evaluation robustness to prompt engineering</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>ASQA (prompt experiments reported; code repository contains prompt details)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4o (used for prompt-length sensitivity experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>GPT-4o used with short/normal/long prompts; internal specs not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Human annotators (for baseline comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Not a single scalar—analysis reports shifts in score distributions and counts above threshold values</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>Prompt wording/length can change evaluation outcomes substantially; lack of prompt robustness undermines reliability as a human substitute.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Short prompts yield more high scores; prompt engineering therefore materially affects LLM-as-judge outputs and must be controlled/standardized.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Flexible prompting allows tuning of evaluation focus (but also introduces variability).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>GPT-4o evaluated ASQA responses under short/normal/long prompt variants; score distributions compared to human ratings (Figure 2).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Empirical Study of Evaluating Long-form Question Answering', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7865.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7865.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>temperature_sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sensitivity of LLM-as-judge to sampling temperature and hyperparameters</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Changing the LLM judge's sampling temperature altered evaluation rankings and scores; e.g., GPT-4o rank changed from 4th to 6th when temperature increased from 0.0 to 0.3, and improved from 4th to 2nd at higher temperatures (0.7/1.0) on WikiEval, showing dataset-dependent sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>An Empirical Study of Evaluating Long-form Question Answering</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>LFQA evaluation robustness to judge hyperparameters</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>ASQA and WikiEval (temperature effects detailed for these datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4o (temperature sweep experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>GPT-4o used as judge with multiple temperature settings (default 0); exact model internals not specified.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Human annotators (baseline references)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Rank changes and score change analysis (win rates/ranks reported in Table 3)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>Rank instability across judge hyperparameter settings; sensitivity varies by dataset (more impact when models' evaluation scores are close).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>When model scores are tightly clustered (e.g., WikiEval), small temperature changes can flip rankings; when score spread is large (e.g., ASQA), rankings are stable.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Configurability (but this is also a liability for reproducibility).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>GPT-4o used as judge with temperature values (0.0, 0.3, 0.7, 1.0); ranks and scores of evaluated models compared pre/post perturbation (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Empirical Study of Evaluating Long-form Question Answering', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7865.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7865.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>length_bias</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Answer-length bias in automatic metrics including LLM judges</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Deterministic metrics (ROUGE-L, BERTScore) display negative correlation with answer length (penalizing longer answers), while some LLM-based evaluators show either stable or positive correlation with length and are less likely to assign low scores to longer answers—indicating a bias toward comprehensiveness in some LLM judges.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>An Empirical Study of Evaluating Long-form Question Answering</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>LFQA evaluation fairness analysis (length effect)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>ASQA, ANTIQUE, WikiEval (length-bin analysis across datasets; Figure 3)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4, GPT-4o, Claude-3.5, Gemini-2.0 (LLM-based metrics analyzed alongside ROUGE-L/BERTScore)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>LLM evaluators referenced by model name; specific versions for some analyses (GPT-4, GPT-4o) noted; no sizes provided.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Human annotators (baseline ratings)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Correlation trends across length bins (no single scalar reported for this bias test)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>Deterministic metrics penalize longer answers; LLM judges may give higher scores to longer responses even when humans do not, leading to length-driven bias.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Different metrics react differently to answer length; combining deterministic and LLM-based metrics or normalizing for length is suggested to mitigate biases.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>More tolerant of long answers and can reward comprehensiveness (depending on prompt).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Answers binned into five equal-sized bins by length; metric scores analyzed per bin for trends (Figure 3).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Empirical Study of Evaluating Long-form Question Answering', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7865.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7865.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>question_type_bias</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Question-type dependent performance and failure modes of LLM evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLM-based metrics outperform deterministic metrics across ANTIQUE question types overall, but they underperform relative to their average on evidence-based and experience-type questions and show slightly higher consistency on informativeness than on correctness—indicating difficulty recognizing factual errors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>An Empirical Study of Evaluating Long-form Question Answering</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>LFQA evaluation across question types (reason, instruction, evidence-based, experience, debate, comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>ANTIQUE (category analysis of 200 questions; Table 4 and Figure 4)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4, Claude-3.5, Gemini-2.0 (primary LLM evaluators analyzed by question type)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>LLM evaluators referenced by name; no size/training details provided.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Human annotators (correctness and informativeness labels used as ground truth)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Kendall tau correlation between metric and human scores per question type (reported in Figure 4)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>Reduced ability to detect factual inaccuracies (correctness) compared to assessing informativeness; lower consistency for evidence-based and experience questions.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>LLM evaluators are better at evaluating informativeness than correctness and have varying reliability across different question genres; deterministic metrics can even show negative correlations for some types.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Outperform deterministic metrics across many question types; more reliable for informativeness judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>200 ANTIQUE questions manually categorized; Kendall correlation computed per metric per question-type for correctness and informativeness (Figure 4).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Empirical Study of Evaluating Long-form Question Answering', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7865.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7865.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>self_reinforcing_bias</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-preference (self-reinforcing) bias of LLM evaluators toward their own generations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>When LLMs are used both as generators and evaluators, they tend to favor outputs produced by themselves; the paper reports higher win rates for GPT-4o, Claude-3.5, and Gemini-2.0 when those same models act as the judge, with example win-rate contrasts (e.g., Gemini-2.0 gives itself win-rate 0.468 vs ~0.262/0.256 when other evaluators judge).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>An Empirical Study of Evaluating Long-form Question Answering</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>LFQA pairwise model comparison (self-evaluation bias study)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>ASQA (self-reinforcing analysis reported on this dataset; Figure 6)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4o, Claude-3.5, Gemini-2.0 (evaluators also used as generators)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Closed-source LLMs used as both generators and judges; internal training/specs not given.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Human annotators used for baseline comparisons but this test focuses on LLM-vs-LLM evaluation bias.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Win rate / pairwise agreement (proportion of instances where one model's answer preferred over another)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>Clear bias toward own outputs (self-reinforcement), inflating self-assessed win rates; threatens impartiality of LLM-as-judge.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Each closed-source LLM tends to rank its own outputs higher than other evaluators do; recommendation: prefer rankings over raw scores and employ multiple independent evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>When using multiple evaluators, comparative rankings remain broadly consistent despite self-bias; however reliance on a single LLM judge is problematic.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Pairwise comparisons between closed-source LLMs (GPT-4o, Claude-3.5, Gemini-2.0) and several open-source baselines; win rates computed per evaluator (Figure 6).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Empirical Study of Evaluating Long-form Question Answering', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7865.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7865.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>idf_lexical_bias</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Lexical (IDF) bias: LLM evaluators favor lexically sophisticated responses under some settings</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper finds that ROUGE-L and BERTScore correlate strongly with answer IDF, while GPT-4o and Gemini-2.0 align more closely with human judgments; under fine-grained settings GPT-4o tends to favor responses with higher average IDF, indicating a bias toward lexical sophistication.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>An Empirical Study of Evaluating Long-form Question Answering</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>LFQA evaluation bias analysis w.r.t. lexical rarity (IDF)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>ASQA, ANTIQUE, WikiEval (IDF relationship analysis across these datasets; Figure 5)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4o, Gemini-2.0 (IDF bias particularly noted for GPT-4o under fine-grained settings)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>LLM evaluators referenced by name; no model size/training details provided.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Human annotators (baseline ratings showed even distribution across IDF values for some datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Correlation analyses between average IDF and metric scores (no single scalar agreement reported)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>Potential bias towards lexically sophisticated (high-IDF) responses, especially under fine-grained prompting; may favor verbose/formal wording.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Human scores do not systematically prefer high-IDF answers, but some LLM evaluators (when finely prompted) do; this may skew evaluations toward 'sophisticated' wording rather than factual quality.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Can capture stylistic/lexical nuances, but risk over-weighting lexical rarity.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Compute average IDF per answer and correlate with human and automatic metric scores; analyze distributions per dataset (Figure 5).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Empirical Study of Evaluating Long-form Question Answering', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7865.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e7865.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>deterministic_metric_failure</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Failure modes of deterministic metrics (ROUGE-L, Exact Match, BERTScore) vs human judgments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Deterministic n-gram/overlap metrics often show weak or dataset-dependent correlation with human judgments; e.g., Exact Match aligned well only for short-answer ASQA cases, while ROUGE-L and Disambig-F1 performed poorly for long-form and open-ended answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>An Empirical Study of Evaluating Long-form Question Answering</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>LFQA metric comparison (deterministic vs model-based vs LLM-based)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>ASQA, ANTIQUE, WikiEval (dataset-dependent reliability reported; Tables 1 & 2)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>N/A (deterministic metrics evaluated against human annotations)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Deterministic metrics: Exact Match (EM), ROUGE-L, Disambig-F1; embedding metric: BERTScore.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Human annotators (used as ground truth for meta-evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Spearman rho / Kendall tau correlations reported in Tables 1 and 2</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>Poor correlation with human judgments for long-form and open-ended QA; sensitivity to reference length and phrasing; sometimes negative correlations by question type.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>ROUGE-L and BERTScore penalize longer or lexically divergent but valid answers; Exact Match only appropriate for short, closed answers; deterministic metrics not reliable substitutes for human judgment in LFQA.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>By contrast, LLM-based evaluators show higher and more stable agreement with humans across diverse LFQA styles.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Correlation analysis (Spearman/Kendall) between deterministic/model-based/LLM metrics and human ratings across three datasets (Tables 1 and 2).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Empirical Study of Evaluating Long-form Question Answering', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Can we trust the evaluation on ChatGPT? <em>(Rating: 2)</em></li>
                <li>G-EVAL: NLG evaluation using gpt-4 with better human alignment <em>(Rating: 2)</em></li>
                <li>LLM-Eval: Unified multi-dimensional automatic evaluation for open-domain conversations with large language models <em>(Rating: 2)</em></li>
                <li>Judging LLM-as-a-judge with MT-bench and Chatbot Arena <em>(Rating: 2)</em></li>
                <li>Can Large Language Models Be an Alternative to Human Evaluations? <em>(Rating: 2)</em></li>
                <li>Exploring the Use of Large Language Models for Reference-Free Text Quality Evaluation: A Preliminary Empirical Study <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7865",
    "paper_id": "paper-278129451",
    "extraction_schema_id": "extraction-schema-146",
    "extracted_data": [
        {
            "name_short": "LLM-vs-Human_correlation_overall",
            "name_full": "Overall correlation between LLM-based evaluators and human judgments",
            "brief_description": "The paper reports that LLM-based evaluation metrics (e.g., GPT-4o, Claude-3.5, Gemini-2.0) show substantially higher consistency with human judgements than deterministic n-gram or embedding-based metrics, with reported Spearman correlations for GPT-4o=42.0, Claude-3.5=33.0, Gemini-2.0=32.4 (Spearman, as reported in-text).",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "An Empirical Study of Evaluating Long-form Question Answering",
            "evaluation_task": "Long-form Question Answering (LFQA) evaluation",
            "dataset_name": "ASQA, ANTIQUE, WikiEval (aggregated analysis across these datasets)",
            "judge_model_name": "GPT-4o, Claude-3.5, Gemini-2.0 (LLM-based evaluators)",
            "judge_model_details": "Closed-source LLM evaluators referenced by name (GPT-4o, Claude-3.5, Gemini-2.0); specific sizes/training corpora not provided in the paper.",
            "human_evaluator_type": "Human annotators (manual ratings collected via a web interface; subset: 2,079 annotated answer pairs, 4,158 ratings; rating scale 1–5 on correctness and informativeness).",
            "agreement_metric": "Spearman rho (value correlation); Kendall tau used for rank correlation in other analyses",
            "agreement_score": 42.0,
            "reported_loss_aspects": "more stringent scoring than humans; sensitivity to prompt/hyperparameters; self-preference bias; lexical/length biases",
            "qualitative_findings": "LLM-based evaluators are more stable across LFQA styles and outperform deterministic metrics in alignment with humans; however they still differ systematically from humans (e.g., stringently lower absolute scores), and are affected by prompting and model identity.",
            "advantages_of_llm_judge": "Higher consistency with human judgments than deterministic/embedding-based metrics; applicability across ambiguous, factoid, and non-factoid LFQA.",
            "experimental_setting": "Evaluations run on outputs from seven generation LLMs across ASQA/ANTIQUE/WikiEval; human ratings on correctness and informativeness (1–5); automatic metrics computed and compared via Spearman/Kendall correlations and pairwise win-rate/agreement.",
            "uuid": "e7865.0",
            "source_info": {
                "paper_title": "An Empirical Study of Evaluating Long-form Question Answering",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "FG_prompt_improvement",
            "name_full": "Fine-grained prompting improves LLM-human agreement (example: GPT-4o)",
            "brief_description": "Providing fine-grained evaluation instructions (detailed prompt criteria) substantially improved alignment between an LLM judge and humans; the paper reports GPT-4o Spearman improving from 42.0 (coarse) to 55.0 (fine-grained).",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "An Empirical Study of Evaluating Long-form Question Answering",
            "evaluation_task": "LFQA evaluator alignment (coarse-grained vs fine-grained prompting)",
            "dataset_name": "ASQA and ANTIQUE (examples reported in-text; overall experiments across datasets)",
            "judge_model_name": "GPT-4o (fine-grained evaluation prompt vs coarse-grained)",
            "judge_model_details": "GPT-4o used as prompt-driven judge; specific internal details not provided.",
            "human_evaluator_type": "Human annotators (same annotation setup as main study: ratings on correctness and informativeness).",
            "agreement_metric": "Spearman rho (comparison reported in-text)",
            "agreement_score": 55.0,
            "reported_loss_aspects": "without fine-grained criteria LLM evaluators align less well with humans; sensitivity to prompt wording and instruction components",
            "qualitative_findings": "Detailed evaluation criteria and structured prompts (task + data + output + criteria) increase LLM-human agreement, but no single prompt configuration uniformly improves alignment for all generator models.",
            "advantages_of_llm_judge": "Improved nuance and alignment to human ratings when given structured, fine-grained instructions.",
            "experimental_setting": "Prompt ablation/combination experiments (9 prompt variants combining Task/Data/Output/Criteria); Kendall correlations computed between GPT-4o outputs under differing prompts and human ratings.",
            "uuid": "e7865.1",
            "source_info": {
                "paper_title": "An Empirical Study of Evaluating Long-form Question Answering",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "LLM_score_distribution_diff",
            "name_full": "Systematic difference in absolute score distributions between LLM judges and humans",
            "brief_description": "LLM judges tended to give systematically lower absolute ratings than humans in score distributions (LLM scores clustered around ~3 while human scores clustered around ~5 on the 1–5 scale), suggesting LLMs are more stringent.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "An Empirical Study of Evaluating Long-form Question Answering",
            "evaluation_task": "LFQA score calibration / comparison of absolute scores",
            "dataset_name": "ASQA (example illustrated in Figure 2)",
            "judge_model_name": "GPT-4o (example distribution comparison; other LLMs discussed similarly)",
            "judge_model_details": "Prompt-driven LLM judge (GPT-4o) used in coarse- and fine-grained settings; exact model internals not specified.",
            "human_evaluator_type": "Human annotators (ratings 1–5 on correctness/informativeness).",
            "agreement_metric": "Score distribution comparison (no single scalar agreement metric reported for this specific observation)",
            "agreement_score": null,
            "reported_loss_aspects": "LLM judges are more stringent (lower absolute ratings) which creates calibration mismatch with human judges; direct score comparisons without recalibration are misleading.",
            "qualitative_findings": "Even when rank-order alignment is reasonable, absolute scores from LLM judges are systematically lower than human scores; prompt length and granularity modulate this effect.",
            "advantages_of_llm_judge": "Not applicable for absolute-score calibration; but LLMs produce consistent internal scoring distributions.",
            "experimental_setting": "Histogram/score-distribution comparison shown for GPT-4o prompts on ASQA (short/normal/long prompts compared vs human score distributions).",
            "uuid": "e7865.2",
            "source_info": {
                "paper_title": "An Empirical Study of Evaluating Long-form Question Answering",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "prompt_length_sensitivity",
            "name_full": "Sensitivity of LLM-as-judge outputs to prompt length and wording",
            "brief_description": "The length/detail of the evaluation prompt strongly affected LLM judge scores; shorter prompts produced more high scores, and longer prompts tended to lower scores (distributional effects observed across short/normal/long prompt variants).",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "An Empirical Study of Evaluating Long-form Question Answering",
            "evaluation_task": "LFQA evaluation robustness to prompt engineering",
            "dataset_name": "ASQA (prompt experiments reported; code repository contains prompt details)",
            "judge_model_name": "GPT-4o (used for prompt-length sensitivity experiments)",
            "judge_model_details": "GPT-4o used with short/normal/long prompts; internal specs not provided.",
            "human_evaluator_type": "Human annotators (for baseline comparisons)",
            "agreement_metric": "Not a single scalar—analysis reports shifts in score distributions and counts above threshold values",
            "agreement_score": null,
            "reported_loss_aspects": "Prompt wording/length can change evaluation outcomes substantially; lack of prompt robustness undermines reliability as a human substitute.",
            "qualitative_findings": "Short prompts yield more high scores; prompt engineering therefore materially affects LLM-as-judge outputs and must be controlled/standardized.",
            "advantages_of_llm_judge": "Flexible prompting allows tuning of evaluation focus (but also introduces variability).",
            "experimental_setting": "GPT-4o evaluated ASQA responses under short/normal/long prompt variants; score distributions compared to human ratings (Figure 2).",
            "uuid": "e7865.3",
            "source_info": {
                "paper_title": "An Empirical Study of Evaluating Long-form Question Answering",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "temperature_sensitivity",
            "name_full": "Sensitivity of LLM-as-judge to sampling temperature and hyperparameters",
            "brief_description": "Changing the LLM judge's sampling temperature altered evaluation rankings and scores; e.g., GPT-4o rank changed from 4th to 6th when temperature increased from 0.0 to 0.3, and improved from 4th to 2nd at higher temperatures (0.7/1.0) on WikiEval, showing dataset-dependent sensitivity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "An Empirical Study of Evaluating Long-form Question Answering",
            "evaluation_task": "LFQA evaluation robustness to judge hyperparameters",
            "dataset_name": "ASQA and WikiEval (temperature effects detailed for these datasets)",
            "judge_model_name": "GPT-4o (temperature sweep experiments)",
            "judge_model_details": "GPT-4o used as judge with multiple temperature settings (default 0); exact model internals not specified.",
            "human_evaluator_type": "Human annotators (baseline references)",
            "agreement_metric": "Rank changes and score change analysis (win rates/ranks reported in Table 3)",
            "agreement_score": null,
            "reported_loss_aspects": "Rank instability across judge hyperparameter settings; sensitivity varies by dataset (more impact when models' evaluation scores are close).",
            "qualitative_findings": "When model scores are tightly clustered (e.g., WikiEval), small temperature changes can flip rankings; when score spread is large (e.g., ASQA), rankings are stable.",
            "advantages_of_llm_judge": "Configurability (but this is also a liability for reproducibility).",
            "experimental_setting": "GPT-4o used as judge with temperature values (0.0, 0.3, 0.7, 1.0); ranks and scores of evaluated models compared pre/post perturbation (Table 3).",
            "uuid": "e7865.4",
            "source_info": {
                "paper_title": "An Empirical Study of Evaluating Long-form Question Answering",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "length_bias",
            "name_full": "Answer-length bias in automatic metrics including LLM judges",
            "brief_description": "Deterministic metrics (ROUGE-L, BERTScore) display negative correlation with answer length (penalizing longer answers), while some LLM-based evaluators show either stable or positive correlation with length and are less likely to assign low scores to longer answers—indicating a bias toward comprehensiveness in some LLM judges.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "An Empirical Study of Evaluating Long-form Question Answering",
            "evaluation_task": "LFQA evaluation fairness analysis (length effect)",
            "dataset_name": "ASQA, ANTIQUE, WikiEval (length-bin analysis across datasets; Figure 3)",
            "judge_model_name": "GPT-4, GPT-4o, Claude-3.5, Gemini-2.0 (LLM-based metrics analyzed alongside ROUGE-L/BERTScore)",
            "judge_model_details": "LLM evaluators referenced by model name; specific versions for some analyses (GPT-4, GPT-4o) noted; no sizes provided.",
            "human_evaluator_type": "Human annotators (baseline ratings)",
            "agreement_metric": "Correlation trends across length bins (no single scalar reported for this bias test)",
            "agreement_score": null,
            "reported_loss_aspects": "Deterministic metrics penalize longer answers; LLM judges may give higher scores to longer responses even when humans do not, leading to length-driven bias.",
            "qualitative_findings": "Different metrics react differently to answer length; combining deterministic and LLM-based metrics or normalizing for length is suggested to mitigate biases.",
            "advantages_of_llm_judge": "More tolerant of long answers and can reward comprehensiveness (depending on prompt).",
            "experimental_setting": "Answers binned into five equal-sized bins by length; metric scores analyzed per bin for trends (Figure 3).",
            "uuid": "e7865.5",
            "source_info": {
                "paper_title": "An Empirical Study of Evaluating Long-form Question Answering",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "question_type_bias",
            "name_full": "Question-type dependent performance and failure modes of LLM evaluators",
            "brief_description": "LLM-based metrics outperform deterministic metrics across ANTIQUE question types overall, but they underperform relative to their average on evidence-based and experience-type questions and show slightly higher consistency on informativeness than on correctness—indicating difficulty recognizing factual errors.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "An Empirical Study of Evaluating Long-form Question Answering",
            "evaluation_task": "LFQA evaluation across question types (reason, instruction, evidence-based, experience, debate, comparison)",
            "dataset_name": "ANTIQUE (category analysis of 200 questions; Table 4 and Figure 4)",
            "judge_model_name": "GPT-4, Claude-3.5, Gemini-2.0 (primary LLM evaluators analyzed by question type)",
            "judge_model_details": "LLM evaluators referenced by name; no size/training details provided.",
            "human_evaluator_type": "Human annotators (correctness and informativeness labels used as ground truth)",
            "agreement_metric": "Kendall tau correlation between metric and human scores per question type (reported in Figure 4)",
            "agreement_score": null,
            "reported_loss_aspects": "Reduced ability to detect factual inaccuracies (correctness) compared to assessing informativeness; lower consistency for evidence-based and experience questions.",
            "qualitative_findings": "LLM evaluators are better at evaluating informativeness than correctness and have varying reliability across different question genres; deterministic metrics can even show negative correlations for some types.",
            "advantages_of_llm_judge": "Outperform deterministic metrics across many question types; more reliable for informativeness judgments.",
            "experimental_setting": "200 ANTIQUE questions manually categorized; Kendall correlation computed per metric per question-type for correctness and informativeness (Figure 4).",
            "uuid": "e7865.6",
            "source_info": {
                "paper_title": "An Empirical Study of Evaluating Long-form Question Answering",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "self_reinforcing_bias",
            "name_full": "Self-preference (self-reinforcing) bias of LLM evaluators toward their own generations",
            "brief_description": "When LLMs are used both as generators and evaluators, they tend to favor outputs produced by themselves; the paper reports higher win rates for GPT-4o, Claude-3.5, and Gemini-2.0 when those same models act as the judge, with example win-rate contrasts (e.g., Gemini-2.0 gives itself win-rate 0.468 vs ~0.262/0.256 when other evaluators judge).",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "An Empirical Study of Evaluating Long-form Question Answering",
            "evaluation_task": "LFQA pairwise model comparison (self-evaluation bias study)",
            "dataset_name": "ASQA (self-reinforcing analysis reported on this dataset; Figure 6)",
            "judge_model_name": "GPT-4o, Claude-3.5, Gemini-2.0 (evaluators also used as generators)",
            "judge_model_details": "Closed-source LLMs used as both generators and judges; internal training/specs not given.",
            "human_evaluator_type": "Human annotators used for baseline comparisons but this test focuses on LLM-vs-LLM evaluation bias.",
            "agreement_metric": "Win rate / pairwise agreement (proportion of instances where one model's answer preferred over another)",
            "agreement_score": null,
            "reported_loss_aspects": "Clear bias toward own outputs (self-reinforcement), inflating self-assessed win rates; threatens impartiality of LLM-as-judge.",
            "qualitative_findings": "Each closed-source LLM tends to rank its own outputs higher than other evaluators do; recommendation: prefer rankings over raw scores and employ multiple independent evaluators.",
            "advantages_of_llm_judge": "When using multiple evaluators, comparative rankings remain broadly consistent despite self-bias; however reliance on a single LLM judge is problematic.",
            "experimental_setting": "Pairwise comparisons between closed-source LLMs (GPT-4o, Claude-3.5, Gemini-2.0) and several open-source baselines; win rates computed per evaluator (Figure 6).",
            "uuid": "e7865.7",
            "source_info": {
                "paper_title": "An Empirical Study of Evaluating Long-form Question Answering",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "idf_lexical_bias",
            "name_full": "Lexical (IDF) bias: LLM evaluators favor lexically sophisticated responses under some settings",
            "brief_description": "The paper finds that ROUGE-L and BERTScore correlate strongly with answer IDF, while GPT-4o and Gemini-2.0 align more closely with human judgments; under fine-grained settings GPT-4o tends to favor responses with higher average IDF, indicating a bias toward lexical sophistication.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "An Empirical Study of Evaluating Long-form Question Answering",
            "evaluation_task": "LFQA evaluation bias analysis w.r.t. lexical rarity (IDF)",
            "dataset_name": "ASQA, ANTIQUE, WikiEval (IDF relationship analysis across these datasets; Figure 5)",
            "judge_model_name": "GPT-4o, Gemini-2.0 (IDF bias particularly noted for GPT-4o under fine-grained settings)",
            "judge_model_details": "LLM evaluators referenced by name; no model size/training details provided.",
            "human_evaluator_type": "Human annotators (baseline ratings showed even distribution across IDF values for some datasets)",
            "agreement_metric": "Correlation analyses between average IDF and metric scores (no single scalar agreement reported)",
            "agreement_score": null,
            "reported_loss_aspects": "Potential bias towards lexically sophisticated (high-IDF) responses, especially under fine-grained prompting; may favor verbose/formal wording.",
            "qualitative_findings": "Human scores do not systematically prefer high-IDF answers, but some LLM evaluators (when finely prompted) do; this may skew evaluations toward 'sophisticated' wording rather than factual quality.",
            "advantages_of_llm_judge": "Can capture stylistic/lexical nuances, but risk over-weighting lexical rarity.",
            "experimental_setting": "Compute average IDF per answer and correlate with human and automatic metric scores; analyze distributions per dataset (Figure 5).",
            "uuid": "e7865.8",
            "source_info": {
                "paper_title": "An Empirical Study of Evaluating Long-form Question Answering",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "deterministic_metric_failure",
            "name_full": "Failure modes of deterministic metrics (ROUGE-L, Exact Match, BERTScore) vs human judgments",
            "brief_description": "Deterministic n-gram/overlap metrics often show weak or dataset-dependent correlation with human judgments; e.g., Exact Match aligned well only for short-answer ASQA cases, while ROUGE-L and Disambig-F1 performed poorly for long-form and open-ended answers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "An Empirical Study of Evaluating Long-form Question Answering",
            "evaluation_task": "LFQA metric comparison (deterministic vs model-based vs LLM-based)",
            "dataset_name": "ASQA, ANTIQUE, WikiEval (dataset-dependent reliability reported; Tables 1 & 2)",
            "judge_model_name": "N/A (deterministic metrics evaluated against human annotations)",
            "judge_model_details": "Deterministic metrics: Exact Match (EM), ROUGE-L, Disambig-F1; embedding metric: BERTScore.",
            "human_evaluator_type": "Human annotators (used as ground truth for meta-evaluation)",
            "agreement_metric": "Spearman rho / Kendall tau correlations reported in Tables 1 and 2",
            "agreement_score": null,
            "reported_loss_aspects": "Poor correlation with human judgments for long-form and open-ended QA; sensitivity to reference length and phrasing; sometimes negative correlations by question type.",
            "qualitative_findings": "ROUGE-L and BERTScore penalize longer or lexically divergent but valid answers; Exact Match only appropriate for short, closed answers; deterministic metrics not reliable substitutes for human judgment in LFQA.",
            "advantages_of_llm_judge": "By contrast, LLM-based evaluators show higher and more stable agreement with humans across diverse LFQA styles.",
            "experimental_setting": "Correlation analysis (Spearman/Kendall) between deterministic/model-based/LLM metrics and human ratings across three datasets (Tables 1 and 2).",
            "uuid": "e7865.9",
            "source_info": {
                "paper_title": "An Empirical Study of Evaluating Long-form Question Answering",
                "publication_date_yy_mm": "2025-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Can we trust the evaluation on ChatGPT?",
            "rating": 2,
            "sanitized_title": "can_we_trust_the_evaluation_on_chatgpt"
        },
        {
            "paper_title": "G-EVAL: NLG evaluation using gpt-4 with better human alignment",
            "rating": 2,
            "sanitized_title": "geval_nlg_evaluation_using_gpt4_with_better_human_alignment"
        },
        {
            "paper_title": "LLM-Eval: Unified multi-dimensional automatic evaluation for open-domain conversations with large language models",
            "rating": 2,
            "sanitized_title": "llmeval_unified_multidimensional_automatic_evaluation_for_opendomain_conversations_with_large_language_models"
        },
        {
            "paper_title": "Judging LLM-as-a-judge with MT-bench and Chatbot Arena",
            "rating": 2,
            "sanitized_title": "judging_llmasajudge_with_mtbench_and_chatbot_arena"
        },
        {
            "paper_title": "Can Large Language Models Be an Alternative to Human Evaluations?",
            "rating": 2,
            "sanitized_title": "can_large_language_models_be_an_alternative_to_human_evaluations"
        },
        {
            "paper_title": "Exploring the Use of Large Language Models for Reference-Free Text Quality Evaluation: A Preliminary Empirical Study",
            "rating": 1,
            "sanitized_title": "exploring_the_use_of_large_language_models_for_referencefree_text_quality_evaluation_a_preliminary_empirical_study"
        }
    ],
    "cost": 0.01847275,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>An Empirical Study of Evaluating Long-form Question Answering
25 Apr 2025</p>
<p>Ning Xian xianning21s@ict.ac.cn 0009-0004-1220-3021
Yixing Fan fanyixing@ict.ac.cn 0000-0003-4317-2702
Ruqing Zhang zhangruqing@ict.ac.cn 
Jiafeng Guo guojiafeng@ict.ac.cn 0000-0002-9509-8674</p>
<p>Institute of Computing Technology
Chinese Academy of Sciences
BeijingChina</p>
<p>Institute of Computing Technology
Chinese Academy of Sciences
BeijingChina</p>
<p>Institute of Computing Technology
Chinese Academy of Sciences
BeijingChina</p>
<p>Maarten de Rijke University of Amsterdam Amsterdam
The Netherlands</p>
<p>Institute of Computing Technology
Chinese Academy of Sciences
BeijingChina</p>
<p>An Empirical Study of Evaluating Long-form Question Answering
25 Apr 20251452A10D249AFD6D2F2FC5E87B80DC9F10.1145/3726302.3729895arXiv:2504.18413v1[cs.IR]Long-form question answering, automatic evaluation
Long-form question answering (LFQA) aims to generate lengthy answers to complex questions.This scenario presents great flexibility as well as significant challenges for evaluation.Most evaluations rely on deterministic metrics that depend on string or n-gram matching, while the reliability of large language model-based evaluations for long-form answers remains relatively unexplored.We address this gap by conducting an in-depth study of long-form answer evaluation with the following research questions: (i) To what extent do existing automatic evaluation metrics serve as a substitute for human evaluations?(ii) What are the limitations of existing evaluation metrics compared to human evaluations?(iii) How can the effectiveness and robustness of existing evaluation methods be improved?We collect 5,236 factoid and non-factoid long-form answers generated by different large language models and conduct a human evaluation on 2,079 of them, focusing on correctness and informativeness.Subsequently, we investigated the performance of automatic evaluation metrics by evaluating these answers, analyzing the consistency between these metrics and human evaluations.We find that the style, length of the answers, and the category of questions can bias the automatic evaluation metrics.However, fine-grained evaluation helps mitigate this issue on some metrics.Our findings have important implications for the use of large language models for evaluating long-form question answering.All code and datasets are available at https://github.com/bugtig6351/lfqa_evaluation.CCS Concepts• Information systems → Evaluation of retrieval results; Question answering.</p>
<p>Introduction</p>
<p>LFQA aims to enable generation or retrieval models to answer openended questions with long answers at the paragraph level [10,20].With the advancement of large language models (LLMs), the ability to generate long-form answers has improved significantly [4,27].While such long-form answers could address more complex and diverse questions, their flexibility presents significant challenges for evaluation.</p>
<p>Currently, much of the research on LFQA focuses on designing models and frameworks to address the hallucination of LLM and improve overall performance.For example, Su et al. [34] propose a framework that uses fine-grained, answer-related salient information to enhance the faithfulness of model responses and reduce hallucinations.Tao et al. [35] introduce a chain-of-discussion framework that takes advantage of synergy among multiple open source LLMs to provide more accurate and comprehensive answers.Despite these advancements, most studies still rely on simple question answering (QA) metrics, such as ROUGE [23] and Exact Match [33], which are based on string or n-gram matching, to evaluate longform answers.However, these metrics often fail to capture the nuanced and flexible evaluation required by the complex structure of long-form answers, demonstrating weak correlation with human judgment [20,25].</p>
<p>LLM-based evaluators.Recently, there has been growing interest in developing LLM-based evaluators to provide more reliable assessments of long-form answers [7].For example, Vu et al. [38] introduce LLM autoraters by training the PaLM-2-24B model on a collection of 102 quality assessment tasks comprising more than 5.3M human judgements (i.e., FLAMe).Fan et al. [11] propose an LLM-based metric, EVA-Score, to evaluate the informativeness in abstract lonng-form summarization.Although LLMs have shown promise in providing more comprehensive evaluations of longform answers, they remain prone to vulnerabilities due to their well-known hallucination issues [39].</p>
<p>Goals and questions.In this paper, we aim to fill the above gap by conducting a comprehensive analysis of existing automatic evaluation metrics, including deterministic metrics and model-based metrics.We compare automatic metrics with human evaluations to examine their strengths and limitations.Given this setup, we explore the following three research questions: (RQ1) To what extent do existing automatic evaluation metrics serve as substitutes for human evaluations in LFQA, and how accurate are these metrics?(RQ2) What are the limitations of automatic evaluation metrics compared to human evaluators?What influences their stability and fairness?(RQ3) How can the effectiveness and robustness of existing evaluation methods be improved?</p>
<p>Main findings.Our study involves both factual and non-factual LFQA tasks, collecting answers generated by seven different LLMs on the ASQA, ANTIQUE and Wikieval datasets, and performing a human evaluation on a subset of these answers (over 2,079 answer pairs with 4,158 ratings and justifications).</p>
<p>• For RQ1, we find that metrics based on large models demonstrate significantly higher consistency with human evaluations than deterministic metrics.They are also more stable in assessing different types of LFQA.• For RQ2, we conduct an analysis from two perspectives.First, we examine the impact of meaningless minor perturbations to the prompt on evaluation results, assessing the stability of the evaluators.Second, we investigate whether factors such as text length, question type, and non-semantic variations in phrasing introduce evaluator bias, thereby affecting the fairness of the evaluators.Our findings indicate that deterministic metrics are influenced by the length of the reference text and tend to penalize longer answers.In contrast, LLM-based methods do not exhibit any significant biases in this regard.• For RQ3, our research analyzes the impact of different prompting strategies on outcomes, highlighting that fine-grained evaluation methods exhibit higher accuracy compared to similar evaluation approaches.</p>
<p>Related Work</p>
<p>This work aims to analyze the shortcomings of existing evaluations in LFQA and provide guidance for future efforts.Therefore, we review previous work, including (i) automated evaluation methods, and (ii) evaluation of evaluators.</p>
<p>Automatic Evaluation Methods</p>
<p>To evaluate the performance of large models, numerous studies have used automatic evaluation metrics across multiple independent benchmarks [2,21,22].These metrics are generally categorized into three types: n-gram-based, embedding-based, and LLM-based.Each category offers distinct approaches for assessing model responses, aiming to balance efficiency and accuracy in evaluating various aspects of generated text.N-gram-based metrics, such as ROUGE [23] and BLEU [31], are widely adopted due to their high efficiency and low cost, primarily verifying the correctness of model responses.However, these metrics can be adversely affected by syntactic errors [32] and may struggle to capture human preferences when comparing outputs from different models [8,14], with recent studies further supporting these limitations [20,43].Embedding-based methods, such as BERTScore [46] and MoverScore [48], use pretrained language models to better capture semantic similarities, thereby reducing the impact of superficial textual changes.Despite their advancements, these methods rely heavily on high-quality reference texts, posing challenges for open-ended questions where crafting appropriate references is difficult [5].</p>
<p>While embedding-based approaches improve upon traditional n-gram methods by focusing on semantic content, their dependence on reference texts limits their applicability in more flexible evaluation scenarios.On the other hand, LLM-based evaluators offer a novel approach for reference-free evaluation, demonstrating reasonable performance in zero-shot settings [1,6,26].Examples include GPTScore [12], G-EVAL [25], LLM-EVAL [24], and RAGAS [9], which leverage large language models to provide multi-dimensional evaluations.However, LLMs may inherit biases from their training data [40] and generate hallucinations [17], with studies confirming biases in these evaluators [39,40,49].Although fine-grained evaluations can help mitigate some issues [28,45], the overall reliability of automatic evaluation methods as substitutes for human evaluators remains an area requiring further investigation.</p>
<p>Evaluation of Evaluators</p>
<p>Since LLM-as-a-Judge has become a new evaluation paradigm, many studies have assessed the effectiveness of these methods.The basic approach of previous research has been to treat various types of evaluators as virtual annotators and evaluate their consistency with human annotators to achieve alignment with human [15].</p>
<p>MT-bench and the Chatbot arena dataset [49] contain a small amount of human-crafted queries annotated by experts and a large volume of crowdsourced user preferences data from real-world users, analyzing the agreement between LLM evaluators and human annotators and bias of the evaluator.Xu et al. [43] collect the consistency between automatic metrics and human experts in the evaluation of LFQA across seven different knowledge domains, focus on overall answer preference, coherence, and factuality.PandaLM [41] trains a LLM evaluator and constructed a general instruction-tuning benchmark that includes a significant amount of automated scoring and human preference data, using a partial order graph to compare the performance of the models.</p>
<p>In this work, we will adopt previous evaluation approaches for meta-evaluation, assessing the performance of evaluators based on their consistency with human annotators.We will focus on the biases and robustness exhibited by different evaluation methods throughout this process.</p>
<p>A Study of LFQA Evaluation Methods</p>
<p>In this section, we examine how existing automatic LFQA evaluation metrics compare to human evaluations from three aspects, namely accuracy, robustness, and fairness.Firstly, the accuracy is to test to what extent the automatic metrics match human judgments.Here, we use responses from seven LLMs across three benchmark datasets as testbeds to assess the alignment between automatic metrics and human evaluations.Secondly, the robustness is to test the reliability of the automatic metrics.We assess the stability of the outcome when subjected to minor perturbations in inputs and hyperparameters.Finally, the fairness is to test whether existing automatic metrics exhibit biases toward specific attributes, e.g., style, length, or topic.</p>
<p>In the following, we first introduce the setting of the empirical study, including automatic evaluation metrics, models, and the testbed.Then, we show results concerning the accuracy, robustness, and fairness of all metrics.</p>
<p>Empirical Setup</p>
<p>3.1.1Testbed.For long-form question answering, we mainly consider two categories: one requires in-depth analysis and detailed explanations, while the other includes opinions, discussions, and other scenarios closely resembling real user interactions, which are often non-factoid.Both types of questions are considered challenging for LLMs, in terms of response generation or evaluation [10,20,49].Specifically, we perform experiments on the following three datasets, each serving as a representative dataset for ambiguous, factoid, and open-ended question answering.</p>
<p>• ASQA [33] is an ambiguous factual questions dataset in which each question has multiple disambiguated question-answer pairs and two long-form grounded answers annotated by humans.• ANTIQUE [16] is an open-ended question answering dataset, including 2626 questions asked by users of Yahoo!Answers, and relevant answers annotated by human experts.• WikiEval [9] is a factoid question answering dataset generated from 50 pages from Wikipedia with edits post 2022, annotated by human experts.Using the datasets mentioned above, we gather responses from seven latest LLMs from five different families for analysis.The models are listed as follows:</p>
<p>• GLM-4-9b-chat [13]: An advanced generative language model designed for dialogue applications with a focus on efficient response generation.• Llama2-7b-chat [37]: A conversational AI model from the Llama family, optimized for contextual understanding in dialogues with 7 billion parameters.• Llama2-13b-chat [37]: A more robust version of its predecessor, this model offers enhanced conversational abilities with 13 billion parameters.• Llama3-8b-instruct [36]: Building on the Llama series, this iteration focuses on instruction-following tasks with an 8 billionparameter architecture.• GPT-3.5-instruct[29]: A finetuned version of GPT-3.5, this model excels in following user instructions with improved precision.• Mistral-7b [18]: A lightweight yet powerful model, designed for streamlined AI tasks.• Solar-10.7b-instruct[19]: A state-of-the-art model tailored for specific instruction adherence, boasting 10.7 billion parameters for diverse task performance.To generate answers, we use the recommended generation parameters for each model and guide the models using few-shot settings.For each input, we select up to 8 examples.We selected 500 samples from the dev set of ASQA, 200 samples from the test set of AN-TIQUE and 50 samples from WikiEval to generate answers.After filtering out invalid QA pairs-such as those resulting from model refusals-we obtained 3,500 valid samples from ASQA, 1,386 from ANTIQUE, and 350 from WikiEval.</p>
<p>3.1.2Meta-evaluations.This involves the evaluation of the evaluation, serving as a method to assess the quality of automatic evaluation metrics.Here, we adopt two types of meta-evaluations, as outlined below.</p>
<p>• Correlation coefficient: When comparing the differences between two distributions, a commonly used metric is the correlation coefficient.Following Chen et al. [5], we use the Spearman correlation coefficient to measure the value correlation and Kendall correlation coefficient to measure the rank correlation between human ratings and automatic metrics.• Win Rate and Agreement: When comparing multiple LLMs, researchers often use the win rate, defined as the fraction of instances in which model A's response outperforms model B's.Furthermore, for two different evaluators, their agreement can be calculated, which represents the proportion of instances where the evaluators made the same judgment regarding the quality of the responses from the two models [49].</p>
<p>Human annotations.</p>
<p>In the annotation of model responses, we drew inspiration from previous work [47] and primarily considered two evaluation aspects.</p>
<p>• Correctness aims to determine the accuracy of the answers, determining whether there are any factual errors or inaccuracies in the model's response.A high-quality response should be factually reliable and free from mistakes.• Informativeness examines whether the response contains sufficient and relevant information, identifying any missing or omitted details, and ensuring there is no excessive redundancy.A highquality response should should be informative enough without being redundant.</p>
<p>For the results of the seven models on the ASQA, ANTIQUE, and WikiEval datasets, we randomly sampled 50, 200, and 50 questions, respectively, for manual evaluation, with each question having 7 answers from different models.After removing a few cases where models refused to answer or generated incorrect responses, we collected 343, 1386, and 350 valid QA pairs that include both correctness and informativeness.We developed a web-based interface to streamline the annotation process.Figure 1 illustrates the interface for a single question.Each question is paired with a generated answer and a reference answer.Deterministic metrics: The deterministic metrics typically assess results based on lexical overlap between the predicted answer and the reference answer.Early works on LFQA usually take Exact Match (EM) and Rouge-L (RL) as the evaluation metrics [10,33].The EM metric is to evaluate whether the predicted answer matches any of a fraction in the reference answer [33].The Rouge-L is to evaluate the quality of the predicted answer by measuring the longest common subsequence between the generated text and the reference answer [23].We also include the Disambig-F1 (DF1) [33], which is to evaluate the fraction of diambiguated questions answered by the predicted answer, for the evaluation study.</p>
<p>Model-based metrics: The model-based metrics often assess the semantic accuracy of the generated answer using pre-trained models, extending beyond traditional n-gram matching.The BERTScore (BS) [46] calculate the cosine similarity between the generated answer and the reference answer.In addition, we considered the answer relevance (AR) from RAGAS [9] to supplement the measurement of responses' informativeness.For LLM-based evaluation, we follow LLM-EVAL [24] to use prompt-driven GPT-4 model as the judge in both coarse-grained (CG) and fine-grained (FG) settings.</p>
<p>Accuracy of Automatic Metrics</p>
<p>In this section, we try to answer RQ1, that is to what extent do existing automatic evaluation metrics serve as substitutes for human evaluations in LFQA.In this part, we take responses of seven LLMs on three datasets as the testbed.For each response, we conduct a human rating and compute the scores for each metric.We then assess the correlation coefficient between these metrics and human ratings across the entire dataset.Additionally, for any pair of model-generated responses, we calculate the agreement between each metric and human rating to determine whether they align in their preference between these two answers.The results of correlation coefficients between automatic metrics and human ratings for the three datasets are presented in Table 1 and 2. Firstly, for deterministic metrics, we can see that exact match demonstrates relatively strong alignment with human evaluation on ASQA dataset, even outperforming LLM-based evaluations such as GPT-4o, Claude-3.5 and Gemini-2.0.This is primarily because exact match focuses solely on assessing short answers in ASQA.However, while evaluating long answers, metrics like Rouge-L and Disambig-F1 exhibit poor consistency with human evaluations.An exception is observed with Rouge-L, which yields significantly different results on the Antique and WikiEval datasets.It shows high consistency with human evaluations on WikiEval but low consistency on Antique.This discrepency arises because Antique features non-factoid QA with open-ended answers, whereas WikiEval focuses on factoid QA with closed-ended answers.</p>
<p>Secondly, for model-based metrics, we can see that BertScore exhibits a trend similar to Rouge-L, showing low consistency with human evaluations on the ASQA and Antique datasets but high consistency on the WikiEval dataset.In contrast, LLM-based evaluations exhibit strong consistency with human evaluations across various styles of LFQA, including the ambiguous QA, fatoid QA, and non-factoid QA, highlighting their stability in evaluation.For example, the fine-grained (FG) GPT4 achieves the best performance on both the ASQA and Antique datasets.</p>
<p>Lastly, when comparing different LLM-based evaluations, we find that GPT-4o evaluation demonstrates superior performance over other LLMs.For example, the GPT-4o, Claude-3.5, and Gemini-2.0achieve spearman correlation score of 42.0, 33.0, and 32.4,respectively, when compared to human evaluations.Moreover, the fine-grained evaluation with GPT-4o improves its score from 42.0 to 55.0, which demonstrates the importance of providing more detailed instructions for LLM-based evaluations.</p>
<p>In summary, LLM-based evaluations demonstrate stability across different types of LFQA, whereas deterministic evaluations are less reliable for open-ended QA.Furthermore, conducting finegrained evaluations with LLM-based assessments would produce more nuanced results.</p>
<p>Robustness of Automatic Metrics</p>
<p>In this section, we focus on the first part of RQ2, which investigates the robustness of automatic evaluation metrics.For this purpose, we introduce small perturbations to each evaluation method, and analyze the changes before and after the perturbation.Here, we concentrate on model-based evaluation metrics.Firstly, we collects the scores generated by automatic evaluation metrics under their default configurations.Subsequently, we introduce perturbations to the experimental conditions and obtain the corresponding perturbed scores.To quantify the consistency of evaluators' decisions before and after the introduction of perturbations, we statistically analyzed the model win rates and the distribution of metric scores under different experimental settings.</p>
<p>Perturbation of prompts. The prompts of LLMs have been</p>
<p>shown to significantly influence their output, giving rise to a new research area known as prompt engineering [44].To better understand the impact of prompts on the evaluation capability of LLMs, we conducted experiments comparing different prompts using GPT-4o on the ASQA dataset, including short prompt, normal prompt, and long prompt.More detailed description of prompts are shown in code repository. 1he results are depicted in Figure 2: (i) LLM-based evaluations tend to yield lower scores compared to human evaluations, with the majority of LLM-based scores clustering around 3, while human evaluation scores are predominantly around 5. This discrepancy may be due to LLM-based evaluations being more stringent than human evaluations.(ii) The length of the prompt has a significant impact on the evaluation scores of LLMs.It is evident that short prompts tend to result in more high scores.For example, the number of samples with scores above 4 for the short prompt is noticeably higher than for the normal and long prompts.</p>
<p>Perturbation of hyper-parameters.</p>
<p>Hyperparameters such as sampling temperature, top-k sampling, repetition penalty, and maximum token length all play a role in shaping the LLM's output and overall performance [30].However, the impact of sampling temperature on LLM-as-a-judge has not been specifically investigated.The selection of sampling temperature is largely based on guesswork and intuition.Here, we take GPT-4o as the judge to analyze the impact of temperature.</p>
<p>All results are summarized in Table 3.If multiple models have the same score, we take the average rank as their shared rank.As we can see: (i) In the ASQA dataset, the evaluation results are highly stable, where minimal changes in score and no changes in the rankings of any LLMs.This could be because the evaluation score for each model vary widely, ranging from 3.03 to 8.66, making it difficult for the rankings to change.(ii) In the WikiEval dataset, the evaluation results vary significantly with changes in the temperature of the LLM judge.For example, the rank of GPT-4o drops from 4th to 6th when the temperature increases from 0.0 to 0.3.In contrast, its rank   improves from 4th to 2nd when the temperature is set to 0.7 or 1.0.This may be because the evaluation scores of each model are very similar, with the minimum and maximum values being 6.60 and 8.62, respectively.Overall, the above observations indicate that the temperature of the LLM does impact the evaluation, but the extent of this impact depends on the dataset.</p>
<p>Fairness of Automatic Metrics</p>
<p>In this section, we address the second part of RQ2, which focuses on the fairness of automatic evaluation metrics.Specifically, we conduct a comprehensive analysis of the evaluators' biases across four key dimensions: response length, question type, answer generation models, and language representations.</p>
<p>Length bias.</p>
<p>Here, we aim to examine whether answer length influences the evaluation of automatic metrics, particularly LLMbased evaluation methods.To this end, we divide all answers into five bins with equal sample sizes based on their length and analyze the performance of different evaluation metrics across these length intervals.All the results are depicted in Figure 3.Our experimental results reveal distinct patterns in the relationship between answer length and various evaluation metrics.We observe a negative correlation between answer length and scores on the Rouge-L and BERTScore metrics.As the length of the answer increases, the Rouge-L and BERTScore metrics tend to decrease.This trend suggests that longer answers may introduce redundancy or deviate from the concise, information-dense content that these metrics favor.Rouge-L, which measures the overlap between generated and reference texts, is particularly sensitive to extraneous information that dilutes the precision of the answer.Similarly, BERTScore, which evaluates semantic similarity, may penalize longer answers that include tangential or less relevant content.</p>
<p>On the ASQA and WikiEval dataset, we observe a positive correlation between answer scores and length in Claude-3.5.While the average scores remained relatively stable, the lower bounds of scores assigned by GPT-4 and Gemini-2.0increased significantly with longer answers.For the Antique dataset, the trend of increasing scores with length is evident, and this trend is more pronounced in LLM-based methods compared to human evaluations.This indicates that LLMs are less likely to assign low scores to longer answers, a phenomenon not observed in human evaluations.These evaluation frameworks may prioritize comprehensiveness and detail over conciseness.</p>
<p>Our analysis reveals that answer length significantly influences the performance of automatic evaluation metrics, with distinct patterns observed across different metrics and models.Combine traditional metrics (e.g., Rouge-L, BERTScore) with LLM-based metrics to balance the strengths and weaknesses of each approach.To address these biases and improve the robustness of automatic evaluation metrics, some strategies could be considered.For example, traditional metrics can ensure precision and conciseness, while LLM-based metrics can capture comprehensiveness and detail.And develop metrics that account for answer length by normalizing scores based on the amount of relevant information.This could involve penalizing redundancy while rewarding additional meaningful content.3.4.2Question type.In this section, we investigate the potential biases of different evaluation metrics when assessing various types of questions.Building upon the research conducted by Bolotova et al. [3], we classified 200 questions from the antique dataset, with the specific quantities and proportions detailed in Table 4.For each category of questions, we calculated the kendall correlation consistency between human evaluators' scores for correctness and informativeness and various metrics, as illustrated in Figure 4.</p>
<p>Our analysis reveals several key findings.First, we observe that LLM-based metrics significantly outperform deterministic metrics (Rouge-L and BERTScore), with the latter showing negative correlation coefficients on this dataset, rendering them practically unreliable for evaluation purposes.Second, LLM-based metrics demonstrate slightly higher consistency in assessing informativeness compared to correctness, particularly for comparison and experience-type questions.This discrepancy may stem from the inherent difficulty of LLMs in identifying factual inaccuracies within generated responses.Finally, evidence-based and experience-type questions emerge as challenging areas for all metrics, with the three primary evaluation models (GPT-4, Claude-3.5, and Gemini-2.0)consistently underperforming relative to their average performance across other question types.</p>
<p>Based on our findings, we can conclude that LLM indeed exhibit evaluation biases across different types of question, particularly for questions relying on precise factual information or personal experiences and recommendations.However, potential improvements can be achieved through the implementation of more granular prompting strategies combined with a diversified evaluation approach.</p>
<p>3.4.3Self-reinforcing.In this section, we investigate whether LLMbased metrics show a preference for results generated by themselves.We take the widely adopted LLMs, i.e., GPT-4o, Claude-3.5, and Gemini-2.0,as both evaluators and generators on the ASQA dataset.Specifically, we take these three LLM to generate answers for each question, and conduct a pairwise comparison between them and other seven open-sourced LLMs.The results are illustrated in Figure 6.</p>
<p>We can see that: (i) The three closed-source LLMs (GPT-4o, Claude-3.5, and Gemini-2.0)consistently outperform the seven baseline LLMs (Mistral-7b, Llama2-7b, Llama3-8b, Solar-10.7b,GLM4-9b, and GPT-3.5-turbo).Among them, Claude-3.5 achieves the best performance in all baselines across all three LLM-based evaluations.</p>
<p>(ii) Among the three evaluation methods, we observe that GPT-4o and Claude-3.5 evaluations exhibit a strong bias towards their own responses.For example, GPT-4o achieves significantly higher win rates against all baselines when evaluated using GPT-4o compared to evaluations conducted by Claude-3.5 and Gemini-2.0.Similarly, Claude-3.5 demonstrates much higher win rates in its own evaluation than when assessed by GPT-4o and Gemini-2.0.Furthermore, the Gemini-2.0evaluation also gives much higher scores to itself compared to GPT-4o and Claude-3.5 evaluations (e.g., 0.468 vs. 0.262/0.256,0.388 vs. 0.163/0.06),when compared with GPT-4o and Claude-3.5.In summary, LLM-based evaluations tend to assign significantly higher scores to their own outputs, demonstrating a clear evaluation bias.Interestingly, despite this bias, the models' ranking remains consistent across different evaluations.Based on these, we recommend the following: (i) Prioritize using rankings over scores for comparisons, (ii) Employ multiple evaluation methods to achieve more reliable and stable results.</p>
<p>Word expression.</p>
<p>Prior work shows that LLMs often produce verbose, formal content [42].To investigate whether LLM-based evaluations favor answers containing rarer terms, we compute each answer's average inverse document frequency (IDF) and examine its correlation with the evaluation scores.The results are illustrated in Figure 5.As observed: On Antique and WikiEval, human scores are evenly distributed across IDF values, showing no clear link between answer quality and term rarity, while ASQA displays a unique bimodal pattern, suggesting different answer characteristics.Automatic metrics like Rouge-L and BERTScore strongly correlate with IDF values, following near-normal distributions on Antique and WikiEval.In contrast, GPT-4o and Gemini-2.0align more closely with human judgments.Notably, under fine-grained settings, GPT-4o tends to favor responses with higher IDF values, indicating a potential bias toward lexically sophisticated responses.These results highlight the importance of dataset-specific evaluation strategies and the need to address IDF-related biases in LLMbased metrics, and this similarity becomes even more pronounced when fine-grained evaluation settings are applied.</p>
<p>Method</p>
<p>In this section, we investigate approaches to improve the performance of LLM-based evaluators to address RQ3: "How can the effectiveness and robustness of existing evaluation methods be improved?"To achieve this, we transfer the findings in previous sections into criteria in the prompt of LLM-based evaluation.Specifically, we decompose the prompt architecture into four key components: task description, data specifications, output requirements, and evaluation criteria.Then, we obtain 9 prompts by different combinations of these four components as outlined in Table 6.Due to space limitations, all detailed prompts can be found in our released code. 2 Finally, we measure performance variations by calculating the Kendall correlation coefficient changes between GPT-4o evaluations and human ratings for each prompt configuration.</p>
<p>All results are shown in Table 5.Our main findings are as follows.(1) All four components are essential for improving LLMbased evaluations: Comparing the performance of P1 with P2, P5 and P6, it can be observed that P1 achieves significantly better performance across all LLMs.P1 incorporates all four components, while P2 excludes the Criteria, P5 omits the Data, and P6 lacks the Task.Moreover, the evaluation of glm4-9b and gpt-turbo-3.5 drop significantly when removes the Criteria from the prompts (i.e., P2 vs. P3).Besides, the Task is important for LLM-based evaluations as performance drops significantly by comparing P3 vs. P6.(2) The order of components influences the LLM-based evaluations: A comparison of P1, P3, P4, P7, and P9 shows that the order of the four components has varying impacts on the evaluation of different LLMs.Notably, the placement of the Output appears to have minimal influence on gpt-3.5-turbo,as the performance is very close between P3 and P4-where Output is positioned last in P3 and first in P4.</p>
<p>Components Instructions</p>
<p>Task Score the following LLM output of a question-answering task with respect to the following aspects using a 1 to 5 star rating system.</p>
<p>Data</p>
<p>The dataset is a Factoid Question-Answering dataset, specifically designed for evaluating factual precision and detailed comparative reasoning in AI-generated answers.</p>
<p>Output</p>
<p>Begin your evaluation by providing a short explanation.Be as objective as possible.After providing your explanation, please provide your evaluation by strictly following the JSON format, such as: [[SCORE]] {"accuracy": 2, "informativeness": 3}.</p>
<p>Criteria</p>
<p>Accuracy: Determine the accuracy of the answers, verifying the correctness and reliability of the information provided.(3) No consistent improvements can be obtained for the evaluation of all LLMs by one strategy: There is no one strategy can boost the performance of all LLMs evaluations, where glm4-9b, llama2-7b, and llama3-8b performs best with P4, gpt-3.5-turboperforms best with P3, llama2-13b performs best with P1, solar-10.7bperforms best with P9.The differences in answer styles between models may significantly affect the evaluator's performance across various prompts.</p>
<p>These results demonstrate the necessity of structured guidance and precise evaluation guidelines for reliable assessments.</p>
<p>Conclusion</p>
<p>In this work, we evaluated a range of automatic evaluation metrics, including traditional deterministic metrics and model-based metrics, across three diverse datasets: ASQA, Antique, and WikiEval.For each dataset, we compared the scores generated by these metrics with human evaluations, analyzed their robustness under perturbations, and investigated potential biases related to answer length, question type, self-reinforcement, and language expression.Additionally, we explored methods to improve the performance of LLM-based evaluators through fine-grained evaluation strategies.</p>
<p>Our evaluation indicates that even with relatively high-quality reference answers, deterministic evaluation metrics still perform poorly and often do not exhibit significant correlation with human judgments.Their performance is highly dependent on the dataset and question type, with limited applicability to complex LFQA tasks.LLM-based metrics, such as GPT-4o and Claude-3.5,demonstrate better alignment with human evaluations and greater stability across different types of questions.However, they are susceptible to biases related to answer length, question type, and selfreinforcement.For example, LLM-based evaluators tend to favor longer answers and their own generated responses.The observed biases in LLM-based evaluations call for the development of more equitable evaluation frameworks that account for factors such as answer length, question type, and language expression.</p>
<p>Although our study has investigated the performance of automatic evaluation metrics across multiple dimensions, there are still some limitations that point to directions for future work: (i) Our 750-question evaluation covers only 3 QA categories, while realworld LFQA involves more diverse formats (e.g., multihop reasoning).Future research should evaluate automatic metrics on broader datasets, including more diverse type and domain of questions.(ii) We adopted a single-point scoring approach, and for human evaluation, we only set two main scoring criteria: correctness and informativeness.This may miss subtle quality differences between answers.A more nuanced evaluation setup would help fully capture various potential responses and edge cases.(iii) Our experiments have revealed the sensitivity of LLM-based models to prompts, but we have not conducted a detailed analysis of how different models respond to this, further investigation is needed.</p>
<p>Figure 1 :
1
Figure 1: Interface used for collecting human annotations.</p>
<p>Figure 2 :
2
Figure 2: Score distribution of different prompts on ASQA.</p>
<p>Figure 3 :
3
Figure 3: Relationship between answer length and metrics.</p>
<p>Figure 4 :
4
Figure 4: Relationship between different metrics and human evaluations across different question types on the ANTIQUE dataset (left: Informativeness, right: Correctness).</p>
<p>(a) Results on the WikiEval dataset.(b) Results on the ASQA dataset.(c) Results on the Antique dataset.</p>
<p>Figure 5 :
5
Figure 5: Relationship between the IDF of answer and different metrics.</p>
<p>Figure 6 :
6
Figure 6: Winrate for different metrics on the ASQA dataset.</p>
<p>1 star: Incorrect information 2 stars: Partially correct information 3 stars: Half correct information 4 stars: Mostly correct information 5 stars: Perfectly correct information Informativeness: Examines whether the answers provide sufficient and meaningful information that is useful to the user and relevant to the question. 1 star: No information or irrelevant information 2 stars: Very little information 3 stars: Some information 4 stars: Enough information 5 stars: Highly informative</p>
<p>Table 1 :
1
Correlation coefficients between automatic metrics and human ratings for ASQA dataset.The best score for each column is highlighted in bold.The second best is underlined.
Spearman/KendallDeterministic MetricsModel-based Metrics(%)RLEMDF1BSARFGGPT-4oClaude-3.5 Gemini-2.0glm4-9b31.0/22.8 26.1/22.4 31.4/24.0 29.4/22.5 22.4/14.6 66.5/52.2 56.6/45.037.2/30.615.2/11.7gpt-3.5-turbo-0.3/-0.5 48.6/40.3 18.5/12.92.6/2.48.3/5.750.8/40.6 32.6/25.122.3/18.315.4/11.9llama2-13b21.9/16.3 56.6/47.714.4/9.8 31.1/23.78.7/6.066.6/54.1 29.1/22.113.1/10.541.1/30.1llama2-7b17.0/12.1 54.3/46.2 14.7/11.1 22.9/16.6 23.7/17.3 43.6/35.6 32.5/24.630.8/22.622.6/15.4llama3-8b-2.4/-1.6 47.5/39.4 59.1/44.8 12.1/8.1 14.0/10.1 72.1/60.5 43.5/33.820.7/17.532.1/23.8mistral-7b53.3/37.9 57.0/49.9 33.2/24.0 33.5/24.0 17.1/12.0 62.0/49.3 47.8/37.359.0/45.751.3/39.5solar-10.7b4.3/2.610.4/8.6-8.9/-6.19.0/8.48.9/7.022.9/22.3 41.4/32.220.6/17.78.5/6.8Average11.4/8.041.4/34.9 23.0/16.8 16.2/11.7 12.2/8.7 55.0/44.9 42.0/32.433.0/26.332.4/24.4</p>
<p>Table 2 :
2
Correlation coefficients between automatic metrics and human ratings for ANTIQUE and WikiEval dataset.The best score for each column is highlighted in bold.The second best is underlined.
Spearman/KendallANTIQUE datasetWikiEval dataset(%)RLBSARCGFGRLBSCGFGglm4-9b-14.4/-11.4-9.8/-7.70.5/0.436.7/33.1 54.6/51.7 44.4/34.4 44.6/34.4 38.1/34.8 39.1/35.7gpt-3.5-turbo2.8/2.0-9.1/-7.043.7/35.0 53.2/47.1 56.5/51.7 57.1/45.1 56.8/43.1 41.9/36.5 46.9/42.1llama2-13b11.0/7.80.5/0.510.9/7.6 74.9/62.5 81.5/71.2 61.4/47.2 57.1/44.3 61.8/51.7 55.4/46.8llama2-7b13.8/10.09.3/6.917.7/13.4 69.3/58.6 74.6/64.7 74.6/60.4 75.2/60.3 60.8/52.8 60.1/52.9llama3-8b2.4/1.58.7/6.32.5/1.665.6/54.6 72.2/63.1 51.7/40.6 44.9/34.5 44.1/37.7 35.7/31.9mistral-7b29.2/21.025.9/18.5 23.3/16.9 73.0/62.5 83.9/73.6 51.3/40.0 60.7/48.9 47.7/40.0 50.4/42.6solar-10.7b-4.6/-3.4-9.7/-6.6
-3.5/-2.354.3/46.270.8/62.556.3/43.354.3/41.7 41.2/35.5 34.1/31.1 Average -7.9/-5.9-24.4/-17.228.3/20.683.0/70.985.2/75.6 55.8/43.455.5/42.6 51.4/43.949.0/43.1 3.1.4Automatic metrics.For evaluation metrics, we include 7 widely used metrics for long-form question answering, including traditional deterministic metrics and recently model-based metrics.</p>
<p>Table 3 :
3
The relationship between model scores and evaluators' temperature, with the default temperature set to 0.
ASQAWikievalScores RanksScores GainRank GainScores RanksScores GainRank Gain</p>
<p>Table 4 :
4
Question types in the ANTIQUE dataset.
Question TypeCount ProportionREASON4270.31INSTRUCTION3570.26EVIDENCE-BASED2660.19EXPERIENCE1540.11DEBATE1050.08COMPARISON770.06</p>
<p>Table 5 :
5
Variation of correlation coefficients under different prompt words.
Kendall (%)LLMsPromptsGPT-4o Claude-3.5 Gemini-2.0P1P2P3P4P5P6P7P8P9glm4-9b34.76-36.15-2.728.56 -17.45 3.489.873.914.102.25-2.387.49gpt-3.5-turbo36.47-2.02-6.4816.440.6127.64 22.548.3615.10 12.36 13.24 19.84llama2-13b51.66-23.504.9316.54 10.52 11.738.3511.32 12.11 12.13 12.418.62llama2-7b52.77-11.17-4.886.082.857.7810.224.435.690.056.025.79llama3-8b37.66-1.70-18.2518.93 18.247.6921.73 15.333.6317.22 15.90 15.67mistral-7b39.97-31.42-31.06-6.44 -10.06 -6.78 -16.19 -17.30 -15.88 -6.74 -16.58 -14.19solar-10.7b35.45-8.168.5612.29 15.01 21.72 14.799.894.6519.94 14.61 24.10Average43.89-15.91-4.8411.685.4812.44 11.157.715.749.498.2111.38</p>
<p>Table 6 :
6
Prompt settings.
Prompt SettingsP1Task+Data+Output+CriteriaP2Task+Data+OutputP3Task+Data+Criteria+OutputP4Output+Task+Data+CriteriaP5Task+Output+CriteriaP6Data+Output+CriteriaP7Output+Criteria+Task+DataP8Data+Task+Criteria(short)+OutputP9Data+Task+Criteria+Output</p>
<p>Table 7 :
7
Evaluation instructions.</p>
<p>https://github.com/bugtig6351/lfqa_evaluation/src/prompts.py (a) Results on the ASQA dataset (b) Results on the Wikieval dataset (c) Results on the Antique dataset
https://github.com/bugtig6351/lfqa_evaluation/src/prompts.py
AcknowledgmentsThis work was funded by the National Natural Science Foundation of China (NSFC) under Grants No. 62372431, 62472408 and 62441229, the Strategic Priority Research Program of the CAS under Grants No. XDB0680102 and XDB0680301, the National Key Research and Development Program of China under Grants No. 2023YFA1011602, the Lenovo-CAS Joint Lab Youth Scientist Project, and the project under Grants No. JCKY2022130C039.This research was (partially) supported by the Dutch Research Council (NWO), under project numbers 024.004.022,NWA.1389.20.183, and KICH3.LTP.20.006, and the European Union's Horizon Europe program under grant agreement No 101070212.All content represents the opinion of the authors, which is not necessarily shared or endorsed by their respective employers and/or sponsors.
Can we trust the evaluation on ChatGPT?. Rachith Aiyappa, Jisun An, Haewoon Kwak, Yong-Yeol Ahn, 10.48550/arXiv.2303.12767arXiv:2303.12767arXiv:2303.127672023. March 2023</p>
<p>Meghana Moorthy Bhat, Rui Meng, Ye Liu, Yingbo Zhou, Semih Yavuz, 10.48550/arXiv.2309.08210arXiv:2309.08210arXiv:2309.08210Investigating Answerability of LLMs for Long-Form Question Answering. 2023. Sept. 2023</p>
<p>A Non-Factoid Question-Answering Taxonomy. Valeriia Bolotova, Vladislav Blinov, W Bruce Falk Scholer, Mark Croft, Sanderson, 10.1145/3477495.3531926Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '22). the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '22)New York, NY, USAAssociation for Computing Machinery2022</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with GPT-4. arXiv. 2023. 2023arXiv preprint</p>
<p>Exploring the Use of Large Language Models for Reference-Free Text Quality Evaluation: A Preliminary Empirical Study. Yi Chen, Rui Wang, Haiyun Jiang, Shuming Shi, Ruifeng Xu, 10.48550/arXiv.2304.00723arXiv:2304.00723arXiv:2304.007232023. April 2023</p>
<p>Can Large Language Models Be an Alternative to Human Evaluations?. Cheng- , Han Chiang, Hung-Yi Lee, 10.48550/arXiv.2305.01937arXiv:2305.01937arXiv:2305.019372023. May 2023</p>
<p>A closer look into automatic evaluation using large language models. Cheng- , Han Chiang, Hung-Yi Lee, arXiv:2310.056572023. 2023arXiv preprint</p>
<p>On The Evaluation of Machine Translation Systems Trained With Back-Translation. Sergey Edunov, Myle Ott, Marc ' , Aurelio Ranzato, Michael Auli, 10.48550/arXiv.1908.05204arXiv:1908.05204arXiv:1908.052042020. Aug. 2020</p>
<p>Shahul Es, Jithin James, Luis Espinosa-Anke, Steven Schockaert, arXiv:2309.15217Ragas: Automated evaluation of retrieval augmented generation. 2023. 2023arXiv preprint</p>
<p>ELI5: Long Form Question Answering. Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, Michael Auli, 10.18653/v1/P19-1346Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational Linguistics2019</p>
<p>Yuchen Fan, Xin Zhong, Yazhe Wan, Chengsi Wang, Haonan Cheng, Gaoche Wu, Ning Ding, Bowen Zhou, arXiv:2407.04969EVA-Score: Evaluating Abstractive Long-form Summarization on Informativeness through Extraction and Validation. 2024. 2024arXiv preprint</p>
<p>Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, Pengfei Liu, arXiv:2302.04166Gptscore: Evaluate as you desire. 2023. 2023arXiv preprint</p>
<p>Glm Team, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Mingdao Liu, Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, Shulin Cao, Shuxun Yang, Weng Lam Tam, Wenyi Zhao, Xiao Liu, Xiao Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan Liu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang, Zhen Yang, Zhengxiao Du, Zhenyu Hou, Zihan Wang, arXiv:2406.12793ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools. 2024</p>
<p>Yvette Graham, Barry Haddow, Philipp Koehn, 10.48550/arXiv.1906.09833arXiv:1906.09833arXiv:1906.09833Translationese in Machine Translation Evaluation. 2019. June 2019</p>
<p>Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, Yuanzhuo Wang, Jian Guo, 10.48550/arXiv.2411.15594arXiv:2411.15594arXiv:2411.15594A Survey on LLM-as-a-Judge. 2024. Dec. 2024</p>
<p>ANTIQUE: A non-factoid question answering benchmark. Helia Hashemi, Mohammad Aliannejadi, Hamed Zamani, Bruce Croft, Advances in Information Retrieval: 42nd European Conference on IR Research. Lisbon, PortugalSpringer2020. April 14-17, 20202020Proceedings, Part II 42</p>
<p>Survey of Hallucination in Natural Language Generation. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Wenliang Dai, Andrea Madotto, Pascale Fung, 10.1145/3571730arXiv:2202.03629Comput. Surveys. 552023. Dec. 2023</p>
<p>. Alexandre Albert Q Jiang, Arthur Sablayrolles, Chris Mensch, Devendra Bamford, Diego Singh Chaplot, Florian De Las Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, arXiv:2310.068252023. 2023Mistral 7B. arXiv preprint</p>
<p>Dahyun Kim, Chanjun Park, Sanghoon Kim, Wonsung Lee, Wonho Song, Yunsu Kim, Hyeonwoo Kim, Yungi Kim, Hyeonju Lee, Jihoo Kim, Changbae Ahn, Seonghoon Yang, Sukyung Lee, Hyunbyung Park, Gyoungjin Gim, Mikyoung Cha, Hwalsuk Lee, Sunghun Kim, arXiv:2312.15166[cs.CL]SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling. 2023</p>
<p>Hurdles to Progress in Long-form Question Answering. Kalpesh Krishna, Aurko Roy, Mohit Iyyer, 10.18653/v1/2021.naacl-main.393Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, Yichao Zhou, the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesAssociation for Computational Linguistics2021</p>
<p>Md Tahmid Rahman Laskar, M Saiful Bari, Mizanur Rahman, 10.48550/arXiv.2305.18486arXiv:2305.18486arXiv:2305.18486Md Amran Hossen Bhuiyan, Shafiq Joty, and Jimmy Xiangji Huang. 2023. A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets. July 2023</p>
<p>Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D Manning, Christopher Ré, Diana Acosta-Navas, Drew A Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, Yuta Koreeda, arXiv:2211.09110arXiv:2211.09110Holistic Evaluation of Language Models. 2022. Nov. 2022</p>
<p>Rouge: A package for automatic evaluation of summaries. Chin-Yew Lin, Text summarization branches out. 2004</p>
<p>Llm-eval: Unified multi-dimensional automatic evaluation for open-domain conversations with large language models. Yen-Ting Lin, Yun-Nung Chen, arXiv:2305.137112023. 2023arXiv preprint</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu, arXiv:2303.16634G-eval: Nlg evaluation using gpt-4 with better human alignment. 2023. 2023arXiv preprint</p>
<p>Adian Liusie, Potsawee Manakul, Mark J F Gales, 10.48550/arXiv.2307.07889arXiv:2307.07889arXiv:2307.07889LLM Comparative Assessment: Zero-shot NLG Evaluation through Pairwise Comparisons using Large Language Models. 2023. Aug. 2023</p>
<p>Language models are few-shot learners. Ben Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, arXiv:2005.1416512020. 2020arXiv preprint</p>
<p>FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation. Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-Tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, Hannaneh Hajishirzi, 10.48550/arXiv.2305.14251arXiv:2305.14251arXiv:2305.142512023. Oct. 2023</p>
<p>GPT-3.5-Turbo-Instruct. 2023OpenAI</p>
<p>Explore developer resources, tutorials, API docs, and dynamic examples to get the most out of OpenAI's platform. 2025OpenAI</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>An Investigation into the Validity of Some Metrics for Automatically Evaluating Natural Language Generation Systems. Ehud Reiter, Anja Belz, 10.1162/coli.2009.35.4.35405Computational Linguistics. 352009. Dec. 2009</p>
<p>ASQA: Factoid Questions Meet Long-Form Answers. Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, Ming-Wei Chang, 10.18653/v1/2022.emnlp-main.566Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Zornitsa Goldberg, Yue Kozareva, Zhang, the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Read before Generate! Faithful Long Form Question Answering with Machine Reading. Dan Su, Xiaoguang Li, Jindi Zhang, Lifeng Shang, Xin Jiang, Qun Liu, Pascale Fung, arXiv:2203.00343[cs.CL2022</p>
<p>Chain-of-Discussion: A Multi-Model Framework for Complex Evidence-Based Question Answering. Mingxu Tao, Dongyan Zhao, Yansong Feng, arXiv:2402.16313[cs.CL2024</p>
<p>Introducing Meta Llama 3: The most capable openly available LLM to date. 2024</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, arXiv:2307.09288Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. 2023arXiv preprint</p>
<p>Tu Vu, Kalpesh Krishna, Salaheddin Alzubi, Chris Tar, Manaal Faruqui, Yun-Hsuan Sung, arXiv:2407.10817Foundational autoraters: Taming large language models for better automatic evaluation. 2024. 2024arXiv preprint</p>
<p>Jiaan Wang, Yunlong Liang, Fandong Meng, Zengkui Sun, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, Jie Zhou, arXiv:2303.04048Is chatgpt a good nlg evaluator? a preliminary study. 2023. 2023arXiv preprint</p>
<p>Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, Zhifang Sui, arXiv:2305.17926[cs.CLLarge Language Models are not Fair Evaluators. 2023</p>
<p>Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, Wei Ye, Shikun Zhang, Yue Zhang, 10.48550/arXiv.2306.05087arXiv:2306.05087arXiv:2306.05087PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization. 2024. May 2024</p>
<p>A survey on LLM-generated text detection: Necessity, methods, and future directions. Junchao Wu, Shu Yang, Runzhe Zhan, Yulin Yuan, Lidia Sam Chao, Derek Fai Wong, Computational Linguistics. 2025. 2025</p>
<p>A critical evaluation of evaluations for long-form question answering. Fangyuan Xu, Yixiao Song, Mohit Iyyer, Eunsol Choi, arXiv:2305.182012023. 2023arXiv preprint</p>
<p>Qinyuan Ye, Maxamed Axmed, Reid Pryzant, Fereshte Khani, arXiv:2311.05661Prompt engineering a prompt engineer. 2023. 2023arXiv preprint</p>
<p>FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets. Seonghyeon Ye, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, Seungone Kim, Yongrae Jo, James Thorne, Juho Kim, Minjoon Seo, 10.48550/arXiv.2307.10928arXiv:2307.10928arXiv:2307.109282023. Oct. 2023</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, arXiv:1904.09675Bertscore: Evaluating text generation with bert. 2019. 2019arXiv preprint</p>
<p>Yue Zhang, Ming Zhang, Haipeng Yuan, Shichun Liu, Yongyao Shi, Tao Gui, Qi Zhang, Xuanjing Huang, 10.48550/arXiv.2312.07398arXiv:2312.07398arXiv:2312.07398LLMEval: A Preliminary Study on How to Evaluate Large Language Models. 2023. Dec. 2023</p>
<p>MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance. Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M Meyer, Steffen Eger, 10.18653/v1/D19-1053Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. Kentaro Inui, Jing Jiang, Vincent Ng, Xiaojun Wan, the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingHong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Advances in Neural Information Processing Systems. 362024. 2024</p>            </div>
        </div>

    </div>
</body>
</html>
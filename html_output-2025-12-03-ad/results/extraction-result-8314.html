<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8314 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8314</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8314</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-152.html">extraction-schema-152</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-277595863</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.03151v1.pdf" target="_blank">Why Reasoning Matters? A Survey of Advancements in Multimodal Reasoning (v1)</a></p>
                <p><strong>Paper Abstract:</strong> Reasoning is central to human intelligence, enabling structured problem-solving across diverse tasks. Recent advances in large language models (LLMs) have greatly enhanced their reasoning abilities in arithmetic, commonsense, and symbolic domains. However, effectively extending these capabilities into multimodal contexts-where models must integrate both visual and textual inputs-continues to be a significant challenge. Multimodal reasoning introduces complexities, such as handling conflicting information across modalities, which require models to adopt advanced interpretative strategies. Addressing these challenges involves not only sophisticated algorithms but also robust methodologies for evaluating reasoning accuracy and coherence. This paper offers a concise yet insightful overview of reasoning techniques in both textual and multimodal LLMs. Through a thorough and up-to-date comparison, we clearly formulate core reasoning challenges and opportunities, highlighting practical methods for post-training optimization and test-time inference. Our work provides valuable insights and guidance, bridging theoretical frameworks and practical implementations, and sets clear directions for future research.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8314.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8314.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that elicits step-by-step natural-language intermediate reasoning steps from LLMs, approximating the solution trajectory by sampling a single reasoning path τ and using it to produce the final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Large Language Models / Multimodal LLMs (e.g., LLaMA series, LLaVA variants referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large transformer-based language models (LLMs) and multimodal LLMs (MLLMs) referenced in the survey (LLaMA series, LLaVA, GPT-style models); sizes are not specified in this survey but described as 'very large' when noting CoT efficacy.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['chain-of-thought (CoT)', 'stepwise decomposition', 'self-explanation/metacognitive prompting']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>CoT prompts the model to decompose complex questions into intermediate steps, generating a single trajectory τ = (Step1...Stept) in natural language and then conditioning the final answer on that trajectory; implemented as prompting or instruction-tuning to elicit intermediate rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Survey contrasts CoT's single-trajectory sampling (P(A|τ,Q) where τ∼P(τ|Q)) with multi-trajectory methods (Tree-of-Thought/MCTS) that generate and evaluate multiple τ_i; CoT is the baseline single-path method used in several multimodal extensions (e.g., LLaVA-CoT, Visual CoT).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Arithmetic, commonsense and symbolic reasoning tasks in text; VisualQA and multimodal reasoning benchmarks when extended to MLLMs (e.g., Visual CoT, LLaVA-CoT, various VQA/VideoQA datasets mentioned).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Paper-level summary: CoT notably improves performance in very large models and when applied in multimodal settings (cited improvements for LLaMA-series using filtered self-generated reasoning paths), but no single unified numeric benchmark score is reported in this survey for CoT alone.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>CoT reduces uncertainty by conditioning on intermediate steps, improves interpretability, encourages System-2-like deliberative reasoning, and helps with compositionality and error-correction; however, sampling only a single τ risks missing correct trajectories when the solution space is multimodal.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>CoT is effective at improving reasoning when a coherent single trajectory suffices, but it can be limited by single-path sampling; generating and selecting among multiple trajectories (Tree-of-Thought/MCTS) can further improve robustness and accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Why Reasoning Matters? A Survey of Advancements in Multimodal Reasoning (v1)', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8314.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8314.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ToT / MCTS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree-of-Thought / Monte Carlo Tree Search</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Multi-path search approaches that generate multiple candidate reasoning trajectories and evaluate them (e.g., via MCTS) to select the most promising path, reducing chance of missing correct solutions compared to single-path CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs/MLLMs augmented with search (e.g., models in CoM-CTS, MCTS-based temporal reasoning studies)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Language models (LLMs/MLLMs) used as the generative policy inside a tree-search framework (MCTS) to propose and expand nodes (partial reasoning states), with external evaluators or value models scoring branches.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Tree-of-Thought (ToT)', 'Monte Carlo Tree Search (MCTS)', 'multi-trajectory search and ranking']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>ToT/MCTS construct a tree of partial reasoning states and explore multiple trajectories {τ1,...,τn}; trajectories are expanded and evaluated (using reward models, value estimation, or preference models) to select a final trajectory; used for temporal/video reasoning and complex multi-step tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Survey contrasts ToT/MCTS (multi-trajectory generation + selection) with CoT (single sampled τ). It cites applications where MCTS reduced hallucinations (temporal reasoning) and where CoM-CTS unifies multiple MLLMs in a collaborative tree search to address model bias.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Temporal reasoning / VideoQA / complex multi-step multimodal tasks (e.g., TemporalVQA, VideoVista, STAR, other temporal/spatial benchmarks referenced).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Survey-level outcomes: MCTS-style methods are reported to reduce hallucinations in time-dependent data (Imam et al., 2025), improve temporal reasoning, and CoM-CTS improved accuracy and efficiency versus single-model baselines; specific numeric scores are not given in the survey text.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Multi-path search increases robustness to local errors in single reasoning chains, helps manage sequential uncertainty, and can mitigate model bias by exploring alternative reasoning hypotheses; comes with increased compute cost and complexity of path evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Generating and evaluating multiple reasoning trajectories (ToT/MCTS) improves solution discovery and reduces hallucinations/temporal errors relative to single-path CoT, especially for sequential and temporal multimodal tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Why Reasoning Matters? A Survey of Advancements in Multimodal Reasoning (v1)', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8314.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8314.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Best-of-N / Ranking</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Best-of-N sampling and ranking-based selection (including reward-guided Best-of-N)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Test-time strategy sampling multiple candidate outputs (diverse reasoning attempts) and selecting the highest-utility output using reward models, ranking, or preference signals (Best-of-N guided by reward).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various LLMs/MLLMs (e.g., IXC-2.5, models using ORM/PRM ranking) referenced in survey</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLMs used with test-time sampling strategies to produce N candidate reasoning trajectories/answers; selection is done via reward models, ranking functions, or preference-based critics.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Best-of-N sampling', 'ranking-based selection', 'reward-model guided sampling', 'self-consistency (implicit via multiple samples and aggregation)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Multiple outputs are sampled (N candidates) at inference; candidates are scored by reward models (absolute rewards, pairwise preference models, ranking-based scores) and the top-ranked is selected; variants include Best-of-N guided by reward (IXC-2.5), ORM/PRM final-step ranking, and token-level reward methods (TLDR).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Survey mentions setups that balance sampling diversity with ranking (e.g., IXC-2.5: Best-of-N guided by reward models; ORM/PRM final-step evaluation vs no ranking). The comparison is Best-of-N + ranking vs single-sample inference.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>General multimodal generation and VQA tasks; image generation fidelity and captioning; GenEval and VL-RewardBench referenced for evaluation of reward-guided selection.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Survey statements: Best-of-N guided by reward yields better trade-offs between performance and response length (IXC-2.5); ORM final-step evaluation significantly enhances image generation; PARM++ and ranking-oriented methods improve prompt fidelity — no uniform numeric scores provided in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Sampling diverse candidates then selecting the highest-utility one improves robustness and can reduce hallucination, but requires reliable reward models/critics; there is a compute vs quality tradeoff.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Combining diverse sampling with strong ranking/selection (reward models, preference learning) improves final-answer quality beyond single-sample CoT, contingent on reliable evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Why Reasoning Matters? A Survey of Advancements in Multimodal Reasoning (v1)', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8314.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8314.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflection / Iterative Refinement (PARM++ / VOLCANO)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflection-based iterative self-assessment and refinement frameworks (e.g., PARM++, VOLCANO)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mechanisms that let models generate outputs, self-evaluate (reflect), and iteratively revise answers over multiple cycles to correct mistakes, reduce hallucinations, and improve alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MLLMs referenced with reflection loops (e.g., PARM++, VOLCANO, LOOKBACK examples)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multimodal LLMs augmented with internal reflection/self-assessment modules that trigger re-generation or targeted corrections across several passes; implementations vary in number of reflection cycles and the form of self-critique.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['iterative reflection', 'self-assessment loops', 'atomic verification and revision']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>The model produces an initial answer, uses internal checks or learned verifiers to detect misalignments or hallucinations, and then revises the output — repeated for multiple reflection cycles (PARM++ reports up to three cycles examined). Variants include image-focused self-assessment (VOLCANO) and explicit atomic verification against visual evidence (LOOKBACK).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Survey reports ablations of reflection cycles versus no-reflection baselines (PARM++ shows iterative improvement across cycles; VISCO compares human critiques to model critiques and finds human critiques more effective for corrections).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Multimodal generation alignment tasks, GenEval, VISCO (critique and correction), general VQA/image-captioning where hallucination is measured.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Survey notes: PARM++ 'significantly improving test-time GenEval scores up to three reflection cycles' (no numerical scores given); VISCO shows human critiques correct 76% of errors compared to model critiques, indicating current limitations of automated reflection.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Iterative reflection improves fidelity and reduces hallucination, enables self-correction and error localization, but model self-critiques still lag behind human critiques; effectiveness depends on the quality of the verifier and the stopping criterion.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Iterative self-reflection/refinement provides measurable improvements in multimodal reasoning and hallucination mitigation, but its benefits rely on reliable internal evaluators and are not yet at human critique levels.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Why Reasoning Matters? A Survey of Advancements in Multimodal Reasoning (v1)', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8314.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8314.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Preference-based / DPO (LLaVA-Critic, CLIP-DPO)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Preference-based alignment and Direct Preference Optimization (DPO) with critic models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Training or fine-tuning approaches that use pairwise preferences or critic models to align model outputs to higher-quality trajectories or responses, improving selection/evaluation of reasoning paths.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MLLMs finetuned with preference signals (LLaVA-Critic, CLIP-DPO examples)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multimodal LLMs combined with learned critic/reward models and DPO-style optimization that directly incorporates pairwise preference signals to prefer higher-utility trajectories; critics may be learned from internal or external data.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['preference-based alignment', 'Direct Preference Optimization (DPO)', 'critic-guided ranking']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Use pairwise preference labels or critic scores (from CLIP or learned reward models) to train models to rank and generate preferred trajectories; can be applied post-training (LLaVA-Critic uses DPO with internal data) or used as rerankers at test-time (CLIP-DPO uses CLIP as a preference signal).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Survey notes comparisons of models with vs without DPO/preference alignment (e.g., LLaVA-Critic vs baseline), and usage of CLIP as reward signals to correct hallucinations (CLIP-DPO). Setup typically re-ranks or fine-tunes models using preference data.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Multimodal reasoning tasks and captioning; benchmarks measuring hallucination and alignment such as VL-RewardBench, GenEval, image-captioning correction tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Survey-level claims: LLaVA-Critic 'boosts multimodal reasoning effectiveness with minimal modality-specific fine-tuning'; CLIP-DPO helps fix hallucinations — specific numeric improvements are not included in the survey summary.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Preference-based alignment improves selection among multiple candidate trajectories, reduces hallucination, and can generalize reward signals without extensive modality-specific labeled data, but depends on quality of preference or critic data.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Direct preference optimization and critic-guided ranking are effective selection mechanisms that complement diverse generation strategies, improving final-answer quality and reducing hallucinations when reliable preference signals exist.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Why Reasoning Matters? A Survey of Advancements in Multimodal Reasoning (v1)', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8314.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8314.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoM-CTS / Mulberry</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Collaborative Monte Carlo Tree Search (CoM-CTS) / Collective MCTS (Mulberry)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Methods that combine multiple MLLMs or multiple agents/models within a collaborative/tree-search framework to diversify reasoning trajectories and counteract single-model biases during search.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Ensembles of MLLMs / multi-agent MCTS systems (CoM-CTS, Mulberry examples)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Frameworks that orchestrate several multimodal language models in a shared tree-search (MCTS) or collective search, aggregating proposals and evaluations from multiple models to form a consensus selection policy.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['collaborative tree search', 'collective Monte Carlo Tree Search', 'ensemble-based trajectory generation and selection']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Multiple MLLMs propose and/or evaluate branches in a shared search tree; their diverse perspectives create a broader set of candidate trajectories that are jointly scored and pruned (CoM-CTS unifies multiple MLLMs; Mulberry uses collective MCTS with o1-like reasoning and reflection).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Survey describes setups where multi-model collaborative tree search is compared against single-model search or static baselines, showing gains in accuracy and robustness (CoM-CTS, Mulberry).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Complex multimodal and temporal reasoning tasks, multi-step planning and video QA where model bias and search completeness are critical.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Survey-level summary: collaborative/collective MCTS methods reported improved accuracy and efficiency versus single-model baselines; concrete numeric benchmarks are not provided in the survey text.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Ensembling models in the search process reduces individual model bias, increases coverage of plausible reasoning trajectories, and leads to more robust decisions, at the cost of increased compute and coordination complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Unifying multiple MLLMs in a collaborative search meaningfully addresses model bias and improves reasoning accuracy/efficiency compared to single-model search.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Why Reasoning Matters? A Survey of Advancements in Multimodal Reasoning (v1)', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8314.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8314.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Visual CoT / LLaVA-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Visual Chain-of-Thought (visual CoT) and LLaVA-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Extensions of CoT to multimodal inputs that explicitly incorporate visual reasoning steps (e.g., bounding box predictions, cropping actions) into the chain-of-thought to improve visual question answering and grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multimodal LLMs with visual CoT instantiations (e.g., LLaVA-CoT, Visual CoT datasets/models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vision-Language Models (VLMs/MLLMs) augmented to generate stepwise visual reasoning chains that may include spatial operations (bounding box predictions, cropping) and multi-step verification aligned with images/video.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['visual chain-of-thought', 'bounding-box/region-based verification', 'iterative visual cropping and re-evaluation']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Models emit intermediate visual reasoning steps (e.g., identify region, propose relation, justify using image evidence) and may perform iterative cropping/verification (VisCoT example) to ground answers; LLaVA-CoT applies CoT to vision-language contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Survey references experiments comparing visual CoT-enabled models to non-CoT multimodal baselines (Visual CoT, LLaVA-CoT); also discusses combining visual CoT with iterative verification mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>VisualQA, grounded spatial reasoning datasets, Visual CoT benchmark datasets, and other VQA/VideoQA multimodal benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Survey notes that integrating CoT across modalities 'can significantly enhance performance of MLLMs' (cites Yao et al., Zhang et al., Bi et al. references), but specific numeric improvements are not consolidated in the survey text.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Visual CoT helps disambiguate cross-modal conflicts, improves grounding and spatial inference, and supports compositional multi-step solutions; still vulnerable to hallucinated self-reflections unless coupled with strong verification.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Applying CoT to multimodal contexts improves compositional understanding and grounding, especially when combined with verification/reflection; multimodal CoT is a promising route to extend textual reasoning benefits into visual domains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Why Reasoning Matters? A Survey of Advancements in Multimodal Reasoning (v1)', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>STaR: Bootstrapping Reasoning With Reasoning <em>(Rating: 2)</em></li>
                <li>CoM-CTS (Yao et al., 2024) <em>(Rating: 2)</em></li>
                <li>LLaVA-Critic: Learning to Evaluate Multimodal Models <em>(Rating: 2)</em></li>
                <li>PARM++ (Guo et al., 2025) <em>(Rating: 2)</em></li>
                <li>Visual CoT: Advancing Multi-Modal Language Models with a Comprehensive Dataset and Benchmark for Chain-of-Thought Reasoning <em>(Rating: 2)</em></li>
                <li>VISCO: Benchmarking Fine-Grained Critique and Correction Towards Self-Improvement in Visual Reasoning <em>(Rating: 2)</em></li>
                <li>Mulberry: Empowering MLLM with o1-like Reasoning and Reflection via Collective Monte Carlo Tree Search <em>(Rating: 2)</em></li>
                <li>R-CoT: Reverse Chain-of-Thought Problem Generation for Geometric Reasoning in Large Multimodal Models <em>(Rating: 1)</em></li>
                <li>TLDR: Token-Level Detective Reward Model for Large Vision Language Models <em>(Rating: 1)</em></li>
                <li>IXC-2.5 (Best-of-N guided by reward models) (Zang et al., 2025) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8314",
    "paper_id": "paper-277595863",
    "extraction_schema_id": "extraction-schema-152",
    "extracted_data": [
        {
            "name_short": "CoT",
            "name_full": "Chain-of-Thought prompting",
            "brief_description": "A prompting technique that elicits step-by-step natural-language intermediate reasoning steps from LLMs, approximating the solution trajectory by sampling a single reasoning path τ and using it to produce the final answer.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Large Language Models / Multimodal LLMs (e.g., LLaMA series, LLaVA variants referenced)",
            "model_description": "Large transformer-based language models (LLMs) and multimodal LLMs (MLLMs) referenced in the survey (LLaMA series, LLaVA, GPT-style models); sizes are not specified in this survey but described as 'very large' when noting CoT efficacy.",
            "reasoning_methods": [
                "chain-of-thought (CoT)",
                "stepwise decomposition",
                "self-explanation/metacognitive prompting"
            ],
            "reasoning_methods_description": "CoT prompts the model to decompose complex questions into intermediate steps, generating a single trajectory τ = (Step1...Stept) in natural language and then conditioning the final answer on that trajectory; implemented as prompting or instruction-tuning to elicit intermediate rationales.",
            "reasoning_diversity": "similar",
            "reasoning_diversity_experimental_setup": "Survey contrasts CoT's single-trajectory sampling (P(A|τ,Q) where τ∼P(τ|Q)) with multi-trajectory methods (Tree-of-Thought/MCTS) that generate and evaluate multiple τ_i; CoT is the baseline single-path method used in several multimodal extensions (e.g., LLaVA-CoT, Visual CoT).",
            "task_or_benchmark": "Arithmetic, commonsense and symbolic reasoning tasks in text; VisualQA and multimodal reasoning benchmarks when extended to MLLMs (e.g., Visual CoT, LLaVA-CoT, various VQA/VideoQA datasets mentioned).",
            "performance_results": "Paper-level summary: CoT notably improves performance in very large models and when applied in multimodal settings (cited improvements for LLaMA-series using filtered self-generated reasoning paths), but no single unified numeric benchmark score is reported in this survey for CoT alone.",
            "qualitative_findings": "CoT reduces uncertainty by conditioning on intermediate steps, improves interpretability, encourages System-2-like deliberative reasoning, and helps with compositionality and error-correction; however, sampling only a single τ risks missing correct trajectories when the solution space is multimodal.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "CoT is effective at improving reasoning when a coherent single trajectory suffices, but it can be limited by single-path sampling; generating and selecting among multiple trajectories (Tree-of-Thought/MCTS) can further improve robustness and accuracy.",
            "uuid": "e8314.0",
            "source_info": {
                "paper_title": "Why Reasoning Matters? A Survey of Advancements in Multimodal Reasoning (v1)",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "ToT / MCTS",
            "name_full": "Tree-of-Thought / Monte Carlo Tree Search",
            "brief_description": "Multi-path search approaches that generate multiple candidate reasoning trajectories and evaluate them (e.g., via MCTS) to select the most promising path, reducing chance of missing correct solutions compared to single-path CoT.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "LLMs/MLLMs augmented with search (e.g., models in CoM-CTS, MCTS-based temporal reasoning studies)",
            "model_description": "Language models (LLMs/MLLMs) used as the generative policy inside a tree-search framework (MCTS) to propose and expand nodes (partial reasoning states), with external evaluators or value models scoring branches.",
            "reasoning_methods": [
                "Tree-of-Thought (ToT)",
                "Monte Carlo Tree Search (MCTS)",
                "multi-trajectory search and ranking"
            ],
            "reasoning_methods_description": "ToT/MCTS construct a tree of partial reasoning states and explore multiple trajectories {τ1,...,τn}; trajectories are expanded and evaluated (using reward models, value estimation, or preference models) to select a final trajectory; used for temporal/video reasoning and complex multi-step tasks.",
            "reasoning_diversity": "diverse",
            "reasoning_diversity_experimental_setup": "Survey contrasts ToT/MCTS (multi-trajectory generation + selection) with CoT (single sampled τ). It cites applications where MCTS reduced hallucinations (temporal reasoning) and where CoM-CTS unifies multiple MLLMs in a collaborative tree search to address model bias.",
            "task_or_benchmark": "Temporal reasoning / VideoQA / complex multi-step multimodal tasks (e.g., TemporalVQA, VideoVista, STAR, other temporal/spatial benchmarks referenced).",
            "performance_results": "Survey-level outcomes: MCTS-style methods are reported to reduce hallucinations in time-dependent data (Imam et al., 2025), improve temporal reasoning, and CoM-CTS improved accuracy and efficiency versus single-model baselines; specific numeric scores are not given in the survey text.",
            "qualitative_findings": "Multi-path search increases robustness to local errors in single reasoning chains, helps manage sequential uncertainty, and can mitigate model bias by exploring alternative reasoning hypotheses; comes with increased compute cost and complexity of path evaluation.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Generating and evaluating multiple reasoning trajectories (ToT/MCTS) improves solution discovery and reduces hallucinations/temporal errors relative to single-path CoT, especially for sequential and temporal multimodal tasks.",
            "uuid": "e8314.1",
            "source_info": {
                "paper_title": "Why Reasoning Matters? A Survey of Advancements in Multimodal Reasoning (v1)",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Best-of-N / Ranking",
            "name_full": "Best-of-N sampling and ranking-based selection (including reward-guided Best-of-N)",
            "brief_description": "Test-time strategy sampling multiple candidate outputs (diverse reasoning attempts) and selecting the highest-utility output using reward models, ranking, or preference signals (Best-of-N guided by reward).",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Various LLMs/MLLMs (e.g., IXC-2.5, models using ORM/PRM ranking) referenced in survey",
            "model_description": "LLMs used with test-time sampling strategies to produce N candidate reasoning trajectories/answers; selection is done via reward models, ranking functions, or preference-based critics.",
            "reasoning_methods": [
                "Best-of-N sampling",
                "ranking-based selection",
                "reward-model guided sampling",
                "self-consistency (implicit via multiple samples and aggregation)"
            ],
            "reasoning_methods_description": "Multiple outputs are sampled (N candidates) at inference; candidates are scored by reward models (absolute rewards, pairwise preference models, ranking-based scores) and the top-ranked is selected; variants include Best-of-N guided by reward (IXC-2.5), ORM/PRM final-step ranking, and token-level reward methods (TLDR).",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Survey mentions setups that balance sampling diversity with ranking (e.g., IXC-2.5: Best-of-N guided by reward models; ORM/PRM final-step evaluation vs no ranking). The comparison is Best-of-N + ranking vs single-sample inference.",
            "task_or_benchmark": "General multimodal generation and VQA tasks; image generation fidelity and captioning; GenEval and VL-RewardBench referenced for evaluation of reward-guided selection.",
            "performance_results": "Survey statements: Best-of-N guided by reward yields better trade-offs between performance and response length (IXC-2.5); ORM final-step evaluation significantly enhances image generation; PARM++ and ranking-oriented methods improve prompt fidelity — no uniform numeric scores provided in the survey.",
            "qualitative_findings": "Sampling diverse candidates then selecting the highest-utility one improves robustness and can reduce hallucination, but requires reliable reward models/critics; there is a compute vs quality tradeoff.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Combining diverse sampling with strong ranking/selection (reward models, preference learning) improves final-answer quality beyond single-sample CoT, contingent on reliable evaluators.",
            "uuid": "e8314.2",
            "source_info": {
                "paper_title": "Why Reasoning Matters? A Survey of Advancements in Multimodal Reasoning (v1)",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Reflection / Iterative Refinement (PARM++ / VOLCANO)",
            "name_full": "Reflection-based iterative self-assessment and refinement frameworks (e.g., PARM++, VOLCANO)",
            "brief_description": "Mechanisms that let models generate outputs, self-evaluate (reflect), and iteratively revise answers over multiple cycles to correct mistakes, reduce hallucinations, and improve alignment.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "MLLMs referenced with reflection loops (e.g., PARM++, VOLCANO, LOOKBACK examples)",
            "model_description": "Multimodal LLMs augmented with internal reflection/self-assessment modules that trigger re-generation or targeted corrections across several passes; implementations vary in number of reflection cycles and the form of self-critique.",
            "reasoning_methods": [
                "iterative reflection",
                "self-assessment loops",
                "atomic verification and revision"
            ],
            "reasoning_methods_description": "The model produces an initial answer, uses internal checks or learned verifiers to detect misalignments or hallucinations, and then revises the output — repeated for multiple reflection cycles (PARM++ reports up to three cycles examined). Variants include image-focused self-assessment (VOLCANO) and explicit atomic verification against visual evidence (LOOKBACK).",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Survey reports ablations of reflection cycles versus no-reflection baselines (PARM++ shows iterative improvement across cycles; VISCO compares human critiques to model critiques and finds human critiques more effective for corrections).",
            "task_or_benchmark": "Multimodal generation alignment tasks, GenEval, VISCO (critique and correction), general VQA/image-captioning where hallucination is measured.",
            "performance_results": "Survey notes: PARM++ 'significantly improving test-time GenEval scores up to three reflection cycles' (no numerical scores given); VISCO shows human critiques correct 76% of errors compared to model critiques, indicating current limitations of automated reflection.",
            "qualitative_findings": "Iterative reflection improves fidelity and reduces hallucination, enables self-correction and error localization, but model self-critiques still lag behind human critiques; effectiveness depends on the quality of the verifier and the stopping criterion.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Iterative self-reflection/refinement provides measurable improvements in multimodal reasoning and hallucination mitigation, but its benefits rely on reliable internal evaluators and are not yet at human critique levels.",
            "uuid": "e8314.3",
            "source_info": {
                "paper_title": "Why Reasoning Matters? A Survey of Advancements in Multimodal Reasoning (v1)",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Preference-based / DPO (LLaVA-Critic, CLIP-DPO)",
            "name_full": "Preference-based alignment and Direct Preference Optimization (DPO) with critic models",
            "brief_description": "Training or fine-tuning approaches that use pairwise preferences or critic models to align model outputs to higher-quality trajectories or responses, improving selection/evaluation of reasoning paths.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "MLLMs finetuned with preference signals (LLaVA-Critic, CLIP-DPO examples)",
            "model_description": "Multimodal LLMs combined with learned critic/reward models and DPO-style optimization that directly incorporates pairwise preference signals to prefer higher-utility trajectories; critics may be learned from internal or external data.",
            "reasoning_methods": [
                "preference-based alignment",
                "Direct Preference Optimization (DPO)",
                "critic-guided ranking"
            ],
            "reasoning_methods_description": "Use pairwise preference labels or critic scores (from CLIP or learned reward models) to train models to rank and generate preferred trajectories; can be applied post-training (LLaVA-Critic uses DPO with internal data) or used as rerankers at test-time (CLIP-DPO uses CLIP as a preference signal).",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Survey notes comparisons of models with vs without DPO/preference alignment (e.g., LLaVA-Critic vs baseline), and usage of CLIP as reward signals to correct hallucinations (CLIP-DPO). Setup typically re-ranks or fine-tunes models using preference data.",
            "task_or_benchmark": "Multimodal reasoning tasks and captioning; benchmarks measuring hallucination and alignment such as VL-RewardBench, GenEval, image-captioning correction tasks.",
            "performance_results": "Survey-level claims: LLaVA-Critic 'boosts multimodal reasoning effectiveness with minimal modality-specific fine-tuning'; CLIP-DPO helps fix hallucinations — specific numeric improvements are not included in the survey summary.",
            "qualitative_findings": "Preference-based alignment improves selection among multiple candidate trajectories, reduces hallucination, and can generalize reward signals without extensive modality-specific labeled data, but depends on quality of preference or critic data.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Direct preference optimization and critic-guided ranking are effective selection mechanisms that complement diverse generation strategies, improving final-answer quality and reducing hallucinations when reliable preference signals exist.",
            "uuid": "e8314.4",
            "source_info": {
                "paper_title": "Why Reasoning Matters? A Survey of Advancements in Multimodal Reasoning (v1)",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "CoM-CTS / Mulberry",
            "name_full": "Collaborative Monte Carlo Tree Search (CoM-CTS) / Collective MCTS (Mulberry)",
            "brief_description": "Methods that combine multiple MLLMs or multiple agents/models within a collaborative/tree-search framework to diversify reasoning trajectories and counteract single-model biases during search.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Ensembles of MLLMs / multi-agent MCTS systems (CoM-CTS, Mulberry examples)",
            "model_description": "Frameworks that orchestrate several multimodal language models in a shared tree-search (MCTS) or collective search, aggregating proposals and evaluations from multiple models to form a consensus selection policy.",
            "reasoning_methods": [
                "collaborative tree search",
                "collective Monte Carlo Tree Search",
                "ensemble-based trajectory generation and selection"
            ],
            "reasoning_methods_description": "Multiple MLLMs propose and/or evaluate branches in a shared search tree; their diverse perspectives create a broader set of candidate trajectories that are jointly scored and pruned (CoM-CTS unifies multiple MLLMs; Mulberry uses collective MCTS with o1-like reasoning and reflection).",
            "reasoning_diversity": "diverse",
            "reasoning_diversity_experimental_setup": "Survey describes setups where multi-model collaborative tree search is compared against single-model search or static baselines, showing gains in accuracy and robustness (CoM-CTS, Mulberry).",
            "task_or_benchmark": "Complex multimodal and temporal reasoning tasks, multi-step planning and video QA where model bias and search completeness are critical.",
            "performance_results": "Survey-level summary: collaborative/collective MCTS methods reported improved accuracy and efficiency versus single-model baselines; concrete numeric benchmarks are not provided in the survey text.",
            "qualitative_findings": "Ensembling models in the search process reduces individual model bias, increases coverage of plausible reasoning trajectories, and leads to more robust decisions, at the cost of increased compute and coordination complexity.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Unifying multiple MLLMs in a collaborative search meaningfully addresses model bias and improves reasoning accuracy/efficiency compared to single-model search.",
            "uuid": "e8314.5",
            "source_info": {
                "paper_title": "Why Reasoning Matters? A Survey of Advancements in Multimodal Reasoning (v1)",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Visual CoT / LLaVA-CoT",
            "name_full": "Visual Chain-of-Thought (visual CoT) and LLaVA-CoT",
            "brief_description": "Extensions of CoT to multimodal inputs that explicitly incorporate visual reasoning steps (e.g., bounding box predictions, cropping actions) into the chain-of-thought to improve visual question answering and grounding.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Multimodal LLMs with visual CoT instantiations (e.g., LLaVA-CoT, Visual CoT datasets/models)",
            "model_description": "Vision-Language Models (VLMs/MLLMs) augmented to generate stepwise visual reasoning chains that may include spatial operations (bounding box predictions, cropping) and multi-step verification aligned with images/video.",
            "reasoning_methods": [
                "visual chain-of-thought",
                "bounding-box/region-based verification",
                "iterative visual cropping and re-evaluation"
            ],
            "reasoning_methods_description": "Models emit intermediate visual reasoning steps (e.g., identify region, propose relation, justify using image evidence) and may perform iterative cropping/verification (VisCoT example) to ground answers; LLaVA-CoT applies CoT to vision-language contexts.",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Survey references experiments comparing visual CoT-enabled models to non-CoT multimodal baselines (Visual CoT, LLaVA-CoT); also discusses combining visual CoT with iterative verification mechanisms.",
            "task_or_benchmark": "VisualQA, grounded spatial reasoning datasets, Visual CoT benchmark datasets, and other VQA/VideoQA multimodal benchmarks.",
            "performance_results": "Survey notes that integrating CoT across modalities 'can significantly enhance performance of MLLMs' (cites Yao et al., Zhang et al., Bi et al. references), but specific numeric improvements are not consolidated in the survey text.",
            "qualitative_findings": "Visual CoT helps disambiguate cross-modal conflicts, improves grounding and spatial inference, and supports compositional multi-step solutions; still vulnerable to hallucinated self-reflections unless coupled with strong verification.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Applying CoT to multimodal contexts improves compositional understanding and grounding, especially when combined with verification/reflection; multimodal CoT is a promising route to extend textual reasoning benefits into visual domains.",
            "uuid": "e8314.6",
            "source_info": {
                "paper_title": "Why Reasoning Matters? A Survey of Advancements in Multimodal Reasoning (v1)",
                "publication_date_yy_mm": "2025-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "STaR: Bootstrapping Reasoning With Reasoning",
            "rating": 2,
            "sanitized_title": "star_bootstrapping_reasoning_with_reasoning"
        },
        {
            "paper_title": "CoM-CTS (Yao et al., 2024)",
            "rating": 2,
            "sanitized_title": "comcts_yao_et_al_2024"
        },
        {
            "paper_title": "LLaVA-Critic: Learning to Evaluate Multimodal Models",
            "rating": 2,
            "sanitized_title": "llavacritic_learning_to_evaluate_multimodal_models"
        },
        {
            "paper_title": "PARM++ (Guo et al., 2025)",
            "rating": 2,
            "sanitized_title": "parm_guo_et_al_2025"
        },
        {
            "paper_title": "Visual CoT: Advancing Multi-Modal Language Models with a Comprehensive Dataset and Benchmark for Chain-of-Thought Reasoning",
            "rating": 2,
            "sanitized_title": "visual_cot_advancing_multimodal_language_models_with_a_comprehensive_dataset_and_benchmark_for_chainofthought_reasoning"
        },
        {
            "paper_title": "VISCO: Benchmarking Fine-Grained Critique and Correction Towards Self-Improvement in Visual Reasoning",
            "rating": 2,
            "sanitized_title": "visco_benchmarking_finegrained_critique_and_correction_towards_selfimprovement_in_visual_reasoning"
        },
        {
            "paper_title": "Mulberry: Empowering MLLM with o1-like Reasoning and Reflection via Collective Monte Carlo Tree Search",
            "rating": 2,
            "sanitized_title": "mulberry_empowering_mllm_with_o1like_reasoning_and_reflection_via_collective_monte_carlo_tree_search"
        },
        {
            "paper_title": "R-CoT: Reverse Chain-of-Thought Problem Generation for Geometric Reasoning in Large Multimodal Models",
            "rating": 1,
            "sanitized_title": "rcot_reverse_chainofthought_problem_generation_for_geometric_reasoning_in_large_multimodal_models"
        },
        {
            "paper_title": "TLDR: Token-Level Detective Reward Model for Large Vision Language Models",
            "rating": 1,
            "sanitized_title": "tldr_tokenlevel_detective_reward_model_for_large_vision_language_models"
        },
        {
            "paper_title": "IXC-2.5 (Best-of-N guided by reward models) (Zang et al., 2025)",
            "rating": 1,
            "sanitized_title": "ixc25_bestofn_guided_by_reward_models_zang_et_al_2025"
        }
    ],
    "cost": 0.020281,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Why Reasoning Matters? A Survey of Advancements in Multimodal Reasoning (v1)
4 Apr 2025</p>
<p>Jing Bi jing.bi@rochester.edu 
University of Rochester</p>
<p>Susan Liang sliang22@rochester.edu 
University of Rochester</p>
<p>Xiaofei Zhou 
University of Rochester</p>
<p>Pinxin Liu 
University of Rochester</p>
<p>Junjia Guo 
University of Rochester</p>
<p>Yunlong Tang yunlong.tang@rochester.edu 
University of Rochester</p>
<p>Luchuan Song 
University of Rochester</p>
<p>Chao Huang 
University of Rochester</p>
<p>Guangyu Sun guangyu@ucf.edu 
University of Central Florida</p>
<p>Jinxi He 
University of Rochester</p>
<p>Jiarui Wu 
University of Rochester</p>
<p>Shu Yang 
University of Rochester</p>
<p>Daoan Zhang daoan.zhang@rochester.edu 
University of Rochester</p>
<p>Chen Chen chen.chen@crcv.ucf.edu 
University of Central Florida</p>
<p>Lianggong Bruce Wen 
Corning Inc</p>
<p>Zhang Liu 
Corning Inc</p>
<p>Jiebo Luo 
University of Rochester</p>
<p>Chenliang Xu chenliang.xu@rochester.edu 
University of Rochester</p>
<p>Why Reasoning Matters? A Survey of Advancements in Multimodal Reasoning (v1)
4 Apr 20254E5582828FDB55895DE6A934C19EEE79arXiv:2504.03151v1[cs.CL]
Reasoning is central to human intelligence, enabling structured problemsolving across diverse tasks.Recent advances in large language models (LLMs) have greatly enhanced their reasoning abilities in arithmetic, commonsense, and symbolic domains.However, effectively extending these capabilities into multimodal contexts-where models must integrate both visual and textual inputs-continues to be a significant challenge.Multimodal reasoning introduces complexities, such as handling conflicting information across modalities, which require models to adopt advanced interpretative strategies.Addressing these challenges involves not only sophisticated algorithms but also robust methodologies for evaluating reasoning accuracy and coherence.This paper offers a concise yet insightful overview of reasoning techniques in both textual and multimodal LLMs.Through a thorough and up-to-date comparison, we clearly formulate core reasoning challenges and opportunities, highlighting practical methods for post-training optimization and test-time inference.Our work provides valuable insights and guidance, bridging theoretical frameworks and practical implementations, and sets clear directions for future research.</p>
<p>Introduction</p>
<p>Reasoning is a fundamental aspect of human intelligence, enabling us to solve complex problems effectively.In LLMs, reasoning has shown promising capabilities, leading to significant advancements in various domains (Bi et al., 2024b), including arithmetic, commonsense, and symbolic reasoning.For instance, Chain-of-Thought (CoT) prompting mimics human stepwise problem-solving to boost LLM performance, especially in very large models.</p>
<p>Beyond CoT, other prompting strategies, such as generated knowledge prompting and tree search algorithms (e.g., STAR search (Zelikman et al., 2022)), have been incorporated into LLM training to further enhance reasoning capabilities.These methods encourage the model to articulate its internal reasoning in natural language, which reinforces its understanding of the task.By laying out intermediate steps, the model can verify and adjust its logic before arriving at the final answer.While reasoning in LLMs has progressed significantly, extending these capabilities to multimodal tasks remains an emerging frontier, as multimodal large language models (MLLMs) must navigate the added complexity of interpreting and integrating information from both visual and textual modalities, often resolving ambiguities, inconsistencies, or gaps that arise when the two sources conflict or diverge.</p>
<p>Effective reasoning is central to enabling MLLMs to achieve a compositional understanding-allowing them to deconstruct complex tasks into interpretable, modality-spanning steps.It also supports iterative processes such as error correction and self-refinement, where models can revise their outputs by reevaluating both visual and textual cues.</p>
<p>Moreover, strong reasoning capabilities help clear up confusion by inferring spatial relationships, handling counterfactuals, and choosing appropriate tools or actions.These mechanisms also play a crucial role in mitigating hallucinations by grounding model outputs in cross-modal evidence, ultimately improving both accuracy and trustworthiness.Finally, reasoning extends the potential of MLLMs to handle hypothetical scenarios, helping them look ahead and explore "what-if" situations that span both language and vision.</p>
<p>Initial research has demonstrated that integrating Chain-of-Thought (CoT) reasoning across different modalities can significantly enhance the performance of MLLMs.For example, Yao et al. (2024); Zhang et al. (2024a); Bi et al. (2024a) reports that the LLama series models' reasoning abilities are greatly improved by solely utilizing filtered self-generated reasoning paths.</p>
<p>Building on this momentum, our work provides a comprehensive overview of reasoning in both LLMs and MLLMs, with a focus on its techniques, applications, and future directions.We aim to bridge the gap between text-based and multimodal reasoning, offering insights into how reasoning can further enhance the capabilities of LLMs in multimodal contexts.In the following sections, we begin with a clear problem formulation and definition of reasoning, followed by an in-depth analysis of reasoning techniques-from post-training strategies to test-time computation.We then examine recent trends in datasets and benchmarking.Our goal is to offer a well-organized and accessible survey that supports both theoretical understanding and practical implementation of reasoning in LLMs and MLLMs.</p>
<p>Background</p>
<p>In complex question-answering tasks, directly predicting an answer can be highly uncertain due to the vast range of possible responses.A more effective approach involves breaking down the reasoning process into a sequence of intermediate steps.This structured method not only improves interpretability but also helps reduce uncertainty at each inference step.</p>
<p>Formally, let Q denote a given question.Conventional language models typically aim to model the conditional probability P(A | Q) of the answer A given the question.However, as the complexity of Q increases, the prediction becomes more uncertain-reflected by a rise in the conditional entropy H(A | Q).A natural approach to mitigate this uncertainty is to decompose the reasoning process into a sequence of intermediate steps.This leads to modeling the answer generation as a structured chain of conditional probabilities:
P(Step 1 | Q) • P(Step 2 | Step 1 , Q) • • • • • P(Step t | Step 1:t−1 , Q) • P(A | Step 1:t , Q) (1)
Such a decomposition encourages the model to reduce uncertainty at each stage and improves interpretability and robustness in complex question answering.This step decom- position is useful because conditioning on prior steps reduces uncertainty.Specifically, the conditional entropy satisfies:
H(Step t | Step &lt;t , Q) ≤ H(Step t | Q),
meaning each step becomes easier to predict as more context accumulates.From a probabilistic standpoint, the entire reasoning process can be viewed as a trajectory, denoted as τ = (Step 1 , . . ., Step t ), which represents a complete reasoning path.Using this, the overall probability of an answer given a question can be written as a marginal over all possible trajectories:
P(A | Q) = ∑ τ P(A | τ, Q) • P(τ | Q).
(2)</p>
<p>However, this formulation presents a challenge: enumerating all possible reasoning paths τ is intractable.In practice, CoT reasoning offers a practical workaround by sampling a single trajectory τ from the distribution P(τ | Q), and using it to approximate the answer:
P(A | Q) ≈ P(A | τ, Q), where τ ∼ P(τ | Q).(3)
This is beneficial for many tasks if the model is capable of generating one coherent reasoning path that captures the correct structure of the solution, thereby reducing uncertainty and improving answer accuracy.More advanced methods such as Tree-of-Thought (ToT) expand on CoT by generating and evaluating multiple reasoning paths, and can be seen as a multiple point estimation approach.In this case, the model generates a set of reasoning paths {τ 1 , τ 2 , . . ., τ n } and evaluates them to select the most promising one.</p>
<p>Given the challenge of enumerating all possible reasoning paths, our objective becomes clear: to generate and identify only the most promising trajectories.Achieving this effectively requires two key components: (i) Generation -a strong model capable of producing highquality reasoning paths τ; (ii) Selection -a reliable mechanism for evaluating and choosing the best candidate path.</p>
<p>These components are deeply interconnected: improved generation policies yield better trajectories, which in turn provide stronger supervision signals to further refine the model.This creates a virtuous cycle of mutual enhancement, as illustrated in Figure 2.</p>
<p>At inference time, we want to search for the best τ under the current model.During training, we aim to improve the model so that it generates reasoning paths that are more likely to lead to correct answers.</p>
<p>Broadly speaking, recent approaches fall into two main categories:</p>
<p>Post-training improvement involves optimizing the model policy π to better align outputs with desired utility, typically through techniques like fine-tuning or reinforcement learning.Formally, this corresponds to solving Equation (4), where the goal is to learn a policy that maximizes expected utility over generated outputs τ.</p>
<p>Test-time compute focuses on improving LLM performance during inference without modifying the model's core parameters.This aligns with the objective in Equation ( 4</p>
<p>In the following sections, we will discuss these two components in detail.</p>
<p>Post-training improvements</p>
<p>To enhance the quality of reasoning, we aim to learn a model that generates high-utility reasoning trajectories.One effective approach frames this as a Markov Decision Process (MDP), where the model is trained to maximize the expected return of a reasoning path τ:
max θ E τ∼π θ ∑ t γ t R(s t , a t ) with ∇ θ J(π θ ) = E τ T ∑ t=1 ∇ θ log π θ (a t | s t )A(s t , a t ) (5)
s t is the sequence of tokens up to time t, and a t is the next token.The advantage function A(s t , a t ) estimates the benefit of choosing a t in state s t , guiding the model toward higherutility token sequences.This objective can be directly optimized with respect to the model parameters using policy gradient methods (Williams, 1992).Notably, this formulation enables the training of LLMs not merely to predict the next token, but to generate complete reasoning trajectories that are optimized for long-term rewards.</p>
<p>Policy optimization is still in its early stages, often relying on existing methods with a primary focus on text token optimization.However, approaches from visual reinforcement learning are worth exploring to better align policies with perceptual and spatial dynamics.Reward alignment remains heavily reliant on textual signals, highlighting an opportunity to shift focus toward visual-based rewards.Developing methods that can interpret and adapt to visual feedback-such as spatial cues, object dynamics, and scene changes-will be key to achieving more grounded and effective visual decision-making.In terms of model architecture, improvements often stem from better attention to visual details that align with long-horizon reasoning steps.Spatial-temporal modeling offers unique opportunities for more creative action definition to manipulate the visual information for better reasoning.Similarly, during data curation, verifiers commonly depend on grounding models or language cues-indicating potential for more creative, vision-focused verifier designs.</p>
<p>Policy Optimization</p>
<p>To solve the above reward maximization problem, recent advancements in visual reasoning have prominently focused on integrating reinforcement learning (RL) and imitation learning (IL) to align MLLMs more closely with human reasoning.IL approaches, such as Thought Cloning (Wei et al., 2025), surpass traditional cloning by aligning intermediate reasoning steps instead of final outputs alone.Coupled with iterative methods like DAgger, this reduces dataset shift and hallucination.LLaVA-Critic (Xiong et al., 2024) further generalizes reward signals via preference-based alignment, boosting multimodal reasoning effectiveness with minimal modality-specific fine-tuning.RL from Simulations (RLS3) (Waite et al., 2025), employing Soft Actor-Critic (SAC) agents, significantly enhances spatial reasoning accuracy and efficiency in models like PaliGemma, outperforming random exploration baselines (Waite et al., 2025).FuRL (Fu et al., 2024c) addresses RL reward misalignment through dual-stage alignment, effectively mitigating sparse reward challenges.Adaptive Reward Programming (ARP-DT+) (Kim et al., 2023), integrating CLIP representations with Decision Transformers, enhances reasoning robustness across domains by employing active inference to counter distractions and domain shifts.</p>
<p>Reward Alignment</p>
<p>Recent approaches have explored iterative refinement and reflection mechanisms, tokenlevel rewards, and automated benchmarking to enhance reasoning quality and mitigate hallucinations.PARM++ (Guo et al., 2025) introduces a reflection-based mechanism that iteratively refines outputs by identifying misalignments and actively correcting them through self-assessment loops, significantly improving test-time GenEval scores up to three reflection cycles (Guo et al., 2025).Similarly, REVERIE employs rationale-based training, explicitly supervising reasoning steps, thus improving coherence and reducing hallucinations compared to models without rationales (Zhang et al., 2024f).Fine-grained reward models, such as MMViG (Yan et al., 2024) and Reward Alignment via Preference Learning (RAPL) (Tian et al., 2024), emphasize granular feedback at individual reasoning steps, achieving precise improvements without manual reward tuning.Similarly, CLIP-DPO (Ouali et al., 2024) leverages pre-trained CLIP directly as a reward function, streamlining reward modeling.Challenges such as constrained perceptual fields are addressed through dynamic evaluation strategies like zooming and shifting, emphasizing the importance of sequential decision-making in visual reasoning (Wang et al., 2024j).Structural issues like positional bias and patch-boundary effects further underscore the need for targeted architectural optimizations (Zhang et al., 2024d).Recent approaches integrate reward signals directly into generation.TLDR (Fu et al., 2024a) applies token-level binary rewards, enabling real-time feedback to mitigate hallucinations and boost both annotation efficiency and backbone model performance (Fu et al., 2024a).Complementarily, EACO enhances intermediate object recognition, significantly reducing hallucinations compared to earlier models like LLaVA-v1.6-7B(Wang et al., 2024g).Distillation frameworks also contribute: FIRE leverages iterative student-teacher feedback and structured evaluation to refine visual reasoning (Li et al., 2024d), while SILKIE demonstrates the scalability of GPT-4V-generated feedback in improving perception and cognition without human intervention (Li et al., 2023b).</p>
<p>Model Architecture</p>
<p>Nested architectures, such as MaGNeTS, have emerged as effective means of balancing computational efficiency with model accuracy by employing parameter-sharing and caching mechanisms during inference (Goyal et al., 2025).Intrinsic activation approaches, notably ROSS, bypass external modules by integrating multimodal understanding directly into the model's core, yielding improved adaptability to diverse visual inputs, including depth maps (Wang et al., 2024c).Additionally, DIFFLMM incorporates diffusion models and attention-based segmentation, enhancing visual grounding precision (Cao et al., 2024a), while Mini-Monkey targets the "sawtooth effect" in lightweight models through adaptive cropping and scale compression, achieving superior visual-text alignment without extensive computational resources (Huang et al., 2024b).SEA dynamically adjusts embedding alignment strategies to maintain performance across varying resolutions and model sizes (Yin et al., 2024).Concurrently, PAE-LaMM demonstrates the combined effectiveness of vision encoder adaptation and pixel reconstruction tasks, systematically enhancing visual detail awareness and question-answering performance (Gou et al., 2024).Conversely, LLAVIDAL and EventLens uniquely integrate domain-specific visual elements, such as object-centric embeddings and event-aware tokens, respectively, directly within LLM architectures to effectively reason about activities and temporal contexts (Reilly et al., 2024;Ma et al., 2024a).</p>
<p>Spatial Temporal Modeling</p>
<p>Unlike pure language reasoning, visual reasoning involves a richer action space, allowing operations such as zooming, cropping, and frame selection, which enables the model to interact with and manipulate visual inputs.The GeoGLIP pipeline utilizes geometric pre-training with a dynamic feature router to advance fine-grained visual mathematical understanding by balancing symbolic and visual modalities based on task demands (Zhang et al., 2025).</p>
<p>Concurrently, hallucination issues are mitigated via targeted fine-tuning and prompting strategies grounded in recent vision-language work like CapQA (Hu et al., 2025).For spatial reasoning, simpler graph topologies have been shown to enhance performance, while structural complexity reduces accuracy in edge-centric tasks (Zhu et al., 2024).Augmentation via uncertainty-aware active inference further boosts attribute detection precision (Zhang et al., 2024e).Intrinsic spatial-temporal modeling is advanced through architectures embedding multi-modal understanding without reliance on external depth tools (Wang et al., 2024c), and dual-branch structures improve video temporal grounding in multi-hop reasoning (Chen et al., 2024f).The shift toward multimodal benchmarks better reflects real-world spatial-temporal reasoning (Li et al., 2024a).Integrating rationalization with answer generation enhances temporal reasoning (Zohar et al., 2024), while automated annotation improves data scalability and bridges gaps between commercial and open-source Video-LLMs (Li et al., 2024e).Multi-modal integration and active inference optimize modality selection for multi-step spatial-temporal reasoning (Zhang et al., 2024l).Adjusting learning objectives to minimize hallucinations improves precision and EOS decision-making (Yue et al., 2024).</p>
<p>Figure 3: Search framework where language models explore and refine reasoning paths.Trajectories are scored using reward models, based on expected utility or final output quality, and guided by feedback, world models, and evaluators to select the most promising steps.</p>
<p>Test-Time compute</p>
<p>At test time, the goal is to find the reasoning trajectory τ that maximizes a utility function as shown as red and green path in Figure 3 τ
* = arg max τ U (τ), where U (τ) =                ∑ t r(s t , a t ) (MDP-style reward) P(goal | τ) (Goal likelihood) P(τ ≻ τ ′ ) (Preference-based) f (rank(τ)) (Ranking-based) . . . (Others: risk-sensitive, etc.)(6)
The utility U (τ) can be defined in various ways, including cumulative rewards (MDP-style), goal likelihood, preference comparisons, and ranking-based scores.The choice of utility depends on the supervision available and the nature of the task.The core challenge remains sampling high-utility trajectories under the chosen formulation.</p>
<p>Since directly modeling U (τ) is often difficult, we use reward models to approximate it.These models can include (1) absolute rewards (as in standard RL), (2) pairwise preference models (e.g., DPO), and (3) ranking-based models.</p>
<p>Search Strategies</p>
<p>Current search algorithms primarily operate on distinct textual tokens.However, innovative approaches are emerging for raw visual tokens, termed Creative Visual Search, expanding search capabilities directly within visual spaces.Moreover, Multi-Granularity Spatial Search is gaining attention as an effective extension, enhancing exploration across diverse spatial granularities.Reward and Feedback Reward systems can significantly benefit from deeper visual insights.Leveraging visual prompts encourages models to engage in self-reflection grounded in visual information, substantially improving semantic alignment and decision-making effectiveness.Iterative Refinement Determining the alignment between textual tokens and visual information remains challenging.Consequently, model self-reflection can produce hallucinated interpretations.Nevertheless, these reflections offer valuable insights, highlighting new pathways to mitigate unnecessary or hallucinated reflections and enhance model reliability.</p>
<p>Search Strategies</p>
<p>Monte Carlo Tree Search (MCTS) has emerged as a powerful tool for managing uncertainty and sequential decision-making.Imam et al. ( 2025) applied MCTS to enhance temporal reasoning, reducing hallucinations in time-dependent data.Building on this, CoM-CTS (Yao et al., 2024) unified multiple MLLMs within a collaborative tree search, addressing model bias and improving accuracy and efficiency.Vision-specific strategies advanced with VisVM (Wang et al., 2024f), which estimates long-term candidate value to reduce hallucinations, outperforming immediate-alignment methods like CLIP-PRM.Video reasoning similarly benefited from reward modeling and adaptive exploration to prioritize key frames (Yang et al., 2024b).Structured search methods also gained traction.LLaVA-o1 (Xu et al., 2025) uses stage-level beam search for complex reasoning, while MVP (Qu et al., 2024) aggregates certainty across multi-perspective captions to resist adversarial inputs.DC2 (Wang et al., 2024e) applies MCTS-based cropping to focus on salient image regions for high-resolution reasoning.Multimodal and temporal search frameworks like VideoVista (Li et al., 2024e), WorldRetriver (Zhang et al., 2024l), and DynRefer (Zhao et al., 2024) surpass static baselines using adaptive sampling, fusion, and stochastic inference.Step-by-step comparative reasoning with LMs also enhances video QA (Nagarajan &amp; Torresani, 2024).Innovations in context refinement (VURF (Mahmood et al., 2024)), image decomposition (V* (Wu &amp; Xie, 2023)), and dynamic tool use (AVIS (Hu et al., 2023)) highlight the shift toward adaptive visual reasoning.FuRL (Fu et al., 2024c) aligns reward modeling with iterative fine-tuning for better performance.In embodied AI, combining chain-of-thought, self-verification, and MCTS-driven planning enables scalable, robust decision-making in dynamic environments (Shin et al., 2024).</p>
<p>Adaptive Inference</p>
<p>Adaptive inference is reshaping vision-language reasoning by enabling dynamic, contextsensitive processing that improves both accuracy and efficiency.Central to this is the iterative evaluation and refinement of outputs, often through internal feedback and external verification.LOOKBACK enhances correction accuracy through iterative visual re-examination within each reasoning step (Wu et al., 2024b).IXC-2.5 balances performance and response length via Best-of-N sampling guided by reward models (Zang et al., 2025), while PARM++ uses reflection-based refinement to align generated images with prompts (Guo et al., 2025).Further innovations include ProgressCaptioner's sliding-window approach for tracking action progress over time (Xue et al., 2024).To combat multi-modal hallucinations, MEMVR triggers visual retracing based on uncertainty, optimizing cost-accuracy tradeoffs (Zou et al., 2024a), while MVP aggregates certainty across diverse views (Qu et al., 2024).SID applies token-level contrastive decoding to filter irrelevant content (Huo et al., 2024).Models like DualFocus integrate macro and micro reasoning for fine-grained attention control (Cao et al., 2024b), and LISA++ enables test-time compute scaling without retraining (Yang et al., 2024a).PerceptionGPT accelerates inference via adaptive token embeddings that encode spatial cues and dynamically weigh layers (Pi et al., 2023).Guo et al. (2025) introduce ORM and PRM, showing that ORM's final-step evaluation significantly enhances image generation through ranking data.PARM++ further refines this method by enabling iterative reflection for self-correction, thus improving prompt fidelity.Complementarily, Hao et al. (2025) highlight persistent errors in VLM spatial reasoning and explanation, proposing reward re-ranking as an interim solution.To assess VL-GenRMs more comprehensively, Li et al. (2024b) propose VL-RewardBench, pinpointing perception and reasoning challenges while advocating critic training and co-evolutionary reward learning.Self-training methods also gain prominence: R3V (Cheng et al., 2024b) leverages synthesized rationales to enhance noise-robust Chain-of-Thought (CoT) refinement, while CECE (Cascante-Bonilla et al., 2024) enriches evaluation using LLM-generated entailment/contradiction captions.Enhanced supervision methods also emerge.Zhang et al. (2024k) enhance CoT reasoning through ShareGPT-4O distillation, Supervised Fine-Tuning (SFT), and Direct Preference Optimization (DPO).LLaVA-Critic (Xiong et al., 2024) builds upon this using critic-guided DPO without external human feedback.FuRL (Fu et al., 2024c) introduces a two-stage tuning approach addressing misalignments and temporal errors, while HYDRA (Ke et al., 2024) employs dynamic loops for adaptive instruction ranking.Additionally, m&amp;m's (Ma et al., 2024b) provides a structured framework for evaluating multi-step planning via tool and argument accuracy.</p>
<p>Reward</p>
<p>Feedback</p>
<p>VOLCANO employs a three-stage iterative framework-initial response generation, visual self-assessment, and revision-to reduce hallucinations by emphasizing accurate image details (Lee et al., 2024).Similarly, LOOKBACK mandates explicit atomic verification against images to enhance both critique and correction processes for LVLMs (Wu et al., 2024b).LLaVA-Critic advances feedback by incorporating diverse critic instructions and iterative DPO training, leveraging internal data (Xiong et al., 2024).Additionally, tailored visual prompting with iterative binary verification effectively enhances semantic grounding in models like LLaVA-1.5, ViP-LLaVA, and CogVLM (Liao et al., 2024a).Minimalist reinforcement learning setups have shown effectiveness, particularly for multimodal mathematical reasoning, with larger models like InternVL2.5-38Bbenefiting from difficulty-based data filtering to stabilize training (Meng et al., 2025).Explicit visual CoT mechanisms, exemplified by VisCoT, integrate bounding box predictions and iterative cropping to enhance visual question-answering performance (Shao et al., 2024).In video understanding, VIDEOTREE utilizes adaptive hierarchical clustering for efficient navigation through structured treebased representations (Wang et al., 2024i).Lastly, prompt engineering strategies like MiHO and MiHI significantly reduce hallucinations without model retraining, post-processing evaluations with GPT-4 further enhancing reliability without architectural modifications (Han et al., 2024;Hu et al., 2025).</p>
<p>Iterative Refinement</p>
<p>Adaptive inference transforms vision-language reasoning by facilitating dynamic, contextsensitive processing to enhance accuracy and efficiency.Central to this approach is iterative output refinement through internal feedback and external verification.For instance, IXC-2.5 employs Best-of-N sampling guided by reward models to optimize performance and response length (Zang et al., 2025), while PARM++ uses reflection-based refinement for better alignment between images and prompts (Guo et al., 2025).Further advancements include LLaVA-o1's structured perception and stage-level beam search for decomposing complex reasoning tasks (Xu et al., 2025).To mitigate multi-modal hallucinations, MEMVR implements visual retracing based on uncertainty to optimize the cost-accuracy tradeoff (Zou et al., 2024a), MVP aggregates certainty across diverse views (Qu et al., 2024), and SID employs token-level contrastive decoding to eliminate irrelevant content (Huo et al., 2024).Moreover, DualFocus integrates macro-and micro-level reasoning for precise attention control (Cao et al., 2024b), LISA++ enables test-time compute scaling without retraining (Yang et al., 2024a), and PerceptionGPT accelerates inference through adaptive token embeddings encoding spatial cues and dynamically weighted layers (Pi et al., 2023).</p>
<p>Dataset Curation</p>
<p>Recent work shows that high-quality, strategically curated datasets outperform large but noisy alternatives in guiding reasoning paths.R-CoT (Deng et al., 2024) enhances geometric reasoning via reverse generation and stepwise synthesis.Task-specific datasets such as SMIR (Li et al., 2025a) show up to 8% performance gains over generic datasets, demonstrating the value of targeted curation.Gradual complexity in dataset construction, as shown in VIREO (Cheng et al., 2024a), ensures foundational reasoning skills before introducing advanced tasks.Scalable dataset creation through automation also proves effective.Video-Vista (Li et al., 2024e) uses auto-annotations for video reasoning, outperforming manually curated sets.DecoVQA+ (Zhang et al., 2024b) explicitly teaches when to apply question decomposition, aided by SelectiveVQD loss for better strategy selection.LocVLM (Ranasinghe et al., 2024) scales with pseudo-data and implicit feedback signals like location and relevance prediction.CogCoM (Qi et al., 2024) incorporates tasks like grounding and manipulation to reinforce step-by-step reasoning.</p>
<p>Datasets and Benchmarks</p>
<p>Structured and Task-Specific Reasoning Datasets such as Visual-RFT (Liu et al., 2025c), CapQA (Hu et al., 2025), GUIDE (Liang et al., 2024a), STAR (Wu et al., 2024a), Visual Genome (Zhang et al., 2024f), VL-GPT (Zhu et al., 2023), and Interfacing (Zou et al., 2024b) have introduced structured prompting that significantly enhances models' reasoning accuracy by explicitly guiding reasoning processes.In contrast to earlier benchmarks like STAR, these datasets place a stronger emphasis on task-specific reward functions and structured inference, aiming to improve generalization across diverse visual and linguistic contexts.</p>
<p>Temporal and Spatial Reasoning Temporal benchmarks like VisualQA, and Tempo-ralVQA (Imam et al., 2025), TLQA (Swetha et al., 2025), REXTIME (Chen et al., 2024c), FrameCap (Xue et al., 2024), and VideoVista (Li et al., 2024e) have revealed substantial limitations in current multimodal language models, achieving accuracy significantly below human levels (15% versus 90%).These benchmarks emphasize the necessity for temporal logic annotations and highlight the difficulty models face when reasoning across video frames.In parallel, spatial datasets such as SpatialVLM (Chen et al., 2024a), WhatsUp (Kamath et al., 2023), DC2 (Wang et al., 2024e), andGrounded (Chen et al., 2024f) showcase improvements through extensive spatial reasoning QA pairs and challenging spatial configurations.</p>
<p>Iterative and Reflective Reasoning Iterative reasoning capabilities have markedly improved through datasets like VISCO (Wu et al., 2024b), Mulberry-260k (Yao et al., 2024), FIRE (Li et al., 2024d), VLFeedback (Li et al., 2024c), Silkie (Li et al., 2023b), TIIL (Huang et al., 2024c), ConMe (Huang et al., 2024a), and Reflective (Zhang et al., 2024f), enabling models to learn from mistakes by both incorporating reflective and iterative feedback loops.For instance, VISCO demonstrated that human critiques significantly outperform model critiques (76% error correction), highlighting a persistent gap in models' ability to independently selfcorrect.</p>
<p>Complex Multimodal Evaluation</p>
<p>Complex multimodal datasets like EMMA (Hao et al., 2025), CoMT (Cheng et al., 2024c), SMIR (Li et al., 2025a), ProVision (Zhang et al., 2024e), MAGEBench (Zhang et al., 2024i), MM-Vet v2 (Yu et al., 2024), JourneyBench (Wang et al., 2025), VERIFY (Bi et al., 2025) and CompCap (Chen et al., 2024g) exposed limitations in current multimodal models' integration capabilities.EMMA specifically shows that models struggle significantly with iterative multimodal interactions, particularly when tasks demand deep integration across domain-specific problems.Benchmarks like FineCops-Ref (Liu et al., 2025b), VL-RewardBench (Li et al., 2024b), MM-SAP (Wang et al., 2024h), PCA-Bench (Chen et al., 2024d), AttCoSeg (Pramanick et al., 2024), and M3CoT (Chen et al., 2024e) emphasize detailed evaluation metrics (Recall@k, AUROC).These benchmarks provide insights into models' actual reasoning capabilities beyond traditional accuracy metrics.Active Perception and Progressive Reasoning Active perception datasets such as Ac-tiView (Wang et al., 2024j) and progressive reasoning benchmarks such as Blink (Fu et al., 2024b), ADL-X (Reilly et al., 2024), and GUIDE (Liang et al., 2024a) challenge models' dynamic reasoning capabilities through active view adjustments and frame-level progression.</p>
<p>Counterfactual and Logical Reasoning</p>
<p>Conclusion</p>
<p>In this paper, we presented a comprehensive survey of existing work on reasoning in Multimodal Large Language Models (MLLMs) and Large Language Models (LLMs), with a focus on optimizing model performance and identifying the most effective reasoning trajectories.We introduced a variety of post-training and test-time computation methods, discussing their potential to enhance model capabilities on complex, real-world tasks.Our analysis highlights the critical role of reasoning in improving the visual understanding abilities of MLLMs.Furthermore, we have provided insights and outlined potential directions for future research to guide the development of more robust and efficient reasoning frameworks.</p>
<p>A Implications from the Learning Science Perspective</p>
<p>The design of CoT reasoning for LLMs aligns with several key cognitive science and learning science theories that enhance structured reasoning, problem-solving, and knowledge construction.</p>
<p>One key theory is Cognitive Load Theory (Sweller, 2011;Paas et al., 2004), which reduces a human's working memory workload by breaking complex problems into manageable steps.CoT reasoning follows this approach by prompting models to generate step-by-step solutions rather than arriving at an answer in one leap.This structured decomposition mirrors how human learners handle intricate problems by offloading cognitive effort across multiple processing stages.Similarly, in the context of learning, scaffolding refers to offering temporary assistance to learners, enabling them to accomplish tasks that would be difficult or impossible for them to complete independently.This support is gradually removed as learners develop the skills and confidence required to perform the task on their own (Van de Pol et al., 2010).This supports CoT's design by guiding LLMs through intermediate steps, much like how educators provide structured support to help students grasp new concepts before transitioning to independent problem-solving.These theories are both rooted in the Zone of Proximal Development (ZPD), a key construct in Lev Vygotsky's theory of learning and development (Vygotsky, 1978).</p>
<p>CoT reasoning also aligns with metacognition (Lai, 2011) and self-explanation (Bisra et al., 2018;Chi et al., 1994) by encouraging models to "think about their thinking".Just as self-explanation improves human learning by prompting individuals to articulate their reasoning (Chi et al., 1994;VanLehn et al., 1992), CoT forces LLMs to justify their steps, reducing reliance on shallow heuristics.This ties into Dual-Process Theory (Frankish, 2010), where CoT shifts LLMs from fast, intuitive decision-making (System 1) to deliberate, analytical reasoning (System 2), leading to more logical and consistent outputs.</p>
<p>Moreover, CoT fosters constructivist learning (Waite-Stupiansky, 2022;Bruner, 2009) by enabling LLMs to incrementally build knowledge structures through active reasoning.Instead of passively retrieving answers from training data, the model synthesizes prior knowledge with new information, improving adaptability.Additionally, analogical reasoning (Gentner &amp; Maravilla, 2017) plays a role in CoT by helping LLMs map relationships between concepts, allowing them to generalize problem-solving strategies across different contexts.</p>
<p>By integrating these cognitive science and learning science principles, CoT reasoning enhances the interpretability, reliability, and generalization of LLM outputs, making them more aligned with human cognitive processes and educational best practices.</p>
<p>Post Training Policy Optimization</p>
<p>Figure 1 :
1
Figure 1: Papers on visual reasoning per quarter over the last three years, with state computed using referenced papers.(Data current to mid-March 2025.)</p>
<p>Figure 2 :
2
Figure 2: Framework illustrating training and inference for reasoning optimization.A virtuous cycle emerges as better policies generate improved trajectories, which in turn enhance the model through stronger supervision.</p>
<p>Datasets like CounterCurate(Zhang et al., 2024c), C-VQA(Zhang et al., 2024g), CausalChaos!(Parmar et al., 2024), LogicAI(Xiao et al., 2024), and MCDGRAPH(Zhu et al., 2024) explicitly test model reasoning robustness, showing improvements through challenging counterfactual examples and structured logical questions.</p>
<p>Fu</p>
<p>Figure 5: Comprehensive Overview of Methods and Frameworks focus on post-training improvement</p>
<p>Mitigating Open-Vocabulary Caption Hallucinations. Assaf Ben-Kish, Moran Yanuka, Morris Alper, Raja Giryes, Hadar Averbuch-Elor, arXiv:2312.03631October 2024</p>
<p>Unveiling visual perception in language models: An attention head analysis approach. Jing Bi, Junjia Guo, Yunlong Tang, Lianggong Bruce Wen, Zhang Liu, Chenliang Xu, 2024a</p>
<p>Eagle: Egocentric aggregated language-video engine. Jing Bi, Yunlong Tang, Luchuan Song, Ali Vosoughi, Nguyen Nguyen, Chenliang Xu, 10.1145/3664647.3681618Proceedings of the 32nd ACM International Conference on Multimedia, MM '24. the 32nd ACM International Conference on Multimedia, MM '24ACMOctober 2024b</p>
<p>Verify: A benchmark of visual explanation and reasoning for investigating multimodal reasoning fidelity. Jing Bi, Junjia Guo, Susan Liang, Guangyu Sun, Luchuan Song, Yunlong Tang, Jinxi He, Jiarui Wu, Ali Vosoughi, Chen Chen, Chenliang Xu, 2025</p>
<p>Inducing self-explanation: A meta-analysis. Kiran Bisra, Qing Liu, John C Nesbit, Farimah Salimi, Philip H Winne, Educational Psychology Review. 302018</p>
<p>The process of education. Jerome S Bruner, 2009Harvard university press</p>
<p>Emerging Pixel Grounding in Large Multimodal Models Without Grounding Supervision. Shengcao Cao, Gui Liang-Yan, Yu-Xiong Wang, arXiv:2410.08209October 2024a</p>
<p>DualFocus: Integrating Macro and Micro Perspectives in Multi-modal Large Language Models. Yuhang Cao, Pan Zhang, Xiaoyi Dong, Dahua Lin, Jiaqi Wang, arXiv:2402.14767February 2024b</p>
<p>Natural Language Inference Improves Compositionality in Vision-Language Models. Paola Cascante-Bonilla, Yu Hou, Yang , Trista Cao, Hal Daumé, Iii , Rachel Rudinger, arXiv:2410.22315October 2024</p>
<p>SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities. Boyuan Chen, Zhuo Xu, Sean Kirmani, Brian Ichter, Danny Driess, Pete Florence, Dorsa Sadigh, Leonidas Guibas, Fei Xia, arXiv:2401.12168January 2024a</p>
<p>Plug-and-Play Grounding of Reasoning in Multimodal Large Language Models. Jiaxing Chen, Yuxuan Liu, Dehu Li, Xiang An, Weimo Deng, Ziyong Feng, Yongle Zhao, Yin Xie, arXiv:2403.19322June 2024b</p>
<p>Jr-Jen Chen, Yu-Chien Liao, Hsi-Che Lin, Yu-Chu Yu, Yen-Chun Chen, Yu-Chiang Frank, Wang , arXiv:2406.19392ReXTime: A Benchmark Suite for Reasoning-Across-Time in Videos. July 2024c</p>
<p>PCA-Bench: Evaluating Multimodal Large Language Models in Perception-Cognition-Action Chain. Liang Chen, Yichi Zhang, Shuhuai Ren, Haozhe Zhao, Zefan Cai, Yuchi Wang, Peiyi Wang, Xiangdi Meng, Tianyu Liu, Baobao Chang, arXiv:2402.15527February 2024d</p>
<p>A Novel Benchmark for Multi-Domain Multi-step Multi-modal Chain-of-Thought. Qiguang Chen, Libo Qin, Jin Zhang, Zhi Chen, Xiao Xu, Wanxiang Che, . M$ˆ3$ Cot, arXiv:2405.16473May 2024e</p>
<p>Grounded Multi-Hop VideoQA in Long-Form Egocentric Videos. Qirui Chen, Shangzhe Di, Weidi Xie, arXiv:2408.14469August 2024f</p>
<p>Xiaohui Chen, Mahmoud Satya Narayan Shukla, Aashu Azab, Qifan Singh, David Wang, Shengyun Yang, Hanchao Peng, Shen Yu, Xuewen Yan, Baosheng Zhang, He, arXiv:2412.05243Comp-Cap: Improving Multimodal Large Language Models with Composite Captions, December 2024g. </p>
<p>From the Least to the Most: Building a Plug-and-Play Visual Reasoner via Data Synthesis. Chuanqi Cheng, Jian Guan, Wei Wu, Rui Yan, arXiv:2406.19934October 2024a</p>
<p>Vision-Language Models Can Self-Improve Reasoning via Reflection. Kanzhi Cheng, Yantao Li, Fangzhi Xu, Jianbing Zhang, Hao Zhou, Yang Liu, arXiv:2411.00855October 2024b</p>
<p>CoMT: A Novel Benchmark for Chain of Multi-modal Thought on Large Vision-Language Models. Zihui Cheng, Qiguang Chen, Jin Zhang, Hao Fei, Xiaocheng Feng, Wanxiang Che, Min Li, Libo Qin, arXiv:2412.12932December 2024c</p>
<p>Eliciting self-explanations improves understanding. T H Michelene, Nicholas De Chi, Mei-Hung Leeuw, Christian Chiu, Lavancher, Cognitive science. 1831994</p>
<p>R-CoT: Reverse Chain-of-Thought Problem Generation for Geometric Reasoning in Large Multimodal Models. Linger Deng, Yuliang Liu, Bohan Li, Dongliang Luo, Liang Wu, Chengquan Zhang, Pengyuan Lyu, Ziyang Zhang, Gang Zhang, Errui Ding, Yingying Zhu, Xiang Bai, arXiv:2410.17885October 2024</p>
<p>VLRM: Vision-Language Models act as Reward Models for Image Captioning. Maksim Dzabraev, Alexander Kunitsyn, Andrei Ivaniuta, arXiv:2404.01911April 2024</p>
<p>MMFactory: A Universal Solution Search Engine for Vision-Language Tasks. Wan-Cyuan Fan, Tanzila Rahman, Leonid Sigal, arXiv:2412.18072December 2024</p>
<p>Dual-process and dual-system theories of reasoning. Keith Frankish, Philosophy Compass. 5102010</p>
<p>TLDR: Token-Level Detective Reward Model for Large Vision Language Models. Deqing Fu, Tong Xiao, Rui Wang, Wang Zhu, Pengchuan Zhang, Guan Pang, Robin Jia, Lawrence Chen, arXiv:2410.04734October 2024a</p>
<p>BLINK: Multimodal Large Language Models Can See but Not Perceive. Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah A Smith, Wei-Chiu Ma, Ranjay Krishna, arXiv:2404.12390July 2024b</p>
<p>FuRL: Visual-Language Models as Fuzzy Rewards for Reinforcement Learning. Yuwei Fu, Haichao Zhang, Di Wu, Wei Xu, Benoit Boulet, arXiv:2406.00645June 2024c</p>
<p>Analogical reasoning. Dedre Gentner, Francisco Maravilla, International handbook of thinking and reasoning. Routledge2017</p>
<p>How Well Can Vision Language Models See Image Details?. Chenhui Gou, Abdulwahab Felemban, Faizan Farooq Khan, Deyao Zhu, Jianfei Cai, Hamid Rezatofighi, Mohamed Elhoseiny, arXiv:2408.03940August 2024</p>
<p>Masked Generative Nested Transformers with Decode Time Scaling. Sahil Goyal, Debapriya Tula, Gagan Jain, Pradeep Shenoy, Prateek Jain, Sujoy Paul, arXiv:2502.00382February 2025</p>
<p>Can We Generate Images with CoT? Let's Verify and Reinforce Image Generation Step by Step. Ziyu Guo, Renrui Zhang, Chengzhuo Tong, Zhizheng Zhao, Peng Gao, Hongsheng Li, Pheng-Ann Heng, arXiv:2501.13926January 2025</p>
<p>Skip \n: A Simple Method to Reduce Hallucination in Large Vision-Language Models. Zongbo Han, Zechen Bai, Haiyang Mei, Qianli Xu, Changqing Zhang, Mike Zheng Shou, arXiv:2402.01345May 2024</p>
<p>Can MLLMs Reason in Multimodality? EMMA: An Enhanced Multi-Modal ReAsoning Benchmark. Yunzhuo Hao, Jiawei Gu, Huichen Will Wang, Linjie Li, Zhengyuan Yang, Lijuan Wang, Yu Cheng, arXiv:2501.05444January 2025</p>
<p>Incorporating Visual Experts to Resolve the Information Loss in Multimodal Large Language Models. Xin He, Longhui Wei, Lingxi Xie, Qi Tian, arXiv:2401.03105January 2024</p>
<p>What Makes a Maze Look Like a Maze?. Joy Hsu, Jiayuan Mao, Joshua B Tenenbaum, Noah D Goodman, Jiajun Wu, arXiv:2409.08202September 2024</p>
<p>Socratic Questioning: Learn to Self-guide Multimodal Reasoning in the Wild. Wanpeng Hu, Haodi Liu, Lin Chen, Feng Zhou, Changming Xiao, Qi Yang, Changshui Zhang, arXiv:2501.02964January 2025</p>
<p>Visual Program Distillation: Distilling Tools and Programmatic Reasoning into Vision-Language Models. Yushi Hu, Otilia Stretcu, Chun-Ta Lu, Krishnamurthy Viswanathan, Kenji Hata, Enming Luo, Ranjay Krishna, Ariel Fuxman, arXiv:2312.03052April 2024</p>
<p>AVIS: Autonomous Visual Information Seeking with Large Language Model Agent. Ziniu Hu, Ahmet Iscen, Chen Sun, Kai-Wei Chang, Yizhou Sun, David A Ross, Cordelia Schmid, Alireza Fathi, arXiv:2306.08129November 2023</p>
<p>ConMe: Rethinking Evaluation of Compositional Reasoning for Modern VLMs. Irene Huang, Wei Lin, M Jehanzeb Mirza, Jacob A Hansen, Sivan Doveh, Ion Victor, Roei Butoi, Assaf Herzig, Hilde Arbelle, Trevor Kuehne, Chuang Darrell, Aude Gan, Rogerio Oliva, Leonid Feris, Karlinsky, arXiv:2406.08164November 2024a</p>
<p>Mini-Monkey: Alleviating the Semantic Sawtooth Effect for Lightweight MLLMs via Complementary Image Pyramid. Mingxin Huang, Yuliang Liu, Dingkang Liang, Lianwen Jin, Xiang Bai, arXiv:2408.02034October 2024b</p>
<p>Exposing Text-Image Inconsistency Using Diffusion Models. Mingzhen Huang, Shan Jia, Zhou Zhou, Yan Ju, Jialing Cai, Siwei Lyu, arXiv:2404.18033April 2024c</p>
<p>Making Large Language Models Better Planners with Reasoning-Decision Alignment. Zhijian Huang, Tao Tang, Shaoxiang Chen, Sihao Lin, Zequn Jie, Lin Ma, Guangrun Wang, Xiaodan Liang, arXiv:2408.13890August 2024d</p>
<p>Self-Introspective Decoding: Alleviating Hallucinations for Large Vision-Language Models. Fushuo Huo, Wenchao Xu, Zhong Zhang, Haozhao Wang, Zhicheng Chen, Peilin Zhao, arXiv:2408.02032October 2024</p>
<p>Can Multimodal LLMs do Visual Temporal Understanding and Reasoning? The answer is No!. Mohamed Fazli Imam, Chenyang Lyu, Alham Fikri, Aji , arXiv:2501.10674January 2025</p>
<p>From Training-Free to Adaptive: Empirical Insights into MLLMs' Understanding of Detection Information. Qirui Jiao, Daoyuan Chen, Yilun Huang, Yaliang Li, Ying Shen, arXiv:2401.17981December 2024</p>
<p>What's "up" with vision-language models? Investigating their struggle with spatial reasoning. Amita Kamath, Jack Hessel, Kai-Wei Chang, arXiv:2310.19785October 2023</p>
<p>ReMI: A Dataset for Reasoning with Multiple Images. Mehran Kazemi, Nishanth Dikkala, Ankit Anand, Petar Devic, Ishita Dasgupta, Fangyu Liu, Bahare Fatemi, Pranjal Awasthi, Dee Guo, Sreenivas Gollapudi, Ahmed Qureshi, arXiv:2406.09175June 2024</p>
<p>HYDRA: A Hyper Agent for Dynamic Compositional Visual Reasoning. Fucai Ke, Zhixi Cai, Simindokht Jahangard, Weiqing Wang, Pari Delir Haghighi, Hamid Rezatofighi, arXiv:2403.12884July 2024</p>
<p>Guide Your Agent with Adaptive Multimodal Rewards. Changyeon Kim, Younggyo Seo, Hao Liu, Lisa Lee, Jinwoo Shin, Honglak Lee, Kimin Lee, arXiv:2309.10790October 2023</p>
<p>Metacognition: A literature review. Emily R Lai, 2011</p>
<p>Volcano: Mitigating Multimodal Hallucination through Self-Feedback Guided Revision. Seongyun Lee, Sue , Hyun Park, Yongrae Jo, Minjoon Seo, arXiv:2311.07362April 2024</p>
<p>SMIR: Efficient Synthetic Data Pipeline To Improve Multi-Image Reasoning. Andrew Li, Rahul Thapa, Rahul Chalamala, Qingyang Wu, Kezhen Chen, James Zou, arXiv:2501.03675January 2025a</p>
<p>Imagine while Reasoning in Space: Multimodal Visualization-of-Thought. Chengzu Li, Wenshan Wu, Huanyu Zhang, Yan Xia, Shaoguang Mao, Li Dong, Ivan Vulić, Furu Wei, arXiv:2501.07542January 2025b</p>
<p>Feng Li, Qing Jiang, Hao Zhang, Tianhe Ren, Shilong Liu, Xueyan Zou, Huaizhe Xu, Hongyang Li, Chunyuan Li, Jianwei Yang, Lei Zhang, Jianfeng Gao, arXiv:2311.13601Visual In-Context Prompting. November 2023a</p>
<p>A Survey on Benchmarks of Multimodal Large Language Models. Jian Li, Weiheng Lu, Hao Fei, Meng Luo, Ming Dai, Min Xia, Yizhang Jin, Zhenye Gan, Ding Qi, Chaoyou Fu, Ying Tai, Wankou Yang, Yabiao Wang, Chengjie Wang, arXiv:2408.08632September 2024a</p>
<p>Silkie: Preference Distillation for Large Visual Language Models. Lei Li, Zhihui Xie, Mukai Li, Shunian Chen, Peiyi Wang, Liang Chen, Yazheng Yang, Benyou Wang, Lingpeng Kong, arXiv:2312.10665December 2023b</p>
<p>VLRewardBench: A Challenging Benchmark for Vision-Language Generative Reward Models. Lei Li, Yuancheng Wei, Zhihui Xie, Xuqing Yang, Yifan Song, Peiyi Wang, Chenxin An, Tianyu Liu, Sujian Li, Bill Yuchen Lin, Lingpeng Kong, Qi Liu, November 2024b</p>
<p>VLFeedback: A Large-Scale AI Feedback Dataset for Large Vision-Language Models Alignment. Lei Li, Zhihui Xie, Mukai Li, Shunian Chen, Peiyi Wang, Liang Chen, Yazheng Yang, Benyou Wang, Lingpeng Kong, Qi Liu, arXiv:2410.09421October 2024c</p>
<p>FIRE: A Dataset for Feedback Integration and Refinement Evaluation of Multimodal Models. Pengxiang Li, Zhi Gao, Bofei Zhang, Tao Yuan, Yuwei Wu, Mehrtash Harandi, Yunde Jia, Song-Chun Zhu, Qing Li, arXiv:2407.11522December 2024d</p>
<p>Video-Vista: A Versatile Benchmark for Video Understanding and Reasoning. Yunxin Li, Xinyu Chen, Baotian Hu, Longyue Wang, Haoyuan Shi, Min Zhang, arXiv:2406.11303June 2024e</p>
<p>GUIDE: A Guideline-Guided Dataset for Instructional Video Comprehension. Jiafeng Liang, Shixin Jiang, Zekun Wang, Haojie Pan, Zerui Chen, Zheng Chu, Ming Liu, Ruiji Fu, Zhongyuan Wang, Bing Qin, arXiv:2406.18227June 2024a</p>
<p>ReasVQA: Advancing VideoQA with Imperfect Reasoning Process. Jianxin Liang, Xiaojun Meng, Huishuai Zhang, Yueqian Wang, Jiansheng Wei, Dongyan Zhao, arXiv:2501.13536January 2025</p>
<p>Neural-Symbolic VideoQA: Learning Compositional Spatio-Temporal Reasoning for Real-world Video Question Answering. Lili Liang, Guanglu Sun, Jin Qiu, Lizhong Zhang, arXiv:2404.04007April 2024b</p>
<p>Can Feedback Enhance Semantic Grounding in Large Vision-Language Models?. Yuan-Hong Liao, Rafid Mahmood, Sanja Fidler, David Acuna, arXiv:2404.06510April 2024a</p>
<p>Align and Aggregate: Compositional Reasoning with Video Alignment and Answer Aggregation for Video Question-Answering. Zhaohe Liao, Jiangtong Li, Li Niu, Liqing Zhang, arXiv:2407.03008July 2024b</p>
<p>Rethinking Visual Prompting for Multimodal Large Language Models with External Knowledge. Yuanze Lin, Yunsheng Li, Dongdong Chen, Weijian Xu, Ronald Clark, Philip Torr, Lu Yuan, arXiv:2407.04681July 2024</p>
<p>Commonsense Video Question Answering through Video-Grounded Entailment Tree Reasoning. Huabin Liu, Filip Ilievski, G M Cees, Snoek, arXiv:2501.05069January 2025a</p>
<p>FineCops-Ref: A new Dataset and Task for Fine-Grained Compositional Referring Expression Comprehension. Junzhuo Liu, Xuzheng Yang, Weiwei Li, Peng Wang, arXiv:2409.14750January 2025b</p>
<p>Visual-RFT: Visual Reinforcement Fine-Tuning. Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, Jiaqi Wang, arXiv:2503.01785March 2025c</p>
<p>Vista-LLaMA: Reliable Video Narrator via Equal Distance to Visual Tokens. Fan Ma, Xiaojie Jin, Heng Wang, Yuchen Xian, Jiashi Feng, Yi Yang, arXiv:2312.08870December 2023</p>
<p>Under review. Preprint, </p>
<p>EventLens: Leveraging Event-Aware Pretraining and Cross-modal Linking Enhances Visual Commonsense Reasoning. Mingjie Ma, Zhihuan Yu, Yichao Ma, Guohui Li, arXiv:2404.13847April 2024a</p>
<p>m&amp;m's: A Benchmark to Evaluate Tool-Use for multi-step multi-modal Tasks. Zixian Ma, Weikai Huang, Jieyu Zhang, Tanmay Gupta, Ranjay Krishna, arXiv:2403.11085September 2024b</p>
<p>VURF: A General-purpose Reasoning and Self-refinement Framework for Video Understanding. Ahmad Mahmood, Ashmal Vayani, Muzammal Naseer, Salman Khan, Fahad Shahbaz Khan, arXiv:2403.14743March 2024</p>
<p>MM-Eureka: Exploring Visual Aha Moment with Rule-based Large-scale Reinforcement Learning. Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Botian Shi, Wenhai Wang, Junjun He, Kaipeng Zhang, Ping Luo, Yu Qiao, Qiaosheng Zhang, Wenqi Shao, arXiv:2503.07365March 2025</p>
<p>MoReVQA: Exploring Modular Reasoning Models for Video Question Answering. Juhong Min, Shyamal Buch, Arsha Nagrani, Minsu Cho, Cordelia Schmid, arXiv:2404.06511April 2024</p>
<p>Step Differences in Instructional Video. Tushar Nagarajan, Lorenzo Torresani, arXiv:2404.16222June 2024</p>
<p>Visual-O1: Understanding Ambiguous Instructions via Multi-modal Multi-turn Chain-of-thoughts Reasoning. Minheng Ni, Yutao Fan, Lei Zhang, Wangmeng Zuo, arXiv:2410.03321October 2024</p>
<p>CLIP-DPO: Vision-Language Models as a Source of Preference for Fixing Hallucinations in LVLMs. Yassine Ouali, Adrian Bulat, Brais Martinez, Georgios Tzimiropoulos, arXiv:2408.10433August 2024</p>
<p>Cognitive load theory: Instructional implications of the interaction between information structures and cognitive architecture. Fred Paas, Alexander Renkl, John Sweller, Instructional science. 321/22004</p>
<p>Generalizing from SIMPLE to HARD Visual Reasoning: Can We Mitigate Modality Imbalance in VLMs?. Simon Park, Abhishek Panigrahi, Yun Cheng, Dingli Yu, Anirudh Goyal, Sanjeev Arora, arXiv:2501.02669January 2025</p>
<p>CausalChaos! Dataset for Comprehensive Causal Action Question Answering Over Longer Causal Chains Grounded in Dynamic Visual Scenes. Paritosh Parmar, Eric Peh, Ruirui Chen, Yuhan Ting En Lam, Elston Chen, Basura Tan, Fernando, arXiv:2404.01299June 2024</p>
<p>PerceptionGPT: Effectively Fusing Visual Perception into LLM. Renjie Pi, Lewei Yao, Jiahui Gao, Jipeng Zhang, Tong Zhang, arXiv:2311.06612November 2023</p>
<p>Jack of All Tasks, Master of Many: Designing General-purpose Coarse-to-Fine Vision-Language Model. Shraman Pramanick, Guangxing Han, Rui Hou, Sayan Nag, Ser-Nam Lim, Nicolas Ballas, Qifan Wang, Rama Chellappa, Amjad Almahairi, arXiv:2312.12423June 2024</p>
<p>CogCoM: Train Large Vision-Language Models Diving into Details through Chain of Manipulations. Ji Qi, Ming Ding, Weihan Wang, Yushi Bai, Qingsong Lv, Wenyi Hong, Bin Xu, Lei Hou, Juanzi Li, Yuxiao Dong, Jie Tang, arXiv:2402.04236May 2024</p>
<p>Look, Compare, Decide: Alleviating Hallucination in Large Vision-Language Models via Multi-View Multi-Path Reasoning. Xiaoye Qu, Jiashuo Sun, Wei Wei, Yu Cheng, arXiv:2408.17150August 2024</p>
<p>Learning to Localize Objects Improves Spatial Reasoning in Visual-LLMs. Kanchana Ranasinghe, Omid Satya Narayan Shukla, Michael S Poursaeed, Tsung-Yu Ryoo, Lin, arXiv:2404.07449April 2024</p>
<p>LLAVIDAL: A Large LAnguage VIsion Model for Daily Activities of Living. Dominick Reilly, Rajatsubhra Chakraborty, Arkaprava Sinha, Manish Kumar Govind, Pu Wang, Francois Bremond, Le Xue, Srijan Das, arXiv:2406.09390December 2024</p>
<p>Visual CoT: Advancing Multi-Modal Language Models with a Comprehensive Dataset and Benchmark for Chain-of-Thought Reasoning. Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, Hongsheng Li, arXiv:2403.16999November 2024</p>
<p>Socratic Planner: Inquiry-Based Zero-Shot Planning for Embodied Instruction Following. Suyeon Shin, Junghyun Sujin Jeon, Gi-Cheon Kim, Byoung-Tak Kang, Zhang, arXiv:2404.15190April 2024</p>
<p>Can We Generate Visual Programs Without Prompting LLMs?. Michal Shlapentokh-Rothman, Yu-Xiong Wang, Derek Hoiem, arXiv:2412.08564December 2024</p>
<p>Investigating Video Reasoning Capability of Large Language Models with Tropes in Movies. Hung-Ting Su, Chun-Tong Chao, Ya-Ching Hsu, Xudong Lin, Yulei Niu, Hung-Yi Lee, Winston H Hsu, arXiv:2406.10923June 2024</p>
<p>Multimodal Latent Language Modeling with Next-Token Diffusion. Yutao Sun, Hangbo Bao, Wenhui Wang, Zhiliang Peng, Li Dong, Shaohan Huang, Jianyong Wang, Furu Wei, arXiv:2412.08635December 2024</p>
<p>Cognitive load theory. John Sweller, Psychology of learning and motivation. Elsevier201155</p>
<p>TimeLogic: A Temporal Logic Benchmark for Video QA. Sirnam Swetha, Hilde Kuehne, Mubarak Shah, arXiv:2501.07214January 2025</p>
<p>What Matters to You? Towards Visual Representation Alignment for Robot Learning. Ran Tian, Chenfeng Xu, Masayoshi Tomizuka, Jitendra Malik, Andrea Bajcsy, arXiv:2310.07932January 2024</p>
<p>Scaffolding in teacher-student interaction: A decade of research. Janneke Van De Pol, Monique Volman, Jos Beishuizen, Educational psychology review. 222010</p>
<p>A model of the self-explanation effect. Kurt Vanlehn, Randolph M Jones, Michelene Th Chi, The journal of the learning sciences. 211992</p>
<p>Mind in society: The development of higher psychological processes. Lev S Vygotsky, 1978Harvard university press86</p>
<p>RLS3: RL-Based Synthetic Sample Selection to Enhance Spatial Reasoning in Vision-Language Models for Indoor Autonomous Perception. Joshua R Waite, Md Zahid Hasan, Qisai Liu, Zhanhong Jiang, Chinmay Hegde, Soumik Sarkar, arXiv:2501.18880January 2025</p>
<p>Jean piaget's constructivist theory of learning. Sandra Waite-Stupiansky, Theories of early childhood education. Routledge2022</p>
<p>Instruction Tuning-free Visual Token Complement for Multimodal LLMs. Dongsheng Wang, Jiequan Cui, Miaoge Li, Wang Lin, Bo Chen, Hanwang Zhang, arXiv:2408.05019August 2024a</p>
<p>GiT: Towards Generalist Vision Transformer through Universal Language Interface. Haiyang Wang, Hao Tang, Li Jiang, Shaoshuai Shi, Muhammad Ferjad Naeem, Hongsheng Li, Bernt Schiele, Liwei Wang, arXiv:2403.09394March 2024b</p>
<p>Reconstructive Visual Instruction Tuning. Haochen Wang, Anlin Zheng, Yucheng Zhao, Tiancai Wang, Zheng Ge, Xiangyu Zhang, Zhaoxiang Zhang, arXiv:2410.09575December 2024c</p>
<p>CogVLM: Visual Expert for Pretrained Language Models. Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, Jie Tang, arXiv:2311.03079February 2024d</p>
<p>Divide, Conquer and Combine: A Training-Free Framework for High-Resolution Image Perception in Multimodal Large Language Models. Wenbin Wang, Liang Ding, Minyan Zeng, Xiabin Zhou, Li Shen, Yong Luo, Dacheng Tao, arXiv:2408.15556August 2024e</p>
<p>Scaling Inference-Time Search with Vision Value Model for Improved Visual Comprehension. Xiyao Wang, Zhengyuan Yang, Linjie Li, Hongjin Lu, Yuancheng Xu, Chung-Ching Lin, Kevin Lin, Furong Huang, Lijuan Wang, arXiv:2412.03704December 2024f</p>
<p>EACO: Enhancing Alignment in Multimodal LLMs via Critical Observation. Yongxin Wang, Meng Cao, Haokun Lin, Mingfei Han, Liang Ma, Jin Jiang, Yuhao Cheng, Xiaodan Liang, arXiv:2412.04903December 2024g</p>
<p>MM-SAP: A Comprehensive Benchmark for Assessing Self-Awareness of Multimodal Large Language Models in Perception. Yuhao Wang, Yusheng Liao, Heyang Liu, Hongcheng Liu, Yu Wang, Yanfeng Wang, arXiv:2401.07529June 2024h</p>
<p>JourneyBench: A Challenging One-Stop Vision-Language Understanding Benchmark of Generated Images. Zhecan Wang, Junzhang Liu, Chia-Wei Tang, Hani Alomari, Anushka Sivakumar, Rui Sun, Wenhao Li, Md Atabuzzaman, Hammad Ayyubi, Haoxuan You, Alvi Ishmam, Kai-Wei Chang, Shih-Fu Chang, Chris Thomas, arXiv:2409.12953January 2025</p>
<p>VideoTree: Adaptive Tree-based Video Representation for LLM Reasoning on Long Videos. Ziyang Wang, Shoubin Yu, Elias Stengel-Eskin, Jaehong Yoon, Feng Cheng, Gedas Bertasius, Mohit Bansal, arXiv:2405.19209October 2024i</p>
<p>ActiView: Evaluating Active Perception Ability for Multimodal Large Language Models. Ziyue Wang, Chi Chen, Fuwen Luo, Yurui Dong, Yuanchi Zhang, Yuzhuang Xu, Xiaolong Wang, Peng Li, Yang Liu, arXiv:2410.04659October 2024j</p>
<p>GTR: Guided Thought Reinforcement Prevents Thought Collapse in RL-based VLM Agent Training. Tong Wei, Yijun Yang, Junliang Xing, Yuanchun Shi, Zongqing Lu, Deheng Ye, arXiv:2503.08525March 2025</p>
<p>Simple statistical gradient-following algorithms for connectionist reinforcement learning. Williams Ronald, Machine learning. 83-41992</p>
<p>STAR: A Benchmark for Situated Reasoning in Real-World Videos. Bo Wu, Shoubin Yu, Zhenfang Chen, Joshua B Tenenbaum, Chuang Gan, arXiv:2405.09711May 2024a</p>
<p>V*: Guided Visual Search as a Core Mechanism in Multimodal LLMs. Penghao Wu, Saining Xie, arXiv:2312.14135December 2023</p>
<p>VISCO: Benchmarking Fine-Grained Critique and Correction Towards Self-Improvement in Visual Reasoning. Xueqing Wu, Yuheng Ding, Bingxuan Li, Pan Lu, Da Yin, Kai-Wei Chang, Nanyun Peng, arXiv:2412.02172December 2024b</p>
<p>Can Atomic Step Decomposition Enhance the Self-structured Reasoning of Multimodal Large Models?. Kun Xiang, Zhili Liu, Zihao Jiang, Yunshuang Nie, Kaixin Cai, Yiyang Yin, Runhui Huang, Haoxiang Fan, Hanhui Li, Weiran Huang, Yihan Zeng, Yu-Jie Yuan, Jianhua Han, Lanqing Hong, Hang Xu, Xiaodan Liang, arXiv:2503.06252March 2025</p>
<p>LogicVista: Multimodal LLM Logical Reasoning Benchmark in Visual Contexts. Yijia Xiao, Edward Sun, Tianyu Liu, Wei Wang, arXiv:2407.04973July 2024</p>
<p>LLaVA-Critic: Learning to Evaluate Multimodal Models. Tianyi Xiong, Xiyao Wang, Dong Guo, Qinghao Ye, Haoqi Fan, Quanquan Gu, Heng Huang, Chunyuan Li, arXiv:2410.02712October 2024</p>
<p>LLaVA-CoT: Let Vision Language Models Reason Step-by-Step. Guowei Xu, Peng Jin, Hao Li, Yibing Song, Lichao Sun, Li Yuan, arXiv:2411.10440January 2025</p>
<p>Progress-Aware Video Frame Captioning. Zihui Xue, Joungbin An, Xitong Yang, Kristen Grauman, arXiv:2412.02071December 2024</p>
<p>ViGoR: Improving Visual Grounding of Large Vision Language Models with Fine-Grained Reward Modeling. Siming Yan, Min Bai, Weifeng Chen, Xiong Zhou, Qixing Huang, Li Erran Li, arXiv:2402.06118October 2024</p>
<p>LISA++: An Improved Baseline for Reasoning Segmentation with Large Language Model. Senqiao Yang, Tianyuan Qu, Xin Lai, Zhuotao Tian, Bohao Peng, Shu Liu, Jiaya Jia, arXiv:2312.17240January 2024a</p>
<p>VCA: Video Curious Agent for Long Video Understanding. Zeyuan Yang, Delin Chen, Xueyang Yu, Maohao Shen, Chuang Gan, arXiv:2412.10471December 2024b</p>
<p>Mulberry: Empowering MLLM with o1-like Reasoning and Reflection via Collective Monte Carlo Tree Search. Huanjin Yao, Jiaxing Huang, Wenhao Wu, Jingyi Zhang, Yibo Wang, Shunyu Liu, Yingjie Wang, Yuxin Song, Haocheng Feng, Li Shen, Dacheng Tao, arXiv:2412.18319December 2024</p>
<p>SEA: Supervised Embedding Alignment for Token-Level Visual-Textual Integration in MLLMs. Yuanyang Yin, Yaqi Zhao, Yajie Zhang, Ke Lin, Jiahao Wang, Xin Tao, Pengfei Wan, Di Zhang, Baoqun Yin, Wentao Zhang, arXiv:2408.11813August 2024</p>
<p>MM-Vet v2: A Challenging Benchmark to Evaluate Large Multimodal Models for Integrated Capabilities. Weihao Yu, Zhengyuan Yang, Lingfeng Ren, Linjie Li, Jianfeng Wang, Kevin Lin, Chung-Ching Lin, Zicheng Liu, Lijuan Wang, Xinchao Wang, arXiv:2408.00765December 2024</p>
<p>Less is More: Mitigating Multimodal Hallucination from an EOS Decision Perspective. Zihao Yue, Liang Zhang, Qin Jin, arXiv:2402.14545May 2024</p>
<p>Yuhang Zang, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Ziyu Liu, Shengyuan Ding, Shenxi Wu, Yubo Ma, Haodong Duan, Wenwei Zhang, Kai Chen, Dahua Lin, Jiaqi Wang, arXiv:2501.12368InternLM-XComposer2.5-Reward: A Simple Yet Effective Multi-Modal Reward Model. January 2025</p>
<p>STaR: Bootstrapping Reasoning With Reasoning. Eric Zelikman, Yuhuai Wu, Jesse Mu, Noah D Goodman, arXiv:2203.14465May 2022</p>
<p>Griffon v2: Advancing Multimodal Perception with High-Resolution Scaling and Visual-Language Co-Referring. Yufei Zhan, Yousong Zhu, Hongyin Zhao, Fan Yang, Ming Tang, Jinqiao Wang, arXiv:2403.09333March 2024</p>
<p>LLaMA-Berry: Pairwise Optimization for O1-like Olympiad-Level Mathematical Reasoning. Di Zhang, Jianbo Wu, Jingdi Lei, Tong Che, Jiatong Li, Tong Xie, Xiaoshui Huang, Shufei Zhang, Marco Pavone, Yuqiang Li, Wanli Ouyang, Dongzhan Zhou, arXiv:2410.02884November 2024a</p>
<p>Visual Question Decomposition on Multimodal Large Language Models. Haowei Zhang, Jianzhe Liu, Zhen Han, Shuo Chen, Bailan He, Zhiqiang Volker Tresp, Jindong Xu, Gu, arXiv:2409.19339October 2024b</p>
<p>CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples. Jianrui Zhang, Mu Cai, Tengyang Xie, Yong Jae Lee, arXiv:2402.13254June 2024c</p>
<p>Exploring Perceptual Limitation of Multimodal Large Language Models. Jiarui Zhang, Jinyi Hu, Mahyar Khayatkhoei, Filip Ilievski, Maosong Sun, arXiv:2402.07384February 2024d</p>
<p>ProVision: Programmatically Scaling Vision-centric Instruction Data for Multimodal Language Models. Jieyu Zhang, Le Xue, Linxin Song, Jun Wang, Weikai Huang, Manli Shu, An Yan, Zixian Ma, Juan Carlos Niebles, Silvio Savarese, Caiming Xiong, Zeyuan Chen, Ranjay Krishna, Ran Xu, arXiv:2412.07012December 2024e</p>
<p>Reflective Instruction Tuning: Mitigating Hallucinations in Large Vision-Language Models. Jinrui Zhang, Teng Wang, Haigang Zhang, Ping Lu, Feng Zheng, arXiv:2407.11422July 2024f</p>
<p>What If the TV Was Off? Examining Counterfactual Reasoning Abilities of Multi-modal Language Models. Letian Zhang, Xiaotong Zhai, Zhongkai Zhao, Yongshuo Zong, Xin Wen, Bingchen Zhao, arXiv:2310.06627April 2024g</p>
<p>OmAgent: A Multimodal Agent Framework for Complex Video Understanding with Task Divide-and-Conquer. Lu Zhang, Tiancheng Zhao, Heting Ying, Yibo Ma, Kyusong Lee, arXiv:2406.16620November 2024h</p>
<p>MageBench: Bridging Large Multimodal Models to Agents. Miaosen Zhang, Qi Dai, Yifan Yang, Jianmin Bao, Dongdong Chen, Kai Qiu, Chong Luo, Xin Geng, Baining Guo, arXiv:2412.04531December 2024i</p>
<p>InternLM-XComposer: A Vision-Language Large Model for Advanced Text-image Comprehension and Composition. Pan Zhang, Xiaoyi Dong, Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Haodong Duan, Songyang Zhang, Shuangrui Ding, Wenwei Zhang, Hang Yan, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, Jiaqi Wang, arXiv:2309.15112December 2023</p>
<p>Direct Preference Optimization of Video Large Multimodal Models from Language Model Reward. Ruohong Zhang, Liangke Gui, Zhiqing Sun, Yihao Feng, Keyang Xu, Yuanhan Zhang, Di Fu, Chunyuan Li, Alexander Hauptmann, Yonatan Bisk, Yiming Yang, arXiv:2404.01258April 2024j</p>
<p>Improve Vision Language Model Chain-of-thought Reasoning. Ruohong Zhang, Bowen Zhang, Yanghao Li, Haotian Zhang, Zhiqing Sun, Zhe Gan, Yinfei Yang, arXiv:2410.16198October 2024kRuoming Pang, and Yiming Yang</p>
<p>Anton van den Hengel, and Yuan Xue. Open Eyes, Then Reason: Fine-grained Visual Mathematical Understanding in MLLMs. Shan Zhang, Aotian Chen, Yanpeng Sun, Jindong Gu, Yi-Yu Zheng, Piotr Koniusz, Kai Zou, arXiv:2501.06430January 2025</p>
<p>WorldQA: Multimodal World Knowledge in Videos through Long-Chain Reasoning. Yuanhan Zhang, Kaichen Zhang, Bo Li, Fanyi Pu, Christopher Arif Setiadharma, Jingkang Yang, Ziwei Liu, arXiv:2405.03272May 2024l</p>
<p>Motion Mamba: Efficient and Long Sequence Motion Generation. Zeyu Zhang, Akide Liu, Ian Reid, Richard Hartley, Bohan Zhuang, Hao Tang, arXiv:2403.07487August 2024m</p>
<p>Tuning LayerNorm in Attention: Towards Efficient Multi-Modal LLM Finetuning. Bingchen Zhao, Haoqin Tu, Chen Wei, Jieru Mei, Cihang Xie, arXiv:2312.11420December 2023</p>
<p>R1-Omni: Explainable Omni-Multimodal Emotion Recognition with Reinforcement Learning. Jiaxing Zhao, Xihan Wei, Liefeng Bo, arXiv:2503.05379March 2025</p>
<p>DynRefer: Delving into Region-level Multi-modality Tasks via Dynamic Resolution. Yuzhong Zhao, Feng Liu, Yue Liu, Mingxiang Liao, Chen Gong, Qixiang Ye, Fang Wan, arXiv:2405.16071May 2024</p>
<p>Thinking Before Looking: Improving Multimodal LLM Reasoning via Mitigating Visual Hallucination. Haojie Zheng, Tianyang Xu, Hanchi Sun, Shu Pu, Ruoxi Chen, Lichao Sun, arXiv:2411.12591November 2024</p>
<p>VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation. Jinguo Zhu, Xiaohan Ding, Yixiao Ge, Yuying Ge, Sijie Zhao, Hengshuang Zhao, Xiaohua Wang, Ying Shan, arXiv:2312.09251December 2023</p>
<p>Benchmarking and Improving Large Vision-Language Models for Fundamental Visual Graph Understanding and Reasoning. Yingjie Zhu, Xuefeng Bai, Kehai Chen, Yang Xiang, Min Zhang, arXiv:2412.13540December 2024</p>
<p>Video-STaR: Self-Training Enables Video Instruction Tuning with Any Supervision. Xiaohan Orr Zohar, Yonatan Wang, Idan Bitton, Serena Szpektor, Yeung-Levy, arXiv:2407.06189July 2024</p>
<p>Look Twice Before You Answer: Memory-Space Visual Retracing for Hallucination Mitigation in Multimodal Large Language Models. Xin Zou, Yizhou Wang, Yibo Yan, Sirui Huang, Kening Zheng, Junkai Chen, Chang Tang, Xuming Hu, arXiv:2410.03577October 2024a</p>
<p>Interfacing Foundation Models' Embeddings. Xueyan Zou, Linjie Li, Jianfeng Wang, Jianwei Yang, Mingyu Ding, Junyi Wei, Zhengyuan Yang, Feng Li, Hao Zhang, Shilong Liu, Arul Aravinthan, Yong Jae Lee, Lijuan Wang, Hu, arXiv:2312.07532July 2024b. 2025. 2025. 2024b. 2024. 2024b. 2024f. 2024a. 2024. 2024k. 2024. 2024b. 2024. 2024e. 2024e. 2025. 2024. 2024a. 2024. 2024b. 2024a. 2024a. 2023a. 2023Inference Adaptive Inference Guo et. Pi et al.</p>
<p>Multimodal Decomposition Xiang. Reward Models, Guo , Feedback Guo et. Scalability Goyal, Wu &amp; Xie2025. 2025. 2024b. 2024c. 2024. 2024. 2024. 2024b. 2024k. 2025. 2024b. 2025. 2025. 2024d. 2024. 2024b. 2024k. 2024. 2024a. 2024. 2024i. 2024. 2024b. 2024. 2024. 2025. 2025. 2025a. 2024. 2024b. 2024f. 2024. 2024. 2024. 2024a. 2024h. 2024. 2024. 2024b. 2024. 2024a. 2023. 2023. 2025. 2025b. 2024. 2024. 2025. 2024e. 2025. 2024. 2024. 2024. 2024i. 2024a. 2024m. 2024. 2024b. 2024b. 2023. 2023. 2024aSun. Pi et al.</p>
<p>. Search Strategies, Yao , Mahmood et al.2024. 2024b. 2024f. 2025. 2025. 2024. 2024e. 2024e. 2024. 2024. 2024l. 2024. 2024c. 2024. 2023. 2023Wu &amp; XieFigure 4: Comprehensive Overview of Methods and Frameworks focus on test-time compute</p>            </div>
        </div>

    </div>
</body>
</html>
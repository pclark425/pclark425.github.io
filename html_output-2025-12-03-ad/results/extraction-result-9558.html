<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9558 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9558</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9558</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-166.html">extraction-schema-166</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using LLMs or related models to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large collections of scholarly papers, including methods, results, challenges, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-2a1cd0bd878ab77742c2f120223f1a44accdd204</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/2a1cd0bd878ab77742c2f120223f1a44accdd204" target="_blank">Benchmarking Large Language Models for Molecule Prediction Tasks</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This investigation reveals several key insights that LLMs generally lag behind ML models in achieving competitive performance on molecule tasks, particularly when compared to models adept at capturing the geometric structure of molecules, highlighting the constrained ability of LLMs to comprehend graph data.</p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) stand at the forefront of a number of Natural Language Processing (NLP) tasks. Despite the widespread adoption of LLMs in NLP, much of their potential in broader fields remains largely unexplored, and significant limitations persist in their design and implementation. Notably, LLMs struggle with structured data, such as graphs, and often falter when tasked with answering domain-specific questions requiring deep expertise, such as those in biology and chemistry. In this paper, we explore a fundamental question: Can LLMs effectively handle molecule prediction tasks? Rather than pursuing top-tier performance, our goal is to assess how LLMs can contribute to diverse molecule tasks. We identify several classification and regression prediction tasks across six standard molecule datasets. Subsequently, we carefully design a set of prompts to query LLMs on these tasks and compare their performance with existing Machine Learning (ML) models, which include text-based models and those specifically designed for analysing the geometric structure of molecules. Our investigation reveals several key insights: Firstly, LLMs generally lag behind ML models in achieving competitive performance on molecule tasks, particularly when compared to models adept at capturing the geometric structure of molecules, highlighting the constrained ability of LLMs to comprehend graph data. Secondly, LLMs show promise in enhancing the performance of ML models when used collaboratively. Lastly, we engage in a discourse regarding the challenges and promising avenues to harness LLMs for molecule prediction tasks. The code and models are available at https://github.com/zhiqiangzhongddu/LLMaMol.</p>
                <p><strong>Cost:</strong> 0.005</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9558",
    "paper_id": "paper-2a1cd0bd878ab77742c2f120223f1a44accdd204",
    "extraction_schema_id": "extraction-schema-166",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00536925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Benchmarking Large Language Models for Molecule Prediction Tasks</h1>
<p>ZHIQIANG ZHONG, Aarhus University, Denmark<br>KUANGYU ZHOU, Microsoft, China<br>DAVIDE MOTTIN, Aarhus University, Denmark</p>
<p>Large Language Models (LLMs) stand at the forefront of a number of Natural Language Processing (NLP) tasks. Despite the widespread adoption of LLMs in NLP, much of their potential in broader fields remains largely unexplored, and significant limitations persist in their design and implementation. Notably, LLMs struggle with structured data, such as graphs, and often falter when tasked with answering domain-specific questions requiring deep expertise, such as those in biology and chemistry. In this paper, we explore a fundamental question: Can LLMs effectively handle molecule prediction tasks? Rather than pursuing top-tier performance, our goal is to assess how LLMs can contribute to diverse molecule tasks. We identify several classification and regression prediction tasks across six standard molecule datasets. Subsequently, we carefully design a set of prompts to query LLMs on these tasks and compare their performance with existing Machine Learning (ML) models, which include text-based models and those specifically designed for analysing the geometric structure of molecules. Our investigation reveals several key insights: Firstly, LLMs generally lag behind ML models in achieving competitive performance on molecule tasks, particularly when compared to models adept at capturing the geometric structure of molecules, highlighting the constrained ability of LLMs to comprehend graph data. Secondly, LLMs show promise in enhancing the performance of ML models when used collaboratively. Lastly, we engage in a discourse regarding the challenges and promising avenues to harness LLMs for molecule prediction tasks. The code and models are available at https://github.com/zhiqiangzhongddu/LLMaMol.</p>
<p>CCS Concepts: $\cdot$ Computing methodologies $\rightarrow$ Natural language processing; Machine learning approaches.
Additional Key Words and Phrases: Large Language Models, Molecule Tasks, Evaluation, Benchmark</p>
<h2>ACM Reference Format:</h2>
<p>Zhiqiang Zhong, Kuangyu Zhou, and Davide Mottin. 2018. Benchmarking Large Language Models for Molecule Prediction Tasks . In Proceedings of Make sure to enter the correct conference title from your rights confirmation emai (Conference acronym 'XX). ACM, New York, NY, USA, 16 pages. https://doi.org/XXXXXXX.XXXXXXX</p>
<h2>1 INTRODUCTION</h2>
<p>In recent decades, Machine Learning (ML) models have become increasingly prevalent in various real-world applications [6, 11, 49]. Both academia and industry have invested significant efforts in enhancing ML efficacy, aiming towards the realisation of Artificial General Intelligence (AGI) [5]. The remarkable advancements in generative models, such as Large Language Models [4, 10, 33, 38, 48], have ushered in a transformative era in Natural Language Processing (NLP). LLMs demonstrate unparalleled proficiency in comprehending and producing human-like text, proving indispensable in diverse NLP tasks such as machine translation [18], commonsense reasoning [26], and coding tasks [5]. A recent breakthrough known as In-Context Learning (ICL) [29], has further enhanced the adaptability of LLMs by enabling them to acquire task-specific knowledge during inference, reducing the need for extensive fine-tuning [9].</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>While LLMs have showcased their effectiveness across an array of NLP applications, the full extent of their potential in broader fields remains largely unexplored [46]. Notably, LLMs encounter challenges with structured data like graphs and often struggle with domain-specific inquiries, such as those in biology and chemistry [3, 24]. To fill the gap, this paper delves into an essential research question: Can LLMs effectively handle molecule prediction tasks?</p>
<p>To answer this research question, this paper identifies different important tasks, including classification and regression prediction tasks, across six benchmark molecule datasets [22, 42], e.g., ogbg-molbace, ogbg-molbbbp, ogbg-molhiv, ogbg-molesol, ogbg-molfreesolv and ogbg-mollipo. Take a molecule, as illustrated in Figure 1, as an example, it can be represented in different representations, including SMILES string [41] and geometric structure [46]. However, a notable limitation of the existing LLMs is their reliance on unstructured text, rendering them unable to incorporate essential geometric structures as input [15, 28]. To address this challenge, Fatemi et al. [12] propose encoding the graph structure into text descriptions. In this paper, depicted in Figure 1, we extend this method by encoding both the moleculeâ€™s atom features and graph structure into textual descriptions. Subsequently, we carefully design a set of prompts to harness various capabilities (e.g., domain-expertise, ICL capability) of LLMs to generate responses for molecule tasks. Then we evaluate these responses in terms of consistency and performance on downstream tasks and compare them with those generated by existing ML models designed for molecule prediction tasks [19, 47].</p>
<p>The outcomes of our study effectively answered the raised question. Firstly, LLMs demonstrate a shortfall in competitive performance compared to existing ML models, particularly those specifically designed to capture the geometric structure of molecules. While ICL techniques offer notable assistance in improving LLM performance, they still trail behind existing ML models, underscoring the limited capability of current LLMs in directly addressing molecule tasks. Secondly, we delve into the potential of integrating LLM responses with existing ML models, observing significant enhancements in numerous scenarios. We posit that leveraging LLMs as augmenters of domain knowledge currently presents a more effective approach than tasking LLMs with directly answering molecule predictive tasks. In the end, we deliver a series of insightful discussions about limitations and promising avenues of existing LLMs in molecule tasks. We hope this work could shed new insight into the interdisciplinary framework design of molecule tasks empowered by LLMs.</p>
<p>The rest of this paper is organised as follows. We begin by briefly reviewing related work in Section 2. Afterwards, in Section 3, we introduce the preliminaries of this study and include methodologies for molecule prediction tasks. Experimental results are shown in Section 4. Finally, we discuss the limitations and future work and conclude the paper in Section 5.</p>
<h1>2 RELATED WORK</h1>
<p>Large Language Models. Traditional language models are typically trained on sequences of tokens, learning the likelihood of the next token dependent on the previous tokens [38]. Recently, Brown et al. [4] demonstrated that increasing the size of language models and the amount of training data can result in new capabilities, such as zero-shot generalisation, where models can perform text-based tasks without specific task-oriented training data. Consequently, Large Language Models (LLMs), such as GPT-3 [4], GPT-4 [32], Flan-T5 [8], Galactica [35], Llama [37] and Gemini [36], have experienced exponential growth in both size and capability in recent years [1]. A wide range of NLP applications have been reshaped by LLMs, including machine translation [18], commonsense reasoning [26] and coding tasks [5]. While the impressive performance and generalisation capabilities of language models have rendered them highly effective across various tasks [39], they have also resulted in larger model parameters and increased computational costs for additional fine-tuning on new downstream tasks [20]. To address this challenge, recent research has introduced</p>
<p>In-Context Learning (ICL), enabling LLMs to excel at new tasks by incorporating a few task samples directly into the prompt [29]. Despite their widespread use in NLP, their potential in broader fields remains largely unexplored. Thus, we conduct a comprehensive empirical analysis to evaluate the capability of LLMs in molecular prediction tasks.</p>
<p>Large Language Models Evaluation. In recent years, due to LLMs' great performance in handling different applications such as general natural language tasks and domain-specific ones, the evaluation of LLMs has become a significant field of inquiry [7]. Gruver et al. [14] showed ChatGPT's proficiency in time-series prediction; Hendy et al. [18] evaluate the performance of LLMs are machine translators, while technical aspects of GPT-4 were analysed in [32]. In the context of mathematical problem-solving, Frieder et al. [13] have highlighted that LLMs encounter challenges with graduate-level problems, primarily due to difficulties in parsing complex syntax. Specifically, in healthcare, the utility and safety of LLMs in clinical settings were explored [31]; Yin et al. [45] explores the applications of the GPTs in general bioinformatics research, including gene and protein named entities extraction, identifying potential coding regions, etc.</p>
<p>Large Language Models for Molecular Tasks. Recent efforts integrating LLMs with the field of molecules mainly focus on harnessing the LLM's text-processing capabilities. For instance, Guo et al. [16] established a comprehensive benchmark comprising eight practical chemistry tasks, specifically designed to gauge the performance of LLMs, including GPT-4 and GPT-3.5, across each task. Zhong et al. [50] harness the LLMs as post-hoc correctors to improve the ML model's molecule property predictions after completing training. Additionally, Qian et al. [34] utilised LLMs to generate explanations for molecule SMILES strings to facilitate predictions, while Jablonka et al. [24] fine-tuned a GPT-3 model for addressing new chemical inquiries. This paper presents a comprehensive to evaluate the LLM's capability to handle both text and graph-structure information of molecules for prediction tasks. Furthermore, we showcase a straightforward yet efficacious framework for integrating LLMs with existing ML models.</p>
<h1>3 PRELIMINARY AND METHODOLOGY</h1>
<p>This paper aims to evaluate the capabilities of LLMs in handling challenging prediction tasks on structured molecule data within the field of biology. For a given molecule, we can represent it using various formats such as SMILES (Simplified molecule Input Line Entry System) string [41] and geometric structures [46] (as shown in Figure 1). However, a notable limitation of existing LLMs is their reliance on unstructured text, rendering them unable to incorporate essential geometric structures as input [15, 28]. To overcome this limitation, Fatemi et al. [12] propose encoding the graph structure into text descriptions, and they have verified the effectiveness of such a design in various graph reasoning tasks. In this paper, as depicted in Figure 1, we extend this method by encoding both the molecule's atom features and graph structure into textual descriptions since the molecule's atom features are important for different prediction tasks $[21,49]$.</p>
<h3>3.1 Problem Setup</h3>
<p>Notion. Given a molecule, we formally represent it as $\mathcal{G}=(S, G, D)$, where $S$, and $G$ denote the SMILES string, and geometric structure. $D$ indicates the generated atom feature and graph structure descriptions of $\mathcal{G}$, as illustrated in Figure 1. $y \in \mathcal{Y}$ stands for the label for $\mathcal{G}$.</p>
<p>Problem Setup. Given a set of molecules $\mathcal{M}=\left{\mathcal{G}<em 2="2">{1}, \mathcal{G}</em>}, \ldots, \mathcal{G<em _mathcal_T="\mathcal{T">{m}\right}$, where $\mathcal{M}</em>}} \subset \mathcal{M}$ contains molecules with known labels $y_{u}$ for all $\mathcal{G<em _mathcal_T="\mathcal{T">{u} \in \mathcal{M}</em>}}$. Our objective is to predict the unknown labels $y_{u}$ for all $\mathcal{G<em _text="{text">{u} \in \mathcal{M}</em>}}$, where $\mathcal{M<em _mathcal_T="\mathcal{T">{\text {text }}=\mathcal{M} \backslash \mathcal{M}</em>}}$. In addition, $\mathcal{M<em _text="\text" _train="{train">{\mathcal{T}}$ is split into two subsets: $\mathcal{M}</em>}}$ and $\mathcal{M<em _text="\text" _train="{train">{\text {val }}$, where $\mathcal{M}</em>$ works as}}$ is the training set and $\mathcal{M}_{\text {val }</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1. A molecule can be represented in different forms, e.g., SMILES string, text description and geometric structure.
the validation set. This separation allows us to fine-tune model parameters, mitigating overfitting, and validate the effectiveness of Machine Learning (ML) models before applying it to test dataset $\mathcal{M}$ test.</p>
<p>ML Models for Molecule Prediction Tasks. The conventional approach to tackle molecule property prediction tasks is employing ML models. Take the supervised molecule property prediction task as an example. The goal is to learn a mapping function $f_{M L}: \mathcal{M} \rightarrow \mathcal{\mathcal { H }}$, by minimising loss function value $\min <em i="1">{\Theta} \sum</em>}^{n} \mathcal{L}\left(\hat{\mathcal{Y}<em _text="\text" _train="{train">{\text {train }}^{i}, \mathcal{Y}</em>}}^{i}\right)$, where $\Theta$ represents the set of trainable parameters of $f_{M L}$. Subsequently, $f_{M L}$ can be employed on test dataset $\mathcal{M<em _test="{test" _text="\text">{\text {test }}$ to generate predictions $\hat{\mathcal{Y}}</em>$.}</p>
<h1>3.2 Prompt Engineering</h1>
<p>The goal in prompt engineering is to find the correct way to formulate a question $\mathcal{Q}$ in such a way that an $\operatorname{LLM}\left(f_{L L M}\right)$ will return the corresponding answer $A$ essentially represented as $A=f_{L L M}(\mathcal{Q})$. In this work, our goal is to provide the LLM with helpful and comprehensive knowledge regarding molecules so that it can make predictions on the test dataset. A variety of approaches exist for modifying the $f_{L L M}$ so that it could better perform downstream tasks such as fine-tuning [9] and LoRA [20]. However, these methods typically require access to the internals of the model and heavy computation capability, which can limit their applicability in many real-world scenarios. In this work, we are instead interested in the case where $f_{L L M}$ and its parameters are fixed, and the system is available only for users in a black box setup where $f_{L L M}$ only consumes and produces text. We believe this setting to be particularly valuable as the number of proprietary models available and their hardware demands increase.</p>
<h2>Box 1. Input-Feature (IF) Prompt Example</h2>
<div class="codehilite"><pre><span></span><code><span class="nt">&lt;Instruction&gt;</span>
You<span class="w"> </span>are<span class="w"> </span>an<span class="w"> </span>expert<span class="w"> </span>in<span class="w"> </span>biomedicine<span class="w"> </span>and<span class="w"> </span>chemistry,<span class="w"> </span>specializing<span class="w"> </span>in<span class="w"> </span>molecules.
<span class="nt">&lt;/Instruction&gt;</span>
<span class="nt">&lt;Question&gt;</span>
Provide<span class="w"> </span>detailed<span class="w"> </span>insights<span class="w"> </span>on<span class="w"> </span>the<span class="w"> </span>molecule<span class="w"> </span>with<span class="w"> </span>the<span class="w"> </span>SMILES<span class="w"> </span>string<span class="w"> </span>{SMILES<span class="w"> </span>STRING}.<span class="w"> </span>{DESCRIPTION}
I<span class="w"> </span>am<span class="w"> </span>particularly<span class="w"> </span>interested<span class="w"> </span>in<span class="w"> </span>understanding<span class="w"> </span>whether<span class="w"> </span>the<span class="w"> </span>molecule<span class="w"> </span>{TASK}.
<span class="nt">&lt;/Question&gt;</span>
</code></pre></div>

<h1>Box 2. Input-Prediction (IP) Prompt Example</h1>
<div class="codehilite"><pre><span></span><code><span class="nt">&lt;Instruction&gt;</span>
You<span class="w"> </span>are<span class="w"> </span>an<span class="w"> </span>expert<span class="w"> </span>in<span class="w"> </span>biomedicine<span class="w"> </span>and<span class="w"> </span>chemistry,<span class="w"> </span>specializing<span class="w"> </span>in<span class="w"> </span>molecules.
<span class="nt">&lt;/Instruction&gt;</span>
<span class="nt">&lt;Question&gt;</span>
The<span class="w"> </span>SMILES<span class="w"> </span>string<span class="w"> </span>of<span class="w"> </span>molecule-{ID}<span class="w"> </span>is<span class="w"> </span>{SMILES<span class="w"> </span>STRING}.<span class="w"> </span>{DESCRIPTION}
Predict<span class="w"> </span>whether<span class="w"> </span>molecule-{ID}<span class="w"> </span>{TASK}.
Answer<span class="w"> </span>the<span class="w"> </span>question<span class="w"> </span>in<span class="w"> </span>the<span class="w"> </span>format:<span class="w"> </span>Prediction:<span class="w"> </span><span class="nt">&lt;True</span><span class="w"> </span><span class="err">or</span><span class="w"> </span><span class="err">False</span><span class="nt">&gt;</span>.
<span class="nt">&lt;/Question&gt;</span>
</code></pre></div>

<h2>Box 3. Input-Explanation (IE) Prompt Example</h2>
<div class="codehilite"><pre><span></span><code><span class="nt">&lt;Instruction&gt;</span>
You<span class="w"> </span>are<span class="w"> </span>an<span class="w"> </span>expert<span class="w"> </span>in<span class="w"> </span>biomedicine<span class="w"> </span>and<span class="w"> </span>chemistry,<span class="w"> </span>specializing<span class="w"> </span>in<span class="w"> </span>molecules.
<span class="nt">&lt;/Instruction&gt;</span>
<span class="nt">&lt;Question&gt;</span>
The<span class="w"> </span>SMILES<span class="w"> </span>string<span class="w"> </span>of<span class="w"> </span>molecule-{ID}<span class="w"> </span>is<span class="w"> </span>{SMILES<span class="w"> </span>STRING}.<span class="w"> </span>{DESCRIPTION}
Predict<span class="w"> </span>whether<span class="w"> </span>molecule-{ID}<span class="w"> </span>{TASK}.
Answer<span class="w"> </span>the<span class="w"> </span>question<span class="w"> </span>in<span class="w"> </span>the<span class="w"> </span>format:<span class="w"> </span>Prediction:<span class="w"> </span><span class="nt">&lt;True</span><span class="w"> </span><span class="err">or</span><span class="w"> </span><span class="err">False</span><span class="nt">&gt;</span>;<span class="w"> </span>Explanation:<span class="w"> </span><span class="nt">&lt;text&gt;</span>.
<span class="nt">&lt;/Question&gt;</span>
</code></pre></div>

<p>Zero-shot Prompting. To this end, the first set of prompts (IF, IP, IE) simply provide the LLM with molecule SMILES string $S$ and descriptions $D$ and asks it to generate the desired output with a desired format without any prior training or knowledge on the task, as illustrated in Box 1, Box 2 and Box 3 [4]. The only guidance we provide to the LLM is instruction, which tells about a little background context. Particularly, IF asks the LLM to provide meaningful insights that might be helpful for the prediction task [34]. IP only asks the LLM to provide predictions about the molecule's properties, while IE further asks for explanations, which may require the LLM to clarify the thought process in explanation generation and provide helpful evidence to help users understand the given prediction. In addition, if we fill out the description of IF, IP and IE, which derives IFD, IPD and IED prompts. description provides more comprehensive information about the molecule graph's features and structure information, yet it introduces a significant number of tokens, which can affect the LLM's answer consistency and over the constraints of LLMs. We will address corresponding details in Section 4.</p>
<h2>Box 4. Few-shot (FS) Prompt Example</h2>
<div class="codehilite"><pre><span></span><code><span class="nt">&lt;Instruction&gt;</span>
You<span class="w"> </span>are<span class="w"> </span>an<span class="w"> </span>expert<span class="w"> </span>in<span class="w"> </span>biomedicine<span class="w"> </span>and<span class="w"> </span>chemistry,<span class="w"> </span>specializing<span class="w"> </span>in<span class="w"> </span>molecules.
<span class="nt">&lt;/Instruction&gt;</span>
<span class="nt">&lt;Knowledge&gt;</span>
<span class="nt">&lt;Question&gt;</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>The<span class="w"> </span>SMILES<span class="w"> </span>string<span class="w"> </span>of<span class="w"> </span>molecule-{ID}<span class="w"> </span>is<span class="w"> </span>{SMILES<span class="w"> </span>STRING}.<span class="w"> </span>{DESCRIPTION}
Predict<span class="w"> </span>whether<span class="w"> </span>molecule-{ID}<span class="w"> </span>{TASK}.
<span class="nt">&lt;/Question&gt;</span>
<span class="nt">&lt;Response&gt;</span>
Molecule-{ID}<span class="w"> </span>{ANSWER}.
//Example<span class="w"> </span>classification<span class="w"> </span>ANSWER<span class="w"> </span>is<span class="w"> </span>&quot;can<span class="w"> </span>inhibit<span class="w"> </span>human<span class="w"> </span>BACE-1.&quot;
<span class="nt">&lt;/Response&gt;</span>
...
<span class="nt">&lt;/Knowledge&gt;</span>
<span class="nt">&lt;Question&gt;</span>
The<span class="w"> </span>SMILES<span class="w"> </span>string<span class="w"> </span>of<span class="w"> </span>molecule-{ID}<span class="w"> </span>is<span class="w"> </span>{SMILES<span class="w"> </span>STRING}.<span class="w"> </span>{DESCRIPTION}
Predict<span class="w"> </span>whether<span class="w"> </span>molecule-{ID}<span class="w"> </span>{TASK}.
Answer<span class="w"> </span>the<span class="w"> </span>question<span class="w"> </span>in<span class="w"> </span>the<span class="w"> </span>format:<span class="w"> </span>Prediction:<span class="w"> </span><span class="nt">&lt;True</span><span class="w"> </span><span class="err">or</span><span class="w"> </span><span class="err">False</span><span class="nt">&gt;</span>;<span class="w"> </span>Explanation:<span class="w"> </span><span class="nt">&lt;text&gt;</span>.
<span class="nt">&lt;/Question&gt;</span>
</code></pre></div>

<p>Few-shot Prompting. The second set of prompts (FS) that we propose provides the LLM with a small number of examples of the task, along with the desired outputs [4]. The LLM then learns from these examples to perform the task on new inputs. This approach can be categorised as a simple In-Context Learning (ICL) technique, An example prompt template is shown in Box 4. FS- $k$ indicates $k$ contextual knowledge instances are included in the prompt. In this work, we do not discuss the FSD prompts since the generated descriptions have tons of tokens, which will easily go over the LLM's input constraints.</p>
<p>We note there are also some popular recent ICL techniques, e.g., Chain-of-thought (CoT) [40], Tree-of-thought (ToT) [44], Graph-of-thought (GoT) [2] and Retrieval Augmented Generation (RaG) [27], which are theoretically available to support complicated tasks and include large knowledge context. However, our initial experiments showed that methods, e.g., CoT, ToT and GoT, perform much worse for molecule property prediction tasks due to the significant difficulties in designing proper chain thoughts without solid expertise. RaG implementations that we tested are unstable and slow with query, and they fall short of the relatively simpler FS's performance. We argue it is caused by the unqualified information retrieval system, and we will discuss it in the future work discussion section.</p>
<h1>3.3 Predictive Models</h1>
<p>To generate predictions on target molecules $\mathcal{M}_{\text {text }}$, this section presents our predictive models, including Large Language Model (LLM) [48], Language Model (LM) [19] and Graph Neural Network (GNN) [47] based approaches, which can capture comprehensive molecule information. In the following section, we will discuss their details.</p>
<p>LLM-based Approach. The LLM-based approaches take generated prompts, which follow the templates as discussed in Section 3.2, as inputs, generating answers $R$ following the given format: $R=f_{L L M}(\mathcal{Q})$. In addition, depending on the number of input information, we categorise them as $\mathrm{LLM}<em _Duo="{Duo" _text="\text">{\text {SoLo }}$ and $\mathrm{LLM}</em>}}$ approaches, as shown in Figure 2 Particularly, $\mathrm{LLM<em _Duo="{Duo" _text="\text">{\text {SoLo }}$ takes queries based on IF, IP, IE and FS templates as input, $\mathrm{LLM}</em>$ takes queries based on IFD, IPD and IED templates as input.}</p>
<p>LM-based Approach. LMs can generate predictions based on available textual information [19], e.g., SMILES string $S$, descriptions $D$ and the response $R$ provided by the LLM model. Because our empirical studies find that the performances of LM models utilising descriptions $D$ are not competitive compared with other settings. Therefore, as illustrated in</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2. An overview of our pipelines for different predictive models.</p>
<p>Figure 2, this work adopts two designs, i.e., only taking SMILES string as input ( $\mathrm{LM}<em _Duo="{Duo" _text="\text">{\text {Solo }}$ ) and taking SMILES string and responses provided by the LLM as input $\left(\mathrm{LM}</em>(S, R)[10]$.}}\right): \hat{\boldsymbol{y}}=f_{L M</p>
<p>GNN-based Approach. GNN models are state-of-the-art approaches to molecule property prediction tasks since they are highly effective in capturing different essential geometric structure information $G$ of molecules. In addition, with the assistance of LMs, available textual information can also be converted into additional features $(X)$ and fed into the GNN models afterwards: $\hat{\boldsymbol{y}}=f_{G N N}(G, X)$. In particular, as shown in Figure 3, benefits from the LM's flexibility in converting textual information into embeddings, which empowers GNN models with flexibility in incorporating information from different perspectives. This work adopts three design, i.e., $\mathrm{GNN}<em _Duo="{Duo" _text="\text">{\text {Solo }}, \mathrm{GNN}</em>$, as illustrated in Figure 2.
}}$ and $\mathrm{GNN}_{\text {Trio }<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3. An illustration of our frameworks for different predictive models.</p>
<h1>4 EXPERIMENTS</h1>
<p>This section presents our empirical studies and analysis in evaluating the effectiveness of LLMs in molecule prediction tasks. Our experimental analysis focuses on the challenging molecule graph property prediction tasks. We will present the experimental setup in Section 4.1, then demonstrate and discuss the experimental results in Section 4.2.</p>
<h3>4.1 Experiment Setup</h3>
<p>Table 1. Datasets statistics and splits from benchmark [22, 42].</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: right;">#Graphs</th>
<th style="text-align: right;">Avg. <br> #Nodes</th>
<th style="text-align: right;">Avg. <br> #Edges</th>
<th style="text-align: right;">#Train</th>
<th style="text-align: right;">#Valid</th>
<th style="text-align: right;">#Test</th>
<th style="text-align: right;">Task Type</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ogbg-molbace [42]</td>
<td style="text-align: right;">1,513</td>
<td style="text-align: right;">34.1</td>
<td style="text-align: right;">73.7</td>
<td style="text-align: right;">1,210</td>
<td style="text-align: right;">151</td>
<td style="text-align: right;">152</td>
<td style="text-align: right;">Binary class.</td>
</tr>
<tr>
<td style="text-align: left;">ogbg-molbbbp [42]</td>
<td style="text-align: right;">2,039</td>
<td style="text-align: right;">24.1</td>
<td style="text-align: right;">51.9</td>
<td style="text-align: right;">1,631</td>
<td style="text-align: right;">204</td>
<td style="text-align: right;">204</td>
<td style="text-align: right;">Binary class.</td>
</tr>
<tr>
<td style="text-align: left;">ogbg-molhiv [22, 42]</td>
<td style="text-align: right;">41,127</td>
<td style="text-align: right;">25.5</td>
<td style="text-align: right;">27.5</td>
<td style="text-align: right;">32,901</td>
<td style="text-align: right;">4,113</td>
<td style="text-align: right;">4,113</td>
<td style="text-align: right;">Binary class.</td>
</tr>
<tr>
<td style="text-align: left;">ogbg-molesol [42]</td>
<td style="text-align: right;">1,128</td>
<td style="text-align: right;">13.3</td>
<td style="text-align: right;">27.4</td>
<td style="text-align: right;">902</td>
<td style="text-align: right;">113</td>
<td style="text-align: right;">113</td>
<td style="text-align: right;">Regression</td>
</tr>
<tr>
<td style="text-align: left;">ogbg-molfreesolv [42]</td>
<td style="text-align: right;">642</td>
<td style="text-align: right;">8.7</td>
<td style="text-align: right;">16.8</td>
<td style="text-align: right;">513</td>
<td style="text-align: right;">64</td>
<td style="text-align: right;">65</td>
<td style="text-align: right;">Regression</td>
</tr>
<tr>
<td style="text-align: left;">ogbg-mollipo [42]</td>
<td style="text-align: right;">4,200</td>
<td style="text-align: right;">27.0</td>
<td style="text-align: right;">59.0</td>
<td style="text-align: right;">3,360</td>
<td style="text-align: right;">420</td>
<td style="text-align: right;">420</td>
<td style="text-align: right;">Regression</td>
</tr>
</tbody>
</table>
<p>Dataset. We consider six benchmark molecule property prediction datasets that are common within ML research, including ogbg-molbace, ogbg-molbbbp, ogbg-molhiv, ogbg-molesol, ogbg-molfreesolv and ogbg-mollipo. The collected datasets are summarised in Table 1. Their detailed descriptions are as follow:
(1) ogbg-molbace. The ogbg-molbace dataset offers both quantitative $\left(\mathrm{IC}_{50}\right)$ and qualitative (binary label) binding results for a series of inhibitors targeting human $\beta$-secretase 1 (BACE-1). These data comprise experimental values sourced from scientific literature spanning the last decade, including instances accompanied by detailed crystal structures. MoleculeNet [42] has amalgamated a repository of 1,522 compounds, incorporating their 2D structures alongside binary labels, formulated as a classification challenge.
(2) ogbg-molbbbp. The Blood-Brain Barrier Penetration (BBBP) dataset originates from scientific investigations focused on modelling and forecasting barrier permeability. The blood-brain barrier functions as a membrane that separates circulating blood from brain extracellular fluid. It obstructs the majority of drugs, hormones, and neurotransmitters. Consequently, traversing this barrier has posed a persistent challenge in drug development aimed at the central nervous system. This dataset encompasses binary labels assigned to over 2,039 compounds based on their permeability characteristics. Scaffold splitting is also advised for this clearly defined target.
(3) ogbg-molhiv. The HIV dataset originated from the Drug Therapeutics Program (DTP) AIDS Antiviral Screen, which assessed the capacity to hinder HIV replication across 41,127 compounds. Screening outcomes were categorised into three groups: confirmed inactive (CI), confirmed active (CA), and confirmed moderately active (CM). We subsequently amalgamated the latter two labels, transforming them into a classification task distinguishing between inactive (CI) and active (CA and CM) compounds. Since our focus is on uncovering novel categories of HIV inhibitors, scaffold splitting is advised for this dataset.
(4) ogbg-molesol. The ESOL dataset comprises water solubility data for 1,128 compounds. It has been employed to train models aimed at estimating solubility directly from chemical structures encoded in SMILES strings. Notably, these structures lack 3D coordinates, as solubility pertains to the molecule itself rather than its specific conformers.</p>
<p>(5) ogbg-molfreesolv. The Free Solvation Database (FreeSolv) offers both experimental and calculated hydrationfree energies of small molecules in water. A subset of these compounds is also utilised in the SAMPL blind prediction challenge. The calculated values are obtained through alchemical free energy calculations employing molecular dynamics simulations. While the experimental values are incorporated into the benchmark collection, the calculated values are utilised for comparison purposes.
(6) ogbg-mollipo. Lipophilicity stands as a crucial characteristic of drug molecules, influencing membrane permeability and solubility alike. Sourced from the ChEMBL database [30], this dataset furnishes experimental findings on the octanol/water distribution coefficient (log D at pH 7.4 ) for 4200 compounds.</p>
<p>ML Models. To investigate the effectiveness of LLMs on molecule prediction tasks. We consider ML models of two different categories: (1) Language Model (LM) that only takes text information as inputs, i.e., DeBERTa [17]. (2) Graph Neural Networks (GNNs) that capture the molecule's geometric structure information and other available features. We consider two classic GNN variants, i.e., GCN [25] and GIN [43]. Their frameworks are illustrated in Figure 3.</p>
<p>LLMs. In this work, we are interested in where the LLM's parameters are fixed, and the system is available for users in a black box setup where the LLM only consumes and produces text. We believe this setting to be particularly valuable as most users would practically have access to LLMs. In this case, we consider Llama-2-7b [37], Llama-2-13b [37], GPT-3.5 and GPT-4 [1] as LLMs in this work, and GPT-3.5 is the major LLM for most experiments. The reasoning for this choice will be addressed in Section 4.2. All responses are obtained by calling their official APIs or their official implementation on https://huggingface.co. Because the generated descriptions following [12] have tons of tokens, easily over the LLM's input token constraints, hence we do not include descriptions in the FS prompt in this study.</p>
<p>Implementation. We implement ML predictive models following their available official implementations. For instance, we adopt the available code of variant GNN models on the OGB benchmark leaderboards, e.g., GCN ${ }^{1}$ and GIN ${ }^{2}$. About DeBERTa, we adopt its official implementation ${ }^{3}$ and incorporate it within our pipeline. For the LLMs, we simply call the API provided by OpenAI or the official implementation with default hyper-parameter settings. We empirically tried with some combinations of recommended important hyper-parameters, e.g., temperature and top_P, yet did not observe significant improvement.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4. Overview of the evaluation process.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Evaluation Process and Setting. Our evaluation process workflow is illustrated in Figure 4. In addition to the conventional evaluation workflow, which assesses the model's performance on downstream tasks, we also analyse the Language Model's (LLM) response consistency. Given that LLMs may produce knowledge hallucinations [23], generating responses that deviate from users' expectations, we calculate the ratio of LLM responses conforming to the required format, termed as response consistency. To ensure fair comparisons, we adopt the fixed splits provided by Hu et al. [21] in this paper. These splits guarantee consistency in evaluation conditions across different experiments, thereby facilitating meaningful comparisons between models.</p>
<h1>4.2 Performance</h1>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 5. Molecule graph property prediction performance of different LLMs for the ogbg-molhiv dataset.</p>
<p>Observation 1: GPT models consistently outperform other LLMs in handling molecule prediction tasks. In our initial investigations, we conduct a series of experiments to assess the efficacy of various LLMs on the ogbg-molhiv dataset. We employ generated prompts following the templates outlined in Section 3.2, encompassing IP, IPD, IE, IED, FS-1, FS-2, and FS-3. Figure 5 presents our findings, demonstrating a consistent trend wherein GPT models exhibit superior performance compared to the Llama models across all evaluated metrics. This observation suggests that GPT models possess enhanced capabilities for handling molecule prediction tasks. Furthermore, it's noteworthy that utilising the GPT-4 API incurs a cost that is 20 times higher than employing GPT-3.5. Additionally, our experiments reveal that the response time of the GPT-4 API is 10 times slower than that of GPT-3.5. Consequently, considering both performance and computational efficiency, we opt to employ GPT-3.5 as our default LLM for subsequent experiments in this study.</p>
<p>Observation 2: LLMs are not potent experts for molecule prediction tasks. Analysing the performance of LLMs in molecule graph property prediction across six datasets as shown in Table 2, it becomes apparent that LLMs consistently underperform compared to the three ML models. This trend suggests that relying on LLMs as experts for molecule prediction tasks may not yield satisfactory results. Further exploration is warranted to understand the limitations of LLMs in this domain and to explore alternative approaches for improving prediction accuracy.</p>
<p>Table 2. Molecule graph property prediction performance for the ogbg-molbace, ogbg-molbbbp, ogbg-molhiv, ogbg-molesol, ogbg-molfreesolv and ogbg-mollipo datasets. LM and GNN models follow the SoLo pipeline. Classification tasks are evaluated on ROC-AUC ( $\uparrow$ : higher is better), and regression tasks are evaluated on RMSE ( $\downarrow$ : lower is better). The best test performance of LLM is marked with underline.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">ogbg-molbace</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ogbg-molbbbp</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ogbg-molhiv</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ogbg-molesol</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ogbg-molfreesolv</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ogbg-mollipo</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ROC-AUC $\uparrow$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">RMSE $\downarrow$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Valid</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">Valid</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">Valid</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">Valid</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">Valid</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">Valid</td>
<td style="text-align: center;">Test</td>
</tr>
<tr>
<td style="text-align: center;">LLM ${ }^{\text {IP }}$</td>
<td style="text-align: center;">0.5690</td>
<td style="text-align: center;">0.5756</td>
<td style="text-align: center;">0.4606</td>
<td style="text-align: center;">0.5399</td>
<td style="text-align: center;">0.5519</td>
<td style="text-align: center;">0.5892</td>
<td style="text-align: center;">2.6221</td>
<td style="text-align: center;">2.0422</td>
<td style="text-align: center;">6.1699</td>
<td style="text-align: center;">4.4421</td>
<td style="text-align: center;">1.9836</td>
<td style="text-align: center;">1.8411</td>
</tr>
<tr>
<td style="text-align: center;">LLM ${ }^{\text {IPD }}$</td>
<td style="text-align: center;">0.4835</td>
<td style="text-align: center;">0.5534</td>
<td style="text-align: center;">0.4643</td>
<td style="text-align: center;">0.4664</td>
<td style="text-align: center;">0.4732</td>
<td style="text-align: center;">0.5693</td>
<td style="text-align: center;">3.7395</td>
<td style="text-align: center;">3.1721</td>
<td style="text-align: center;">8.1598</td>
<td style="text-align: center;">7.2877</td>
<td style="text-align: center;">2.6464</td>
<td style="text-align: center;">2.5046</td>
</tr>
<tr>
<td style="text-align: center;">LLM ${ }^{\text {IE }}$</td>
<td style="text-align: center;">0.4769</td>
<td style="text-align: center;">0.5220</td>
<td style="text-align: center;">0.4463</td>
<td style="text-align: center;">0.5237</td>
<td style="text-align: center;">0.5487</td>
<td style="text-align: center;">0.5419</td>
<td style="text-align: center;">2.1055</td>
<td style="text-align: center;">2.5549</td>
<td style="text-align: center;">5.9059</td>
<td style="text-align: center;">4.3097</td>
<td style="text-align: center;">2.1044</td>
<td style="text-align: center;">1.9158</td>
</tr>
<tr>
<td style="text-align: center;">LLM ${ }^{\text {IED }}$</td>
<td style="text-align: center;">0.5299</td>
<td style="text-align: center;">0.4761</td>
<td style="text-align: center;">0.4742</td>
<td style="text-align: center;">0.4091</td>
<td style="text-align: center;">0.5361</td>
<td style="text-align: center;">0.5512</td>
<td style="text-align: center;">3.9001</td>
<td style="text-align: center;">4.2289</td>
<td style="text-align: center;">7.4837</td>
<td style="text-align: center;">5.3689</td>
<td style="text-align: center;">2.4191</td>
<td style="text-align: center;">2.4219</td>
</tr>
<tr>
<td style="text-align: center;">LLM ${ }^{\text {FS-1 }}$</td>
<td style="text-align: center;">0.4822</td>
<td style="text-align: center;">0.5122</td>
<td style="text-align: center;">0.5955</td>
<td style="text-align: center;">0.4954</td>
<td style="text-align: center;">0.5229</td>
<td style="text-align: center;">0.5268</td>
<td style="text-align: center;">1.7699</td>
<td style="text-align: center;">2.8762</td>
<td style="text-align: center;">6.4785</td>
<td style="text-align: center;">4.7553</td>
<td style="text-align: center;">1.9810</td>
<td style="text-align: center;">1.8432</td>
</tr>
<tr>
<td style="text-align: center;">LLM ${ }^{\text {FS-2 }}$</td>
<td style="text-align: center;">0.4277</td>
<td style="text-align: center;">0.6090</td>
<td style="text-align: center;">0.6019</td>
<td style="text-align: center;">0.5075</td>
<td style="text-align: center;">0.5619</td>
<td style="text-align: center;">0.5731</td>
<td style="text-align: center;">1.9271</td>
<td style="text-align: center;">2.1020</td>
<td style="text-align: center;">5.5078</td>
<td style="text-align: center;">4.5606</td>
<td style="text-align: center;">1.9138</td>
<td style="text-align: center;">1.8118</td>
</tr>
<tr>
<td style="text-align: center;">LLM ${ }^{\text {FS-3 }}$</td>
<td style="text-align: center;">0.5405</td>
<td style="text-align: center;">0.5949</td>
<td style="text-align: center;">0.6000</td>
<td style="text-align: center;">0.5388</td>
<td style="text-align: center;">0.5475</td>
<td style="text-align: center;">0.5616</td>
<td style="text-align: center;">1.9548</td>
<td style="text-align: center;">1.9963</td>
<td style="text-align: center;">6.3753</td>
<td style="text-align: center;">4.7241</td>
<td style="text-align: center;">1.8291</td>
<td style="text-align: center;">1.7923</td>
</tr>
<tr>
<td style="text-align: center;">LM</td>
<td style="text-align: center;">0.5584</td>
<td style="text-align: center;">0.6163</td>
<td style="text-align: center;">0.9307</td>
<td style="text-align: center;">0.6727</td>
<td style="text-align: center;">0.5305</td>
<td style="text-align: center;">0.5037</td>
<td style="text-align: center;">2.1139</td>
<td style="text-align: center;">2.2549</td>
<td style="text-align: center;">6.6189</td>
<td style="text-align: center;">4.4532</td>
<td style="text-align: center;">1.2095</td>
<td style="text-align: center;">1.1066</td>
</tr>
<tr>
<td style="text-align: center;">GCN</td>
<td style="text-align: center;">0.7879</td>
<td style="text-align: center;">0.7147</td>
<td style="text-align: center;">0.9582</td>
<td style="text-align: center;">0.6707</td>
<td style="text-align: center;">0.8461</td>
<td style="text-align: center;">0.7376</td>
<td style="text-align: center;">0.8538</td>
<td style="text-align: center;">1.2561</td>
<td style="text-align: center;">2.8275</td>
<td style="text-align: center;">2.5096</td>
<td style="text-align: center;">0.6985</td>
<td style="text-align: center;">0.7201</td>
</tr>
<tr>
<td style="text-align: center;">GIN</td>
<td style="text-align: center;">0.8012</td>
<td style="text-align: center;">0.7673</td>
<td style="text-align: center;">0.9608</td>
<td style="text-align: center;">0.6708</td>
<td style="text-align: center;">0.8406</td>
<td style="text-align: center;">0.7601</td>
<td style="text-align: center;">0.8010</td>
<td style="text-align: center;">0.9555</td>
<td style="text-align: center;">2.2106</td>
<td style="text-align: center;">2.1610</td>
<td style="text-align: center;">0.6482</td>
<td style="text-align: center;">0.7019</td>
</tr>
</tbody>
</table>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Fig. 6. Response consistency for the ogbg-molbace, ogbg-molbbbp, ogbg-molhiv, ogbg-molesol, ogbg-molfreesolv and ogbg-mollipo datasets.</p>
<p>Observation 3: Descriptions ( $D$ ) do not help LLMs to understand molecule geometry structure. One significant constraint of current LLMs is their reliance on unstructured text, which limits their ability to incorporate crucial geometric structures inherent in molecules as input [15, 28]. To address this limitation, Fatemi et al. [12] propose encoding the graph structure into textual descriptions. Expanding on this concept, we integrate both atom features and graph structure into textual descriptions. However, our findings from Table 2 reveal that augmenting prompts with descriptions does not consistently enhance performance; rather, it detrimentally affects performance in some instances. Furthermore, the decrease in response consistency reported in Figure 6, when descriptions are added to prompts, suggests that such additions may hinder LLMs' ability to maintain adherence to format requirements. We attribute this to the increased complexity introduced by the additional tokens in the description, thereby exacerbating the LLMs' attentional challenges. Consequently, the practice of converting geometry structures into text for LLM</p>
<p>consumption appears to be an insufficient solution. Addressing this limitation and exploring alternative strategies for effectively incorporating molecule geometry into LLMs remain promising areas for future investigation.</p>
<p>Observation 4: The significance of geometric structure. The findings presented in Table 2 emphasise the superiority of models that integrate geometric structure compared to those that do not, underscoring the pivotal role of geometric information in precisely predicting a molecule's properties. Despite the evident importance of geometric structure, existing LLMs face constraints in directly incorporating this information into prompts, primarily due to limitations in token count within generated descriptions exceeding the LLM's constraints. This limitation hampers the LLM's ability to effectively leverage geometric details, potentially compromising the accuracy of predictions. Thus, addressing this challenge represents a promising avenue for future research, with potential solutions including innovative token management techniques, refined prompt engineering strategies, or alternative model architectures capable of handling more extensive input representations. By overcoming this hurdle, LLMs can be empowered to better capture the intricacies of molecule geometry, thereby enhancing their predictive capabilities in various chemical modelling tasks.</p>
<p>Table 3. Molecule graph property prediction performance for the ogbg-molbace, ogbg-molbbbp, ogbg-molhiv, ogbg-molesol, ogbg-molfreesolv and ogbg-mollipo datasets, follow the Duo pipeline. Classification tasks are evaluated on ROC-AUC ( $\uparrow$ : higher is better), and regression tasks are evaluated on RMSE ( $\downarrow$ : lower is better). The best performance of each model is marked with underline, and the overall best performance is marked as bold.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">ogbg-molbace</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ogbg-molbbbp</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ogbg-molhiv</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ogbg-molesol</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ogbg-molfreesolv</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ogbg-mollipo</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ROC-AUC $\uparrow$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">RMSE $\downarrow$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Valid</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">Valid</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">Valid</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">Valid</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">Valid</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">Valid</td>
<td style="text-align: center;">Test</td>
</tr>
<tr>
<td style="text-align: center;">LM</td>
<td style="text-align: center;">0.5584</td>
<td style="text-align: center;">0.6163</td>
<td style="text-align: center;">0.9307</td>
<td style="text-align: center;">0.6727</td>
<td style="text-align: center;">0.5305</td>
<td style="text-align: center;">0.5037</td>
<td style="text-align: center;">2.1139</td>
<td style="text-align: center;">2.2549</td>
<td style="text-align: center;">6.6189</td>
<td style="text-align: center;">4.4532</td>
<td style="text-align: center;">1.2095</td>
<td style="text-align: center;">1.1066</td>
</tr>
<tr>
<td style="text-align: center;">LM ${ }^{\text {IF }}$</td>
<td style="text-align: center;">0.6075</td>
<td style="text-align: center;">0.6045</td>
<td style="text-align: center;">0.9347</td>
<td style="text-align: center;">0.6664</td>
<td style="text-align: center;">0.5669</td>
<td style="text-align: center;">0.5453</td>
<td style="text-align: center;">2.1292</td>
<td style="text-align: center;">2.2687</td>
<td style="text-align: center;">6.6526</td>
<td style="text-align: center;">4.4754</td>
<td style="text-align: center;">1.2112</td>
<td style="text-align: center;">1.1057</td>
</tr>
<tr>
<td style="text-align: center;">LM ${ }^{\text {IFD }}$</td>
<td style="text-align: center;">0.5357</td>
<td style="text-align: center;">0.6012</td>
<td style="text-align: center;">0.9401</td>
<td style="text-align: center;">0.6566</td>
<td style="text-align: center;">0.5323</td>
<td style="text-align: center;">0.5209</td>
<td style="text-align: center;">2.1193</td>
<td style="text-align: center;">2.2614</td>
<td style="text-align: center;">6.6639</td>
<td style="text-align: center;">4.4974</td>
<td style="text-align: center;">1.2096</td>
<td style="text-align: center;">1.1026</td>
</tr>
<tr>
<td style="text-align: center;">LM ${ }^{\text {IE }}$</td>
<td style="text-align: center;">0.5851</td>
<td style="text-align: center;">0.6107</td>
<td style="text-align: center;">0.9404</td>
<td style="text-align: center;">0.6694</td>
<td style="text-align: center;">0.5760</td>
<td style="text-align: center;">0.5514</td>
<td style="text-align: center;">2.1264</td>
<td style="text-align: center;">2.2687</td>
<td style="text-align: center;">6.6657</td>
<td style="text-align: center;">4.4989</td>
<td style="text-align: center;">1.2109</td>
<td style="text-align: center;">1.1068</td>
</tr>
<tr>
<td style="text-align: center;">LM ${ }^{\text {IED }}$</td>
<td style="text-align: center;">0.6228</td>
<td style="text-align: center;">0.6107</td>
<td style="text-align: center;">0.9370</td>
<td style="text-align: center;">0.6735</td>
<td style="text-align: center;">0.5487</td>
<td style="text-align: center;">0.5342</td>
<td style="text-align: center;">2.1158</td>
<td style="text-align: center;">2.2587</td>
<td style="text-align: center;">6.6737</td>
<td style="text-align: center;">4.5062</td>
<td style="text-align: center;">1.2085</td>
<td style="text-align: center;">1.1020</td>
</tr>
<tr>
<td style="text-align: center;">LM ${ }^{\text {FS-1 }}$</td>
<td style="text-align: center;">0.6059</td>
<td style="text-align: center;">0.6034</td>
<td style="text-align: center;">0.9401</td>
<td style="text-align: center;">0.6576</td>
<td style="text-align: center;">0.5458</td>
<td style="text-align: center;">0.5037</td>
<td style="text-align: center;">2.1319</td>
<td style="text-align: center;">2.2729</td>
<td style="text-align: center;">6.6516</td>
<td style="text-align: center;">4.5003</td>
<td style="text-align: center;">1.2103</td>
<td style="text-align: center;">1.1039</td>
</tr>
<tr>
<td style="text-align: center;">LM ${ }^{\text {FS-2 }}$</td>
<td style="text-align: center;">0.6648</td>
<td style="text-align: center;">0.6287</td>
<td style="text-align: center;">0.9298</td>
<td style="text-align: center;">0.6738</td>
<td style="text-align: center;">0.5458</td>
<td style="text-align: center;">0.5037</td>
<td style="text-align: center;">2.1096</td>
<td style="text-align: center;">2.2519</td>
<td style="text-align: center;">6.6772</td>
<td style="text-align: center;">4.5148</td>
<td style="text-align: center;">1.2093</td>
<td style="text-align: center;">1.1040</td>
</tr>
<tr>
<td style="text-align: center;">LM ${ }^{\text {FS-3 }}$</td>
<td style="text-align: center;">0.5937</td>
<td style="text-align: center;">0.6165</td>
<td style="text-align: center;">0.9299</td>
<td style="text-align: center;">0.6645</td>
<td style="text-align: center;">0.5909</td>
<td style="text-align: center;">0.5875</td>
<td style="text-align: center;">2.1194</td>
<td style="text-align: center;">2.2597</td>
<td style="text-align: center;">6.6326</td>
<td style="text-align: center;">4.4386</td>
<td style="text-align: center;">1.2104</td>
<td style="text-align: center;">1.1063</td>
</tr>
<tr>
<td style="text-align: center;">GCN</td>
<td style="text-align: center;">0.7879</td>
<td style="text-align: center;">0.7147</td>
<td style="text-align: center;">0.9582</td>
<td style="text-align: center;">0.6707</td>
<td style="text-align: center;">0.8461</td>
<td style="text-align: center;">0.7376</td>
<td style="text-align: center;">0.8538</td>
<td style="text-align: center;">1.2561</td>
<td style="text-align: center;">2.8275</td>
<td style="text-align: center;">2.5096</td>
<td style="text-align: center;">0.6985</td>
<td style="text-align: center;">0.7201</td>
</tr>
<tr>
<td style="text-align: center;">GCN ${ }^{\text {S }}$</td>
<td style="text-align: center;">0.7620</td>
<td style="text-align: center;">0.7107</td>
<td style="text-align: center;">0.9572</td>
<td style="text-align: center;">0.6688</td>
<td style="text-align: center;">0.8483</td>
<td style="text-align: center;">0.7306</td>
<td style="text-align: center;">0.8495</td>
<td style="text-align: center;">0.9537</td>
<td style="text-align: center;">2.8358</td>
<td style="text-align: center;">2.4803</td>
<td style="text-align: center;">0.6843</td>
<td style="text-align: center;">0.7132</td>
</tr>
<tr>
<td style="text-align: center;">GCN ${ }^{\text {IF }}$</td>
<td style="text-align: center;">0.7855</td>
<td style="text-align: center;">0.7542</td>
<td style="text-align: center;">0.9575</td>
<td style="text-align: center;">0.6647</td>
<td style="text-align: center;">0.8602</td>
<td style="text-align: center;">0.7535</td>
<td style="text-align: center;">0.8571</td>
<td style="text-align: center;">0.9534</td>
<td style="text-align: center;">2.8351</td>
<td style="text-align: center;">2.2911</td>
<td style="text-align: center;">0.6897</td>
<td style="text-align: center;">0.7037</td>
</tr>
<tr>
<td style="text-align: center;">GCN ${ }^{\text {IFD }}$</td>
<td style="text-align: center;">0.8004</td>
<td style="text-align: center;">0.7581</td>
<td style="text-align: center;">0.9543</td>
<td style="text-align: center;">0.6710</td>
<td style="text-align: center;">0.8572</td>
<td style="text-align: center;">0.7416</td>
<td style="text-align: center;">0.8457</td>
<td style="text-align: center;">1.1430</td>
<td style="text-align: center;">2.7641</td>
<td style="text-align: center;">2.2033</td>
<td style="text-align: center;">0.6939</td>
<td style="text-align: center;">0.7187</td>
</tr>
<tr>
<td style="text-align: center;">GCN ${ }^{\text {IE }}$</td>
<td style="text-align: center;">0.8017</td>
<td style="text-align: center;">0.7396</td>
<td style="text-align: center;">0.9577</td>
<td style="text-align: center;">0.6741</td>
<td style="text-align: center;">0.8491</td>
<td style="text-align: center;">0.7543</td>
<td style="text-align: center;">0.8439</td>
<td style="text-align: center;">0.9559</td>
<td style="text-align: center;">2.6150</td>
<td style="text-align: center;">2.2197</td>
<td style="text-align: center;">0.6846</td>
<td style="text-align: center;">0.6962</td>
</tr>
<tr>
<td style="text-align: center;">GCN ${ }^{\text {IED }}$</td>
<td style="text-align: center;">0.7936</td>
<td style="text-align: center;">0.7020</td>
<td style="text-align: center;">0.9562</td>
<td style="text-align: center;">0.6550</td>
<td style="text-align: center;">0.8497</td>
<td style="text-align: center;">0.7573</td>
<td style="text-align: center;">0.8369</td>
<td style="text-align: center;">0.9650</td>
<td style="text-align: center;">2.7071</td>
<td style="text-align: center;">2.2775</td>
<td style="text-align: center;">0.7100</td>
<td style="text-align: center;">0.6953</td>
</tr>
<tr>
<td style="text-align: center;">GCN ${ }^{\text {FS-1 }}$</td>
<td style="text-align: center;">0.7933</td>
<td style="text-align: center;">0.7535</td>
<td style="text-align: center;">0.9558</td>
<td style="text-align: center;">0.6689</td>
<td style="text-align: center;">0.8446</td>
<td style="text-align: center;">0.7412</td>
<td style="text-align: center;">0.8569</td>
<td style="text-align: center;">0.9439</td>
<td style="text-align: center;">2.7189</td>
<td style="text-align: center;">2.4021</td>
<td style="text-align: center;">0.6984</td>
<td style="text-align: center;">0.6991</td>
</tr>
<tr>
<td style="text-align: center;">GCN ${ }^{\text {FS-2 }}$</td>
<td style="text-align: center;">0.7905</td>
<td style="text-align: center;">0.7903</td>
<td style="text-align: center;">0.9566</td>
<td style="text-align: center;">0.6704</td>
<td style="text-align: center;">0.8529</td>
<td style="text-align: center;">0.7480</td>
<td style="text-align: center;">0.8557</td>
<td style="text-align: center;">0.9402</td>
<td style="text-align: center;">2.7440</td>
<td style="text-align: center;">2.3545</td>
<td style="text-align: center;">0.6873</td>
<td style="text-align: center;">0.7245</td>
</tr>
<tr>
<td style="text-align: center;">GCN ${ }^{\text {FS-3 }}$</td>
<td style="text-align: center;">0.7933</td>
<td style="text-align: center;">0.7682</td>
<td style="text-align: center;">0.9566</td>
<td style="text-align: center;">0.6552</td>
<td style="text-align: center;">0.8520</td>
<td style="text-align: center;">0.7544</td>
<td style="text-align: center;">0.8567</td>
<td style="text-align: center;">0.9441</td>
<td style="text-align: center;">2.7580</td>
<td style="text-align: center;">2.3575</td>
<td style="text-align: center;">0.7066</td>
<td style="text-align: center;">0.7213</td>
</tr>
<tr>
<td style="text-align: center;">GIN</td>
<td style="text-align: center;">0.8012</td>
<td style="text-align: center;">0.7673</td>
<td style="text-align: center;">0.9608</td>
<td style="text-align: center;">0.6708</td>
<td style="text-align: center;">0.8406</td>
<td style="text-align: center;">0.7601</td>
<td style="text-align: center;">0.8010</td>
<td style="text-align: center;">0.9555</td>
<td style="text-align: center;">2.2106</td>
<td style="text-align: center;">2.1610</td>
<td style="text-align: center;">0.6482</td>
<td style="text-align: center;">0.7019</td>
</tr>
<tr>
<td style="text-align: center;">GIN ${ }^{\text {S }}$</td>
<td style="text-align: center;">0.7975</td>
<td style="text-align: center;">0.7802</td>
<td style="text-align: center;">0.9579</td>
<td style="text-align: center;">0.6597</td>
<td style="text-align: center;">0.8548</td>
<td style="text-align: center;">0.7837</td>
<td style="text-align: center;">0.8174</td>
<td style="text-align: center;">0.9142</td>
<td style="text-align: center;">2.6264</td>
<td style="text-align: center;">2.3493</td>
<td style="text-align: center;">0.6440</td>
<td style="text-align: center;">0.6920</td>
</tr>
<tr>
<td style="text-align: center;">GIN ${ }^{\text {IF }}$</td>
<td style="text-align: center;">0.7976</td>
<td style="text-align: center;">0.7804</td>
<td style="text-align: center;">0.9583</td>
<td style="text-align: center;">0.6773</td>
<td style="text-align: center;">0.8497</td>
<td style="text-align: center;">0.7684</td>
<td style="text-align: center;">0.7797</td>
<td style="text-align: center;">0.9349</td>
<td style="text-align: center;">2.4530</td>
<td style="text-align: center;">2.2394</td>
<td style="text-align: center;">0.6454</td>
<td style="text-align: center;">0.6966</td>
</tr>
<tr>
<td style="text-align: center;">GIN ${ }^{\text {IFD }}$</td>
<td style="text-align: center;">0.8001</td>
<td style="text-align: center;">0.7593</td>
<td style="text-align: center;">0.9577</td>
<td style="text-align: center;">0.6763</td>
<td style="text-align: center;">0.8431</td>
<td style="text-align: center;">0.7577</td>
<td style="text-align: center;">0.7843</td>
<td style="text-align: center;">0.9497</td>
<td style="text-align: center;">2.5720</td>
<td style="text-align: center;">2.1906</td>
<td style="text-align: center;">0.6507</td>
<td style="text-align: center;">0.6836</td>
</tr>
<tr>
<td style="text-align: center;">GIN ${ }^{\text {IE }}$</td>
<td style="text-align: center;">0.8046</td>
<td style="text-align: center;">0.7549</td>
<td style="text-align: center;">0.9606</td>
<td style="text-align: center;">0.6762</td>
<td style="text-align: center;">0.8481</td>
<td style="text-align: center;">0.7339</td>
<td style="text-align: center;">0.7924</td>
<td style="text-align: center;">0.9617</td>
<td style="text-align: center;">2.3844</td>
<td style="text-align: center;">2.1509</td>
<td style="text-align: center;">0.6384</td>
<td style="text-align: center;">0.6909</td>
</tr>
<tr>
<td style="text-align: center;">GIN ${ }^{\text {IED }}$</td>
<td style="text-align: center;">0.7916</td>
<td style="text-align: center;">0.7589</td>
<td style="text-align: center;">0.9604</td>
<td style="text-align: center;">0.6494</td>
<td style="text-align: center;">0.8493</td>
<td style="text-align: center;">0.7798</td>
<td style="text-align: center;">0.7961</td>
<td style="text-align: center;">0.9623</td>
<td style="text-align: center;">2.4639</td>
<td style="text-align: center;">2.1956</td>
<td style="text-align: center;">0.6424</td>
<td style="text-align: center;">0.6846</td>
</tr>
<tr>
<td style="text-align: center;">GIN ${ }^{\text {FS-1 }}$</td>
<td style="text-align: center;">0.8001</td>
<td style="text-align: center;">0.7594</td>
<td style="text-align: center;">0.9603</td>
<td style="text-align: center;">0.6748</td>
<td style="text-align: center;">0.8547</td>
<td style="text-align: center;">0.7504</td>
<td style="text-align: center;">0.7863</td>
<td style="text-align: center;">0.9661</td>
<td style="text-align: center;">2.4021</td>
<td style="text-align: center;">2.2439</td>
<td style="text-align: center;">0.6422</td>
<td style="text-align: center;">0.6950</td>
</tr>
<tr>
<td style="text-align: center;">GIN ${ }^{\text {FS-2 }}$</td>
<td style="text-align: center;">0.8062</td>
<td style="text-align: center;">0.7677</td>
<td style="text-align: center;">0.9595</td>
<td style="text-align: center;">0.6659</td>
<td style="text-align: center;">0.8492</td>
<td style="text-align: center;">0.7824</td>
<td style="text-align: center;">0.7949</td>
<td style="text-align: center;">0.9171</td>
<td style="text-align: center;">2.3584</td>
<td style="text-align: center;">2.2675</td>
<td style="text-align: center;">0.6399</td>
<td style="text-align: center;">0.6969</td>
</tr>
<tr>
<td style="text-align: center;">GIN ${ }^{\text {FS-3 }}$</td>
<td style="text-align: center;">0.8038</td>
<td style="text-align: center;">0.7388</td>
<td style="text-align: center;">0.9586</td>
<td style="text-align: center;">0.6762</td>
<td style="text-align: center;">0.8515</td>
<td style="text-align: center;">0.7602</td>
<td style="text-align: center;">0.8118</td>
<td style="text-align: center;">0.9155</td>
<td style="text-align: center;">2.3942</td>
<td style="text-align: center;">2.3633</td>
<td style="text-align: center;">0.6447</td>
<td style="text-align: center;">0.6865</td>
</tr>
</tbody>
</table>
<p>Observation 5: LLMs exhibit significant effectiveness as ML augmenters. In addition to utilising LLMs for direct molecule prediction tasks, we delve into the potential benefits of integrating LLMs with existing ML models.</p>
<p>Table 4. Molecule graph property prediction performance for the ogbg-molbace, ogbg-molbbbp, ogbg-molhiv, ogbg-molesol, ogbg-molfreesolv and ogbg-mollipo datasets, follow the Two pipeline. Classification tasks are evaluated on ROC-AUC ( $\uparrow$ : higher is better), and regression tasks are evaluated on RMSE ( $\downarrow$ : lower is better). GNN ${ }^{\text {DIOI }}$ indicates the best performance collected from Table 3. The best performance of each model is marked with underline, and the overall best performance is marked as bold.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">ogbg-molbace</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ogbg-molbbbp</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ogbg-molhiv</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ogbg-molesol</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ogbg-molfreesolv</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ogbg-mollipo</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ROC-AUC $\uparrow$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">RMSE $\downarrow$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Valid</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">Valid</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">Valid</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">Valid</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">Valid</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">Valid</td>
<td style="text-align: center;">Test</td>
</tr>
<tr>
<td style="text-align: center;">GCN</td>
<td style="text-align: center;">0.7879</td>
<td style="text-align: center;">0.7147</td>
<td style="text-align: center;">0.9582</td>
<td style="text-align: center;">0.6707</td>
<td style="text-align: center;">0.8461</td>
<td style="text-align: center;">0.7376</td>
<td style="text-align: center;">0.8538</td>
<td style="text-align: center;">1.2561</td>
<td style="text-align: center;">2.8275</td>
<td style="text-align: center;">2.5096</td>
<td style="text-align: center;">0.6985</td>
<td style="text-align: center;">0.7201</td>
</tr>
<tr>
<td style="text-align: center;">GCN ${ }^{\text {DIOI }}$</td>
<td style="text-align: center;">0.8017</td>
<td style="text-align: center;">0.7903</td>
<td style="text-align: center;">0.9582</td>
<td style="text-align: center;">0.6741</td>
<td style="text-align: center;">0.8602</td>
<td style="text-align: center;">0.7573</td>
<td style="text-align: center;">0.8369</td>
<td style="text-align: center;">0.9402</td>
<td style="text-align: center;">2.6150</td>
<td style="text-align: center;">2.2033</td>
<td style="text-align: center;">0.6843</td>
<td style="text-align: center;">0.6953</td>
</tr>
<tr>
<td style="text-align: center;">GCN ${ }^{\text {S-IF }}$</td>
<td style="text-align: center;">0.7886</td>
<td style="text-align: center;">0.7566</td>
<td style="text-align: center;">0.9602</td>
<td style="text-align: center;">0.6564</td>
<td style="text-align: center;">0.8430</td>
<td style="text-align: center;">0.7649</td>
<td style="text-align: center;">0.8461</td>
<td style="text-align: center;">0.9366</td>
<td style="text-align: center;">2.7679</td>
<td style="text-align: center;">2.4457</td>
<td style="text-align: center;">0.6900</td>
<td style="text-align: center;">0.7040</td>
</tr>
<tr>
<td style="text-align: center;">GCN ${ }^{\text {S-IFD }}$</td>
<td style="text-align: center;">0.7900</td>
<td style="text-align: center;">0.7896</td>
<td style="text-align: center;">0.9540</td>
<td style="text-align: center;">0.6700</td>
<td style="text-align: center;">0.8322</td>
<td style="text-align: center;">0.7539</td>
<td style="text-align: center;">0.8379</td>
<td style="text-align: center;">9.6060</td>
<td style="text-align: center;">2.8279</td>
<td style="text-align: center;">2.3047</td>
<td style="text-align: center;">0.7020</td>
<td style="text-align: center;">0.7105</td>
</tr>
<tr>
<td style="text-align: center;">GCN ${ }^{\text {S-IE }}$</td>
<td style="text-align: center;">0.8044</td>
<td style="text-align: center;">0.7727</td>
<td style="text-align: center;">0.9579</td>
<td style="text-align: center;">0.6706</td>
<td style="text-align: center;">0.8548</td>
<td style="text-align: center;">0.7643</td>
<td style="text-align: center;">0.8312</td>
<td style="text-align: center;">1.4489</td>
<td style="text-align: center;">2.7831</td>
<td style="text-align: center;">2.2401</td>
<td style="text-align: center;">0.6921</td>
<td style="text-align: center;">0.7170</td>
</tr>
<tr>
<td style="text-align: center;">GCN ${ }^{\text {S-IED }}$</td>
<td style="text-align: center;">0.7811</td>
<td style="text-align: center;">0.7587</td>
<td style="text-align: center;">0.9576</td>
<td style="text-align: center;">0.6639</td>
<td style="text-align: center;">0.8470</td>
<td style="text-align: center;">0.7414</td>
<td style="text-align: center;">0.8560</td>
<td style="text-align: center;">0.9598</td>
<td style="text-align: center;">2.7709</td>
<td style="text-align: center;">2.2807</td>
<td style="text-align: center;">0.6975</td>
<td style="text-align: center;">0.7256</td>
</tr>
<tr>
<td style="text-align: center;">GCN ${ }^{\text {S-FS-1 }}$</td>
<td style="text-align: center;">0.7899</td>
<td style="text-align: center;">0.7662</td>
<td style="text-align: center;">0.9578</td>
<td style="text-align: center;">0.6774</td>
<td style="text-align: center;">0.8529</td>
<td style="text-align: center;">0.7584</td>
<td style="text-align: center;">0.8548</td>
<td style="text-align: center;">0.9419</td>
<td style="text-align: center;">2.7675</td>
<td style="text-align: center;">2.4254</td>
<td style="text-align: center;">0.7051</td>
<td style="text-align: center;">0.7101</td>
</tr>
<tr>
<td style="text-align: center;">GCN ${ }^{\text {S-FS-2 }}$</td>
<td style="text-align: center;">0.7851</td>
<td style="text-align: center;">0.7625</td>
<td style="text-align: center;">0.9624</td>
<td style="text-align: center;">0.6679</td>
<td style="text-align: center;">0.8592</td>
<td style="text-align: center;">0.7386</td>
<td style="text-align: center;">0.8496</td>
<td style="text-align: center;">1.9680</td>
<td style="text-align: center;">2.7437</td>
<td style="text-align: center;">2.3693</td>
<td style="text-align: center;">0.7015</td>
<td style="text-align: center;">0.7015</td>
</tr>
<tr>
<td style="text-align: center;">GCN ${ }^{\text {S-FS-3 }}$</td>
<td style="text-align: center;">0.7950</td>
<td style="text-align: center;">0.7939</td>
<td style="text-align: center;">0.9593</td>
<td style="text-align: center;">0.6710</td>
<td style="text-align: center;">0.8471</td>
<td style="text-align: center;">0.7635</td>
<td style="text-align: center;">0.8810</td>
<td style="text-align: center;">1.0457</td>
<td style="text-align: center;">2.6897</td>
<td style="text-align: center;">2.2867</td>
<td style="text-align: center;">0.6939</td>
<td style="text-align: center;">0.6937</td>
</tr>
<tr>
<td style="text-align: center;">GIN</td>
<td style="text-align: center;">0.8012</td>
<td style="text-align: center;">0.7673</td>
<td style="text-align: center;">0.9608</td>
<td style="text-align: center;">0.6708</td>
<td style="text-align: center;">0.8406</td>
<td style="text-align: center;">0.7601</td>
<td style="text-align: center;">0.8010</td>
<td style="text-align: center;">0.9555</td>
<td style="text-align: center;">2.2106</td>
<td style="text-align: center;">2.1610</td>
<td style="text-align: center;">0.6482</td>
<td style="text-align: center;">0.7019</td>
</tr>
<tr>
<td style="text-align: center;">GIN ${ }^{\text {DIOI }}$</td>
<td style="text-align: center;">0.8062</td>
<td style="text-align: center;">0.7804</td>
<td style="text-align: center;">0.9608</td>
<td style="text-align: center;">0.6773</td>
<td style="text-align: center;">0.8548</td>
<td style="text-align: center;">0.7837</td>
<td style="text-align: center;">0.7797</td>
<td style="text-align: center;">0.9142</td>
<td style="text-align: center;">2.2106</td>
<td style="text-align: center;">2.1509</td>
<td style="text-align: center;">0.6399</td>
<td style="text-align: center;">0.6836</td>
</tr>
<tr>
<td style="text-align: center;">GIN ${ }^{\text {S-IF }}$</td>
<td style="text-align: center;">0.8026</td>
<td style="text-align: center;">0.7606</td>
<td style="text-align: center;">0.9547</td>
<td style="text-align: center;">0.6728</td>
<td style="text-align: center;">0.8345</td>
<td style="text-align: center;">0.7667</td>
<td style="text-align: center;">0.7900</td>
<td style="text-align: center;">0.9547</td>
<td style="text-align: center;">2.4822</td>
<td style="text-align: center;">2.1688</td>
<td style="text-align: center;">0.6388</td>
<td style="text-align: center;">0.6791</td>
</tr>
<tr>
<td style="text-align: center;">GIN ${ }^{\text {S-IFD }}$</td>
<td style="text-align: center;">0.7974</td>
<td style="text-align: center;">0.7996</td>
<td style="text-align: center;">0.9584</td>
<td style="text-align: center;">0.6751</td>
<td style="text-align: center;">0.8351</td>
<td style="text-align: center;">0.7549</td>
<td style="text-align: center;">0.7956</td>
<td style="text-align: center;">0.9305</td>
<td style="text-align: center;">2.3015</td>
<td style="text-align: center;">2.2223</td>
<td style="text-align: center;">0.6465</td>
<td style="text-align: center;">0.6891</td>
</tr>
<tr>
<td style="text-align: center;">GIN ${ }^{\text {S-IE }}$</td>
<td style="text-align: center;">0.8035</td>
<td style="text-align: center;">0.7459</td>
<td style="text-align: center;">0.9597</td>
<td style="text-align: center;">0.6719</td>
<td style="text-align: center;">0.8475</td>
<td style="text-align: center;">0.7591</td>
<td style="text-align: center;">0.8217</td>
<td style="text-align: center;">0.9320</td>
<td style="text-align: center;">2.3971</td>
<td style="text-align: center;">2.2266</td>
<td style="text-align: center;">0.6423</td>
<td style="text-align: center;">0.6859</td>
</tr>
<tr>
<td style="text-align: center;">GIN ${ }^{\text {S-IED }}$</td>
<td style="text-align: center;">0.7914</td>
<td style="text-align: center;">0.7638</td>
<td style="text-align: center;">0.9594</td>
<td style="text-align: center;">0.6584</td>
<td style="text-align: center;">0.8519</td>
<td style="text-align: center;">0.7512</td>
<td style="text-align: center;">0.7845</td>
<td style="text-align: center;">0.9599</td>
<td style="text-align: center;">2.3271</td>
<td style="text-align: center;">2.1759</td>
<td style="text-align: center;">0.6454</td>
<td style="text-align: center;">0.6900</td>
</tr>
<tr>
<td style="text-align: center;">GIN ${ }^{\text {S-FS-1 }}$</td>
<td style="text-align: center;">0.7989</td>
<td style="text-align: center;">0.7542</td>
<td style="text-align: center;">0.9591</td>
<td style="text-align: center;">0.6556</td>
<td style="text-align: center;">0.8400</td>
<td style="text-align: center;">0.7922</td>
<td style="text-align: center;">0.8010</td>
<td style="text-align: center;">0.9130</td>
<td style="text-align: center;">2.4698</td>
<td style="text-align: center;">2.2435</td>
<td style="text-align: center;">0.6490</td>
<td style="text-align: center;">0.6967</td>
</tr>
<tr>
<td style="text-align: center;">GIN ${ }^{\text {S-FS-2 }}$</td>
<td style="text-align: center;">0.7886</td>
<td style="text-align: center;">0.7704</td>
<td style="text-align: center;">0.9604</td>
<td style="text-align: center;">0.6789</td>
<td style="text-align: center;">0.8461</td>
<td style="text-align: center;">0.7619</td>
<td style="text-align: center;">0.7967</td>
<td style="text-align: center;">0.9423</td>
<td style="text-align: center;">2.4492</td>
<td style="text-align: center;">2.1307</td>
<td style="text-align: center;">0.6452</td>
<td style="text-align: center;">0.6939</td>
</tr>
<tr>
<td style="text-align: center;">GIN ${ }^{\text {S-FS-3 }}$</td>
<td style="text-align: center;">0.8013</td>
<td style="text-align: center;">0.7762</td>
<td style="text-align: center;">0.9587</td>
<td style="text-align: center;">0.6732</td>
<td style="text-align: center;">0.8493</td>
<td style="text-align: center;">0.7645</td>
<td style="text-align: center;">0.8099</td>
<td style="text-align: center;">0.9302</td>
<td style="text-align: center;">2.3327</td>
<td style="text-align: center;">2.1829</td>
<td style="text-align: center;">0.6407</td>
<td style="text-align: center;">0.6912</td>
</tr>
</tbody>
</table>
<p>Following the framework delineated in Figure 3, we augment the input features of ML models, specifically GNNs, with responses generated by LLMs. The results presented in both Table 3 and Table 4 indicate a discernible enhancement in predictive performance upon introducing LLM responses as additional input features. This suggests that leveraging LLM-generated responses can effectively supplement the information captured by traditional ML models, leading to improved predictive accuracy. Furthermore, established ML models, such as GNN variants, offer viable solutions for capturing geometric structure information inherent in molecules. By integrating LLM-generated responses with these models, we can potentially enhance their capacity to comprehend and leverage complex geometric features, thus further refining their predictive capabilities in chemical modelling tasks. This hybrid approach represents a promising avenue for advancing the state-of-the-art in molecular property prediction.</p>
<h1>5 CONCLUDING REMARKS</h1>
<p>In conclusion, our empirical investigations have provided valuable insights into the capacity of LLMs to handle molecular tasks. Our comprehensive analysis across six benchmark datasets has demonstrated that LLMs generally exhibit less competitive performance in molecular prediction tasks when compared to established ML models, especially those explicitly designed to capture geometric structures within molecules. Moreover, our findings underscore the potential of leveraging LLMs as complementary tools to enhance the performance of existing ML models. By integrating LLMs as augmenters, we observed improvements in predictive accuracy, suggesting a promising avenue for effectively harnessing the capabilities of both LLMs and traditional ML models in tandem. While our study highlights LLMs' current limitations in molecular tasks, it also opens up new avenues for future research. Exploring innovative methodologies to better integrate LLMs with domain-specific knowledge and structural information could potentially bridge the</p>
<p>performance gap observed in our experiments. Overall, our work contributes to a deeper understanding of the strengths and weaknesses of LLMs in molecular tasks, paving the way for more informed strategies for leveraging these models for practical applications in chemistry, biology, and related fields.</p>
<p>Our work makes an important step toward leveraging LLMs for challenging molecule prediction tasks. There exist numerous promising avenues for future work. One critical area for further investigation is addressing the inherent limitation of LLMs in comprehending molecule geometric structures. As evidenced by the performance disparities highlighted in Section 4.2, the inability of LLMs to grasp such structural nuances often results in inaccuracies. Therefore, overcoming this limitation and enhancing LLMs' understanding of molecule geometric structure is imperative for their broader applicability in molecular tasks. Additionally, while our study introduces straightforward yet effective frameworks for integrating LLMs with traditional ML models, there remains ample room for developing more sophisticated methodologies in this regard. Designing advanced frameworks that seamlessly incorporate LLMs with existing ML models presents a promising avenue for future research, potentially yielding enhanced predictive performance and model interpretability [49]. Finally, developing molecule specialist LLMs would be impressive to the community. Despite LLMs' underperformance compared to baselines across many tasks, their ability to derive solutions from limited examples underscores their potential for generalised intelligence in the molecular domain. However, current LLMs exhibit notable hallucinations in chemistry tasks, suggesting room for improvement. Continuous LLM development and research into mitigating hallucinations offer optimism for enhancing their efficacy in practical chemistry problem-solving.</p>
<h1>ACKNOWLEDGMENTS</h1>
<p>This work is supported by the Horizon Europe and Innovation Fund Denmark under the Eureka, Eurostar grant no E115712 - AAVanguard.</p>
<h2>REFERENCES</h2>
<p>[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. CoRR abs/2307.09288 (2023).
[2] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, et al. 2023. Graph of thoughts: Solving elaborate problems with large language models. CoRR abs/2308.09687 (2023).
[3] Daniil A Boiko, Robert MacKnight, Ben Kline, and Gabe Gomes. 2023. Autonomous chemical research with large language models. Nature 624, 7992 (2023), $570-578$.
[4] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. In Proceedings of the 2020 Annual Conference on Neural Information Processing Systems (NeurIPS). 1877-1901.
[5] SÃ©bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4. CoRR abs/2303.12712 (2023).
[6] Keith T Butler, Daniel W Davies, Hugh Cartwright, Olexandr Isayev, and Aron Walsh. 2018. Machine learning for molecular and materials science. Nature 559, 7715 (2018), 547-555.
[7] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. 2023. A survey on evaluation of large language models. ACM Transactions on Intelligent Systems and Technology (TOST) (2023).
[8] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. CoRR abs/2210.11416 (2022).
[9] Peter Clark, Oyvind Tafjord, and Kyle Richardson. 2020. Transformers as soft reasoners over language. CoRR abs/2002.05867 (2020).
[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. CoRR abs/1810.04805 (2018).
[11] Matthew F Dixon, Igor Halperin, and Paul Bilokon. 2020. Machine learning in finance. Vol. 1170. Springer.</p>
<p>[12] Bahare Fatemi, Jonathan Halcrow, and Bryan Perozzi. 2023. Talk like a graph: Encoding graphs for large language models. CoRR abs/2310.04560 (2023).
[13] Simon Frieder, Luca Pinchetti, Ryan-Rbys Griffiths, Tommaso Salvatori, Thomas Lukasiewicz, Philipp Petersen, and Julius Berner. 2023. Mathematical capabilities of chatgpt. In Proceedings of the 2023 Annual Conference on Neural Information Processing Systems (NeurIPS).
[14] Nate Gruver, Marc Finzi, Shikai Qiu, and Andrew G Wilson. 2023. Large language models are zero-shot time series forecasters. In Proceedings of the 2023 Annual Conference on Neural Information Processing Systems (NeurIPS).
[15] Jiayan Guo, Lun Du, and Hengyu Liu. 2023. GPT4Graph: Can Large Language Models Understand Graph Structured Data? An Empirical Evaluation and Benchmarking. CoRR abs/2305.15066 (2023).
[16] Taicheng Guo, Bozhao Nan, Zhenwen Liang, Zhichun Guo, Nitesh Chawla, Olaf Wiest, Xiangliang Zhang, et al. 2023. What can large language models do in chemistry? a comprehensive benchmark on eight tasks. In Proceedings of the 2023 Annual Conference on Neural Information Processing Systems (NeurIPS).
[17] Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2021. DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with GradientDisentangled Embedding Sharing. CoRR abs/2111.09543 (2021).
[18] Amr Hendy, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr, Hitokazu Matsushita, Young Jin Kim, Mohamed Afify, and Hany Hassan Awadalla. 2023. How good are gpt models at machine translation? a comprehensive evaluation. CoRR abs/2302.09210 (2023).
[19] Jeremy Howard and Sebastian Ruder. 2018. Universal language model fine-tuning for text classification. CoRR abs/1801.0614 (2018).
[20] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. CoRR abs/2106.09685 (2021).
[21] Weihua Hu, Matthias Fey, Hongyu Ren, Maho Nakata, Yuxiao Dong, and Jure Leskovec. 2021. OGB-LSC: A Large-Scale Challenge for Machine Learning on Graphs. In Proceedings of the 2021 Annual Conference on Neural Information Processing Systems (NeurIPS).
[22] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. 2020. Open Graph Benchmark: Datasets for Machine Learning on Graphs. In Proceedings of the 2020 Annual Conference on Neural Information Processing Systems (NeurIPS). 22118-22133.
[23] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et al. 2023. A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. CoRR abs/2311.05232 (2023).
[24] Kevin Maik Jablonka, Philippe Schwaller, Andres Ortega-Guerrero, and Berend Smit. 2024. Leveraging large language models for predictive chemistry. Nature Machine Intelligence (2024), 1-9.
[25] Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification with Graph Convolutional Networks. In Proceedings of the 2017 International Conference on Learning Representations (ICLR).
[26] Satyapriya Krishna, Jiaqi Ma, Dylan Slack, Asma Ghandeharioun, Sameer Singh, and Himabindu Lakkaraju. 2023. Post hoc explanations of language models can improve language models. CoRR abs/2305.11426 (2023).
[27] Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich KÃ¼ttler, Mike Lewis, Wen-tau Yih, Tim RocktÃ¤schel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. In Proceedings of the 2020 Annual Conference on Neural Information Processing Systems (NeurIPS). 9459-9474.
[28] Yuhan Li, Zhixun Li, Peisong Wang, Jia Li, Xiangguo Sun, Hong Cheng, and Jeffrey Xu Yu. 2023. A survey of graph meets large language model: Progress and future directions. CoRR abs/2311.12399 (2023).
[29] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2023. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys (CSUR) 55, 9 (2023), 1-35.
[30] David Mendez, Anna Gaulton, A Patricia Bento, Jon Chambers, Marleen De Veij, Eloy FÃ©lix, Maria Paula MagariÃ±os, Juan F Mosquera, Prudence Mutowo, MichaÅ‚ Nowotka, et al. 2019. ChEMBL: towards direct deposition of bioassay data. Nucleic Acids Research 47, D1 (2019), D930-D940.
[31] Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. 2023. Capabilities of gpt-4 on medical challenge problems. CoRR abs/2303.13375 (2023).
[32] OpenAI. 2023. GPT-4. https://openai.com/research/gpt-4
[33] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In Proceedings of the 2020 Annual Conference on Neural Information Processing Systems (NeurIPS). 27730-27744.
[34] Chen Qian, Huayi Tang, Zhirui Yang, Hong Liang, and Yong Liu. 2023. Can large language models empower molecular property prediction? CoRR abs/2307.07443 (2023).
[35] Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. 2022. Galactica: A large language model for science. CoRR abs/2211.09085 (2022).
[36] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. 2023. Gemini: a family of highly capable multimodal models. CoRR abs/2312.11805 (2023).
[37] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. CoRR abs/2307.09288 (2023).</p>
<p>[38] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In Proceedings of the 2017 Annual Conference on Neural Information Processing Systems (NIPS). 5998-6008.
[39] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. 2022. Emergent abilities of large language models. CoRR abs/2206.07682 (2022).
[40] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. In Proceedings of the 2022 Annual Conference on Neural Information Processing Systems (NeurIPS). 24824-24837.
[41] David Weininger. 1988. SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules. Journal of Chemical Information and Computer Sciences 28, 1 (1988), 31-36.
[42] Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S Pappu, Karl Leswing, and Vijay Pande. 2018. MoleculeNet: a benchmark for molecular machine learning. Chemical Science 9, 2 (2018), 513-530.
[43] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2019. How Powerful are Graph Neural Networks?. In Proceedings of the 2019 International Conference on Learning Representations (ICLR).
[44] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. CoRR abs/2305.10601 (2023).
[45] Hengchuang Yin, Zhonghui Gu, Fanhao Wang, Yiparemu Abuduhaihaier, Yanqiao Zhu, Xinming Tu, Xian-Sheng Hua, Xiao Luo, and Yizhou Sun. 2024. An Evaluation of Large Language Models in Bioinformatics Research. CoRR abs/2402.13714 (2024).
[46] Qiang Zhang, Keyang Ding, Tianwen Lyv, Xinda Wang, Qingyu Yin, Yiwen Zhang, Jing Yu, Yuhao Wang, Xiaotong Li, Zhuoyi Xiang, et al. 2024. Scientific Large Language Models: A Survey on Biological \&amp; Chemical Domains. CoRR abs/2401.14656 (2024).
[47] Ziwei Zhang, Peng Cui, and Wenwu Zhu. 2020. Deep Learning on Graphs: A Survey. IEEE Transactions on Knowledge and Data Engineering (TKDE) 34, 1 (2020), 249-270.
[48] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A survey of large language models. CoRR abs/2303.18223 (2023).
[49] Zhiqiang Zhong, Anastasia Barkova, and Davide Mottin. 2023. Knowledge-augmented Graph Machine Learning for Drug Discovery: A Survey from Precision to Interpretability. CoRR abs/2302.08261 (2023).
[50] Zhiqiang Zhong, Kuangyu Zhou, and Davide Mottin. 2024. Harnessing Large Language Models as Post-hoc Correctors. CoRR abs/2402.13414 (2024).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://github.com/snap-stanford/ogh/tree/master/examples/graphproppred/mol
${ }^{2}$ https://github.com/snap-stanford/ogh/tree/master/examples/graphproppred/mol
${ }^{3}$ https://huggingface.co/microsoft/deberta-v3-base&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6920 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6920</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6920</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-133.html">extraction-schema-133</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the representational format of conceptual knowledge in the brain at a functional level, including theoretical models, their descriptions, claimed representational formats, supporting or contradictory empirical evidence, experimental paradigms, key findings, and citation information.</div>
                <p><strong>Paper ID:</strong> paper-bfab20022d4d251704984d7f66a31168ecffedba</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/bfab20022d4d251704984d7f66a31168ecffedba" target="_blank">Evaluating semantic models with word-sentence relatedness</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> A data set for evaluating semantic models was developed consisting of 775 English word-sentence pairs annotated for semantic relatedness by human raters engaged in a Maximum Difference Scaling (MDS) task, as well as a faster alternative task.</p>
                <p><strong>Paper Abstract:</strong> Semantic textual similarity (STS) systems are designed to encode and evaluate the semantic similarity between words, phrases, sentences, and documents. One method for assessing the quality or authenticity of semantic information encoded in these systems is by comparison with human judgments. A data set for evaluating semantic models was developed consisting of 775 English word-sentence pairs, each annotated for semantic relatedness by human raters engaged in a Maximum Difference Scaling (MDS) task, as well as a faster alternative task. As a sample application of this relatedness data, behavior-based relatedness was compared to the relatedness computed via four off-the-shelf STS models: n-gram, Latent Semantic Analysis (LSA), Word2Vec, and UMBC Ebiquity. Some STS models captured much of the variance in the human judgments collected, but they were not sensitive to the implicatures and entailments that were processed and considered by the participants. All text stimuli and judgment data have been made freely available.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6920.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6920.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the representational format of conceptual knowledge in the brain at a functional level, including theoretical models, their descriptions, claimed representational formats, supporting or contradictory empirical evidence, experimental paradigms, key findings, and citation information.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LSA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Latent Semantic Analysis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A distributional, matrix-factorization method that represents words as vectors in a reduced-rank semantic space derived from word–document co-occurrence; sentence meaning is typically composed by combining word vectors and computing cosine similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Indexing by latent semantic analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>Distributional Semantic Model (LSA)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>high-dimensional space (feature-based vector)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Concepts are represented as points (vectors) in a low-dimensional semantic space derived from co-occurrence statistics; similarity is given by geometric proximity (e.g., cosine). Sentences are represented by composition (e.g., averaging) of constituent word vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_claims</strong></td>
                            <td>Accounts for graded semantic relatedness between words and larger linguistic units; can predict human similarity/relatedness judgments based on distributional context.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>behavioral experiment (human relatedness judgments) + computational model comparison</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_paradigm</strong></td>
                            <td>Human Maximum Difference Scaling (MDS) and free-ranking relatedness tasks; Spearman correlation of model-produced sentence rankings with human rankings.</td>
                        </tr>
                        <tr>
                            <td><strong>key_result</strong></td>
                            <td>LSA correlations with behavior were substantial (reported ρ ≈ 0.72 across targets) indicating the model captures much variance in human relatedness rankings, but it fails on cases requiring entailment/implicature or discriminating antonyms.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_theory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence</strong></td>
                            <td>Systematic mismatches: LSA (and similar distributional models) rank antonyms and contextually-entailed relations as similar (e.g., 'big' vs 'small' contexts), and are insensitive to entailments/implicatures (e.g., prototypically small objects like 'dime' not recognized as related to 'small'), indicating limitations for capturing some human semantic inferences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Deerwester et al., 1990; Glasgow et al., (unknown year)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating semantic models with word-sentence relatedness', 'publication_date_yy_mm': '2016-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6920.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6920.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the representational format of conceptual knowledge in the brain at a functional level, including theoretical models, their descriptions, claimed representational formats, supporting or contradictory empirical evidence, experimental paradigms, key findings, and citation information.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Word2Vec</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Word2Vec (skip-gram neural embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neural, distributional embedding method that learns dense vector representations for words by predicting surrounding context (skip-gram); semantic similarity computed via cosine between vectors and simple sentence representations composed by averaging word vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Distributed representations of words and phrases and their compositionality.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>Distributional Semantic Model (Word2Vec)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>high-dimensional space (feature-based vector)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Concepts are encoded as continuous-valued vectors learned from local context prediction; semantic composition for phrases/sentences uses vector combination (e.g., averaging) to yield representations comparable to word vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_claims</strong></td>
                            <td>Predicts semantic relatedness and captures many lexical-semantic regularities; can be used compositionally to compare words to sentences and predict human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>behavioral experiment (human relatedness judgments) + computational model comparison</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_paradigm</strong></td>
                            <td>MDS and free-ranking relatedness tasks; model-to-human ranking correlations (Spearman).</td>
                        </tr>
                        <tr>
                            <td><strong>key_result</strong></td>
                            <td>Word2Vec achieved high correlation with human rankings (reported ρ ≈ 0.78), indicating strong alignment with human relatedness judgments, but it also failed on entailment/implicature and antonymy cases.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_theory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence</strong></td>
                            <td>Misses inferential relations (entailments/implicatures) and sometimes treats antonyms as similar due to shared contexts (e.g., 'big' contexts similar to 'small'), producing large rank discrepancies for specific sentences compared to human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Mikolov et al., 2013; Glasgow et al., (unknown year)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating semantic models with word-sentence relatedness', 'publication_date_yy_mm': '2016-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6920.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6920.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the representational format of conceptual knowledge in the brain at a functional level, including theoretical models, their descriptions, claimed representational formats, supporting or contradictory empirical evidence, experimental paradigms, key findings, and citation information.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>N-gram</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Local co-occurrence / N-gram model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A surface co-occurrence model that estimates semantic relatedness from local word co-occurrence counts within a sliding window (here: five-word window) in a corpus, normalized to produce pairwise co-occurrence frequencies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Class-based n-gram models of natural language.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>Local Co-occurrence (n-gram) Model</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>co-occurrence frequency matrix (symbolic / count-based)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Represents concepts via counts of nearby word occurrences (local context); sentence relatedness computed by summing co-occurrence associations between a target and words in the sentence.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_claims</strong></td>
                            <td>Surface-level co-occurrence patterns reflect semantic relatedness; simple co-occurrence can predict some human similarity judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>behavioral experiment + computational model comparison</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_paradigm</strong></td>
                            <td>MDS and free-ranking relatedness tasks; Spearman correlations between model rankings and human rankings.</td>
                        </tr>
                        <tr>
                            <td><strong>key_result</strong></td>
                            <td>N-gram (local co-occurrence) performed poorly relative to higher-order models (reported ρ ≈ 0.39) and often fell below baseline performance, indicating surface co-occurrence is insufficient to capture higher-level conceptual relatedness.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_theory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence</strong></td>
                            <td>Sub-baseline performance shows that simple local co-occurrence cannot account for many human judgments that rely on abstract contextual knowledge, entailment, or implicature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Brown et al., 1992; Glasgow et al., (unknown year)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating semantic models with word-sentence relatedness', 'publication_date_yy_mm': '2016-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6920.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6920.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the representational format of conceptual knowledge in the brain at a functional level, including theoretical models, their descriptions, claimed representational formats, supporting or contradictory empirical evidence, experimental paradigms, key findings, and citation information.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WordNet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>WordNet lexical taxonomy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A manually curated lexical database organizing words/concepts into synsets and explicit semantic relations (e.g., hypernymy, hyponymy, meronymy), usable to compute taxonomic distances between concepts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>WordNet: a lexical database for English.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>Knowledge-based Taxonomic Representation (WordNet)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>relational network / taxonomic hierarchy</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Conceptual knowledge is represented as nodes in a structured graph/taxonomy where relations (is-a, part-of, etc.) encode semantic relationships and distances reflect conceptual similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_claims</strong></td>
                            <td>Encodes explicit lexical and taxonomic relations that can complement corpus-based distributional information to improve semantic similarity judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>computational model comparison (hybrid system performance)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_paradigm</strong></td>
                            <td>STS model comparisons to human relatedness rankings; Ebiquity fused WordNet-based distances with LSA outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>key_result</strong></td>
                            <td>Incorporating WordNet taxonomy into a hybrid STS system (Ebiquity) improved correlation with human judgments (Ebiquity ρ ≈ 0.80), suggesting taxonomy adds complementary, useful structure beyond pure distributional signals.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_theory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence</strong></td>
                            <td>Even with taxonomic information, the hybrid system still missed many human inferences based on entailment and implicature, indicating explicit taxonomy alone is not sufficient for all aspects of human conceptual inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Miller, 1995; Han et al., 2013; Glasgow et al., (unknown year)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating semantic models with word-sentence relatedness', 'publication_date_yy_mm': '2016-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6920.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6920.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the representational format of conceptual knowledge in the brain at a functional level, including theoretical models, their descriptions, claimed representational formats, supporting or contradictory empirical evidence, experimental paradigms, key findings, and citation information.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ebiquity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>UMBC Ebiquity (hybrid STS system)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A semantic textual similarity system that fuses corpus-based (LSA) and knowledge-based (WordNet) similarity estimates, weighting and combining them to produce a single similarity score for words/sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>UMBC EBIQUITY-CORE: Semantic textual similarity systems.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>Hybrid Distributional + Taxonomic Model (Ebiquity)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>hybrid: high-dimensional vector space + relational taxonomy</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Combines distributional vector-space representations with explicit taxonomic distances from WordNet, leveraging complementary strengths: statistical contextual similarity plus curated hierarchical relations.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_claims</strong></td>
                            <td>By integrating corpus-derived and knowledge-derived signals, the model better predicts human semantic relatedness and composes sentence meaning from constituent word meanings.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>behavioral experiment + computational model comparison</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_paradigm</strong></td>
                            <td>MDS and free-ranking human relatedness tasks; Spearman correlation between Ebiquity sentence rankings and human rankings; further qualitative inspection of sentence-level mismatches.</td>
                        </tr>
                        <tr>
                            <td><strong>key_result</strong></td>
                            <td>Ebiquity produced the highest overall correlation with human rankings among tested systems (reported ρ ≈ 0.80), outperforming pure distributional and simple baselines, but still showed notable failures for entailment/implicature and antonymy cases.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_theory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence</strong></td>
                            <td>Large sentence-level discrepancies (e.g., for 'small' and prototypically small objects like 'dime'; for 'speak' and sentences about negotiation/interview) indicate the hybrid model still lacks mechanisms for pragmatic inference (implicature) and certain entailments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Han et al., 2013; Glasgow et al., (unknown year)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating semantic models with word-sentence relatedness', 'publication_date_yy_mm': '2016-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6920.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6920.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the representational format of conceptual knowledge in the brain at a functional level, including theoretical models, their descriptions, claimed representational formats, supporting or contradictory empirical evidence, experimental paradigms, key findings, and citation information.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WordSpot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Word-spotting baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple baseline oracle that ranks any sentence containing the target word above sentences that do not; beyond that, rankings are random. Used to set a minimal performance benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>Symbol Presence Baseline (WordSpot)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>symbolic indicator / Boolean presence</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Represents conceptual relatedness purely as binary presence/absence of the target lexical item within a sentence; no graded or contextual representation.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_claims</strong></td>
                            <td>If presence of a lexical item is the main determinant of relatedness, this oracle should approximate human judgments; used as a baseline to gauge model sophistication.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>computational model comparison (simulated baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_paradigm</strong></td>
                            <td>Monte Carlo simulation of constrained random ranking compared to human rankings (Spearman); MDS/free-ranking human data as ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>key_result</strong></td>
                            <td>WordSpot correlation with human rankings was modest (reported mean ρ ≈ 0.46) and was outperformed by nearly all substantive models, indicating lexical presence alone is insufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_theory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence</strong></td>
                            <td>Human raters frequently ranked sentences without the explicit target word as highly related (due to entailment/implicature/prototypical instances), so pure word-spotting fails to capture many human inferences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Glasgow et al., (unknown year)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating semantic models with word-sentence relatedness', 'publication_date_yy_mm': '2016-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6920.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6920.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the representational format of conceptual knowledge in the brain at a functional level, including theoretical models, their descriptions, claimed representational formats, supporting or contradictory empirical evidence, experimental paradigms, key findings, and citation information.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RelSpot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Relatedness-spotting baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline oracle that classifies words into three categories (target, related-to-target, unrelated) and constrains rankings so target-containing sentences rank highest, then related-containing sentences, then unrelated sentences; within categories ordering is random.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>Categorical Relatedness Baseline (RelSpot)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>symbolic categorical representation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Represents conceptual relatedness with coarse categorical labels (target / related / unrelated) rather than graded continuous representations; serves as an intermediate benchmark between word-spotting and continuous models.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_claims</strong></td>
                            <td>If coarse categorical relatedness suffices for human judgments, this oracle should approach human performance; used to contextualize model performance.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>computational model comparison (simulated baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_paradigm</strong></td>
                            <td>Monte Carlo simulation constrained by categories compared to human MDS/free-ranking data; Spearman correlation reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_result</strong></td>
                            <td>RelSpot performed substantially better than WordSpot (reported ρ ≈ 0.76) and was outperformed only by the strongest STS systems, indicating that coarse categorical knowledge of related words captures much but not all of human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_theory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence</strong></td>
                            <td>Participants sometimes ranked sentences with related words lower than some target-containing sentences and used richer contextual/pragmatic reasoning; thus coarse categorical labels are insufficient for full human-like judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Glasgow et al., (unknown year)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating semantic models with word-sentence relatedness', 'publication_date_yy_mm': '2016-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6920.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6920.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the representational format of conceptual knowledge in the brain at a functional level, including theoretical models, their descriptions, claimed representational formats, supporting or contradictory empirical evidence, experimental paradigms, key findings, and citation information.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Distributed account</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Distributed account of conceptual knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A theoretical stance proposing that conceptual knowledge is represented in distributed networks across modality-specific and association cortex rather than in single localized symbols, with meaning emerging from patterns of activation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Towards a distributed account of conceptual knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>Distributed Representational Account</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>distributed neural pattern / feature-based</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Concepts are encoded as distributed patterns of activation across neural populations (features or modality-specific representations); no single node stores a concept — meaning arises from the pattern and interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_claims</strong></td>
                            <td>Explains category-generalization, graded typicality, and lesion patterns (partial loss of features); predicts distributed neural activation patterns correlated with conceptual content.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>theoretical review (cited literature); not tested in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_paradigm</strong></td>
                            <td>not applicable in this paper (referenced literature includes lesion studies, neuroimaging, behavioral tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>key_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>supports_theory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence</strong></td>
                            <td>No direct tests reported in this paper; the current behavioral/computational comparisons are consistent with distributed/graded representations in that graded similarity is observed, but the paper does not evaluate neural distributed accounts directly.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Tyler & Moss, 2001</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating semantic models with word-sentence relatedness', 'publication_date_yy_mm': '2016-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6920.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6920.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the representational format of conceptual knowledge in the brain at a functional level, including theoretical models, their descriptions, claimed representational formats, supporting or contradictory empirical evidence, experimental paradigms, key findings, and citation information.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Semantic networks</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large-scale semantic networks / graph models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Representations that model conceptual knowledge as networks of nodes (concepts) and edges (relations), capturing large-scale statistical structure and growth dynamics of semantic knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The Large-scale structure of semantic networks: Statistical analyses and a model of semantic growth.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>Semantic Network / Graph Models</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>relational network</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Concepts are nodes in a graph linked by various semantic relations; network topology (connectivity, hubs) and growth processes explain semantic organization and associative behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_claims</strong></td>
                            <td>Accounts for associative priming, network-based semantic distances, and large-scale statistical properties of conceptual knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>citation to empirical/statistical analyses and modeling in referenced work; not directly tested in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_paradigm</strong></td>
                            <td>not applicable in this paper (referenced literature uses corpus analysis, modeling, behavioral data)</td>
                        </tr>
                        <tr>
                            <td><strong>key_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>supports_theory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence</strong></td>
                            <td>Paper cites semantic-network work as background; no direct supporting or contradicting empirical tests presented here.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Steyvers & Tenenbaum, 2005</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating semantic models with word-sentence relatedness', 'publication_date_yy_mm': '2016-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6920.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e6920.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the representational format of conceptual knowledge in the brain at a functional level, including theoretical models, their descriptions, claimed representational formats, supporting or contradictory empirical evidence, experimental paradigms, key findings, and citation information.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Patterson review</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Where do you know what you know? The representation of semantic knowledge in the human brain.</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A comprehensive review of theories and evidence regarding the neural representation of semantic memory, discussing distributed vs. hub-based and other accounts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Where do you know what you know? The representation of semantic knowledge in the human brain.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>Semantic memory review (multiple representational formats discussed)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>mixed (review of distributed, hub-and-spoke, and other models)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Survey of competing theoretical accounts for neural representation of conceptual knowledge, including distributed modality-specific representations, central semantic hubs, and network models.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_claims</strong></td>
                            <td>Synthesizes evidence relating neural structures to aspects of conceptual representation (e.g., modality-specific contributions, anterior temporal lobe hub hypotheses).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>review of lesion, neuroimaging, and behavioral studies (cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_paradigm</strong></td>
                            <td>not applicable in this paper (referenced as background)</td>
                        </tr>
                        <tr>
                            <td><strong>key_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>supports_theory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence</strong></td>
                            <td>Referenced as background; the current paper does not test neural hypotheses from this review.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Patterson et al., 2007</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating semantic models with word-sentence relatedness', 'publication_date_yy_mm': '2016-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Where do you know what you know? The representation of semantic knowledge in the human brain. <em>(Rating: 2)</em></li>
                <li>Towards a distributed account of conceptual knowledge. <em>(Rating: 2)</em></li>
                <li>Indexing by latent semantic analysis. <em>(Rating: 2)</em></li>
                <li>Distributed representations of words and phrases and their compositionality. <em>(Rating: 2)</em></li>
                <li>UMBC EBIQUITY-CORE: Semantic textual similarity systems. <em>(Rating: 2)</em></li>
                <li>The Large-scale structure of semantic networks: Statistical analyses and a model of semantic growth. <em>(Rating: 2)</em></li>
                <li>Class-based n-gram models of natural language. <em>(Rating: 1)</em></li>
                <li>WordNet: a lexical database for English. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6920",
    "paper_id": "paper-bfab20022d4d251704984d7f66a31168ecffedba",
    "extraction_schema_id": "extraction-schema-133",
    "extracted_data": [
        {
            "name_short": "LSA",
            "name_full": "Latent Semantic Analysis",
            "brief_description": "A distributional, matrix-factorization method that represents words as vectors in a reduced-rank semantic space derived from word–document co-occurrence; sentence meaning is typically composed by combining word vectors and computing cosine similarity.",
            "citation_title": "Indexing by latent semantic analysis.",
            "mention_or_use": "use",
            "theory_name": "Distributional Semantic Model (LSA)",
            "theory_type": "high-dimensional space (feature-based vector)",
            "theory_description": "Concepts are represented as points (vectors) in a low-dimensional semantic space derived from co-occurrence statistics; similarity is given by geometric proximity (e.g., cosine). Sentences are represented by composition (e.g., averaging) of constituent word vectors.",
            "functional_claims": "Accounts for graded semantic relatedness between words and larger linguistic units; can predict human similarity/relatedness judgments based on distributional context.",
            "evidence_source": "behavioral experiment (human relatedness judgments) + computational model comparison",
            "experimental_paradigm": "Human Maximum Difference Scaling (MDS) and free-ranking relatedness tasks; Spearman correlation of model-produced sentence rankings with human rankings.",
            "key_result": "LSA correlations with behavior were substantial (reported ρ ≈ 0.72 across targets) indicating the model captures much variance in human relatedness rankings, but it fails on cases requiring entailment/implicature or discriminating antonyms.",
            "supports_theory": true,
            "counter_evidence": "Systematic mismatches: LSA (and similar distributional models) rank antonyms and contextually-entailed relations as similar (e.g., 'big' vs 'small' contexts), and are insensitive to entailments/implicatures (e.g., prototypically small objects like 'dime' not recognized as related to 'small'), indicating limitations for capturing some human semantic inferences.",
            "citation": "Deerwester et al., 1990; Glasgow et al., (unknown year)",
            "uuid": "e6920.0",
            "source_info": {
                "paper_title": "Evaluating semantic models with word-sentence relatedness",
                "publication_date_yy_mm": "2016-03"
            }
        },
        {
            "name_short": "Word2Vec",
            "name_full": "Word2Vec (skip-gram neural embeddings)",
            "brief_description": "A neural, distributional embedding method that learns dense vector representations for words by predicting surrounding context (skip-gram); semantic similarity computed via cosine between vectors and simple sentence representations composed by averaging word vectors.",
            "citation_title": "Distributed representations of words and phrases and their compositionality.",
            "mention_or_use": "use",
            "theory_name": "Distributional Semantic Model (Word2Vec)",
            "theory_type": "high-dimensional space (feature-based vector)",
            "theory_description": "Concepts are encoded as continuous-valued vectors learned from local context prediction; semantic composition for phrases/sentences uses vector combination (e.g., averaging) to yield representations comparable to word vectors.",
            "functional_claims": "Predicts semantic relatedness and captures many lexical-semantic regularities; can be used compositionally to compare words to sentences and predict human judgments.",
            "evidence_source": "behavioral experiment (human relatedness judgments) + computational model comparison",
            "experimental_paradigm": "MDS and free-ranking relatedness tasks; model-to-human ranking correlations (Spearman).",
            "key_result": "Word2Vec achieved high correlation with human rankings (reported ρ ≈ 0.78), indicating strong alignment with human relatedness judgments, but it also failed on entailment/implicature and antonymy cases.",
            "supports_theory": true,
            "counter_evidence": "Misses inferential relations (entailments/implicatures) and sometimes treats antonyms as similar due to shared contexts (e.g., 'big' contexts similar to 'small'), producing large rank discrepancies for specific sentences compared to human judgments.",
            "citation": "Mikolov et al., 2013; Glasgow et al., (unknown year)",
            "uuid": "e6920.1",
            "source_info": {
                "paper_title": "Evaluating semantic models with word-sentence relatedness",
                "publication_date_yy_mm": "2016-03"
            }
        },
        {
            "name_short": "N-gram",
            "name_full": "Local co-occurrence / N-gram model",
            "brief_description": "A surface co-occurrence model that estimates semantic relatedness from local word co-occurrence counts within a sliding window (here: five-word window) in a corpus, normalized to produce pairwise co-occurrence frequencies.",
            "citation_title": "Class-based n-gram models of natural language.",
            "mention_or_use": "use",
            "theory_name": "Local Co-occurrence (n-gram) Model",
            "theory_type": "co-occurrence frequency matrix (symbolic / count-based)",
            "theory_description": "Represents concepts via counts of nearby word occurrences (local context); sentence relatedness computed by summing co-occurrence associations between a target and words in the sentence.",
            "functional_claims": "Surface-level co-occurrence patterns reflect semantic relatedness; simple co-occurrence can predict some human similarity judgments.",
            "evidence_source": "behavioral experiment + computational model comparison",
            "experimental_paradigm": "MDS and free-ranking relatedness tasks; Spearman correlations between model rankings and human rankings.",
            "key_result": "N-gram (local co-occurrence) performed poorly relative to higher-order models (reported ρ ≈ 0.39) and often fell below baseline performance, indicating surface co-occurrence is insufficient to capture higher-level conceptual relatedness.",
            "supports_theory": false,
            "counter_evidence": "Sub-baseline performance shows that simple local co-occurrence cannot account for many human judgments that rely on abstract contextual knowledge, entailment, or implicature.",
            "citation": "Brown et al., 1992; Glasgow et al., (unknown year)",
            "uuid": "e6920.2",
            "source_info": {
                "paper_title": "Evaluating semantic models with word-sentence relatedness",
                "publication_date_yy_mm": "2016-03"
            }
        },
        {
            "name_short": "WordNet",
            "name_full": "WordNet lexical taxonomy",
            "brief_description": "A manually curated lexical database organizing words/concepts into synsets and explicit semantic relations (e.g., hypernymy, hyponymy, meronymy), usable to compute taxonomic distances between concepts.",
            "citation_title": "WordNet: a lexical database for English.",
            "mention_or_use": "use",
            "theory_name": "Knowledge-based Taxonomic Representation (WordNet)",
            "theory_type": "relational network / taxonomic hierarchy",
            "theory_description": "Conceptual knowledge is represented as nodes in a structured graph/taxonomy where relations (is-a, part-of, etc.) encode semantic relationships and distances reflect conceptual similarity.",
            "functional_claims": "Encodes explicit lexical and taxonomic relations that can complement corpus-based distributional information to improve semantic similarity judgments.",
            "evidence_source": "computational model comparison (hybrid system performance)",
            "experimental_paradigm": "STS model comparisons to human relatedness rankings; Ebiquity fused WordNet-based distances with LSA outputs.",
            "key_result": "Incorporating WordNet taxonomy into a hybrid STS system (Ebiquity) improved correlation with human judgments (Ebiquity ρ ≈ 0.80), suggesting taxonomy adds complementary, useful structure beyond pure distributional signals.",
            "supports_theory": true,
            "counter_evidence": "Even with taxonomic information, the hybrid system still missed many human inferences based on entailment and implicature, indicating explicit taxonomy alone is not sufficient for all aspects of human conceptual inference.",
            "citation": "Miller, 1995; Han et al., 2013; Glasgow et al., (unknown year)",
            "uuid": "e6920.3",
            "source_info": {
                "paper_title": "Evaluating semantic models with word-sentence relatedness",
                "publication_date_yy_mm": "2016-03"
            }
        },
        {
            "name_short": "Ebiquity",
            "name_full": "UMBC Ebiquity (hybrid STS system)",
            "brief_description": "A semantic textual similarity system that fuses corpus-based (LSA) and knowledge-based (WordNet) similarity estimates, weighting and combining them to produce a single similarity score for words/sentences.",
            "citation_title": "UMBC EBIQUITY-CORE: Semantic textual similarity systems.",
            "mention_or_use": "use",
            "theory_name": "Hybrid Distributional + Taxonomic Model (Ebiquity)",
            "theory_type": "hybrid: high-dimensional vector space + relational taxonomy",
            "theory_description": "Combines distributional vector-space representations with explicit taxonomic distances from WordNet, leveraging complementary strengths: statistical contextual similarity plus curated hierarchical relations.",
            "functional_claims": "By integrating corpus-derived and knowledge-derived signals, the model better predicts human semantic relatedness and composes sentence meaning from constituent word meanings.",
            "evidence_source": "behavioral experiment + computational model comparison",
            "experimental_paradigm": "MDS and free-ranking human relatedness tasks; Spearman correlation between Ebiquity sentence rankings and human rankings; further qualitative inspection of sentence-level mismatches.",
            "key_result": "Ebiquity produced the highest overall correlation with human rankings among tested systems (reported ρ ≈ 0.80), outperforming pure distributional and simple baselines, but still showed notable failures for entailment/implicature and antonymy cases.",
            "supports_theory": true,
            "counter_evidence": "Large sentence-level discrepancies (e.g., for 'small' and prototypically small objects like 'dime'; for 'speak' and sentences about negotiation/interview) indicate the hybrid model still lacks mechanisms for pragmatic inference (implicature) and certain entailments.",
            "citation": "Han et al., 2013; Glasgow et al., (unknown year)",
            "uuid": "e6920.4",
            "source_info": {
                "paper_title": "Evaluating semantic models with word-sentence relatedness",
                "publication_date_yy_mm": "2016-03"
            }
        },
        {
            "name_short": "WordSpot",
            "name_full": "Word-spotting baseline",
            "brief_description": "A simple baseline oracle that ranks any sentence containing the target word above sentences that do not; beyond that, rankings are random. Used to set a minimal performance benchmark.",
            "citation_title": "here",
            "mention_or_use": "use",
            "theory_name": "Symbol Presence Baseline (WordSpot)",
            "theory_type": "symbolic indicator / Boolean presence",
            "theory_description": "Represents conceptual relatedness purely as binary presence/absence of the target lexical item within a sentence; no graded or contextual representation.",
            "functional_claims": "If presence of a lexical item is the main determinant of relatedness, this oracle should approximate human judgments; used as a baseline to gauge model sophistication.",
            "evidence_source": "computational model comparison (simulated baseline)",
            "experimental_paradigm": "Monte Carlo simulation of constrained random ranking compared to human rankings (Spearman); MDS/free-ranking human data as ground truth.",
            "key_result": "WordSpot correlation with human rankings was modest (reported mean ρ ≈ 0.46) and was outperformed by nearly all substantive models, indicating lexical presence alone is insufficient.",
            "supports_theory": false,
            "counter_evidence": "Human raters frequently ranked sentences without the explicit target word as highly related (due to entailment/implicature/prototypical instances), so pure word-spotting fails to capture many human inferences.",
            "citation": "Glasgow et al., (unknown year)",
            "uuid": "e6920.5",
            "source_info": {
                "paper_title": "Evaluating semantic models with word-sentence relatedness",
                "publication_date_yy_mm": "2016-03"
            }
        },
        {
            "name_short": "RelSpot",
            "name_full": "Relatedness-spotting baseline",
            "brief_description": "A baseline oracle that classifies words into three categories (target, related-to-target, unrelated) and constrains rankings so target-containing sentences rank highest, then related-containing sentences, then unrelated sentences; within categories ordering is random.",
            "citation_title": "here",
            "mention_or_use": "use",
            "theory_name": "Categorical Relatedness Baseline (RelSpot)",
            "theory_type": "symbolic categorical representation",
            "theory_description": "Represents conceptual relatedness with coarse categorical labels (target / related / unrelated) rather than graded continuous representations; serves as an intermediate benchmark between word-spotting and continuous models.",
            "functional_claims": "If coarse categorical relatedness suffices for human judgments, this oracle should approach human performance; used to contextualize model performance.",
            "evidence_source": "computational model comparison (simulated baseline)",
            "experimental_paradigm": "Monte Carlo simulation constrained by categories compared to human MDS/free-ranking data; Spearman correlation reported.",
            "key_result": "RelSpot performed substantially better than WordSpot (reported ρ ≈ 0.76) and was outperformed only by the strongest STS systems, indicating that coarse categorical knowledge of related words captures much but not all of human judgments.",
            "supports_theory": false,
            "counter_evidence": "Participants sometimes ranked sentences with related words lower than some target-containing sentences and used richer contextual/pragmatic reasoning; thus coarse categorical labels are insufficient for full human-like judgments.",
            "citation": "Glasgow et al., (unknown year)",
            "uuid": "e6920.6",
            "source_info": {
                "paper_title": "Evaluating semantic models with word-sentence relatedness",
                "publication_date_yy_mm": "2016-03"
            }
        },
        {
            "name_short": "Distributed account",
            "name_full": "Distributed account of conceptual knowledge",
            "brief_description": "A theoretical stance proposing that conceptual knowledge is represented in distributed networks across modality-specific and association cortex rather than in single localized symbols, with meaning emerging from patterns of activation.",
            "citation_title": "Towards a distributed account of conceptual knowledge.",
            "mention_or_use": "mention",
            "theory_name": "Distributed Representational Account",
            "theory_type": "distributed neural pattern / feature-based",
            "theory_description": "Concepts are encoded as distributed patterns of activation across neural populations (features or modality-specific representations); no single node stores a concept — meaning arises from the pattern and interactions.",
            "functional_claims": "Explains category-generalization, graded typicality, and lesion patterns (partial loss of features); predicts distributed neural activation patterns correlated with conceptual content.",
            "evidence_source": "theoretical review (cited literature); not tested in this paper",
            "experimental_paradigm": "not applicable in this paper (referenced literature includes lesion studies, neuroimaging, behavioral tasks)",
            "key_result": null,
            "supports_theory": null,
            "counter_evidence": "No direct tests reported in this paper; the current behavioral/computational comparisons are consistent with distributed/graded representations in that graded similarity is observed, but the paper does not evaluate neural distributed accounts directly.",
            "citation": "Tyler & Moss, 2001",
            "uuid": "e6920.7",
            "source_info": {
                "paper_title": "Evaluating semantic models with word-sentence relatedness",
                "publication_date_yy_mm": "2016-03"
            }
        },
        {
            "name_short": "Semantic networks",
            "name_full": "Large-scale semantic networks / graph models",
            "brief_description": "Representations that model conceptual knowledge as networks of nodes (concepts) and edges (relations), capturing large-scale statistical structure and growth dynamics of semantic knowledge.",
            "citation_title": "The Large-scale structure of semantic networks: Statistical analyses and a model of semantic growth.",
            "mention_or_use": "mention",
            "theory_name": "Semantic Network / Graph Models",
            "theory_type": "relational network",
            "theory_description": "Concepts are nodes in a graph linked by various semantic relations; network topology (connectivity, hubs) and growth processes explain semantic organization and associative behavior.",
            "functional_claims": "Accounts for associative priming, network-based semantic distances, and large-scale statistical properties of conceptual knowledge.",
            "evidence_source": "citation to empirical/statistical analyses and modeling in referenced work; not directly tested in this paper",
            "experimental_paradigm": "not applicable in this paper (referenced literature uses corpus analysis, modeling, behavioral data)",
            "key_result": null,
            "supports_theory": null,
            "counter_evidence": "Paper cites semantic-network work as background; no direct supporting or contradicting empirical tests presented here.",
            "citation": "Steyvers & Tenenbaum, 2005",
            "uuid": "e6920.8",
            "source_info": {
                "paper_title": "Evaluating semantic models with word-sentence relatedness",
                "publication_date_yy_mm": "2016-03"
            }
        },
        {
            "name_short": "Patterson review",
            "name_full": "Where do you know what you know? The representation of semantic knowledge in the human brain.",
            "brief_description": "A comprehensive review of theories and evidence regarding the neural representation of semantic memory, discussing distributed vs. hub-based and other accounts.",
            "citation_title": "Where do you know what you know? The representation of semantic knowledge in the human brain.",
            "mention_or_use": "mention",
            "theory_name": "Semantic memory review (multiple representational formats discussed)",
            "theory_type": "mixed (review of distributed, hub-and-spoke, and other models)",
            "theory_description": "Survey of competing theoretical accounts for neural representation of conceptual knowledge, including distributed modality-specific representations, central semantic hubs, and network models.",
            "functional_claims": "Synthesizes evidence relating neural structures to aspects of conceptual representation (e.g., modality-specific contributions, anterior temporal lobe hub hypotheses).",
            "evidence_source": "review of lesion, neuroimaging, and behavioral studies (cited work)",
            "experimental_paradigm": "not applicable in this paper (referenced as background)",
            "key_result": null,
            "supports_theory": null,
            "counter_evidence": "Referenced as background; the current paper does not test neural hypotheses from this review.",
            "citation": "Patterson et al., 2007",
            "uuid": "e6920.9",
            "source_info": {
                "paper_title": "Evaluating semantic models with word-sentence relatedness",
                "publication_date_yy_mm": "2016-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Where do you know what you know? The representation of semantic knowledge in the human brain.",
            "rating": 2,
            "sanitized_title": "where_do_you_know_what_you_know_the_representation_of_semantic_knowledge_in_the_human_brain"
        },
        {
            "paper_title": "Towards a distributed account of conceptual knowledge.",
            "rating": 2,
            "sanitized_title": "towards_a_distributed_account_of_conceptual_knowledge"
        },
        {
            "paper_title": "Indexing by latent semantic analysis.",
            "rating": 2,
            "sanitized_title": "indexing_by_latent_semantic_analysis"
        },
        {
            "paper_title": "Distributed representations of words and phrases and their compositionality.",
            "rating": 2,
            "sanitized_title": "distributed_representations_of_words_and_phrases_and_their_compositionality"
        },
        {
            "paper_title": "UMBC EBIQUITY-CORE: Semantic textual similarity systems.",
            "rating": 2,
            "sanitized_title": "umbc_ebiquitycore_semantic_textual_similarity_systems"
        },
        {
            "paper_title": "The Large-scale structure of semantic networks: Statistical analyses and a model of semantic growth.",
            "rating": 2,
            "sanitized_title": "the_largescale_structure_of_semantic_networks_statistical_analyses_and_a_model_of_semantic_growth"
        },
        {
            "paper_title": "Class-based n-gram models of natural language.",
            "rating": 1,
            "sanitized_title": "classbased_ngram_models_of_natural_language"
        },
        {
            "paper_title": "WordNet: a lexical database for English.",
            "rating": 2,
            "sanitized_title": "wordnet_a_lexical_database_for_english"
        }
    ],
    "cost": 0.018026249999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Evaluating semantic models with word-sentence relatedness</h1>
<p>Kimberly Glasgow , Matthew Roos , Amy Haufler , Mark Chevillet , and Michael Wolmetz*</p>
<p>The Johns Hopkins University Applied Physics Laboratory
Laurel, MD</p>
<h4>Abstract</h4>
<p>Semantic textual similarity (STS) systems are designed to encode and evaluate the semantic similarity between words, phrases, sentences, and documents. One method for assessing the quality or authenticity of semantic information encoded in these systems is by comparison with human judgments. A data set for evaluating semantic models was developed consisting of 775 English word-sentence pairs, each annotated for semantic relatedness by human raters engaged in a Maximum Difference Scaling (MDS) task, as well as a faster alternative task. As a sample application of this relatedness data, behavior-based relatedness was compared to the relatedness computed via four off-the-shelf STS models: n-gram, Latent Semantic Analysis (LSA), Word2Vec, and UMBC Ebiquity. Some STS models captured much of the variance in the human judgments collected, but they were not sensitive to the implicatures and entailments that were processed and considered by the participants. All text stimuli and judgment data have been made freely available.</p>
<h2>Author Keywords</h2>
<p>Semantic similarity; semantic relatedness; semeval; semantic textual similarity; conceptual knowledge; datasets; evaluation; benchmarking</p>
<h2>Corresponding Author</h2>
<p>Michael Wolmetz, michael.wolmetz@jhuapl.edu</p>
<h2>INTRODUCTION</h2>
<p>There is some disagreement about what is meant by the terms conceptual knowledge and semantic memory, but most characterizations involve the encoding of beliefs or propositions about concepts (e.g. objects, actions, properties, etc.), the ability to organize those concepts into useful sets, and the relationships within and between sets [16, 18, 20]. From the perspective of the cognitive sciences, distinctions are typically made between the knowledge or memories closely tied to an individual's personal experiences, and those beliefs generally held in common by many individuals across a society, or many societies. Use of the term conceptual knowledge is</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>often biased toward these more universal beliefs and knowledge about concepts.
Conceptual knowledge is studied by several related but distinct communities. In cognitive science, cognitive psychology, cognitive neuroscience, and computational linguistics, conceptual knowledge is typically studied in the context of discovering how concepts are learned, represented, and applied by humans. In natural language processing (NLP) and information retrieval, conceptual knowledge is often studied in the context of automated systems designed to make semantic inferences and retrieve information based on conceptual information. The semantic models that drive these semantic textual similarity (STS) systems are learned from different properties or statistics extracted from text corpora (e.g. Wikipedia) or from the different relationships extracted from structured or taxonomic lexical knowledge databases (e.g. WordNet).</p>
<p>Across all of these communities, ground-truths about concepts are necessary for testing theories and evaluating models or systems. These ground-truths can be challenging to generate because the attributes or dimensions of what should be considered part of a ground-truth may be dependent on the particular theory, model, or system being evaluated. One general solution to this problem has been to abstract away from specific attributes in favor of ground-truths about the relationships between concepts [17]. Models can then be evaluated in terms of how well they predict either the semantic distances between concepts [3] or the structures or networks estimated from those distances [19]. These distances or structures can be estimated from behavior-based measures like ratings or response times, corpus metrics, or relationships encoded in knowledge bases.</p>
<p>A variety of semantic similarity and relatedness data sets have been used for these purposes. Many of the data sets used for NLP and evaluating STS systems are public, including those adopted by the International Workshop on Semantic Evaluation (Semeval, e.g. [12]). Many of the data sets used in the cognitive sciences, though often reported on in published studies, have not been publicly released.</p>
<p>Here we present a new data set for evaluating models of conceptual knowledge based on the relatedness between words and sentences. It consists of 25 target concepts (i.e. words) that span several semantic dimensions (objects, actions, settings, roles, states, and events), and parts of speech (nouns, verbs, and adjectives). Each of the 25 target words was paired with 31 simple sentences written and selected to elicit vary-</p>
<p>ing degrees of perceived relatedness to the target word, for a total of 775 word-sentence pairings. For example, the 31 sentences paired with the target word family are shown in Table 1.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Sentence ID</th>
<th style="text-align: center;">Sentences for family</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">The family was happy.</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">The family played at the beach.</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">The family survived the powerful hurricane.</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">The wealthy family celebrated at the party.</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">The politician visited the family.</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">The parent watched the sick child.</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">The priest approached the lonely family.</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">The parent visited the school.</td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">The parent shouted at the child.</td>
</tr>
<tr>
<td style="text-align: center;">10</td>
<td style="text-align: center;">The parent took the cellphone.</td>
</tr>
<tr>
<td style="text-align: center;">11</td>
<td style="text-align: center;">The couple planned the vacation.</td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">The happy couple visited the embassy.</td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">The parent bought the magazine.</td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">The couple laughed at dinner.</td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">The couple read on the beach.</td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">The wealthy couple left the theater.</td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">The happy child found the dime.</td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">The child broke the glass in the restaurant.</td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">The child gave the flower to the artist.</td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">The child held the soft feather.</td>
</tr>
<tr>
<td style="text-align: center;">21</td>
<td style="text-align: center;">The angry child threw the book.</td>
</tr>
<tr>
<td style="text-align: center;">22</td>
<td style="text-align: center;">The girl dropped the shiny dime.</td>
</tr>
<tr>
<td style="text-align: center;">23</td>
<td style="text-align: center;">The actor gave the football to the team.</td>
</tr>
<tr>
<td style="text-align: center;">24</td>
<td style="text-align: center;">The commander listened to the soldier.</td>
</tr>
<tr>
<td style="text-align: center;">25</td>
<td style="text-align: center;">The editor drank tea at dinner.</td>
</tr>
<tr>
<td style="text-align: center;">26</td>
<td style="text-align: center;">The soldier crossed the field.</td>
</tr>
<tr>
<td style="text-align: center;">27</td>
<td style="text-align: center;">The judge met the mayor.</td>
</tr>
<tr>
<td style="text-align: center;">28</td>
<td style="text-align: center;">The beach was empty.</td>
</tr>
<tr>
<td style="text-align: center;">29</td>
<td style="text-align: center;">The artist drew the river.</td>
</tr>
<tr>
<td style="text-align: center;">30</td>
<td style="text-align: center;">The doctor stole the book.</td>
</tr>
<tr>
<td style="text-align: center;">31</td>
<td style="text-align: center;">The window was dusty.</td>
</tr>
</tbody>
</table>
<p>Table 1. The set 31 sentences paired with the target concept family. Sentences are ranked from most related to the concept family to least related to the concept family, based on relatedness scores averaged across 47 participants.</p>
<p>Some sentences are clearly related to the concept family, others appear somewhat related or slightly related, and others have no clear or obvious relationship to the target concept. These relationships were tested experimentally using a Maximum Difference Scaling procedure [6], and replicated using a simpler procedure. As a sample application, the resulting ground-truth ratings were compared to the outputs of four commercial off-the-shelf (COTS) STS systems (n-gram, LSA, Google word2vec, and UMBC Ebiquity), and two baseline STS models to help interpret performance levels.
This data set was originally generated for use with neural data as part of the Intelligence Advanced Research Projects Activity (IARPA) Knowledge Representation in Neural Systems (KRNS) Program. While results are not reported here, the data set was used to evaluate whether semantic relatedness was encoded in the neural responses evoked by these sentences, as measured by functional Magnetic Resonance Imag-
ing. All stimuli and results have been made available in the ancillary files published with this article.</p>
<h2>METHODS \&amp; MATERIALS</h2>
<h2>Stimuli</h2>
<p>Target concepts. Table 2 lists the 25 target concepts included. Each target concept is restricted to a specific sense or meaning, adapted from WordNet senses for the term [14]. The set of target concepts consists of nouns, verbs and adjectives, and is biased away from abstract or uncommon concepts, and toward vivid, imageable [15], and concrete concepts. The mean concreteness for target concepts is 4.18 on the 5-point scale reported by Brysbaert [2]. Target concepts were selected to span the six semantic dimensions described below.</p>
<ol>
<li>Objects: things that physically exist. They may be animate or inanimate, natural or artifactual (man-made). Objects will commonly have physical substance, or be detectable by human senses. These often take the form of nouns. Objects may be count nouns or mass nouns. Examples: rabbit, hammer, pear, clock, water.</li>
<li>Actions: things that are done or experienced (felt or sensed), typically by people or other living things. These often take the form of verbs. Actions may involve moving, perceiving, feeling, creating, and so on. Examples: walks, hears, eats, builds, cooks.</li>
<li>Settings: where or when things happen. They may be places, spaces, or times. Indoor or outdoor locations, seasons, and times of day are appropriate. Examples: dining room, plaza, winter, morning.</li>
<li>Roles: what people do or who they are. They include vocations and professions, kinship, and social roles. Examples: athlete, victim, friend, brother, coach, plumber.</li>
<li>States, properties, conditions, and emotions: concepts that typically describe or characterize. They include human emotions, physical properties, conditions, colors, and so on. They may take the form of adjectives. Examples: dry, red, damaged, sad.</li>
<li>Events and activities: things that happen (e.g. human organized or natural events) or activities that are engaged in (e.g. hobbies). Examples: wedding, tornado, parade, tennis match.</li>
</ol>
<h2>Comparison sentences</h2>
<p>Each of the 25 target concepts was paired with 31 comparison sentences. Many comparison sentences were used across multiple target concepts, and in total, 240 distinct comparison sentences were generated (listed in the accompanying downloadable data).</p>
<p>The specific set of 31 sentences associated and tested with a given target concept was constructed and selected to represent a range of semantic relatedness between the target concept and the selected comparison sentences. To achieve this range of relatedness, some sentences included the target concept, some sentences included at least one concept related to the</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Target <br> Concept</th>
<th style="text-align: left;">POS</th>
<th style="text-align: left;">Dimension</th>
<th style="text-align: left;">Sense</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">family</td>
<td style="text-align: left;">N</td>
<td style="text-align: left;">Role</td>
<td style="text-align: left;">primary social group; parents and children</td>
</tr>
<tr>
<td style="text-align: left;">school</td>
<td style="text-align: left;">N</td>
<td style="text-align: left;">Setting</td>
<td style="text-align: left;">a building where young people receive education</td>
</tr>
<tr>
<td style="text-align: left;">small</td>
<td style="text-align: left;">A</td>
<td style="text-align: left;">State</td>
<td style="text-align: left;">limited or below average in number or quantity or magnitude or extent</td>
</tr>
<tr>
<td style="text-align: left;">speak</td>
<td style="text-align: left;">V</td>
<td style="text-align: left;">Action</td>
<td style="text-align: left;">exchange thoughts; talk with</td>
</tr>
<tr>
<td style="text-align: left;">break</td>
<td style="text-align: left;">V</td>
<td style="text-align: left;">Action</td>
<td style="text-align: left;">destroy the integrity of, usually by force; cause to separate into pieces or fragment</td>
</tr>
<tr>
<td style="text-align: left;">trial</td>
<td style="text-align: left;">N</td>
<td style="text-align: left;">Event</td>
<td style="text-align: left;">legal proceedings consisting of the judicial examination of issues by a competent tribunal, <br> including the determination of innocence or guilt by due process of law</td>
</tr>
<tr>
<td style="text-align: left;">protest</td>
<td style="text-align: left;">N</td>
<td style="text-align: left;">Event</td>
<td style="text-align: left;">the act of protesting; a public (often organized) manifestation of dissent</td>
</tr>
<tr>
<td style="text-align: left;">lawyer</td>
<td style="text-align: left;">N</td>
<td style="text-align: left;">Role</td>
<td style="text-align: left;">a professional person authorized to practice law; conducts lawsuits or gives legal advice</td>
</tr>
<tr>
<td style="text-align: left;">doctor</td>
<td style="text-align: left;">N</td>
<td style="text-align: left;">Role</td>
<td style="text-align: left;">a licensed medical practitioner</td>
</tr>
<tr>
<td style="text-align: left;">walk</td>
<td style="text-align: left;">V</td>
<td style="text-align: left;">Action</td>
<td style="text-align: left;">use ones feet to advance, advance by steps</td>
</tr>
<tr>
<td style="text-align: left;">computer</td>
<td style="text-align: left;">N</td>
<td style="text-align: left;">Object</td>
<td style="text-align: left;">a machine for performing calculations automatically</td>
</tr>
<tr>
<td style="text-align: left;">spring</td>
<td style="text-align: left;">N</td>
<td style="text-align: left;">Setting</td>
<td style="text-align: left;">the season of growth</td>
</tr>
<tr>
<td style="text-align: left;">tourist</td>
<td style="text-align: left;">N</td>
<td style="text-align: left;">Role</td>
<td style="text-align: left;">someone who travels for pleasure</td>
</tr>
<tr>
<td style="text-align: left;">eat</td>
<td style="text-align: left;">V</td>
<td style="text-align: left;">Action</td>
<td style="text-align: left;">take in solid food</td>
</tr>
<tr>
<td style="text-align: left;">magazine</td>
<td style="text-align: left;">N</td>
<td style="text-align: left;">Object</td>
<td style="text-align: left;">product consisting of a paperback periodic publication as a physical object</td>
</tr>
<tr>
<td style="text-align: left;">angry</td>
<td style="text-align: left;">A</td>
<td style="text-align: left;">State</td>
<td style="text-align: left;">feeling or showing anger</td>
</tr>
<tr>
<td style="text-align: left;">park</td>
<td style="text-align: left;">N</td>
<td style="text-align: left;">Setting</td>
<td style="text-align: left;">a large area of land preserved in its natural state as public property, a piece of open land for <br> recreational use</td>
</tr>
<tr>
<td style="text-align: left;">wealthy</td>
<td style="text-align: left;">A</td>
<td style="text-align: left;">State</td>
<td style="text-align: left;">having an abundant supply of money or possessions of value</td>
</tr>
<tr>
<td style="text-align: left;">storm</td>
<td style="text-align: left;">N</td>
<td style="text-align: left;">Event</td>
<td style="text-align: left;">a violent weather condition with strong winds, commonly featuring precipitation, thunder <br> and lightning</td>
</tr>
<tr>
<td style="text-align: left;">soccer</td>
<td style="text-align: left;">N</td>
<td style="text-align: left;">Event</td>
<td style="text-align: left;">a football game in which two teams of 11 players try to kick or head a ball into the opponent <br> goal</td>
</tr>
<tr>
<td style="text-align: left;">kick</td>
<td style="text-align: left;">V</td>
<td style="text-align: left;">Action</td>
<td style="text-align: left;">strike, drive or propel with the foot</td>
</tr>
<tr>
<td style="text-align: left;">bird</td>
<td style="text-align: left;">N</td>
<td style="text-align: left;">Object</td>
<td style="text-align: left;">warm-blooded egg-laying vertebrates characterized by feathers and forelimbs modified as <br> wings</td>
</tr>
<tr>
<td style="text-align: left;">teacher</td>
<td style="text-align: left;">N</td>
<td style="text-align: left;">Role</td>
<td style="text-align: left;">a person whose occupation is teaching</td>
</tr>
<tr>
<td style="text-align: left;">dog</td>
<td style="text-align: left;">N</td>
<td style="text-align: left;">Object</td>
<td style="text-align: left;">a member of the genus Canis (probably descended from the common wolf) that has been <br> domesticated by man since prehistoric times</td>
</tr>
<tr>
<td style="text-align: left;">door</td>
<td style="text-align: left;">N</td>
<td style="text-align: left;">Object</td>
<td style="text-align: left;">a swinging or sliding barrier that will close the entrance to a room or building or vehicle</td>
</tr>
</tbody>
</table>
<p>Table 2. The set of target concepts and associated metadata, including parts of speech (POS), noun (N), verb (V), or adjective (A); dimension; and sense.
target concept, and the remaining sentences did not contain any concepts thought to be highly related to the target concept. Where grammatically and semantically feasible, target concepts or related concepts appeared in the subject for some sentences and in the predicate for others, and occurred with or without adjectival modifiers (for nouns).</p>
<h2>BEHAVIORAL PROCEDURE</h2>
<p>Human judgments were collected via a web-based application to assess the semantic relatedness between target concepts and comparison sentences. First, judgments were collected and analyzed using a Maximum Difference Scaling (MDS) or Best-Worst scaling paradigm [11]. Because MDS sometimes requires very large numbers of responses, we also tested the applicability of a second paradigm, thought to be simpler and less resource-intensive ranking paradigm.
MDS is a discrete choice technique used to evaluate relative importance or preference by asking participants to choose the best and worst options from a set of presented items. A variant of MDS was used in which three sentences were presented with a target concept on each trial, as shown in Figure 1. Participants were asked to consider the relatedness between the
target concepts and the three sentences, and select the sentence that best relates to the target concept, and the sentence that worst relates to the target concept. While the terms similarity and relatedness are at times used interchangeably, and at other times used to refer to different relationships, the word related was used in all instructions given to participants, and so all results should be interpreted in that context.</p>
<p>Responses from 155 of these trials were necessary to reconstruct relatedness scores for each of the comparison sentences for a single target concept, and a total of 3875 trials were collected for each participant. Presentation order was randomized across participants, and participants were permitted to $\log$ in and out of the application from the remote location of their choice, and complete the trials over as many sessions as necessary over the course of two weeks.</p>
<p>The relatedness between a particular target word $t$ and comparison sentence $s$ was computed via counting analysis: the proportion of times each $s$ was selected best for $t$ (i.e. most related) less the proportion of times $s$ was selected worst for $t$ (i.e. least related):</p>
<p>$$
\text { Relatedness }<em _="{" _text="\text" best="best" t_="t,">{s, t}=\left(s</em>
$$}}-s_{t, \text { worst }}\right) / s_{t, t o t a l</p>
<table>
<thead>
<tr>
<th style="text-align: left;">6. Select the Sentence that BEST relates to the word DOCTOR. Select the Sentence that</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">WORST relates to the word DOCTOR.</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Best</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">A.The doctor helped</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: left;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">the injured policeman.</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">B.The patient put the</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">medicine in the</td>
<td style="text-align: left;">$\square$</td>
<td style="text-align: left;">$\square$</td>
</tr>
<tr>
<td style="text-align: left;">cabinet.</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">C. The actual listened</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">to the tired victim.</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: left;">$\checkmark$</td>
</tr>
</tbody>
</table>
<p>Figure 1. One trial of the MDS paradigm for the target concept doctor. The participant selects the sentence that best relates to the target concept, and the sentence that worst relates to the target concept. Each participant responded to 155 of these trials for the target concept doctor, and a total of 3875 of these trials in total.</p>
<p>In this way, similarity scores ranged between -1 (not similar) and 1 (very similar), and could be used to rank the sentences from the most related (highest score) to the least related (least related). All further comparisons were done using the mean ranks of the sentences across participants.</p>
<p>The very precise scores produced by MDS require large amounts of data collected over many trials. To assess whether this degree of precision was necessary, an alternative paradigm was explored in which participants were presented with all 31 sentences at once (in random order) and asked to rank them in terms of their relatedness to the target concept (from most related to least related) by dragging and dropping them into a list. Participants repeated this free-ranking procedure to produce 25 ranked lists of sentences: one for each of the 25 different target concepts.</p>
<h2>PARTICIPANTS</h2>
<p>Participants were recruited via flyers posted at local university campuses, and earned $\$ 10$ per hour for participation in accordance with a protocol approved by The Johns Hopkins Medical Institutions Institutional Review Board. All participants were fluent in English, had a high school diploma or equivalent, had normal or corrected to normal vision, and did not have any reading disorders and language impairments, as self-reported. Fifty-five participants (mean age $=24$ ) completed the MDS task for all 25 concepts using a web-based application. The data collected was analyzed for outliers, and eight of the 55 participants produced responses that deviated by more than three standard deviations from group means. These participants were removed from further analysis; 20 of the remaining 47 subjects also completed the free-ranking paradigm.</p>
<h2>STS SYSTEMS</h2>
<p>The relatedness rankings produced by human participants were compared to relatedness rankings produced by four STS models, as well as two baseline text-based models to aid in interpretation. Each of the models listed below were run on the 25 target concept lists, and each produced 25 ranked lists of comparison sentences (just as the participants did). These ranked lists were compared to the behavior-based ranked lists using Spearman's rank correlation, and mean Spearman values are reported. All analyses were run in Matlab.</p>
<ol>
<li>N-gram: The basic n-gram model assumes that words that are related in meaning will occur in close proximity to one another [1]. In practice, a matrix of co-occurrence frequencies between pairs of content words was generated, where co-occurrence frequency refers to the frequency with which two words appear within a five-word window of one another in the Corpus of Contemporary American English (COCA) [4]. In this way, co-occurrence frequency was used as a measure of semantic relatedness. More specifically, the matrix of co-occurrence frequencies was constructed as follows: A raw count matrix was generated in which each row and column entry contained a count of the number of times the two indexed words were both found within a five-word window. The counts along the diagonal of the matrix were replaced by the total number of occurrences of the indexed word in the five-word windows that is, each time a word was observed, it was counted as being paired with itself. The columns of the matrix were then normalized by their sums, resulting in a matrix of relative co-occurrence frequencies. Similarity scores between target words and sentences were computed by summing the [target word, sentence word] elements of the matrix for all words in the sentence.</li>
<li>LSA: Latent Semantic Analysis is based on the principle that words that are related in meaning will occur in related texts or documents [5]. Similarity was computed as the cosine distance between entries in the low-rank matrix. Analyses were run both with and without the use of a POS tagger, but POS information did not consistently change performance.</li>
<li>Word2Vec: The Word2Vec system was built by Google using a neural network and a skip-gram model [13]. Skipgram models are trained to predict the surrounding context given a target word. In this way, the similarity between two Word2Vec representations is computed as the cosine distance between vectors, and is associated with the similarity between the local contexts they could appear in based on the model.</li>
<li>UMBC Ebiquity (Ebiquity): The University of Maryland Baltimore County (UMBC) Ebiquity team provided its STS for inclusion in this comparison. The Ebiquity system fuses corpus-based (i.e. LSA) and knowledge-based (i.e. WordNet) methods to produce semantic similarity scores [9]. To augment the POS-tagged LSA-based semantic similarity, the WordNet database of semantic and lexical relations is used to produce an alternative estimate of similarity based on the distance between two words or concepts in the WordNet taxonomy (e.g. couple and family are similar because they both appear as instances of social groups within the WordNet hierarchy). The corpus-based and knowledge-based similarities are then weighted based on a training process to produce a single similarity score. In this way, the WordNet taxonomy effectively fills in certain gaps in distributional information.
Ebiquity is specifically designed to compare words and sentences and compose a meaning for a sentence or phrase from the meanings of its constituent words. The other</li>
</ol>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2. Sentences for target concepts eat and storm clustered based on mean ranks using k-means clustering with k=4. Colors represent cluster assignments; error bars represent one standard deviation.</p>
<p>models had not been optimized for this task, and so some method for comparing sentences to words had to be adopted. For the vector-based approaches reported here, we adopted similar methods to the LSA component of the Ebiquity system for comparing sentences to words: average the normalized individual word vectors from a comparison sentence, then compute the cosine distance between the target concept vector and the averaged sentence vector to produce a relatedness score between a target concept and a comparison sentence. This method of comparing words to sentences performed best amongst several alternatives tested.</p>
<ol>
<li>
<p><strong>Word-spotting Baseline (WordSpot):</strong> The word-spotting model adopted here as a baseline is an oracle that can only determine whether a comparison sentence contains the target concept or not. Based on this, the word-spotting model is constrained to rank target-containing sentences above sentences that do not include the target, but beyond this constraint, ranks are random. WordSpot was simulated 1000 times for each target concept, and correlations at the 95% confidence interval were reported.</p>
</li>
<li>
<p><strong>Relatedness-spotting Baseline (RelSpot):</strong> This is similar to the word-spotting baseline model, but the relatedness-spotting model is an oracle that classifies words into one of three categories: targets, words related to the target, and unrelated words. Based on this, the relatedness-spotting model is constrained to rank target-containing sentences above sentences containing words related to the target, and these sentences, in turn, are constrained to be ranked above sentences comprised of unrelated words only. As with the WordSpot model, beyond this constraint, rankings are random. Note that the ranking heuristic used by RelSpot is often consistent with the behavioral responses, but participants do rank comparison sentences with related words higher than some target-containing sentences at non-trivial rates. RelSpot was simulated 1000 times for each target concept, and correlations at the 95% confidence interval were reported.</p>
</li>
</ol>
<h3>3.3 RESULTS AND DISCUSSION</h3>
<p>Data collected from both the MDS task and free-ranking task were used to produce group rankings for the sentences associated with each target concept (see ancillary files). As expected, participants often ranked sentences that contained the target concept higher than sentences that did not, and often ranked sentences that contained a concept related to the target concept higher than sentences that did not contain the target concept or any related concepts, but these patterns were not absolute. For example (as shown in Table 1), the sentence <em>The parent watched the sick child</em> was considered more related to the target concept family as compared to the sentence <em>The priest approached the lonely family</em>, suggesting that participants considered the larger context when evaluating relationships between target concepts and comparison sentences. K-means clustering for k=3 to 5 was run on the mean rank data (as well as the mean ranks and variances) to assess whether relatedness varied continuously or discontinuously across sentences. For some target concepts, the mean sentence ranks across participants were clearly clustered by relatedness to the target concept, while for other target concepts, the sentence rankings varied more continuously (as illustrated in Figure 2). For example, data collected for the target concept <em>eat</em> suggests that comparison sentences can be grouped into four somewhat separable classes: sentences highly related, related, somewhat related, and unrelated to the target concept. Alternatively, data collected for the target concept <em>storm</em> do not clearly fall in to these relatedness classes.</p>
<p>Cross-test reliability was calculated for the subset of participants who completed both the MDS task and the free-ranking task (N=20). While the MDS task produces scores ranging from -1 to 1 associated with relatedness, the free-ranking task only produces ranks, and so only ranks were compared between tasks. The mean rank difference between MDS and</p>
<table>
<thead>
<tr>
<th>System</th>
<th>Correlation with Behavior $(\rho)$</th>
</tr>
</thead>
<tbody>
<tr>
<td>N-gram</td>
<td>0.39</td>
</tr>
<tr>
<td>WordSpot</td>
<td>0.46</td>
</tr>
<tr>
<td>LSA</td>
<td>0.72</td>
</tr>
<tr>
<td>RelSpot</td>
<td>0.76</td>
</tr>
<tr>
<td>Word2Vec</td>
<td>0.78</td>
</tr>
<tr>
<td>Ebiquity</td>
<td>0.80</td>
</tr>
</tbody>
</table>
<p>Table 3. Correlations with judgment-based rankings across all Target Concepts. The WordSpot and RelSpon baseline models are shown in gray; correlations for these baseline models reflect the $95 \%$ Confidence Interval of performance based on a 1000 iteration Monte Carlo simulation.
free-ranking group rankings was 1.3: on average, a comparison sentences rank (of 31) on the MDS task was 1.3 positions away from that sentences rank according to the free-ranking task results, which is similar to the within-task reliability observed for these tasks. Given that the free-ranking task was on the order of seven times faster to complete, it may be a preferred method for collection of relatedness judgments.</p>
<h2>STS SYSTEM COMPARISON</h2>
<p>The sentence rankings from each of the STS models were correlated with the mean MDS rankings from participants, and the results are reported in Table 3. All models except for the n-gram model unambiguously outperformed the wordspotting baseline, but only the Ebiquity and Word2Vec systems outperformed the relationship-spotting benchmark system. These results suggest that all models except the n-gram model are capable of conceptual inference beyond simple word-spotting. The sub-baseline performance of the n-gram model might marginally improve with a larger corpus, but the interpretation would not change: higher-order models which can encode more abstract contextual information are consistently more predictive of human judgments as compared to the lower-order models that encode only surface information about local context. Indeed, both the Word2Vec and Ebiquity models consistently ranked sentences containing words related to the target above sentences that did not contain any related words. Given their beyond-benchmark performance, these models also appear capable of inferring some additional and useful conceptual structure.</p>
<p>Performance was not uniform across all target concepts. For some target concepts, STS results were quite similar to participant ranking (e.g. storm), and for others, there were large discrepancies (e.g. speak). Results for three representative target concepts compared to aggregate behavioral rankings are shown in Figure 3. To better understand the relationship between human and system ratings, the Ebiquity results were further examined at the individual sentence level.
For many comparison sentences and for most target concepts, the Ebiquity model produced rankings quite similar to those produced by the human raters, but there were notable exceptions. For several target concepts, some sentences were ranked more than 15 positions (of 31) higher or lower by the STS system than by human judgment. Some of these discrepancies suggest incompatibilities between the human task (i.e.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3. Model comparisons with behavior for rankings of three representative target concepts. For the target concept family, the n-gram model performs substantially worse than each of the other models. For storm, the Ebiquity system performs substantially better than each of the other models. For speak, all models perform similarly poorly at predicting behavioral responses. The collective poor performance for speak was partially due to the failure across models to infer that to speak is an important component of interview and negotiate, which are content words appearing in many of the comparison sentences associated with speak.
relatedness judgments) and the STS task (semantic similarity). Other discrepancies suggest true limitations of the STS systems.</p>
<p>A number of these large discrepancies occurred when concepts were related by contrasts in meaning (i.e. antonyms). For example, some of the largest discrepancies between the human and Ebiquity rankings involved the target concept small. Small is a gradable antonym, on the opposite end of a pole with big. Because big and small are both adjectives describing two poles of size, they might appear in similar contexts in corpora. As a result, the sentence The big horse drank from the lake was ranked the seventh most similar sentence to the target concept small (i.e. 7/31) by the Ebiquity system, and 30/31 as "best related" to the target concept small by human participants.</p>
<p>Multiple incompatible antonyms [7], which are mutually exclusive terms that belong to a set (e.g., the seasons of the year: spring, summer, fall, winter) showed similar patterns of re-</p>
<p>sults. This was observed with spring (a target concept) and other seasons that appear in comparison sentences like winter (i.e. The park was empty in winter: Human rank 25/31; Ebiquity rank 7/31). These and other similar results highlight the different conceptions of similarity and relatedness, as well as the limitations inherent in low-dimensional similarity and relatedness scores. Different instructions or a different task may have induced behavioral results more in line with the Ebiquity rankings, but this is also a relatively unexplored distinction with respect to STS research.</p>
<p>Other discrepancies between the human and STS results involved relationships that the STS model was not sensitive to, and that would not be remedied by alternative instructions or tasks. In these cases, the comparison sentences do not include the target concept, but instead, include concepts that prototypically embody the target concept or evoke it through entailment or implicature.</p>
<p>Entailments are truths that logically must also hold, given that an initial statement was true, as in "Bob was murdered" entails "Bob is dead." Similarly, implicatures are highly likely to be true, but not definitively so [8]. They convey meaning indirectly and are understood implicitly. The statement "Bob was buried" suggests "Bob is dead" via implicature, yet it is possible, though extremely unlikely, for someone to be buried alive. To see how the Ebiquity model is insensitive to important entailments, consider the target concept small and the human and Ebiquity rankings of comparison sentences that reference prototypically small objects, as shown in Table 4.</p>
<p>Participants clearly associated dimes and the concepts of finding or losing dimes with small, but the STS model did not. Participants rated sentences about dimes as more related to small than sentences containing the word small, but in reference to items that are not protypically small. The STS did not mimic this behavior, nor was it sensitive to the smallness of mice.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Rank (of 31)</th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Comparison sentence (small)</td>
<td style="text-align: center;">Human</td>
<td style="text-align: left;">Ebiquity</td>
</tr>
<tr>
<td style="text-align: left;">The girl dropped the shiny dime.</td>
<td style="text-align: center;">5</td>
<td style="text-align: left;">30</td>
</tr>
<tr>
<td style="text-align: left;">The dime was new.</td>
<td style="text-align: center;">7</td>
<td style="text-align: left;">23</td>
</tr>
<tr>
<td style="text-align: left;">The mouse ran into the forest.</td>
<td style="text-align: center;">11</td>
<td style="text-align: left;">22</td>
</tr>
</tbody>
</table>
<p>Table 4. A subset of human relatedness and Ebiquity similarity rankings with respect to the target concept small.</p>
<p>Results from other target concepts similarly demonstrated the lack of STS model sensitivity to entailment and implicature. For the target concept speak, participants considered sentences with concepts like negotiate and interview: acts which involve speaking. Participants found these sentences highly related to speak, but the STS model did not: The commander negotiated with the council (Human rank 7/31; Ebiquity rank 24/31) and The author interviewed the scientist after the flood (Human rank 6/31; Ebiquity rank 18/31). For the target concept door, participants recognize that leaving a building such as a theater implicitly requires passing through a door, while the Ebiquity system does not, as in The wealthy couple left the theater (Human rank 10/31; Ebiquity rank 24/31).</p>
<p>Finally, there were more complex implicatures that participants incorporated into their judgments that the Ebiquity system did not. As an example, the target concept tourist was evaluated with the comparison sentence The minister visited the prison (Human rank 23/31; Ebiquity rank 8/31). While visiting a location is the essence of tourism, prisons are perhaps the antithesis of a vacation destination, and so participants modulated their interpretation accordingly. A more complete and human-like conceptual knowledge representation and interpretation process that is better able to incorporate entailment, implicature, and antonymy should be expected to perform better in these contexts. Indeed, some automated approaches to text summarization and questionanswering have considered these factors [10]. Note that results from both the MDS and free-rank task were assembled into a collection of ancillary text files and have been made available for download. The files include lists of all concepts and sentences used in the tasks, along with word-sentence pairings for the 25 tested concepts. Mean ranks and standard deviations are reported for the two tasks individually, with sentences ordered by mean rank for each concept.</p>
<h2>CONCLUSION</h2>
<p>We have presented a data set for evaluating semantic models based on human behavioral rankings. 775 English wordsentence pairs were constructed to embed concepts in meaningful contexts and generate a range of word-sentence relationships. Word-sentence pairs were annotated for semantic relatedness by human raters, and ratings were found reliable across rating tasks.</p>
<p>To illustrate the potential utility of this data set, results of the rating task were compared to several semantic textual similarity systems. Higher-order text systems like Word2Vec and UMBC Ebiquity often closely predicted human ratings, but failed to match human judgments in some cases. The first major disagreement between human and STS ratings arose from subtle differences between semantic relatedness and semantic similarity, and more attention may need to be paid to this distinction when developing evaluations and applications for semantic models. The second major disagreement came from a failure of the semantic models to capture entailment, implicature, and context. Human relatedness judgments are shaped by understanding of context and the ability to infer entailed and implied information. Semantic models will need to be enriched with additional sources to more closely parallel human semantic judgments.</p>
<h2>ACKNOWLEDGMENTS</h2>
<p>This research is based upon work supported by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via IARPA Contract No. 2012 12050800010. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon.</p>
<p>The authors also gratefully acknowledge the generous contribution of Dr. Lushan Han, Samsung Research America, and Dr. Tim Finin, University of Maryland, Baltimore County, in making the code for the Ebiquity STS system available, Mary Luongo in helping to collect behavioral responses, and Dr. Christine Piatko in discussing relevant research topics.</p>
<h2>REFERENCES</h2>
<ol>
<li>Peter F. Brown, Peter V. Desouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Class-based n-gram models of natural language. Computational linguistics 18, 4 (1992), 467-479.</li>
<li>Marc Brysbaert, Amy Beth Warriner, and Victor Kuperman. 2014. Concreteness ratings for 40 thousand generally known English word lemmas. Behavior research methods 46, 3 (2014), 904-911.</li>
<li>Thomas A Carlson, Ryan A Simmons, Nikolaus Kriegeskorte, and L Robert Slevc. 2014. The emergence of semantic meaning in the ventral temporal pathway. Journal of cognitive neuroscience 26, 1 (2014), $120-131$.</li>
<li>Mark Davies. 2008. COCA. Corpus of Contemporary American English.</li>
<li>Scott C. Deerwester, Susan T. Dumais, Thomas K. Landauer, George W. Furnas, and Richard A. Harshman. 1990. Indexing by latent semantic analysis. JAsIs 41, 6 (1990), 391-407.</li>
<li>Roberto Furlan and Graham Turner. 2014. Maximum difference scaling. International Journal of Market Research 56, 3 (2014), 367-385.</li>
<li>Ruth Gairns and Stuart Redman. 1993. Working with words. Ernst Klett Sprachen.</li>
<li>Peter Grundy. 2013. Doing pragmatics. Routledge.</li>
<li>Lushan Han, Abhay Kashyap, Tim Finin, James Mayfield, and Jonathan Weese. 2013. UMBC EBIQUITY-CORE: Semantic textual similarity systems. Atlanta, Georgia, USA 44 (2013).</li>
<li>Elena Lloret and Manuel Palomar. 2012. Text summarisation in progress: a literature review. Artificial Intelligence Review 37, 1 (2012), 1-41.</li>
<li>Jordan J. Louviere and George G. Woodworth. 1991. Best-worst scaling: A model for the largest difference judgments. University of Alberta (1991).</li>
<li>Marco Marelli, Luisa Bentivogli, Marco Baroni, Raffaella Bernardi, Stefano Menini, and Roberto Zamparelli. 2014. Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment. SemEval-2014 (2014).</li>
<li>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems. 3111-3119.</li>
<li>George A. Miller. 1995. WordNet: a lexical database for English. Commun. ACM 38, 11 (1995), 39-41.</li>
<li>Allan Paivio, John C. Yuille, and Stephen A. Madigan. 1968. Concreteness, imagery, and meaningfulness: Values for 925 nouns. American Psychological Association.</li>
<li>Karalyn Patterson, Peter J. Nestor, and Timothy T. Rogers. 2007. Where do you know what you know? The representation of semantic knowledge in the human brain. Nature Reviews Neuroscience 8, 12 (Dec. 2007), 976-987. DOI :http://dx.doi.org/10.1038/nrn2277</li>
<li>Lance J. Rips, Edward J. Shoben, and Edward E. Smith. 1973. Semantic distance and the verification of semantic relations. Journal of verbal learning and verbal behavior 12, 1 (1973), 1-20.</li>
<li>Timothy T. Rogers and Christopher R. Cox. 2015. Revisiting a Golden Age Hypothesis in the Era of Cognitive Neuroscience. The Wiley Handbook on The Cognitive Neuroscience of Memory (2015), 60.</li>
<li>Mark Steyvers and Joshua B Tenenbaum. 2005. The Large-scale structure of semantic networks: Statistical analyses and a model of semantic growth. Cognitive science 29, 1 (2005), 41-78.</li>
<li>Lorraine K Tyler and Helen E Moss. 2001. Towards a distributed account of conceptual knowledge. Trends in cognitive sciences 5, 6 (2001), 244-252.</li>
</ol>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
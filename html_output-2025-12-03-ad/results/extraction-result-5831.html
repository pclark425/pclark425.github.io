<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5831 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5831</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5831</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-116.html">extraction-schema-116</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <p><strong>Paper ID:</strong> paper-a2ce9963f1f072d578b1a1f1b995fec75e8c2247</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/a2ce9963f1f072d578b1a1f1b995fec75e8c2247" target="_blank">PromptRobust: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts</a></p>
                <p><strong>Paper Venue:</strong> LAMPS@CCS</p>
                <p><strong>Paper TL;DR:</strong> This study uses a plethora of adversarial textual attacks targeting prompts across multiple levels: character, word, sentence, and semantic to demonstrate that contemporary LLMs are not robust to adversarial prompts.</p>
                <p><strong>Paper Abstract:</strong> The increasing reliance on Large Language Models (LLMs) across academia and industry necessitates a comprehensive understanding of their robustness to prompts. In response to this vital need, we introduce PromptRobust, a robustness benchmark designed to measure LLMs' resilience to adversarial prompts. This study uses a plethora of adversarial textual attacks targeting prompts across multiple levels: character, word, sentence, and semantic. The adversarial prompts, crafted to mimic plausible user errors like typos or synonyms, aim to evaluate how slight deviations can affect LLM outcomes while maintaining semantic integrity. These prompts are then employed in diverse tasks including sentiment analysis, natural language inference, reading comprehension, machine translation, and math problem-solving. Our study generates 4, 788 adversarial prompts, meticulously evaluated over 8 tasks and 13 datasets. Our findings demonstrate that contemporary LLMs are not robust to adversarial prompts. Furthermore, we present comprehensive analysis to understand the mystery behind prompt robustness and its transferability. We then offer insightful robustness analysis, beneficial to both researchers and everyday users. Code is available at https://github.com/microsoft/promptbench.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5831.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5831.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ZS-vs-FS prompts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot versus Few-shot prompt formats (task-oriented and role-oriented)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison of prompt presentation formats where zero-shot prompts give only instructions and few-shot prompts include task examples; evaluated in both task-oriented and role-oriented formulations across multiple tasks and LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>All evaluated LLMs (Flan-T5-large, Vicuna-13B, Llama2-13B-chat, UL2, ChatGPT, GPT-4, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple tasks (SST-2, CoLA, QQP, MRPC, MNLI, QNLI, RTE, WNLI, MMLU, SQuAD V2, IWSLT, UN Multi, Math)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>A broad set of classification, QA, translation and math reasoning tasks used to measure robustness to prompt perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Four prompt types studied: zero-shot task-oriented (ZS-task), zero-shot role-oriented (ZS-role), few-shot task-oriented (FS-task) and few-shot role-oriented (FS-role). Few-shot prompts add three example [x_i, y_i] pairs to the prompt; role-oriented prompts frame a persona/role; task-oriented prompts explicitly describe the task.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>ZS-task vs ZS-role vs FS-task vs FS-role (each compared against the others)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Average APDR (relative performance drop after adversarial prompt attacks): ZS-task = 0.33, ZS-role = 0.34, FS-task = 0.21, FS-role = 0.21 (APDR = average Performance Drop Rate across attacks/models/datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Few-shot (FS-task/FS-role) APDR 0.21 vs Zero-shot (ZS-task/ZS-role) APDR 0.33-0.34.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Approx. 0.12 absolute reduction in APDR (33% -> 21%) when using few-shot instead of zero-shot task-oriented prompts (i.e., fewer relative performance drops under adversarial prompt attacks).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Including in-context examples (few-shot) stabilizes model behavior, giving stronger, task-specific anchors that reduce sensitivity to small adversarial variations in the prompt; task vs role differences are small and dataset dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PromptRobust: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5831.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5831.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Word-level perturbation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Word-level prompt perturbations (TextFooler, BERT-Attack)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Replacements of words in prompts with synonyms or contextually similar words (word-level attacks) that are semantically close but designed to cause LLM failure.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>All evaluated LLMs (aggregate)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple tasks (same as above)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks include classification, NLI, translation, reading comprehension and math problems; adversarial prompts are applied to the instruction text.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Prompts modified at the word level by synonym/contextual replacements using TextFooler and BertAttack.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to character-level (TextBugger/DeepWordBug), sentence-level (CheckList/StressTest), and semantic-level (translate-back) prompt perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Average APDR: TextFooler = 0.31, BertAttack = 0.33 (i.e., ~31–33% average relative performance drop across tasks/models).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Word-level APDR (0.31–0.33) vs Character-level APDR (~0.17–0.21) vs Sentence-level APDR (~0.11–0.12) vs Semantic-level APDR (~0.22).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Word-level attacks produce ~10–22 percentage points higher APDR than sentence-level attacks and ~10–15 points higher than semantic/character-level in many cases (e.g., 0.33 vs 0.11–0.22).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>reduced</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Word-level substitutions alter key lexical cues while preserving surface semantics, and the models' attention and decision rules are sensitive to these lexical changes; attention visualizations show the model shifts focus toward substituted/perturbed tokens leading to misclassification.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PromptRobust: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5831.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5831.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Character-level perturbation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Character-level prompt perturbations (TextBugger, DeepWordBug)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Perturbations that introduce typos or character edits in prompt tokens (insertion, deletion, replacement, repetition) to mimic realistic user typos.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>All evaluated LLMs (aggregate)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same task suite; character-level perturbations applied to prompts to simulate typos and small character edits.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Prompts with character edits (typos), e.g., misspellings created by TextBugger or DeepWordBug.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to word-level, sentence-level, and semantic-level perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Average APDR: TextBugger = 0.21, DeepWordBug = 0.17 (i.e., ~17–21% average relative drop).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Character-level APDR lower than word-level (0.31–0.33) and similar to semantic-level (0.22), but greater than sentence-level (0.11–0.12).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Character-level attacks cause ~10–12 percentage points smaller APDR than word-level attacks on average (0.21 vs ~0.33).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>reduced</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Character edits create subtle orthographic noise that shifts attention away from key tokens; while some character errors are caught by grammar/spelling detectors, LLM internal tokenization and attention remain vulnerable, causing mispredictions.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PromptRobust: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5831.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5831.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sentence-level perturbation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sentence-level prompt perturbations (CheckList, StressTest)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Appending irrelevant or distracting sentences/sequences to prompts (random sequences or repeated trivial clauses) to observe whether LLMs are distracted by extraneous prompt material.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>All evaluated LLMs (aggregate)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same task suite; sentence-level attacks append distracting content to the end of prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Prompts with appended random sequences (CheckList) or repeated trivial clauses (StressTest, e.g., 'and true is true' repeated).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to character-level, word-level, semantic-level perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Average APDR: CheckList = 0.12, StressTest = 0.11 (i.e., ~11–12% average relative drop).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Sentence-level attacks are less effective than word-level (0.31–0.33) and character/semantic-level (~0.17–0.22).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Sentence-level APDR is ~20 percentage points lower than the strongest word-level APDR (0.12 vs 0.33).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>reduced (but weak)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Appending extraneous sentences often dilutes but does not always redirect the model's attention; models sometimes focus both on the extraneous material and the task, producing mixed vulnerability. The study also found paradoxical cases where sentence-level perturbations improved performance on certain datasets (StressTest on MRPC showing large effect and StressTest on SQuAD V2 showing only 2% drop), indicating dataset- and model-dependent interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>StressTest sometimes increases model performance on some datasets (described in Appendix D.3 and observed in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PromptRobust: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5831.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5831.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Semantic-level perturbation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Semantic-level prompt perturbations (translate-back variations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Generate prompts by creating variants in other languages and translating them back to English to introduce subtle phrasing and linguistic nuances.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>All evaluated LLMs (aggregate)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same task suite; semantic perturbations simulate multilingual rephrasing and translation artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Prompts are rewritten in other languages (Chinese, French, Arabic, Spanish, Japanese, Korean) then translated back to English to induce semantic-level stylistic/phrasing changes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to word-, character-, and sentence-level perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Average APDR: Semantic-level = 0.22 (i.e., ~22% average relative drop across datasets/models).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Semantic-level APDR similar to character-level (0.17–0.21) but lower than word-level (0.31–0.33) and higher than sentence-level (0.11–0.12).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Semantic-level APDR ~10 percentage points lower than word-level attacks but comparable to character-level attacks.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>reduced</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Subtle rephrasings and translation-induced wording shifts alter the surface presentation of the task in ways that can misalign model interpretation; suggests LLMs are sensitive to presentation phrasing and translation artifacts even when semantics are roughly preserved.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PromptRobust: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5831.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5831.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Model-specific robustness</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model-wise differences in robustness to prompt presentation/perturbations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Observed differences in average vulnerability (APDR) across models: some models (UL2, GPT-4, T5-large) show lower APDR (more robust) while others (Vicuna, Llama2) are more vulnerable.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Individual models (Flan-T5-large, Vicuna-13B, Llama2-13B-chat, UL2, ChatGPT, GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same task suite used to compute per-model average APDR.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Adversarial prompt attacks applied to the same prompt types across models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Comparison of APDR aggregated per-model across all attacks and prompt types.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Average APDR by model (from Table 3 aggregated rows): Vicuna ≈ 0.69, Llama2 ≈ 0.51, T5-large ≈ 0.13, UL2 ≈ 0.08, ChatGPT ≈ 0.18, GPT-4 ≈ 0.08.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Vicuna and Llama2 show much higher APDR (more vulnerable) compared to UL2 and GPT-4 which show low APDR (more robust).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Large spread: worst (Vicuna 0.69) vs best (UL2/GPT-4 ~0.08) is ≈0.61 absolute APDR difference.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>varied (some models strongly reduced performance under adversarial prompt formats while others were relatively robust)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Differences attributed to fine-tuning regimes, architectures, pretraining data and potential memorization; models fine-tuned on instruction datasets or with more supervised fine-tuning (e.g., UL2, T5 variants) were generally more robust; proprietary models (GPT-4) also show stronger robustness, though transferability caveats apply.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PromptRobust: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5831.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5831.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Attention-shift mechanism</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Attention reallocation as mechanism for format sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Analysis showing adversarial prompt perturbations cause LLMs to reallocate attention away from task-relevant words toward perturbed tokens, producing incorrect outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Examples shown across models in attention visualizations (e.g., ChatGPT, T5 variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Sentiment classification and other tasks (illustrative examples in Table 5)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Case analyses where a clean prompt leads to correct classification but adversarial prompt causes misclassification.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Clean prompt vs adversarial prompt (word/character/sentence perturbation) with same sample input.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Clean prompt (baseline) vs adversarially perturbed prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: clean prompts produce correct predictions and concentrated attention on key tokens; adversarial prompts often lead to incorrect predictions and attention concentrated on perturbations (no single scalar metric reported here).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>reduced (adversarial prompts reduce correct attention allocation and thus accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Two attention visualization methods (Attention by Gradient and Attention by Deletion) show that adversarial prompts reroute model attention to perturbed tokens or appended noise, thereby reducing focus on task-critical tokens and causing errors.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PromptRobust: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5831.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5831.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transferability</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transferability of adversarial prompts between models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of how adversarial prompts crafted for a source model affect target models when directly transferred (black-box transfer).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Pairs of models (source -> target), e.g., ChatGPT -> T5, ChatGPT -> UL2, UL2 -> Vicuna, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Aggregate across datasets</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Selected the most vulnerable prompts from a source model and applied them to target models; APDR_transfer computed.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Adversarial prompts generated for source model (same prompt types/attacks) then applied unchanged to target model.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Original APDR on source model vs transferred APDR on target model.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>General observation: transfer APDRs are small compared to original APDRs; transferability is inconsistent (some prompts transfer successfully, others produce no effect or even improve target model performance).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Transfer APDR much smaller than original APDR in source model (see Table 6); some reported negative APDRs (improved accuracy) in transfers, e.g., UL2 -> Vicuna BertAttack example shows a -0.70 value in Table 6 indicating unexpected improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Transfer APDR values are typically low and have large standard deviation; effect sizes are dataset- and model-dependent and generally much smaller than intra-model APDRs.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>mixed (mostly reduced but marginal; sometimes improved)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Models have distinct failure modes; prompts that exploit one model's sensitivities do not reliably exploit another's. Transferability exists but is weaker and inconsistent, suggesting attacker-generated adversarial prompts may need model-specific tuning for strong effect.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PromptRobust: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5831.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e5831.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Model size & fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effects of model size and instruction fine-tuning on prompt robustness</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Analysis across Llama2 family (7B, 13B, 70B) and vanilla vs chat-fine-tuned variants showing size and fine-tuning influence vulnerability to prompt perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama2 series (Llama2-7B-chat, Llama2-13B-chat, Llama2-70B-chat) and corresponding vanilla models</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B, 13B, 70B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SST-2, CoLA (examples presented), aggregate commentary</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Sentiment and grammar acceptability tasks used to compare sizes and fine-tuning effect.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Same adversarial prompt attacks applied to different model sizes and to fine-tuned chat variants vs vanilla models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Model size (7B vs 13B vs 70B) and fine-tuned (chat) vs vanilla versions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: larger models are typically more robust in non-adversarial settings and often more robust under adversarial prompts, but exceptions occur where smaller models outperform larger ones on adversarial prompts; fine-tuned (chat) models consistently show improved robustness versus vanilla.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>varied (generally improved robustness with size and with instruction/CHAT fine-tuning, but exceptions exist)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Fine-tuning on human-instruction datasets (chat variants) tends to reduce susceptibility to adversarial prompt perturbations by better aligning model responses to task instructions; larger models have more capacity to be robust but can still have distinct vulnerabilities dependent on training/fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Observed exceptions where smaller models outperformed larger ones under adversarial prompts (no single monotonic size-robustness relationship).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PromptRobust: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Adversarial glue: A multi-task benchmark for robustness evaluation of language models <em>(Rating: 2)</em></li>
                <li>Is bert really robust? natural language attack on text classification and entailment <em>(Rating: 2)</em></li>
                <li>Generating adversarial text sequences to evade deep learning classifiers <em>(Rating: 2)</em></li>
                <li>BERT-ATTACK: Adversarial attack against BERT using BERT <em>(Rating: 2)</em></li>
                <li>CheckList: Beyond Accuracy: Behavioral Testing of NLP Models with CheckList <em>(Rating: 1)</em></li>
                <li>Stress Test Evaluation for Natural Language Inference <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5831",
    "paper_id": "paper-a2ce9963f1f072d578b1a1f1b995fec75e8c2247",
    "extraction_schema_id": "extraction-schema-116",
    "extracted_data": [
        {
            "name_short": "ZS-vs-FS prompts",
            "name_full": "Zero-shot versus Few-shot prompt formats (task-oriented and role-oriented)",
            "brief_description": "Comparison of prompt presentation formats where zero-shot prompts give only instructions and few-shot prompts include task examples; evaluated in both task-oriented and role-oriented formulations across multiple tasks and LLMs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "All evaluated LLMs (Flan-T5-large, Vicuna-13B, Llama2-13B-chat, UL2, ChatGPT, GPT-4, etc.)",
            "model_size": null,
            "task_name": "Multiple tasks (SST-2, CoLA, QQP, MRPC, MNLI, QNLI, RTE, WNLI, MMLU, SQuAD V2, IWSLT, UN Multi, Math)",
            "task_description": "A broad set of classification, QA, translation and math reasoning tasks used to measure robustness to prompt perturbations.",
            "problem_format": "Four prompt types studied: zero-shot task-oriented (ZS-task), zero-shot role-oriented (ZS-role), few-shot task-oriented (FS-task) and few-shot role-oriented (FS-role). Few-shot prompts add three example [x_i, y_i] pairs to the prompt; role-oriented prompts frame a persona/role; task-oriented prompts explicitly describe the task.",
            "comparison_format": "ZS-task vs ZS-role vs FS-task vs FS-role (each compared against the others)",
            "performance": "Average APDR (relative performance drop after adversarial prompt attacks): ZS-task = 0.33, ZS-role = 0.34, FS-task = 0.21, FS-role = 0.21 (APDR = average Performance Drop Rate across attacks/models/datasets).",
            "performance_comparison": "Few-shot (FS-task/FS-role) APDR 0.21 vs Zero-shot (ZS-task/ZS-role) APDR 0.33-0.34.",
            "format_effect_size": "Approx. 0.12 absolute reduction in APDR (33% -&gt; 21%) when using few-shot instead of zero-shot task-oriented prompts (i.e., fewer relative performance drops under adversarial prompt attacks).",
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "Including in-context examples (few-shot) stabilizes model behavior, giving stronger, task-specific anchors that reduce sensitivity to small adversarial variations in the prompt; task vs role differences are small and dataset dependent.",
            "counterexample_or_null_result": null,
            "uuid": "e5831.0",
            "source_info": {
                "paper_title": "PromptRobust: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Word-level perturbation",
            "name_full": "Word-level prompt perturbations (TextFooler, BERT-Attack)",
            "brief_description": "Replacements of words in prompts with synonyms or contextually similar words (word-level attacks) that are semantically close but designed to cause LLM failure.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "All evaluated LLMs (aggregate)",
            "model_size": null,
            "task_name": "Multiple tasks (same as above)",
            "task_description": "Tasks include classification, NLI, translation, reading comprehension and math problems; adversarial prompts are applied to the instruction text.",
            "problem_format": "Prompts modified at the word level by synonym/contextual replacements using TextFooler and BertAttack.",
            "comparison_format": "Compared to character-level (TextBugger/DeepWordBug), sentence-level (CheckList/StressTest), and semantic-level (translate-back) prompt perturbations.",
            "performance": "Average APDR: TextFooler = 0.31, BertAttack = 0.33 (i.e., ~31–33% average relative performance drop across tasks/models).",
            "performance_comparison": "Word-level APDR (0.31–0.33) vs Character-level APDR (~0.17–0.21) vs Sentence-level APDR (~0.11–0.12) vs Semantic-level APDR (~0.22).",
            "format_effect_size": "Word-level attacks produce ~10–22 percentage points higher APDR than sentence-level attacks and ~10–15 points higher than semantic/character-level in many cases (e.g., 0.33 vs 0.11–0.22).",
            "format_effect_direction": "reduced",
            "explanation_or_hypothesis": "Word-level substitutions alter key lexical cues while preserving surface semantics, and the models' attention and decision rules are sensitive to these lexical changes; attention visualizations show the model shifts focus toward substituted/perturbed tokens leading to misclassification.",
            "counterexample_or_null_result": null,
            "uuid": "e5831.1",
            "source_info": {
                "paper_title": "PromptRobust: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Character-level perturbation",
            "name_full": "Character-level prompt perturbations (TextBugger, DeepWordBug)",
            "brief_description": "Perturbations that introduce typos or character edits in prompt tokens (insertion, deletion, replacement, repetition) to mimic realistic user typos.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "All evaluated LLMs (aggregate)",
            "model_size": null,
            "task_name": "Multiple tasks",
            "task_description": "Same task suite; character-level perturbations applied to prompts to simulate typos and small character edits.",
            "problem_format": "Prompts with character edits (typos), e.g., misspellings created by TextBugger or DeepWordBug.",
            "comparison_format": "Compared to word-level, sentence-level, and semantic-level perturbations.",
            "performance": "Average APDR: TextBugger = 0.21, DeepWordBug = 0.17 (i.e., ~17–21% average relative drop).",
            "performance_comparison": "Character-level APDR lower than word-level (0.31–0.33) and similar to semantic-level (0.22), but greater than sentence-level (0.11–0.12).",
            "format_effect_size": "Character-level attacks cause ~10–12 percentage points smaller APDR than word-level attacks on average (0.21 vs ~0.33).",
            "format_effect_direction": "reduced",
            "explanation_or_hypothesis": "Character edits create subtle orthographic noise that shifts attention away from key tokens; while some character errors are caught by grammar/spelling detectors, LLM internal tokenization and attention remain vulnerable, causing mispredictions.",
            "counterexample_or_null_result": null,
            "uuid": "e5831.2",
            "source_info": {
                "paper_title": "PromptRobust: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Sentence-level perturbation",
            "name_full": "Sentence-level prompt perturbations (CheckList, StressTest)",
            "brief_description": "Appending irrelevant or distracting sentences/sequences to prompts (random sequences or repeated trivial clauses) to observe whether LLMs are distracted by extraneous prompt material.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "All evaluated LLMs (aggregate)",
            "model_size": null,
            "task_name": "Multiple tasks",
            "task_description": "Same task suite; sentence-level attacks append distracting content to the end of prompts.",
            "problem_format": "Prompts with appended random sequences (CheckList) or repeated trivial clauses (StressTest, e.g., 'and true is true' repeated).",
            "comparison_format": "Compared to character-level, word-level, semantic-level perturbations.",
            "performance": "Average APDR: CheckList = 0.12, StressTest = 0.11 (i.e., ~11–12% average relative drop).",
            "performance_comparison": "Sentence-level attacks are less effective than word-level (0.31–0.33) and character/semantic-level (~0.17–0.22).",
            "format_effect_size": "Sentence-level APDR is ~20 percentage points lower than the strongest word-level APDR (0.12 vs 0.33).",
            "format_effect_direction": "reduced (but weak)",
            "explanation_or_hypothesis": "Appending extraneous sentences often dilutes but does not always redirect the model's attention; models sometimes focus both on the extraneous material and the task, producing mixed vulnerability. The study also found paradoxical cases where sentence-level perturbations improved performance on certain datasets (StressTest on MRPC showing large effect and StressTest on SQuAD V2 showing only 2% drop), indicating dataset- and model-dependent interactions.",
            "counterexample_or_null_result": "StressTest sometimes increases model performance on some datasets (described in Appendix D.3 and observed in experiments).",
            "uuid": "e5831.3",
            "source_info": {
                "paper_title": "PromptRobust: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Semantic-level perturbation",
            "name_full": "Semantic-level prompt perturbations (translate-back variations)",
            "brief_description": "Generate prompts by creating variants in other languages and translating them back to English to introduce subtle phrasing and linguistic nuances.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "All evaluated LLMs (aggregate)",
            "model_size": null,
            "task_name": "Multiple tasks",
            "task_description": "Same task suite; semantic perturbations simulate multilingual rephrasing and translation artifacts.",
            "problem_format": "Prompts are rewritten in other languages (Chinese, French, Arabic, Spanish, Japanese, Korean) then translated back to English to induce semantic-level stylistic/phrasing changes.",
            "comparison_format": "Compared to word-, character-, and sentence-level perturbations.",
            "performance": "Average APDR: Semantic-level = 0.22 (i.e., ~22% average relative drop across datasets/models).",
            "performance_comparison": "Semantic-level APDR similar to character-level (0.17–0.21) but lower than word-level (0.31–0.33) and higher than sentence-level (0.11–0.12).",
            "format_effect_size": "Semantic-level APDR ~10 percentage points lower than word-level attacks but comparable to character-level attacks.",
            "format_effect_direction": "reduced",
            "explanation_or_hypothesis": "Subtle rephrasings and translation-induced wording shifts alter the surface presentation of the task in ways that can misalign model interpretation; suggests LLMs are sensitive to presentation phrasing and translation artifacts even when semantics are roughly preserved.",
            "counterexample_or_null_result": null,
            "uuid": "e5831.4",
            "source_info": {
                "paper_title": "PromptRobust: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Model-specific robustness",
            "name_full": "Model-wise differences in robustness to prompt presentation/perturbations",
            "brief_description": "Observed differences in average vulnerability (APDR) across models: some models (UL2, GPT-4, T5-large) show lower APDR (more robust) while others (Vicuna, Llama2) are more vulnerable.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Individual models (Flan-T5-large, Vicuna-13B, Llama2-13B-chat, UL2, ChatGPT, GPT-4)",
            "model_size": null,
            "task_name": "Multiple tasks",
            "task_description": "Same task suite used to compute per-model average APDR.",
            "problem_format": "Adversarial prompt attacks applied to the same prompt types across models.",
            "comparison_format": "Comparison of APDR aggregated per-model across all attacks and prompt types.",
            "performance": "Average APDR by model (from Table 3 aggregated rows): Vicuna ≈ 0.69, Llama2 ≈ 0.51, T5-large ≈ 0.13, UL2 ≈ 0.08, ChatGPT ≈ 0.18, GPT-4 ≈ 0.08.",
            "performance_comparison": "Vicuna and Llama2 show much higher APDR (more vulnerable) compared to UL2 and GPT-4 which show low APDR (more robust).",
            "format_effect_size": "Large spread: worst (Vicuna 0.69) vs best (UL2/GPT-4 ~0.08) is ≈0.61 absolute APDR difference.",
            "format_effect_direction": "varied (some models strongly reduced performance under adversarial prompt formats while others were relatively robust)",
            "explanation_or_hypothesis": "Differences attributed to fine-tuning regimes, architectures, pretraining data and potential memorization; models fine-tuned on instruction datasets or with more supervised fine-tuning (e.g., UL2, T5 variants) were generally more robust; proprietary models (GPT-4) also show stronger robustness, though transferability caveats apply.",
            "counterexample_or_null_result": null,
            "uuid": "e5831.5",
            "source_info": {
                "paper_title": "PromptRobust: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Attention-shift mechanism",
            "name_full": "Attention reallocation as mechanism for format sensitivity",
            "brief_description": "Analysis showing adversarial prompt perturbations cause LLMs to reallocate attention away from task-relevant words toward perturbed tokens, producing incorrect outputs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Examples shown across models in attention visualizations (e.g., ChatGPT, T5 variants)",
            "model_size": null,
            "task_name": "Sentiment classification and other tasks (illustrative examples in Table 5)",
            "task_description": "Case analyses where a clean prompt leads to correct classification but adversarial prompt causes misclassification.",
            "problem_format": "Clean prompt vs adversarial prompt (word/character/sentence perturbation) with same sample input.",
            "comparison_format": "Clean prompt (baseline) vs adversarially perturbed prompt.",
            "performance": "Qualitative: clean prompts produce correct predictions and concentrated attention on key tokens; adversarial prompts often lead to incorrect predictions and attention concentrated on perturbations (no single scalar metric reported here).",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "reduced (adversarial prompts reduce correct attention allocation and thus accuracy)",
            "explanation_or_hypothesis": "Two attention visualization methods (Attention by Gradient and Attention by Deletion) show that adversarial prompts reroute model attention to perturbed tokens or appended noise, thereby reducing focus on task-critical tokens and causing errors.",
            "counterexample_or_null_result": null,
            "uuid": "e5831.6",
            "source_info": {
                "paper_title": "PromptRobust: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Transferability",
            "name_full": "Transferability of adversarial prompts between models",
            "brief_description": "Evaluation of how adversarial prompts crafted for a source model affect target models when directly transferred (black-box transfer).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Pairs of models (source -&gt; target), e.g., ChatGPT -&gt; T5, ChatGPT -&gt; UL2, UL2 -&gt; Vicuna, etc.",
            "model_size": null,
            "task_name": "Aggregate across datasets",
            "task_description": "Selected the most vulnerable prompts from a source model and applied them to target models; APDR_transfer computed.",
            "problem_format": "Adversarial prompts generated for source model (same prompt types/attacks) then applied unchanged to target model.",
            "comparison_format": "Original APDR on source model vs transferred APDR on target model.",
            "performance": "General observation: transfer APDRs are small compared to original APDRs; transferability is inconsistent (some prompts transfer successfully, others produce no effect or even improve target model performance).",
            "performance_comparison": "Transfer APDR much smaller than original APDR in source model (see Table 6); some reported negative APDRs (improved accuracy) in transfers, e.g., UL2 -&gt; Vicuna BertAttack example shows a -0.70 value in Table 6 indicating unexpected improvement.",
            "format_effect_size": "Transfer APDR values are typically low and have large standard deviation; effect sizes are dataset- and model-dependent and generally much smaller than intra-model APDRs.",
            "format_effect_direction": "mixed (mostly reduced but marginal; sometimes improved)",
            "explanation_or_hypothesis": "Models have distinct failure modes; prompts that exploit one model's sensitivities do not reliably exploit another's. Transferability exists but is weaker and inconsistent, suggesting attacker-generated adversarial prompts may need model-specific tuning for strong effect.",
            "counterexample_or_null_result": null,
            "uuid": "e5831.7",
            "source_info": {
                "paper_title": "PromptRobust: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Model size & fine-tuning",
            "name_full": "Effects of model size and instruction fine-tuning on prompt robustness",
            "brief_description": "Analysis across Llama2 family (7B, 13B, 70B) and vanilla vs chat-fine-tuned variants showing size and fine-tuning influence vulnerability to prompt perturbations.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama2 series (Llama2-7B-chat, Llama2-13B-chat, Llama2-70B-chat) and corresponding vanilla models",
            "model_size": "7B, 13B, 70B",
            "task_name": "SST-2, CoLA (examples presented), aggregate commentary",
            "task_description": "Sentiment and grammar acceptability tasks used to compare sizes and fine-tuning effect.",
            "problem_format": "Same adversarial prompt attacks applied to different model sizes and to fine-tuned chat variants vs vanilla models.",
            "comparison_format": "Model size (7B vs 13B vs 70B) and fine-tuned (chat) vs vanilla versions.",
            "performance": "Qualitative: larger models are typically more robust in non-adversarial settings and often more robust under adversarial prompts, but exceptions occur where smaller models outperform larger ones on adversarial prompts; fine-tuned (chat) models consistently show improved robustness versus vanilla.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "varied (generally improved robustness with size and with instruction/CHAT fine-tuning, but exceptions exist)",
            "explanation_or_hypothesis": "Fine-tuning on human-instruction datasets (chat variants) tends to reduce susceptibility to adversarial prompt perturbations by better aligning model responses to task instructions; larger models have more capacity to be robust but can still have distinct vulnerabilities dependent on training/fine-tuning.",
            "counterexample_or_null_result": "Observed exceptions where smaller models outperformed larger ones under adversarial prompts (no single monotonic size-robustness relationship).",
            "uuid": "e5831.8",
            "source_info": {
                "paper_title": "PromptRobust: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts",
                "publication_date_yy_mm": "2023-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Adversarial glue: A multi-task benchmark for robustness evaluation of language models",
            "rating": 2
        },
        {
            "paper_title": "Is bert really robust? natural language attack on text classification and entailment",
            "rating": 2
        },
        {
            "paper_title": "Generating adversarial text sequences to evade deep learning classifiers",
            "rating": 2
        },
        {
            "paper_title": "BERT-ATTACK: Adversarial attack against BERT using BERT",
            "rating": 2
        },
        {
            "paper_title": "CheckList: Beyond Accuracy: Behavioral Testing of NLP Models with CheckList",
            "rating": 1
        },
        {
            "paper_title": "Stress Test Evaluation for Natural Language Inference",
            "rating": 1
        }
    ],
    "cost": 0.019744499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>PromptRobust: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts</h1>
<p>Kaijie Zhu ${ }^{1,2}$, Jindong Wang ${ }^{1,}$ Jiaheng Zhou ${ }^{2}$, Zeek Wang ${ }^{1}$, Hao Chen ${ }^{3}$, Yidong Wang ${ }^{4}$, Linyi Yang ${ }^{5}$, Wei Ye ${ }^{4}$, Neil Zhenqiang Gong ${ }^{6}$, Yue Zhang ${ }^{5}$, Xing Xie ${ }^{1}$<br>${ }^{1}$ Microsoft Research ${ }^{2}$ Institute of Automation, CAS ${ }^{3}$ Carnegie Mellon University<br>${ }^{4}$ Peking University ${ }^{5}$ Westlake University ${ }^{6}$ Duke University</p>
<h4>Abstract</h4>
<p>The increasing reliance on Large Language Models (LLMs) necessitates a comprehensive understanding of their robustness to prompts. In this paper, we introduce PromptRobust, a robustness benchmark designed to measure LLMs' resilience to adversarial prompts. This study uses a plethora of adversarial textual attacks on prompts across multiple levels: character, word, sentence, and semantic. The adversarial prompts, crafted to mimic plausible user errors like typos or synonyms, aim to evaluate how slight deviations can affect LLM outcomes while maintaining semantic integrity. These prompts are then used in various tasks, including sentiment analysis, natural language inference, reading comprehension, machine translation, and math. We generate 4,788 adversarial prompts and evaluated over 8 tasks and 13 datasets. Our findings demonstrate that LLMs are not robust to adversarial prompts. Furthermore, we present a comprehensive analysis to understand the mystery behind prompt robustness and its transferability. We then offer insightful analysis and pragmatic recommendations for prompt composition, beneficial to both researchers and everyday users.</p>
<h2>1 Introduction</h2>
<p>Large language models (LLMs) have gained increasing popularity due to their unprecedented performance in various tasks such as sentiment analysis (Wang et al., 2019), question answering (Wang et al., 2019), logical reasoning (Liu et al., 2023a), etc. An input to an LLM is the concatenation of a prompt and (optionally) a sample, where the prompt aims to instruct the LLM what task to perform and the sample is the data for the task.
Given the popular adoption of LLMs, particularly in the safety-critical and decisionmaking domains, it becomes essential to examine the robustness of LLMs to perturbations in an input. Indeed, existing work (Wang et al., 2021; Nie et al., 2020; Wang et al., 2023b; Zhuo et al., 2023; Yang et al., 2023) has at-
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: LLMs are not robust to prompts: typos and synonyms lead to errors in math and sentiment analysis problems. The red characters and words are perturbations.
tempted to assess the robustness of LLMs from different perspectives. For instance, AdvGLUE (Wang et al., 2021) and ANLI (Nie et al., 2020) are two public datasets to evaluate the robustness of language models to adversarial samples, which are carefully perturbed samples to make a language model produce incorrect responses. In the era of large language models, Wang et al. (2023b) evaluated ChatGPT and other LLMs with respect to their robustness to</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>adversarial samples and out-of-distribution (OOD) samples. Zhuo et al. (2023) evaluated the robustness of LLMs for a particular task called semantic parsing.
These studies demonstrated that the current LLMs are not robust to adversarial and OOD samples for some popular natural language processing tasks. A single prompt is often used to instruct an LLM to perform a task for multiple samples. For example, in a math problem (shown in Figure 1), a prompt can be used for multiple samples (i.e., math problems). Therefore, a perturbed prompt may cause an LLM to output incorrect responses for multiple clean samples. As a result, a perturbed prompt arguably has a larger impact on LLMs than an adversarial sample, as the latter only influences the response of an LLM for a single sample. However, despite its central importance, the robustness of LLMs to prompt perturbations is largely unexplored.
In this paper, we aim to bridge the gap by introducing PromptRobust, a comprehensive benchmark designed to evaluate the robustness of LLMs to perturbations in prompts, understanding the factors that contribute to their robustness (or lack thereof), and identifying the key attributes of robust prompts. We consider a variety of prompt perturbations including 1) minor typos, synonyms, and different ways of expressing sentences with the same semantic meaning, which may commonly occur to normal users or developers in their daily use of LLMs in non-adversarial settings, as well as 2) perturbations strategically crafted by attackers in adversarial settings. With a slight abuse of terminology, we call such a perturbed prompt in both scenarios adversarial prompt. Figure 1 shows examples of adversarial prompts with typos and synonyms that lead to incorrect responses.
PromptRobust consists of prompts, attacks, models, tasks, datasets, and analysis. Specifically, we evaluate 4 types of prompts: zero-shot (ZS), few-shot (FS), role-oriented, and task-oriented prompts. We create 4 types of attacks (called prompt attacks) to craft adversarial prompts: character-level, word-level, sentence-level, and semantic-level attacks by extending 7 adversarial attacks (Li et al., 2019; Gao et al., 2018; Li et al., 2020; Jin et al., 2019; Naik et al., 2018; Ribeiro et al., 2020) that were originally designed to generate adversarial samples. We note that, although we call them attacks, their generated adversarial prompts also serve as testbeds for mimicking potential diverse prompts with naturally occurred perturbations from real LLM users. PromptRobust spans across 9 prevalent LLMs, ranging from smaller models such as Flan-T5-large (Chung et al., 2022) to larger ones like ChatGPT (OpenAI, 2023a) and GPT-4 (OpenAI, 2023b). Moreover, we select 8 tasks for evaluation, namely, sentiment analysis (SST-2 (Socher et al., 2013)), grammar correctness (CoLA (Warstadt et al., 2018)), duplicate sentence detection (QQP (Wang et al., 2017) and MRPC (Dolan \&amp; Brockett, 2005)), natural language inference (MNLI (Williams et al., 2018), QNLI (Wang et al., 2019), RTE (Wang et al., 2019), and WNLI (Levesque et al., 2012)), multi-task knowledge (MMLU (Hendrycks et al., 2021)), reading comprehension (SQuAD V2 (Rajpurkar et al., 2018)), translation (UN Multi (Eisele \&amp; Chen, 2010) and IWSLT 2017 (Cettolo et al., 2017)), and math problemsolving (Mathematics (Saxton et al., 2019)). In total, we created 4,788 adversarial prompts, representing diverse, practical, and challenging scenarios.
We carry out extensive experiments and analysis using PromptRobust. The results highlight a prevailing lack of robustness to adversarial prompts among current LLMs, with word-level attacks proving the most effective ( $39 \%$ average performance drop in all tasks). We delve into the reasons behind this vulnerability by exploring LLMs' attention weights of each word in inputs for erroneous responses associated with clean and adversarial inputs, where an adversarial input is the concatenation of an adversarial prompt and a clean sample. Our findings reveal that adversarial prompts cause LLMs to shift their focus towards the perturbed elements thus producing wrong responses. We also examine the transferability of adversarial prompts between models, and suggest a successful transferability of adversarial prompts from one LLM to another. Furthermore, we analyze word frequency patterns to guide future research in improving robustness and to aid end-users in crafting more robust prompts. We conclude by discussing potential strategies for improving robustness.
To summarize, our contributions are as follows:</p>
<ol>
<li>
<p>We introduce PromptRobust, the first systematic benchmark for evaluating, understanding, and analyzing the robustness of LLMs to adversarial prompts.</p>
</li>
<li>
<p>We conduct comprehensive evaluations on the robustness of LLMs to adversarial prompts and perform extensive analysis, including visual explanations for observed vulnerabilities, transferability analysis of adversarial prompts, and word frequency analysis to offer practical guidance to downstream users and prompt engineers to craft more robust prompts.</p>
</li>
</ol>
<h1>2 PromptRobust</h1>
<h3>2.1 Prompts and models</h3>
<p>We investigate four different types of prompt. Task-oriented prompts explicitly describe the task the model is required to perform, which encourages the model to generate taskspecific outputs based solely on its pre-training knowledge. While role-oriented prompts typically frame the model as an entity with a specific role, such as an expert, advisor, or translator. By incorporating role information, these prompts aim to implicitly convey the expected output format and behavior. Each of the two categories of prompts can be designed for both zero-shot (ZS) and few-shot (FS) learning scenarios. In the zero-shot scenario, an input is defined as $[P, x]$, where $P$ denotes a prompt, $x$ is a sample, and $[$,$] denotes the concatenation operation. For the few-shot scenario, some examples are added to the input, resulting in the format $[P, E, x]$, where $E$ represents the examples. For instance, $E=\left{\left[x_{1}, y_{1}\right],\left[x_{2}, y_{2}\right],\left[x_{3}, y_{3}\right]\right}$ represents three examples in a three-shot learning scenario. In our experiments, we randomly select three examples in the training set of a task and append them to a prompt. Appendix A. 1 shows examples of different types of prompts.
Our evaluation includes a diverse set of LLMs to comprehensively assess their performance across various tasks and domains. The models we consider are as follows: Flan-T5-large (Chung et al., 2022) (0.8B), Dolly-6B (Databricks, 2023), Vicuna-13B (Chiang et al., 2023), Llama2-13b-chat (Touvron et al., 2023b), Cerebras-GPT-13B (Dey et al., 2023), GPT-NEOX20B (Black et al., 2022), Flan-UL2 (20B) (Brain, 2023), ChatGPT (OpenAI, 2023a), and GPT4 (OpenAI, 2023b). ${ }^{1}$ By incorporating LLMs with different architectures and sizes, we aim to provide insights into their strengths and weaknesses, ultimately facilitating model selection for a specific application or use case. Details of these LLMs are in Appendix B.1.</p>
<h3>2.2 Attacks</h3>
<p>Given a single sample $x$ and its label $y$, a textual adversarial attack aims to find a perturbation $\delta$ such that an LLM $f_{\theta}$ produces an incorrect response. Formally, $\delta$ is found by solving the following optimization problem: $\max <em _theta="\theta">{\delta \in \mathcal{C}} \mathcal{L}\left[f</em>$ represents a loss function.
Prompt attack. In this paper, our focus is to attack the prompts rather than samples. This is due to the popularity of LLMs in different applications, which generate responses using in-context learning on prompts (i.e., instructions) and samples. Prompts are either input by users or generated by the system or developers. Moreover, the ultimate purpose of performing such "attack" is actually not to genuinely attack the models, but to simulate possible perturbations that may naturally occur in real situations. Table 8 shows multiple prompts generated by adversarial approaches that are used to mimic possible user prompts, which are popular errors or expressions made by users. Since users can make different mistakes when entering prompts, such as typos, different word usage, different sentence styles, etc., the study on prompt robustness is necessary to understand LLMs.
We denote an input to LLMs as $[P, x]$, where $P$ is a prompt, $x$ is a sample, and $[$,$]$ denotes concatenation. Note that in the few-shot learning scenario, a few examples are appended to the prompt; and the sample $x$ is optional in certain application scenarios. Our prompt attack can also be extended to such scenarios, but we use the notation $[P, x]$ for simplicity. Given a dataset $\mathcal{D}=\left{\left(x_{i}, y_{i}\right)\right}_{i \in[N]}$ with $N$ samples and their ground-truth labels, a prompt}(x+\delta) ; y\right]$, where $x+\delta$ is the adversarial sample, $f_{\theta}(x+\delta)$ is the response of the LLM when taking the adversarial sample alone as input, $\mathcal{C}$ indicates the constraints for the perturbation $\delta$, and $\mathcal{L</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>attack aims to perturb $P$ such that an LLM $f_{\theta}$ produces incorrect responses for all samples in the dataset $\mathcal{D}$.
Definition 2.1 (Prompt Attack). Given an LLM $f_{\theta}$, a dataset $\mathcal{D}=\left{\left(x_{i}, y_{i}\right)\right}_{i \in[N]}$, and a clean prompt $P$, the objective of a prompt attack can be formulated as follows:</p>
<p>$$
\max <em _in="\in" _mathcal_D="\mathcal{D" _x_="(x," y_="y)">{\delta \in \mathcal{C}} \sum</em>([P+\delta, x]), y\right]
$$}} \mathcal{L}\left[f_{\theta</p>
<p>where $\delta$ is the textual perturbation added to the clean prompt $P$ and $\mathcal{C}$ is the allowable perturbation set, i.e., perturbation constraint. We note that this attack is analogous to universal adversarial perturbation (UAP) (Moosavi-Dezfooli et al., 2017; Brown et al., 2017) and universal adversarial trigger (UAT) (Wallace et al., 2019), extending these concepts to the realm of prompts. Appendix A. 4 shows more comparisons.</p>
<p>Different attacks. We then modify the existing black-box textual attacks to implement Eq. (1) due to their efficiency and no reliance on the model gradient. Thus, both opensourced and proprietary LLMs can be attack targets. Our instantiations span four distinct levels, capturing a broad spectrum of complexities from simple character manipulations to sophisticated semantic alterations. Details of each attack are in Appendix A.3.</p>
<ul>
<li>Character-level: We employ TextBugger (Li et al., 2019) and DeepWordBug (Gao et al., 2018), which manipulate texts by introducing typos or errors to words, e.g., by adding, deleting, repeating, replacing, and permuting characters for certain words.</li>
<li>Word-level: We use BertAttack (Li et al., 2020) and TextFooler (Jin et al., 2019) to replace words with synonyms or contextually similar words to deceive LLMs.</li>
<li>Sentence-level: We implement StressTest (Naik et al., 2018) and CheckList (Ribeiro et al., 2020) to append irrelevant or extraneous sentences to the end of prompts, intending to distract LLMs. For StressTest, we adopt settings similar to those in (Wang et al., 2019), appending "and true is true", "and false is not true", or "and true is true" for five times to the end of a prompt. For the CheckList attack, we generate 50 random sequences consisting of alphabets and digits, each with a length of 10 , and append this random sequence to the end of a prompt.</li>
<li>Semantic-level: We simulate the linguistic behavior of people from different countries by choosing 6 common languages (Chinese, French, Arabic, Spanish, Japanese, and Korean) and constructing 10 prompts for each language per dataset. These prompts are then translated into English, introducing linguistic nuances and variations that could potentially impact LLMs.</li>
</ul>
<h1>Semantic preservation of adversarial prompts.</h1>
<p>Are adversarial prompts realistic? Our purpose is to simulate plausible user errors; thus, it is imperative that these prompts preserve semantic integrity, ensuring they remain both acceptable and imperceptible to human comprehension. It is of paramount importance that our adversarially engineered prompts retain coherence and realism. Our human study in Appendix A. 2 shows that at least $85 \%$ of subjects agreed that the generated prompts are acceptable.</p>
<p>Table 1: Statistics of datasets used in this paper.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">#Sample</th>
<th style="text-align: center;">#Class</th>
<th style="text-align: center;">#[Adv. prompt, sample]</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Sentiment analysis</td>
<td style="text-align: center;">SST2</td>
<td style="text-align: center;">872</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">73,248</td>
</tr>
<tr>
<td style="text-align: center;">Grammar correctness</td>
<td style="text-align: center;">CoLA</td>
<td style="text-align: center;">1,000</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">84,000</td>
</tr>
<tr>
<td style="text-align: center;">Duplicate sentence detection</td>
<td style="text-align: center;">QGE <br> MRPL</td>
<td style="text-align: center;">$\begin{gathered} 1,000 \ 808 \end{gathered}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 2 \ &amp; 2 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 84,000 \ &amp; 74,372 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">Natural language inference</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { MNLI } \ &amp; \text { QNLI } \ &amp; \text { RTI } \ &amp; \text { WNLI } \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 1,000 \ &amp; 1,000 \ &amp; 277 \ &amp; 71 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 3 \ &amp; 3 \ &amp; 3 \ &amp; 2 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 84,000 \ &amp; 84,000 \ &amp; 23,268 \ &amp; 3,964 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">Multi-task knowledge</td>
<td style="text-align: center;">MMLU</td>
<td style="text-align: center;">564</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">47,376</td>
</tr>
<tr>
<td style="text-align: center;">Reading comprehension</td>
<td style="text-align: center;">5QloAD V2</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">16,800</td>
</tr>
<tr>
<td style="text-align: center;">Translation</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { Multi UN } \ &amp; \text { IWSLT } 2017 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 99 \ &amp; 100 \end{aligned}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\begin{aligned} &amp; 8,316 \ &amp; 8,400 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">Math reasoning</td>
<td style="text-align: center;">Math</td>
<td style="text-align: center;">160</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">13,440</td>
</tr>
</tbody>
</table>
<h3>2.3 Tasks and datasets</h3>
<p>Currently, PromptRobust supports 8 tasks and 13 datasets across sentiment analysis to math reasoning. Due to space limit, we leave the details of the datasets in Appendix B.2.</p>
<h2>3 Experiments</h2>
<p>Setup. The extensive computational need to generate an adversarial prompt requires iterating throughout the dataset 100 on average. Therefore, the evaluation of an entire</p>
<p>Table 2: The APDR and standard deviations of different attacks on different datasets.</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Character-level</th>
<th></th>
<th>Word-level</th>
<th></th>
<th>Sentence-level</th>
<th></th>
<th>Semantic-level</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>TextBugger</td>
<td>DeepWordBug</td>
<td>TextFooler</td>
<td>BertAttack</td>
<td>CheckList</td>
<td>StressTest</td>
<td>Semantic</td>
<td></td>
</tr>
<tr>
<td>SST-2</td>
<td>0.25 (-)</td>
<td>0.18 (-)</td>
<td>0.35 (-)</td>
<td>0.34 (-)</td>
<td>0.22 (-)</td>
<td>0.15 (-)</td>
<td>0.28 (-)</td>
<td>0.07 (-)</td>
</tr>
<tr>
<td>CoLA</td>
<td>0.39 (-)</td>
<td>0.27 (-)</td>
<td>0.43 (-)</td>
<td>0.45 (-)</td>
<td>0.23 (-)</td>
<td>0.18 (-)</td>
<td>0.34 (-)</td>
<td>0.34 (-)</td>
</tr>
<tr>
<td>QQP</td>
<td>0.30 (-)</td>
<td>0.22 (-)</td>
<td>0.31 (-)</td>
<td>0.33 (-)</td>
<td>0.18 (-)</td>
<td>0.06 (-)</td>
<td>0.40 (-)</td>
<td>0.40 (-)</td>
</tr>
<tr>
<td>MRPC</td>
<td>0.37 (-)</td>
<td>0.34 (-)</td>
<td>0.37 (-)</td>
<td>0.42 (-)</td>
<td>0.24 (-)</td>
<td>0.25 (-)</td>
<td>0.39 (-)</td>
<td>0.39 (-)</td>
</tr>
<tr>
<td>MMLU</td>
<td>0.32 (-)</td>
<td>0.18 (-)</td>
<td>0.32 (-)</td>
<td>0.34 (-)</td>
<td>0.14 (-)</td>
<td>0.10 (-)</td>
<td>0.22 (-)</td>
<td>0.22 (-)</td>
</tr>
<tr>
<td>QNLI</td>
<td>0.38 (-)</td>
<td>0.40 (-)</td>
<td>0.50 (-)</td>
<td>0.52 (-)</td>
<td>0.25 (-)</td>
<td>0.23 (-)</td>
<td>0.40 (-)</td>
<td>0.40 (-)</td>
</tr>
<tr>
<td>RTE</td>
<td>0.33 (-)</td>
<td>0.25 (-)</td>
<td>0.37 (-)</td>
<td>0.40 (-)</td>
<td>0.18 (-)</td>
<td>0.17 (-)</td>
<td>0.42 (-)</td>
<td>0.42 (-)</td>
</tr>
<tr>
<td>WNLI</td>
<td>0.39 (-)</td>
<td>0.31 (-)</td>
<td>0.41 (-)</td>
<td>0.41 (-)</td>
<td>0.24 (-)</td>
<td>0.20 (-)</td>
<td>0.49 (-)</td>
<td>0.49 (-)</td>
</tr>
<tr>
<td>MMLU</td>
<td>0.21 (-)</td>
<td>0.12 (-)</td>
<td>0.21 (-)</td>
<td>0.40 (-)</td>
<td>0.13 (-)</td>
<td>0.03 (-)</td>
<td>0.20 (-)</td>
<td>0.13 (-)</td>
</tr>
<tr>
<td>SQuAD V2</td>
<td>0.09 (-)</td>
<td>0.05 (-)</td>
<td>0.25 (-)</td>
<td>0.31 (-)</td>
<td>0.02 (-)</td>
<td>0.02 (-)</td>
<td>0.08 (-)</td>
<td>0.08 (-)</td>
</tr>
<tr>
<td>IWSLT</td>
<td>0.08 (-)</td>
<td>0.10 (-)</td>
<td>0.27 (-)</td>
<td>0.12 (-)</td>
<td>0.10 (-)</td>
<td>0.17 (-)</td>
<td>0.18 (-)</td>
<td>0.18 (-)</td>
</tr>
<tr>
<td>UN Multi</td>
<td>0.06 (-)</td>
<td>0.08 (-)</td>
<td>0.15 (-)</td>
<td>0.10 (-)</td>
<td>0.06 (-)</td>
<td>0.09 (-)</td>
<td>0.15 (-)</td>
<td>0.15 (-)</td>
</tr>
<tr>
<td>Math</td>
<td>0.18 (-)</td>
<td>0.14 (-)</td>
<td>0.49 (-)</td>
<td>0.42 (-)</td>
<td>0.15 (-)</td>
<td>0.13 (-)</td>
<td>0.23 (-)</td>
<td>0.23 (-)</td>
</tr>
<tr>
<td>Avg</td>
<td>0.21 (-)</td>
<td>0.17 (-)</td>
<td>0.31 (-)</td>
<td>0.33 (-)</td>
<td>0.12 (-)</td>
<td>0.11 (-)</td>
<td>0.22 (-)</td>
<td>0.22 (-)</td>
</tr>
</tbody>
</table>
<p>Table 3: The APDR on different LLMs.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: left;">T5-large</th>
<th style="text-align: left;">Vicuna</th>
<th style="text-align: left;">Llama2</th>
<th style="text-align: left;">UL2</th>
<th style="text-align: left;">ChatGPT</th>
<th style="text-align: left;">GPT-4</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SST-2</td>
<td style="text-align: left;">0.04 (-)</td>
<td style="text-align: left;">0.83 (-)</td>
<td style="text-align: left;">0.24 (-)</td>
<td style="text-align: left;">0.03 (-)</td>
<td style="text-align: left;">0.17 (-)</td>
<td style="text-align: left;">0.24 (-)</td>
</tr>
<tr>
<td style="text-align: left;">CoLA</td>
<td style="text-align: left;">0.16 (-)</td>
<td style="text-align: left;">0.81 (-)</td>
<td style="text-align: left;">0.38 (-)</td>
<td style="text-align: left;">0.13 (-)</td>
<td style="text-align: left;">0.21 (-)</td>
<td style="text-align: left;">0.13 (-)</td>
</tr>
<tr>
<td style="text-align: left;">QQP</td>
<td style="text-align: left;">0.09 (-)</td>
<td style="text-align: left;">0.51 (-)</td>
<td style="text-align: left;">0.59 (-)</td>
<td style="text-align: left;">0.02 (-)</td>
<td style="text-align: left;">0.16 (-)</td>
<td style="text-align: left;">0.16 (-)</td>
</tr>
<tr>
<td style="text-align: left;">MRPC</td>
<td style="text-align: left;">0.17 (-)</td>
<td style="text-align: left;">0.52 (-)</td>
<td style="text-align: left;">0.84 (-)</td>
<td style="text-align: left;">0.06 (-)</td>
<td style="text-align: left;">0.22 (-)</td>
<td style="text-align: left;">0.04 (-)</td>
</tr>
<tr>
<td style="text-align: left;">MMLU</td>
<td style="text-align: left;">0.08 (-)</td>
<td style="text-align: left;">0.67 (-)</td>
<td style="text-align: left;">0.32 (-)</td>
<td style="text-align: left;">0.06 (-)</td>
<td style="text-align: left;">0.13 (-)</td>
<td style="text-align: left;">-0.03 (-)</td>
</tr>
<tr>
<td style="text-align: left;">QNLI</td>
<td style="text-align: left;">0.33 (-)</td>
<td style="text-align: left;">0.87 (-)</td>
<td style="text-align: left;">0.51 (-)</td>
<td style="text-align: left;">0.05 (-)</td>
<td style="text-align: left;">0.25 (-)</td>
<td style="text-align: left;">0.05 (-)</td>
</tr>
<tr>
<td style="text-align: left;">RTE</td>
<td style="text-align: left;">0.08 (-)</td>
<td style="text-align: left;">0.78 (-)</td>
<td style="text-align: left;">0.68 (-)</td>
<td style="text-align: left;">0.02 (-)</td>
<td style="text-align: left;">0.09 (-)</td>
<td style="text-align: left;">0.03 (-)</td>
</tr>
<tr>
<td style="text-align: left;">WNLI</td>
<td style="text-align: left;">0.13 (-)</td>
<td style="text-align: left;">0.78 (-)</td>
<td style="text-align: left;">0.73 (-)</td>
<td style="text-align: left;">0.04 (-)</td>
<td style="text-align: left;">0.14 (-)</td>
<td style="text-align: left;">0.04 (-)</td>
</tr>
<tr>
<td style="text-align: left;">MMLU</td>
<td style="text-align: left;">0.11 (-)</td>
<td style="text-align: left;">0.41 (-)</td>
<td style="text-align: left;">0.28 (-)</td>
<td style="text-align: left;">0.05 (-)</td>
<td style="text-align: left;">0.14 (-)</td>
<td style="text-align: left;">0.04 (-)</td>
</tr>
<tr>
<td style="text-align: left;">SQuAD V2</td>
<td style="text-align: left;">0.05 (-)</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">0.10 (-)</td>
<td style="text-align: left;">0.22 (-)</td>
<td style="text-align: left;">0.27 (-)</td>
</tr>
<tr>
<td style="text-align: left;">IWSLT</td>
<td style="text-align: left;">0.14 (-)</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">0.15 (-)</td>
<td style="text-align: left;">0.17 (-)</td>
<td style="text-align: left;">0.07 (-)</td>
</tr>
<tr>
<td style="text-align: left;">UN Multi</td>
<td style="text-align: left;">0.13 (-)</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">0.05 (-)</td>
<td style="text-align: left;">0.12 (-)</td>
<td style="text-align: left;">-0.02 (-)</td>
</tr>
<tr>
<td style="text-align: left;">Math</td>
<td style="text-align: left;">0.24 (-)</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">0.21 (-)</td>
<td style="text-align: left;">0.33 (-)</td>
<td style="text-align: left;">0.02 (-)</td>
</tr>
<tr>
<td style="text-align: left;">Avg</td>
<td style="text-align: left;">0.13 (-)</td>
<td style="text-align: left;">0.69 (-)</td>
<td style="text-align: left;">0.51 (-)</td>
<td style="text-align: left;">0.08 (-)</td>
<td style="text-align: left;">0.18 (-)</td>
<td style="text-align: left;">0.08 (-)</td>
</tr>
</tbody>
</table>
<p>Table 4: APDR on different prompts.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: left;">ZS-task</th>
<th style="text-align: left;">ZS-role</th>
<th style="text-align: left;">FS-task</th>
<th style="text-align: left;">FS-role</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SST-2</td>
<td style="text-align: left;">0.31 (-)</td>
<td style="text-align: left;">0.28 (-)</td>
<td style="text-align: left;">0.22 (-)</td>
<td style="text-align: left;">0.24 (-)</td>
</tr>
<tr>
<td style="text-align: left;">CoLA</td>
<td style="text-align: left;">0.43 (-)</td>
<td style="text-align: left;">0.43 (-)</td>
<td style="text-align: left;">0.34 (-)</td>
<td style="text-align: left;">0.25 (-)</td>
</tr>
<tr>
<td style="text-align: left;">QQP</td>
<td style="text-align: left;">0.43 (-)</td>
<td style="text-align: left;">0.34 (-)</td>
<td style="text-align: left;">0.16 (-)</td>
<td style="text-align: left;">0.14 (-)</td>
</tr>
<tr>
<td style="text-align: left;">MRPC</td>
<td style="text-align: left;">0.44 (-)</td>
<td style="text-align: left;">0.51 (-)</td>
<td style="text-align: left;">0.24 (-)</td>
<td style="text-align: left;">0.23 (-)</td>
</tr>
<tr>
<td style="text-align: left;">MMLU</td>
<td style="text-align: left;">0.29 (-)</td>
<td style="text-align: left;">0.26 (-)</td>
<td style="text-align: left;">0.19 (-)</td>
<td style="text-align: left;">0.21 (-)</td>
</tr>
<tr>
<td style="text-align: left;">QNLI</td>
<td style="text-align: left;">0.46 (-)</td>
<td style="text-align: left;">0.51 (-)</td>
<td style="text-align: left;">0.30 (-)</td>
<td style="text-align: left;">0.32 (-)</td>
</tr>
<tr>
<td style="text-align: left;">RTE</td>
<td style="text-align: left;">0.33 (-)</td>
<td style="text-align: left;">0.35 (-)</td>
<td style="text-align: left;">0.31 (-)</td>
<td style="text-align: left;">0.27 (-)</td>
</tr>
<tr>
<td style="text-align: left;">WNLI</td>
<td style="text-align: left;">0.36 (-)</td>
<td style="text-align: left;">0.39 (-)</td>
<td style="text-align: left;">0.37 (-)</td>
<td style="text-align: left;">0.33 (-)</td>
</tr>
<tr>
<td style="text-align: left;">MMLU</td>
<td style="text-align: left;">0.25 (-)</td>
<td style="text-align: left;">0.22 (-)</td>
<td style="text-align: left;">0.18 (-)</td>
<td style="text-align: left;">0.14 (-)</td>
</tr>
<tr>
<td style="text-align: left;">SQuAD V2</td>
<td style="text-align: left;">0.18 (-)</td>
<td style="text-align: left;">0.20 (-)</td>
<td style="text-align: left;">0.06 (-)</td>
<td style="text-align: left;">0.07 (-)</td>
</tr>
<tr>
<td style="text-align: left;">IWSLT</td>
<td style="text-align: left;">0.18 (-)</td>
<td style="text-align: left;">0.24 (-)</td>
<td style="text-align: left;">0.08 (-)</td>
<td style="text-align: left;">0.11 (-)</td>
</tr>
<tr>
<td style="text-align: left;">UN Multi</td>
<td style="text-align: left;">0.17 (-)</td>
<td style="text-align: left;">0.15 (-)</td>
<td style="text-align: left;">0.04 (-)</td>
<td style="text-align: left;">0.04 (-)</td>
</tr>
<tr>
<td style="text-align: left;">Math</td>
<td style="text-align: left;">0.33 (-)</td>
<td style="text-align: left;">0.39 (-)</td>
<td style="text-align: left;">0.16 (-)</td>
<td style="text-align: left;">0.17 (-)</td>
</tr>
<tr>
<td style="text-align: left;">Avg</td>
<td style="text-align: left;">0.33 (-)</td>
<td style="text-align: left;">0.34 (-)</td>
<td style="text-align: left;">0.21 (-)</td>
<td style="text-align: left;">0.21 (-)</td>
</tr>
</tbody>
</table>
<p>dataset using LLMs is unfeasible. To alleviate the computational constraint and preserve a fair study process, we adopt a sampling strategy that entails selecting a subset of samples from the validation or test sets across various datasets. The statistics of each dataset and tasks are summarized in Table 1. ${ }^{2}$ The sampling details are in Appendix C.
We initially assess the performance of all LLMs without prompt attacks to provide a performance baseline. We find that certain LLMs do not even demonstrate satisfactory performance with clean prompts, narrowing our selection to 6 LLMs: Flan-T5-large, Vicuna-13B, Llama2-13B-chat, UL2, ChatGPT, and GPT-4. Further details and discussions on clean prompt performance in all LLMs are available in Appendix C.2. We generate 10 distinct prompts for both role-oriented and task-oriented categories. Each prompt can be augmented with three examples, forming the few-shot prompts. In total, we have 40 prompts for each dataset on each LLM. For better efficiency and performance, we select the top 3 best-performing prompts of each type to perform prompt attacks. As a result, we evaluate the adversarial vulnerabilities of 9 LLMs across 13 datasets, encompassing a total of 4,788 prompts ${ }^{3}$ and their respective adversarial counterparts. This comprehensive evaluation allows us to gain valuable insight into the robustness and performance of LLMs in a wide range of scenarios and prompt styles.
Evaluation metrics. Considering the diverse evaluation metrics across tasks and varying baseline performances across models and datasets, the absolute performance drop may not provide a meaningful comparison. Thus, we introduce a unified metric, the Performance Drop Rate (PDR). PDR quantifies the relative performance decline following a prompt attack, offering a contextually normalized measure to compare different attacks, datasets, and models. The PDR is given by: $P D R\left(A, P, f_{\theta}, \mathcal{D}\right)=1-\frac{\sum_{x \mid y \mid \in \mathcal{D}} \mathcal{M}\left[f_{\theta}([A(P), x]), y\right]}{\sum_{x \mid y \mid \in \mathcal{D}} \mathcal{M}\left[f_{\theta}([P, x]), y\right]}$, where $A$ is the</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>adversarial attack applied to prompt $P$, and $\mathcal{M}[\cdot]$ is the evaluation function: for classification task, $\mathcal{M}[\cdot]$ is the indicator function $\mathbb{1}[\hat{y}, y]$ which equals to 1 when $\hat{y}=y$, and 0 otherwise; for reading comprehension task, $\mathcal{M}[\cdot]$ is the F1-score; for translation tasks, $\mathcal{M}[\cdot]$ is the Bleu metric (Papineni et al., 2002). Note that a negative PDR implies that adversarial prompts can enhance performance.</p>
<h1>4 Results and analysis</h1>
<h3>4.1 Results across attacks, models, and prompts</h3>
<p>We report and discuss the Average PDR (APDR) across different attacks, LLMs, and prompts. Our main results are based on all the prompts, whose conclusions are in consistent with Appendix C. 4 where we show result by excluding unacceptable adversarial prompts. Note that although our semantic preserving study demonstrated that at least $85 \%$ of adversarial prompts are acceptable, there are still some adversarial prompts diverged from their intended semantic meaning. Furthermore, note that the discrepancies in APDR variance values are due to varying PDR values across different attacks, prompts, models and datasets.
Analysis on attacks. Table 2 summarizes the APDR of 7 attacks on 13 datasets, calculated by $A P D R_{A}(A, \mathcal{D})=\frac{1}{|\mathcal{P}|} \frac{1}{|\mathcal{F}|} \sum_{P \in \mathcal{P}} \sum_{f_{\theta} \in \mathcal{F}} P D R\left(A, P, f_{\theta}, \mathcal{D}\right)$, where $\mathcal{P}$ is the set of 4 types of prompts and $\mathcal{F}$ is the set of all models. The results offer several key insights. Firstly, attack effectiveness is highly variable, with word-level attacks proving the most potent, leading to an average performance decline of $33 \%$ across all datasets. Character-level attacks rank second, inducing a $20 \%$ performance drop in most datasets. Notably, semantic-level attacks exhibit potency nearly commensurate with character-level attacks, emphasizing the profound impact of nuanced linguistic variations on LLMs' performance. On the contrary, sentence-level attacks pose less of a threat, suggesting adversarial interventions at this level have a diminished effect. Moreover, the effect of prompt attack varies across different datasets. For instance, StressTest attacks on SQUAD V2 yield a mere 2\% performance drop, while inflicting a $25 \%$ drop on MRPC. Furthermore, we observe that the StressTest attack paradoxically bolsters the model's performance in some datasets, we delve into this phenomenon in Appendix D.3.
Note that while character-level attacks are detectable by grammar detection tools, wordand semantic-level attacks underscore the importance of robust semantic understanding and accurate task presentation/translation for LLMs. A comprehensive understanding of these nuances will inform a deeper comprehension of adversarial attacks on LLMs.
Analysis on LLMs. Table 3 summarizes the APDR of 9 LLMs on 13 datasets, calculated by $A P D R_{f_{\theta}}\left(f_{\theta}, \mathcal{D}\right)=\frac{1}{|\mathcal{A}|} \frac{1}{|\mathcal{P}|} \sum_{A \in \mathcal{A}} \sum_{P \in \mathcal{P}} P D R\left(A, P, f_{\theta}, \mathcal{D}\right)$, where $\mathcal{P}$ is the set of 4 types of prompts and $\mathcal{A}$ is the set of 7 attacks. Our analysis reveals that GPT-4 and UL2 significantly outperform other models in terms of robustness, followed by T5-large, ChatGPT, and Llama2, with Vicuna presenting the least robustness. The robustness against adversarial prompts of UL2, T5-large, and ChatGPT varies across datasets, with UL2 and T5-large showing less vulnerability to attacks on sentiment classification (SST-2), most NLI tasks, and reading comprehension (SQuAD V2). Specifically, UL2 excels in translation tasks, while ChatGPT displays robustness in certain NLI tasks. Vicuna, however, exhibits consistently high susceptibility to attacks across all tasks. It can be seen that, given the same adversarial prompts generated by ChatGPT, GPT-4 exhibits superior robustness in all tasks. However, it is crucial to realize that this observed robustness might attribute to the weak transferability of the adversarial prompts crafted specifically for ChatGPT. In the future, the performance of GPT-4 and ChatGPT could be improved since these proprietary models keep evolving. Models vs. attacks. We show in Table 10 the relation between models and attacks. Generally, word-level attacks emerge as the most potent, and BertAttack consistently outperforms others across all models. However, no discernible pattern emerges for the efficacy of the other attacks. For instance, while TextBugger proves more effective than DeepWordBug for some models such as Llama2 and ChatGPT, the inverse holds true for T5-large. Notably, Vicuna and Llama2 are distinctly vulnerable to sentence-level attacks, in contrast to models like T5-large and ChatGPT, which remain largely unaffected. Such observations may hint at inherent vulnerabilities specific to Llama-based models.</p>
<p>Table 5: Attention visualization of samples that are correctly classified by clean prompts but misclassified by adv. prompts. For each attack, the above is the clean prompt with sample text, the below is the corresponding adversarial prompt with the same sample text. N=Negative, $\mathrm{P}=$ Positive and $\mathrm{N} / \mathrm{A}$ means the response is not available. The green and red color denote right and wrong answers, respectively. Color intensity denotes different attention weights (heavier color means larger weights).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Attack</th>
<th style="text-align: center;">Pred.</th>
<th style="text-align: center;">[Prompt, sample]</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">BertAttack</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">In the role of a sentiment analysis tool, respond with "positive" or "negative" to classify this statement for title not only describes its main characters but the people behind the camera as well. Answer:</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">In the role of a compliment analysis tool, respond with "positive" or "negative" to classify this statement for title not only describes its main characters but the key people behind the camera as well. Answer:</td>
</tr>
<tr>
<td style="text-align: center;">CheckList</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">Given the context of this text, indicate if the emotion conveyed is "positive" or "negative" good, this text very well. Answer:</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">Given the context of this text, indicate if the emotion conveyed is "positive" or "negative" LAPATAMASgood, this text very well. Answer:</td>
</tr>
<tr>
<td style="text-align: center;">DeepWordBug</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">Serving as a sentiment evaluation model, determine if the given statement is "positive" or "negative" classify bad to lack away this was good. Answer:</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">In the role of a sentiment evaluation model, show if the three statement is "positive" or "negative" classify I bad to lack away this was good andd. Answer:</td>
</tr>
<tr>
<td style="text-align: center;">Semantic</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">In the role of a sentiment analysis tool, respond with "positive" or "negative" to classify this statement.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">Classify what you're trying to convey in this sentence as "positive" if it's positive, and "negative" if it's negative. Answer:</td>
</tr>
</tbody>
</table>
<p>Table 6: The APDR of transferability of several LLMs.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Attacks</th>
<th style="text-align: center;">Chat $\rightarrow T 5$</th>
<th style="text-align: center;">Chat $\rightarrow$ UL2</th>
<th style="text-align: center;">Chat $\rightarrow$ V</th>
<th style="text-align: center;">T5 $\rightarrow$ Chat</th>
<th style="text-align: center;">T3 $\rightarrow$ UL2</th>
<th style="text-align: center;">T3 $\rightarrow$ V</th>
<th style="text-align: center;">UL2 $\rightarrow$ Chat</th>
<th style="text-align: center;">UL2 $\rightarrow$ T5</th>
<th style="text-align: center;">UL2 $\rightarrow$ V</th>
<th style="text-align: center;">V $\rightarrow$ Chat</th>
<th style="text-align: center;">V $\rightarrow$ T5</th>
<th style="text-align: center;">V $\rightarrow$ UL2</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">BertAttack</td>
<td style="text-align: center;">$0.05 \times \times 1$</td>
<td style="text-align: center;">$0.08 \times \times 10$</td>
<td style="text-align: center;">$0.08 \times \times 48$</td>
<td style="text-align: center;">$0.18 \times \times 11$</td>
<td style="text-align: center;">$0.11 \times \times 23$</td>
<td style="text-align: center;">$-1.39 \times \times 47$</td>
<td style="text-align: center;">$0.15 \times \times 37$</td>
<td style="text-align: center;">$0.05 \times \times 11$</td>
<td style="text-align: center;">$-0.70 \times 118$</td>
<td style="text-align: center;">$0.06 \times \times 19$</td>
<td style="text-align: center;">$0.05 \times \times 11$</td>
<td style="text-align: center;">$0.03 \times \times 11$</td>
</tr>
<tr>
<td style="text-align: left;">CheckList</td>
<td style="text-align: center;">$0.00 \times \times 18$</td>
<td style="text-align: center;">$0.01 \times \times 10$</td>
<td style="text-align: center;">$0.19 \times \times 39$</td>
<td style="text-align: center;">$0.00 \times \times 67$</td>
<td style="text-align: center;">$0.01 \times \times 21$</td>
<td style="text-align: center;">$-0.09 \times \times 16$</td>
<td style="text-align: center;">$0.01 \times \times 26$</td>
<td style="text-align: center;">$0.01 \times \times 10$</td>
<td style="text-align: center;">$-0.13 \times 130$</td>
<td style="text-align: center;">$0.01 \times \times 24$</td>
<td style="text-align: center;">$0.00 \times \times 10$</td>
<td style="text-align: center;">$0.00 \times \times 10$</td>
</tr>
<tr>
<td style="text-align: left;">TextFischer</td>
<td style="text-align: center;">$0.04 \times \times 18$</td>
<td style="text-align: center;">$0.03 \times \times 10$</td>
<td style="text-align: center;">$-0.25 \times \times 10$</td>
<td style="text-align: center;">$0.11 \times \times 13$</td>
<td style="text-align: center;">$0.08 \times \times 16$</td>
<td style="text-align: center;">$-0.30 \times \times 39$</td>
<td style="text-align: center;">$0.11 \times \times 33$</td>
<td style="text-align: center;">$0.07 \times \times 14$</td>
<td style="text-align: center;">$-0.17 \times 146$</td>
<td style="text-align: center;">$0.04 \times \times 14$</td>
<td style="text-align: center;">$0.02 \times \times 16$</td>
<td style="text-align: center;">$0.00 \times \times 10$</td>
</tr>
<tr>
<td style="text-align: left;">TextBuggest</td>
<td style="text-align: center;">$-0.00 \times \times 10$</td>
<td style="text-align: center;">$-0.01 \times \times 10$</td>
<td style="text-align: center;">$0.02 \times \times 94$</td>
<td style="text-align: center;">$0.04 \times \times 17$</td>
<td style="text-align: center;">$0.01 \times \times 24$</td>
<td style="text-align: center;">$-0.45 \times \times 10$</td>
<td style="text-align: center;">$0.04 \times \times 13$</td>
<td style="text-align: center;">$0.02 \times \times 37$</td>
<td style="text-align: center;">$-0.84 \times \times 42$</td>
<td style="text-align: center;">$0.03 \times \times 11$</td>
<td style="text-align: center;">$0.01 \times \times 10$</td>
<td style="text-align: center;">$0.00 \times \times 10$</td>
</tr>
<tr>
<td style="text-align: left;">DeepWordBug</td>
<td style="text-align: center;">$0.03 \times \times 11$</td>
<td style="text-align: center;">$0.01 \times \times 10$</td>
<td style="text-align: center;">$0.10 \times \times 46$</td>
<td style="text-align: center;">$0.00 \times \times 36$</td>
<td style="text-align: center;">$0.01 \times \times 22$</td>
<td style="text-align: center;">$-0.18 \times \times 20$</td>
<td style="text-align: center;">$0.01 \times \times 33$</td>
<td style="text-align: center;">$0.02 \times \times 36$</td>
<td style="text-align: center;">$-0.09 \times 370$</td>
<td style="text-align: center;">$0.00 \times \times 33$</td>
<td style="text-align: center;">$0.02 \times \times 11$</td>
<td style="text-align: center;">$0.00 \times \times 10$</td>
</tr>
<tr>
<td style="text-align: left;">StressTest</td>
<td style="text-align: center;">$0.04 \times \times 17$</td>
<td style="text-align: center;">$0.03 \times \times 11$</td>
<td style="text-align: center;">$0.01 \times \times 28$</td>
<td style="text-align: center;">$-0.02 \times \times 12$</td>
<td style="text-align: center;">$0.03 \times \times 16$</td>
<td style="text-align: center;">$0.04 \times \times 14$</td>
<td style="text-align: center;">$0.00 \times \times 24$</td>
<td style="text-align: center;">$0.05 \times \times 14$</td>
<td style="text-align: center;">$0.06 \times \times 10$</td>
<td style="text-align: center;">$0.00 \times \times 14$</td>
<td style="text-align: center;">$0.09 \times \times 14$</td>
<td style="text-align: center;">$0.02 \times \times 14$</td>
</tr>
<tr>
<td style="text-align: left;">Semantic</td>
<td style="text-align: center;">$0.04 \times \times 11$</td>
<td style="text-align: center;">$0.02 \times \times 16$</td>
<td style="text-align: center;">$0.25 \times \times 47$</td>
<td style="text-align: center;">$0.07 \times \times 37$</td>
<td style="text-align: center;">$0.00 \times \times 23$</td>
<td style="text-align: center;">$-0.81 \times \times 13$</td>
<td style="text-align: center;">$0.02 \times \times 11$</td>
<td style="text-align: center;">$-0.13 \times \times 73$</td>
<td style="text-align: center;">$-0.50 \times 139$</td>
<td style="text-align: center;">$0.07 \times \times 11$</td>
<td style="text-align: center;">$0.00 \times \times 19$</td>
<td style="text-align: center;">$0.00 \times \times 10$</td>
</tr>
</tbody>
</table>
<p>Analysis on types of prompts. Table 4 summarizes the APDR of 4 types of prompts on 13 datasets, calculated by $A P D R_{t}(\mathcal{D})=\frac{1}{|\mathcal{A}|} \frac{1}{|\mathcal{P}<em _mathcal_A="\mathcal{A">{t}|} \frac{1}{|\mathcal{F}|} \sum</em>} \in \mathcal{A}} \sum_{P \in \mathcal{P<em f__theta="f_{\theta">{t}} \sum</em>$ is the set of all models. In our analysis, few-shot prompts consistently demonstrate superior robustness compared to zero-shot prompts across all datasets. Furthermore, while task-oriented prompts marginally outperform role-oriented prompts in overall robustness, both show varying strengths across different datasets and tasks. For example, role-oriented prompts present increased robustness within the SST-2 and QQP datasets, while task-oriented prompts are more resilient within the MRPC, QNLI, SQuAD V2, and IWSLT datasets. Insights into different effects of prompt types on model vulnerability can inform better prompt design and tuning strategies, enhancing LLMs robustness against adversarial prompt attacks.} \in \mathcal{F}} P D R\left(A, P, f_{\theta}, \mathcal{D}\right)$, where $\mathcal{P}_{t}$ is the set of prompts of certain type $t, \mathcal{A}$ is the set of 7 attacks and $\mathcal{F</p>
<h1>4.2 Results on model size, fine-tuning, and adversarial inputs</h1>
<p>Model size and fine-tuning. We analyze the performance on different models sizes using the Llama2 series (7B, 13B, and 70B). Our results in Appendix C. 3 show that larger models are typically more robust than smaller ones, but exceptions can occur when smaller models outperform larger ones, which is an interesting finding that can trigger future research. We also evaluated the impact of fine-tuning using vanilla Llama2 and Llama2-chat models in Appendix C.3, indicating that fine-tuned models are generally better at adversarial prompts.
Attacking both prompts and samples. We attacked both prompts and tested on AdvGLUE (Wang et al., 2021), which contains adversarial samples. Our results in Table 11 show that attacking both will perform even worse. However, intriguing things happen, since attacking both can sometimes enhance the performance that needs further effort.</p>
<h3>4.3 Understanding the vulnerability of LLMs to adversarial prompts</h3>
<p>We study the magic behind adversarial prompts to analyze why they lead to errors for LLMs from different aspects: attention visualization, erroneous response analysis (Appendix D.1), and sentence-level analysis (Appendix D.3).</p>
<p>We visualize the attentions to investigate the influence of adversarial prompts on LLMs' focus on input words. Specifically, we propose two attention visualization techniques: 1) Attention by Gradient, which assigns an attention score to each word based on the gradient norm, and 2) Attention by Deletion, which assigns an attention score to each word by examining the absolute change in loss when the word is removed. The details of these methods can be found in Appendix D.2. Both techniques produce similar results; hence, we focus on results from the Attention by Gradient method for simplicity. Our key findings, as demonstrated in Table 5, are as follows:</p>
<ul>
<li>Clean prompts: efficient attention allocation. LLMs predominantly concentrate on key terms within clean prompts, aiding in accurate classifications. For instance, for clean prompts of BertAttack in Table 5, LLMs mainly allocate attention to the term 'lazy', correctly deducing a 'Negative' sentiment.</li>
<li>Adversarial prompts: attention divergence. Adversarial prompts can reroute LLMs' attention from integral text segments, causing misclassifications. In some attacks like CheckList and StressTest, the model simultaneously concentrates on the target text and adversarial content, amplifying its susceptibility to adversarial perturbations. For instance, introducing a random sequence 'LKFOFZxMZ4' during a CheckList attack distracts the model, reducing focus on the critical word 'good' for accurate classification. In other attacks, such as BertAttack and DeepWordBug, the model's attention is entirely diverted from the text requiring classification towards adversarial prompts, leading to a significant shift in focus. For example, in DeepWordBug attack, typos in specific words divert the model's attention from 'awful' to the altered word 'Qetermine'.</li>
</ul>
<h1>4.4 Transferability of adversarial prompts</h1>
<p>Table 6 displays the effectiveness of various attacks in transferring adversarial prompts between several LLMs. For each dataset and prompt type, we selected the most vulnerable prompts generated by a source model (e.g., ChatGPT). These prompts were then utilized to launch transfer attacks against the target models (e.g., T5-large). The impact of these transfer attacks was quantified by calculating $A P D R_{\text {transfer }}\left(A, f_{\theta}^{\text {target }}\right)=$ $\frac{1}{\left|\mathcal{P}<em P="P" _in="\in" _mathcal_P="\mathcal{P">{\text {source }}\right|}[\mathbf{D}] \sum</em><em _mathcal_D="\mathcal{D">{\text {source }}} \sum</em>$ is the set of all datasets.
In general, we observe that adversarial prompts exhibit some degree of transferability. However, it is marginal compared to Table 2 and 3. Specifically, the APDR in the target model by adversarial prompts from source model is small compared to the original APDR of the source model. Furthermore, the standard deviation tends to be larger than the APDR, indicating that the transferability is inconsistent. Some adversarial prompts can be successfully transferred, causing a performance drop, while others may unexpectedly improve the performance of the target model. A prime example is the BertAttack transfer from UL2 to Vicuna, which resulted in a $-0.70(3.18)$ value, suggesting an unanticipated enhancement in Vicuna's performance when subjected to these adversarial prompts. These phenomena illustrate the complex robustness traits of different models. The transferability to ChatGPT is better compared to T5-large and UL2. This suggests an avenue to generate adversarial prompts to attack black-box models such as ChatGPT by training on small models like T5-large, which could be used for future robustness research.} \in \mathbf{D}} P D R\left(A, P, f_{\theta}^{\text {target }}, \mathcal{D}\right)$, where $f_{\theta}^{\text {target }}$ is the target model, $\mathcal{P}_{\text {source }}$ is the prompts selected from source model and $\mathbb{D</p>
<h3>4.5 Which prompts are more robust? Word frequency</h3>
<p>The word frequency results of these two datasets are presented in Appendix E. Our findings underscore that the resilience of a prompt is intricately tied to the contextual use of words, rather than the mere presence of certain terms. This complexity suggests that factors beyond word frequency, such as semantic coherence and syntactic structures, might be instrumental in determining robustness. This knowledge is valuable as it can influence future research on the robustness of LLMs, provide guidance for crafting more resistant prompts, and facilitate the creation of defensive mechanisms against adversarial prompt attacks. It is essential to emphasize that our observations are rooted in the current scope of models and datasets. Furthermore, the robustness or vulnerability of words remains deeply context-dependent.</p>
<p>Hence, direct determination of word robustness without considering the broader context may lead to oversimplified or inaccurate conclusions.</p>
<h1>4.6 Countermeasures and defenses</h1>
<p>We discuss potential countermeasures. 1) Input preprocessing: One approach involves directly detecting and addressing potential adversaries, such as detecting typos, irrelevant sequences, and enhancing clarity and conciseness of prompts. 2) Incorporate low-quality data in pre-training: Low-quality data can serve as potential adversaries, and explicitly including low-quality data during pre-training may develop a better understanding of diverse inputs and build resilience against adversaries. 3) Improved fine-tuning: Finetuning could lead to improved robustness. As we demonstrated before, models such as T5 and UL2 exhibit greater robustness compared to ChatGPT, suggesting potential benefits of large-scale supervised fine-tuning. More details are in Appendix F.</p>
<h2>5 Related work</h2>
<p>LLM Robustness Evaluation. AdvGLUE (Wang et al., 2021) stands as a static dataset for evaluating adversarial robustness of input samples. DecodingTrust (Wang et al., 2023a) undertakes a comprehensive assessment of trustworthiness in GPT models, notably GPT3.5 and GPT-4. The research delves into areas like toxicity, stereotype bias, adversarial challenges, and privacy. Specifically, they evaluate the robustness on standard datasets AdvGLUE(Wang et al., 2021) and AdvGLUE++ (Wang et al., 2023a). Specifically for adversarial robustness, DecodingTrust also focuses on evaluating the robustness of input samples instead of prompts and it still uses static datasets rather than an actionable benchmark suite. In contrast, PromptRobust is positioned as an open benchmark concentrating on adversarial prompts rather than samples (and it can be extended to include samples). Note that the prompts are general instructions to assist the in-context learning of LLMs to perform specific tasks, and they can be combined with many samples in certain tasks. Prompts are indispensable in human-LLMs interaction while input samples may not be needed, which means that prompts are versatile and it is essential to evaluate their robustness.</p>
<p>Safety of LLMs. We mimic the potential user prompts by creating adversarial prompts, but the main purpose is not to actually attack the model. This distinguishes our work from existing efforts in safety research of LLMs. Specifically, both SafetyPrompts (Sun et al., 2023) and prompt injection attacks (Perez \&amp; Ribeiro, 2022; Greshake et al., 2023; Liu et al., 2023b) are engineered to spotlight potentially harmful instructions that could steer LLMs into delivering outputs misaligned with human values or perform unintended actions such as data leakage and unauthorized access. Adversarial prompts are crafted to mimic plausible mistakes an end-user might inadvertently make. Our goal is to assess the extent to which these prompts, even if they slightly deviate from the norm, can skew LLM outcomes. These adversarial prompts retain their semantic integrity, ensuring they're virtually imperceptible for humans. The adversarial prompts are not designed to elicit harmful or misleading responses.</p>
<h2>6 Conclusion and Limitation</h2>
<p>The robustness of the prompts in LLMs is of paramount concern in security and humancomputer interaction. In this paper, we thoroughly evaluated the robustness of LLMs to adversarial prompts using the proposed PromptRobust benchmark. The key is to leverage adversarial attack approaches to mimic potential perturbations such as typos, synonyms, and stylistic differences. We then performed extensive experiments and analysis on various tasks and models. While the results show that current LLMs are not robust enough to adversarial prompts, we further analyzed the reason behind it using attention visualization. Moreover, we analyzed the frequent words to provide guidance for both experts and nonexperts in developing better prompt engineering tools. PromptRobust will be open-sourced to serve as a foundational tool for robust LLMs research.
There are several limitations. First, due to the substantial computation, we did not perform evaluations on the full datasets, but relied on sampling. Future research may evaluate on the entire datasets to gain more insights. Second, we cannot included all LLMs and datasets due to computation constraint. Including more in the future could provide a more diverse perspective. Third, we did not evaluate more advanced techniques of prompt engineering</p>
<p>such as chain-of-thought (CoT) (Wei et al., 2022) and tree-of-thought (ToT) (Yao et al., 2023). We believe more evaluations can be done on latest prompt engineering techniques. Fourth, we considered black-box prompt attacks, which can generate perturbations that can mimic naturally occurred errors. Optimized white-box prompt attacks may produce better adversarial prompts, which is an interesting future work.</p>
<h1>References</h1>
<p>Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the dangers of stochastic parrots: Can language models be too big? FAccT 2021, pp. 610-623, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450383097.</p>
<p>Stella Biderman, USVSN Sai Prashanth, Lintang Sutawika, Hailey Schoelkopf, Quentin Anthony, Shivanshu Purohit, and Edward Raf. Emergent and predictable memorization in large language models. arXiv preprint arXiv:2304.11158, 2023.</p>
<p>Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. Gpt-neox-20b: An open-source autoregressive language model, 2022. URL https://arxiv.org/abs/2204.06745.</p>
<p>Google Brain. A new open source flan 20b with ul2, 2023. URL https://www.yitay.net/ blog/flan-ul2-20b.</p>
<p>Tom B Brown, Dandelion Mané, Aurko Roy, Martín Abadi, and Justin Gilmer. Adversarial patch. arXiv preprint arXiv:1712.09665, 2017.</p>
<p>Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang. Quantifying memorization across neural language models. In The Eleventh International Conference on Learning Representations, 2023.</p>
<p>Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St. John, Noah Constant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, Brian Strope, and Ray Kurzweil. Universal sentence encoder for English. In EMNLP, pp. 169-174, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/ D18-2029. URL https://aclanthology.org/D18-2029.</p>
<p>Mauro Cettolo, Marcello Federico, Luisa Bentivogli, Jan Niehues, Sebastian Stüker, Katsuhito Sudoh, Koichiro Yoshino, and Christian Federmann. Overview of the IWSLT 2017 evaluation campaign. In Proceedings of the 14th International Conference on Spoken Language Translation, pp. 2-14, Tokyo, Japan, December 14-15 2017. International Workshop on Spoken Language Translation. URL https://aclanthology.org/2017.iwslt-1.1.</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with $90 \%$ * chatgpt quality, March 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/.</p>
<p>Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models, 2022. URL https: //arxiv.org/abs/2210.11416.</p>
<p>Fred J Damerau. A technique for computer detection and correction of spelling errors. Communications of the ACM, 7(3):171-176, 1964.</p>
<p>Databricks. Hello dolly: Democratizing the magic of chatgpt with open models, 2023. URL https://www.databricks.com/blog/2023/03/24/ hello-dolly-democratizing-magic-chatgpt-open-models.html.</p>
<p>Nolan Dey, Gurpreet Gosal, Zhiming, Chen, Hemant Khachane, William Marshall, Ribhu Pathria, Marvin Tom, and Joel Hestness. Cerebras-gpt: Open compute-optimal language models trained on the cerebras wafer-scale cluster, 2023.</p>
<p>William B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005), 2005. URL https://aclanthology.org/105-5002.</p>
<p>Andreas Eisele and Yu Chen. MultiUN: A multilingual corpus from united nation documents. In Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC'10), Valletta, Malta, May 2010. European Language Resources Association (ELRA). URL http://www.lrec-conf.org/proceedings/lrec2010/pdf/686_ Paper.pdf.</p>
<p>Marzieh Fadaee, Arianna Bisazza, and Christof Monz. Data augmentation for low-resource neural machine translation. arXiv preprint arXiv:1705.00440, 2017.
J. Gao, J. Lanchantin, M. L. Soffa, and Y. Qi. Black-box generation of adversarial text sequences to evade deep learning classifiers. In 2018 IEEE Security and Privacy Workshops (SPW), pp. 50-56, May 2018. doi: 10.1109/SPW.2018.00016.</p>
<p>Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.</p>
<p>Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten Holz, and Mario Fritz. More than you've asked for: A comprehensive analysis of novel prompt injection threats to application-integrated large language models. arXiv preprint arXiv:2302.12173, 2023.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id= d7KBjmI3GmQ.</p>
<p>Jordan Hoffmann et al. Training compute-optimal large language models, 2022.
Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. Adaptive mixtures of local experts. Neural computation, 3(1):79-87, 1991.</p>
<p>Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits. Is bert really robust? natural language attack on text classification and entailment. arXiv preprint arXiv:1907.11932, 2019.</p>
<p>Karen Kukich. Techniques for automatically correcting words in text. ACM computing surveys (CSUR), 24(4):377-439, 1992.</p>
<p>Alexey Kurakin, Ian Goodfellow, Samy Bengio, et al. Adversarial examples in the physical world, 2016.</p>
<p>Hector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge. In Thirteenth international conference on the principles of knowledge representation and reasoning, 2012.</p>
<p>Jinfeng Li, Shouling Ji, Tianyu Du, Bo Li, and Ting Wang. TextBugger: Generating adversarial text against real-world applications. In Proceedings 2019 Network and Distributed System Security Symposium. Internet Society, 2019. doi: 10.14722/ndss.2019.23138. URL https: //doi.org/10.14722\%2Fndss.2019.23138.</p>
<p>Linyang Li, Ruotian Ma, Qipeng Guo, Xiangyang Xue, and Xipeng Qiu. BERT-ATTACK: Adversarial attack against BERT using BERT. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 6193-6202, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.500. URL https://aclanthology.org/2020.emnlp-main. 500.</p>
<p>Hanmeng Liu, Ruoxi Ning, Zhiyang Teng, Jian Liu, Qiji Zhou, and Yue Zhang. Evaluating the logical reasoning ability of chatgpt and gpt-4. arXiv preprint arXiv:2304.03439, 2023a.</p>
<p>Yi Liu, Gelei Deng, Yuekang Li, Kailong Wang, Tianwei Zhang, Yepang Liu, Haoyu Wang, Yan Zheng, and Yang Liu. Prompt injection attack against llm-integrated applications. arXiv preprint arXiv:2306.05499, 2023b.</p>
<p>Inbal Magar and Roy Schwartz. Data contamination: From memorization to exploitation. arXiv preprint arXiv:2203.08242, 2022.</p>
<p>Pankaj Malhotra, Lovekesh Vig, Gautam Shroff, Puneet Agarwal, et al. Long short term memory networks for anomaly detection in time series. In Esann, volume 2015, pp. 89, 2015.</p>
<p>Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Universal adversarial perturbations. In CVPR, pp. 1765-1773, 2017.</p>
<p>John Morris, Eli Lifland, Jin Yong Yoo, Jake Grigsby, Di Jin, and Yanjun Qi. Textattack: A framework for adversarial attacks, data augmentation, and adversarial training in nlp. In EMNLP, pp. 119-126, 2020.</p>
<p>Aakanksha Naik, Abhilasha Ravichander, Norman Sadeh, Carolyn Rose, and Graham Neubig. Stress test evaluation for natural language inference. In ACL, pp. 2340-2353, Santa Fe, New Mexico, USA, August 2018. Association for Computational Linguistics. URL https://aclanthology.org/C18-1198.</p>
<p>Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adversarial nli: A new benchmark for natural language understanding. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 4885-4901, 2020.</p>
<p>OpenAI. https://chat.openai.com.chat, 2023a.
OpenAI. Gpt-4 technical report, 2023b.
Tianyu Pang, Kun Xu, Chao Du, Ning Chen, and Jun Zhu. Improving adversarial robustness via promoting ensemble diversity. In International Conference on Machine Learning, pp. 4970-4979. PMLR, 2019.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In ACL, pp. 311-318, July 2002. doi: 10.3115/ 1073083.1073135. URL https://aclanthology.org/P02-1040.</p>
<p>Fábio Perez and Ian Ribeiro. Ignore previous prompt: Attack techniques for language models. arXiv preprint arXiv:2211.09527, 2022.</p>
<p>Massimo Quadrana, Alexandros Karatzoglou, Balázs Hidasi, and Paolo Cremonesi. Personalizing session-based recommendations with hierarchical recurrent neural networks. In proceedings of the Eleventh ACM Conference on Recommender Systems, pp. 130-137, 2017.</p>
<p>Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don't know: Unanswerable questions for SQuAD. In ACL, pp. 784-789, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-2124. URL https://aclanthology. org/P18-2124.</p>
<p>Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Semantically equivalent adversarial rules for debugging nlp models. In Proceedings of the 56th annual meeting of the association for computational linguistics (volume 1: long papers), pp. 856-865, 2018.</p>
<p>Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, and Sameer Singh. Beyond accuracy: Behavioral testing of NLP models with CheckList. In ACL, pp. 4902-4912, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.442. URL https://aclanthology.org/2020.acl-main. 442.</p>
<p>David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical reasoning abilities of neural models. In ICLR, 2019. URL https://openreview.net/ forum?id=H1gR5iR5FX.</p>
<p>Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017.</p>
<p>Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In EMNLP, pp. 1631-1642, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/ D13-1170.</p>
<p>Hao Sun, Zhexin Zhang, Jiawen Deng, Jiale Cheng, and Minlie Huang. Safety assessment of chinese large language models. arXiv preprint arXiv:2304.10436, 2023.</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023a.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.</p>
<p>Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh. Universal adversarial triggers for attacking and analyzing nlp. arXiv preprint arXiv:1908.07125, 2019.</p>
<p>Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. 2019. In the Proceedings of ICLR.</p>
<p>Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax, May 2021.</p>
<p>Boxin Wang, Chejian Xu, Shuohang Wang, Zhe Gan, Yu Cheng, Jianfeng Gao, Ahmed Hassan Awadallah, and Bo Li. Adversarial glue: A multi-task benchmark for robustness evaluation of language models. arXiv preprint arXiv:2111.02840, 2021.</p>
<p>Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, et al. Decodingtrust: A comprehensive assessment of trustworthiness in gpt models. arXiv preprint arXiv:2306.11698, 2023a.</p>
<p>Jindong Wang, Xixu Hu, Wenxin Hou, Hao Chen, Runkai Zheng, Yidong Wang, Linyi Yang, Haojun Huang, Wei Ye, Xiubo Geng, et al. On the robustness of chatgpt: An adversarial and out-of-distribution perspective. In International conference on learning representations (ICLR) workshop on Trustworthy and Reliable Large-Scale Machine Learning Models, 2023b.</p>
<p>Zhiguo Wang, Wael Hamza, and Radu Florian. Bilateral multi-perspective matching for natural language sentences, 2017.</p>
<p>Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability judgments. arXiv preprint arXiv:1805.12471, 2018.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022.</p>
<p>Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In NAACL HLT, pp. 1112-1122. Association for Computational Linguistics, 2018. URL http://aclweb.org/anthology/#18-1101.</p>
<p>Thomas Wolf et al. Huggingface's transformers: State-of-the-art natural language processing, 2020.</p>
<p>Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael GontijoLopes, Ari S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, et al. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In International Conference on Machine Learning, pp. 23965-23998. PMLR, 2022a.</p>
<p>Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs, Raphael Gontijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, et al. Robust fine-tuning of zero-shot models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7959-7971, 2022b.</p>
<p>Linyi Yang, Shuibai Zhang, Libo Qin, Yafu Li, Yidong Wang, Hanmeng Liu, Jindong Wang, Xing Xie, and Yue Zhang. Glue-x: Evaluating natural language understanding models from an out-of-distribution generalization perspective. In ACL 2023 Findings, 2023.</p>
<p>Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of Thoughts: Deliberate problem solving with large language models, 2023.</p>
<p>Yuan Zang, Fanchao Qi, Chenghao Yang, Zhiyuan Liu, Meng Zhang, Qun Liu, and Maosong Sun. Word-level textual adversarial attacking as combinatorial optimization. In Proceedings of $A C L, 2020$.</p>
<p>Ziqi Zhang, Yuanchun Li, Jindong Wang, Bingyan Liu, Ding Li, Yao Guo, Xiangqun Chen, and Yunxin Liu. Remos: reducing defect inheritance in transfer learning via relevant model slicing. In Proceedings of the 44th International Conference on Software Engineering, pp. 1856-1868, 2022.</p>
<p>Terry Yue Zhuo, Zhuang Li, Yujin Huang, Yuan-Fang Li, Weiqing Wang, Gholamreza Haffari, and Fatemeh Shiri. On robustness of prompt-based semantic parsing with large pre-trained language model: An empirical study on codex. arXiv preprint arXiv:2301.12868, 2023.</p>
<h1>A Details on Adversarial Prompts</h1>
<h2>A. 1 Examples of prompts</h2>
<p>Table 7 shows the details of the task and role-oriented prompts in zero-shot and few-shot settings. Table 8 shows examples of 7 adversarial attacks on prompts.</p>
<p>Table 7: Examples of 4 types of prompts.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Zero-shot</th>
<th style="text-align: center;">Task-oriented</th>
<th style="text-align: center;">Evaluate the sentiment of the given text and classify it as 'positive' or 'negative'.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Role-oriented</td>
<td style="text-align: center;">In the role of a sentiment analysis tool, respond with 'positive' or 'negative' to classify this statement.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Task-oriented</td>
<td style="text-align: center;">Analyze the tone of this statement and respond with either 'positive' or 'negative'. Here are three examples. <br> Sentence: kids see secretions from the parental units. Answer: negative. Sentence: contains no wit , only labored <br> gags. Answer: negative. Sentence: that loves its characters and communicates something rather beautiful about human <br> nature. Answer: positive.</td>
</tr>
<tr>
<td style="text-align: center;">Few-shot</td>
<td style="text-align: center;">Role-oriented</td>
<td style="text-align: center;">As a sentiment classifier, determine whether the following text is 'positive' or 'negative'. Here are three examples. <br> Sentence: kids see secretions from the parental units. Answer: negative. Sentence: contains no wit , only labored <br> gags. Answer: negative. Sentence: that loves its characters and communicates something rather beautiful about human <br> nature. Answer: positive.</td>
</tr>
</tbody>
</table>
<p>Table 8: Example of adversarial prompts generated by 7 prompt attacks to mimic possible prompts. The characters and words marked with red are generated by prompt attacks.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Scenario</th>
<th style="text-align: left;">Prompt</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Clean</td>
<td style="text-align: left;">As a mathematics instructor, calculate the answer to the following problem related to {}:</td>
</tr>
<tr>
<td style="text-align: left;">TextBugger</td>
<td style="text-align: left;">As a mathematics instructor, calculate the answer to the following problem related to {}:</td>
</tr>
<tr>
<td style="text-align: left;">DeepWordBug</td>
<td style="text-align: left;">As a mathematics instructor, calculate the answer to the following problem related to {}:</td>
</tr>
<tr>
<td style="text-align: left;">TextFeeder</td>
<td style="text-align: left;">As a mathematics prof, calculate the address to the following problem related to {}:</td>
</tr>
<tr>
<td style="text-align: left;">BertAttack</td>
<td style="text-align: left;">As a mathematics instructor, calculate the sum to the following problem related to {}:</td>
</tr>
<tr>
<td style="text-align: left;">CheckList</td>
<td style="text-align: left;">As a mathematics instructor, calculate the answer to the following problem related to RjFJJ2a7RB {}:</td>
</tr>
<tr>
<td style="text-align: left;">StressTest</td>
<td style="text-align: left;">As a mathematics instructor, calculate the answer to the following problem related to and false is not <br> true {}:</td>
</tr>
<tr>
<td style="text-align: left;">Semantic</td>
<td style="text-align: left;">Compute the result of {}.</td>
</tr>
</tbody>
</table>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Results of human study on semantic preserving of the adversarial prompts. The dotted red line ( $&gt;85 \%$ ) is the average of all volunteers on all attacks.</p>
<h1>A. 2 Semantic preservation of adversarial prompts</h1>
<p>In our endeavor to validate the efficacy of our adversarial prompt generation, we engaged in a human-centric study. We enlisted the expertise of 5 independent evaluators with proficiency in the domain to critically assess the semantic congruence between the original and the generated adversarial prompts. For the study, we randomly sampled a set of 100 adversarial prompts along with their respective original prompts. The evaluators were tasked with determining if each adversarial prompt was semantically equivalent to its original counterpart, ensuring the absence of semantic drift. Such an evaluation provides insights into the reliability and robustness of our adversarial prompts.</p>
<p>To address the challenges associated with word-level attacks, we have diligently finetuned the hyperparameters of each attack approach, thus striving to maintain semantic continuity. Then, we conduct a human study to recruit five volunteers to judge if the generated adversarial prompts can preserve semantics. The evaluators were presented with the original prompt $P$ juxtaposed with its adversarial version $\tilde{P}$, and were tasked with determining their semantic congruence. Sentence-level attacks are excluded in this study since they do not change the original prompts, but only to add extra perturbations in the end. The results in Figure 2 demonstrate that these adversarial prompts generated by character-level, word-level and semantic-level attacks are at least $85 \%$ acceptable by humans, indicating that our attack is realistic and meaningful.</p>
<p>Here, we present a few exemplars of both acceptable and unacceptable adversarial prompts as discerned by the evaluators:</p>
<ul>
<li>SST-2 Original prompt: Read the provided excerpt and choose between 'positive' and 'negative' to describe its sentiment:
Attacked prompt: read the standard excerpt and pause between 'positive' and 'negative' to describe its thoughtful:
Unacceptable.</li>
<li>CoLA Original prompt: Functioning as a grammar evaluation tool, analyze the given sentence and decide if it is grammatically correct, responding with 'acceptable' or 'unacceptable':</li>
</ul>
<p>Attacked prompt: Functioning as a grammar evaluation Lool, analyze the Tgiven sentence and deciRe if it is grammXatically correct, respoOnding with 'acceptable' or 'unacceptable':
Acceptable.</p>
<ul>
<li>MMLU Original prompt: As a knowledgeable individual in , provide your response to the multiple-choice question by choosing 'A', 'B', 'C', or 'D'.
Attacked prompt: As a knowledgeable everybody in , prescribes your rebuttal to the multiple-choice item by chooses 'A', 'B', 'C', or 'D'.
Unacceptable.</li>
<li>Math Original prompt: Resolve the following mathematical question associated with:
Attacked prompt: Resolve the following mathematical answer along with : Acceptable.</li>
</ul>
<h1>A. 3 Prompt attack</h1>
<p>The majority of our prompt attacks have been developed by adapting and revising strategies from TextAttack ${ }^{4}$ (Morris et al., 2020). For the detailed settings of each attack, please refer to our code.
Character Level: Techniques such as TextBugger and DeepWordBug manipulate text at the character level by introducing typos or errors within words through insertions, deletions, replacements, and replications. These methods capitalize on the model's vulnerability to minor perturbations in individual characters, frequently resulting in misclassification or erroneous interpretations.
We primarily adopt the settings from TextAttack for TextBugger and DeepWordBug, such as the repeat constraint which prohibits modifying words that have already been altered. Additionally, For TextBugger, TextAttack enforces a constraint on the overall similarity between the sentence encodings of clean and adversarial prompts, utilizing the Universal Sentence Encoder (Cer et al., 2018) to generate text embeddings. In our study, we set this minimum similarity threshold to 0.8 . For DeepWordBug, TextAttack set constraint on edit distance (Levenshtein Distance) as 30.
Word Level: In this study, we employ BertAttack and TextFooler for word-level attacks. These approaches focus on replacing words within the text with synonyms or contextually similar words. By making ostensibly minor alterations to the input text, these attacks can deceive large language models into producing incorrect outputs or substantially modifying their predictions. We meticulously fine-tune the hyperparameters of BertAttack and TextFooler to obtain more appropriate synonyms.
For TextFooler, we set the minimum embedding cosine similarity between word and its synonyms as 0.6 , and the minimum Universal Sentence Encoder similarity is 0.84 . For BertAttack, the minimum Universal Sentence Encoder similarity is 0.8 .
Sentence Level: StressTest and CheckList serve as examples of sentence-level attacks, wherein adversaries attempt to distract the model by adding irrelevant or extraneous sentences to the input text. By incorporating misleading information into the text, these methods can potentially cause the model to lose focus on the primary context, leading to inaccurate results. For the StressTest attack, we adopt similar settings to those in (Wang et al., 2019), appending "and true is true," "and false is not true," or "and true is true" for five times to the end of a prompt. For the CheckList attack, we generate 50 random sequences consisting of alphabets and digits, each with a length of 10 , and append this random sequences into the end of a prompt.
Semantic Level: At the human level, adversaries can construct prompts using various languages, such as Chinese, French, Arabic, Spanish, Japanese, and Korean, subsequently translating these prompts into English. By exploiting the nuances and idiosyncrasies of</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>different languages during translation, it can introduce subtle ambiguities, grammatical errors, or inconsistencies in the input prompt. This poses a formidable challenge for NLP models in generating accurate and coherent responses.
For each language, we first construct 10 prompts based on a English prompt by GPT4 (OpenAI, 2023b), then translate it back to English by Google Translator.</p>
<h1>A. 4 Our attack vs. existing textual adversarial attacks</h1>
<p>Prompt attacks and textual adversarial attacks (Li et al., 2019; Jin et al., 2019; Gao et al., 2018; Li et al., 2020; Ribeiro et al., 2020; Naik et al., 2018; Zang et al., 2020) are both rooted in similar foundational algorithms, but differ in critical ways:</p>
<ul>
<li>Target of attack: Prompt attacks target the instruction (prompts) for LLMs while vanilla adversarial attacks focus on the samples. In numerous tasks, the data might be optional, while prompts remain indispensable. For example, "Write a story about a fox." and "Give me some investigation suggestions." are all prompts with no samples. This makes our prompt attack more general.</li>
<li>Universality of adversarial prompts: An adversarial prompt, represented as $\bar{P}$, works as a common threat for all samples related to a specific task. For example, if $P$ is designed to instruct LLMs to solve math problems, then $\bar{P}$ can be used for many different math problems and datasets. This ability is significantly different from current NLP adversarial benchmarks.</li>
</ul>
<p>In essence, prompt attacks seek to delve into the universality (Moosavi-Dezfooli et al., 2017; Wallace et al., 2019) of adversarial prompts. We argue this offers an innovative lens to assess the robustness of language models, complementing insights from existing benchmarks like AdvGLUE (Wang et al., 2021), and AdvGLUE++(Wang et al., 2023a).</p>
<h2>B Models, Datasets, and Environments</h2>
<h2>B. 1 Models</h2>
<p>Here, we list the brief introduction of each LLM in our experiments. For more details about Vicuna, please refer to its GitHub repository ${ }^{5}$. For the other LLMs, please refer to Huggingface transformer repository (Wolf et al., 2020).</p>
<ul>
<li>Flan-T5-large (Chung et al., 2022): Flan-T5-large is a derivative of the Text-to-Text Transfer Transformer (T5) model, developed by Google.</li>
<li>Dolly-6B (Databricks, 2023): The Dolly-v1-6b model is a 6-billion parameter causal language model developed by Databricks. It originates from EleutherAI's GPT-J (Wang \&amp; Komatsuzaki, 2021) and has been fine-tuned on the Stanford Alpaca (Taori et al., 2023) corpus, which comprises roughly 52 K question/answer pairs.</li>
<li>Vicuna-13B (Chiang et al., 2023): Vicuna-13B, fine-tuned from the LLaMA-13B base model, was developed using approximately 70 K user-shared conversations collected from ShareGPT.com via public APIs.</li>
<li>Cerebras-13B (Dey et al., 2023): Cerebras-13B is based on the GPT-3 style architecture. All models in the Cerebras-GPT series have been trained according to Chinchilla scaling laws (Hoffmann et al., 2022), which optimize compute efficiency by maintaining a ratio of 20 tokens per model parameter.</li>
<li>Llama2-13B (Touvron et al., 2023a): The Llama2 model, developed by the FAIR team at Meta AI, is an autoregressive language model that employs the transformer architecture.</li>
<li>GPT-NEOX-20B (Black et al., 2022): GPT-NEOX-20B is a large-scale implementation of GPT-based models, with NEOX-20B specifically referring to a variant of this series comprising 20 billion parameters.</li>
</ul>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<ul>
<li>Flan-UL2 (Brain, 2023): Flan-UL2 is an encoder decoder model based on the T5 architecture. It uses the same configuration as the UL2 model. It was fine-tuned using the "Flan" prompt tuning and dataset collection.</li>
<li>ChatGPT (OpenAI, 2023a) and GPT-4 (OpenAI, 2023b): Developed by OpenAI, ChatGPT is a large language model trained to generate human-like text based on the prompt it's given. It uses the GPT-3 architecture and has been fine-tuned for more interactive and conversational tasks. GPT-4 is by far the best-performing LLMs.</li>
</ul>
<h1>B. 2 Tasks and datasets</h1>
<p>We adopt the following public datasets for evaluation and PromptRobust can easily take as inputs other datasets.</p>
<ul>
<li>GLUE The GLUE dataset (General Language Understanding Evaluation) (Wang et al., 2019) is a collection of resources designed to assess and benchmark the performance of natural language processing (NLP) models across various language understanding tasks. In this study, we selected 8 tasks, including Sentiment Analysis (SST-2 (Socher et al., 2013)), Grammar Correctness (CoLA (Warstadt et al., 2018)), Duplicate Sentence Detection (QQP (Wang et al., 2017), MRPC (Dolan \&amp; Brockett, 2005)), and Natural Language Inference (MNLI (Williams et al., 2018), QNLI (Wang et al., 2019), RTE (Wang et al., 2019), and WNLI (Levesque et al., 2012)).</li>
<li>MMLU (Hendrycks et al., 2021) To evaluate the extensive world knowledge and problemsolving abilities of large language models, the MMLU dataset encompasses 57 tasks consisting of multiple-choice questions from diverse domains, such as mathematics, history, computer science, law, and more. This dataset serves as a massive multitask test.</li>
<li>SQuAD V2 (Rajpurkar et al., 2018) SQuAD v2 is a widely used dataset for training and evaluating natural language processing models in the domain of machine reading comprehension. SQuAD v2 enhances the original SQuAD dataset (SQuAD v1) by introducing unanswerable questions, increasing the challenge for models. For each question, the model must either (1) identify the correct answer span within the passage (if the question is answerable) or (2) predict that the question is unanswerable (if there is no answer span within the passage).</li>
<li>UN Multi (Eisele \&amp; Chen, 2010) The Multi UN dataset is a large parallel corpus of text gathered from official United Nations documents. It comprises texts in six official languages of the United Nations: Arabic, Chinese, English, French, Russian, and Spanish. The Multi UN dataset primarily contains formal texts, which may limit its applicability to more informal language domains or conversational applications.</li>
<li>IWSLT 2017 (Cettolo et al., 2017) The IWSLT 2017 dataset (International Workshop on Spoken Language Translation 2017) is a collection of multilingual, multi-domain parallel text data specifically designed for evaluating spoken language translation systems. The translation tasks include data from the TED Talks Open Translation Project, featuring parallel text data for multiple language pairs such as English-German, English-French, English-Chinese, and English-Czech. The dataset consists of both spoken language transcriptions and their corresponding translations.</li>
<li>Math (Saxton et al., 2019) DeepMind Mathematics Dataset is a collection of math problems aimed at evaluating the mathematical reasoning abilities of artificial intelligence models. The dataset challenges AI models to solve a diverse range of mathematical problems, spanning from algebra to calculus, and tests their ability to comprehend and reason via complex mathematical concepts.</li>
</ul>
<h2>B. 3 Environments</h2>
<p>To reproduce the computational environment used in this study, an environment file, environment.yml, is provided in our repository. This YAML file lists all the dependencies and their specific versions used in the study. Users can create an identical Conda environment using the command conda env create -f environment.yml. The computational</p>
<p>experiments were conducted on machines equipped with NVIDIA Tesla V100 GPUs (16GB of GPU memory each).</p>
<h1>C Details on Experiments and Results</h1>
<h2>C. 1 Details on dataset sampling</h2>
<p>Note that we cannot run evaluations on all samples due to significantly extensive computing requirements. Instead, we turn to sampling. Specifically, for the GLUE datasets, we sample 1,000 instances when the validation set exceeds this size; otherwise, we utilize the entire validation set. With respect to ChatGPT and GPT4, we adopt a smaller sample size of 200 instances for computational efficiency. For the MMLU dataset, we select 10 instances for each of the 57 tasks if the validation set exceeds this size; if not, the entire validation set is used. For the SQUAD V2 dataset, we randomly select 200 validation instances. Regarding the translation datasets UN Multi and IWSLT 2017, we focus on three languages-English, French, and German, which are primarily supported by T5-large and UL2. We select a total of 100 validation instances, evenly distributed among all possible translation pairs, e.g., English to French. For the Math dataset, we select 20 types of math problems, choosing either 5 or 10 instances per type, resulting in a total of 160 instances. This sampling strategy ensures the formation of a manageable and representative evaluation set for each dataset, thereby enabling an effective assessment of the performance and robustness of LLMs across various tasks and domains.</p>
<h2>C. 2 Results of clean prompts on all LLMs</h2>
<p>Table 9 showcases the performance of different models across various datasets when using clean prompts. Certain LLMs, including Dolly, Cerebras, and NEXO, encounter difficulties with some datasets. For instance, Dolly's accuracy for the QQP dataset is merely $0.53 \%$, a stark contrast to T5's accuracy of $86.67 \%$. Consequently, we focus our attack study on models that demonstrate superior performance, namely T5, Vicuna, Llama2, UL2, ChatGPT, and GPT4.</p>
<p>Table 9: The Average performance and standard deviations of different models on different datasets.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">T5</th>
<th style="text-align: center;">Dolly</th>
<th style="text-align: center;">Vicuna</th>
<th style="text-align: center;">Cerebras</th>
<th style="text-align: center;">Llama2</th>
<th style="text-align: center;">NEOX</th>
<th style="text-align: center;">UL2</th>
<th style="text-align: center;">ChatGPT</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">SST-2</td>
<td style="text-align: center;">94.79 $\pm 0.49$</td>
<td style="text-align: center;">47.80 $\pm 0.30$</td>
<td style="text-align: center;">21.12 $\pm 0.40$</td>
<td style="text-align: center;">21.33 $\pm 0.02$</td>
<td style="text-align: center;">90.25 $\pm 2.21$</td>
<td style="text-align: center;">21.49 $\pm 13.35$</td>
<td style="text-align: center;">95.92 $\pm 1.05$</td>
<td style="text-align: center;">92.91 $\pm 3.32$</td>
</tr>
<tr>
<td style="text-align: center;">CoLA</td>
<td style="text-align: center;">76.11 $\pm 1.28$</td>
<td style="text-align: center;">4.92 $\pm 0.04$</td>
<td style="text-align: center;">35.28 $\pm 10.12$</td>
<td style="text-align: center;">18.18 $\pm 10.82$</td>
<td style="text-align: center;">74.53 $\pm 1.87$</td>
<td style="text-align: center;">7.96 $\pm 14.25$</td>
<td style="text-align: center;">86.07 $\pm 0.36$</td>
<td style="text-align: center;">78.91 $\pm 1.75$</td>
</tr>
<tr>
<td style="text-align: center;">QQP</td>
<td style="text-align: center;">86.67 $\pm 1.05$</td>
<td style="text-align: center;">0.53 $\pm 1.66$</td>
<td style="text-align: center;">24.74 $\pm 10.03$</td>
<td style="text-align: center;">0.00 $\pm 0.00$</td>
<td style="text-align: center;">23.23 $\pm 6.97$</td>
<td style="text-align: center;">0.00 $\pm 0.02$</td>
<td style="text-align: center;">88.25 $\pm 0.54$</td>
<td style="text-align: center;">81.49 $\pm 1.47$</td>
</tr>
<tr>
<td style="text-align: center;">MRPC</td>
<td style="text-align: center;">80.75 $\pm 1.73$</td>
<td style="text-align: center;">0.17 $\pm 0.30$</td>
<td style="text-align: center;">50.15 $\pm 19.65$</td>
<td style="text-align: center;">0.01 $\pm 0.01$</td>
<td style="text-align: center;">49.15 $\pm 4.36$</td>
<td style="text-align: center;">0.01 $\pm 0.05$</td>
<td style="text-align: center;">86.03 $\pm 1.41$</td>
<td style="text-align: center;">72.71 $\pm 2.82$</td>
</tr>
<tr>
<td style="text-align: center;">MNLI</td>
<td style="text-align: center;">81.39 $\pm 4.7$</td>
<td style="text-align: center;">0.78 $\pm 0.88$</td>
<td style="text-align: center;">12.90 $\pm 8.21$</td>
<td style="text-align: center;">0.87 $\pm 1.16$</td>
<td style="text-align: center;">57.30 $\pm 1.51$</td>
<td style="text-align: center;">0.00 $\pm 0.00$</td>
<td style="text-align: center;">83.50 $\pm 4.79$</td>
<td style="text-align: center;">76.71 $\pm 2.44$</td>
</tr>
<tr>
<td style="text-align: center;">QNLI</td>
<td style="text-align: center;">85.12 $\pm 5.57$</td>
<td style="text-align: center;">0.05 $\pm 0.07$</td>
<td style="text-align: center;">27.78 $\pm 10.04$</td>
<td style="text-align: center;">0.00 $\pm 0.00$</td>
<td style="text-align: center;">14.90 $\pm 8.48$</td>
<td style="text-align: center;">4.22 $\pm 3.46$</td>
<td style="text-align: center;">93.68 $\pm 0.41$</td>
<td style="text-align: center;">77.53 $\pm 7.49$</td>
</tr>
<tr>
<td style="text-align: center;">RTE</td>
<td style="text-align: center;">84.24 $\pm 1.34$</td>
<td style="text-align: center;">0.19 $\pm 0.77$</td>
<td style="text-align: center;">29.51 $\pm 15.12$</td>
<td style="text-align: center;">0.00 $\pm 0.00$</td>
<td style="text-align: center;">47.67 $\pm 1.92$</td>
<td style="text-align: center;">3.16 $\pm 4.40$</td>
<td style="text-align: center;">93.26 $\pm 0.51$</td>
<td style="text-align: center;">80.73 $\pm 3.24$</td>
</tr>
<tr>
<td style="text-align: center;">WNLI</td>
<td style="text-align: center;">62.34 $\pm 3.51$</td>
<td style="text-align: center;">0.00 $\pm 0.00$</td>
<td style="text-align: center;">22.57 $\pm 15.96$</td>
<td style="text-align: center;">0.00 $\pm 0.00$</td>
<td style="text-align: center;">41.08 $\pm 1.71$</td>
<td style="text-align: center;">3.62 $\pm 5.10$</td>
<td style="text-align: center;">77.53 $\pm 1.4$</td>
<td style="text-align: center;">61.07 $\pm 6.22$</td>
</tr>
<tr>
<td style="text-align: center;">MMLU</td>
<td style="text-align: center;">45.25 $\pm 0.83$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">15.31 $\pm 7.41$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">36.05 $\pm 7.50$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">53.04 $\pm 0.67$</td>
<td style="text-align: center;">63.33 $\pm 2.56$</td>
</tr>
<tr>
<td style="text-align: center;">SQuAD V2</td>
<td style="text-align: center;">87.32 $\pm 0.43$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">89.78 $\pm 0.71$</td>
<td style="text-align: center;">68.35 $\pm 4.36$</td>
</tr>
<tr>
<td style="text-align: center;">IWSLT</td>
<td style="text-align: center;">0.18 $\pm 0.04$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.21 $\pm 0.04$</td>
<td style="text-align: center;">0.23 $\pm 0.01$</td>
</tr>
<tr>
<td style="text-align: center;">UN Multi</td>
<td style="text-align: center;">0.29 $\pm 0.02$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.33 $\pm 0.02$</td>
<td style="text-align: center;">0.34 $\pm 0.01$</td>
</tr>
<tr>
<td style="text-align: center;">Math</td>
<td style="text-align: center;">14.22 $\pm 3.25$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">14.81 $\pm 1.35$</td>
<td style="text-align: center;">13.14 $\pm 8.49$</td>
</tr>
</tbody>
</table>
<h2>C. 3 Analysis on model size and fine-tuning</h2>
<p>As shown in Table 10 and 3, there seems to be no clear correlation between model robustness and size, for example, despite being the smallest, T5-large demonstrates robustness on par with larger models such as ChatGPT on our evaluated datasets.</p>
<p>The observed differences in model robustness might stem from two aspects: 1) the specific fine-tuning techniques employed. For example, both UL2 and T5-large, fine-tuned on large datasets, and ChatGPT, fine-tuned via RLHF (Christiano et al., 2017), exhibit better robustness than Vicuna. These findings encourage further investigation of fine-tuning strategies to enhance robustness. 2) the memorization of training data. Recent studies suggest that the remarkable performance of some LLMs might be rooted in their ability to memorize training data (Bender et al., 2021; Magar \&amp; Schwartz, 2022; Carlini et al.,</p>
<p>Table 10: The APDR and standard deviations of different attacks on different models.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Character-level</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Word-level</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Sentence-level</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Semantic-level</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">TextBugger</td>
<td style="text-align: center;">DeepWordBug</td>
<td style="text-align: center;">TextFooler</td>
<td style="text-align: center;">BertAttack</td>
<td style="text-align: center;">CheckList</td>
<td style="text-align: center;">StressTest</td>
<td style="text-align: center;">Semantic</td>
</tr>
<tr>
<td style="text-align: left;">T5-large</td>
<td style="text-align: center;">$0.09 \pm 0.10$</td>
<td style="text-align: center;">$0.13 \pm 0.18$</td>
<td style="text-align: center;">$0.20 \pm 0.24$</td>
<td style="text-align: center;">$0.21 \pm 0.24$</td>
<td style="text-align: center;">$0.04 \pm 0.08$</td>
<td style="text-align: center;">$0.18 \pm 0.24$</td>
<td style="text-align: center;">$0.10 \pm 0.09$</td>
</tr>
<tr>
<td style="text-align: left;">Vicuna</td>
<td style="text-align: center;">$0.81 \pm 0.25$</td>
<td style="text-align: center;">$0.69 \pm 0.30$</td>
<td style="text-align: center;">$0.80 \pm 0.26$</td>
<td style="text-align: center;">$0.84 \pm 0.25$</td>
<td style="text-align: center;">$0.64 \pm 0.27$</td>
<td style="text-align: center;">$0.29 \pm 0.40$</td>
<td style="text-align: center;">$0.74 \pm 0.25$</td>
</tr>
<tr>
<td style="text-align: left;">Llama2</td>
<td style="text-align: center;">$0.67 \pm 0.36$</td>
<td style="text-align: center;">$0.41 \pm 0.34$</td>
<td style="text-align: center;">$0.68 \pm 0.36$</td>
<td style="text-align: center;">$0.74 \pm 0.35$</td>
<td style="text-align: center;">$0.34 \pm 0.33$</td>
<td style="text-align: center;">$0.20 \pm 0.30$</td>
<td style="text-align: center;">$0.66 \pm 0.35$</td>
</tr>
<tr>
<td style="text-align: left;">UL2</td>
<td style="text-align: center;">$0.04 \pm 0.06$</td>
<td style="text-align: center;">$0.03 \pm 0.04$</td>
<td style="text-align: center;">$0.14 \pm 0.20$</td>
<td style="text-align: center;">$0.16 \pm 0.22$</td>
<td style="text-align: center;">$0.04 \pm 0.07$</td>
<td style="text-align: center;">$0.06 \pm 0.08$</td>
<td style="text-align: center;">$0.06 \pm 0.08$</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: center;">$0.14 \pm 0.20$</td>
<td style="text-align: center;">$0.08 \pm 0.15$</td>
<td style="text-align: center;">$0.32 \pm 0.35$</td>
<td style="text-align: center;">$0.34 \pm 0.34$</td>
<td style="text-align: center;">$0.07 \pm 0.13$</td>
<td style="text-align: center;">$0.06 \pm 0.12$</td>
<td style="text-align: center;">$0.26 \pm 0.22$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: center;">$0.03 \pm 0.10$</td>
<td style="text-align: center;">$0.02 \pm 0.08$</td>
<td style="text-align: center;">$0.18 \pm 0.19$</td>
<td style="text-align: center;">$0.27 \pm 0.40$</td>
<td style="text-align: center;">$-0.02 \pm 0.09$</td>
<td style="text-align: center;">$0.03 \pm 0.15$</td>
<td style="text-align: center;">$0.03 \pm 0.16$</td>
</tr>
<tr>
<td style="text-align: left;">Avg</td>
<td style="text-align: center;">$0.21 \pm 0.30$</td>
<td style="text-align: center;">$0.17 \pm 0.26$</td>
<td style="text-align: center;">$0.31 \pm 0.33$</td>
<td style="text-align: center;">$0.33 \pm 0.34$</td>
<td style="text-align: center;">$0.12 \pm 0.23$</td>
<td style="text-align: center;">$0.11 \pm 0.23$</td>
<td style="text-align: center;">$0.22 \pm 0.26$</td>
</tr>
</tbody>
</table>
<p>2023; Biderman et al., 2023), rather than in generalization. Hence, even when confronted with adversarial prompts, models might leverage this memorization to produce accurate responses.</p>
<p>In this section, we conduct experiments to analyze the effects of different model sizes and fine-tuning on adversarial prompts. Particularly, we leverage the open-source Llama2 (Touvron et al., 2023b) series due to their support on different sizes and their corresponding fine-tuned versions. The chat versions of Llama2 (Llama2-chat) are fine-tuned on human instructions datasets to better follow the instructions and support multi-turn conversations, while the original version can only be used for inference.
Robustness of different model sizes We analyzed three models from the open-source Llama2 series (Touvron et al., 2023b): Llama2-7B-chat, Llama2-13B-chat, and Llama2-70Bchat. These were chosen due to their distinct sizes, further, their fine-tuning datasets and methods are the same. Our results, depicted in Figure 3(a), reveal that, in a non-adversarial setting, larger models like the 70B model typically surpass the performance of their smaller counterparts. Yet, when subjected to adversarial attacks, the performance dynamics change: at times, smaller models outshine the larger ones. The reasons for these abnormal behaviors could trigger interests for future research.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: (a) Accuracy of Llama2 models (7B-chat, 13B-chat, 70B-chat) on SST2 and CoLA datasets. (b) Accuracy of Llama2 models with fine-tuning and w/o fine-tuning (vanilla) on SST-2 dataset.</p>
<p>Robustness of fine-tuning To delve into the intricacies of fine-tuning, we compared the performances of Llama2-7B and Llama2-7B-chat on the SST2 and COLA tasks. Our analysis, as visualized in Figure 3(b), underscores a consistent trend: models fine-tuned using human-instruction datasets fare better against adversarial onslaughts than models that are not fine-tuned. This observation implies that fine-tuning could be further utilized as the countermeasures for adversarial inputs.</p>
<p>Table 11: Accuracy (\%) of GPT-4 on clean and adversarial prompts and samples on the AdvGLUE Wang et al. (2021) dataset, i.e., attacking both prompts and samples.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Attack</th>
<th style="text-align: center;">SST-2</th>
<th style="text-align: center;">QQP</th>
<th style="text-align: center;">MNLI</th>
<th style="text-align: center;">QNLI</th>
<th style="text-align: center;">RTE</th>
<th style="text-align: center;">AVG</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Clean prompts <br> \&amp; clean samples</td>
<td style="text-align: center;">96.10</td>
<td style="text-align: center;">78.23</td>
<td style="text-align: center;">81.05</td>
<td style="text-align: center;">64.50</td>
<td style="text-align: center;">87.54</td>
<td style="text-align: center;">81.48</td>
</tr>
<tr>
<td style="text-align: left;">Clean prompts <br> \&amp; AdvGLUE</td>
<td style="text-align: center;">63.51</td>
<td style="text-align: center;">70.51</td>
<td style="text-align: center;">63.64</td>
<td style="text-align: center;">62.84</td>
<td style="text-align: center;">74.07</td>
<td style="text-align: center;">66.91</td>
</tr>
<tr>
<td style="text-align: left;">TextBugger</td>
<td style="text-align: center;">58.78</td>
<td style="text-align: center;">44.87</td>
<td style="text-align: center;">47.93</td>
<td style="text-align: center;">60.81</td>
<td style="text-align: center;">76.54</td>
<td style="text-align: center;">57.79</td>
</tr>
<tr>
<td style="text-align: left;">DeepWordBug</td>
<td style="text-align: center;">66.22</td>
<td style="text-align: center;">61.54</td>
<td style="text-align: center;">59.50</td>
<td style="text-align: center;">61.49</td>
<td style="text-align: center;">72.84</td>
<td style="text-align: center;">64.32</td>
</tr>
<tr>
<td style="text-align: left;">TextFooler</td>
<td style="text-align: center;">2.03</td>
<td style="text-align: center;">1.28</td>
<td style="text-align: center;">46.28</td>
<td style="text-align: center;">4.05</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">10.73</td>
</tr>
<tr>
<td style="text-align: left;">BertAttack</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">27.27</td>
<td style="text-align: center;">24.32</td>
<td style="text-align: center;">71.60</td>
<td style="text-align: center;">24.64</td>
</tr>
<tr>
<td style="text-align: left;">Checklist</td>
<td style="text-align: center;">69.59</td>
<td style="text-align: center;">66.67</td>
<td style="text-align: center;">57.85</td>
<td style="text-align: center;">56.08</td>
<td style="text-align: center;">72.84</td>
<td style="text-align: center;">64.61</td>
</tr>
<tr>
<td style="text-align: left;">StressTest</td>
<td style="text-align: center;">50.68</td>
<td style="text-align: center;">56.41</td>
<td style="text-align: center;">59.50</td>
<td style="text-align: center;">59.46</td>
<td style="text-align: center;">76.54</td>
<td style="text-align: center;">60.52</td>
</tr>
<tr>
<td style="text-align: left;">Semantic</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">38.46</td>
<td style="text-align: center;">48.76</td>
<td style="text-align: center;">65.54</td>
<td style="text-align: center;">41.98</td>
<td style="text-align: center;">38.95</td>
</tr>
</tbody>
</table>
<h1>C. 4 Results excluding non-semantic preserving adversarial prompts</h1>
<p>Table 12 presents the attack results after excluding adversarial prompts that do not preserve semantics. It can be observed that the APDR is still considerably high for each dataset.</p>
<h2>C. 5 Attacking both prompts and samples</h2>
<p>The primary focus of this work is to evaluate the robustness of prompts rather than input samples since the samples can be omitted in certain situations, as discussed in Sec. 2.2. In this section, we explore the possibility of attacking both prompts and samples, i.e., evaluating the performance of LLMs in adversarial prompts and samples. Note that since the generation of adversarial examples is expensive and time-consuming, we leverage an existing adversarial dataset called AdvGLUE (Wang et al., 2021), which contains adversarial examples from GLUE (Wang et al., 2019) and it consists of five same tasks as GLUE: SST-2, QQP, MNLI, QNLI, and RTE. Then, we leverage the adversarial prompts and the AdvGLUE dataset (Wang et al., 2021) to evaluate the performance when attacking both prompts and samples.
Table ?? shows the accuracy results using both clean and adversarial prompts on AdvGLUE and clean dataset, respectively. The results demonstrate that on average, all attacking approaches are effective since the accuracy is dramatically declined in face of adversarial prompts. Similar to Table 2, word-level attacks (TextFooler and BertAttack) are the most effective with more than $49 \%$ of accuracy drop. Moreover, surprising results emerge for Checklist attack since the performance can sometimes be improved (e.g., $69.59 \%$ on SST-2 vs. the original $63.51 \%$ ). This is also consistent with our previous observation in Sec. D.3. The results in this section show that attacking both the prompts and samples can further reduce the performance of LLMs. However, certain attacks can even enhance the performance, which is left for future research.</p>
<h2>D Effectiveness Analysis of LLM Response and Attention Visualization</h2>
<h2>D. 1 Erroneous response analysis</h2>
<p>We first analyze the erroneous response analysis produced by adversarial prompts. The results suggest that adversarial prompts impact LLMs' performance by inducing misclassification errors and hindering their ability to generate meaningful responses.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ https://github.com/lm-sys/FastChat&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7719 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7719</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7719</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-144.html">extraction-schema-144</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <p><strong>Paper ID:</strong> paper-276961662</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.09894v2.pdf" target="_blank">What's In Your Field? Mapping Scientific Research with Knowledge Graphs and Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> The scientific literature's exponential growth makes it increasingly challenging to navigate and synthesize knowledge across disciplines. Large language models (LLMs) are powerful tools for understanding scientific text, but they fail to capture detailed relationships across large bodies of work. Unstructured approaches, like retrieval augmented generation, can sift through such corpora to recall relevant facts; however, when millions of facts influence the answer, unstructured approaches become cost prohibitive. Structured representations offer a natural complement -- enabling systematic analysis across the whole corpus. Recent work enhances LLMs with unstructured or semistructured representations of scientific concepts; to complement this, we try extracting structured representations using LLMs. By combining LLMs' semantic understanding with a schema of scientific concepts, we prototype a system that answers precise questions about the literature as a whole. Our schema applies across scientific fields and we extract concepts from it using only 20 manually annotated abstracts. To demonstrate the system, we extract concepts from 30,000 papers on arXiv spanning astrophysics, fluid dynamics, and evolutionary biology. The resulting database highlights emerging trends and, by visualizing the knowledge graph, offers new ways to explore the ever-growing landscape of scientific knowledge. Demo: abby101/surveyor-0 on HF Spaces. Code: https://github.com/chiral-carbon/kg-for-science.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7719.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7719.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Surveyor</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Schema-based LLM concept extraction pipeline (Surveyor)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A domain-agnostic pipeline that uses an LLM to extract categorized scientific concepts (models, tasks, datasets, fields, modalities, methods, objects, properties, instruments) from paper text and stores them as structured records for SQL queries and knowledge-graph visualizations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>What's In Your Field? Mapping Scientific Research with Knowledge Graphs and LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Abhipsha Das; Nicholas Lourie; Siavash Golkar; Mariel Pettee</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2025</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Schema-based LLM extraction pipeline (Surveyor)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>The method prompts an LLM (Llama-3 70B Instruct) with a coarse, cross-domain schema and few-shot annotated examples to tag sentences in titles/abstracts with concept categories; outputs are stored in a SQL database and visualized as co-occurrence knowledge graphs for corpus-level analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>titles and abstracts (sentence-level processing)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>structured schema-tagged concept records and co-occurrence knowledge graphs</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>few-shot prompting with iterative prompt engineering (varying examples, prompt order, input granularity, output format), occasionally JSON output</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3 70B Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>≈30,000 arXiv titles+abstracts (astrophysics, fluid dynamics, evolutionary biology)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>precision, recall, F1 (exact match) computed on a 20-paper manually annotated development set</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>On the development set: human-readable format precision 44% ±12%, recall 31% ±11%; JSON output gave similar precision and recall ≈40% ±12%; processing ≈2.8–4 s per sentence. Applied extraction to 29,980 papers for downstream analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Moderate extraction precision and recall; concept hallucination/noise; difficulty disambiguating named entities vs generic concepts; coarse schema trades off coverage for precision; current pipeline uses only titles/abstracts (not full text).</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "What's In Your Field? Mapping Scientific Research with Knowledge Graphs and Large Language Models", 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7719.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7719.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciMuse</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciMuse (Interesting Scientific Idea Generation Using Knowledge Graphs and LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system that combines LLMs with keyword-extraction (RAKE) and knowledge-graph techniques to extract concepts and support scientific idea generation/evaluation with human experts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Interesting Scientific Idea Generation Using Knowledge Graphs and LLMs: Evaluations with 100 Research Group Leaders.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Interesting Scientific Idea Generation Using Knowledge Graphs and LLMs: Evaluations with 100 Research Group Leaders.</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Xuemei Gu; Mario Krenn</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2024</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>SciMuse (LLM + RAKE idea-generation)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Combines statistical keyword extraction (RAKE) with LLM-driven semantic processing and knowledge-graph construction to produce concept representations and generate scientific ideas; evaluated with a human expert study.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>scientific text (titles/abstracts/full text not specified in the citing paper)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>keyword/concept lists and knowledge-graph-based idea outputs (human-evaluated ideas)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>LLM-based generation (details not specified in the citing paper); pipeline uses RAKE for initial phrase extraction</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>human expert evaluation (survey of research group leaders) as reported in the paper title</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Reported positive evaluations in human study for idea-generation utility (details not provided in the citing paper).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Relies on RAKE for initial extraction which the citing paper notes can lack semantic depth; possible fragmentation of phrases and noise from unsupervised keyword extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "What's In Your Field? Mapping Scientific Research with Knowledge Graphs and Large Language Models", 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7719.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7719.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG-Astro (Sun et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge Graph in Astronomical Research with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach applying LLMs to extract concepts from astronomical literature and build knowledge graphs, using vector-based semantic clustering to group concepts and quantify interdisciplinary driving forces.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Knowledge Graph in Astronomical Research with Large Language Models: Quantifying Driving Forces in Interdisciplinary Scientific Discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Knowledge Graph in Astronomical Research with Large Language Models: Quantifying Driving Forces in Interdisciplinary Scientific Discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Zechang Sun et al.</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2024</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LLM-driven astronomical KG construction (Sun et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Uses LLMs to extract concepts from astronomy texts and forms knowledge graphs where concepts are grouped via vector-based semantic similarity to analyze interdisciplinary influences.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>scientific texts in astronomy (exact input granularity not specified in the citing paper)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>knowledge graphs of extracted concepts, vector-clustered concept groups</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>LLM-based concept extraction combined with embedding/vector similarity clustering (details not given in the citing paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Paper focuses on constructing KGs and quantifying driving forces; specific quantitative evaluation details are not given in the citing paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Not fully detailed in the citing paper; vector-based grouping depends on embedding quality and may conflate concept types.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "What's In Your Field? Mapping Scientific Research with Knowledge Graphs and Large Language Models", 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7719.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7719.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaperQA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaperQA: Retrieval-Augmented Generative Agent for Scientific Research</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented generation system designed to answer scientific queries by retrieving relevant documents and using a generative model to synthesize responses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PaperQA: Retrieval-Augmented Generative Agent for Scientific Research.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>PaperQA: Retrieval-Augmented Generative Agent for Scientific Research.</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Jakub Lála et al.</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2023</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>PaperQA (RAG-based generative agent)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Combines document retrieval with a generative LLM to produce answers to research questions by conditioning generation on retrieved scientific passages (retrieval-augmented generation).</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>scientific papers/documents (retrieval over corpora)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>unstructured generated answers/summaries to research questions (RAG outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>retrieval-augmented generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Described as a retrieval-augmented generative pipeline for scientific research; specifics not detailed in the citing paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>RAG methods are noted by the citing paper to struggle for systematic, corpus-level quantitative analyses and can be cost-prohibitive when many facts influence answers.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "What's In Your Field? Mapping Scientific Research with Knowledge Graphs and Large Language Models", 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7719.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7719.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bio-LM+KG (Nadkarni et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scientific Language Models for Biomedical Knowledge Base Completion</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Work integrating domain-specific language models with knowledge-graph embeddings to improve knowledge-base completion in biomedicine, showing gains but requiring domain-specific finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Scientific Language Models for Biomedical Knowledge Base Completion: An Empirical Study.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Scientific Language Models for Biomedical Knowledge Base Completion: An Empirical Study.</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Rahul Nadkarni; David Wadden; Iz Beltagy; Noah A. Smith; Hannaneh Hajishirzi; Tom Hope</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2021</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Domain-specific LM + KG embeddings for KB completion</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Integrates domain-adapted language models with knowledge-graph embedding methods to complete and enrich biomedical knowledge bases.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>biomedical corpora/text (not specified in citing paper)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>knowledge-base completions (structured triples/KB entries)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Reported improved performance in knowledge-base completion tasks when using domain-specific LMs combined with KG embeddings, but required domain finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires field-specific finetuning and domain adaptation; not directly domain-agnostic.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "What's In Your Field? Mapping Scientific Research with Knowledge Graphs and Large Language Models", 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7719.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7719.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI Scientist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proposed framework and demonstration toward autonomous systems that generate scientific hypotheses and carry out open-ended discovery workflows using advanced AI components including language models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Chris Lu et al.</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2024</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>AI Scientist (automated discovery pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>A vision and prototype work that leverages generative models (including LLMs) and automation components to propose and iteratively test scientific hypotheses in an open-ended pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>varied scientific data and literature (details not specified in citing paper)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>generated hypotheses and proposed discovery actions (automated scientific proposals)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Positioned as initial progress toward autonomous scientific discovery; details are in the cited preprint (not expanded in the citing paper).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Broad challenges of automation, evaluation of generated hypotheses, and reliability/hallucination of LLM outputs remain open.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "What's In Your Field? Mapping Scientific Research with Knowledge Graphs and Large Language Models", 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7719.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7719.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 Discovery Study</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>The Impact of Large Language Models on Scientific Discovery: a Preliminary Study using GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A preliminary study exploring how GPT-4 can assist scientific discovery tasks and what impacts LLMs may have on research workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The Impact of Large Language Models on Scientific Discovery: a Preliminary Study using GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>The Impact of Large Language Models on Scientific Discovery: a Preliminary Study using GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Microsoft Research AI4Science and Microsoft Azure Quantum</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2023</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>GPT-4 preliminary study for scientific discovery</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Uses GPT-4 to perform and evaluate a variety of tasks related to supporting scientific discovery, assessing capabilities and limitations in research assistance.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>scientific text and tasks (study-dependent)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>analyses, generated hypotheses, task-specific outputs from GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Preliminary insights into strengths and failure modes of GPT-4 when applied to scientific tasks; specifics are in the cited report.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Study highlights hallucination and reliability concerns when using LLMs for scientific inference.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "What's In Your Field? Mapping Scientific Research with Knowledge Graphs and Large Language Models", 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Interesting Scientific Idea Generation Using Knowledge Graphs and LLMs: Evaluations with 100 Research Group Leaders. <em>(Rating: 2)</em></li>
                <li>Knowledge Graph in Astronomical Research with Large Language Models: Quantifying Driving Forces in Interdisciplinary Scientific Discovery. <em>(Rating: 2)</em></li>
                <li>PaperQA: Retrieval-Augmented Generative Agent for Scientific Research. <em>(Rating: 2)</em></li>
                <li>Scientific Language Models for Biomedical Knowledge Base Completion: An Empirical Study. <em>(Rating: 2)</em></li>
                <li>The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery. <em>(Rating: 2)</em></li>
                <li>The Impact of Large Language Models on Scientific Discovery: a Preliminary Study using GPT-4. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7719",
    "paper_id": "paper-276961662",
    "extraction_schema_id": "extraction-schema-144",
    "extracted_data": [
        {
            "name_short": "Surveyor",
            "name_full": "Schema-based LLM concept extraction pipeline (Surveyor)",
            "brief_description": "A domain-agnostic pipeline that uses an LLM to extract categorized scientific concepts (models, tasks, datasets, fields, modalities, methods, objects, properties, instruments) from paper text and stores them as structured records for SQL queries and knowledge-graph visualizations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "What's In Your Field? Mapping Scientific Research with Knowledge Graphs and LLMs",
            "authors": "Abhipsha Das; Nicholas Lourie; Siavash Golkar; Mariel Pettee",
            "year": 2025,
            "method_name": "Schema-based LLM extraction pipeline (Surveyor)",
            "method_description": "The method prompts an LLM (Llama-3 70B Instruct) with a coarse, cross-domain schema and few-shot annotated examples to tag sentences in titles/abstracts with concept categories; outputs are stored in a SQL database and visualized as co-occurrence knowledge graphs for corpus-level analysis.",
            "input_type": "titles and abstracts (sentence-level processing)",
            "output_type": "structured schema-tagged concept records and co-occurrence knowledge graphs",
            "prompting_technique": "few-shot prompting with iterative prompt engineering (varying examples, prompt order, input granularity, output format), occasionally JSON output",
            "model_name": "Llama-3 70B Instruct",
            "model_size": "70B parameters",
            "datasets_used": "≈30,000 arXiv titles+abstracts (astrophysics, fluid dynamics, evolutionary biology)",
            "evaluation_metric": "precision, recall, F1 (exact match) computed on a 20-paper manually annotated development set",
            "reported_results": "On the development set: human-readable format precision 44% ±12%, recall 31% ±11%; JSON output gave similar precision and recall ≈40% ±12%; processing ≈2.8–4 s per sentence. Applied extraction to 29,980 papers for downstream analyses.",
            "limitations": "Moderate extraction precision and recall; concept hallucination/noise; difficulty disambiguating named entities vs generic concepts; coarse schema trades off coverage for precision; current pipeline uses only titles/abstracts (not full text).",
            "counterpoint": true,
            "uuid": "e7719.0",
            "source_info": {
                "paper_title": "What's In Your Field? Mapping Scientific Research with Knowledge Graphs and Large Language Models",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "SciMuse",
            "name_full": "SciMuse (Interesting Scientific Idea Generation Using Knowledge Graphs and LLMs)",
            "brief_description": "A system that combines LLMs with keyword-extraction (RAKE) and knowledge-graph techniques to extract concepts and support scientific idea generation/evaluation with human experts.",
            "citation_title": "Interesting Scientific Idea Generation Using Knowledge Graphs and LLMs: Evaluations with 100 Research Group Leaders.",
            "mention_or_use": "mention",
            "paper_title": "Interesting Scientific Idea Generation Using Knowledge Graphs and LLMs: Evaluations with 100 Research Group Leaders.",
            "authors": "Xuemei Gu; Mario Krenn",
            "year": 2024,
            "method_name": "SciMuse (LLM + RAKE idea-generation)",
            "method_description": "Combines statistical keyword extraction (RAKE) with LLM-driven semantic processing and knowledge-graph construction to produce concept representations and generate scientific ideas; evaluated with a human expert study.",
            "input_type": "scientific text (titles/abstracts/full text not specified in the citing paper)",
            "output_type": "keyword/concept lists and knowledge-graph-based idea outputs (human-evaluated ideas)",
            "prompting_technique": "LLM-based generation (details not specified in the citing paper); pipeline uses RAKE for initial phrase extraction",
            "model_name": null,
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": "human expert evaluation (survey of research group leaders) as reported in the paper title",
            "reported_results": "Reported positive evaluations in human study for idea-generation utility (details not provided in the citing paper).",
            "limitations": "Relies on RAKE for initial extraction which the citing paper notes can lack semantic depth; possible fragmentation of phrases and noise from unsupervised keyword extraction.",
            "counterpoint": null,
            "uuid": "e7719.1",
            "source_info": {
                "paper_title": "What's In Your Field? Mapping Scientific Research with Knowledge Graphs and Large Language Models",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "KG-Astro (Sun et al.)",
            "name_full": "Knowledge Graph in Astronomical Research with Large Language Models",
            "brief_description": "An approach applying LLMs to extract concepts from astronomical literature and build knowledge graphs, using vector-based semantic clustering to group concepts and quantify interdisciplinary driving forces.",
            "citation_title": "Knowledge Graph in Astronomical Research with Large Language Models: Quantifying Driving Forces in Interdisciplinary Scientific Discovery.",
            "mention_or_use": "mention",
            "paper_title": "Knowledge Graph in Astronomical Research with Large Language Models: Quantifying Driving Forces in Interdisciplinary Scientific Discovery.",
            "authors": "Zechang Sun et al.",
            "year": 2024,
            "method_name": "LLM-driven astronomical KG construction (Sun et al.)",
            "method_description": "Uses LLMs to extract concepts from astronomy texts and forms knowledge graphs where concepts are grouped via vector-based semantic similarity to analyze interdisciplinary influences.",
            "input_type": "scientific texts in astronomy (exact input granularity not specified in the citing paper)",
            "output_type": "knowledge graphs of extracted concepts, vector-clustered concept groups",
            "prompting_technique": "LLM-based concept extraction combined with embedding/vector similarity clustering (details not given in the citing paper)",
            "model_name": null,
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": "Paper focuses on constructing KGs and quantifying driving forces; specific quantitative evaluation details are not given in the citing paper.",
            "limitations": "Not fully detailed in the citing paper; vector-based grouping depends on embedding quality and may conflate concept types.",
            "counterpoint": null,
            "uuid": "e7719.2",
            "source_info": {
                "paper_title": "What's In Your Field? Mapping Scientific Research with Knowledge Graphs and Large Language Models",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "PaperQA",
            "name_full": "PaperQA: Retrieval-Augmented Generative Agent for Scientific Research",
            "brief_description": "A retrieval-augmented generation system designed to answer scientific queries by retrieving relevant documents and using a generative model to synthesize responses.",
            "citation_title": "PaperQA: Retrieval-Augmented Generative Agent for Scientific Research.",
            "mention_or_use": "mention",
            "paper_title": "PaperQA: Retrieval-Augmented Generative Agent for Scientific Research.",
            "authors": "Jakub Lála et al.",
            "year": 2023,
            "method_name": "PaperQA (RAG-based generative agent)",
            "method_description": "Combines document retrieval with a generative LLM to produce answers to research questions by conditioning generation on retrieved scientific passages (retrieval-augmented generation).",
            "input_type": "scientific papers/documents (retrieval over corpora)",
            "output_type": "unstructured generated answers/summaries to research questions (RAG outputs)",
            "prompting_technique": "retrieval-augmented generation (RAG)",
            "model_name": null,
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": "Described as a retrieval-augmented generative pipeline for scientific research; specifics not detailed in the citing paper.",
            "limitations": "RAG methods are noted by the citing paper to struggle for systematic, corpus-level quantitative analyses and can be cost-prohibitive when many facts influence answers.",
            "counterpoint": null,
            "uuid": "e7719.3",
            "source_info": {
                "paper_title": "What's In Your Field? Mapping Scientific Research with Knowledge Graphs and Large Language Models",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Bio-LM+KG (Nadkarni et al.)",
            "name_full": "Scientific Language Models for Biomedical Knowledge Base Completion",
            "brief_description": "Work integrating domain-specific language models with knowledge-graph embeddings to improve knowledge-base completion in biomedicine, showing gains but requiring domain-specific finetuning.",
            "citation_title": "Scientific Language Models for Biomedical Knowledge Base Completion: An Empirical Study.",
            "mention_or_use": "mention",
            "paper_title": "Scientific Language Models for Biomedical Knowledge Base Completion: An Empirical Study.",
            "authors": "Rahul Nadkarni; David Wadden; Iz Beltagy; Noah A. Smith; Hannaneh Hajishirzi; Tom Hope",
            "year": 2021,
            "method_name": "Domain-specific LM + KG embeddings for KB completion",
            "method_description": "Integrates domain-adapted language models with knowledge-graph embedding methods to complete and enrich biomedical knowledge bases.",
            "input_type": "biomedical corpora/text (not specified in citing paper)",
            "output_type": "knowledge-base completions (structured triples/KB entries)",
            "prompting_technique": null,
            "model_name": null,
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": "Reported improved performance in knowledge-base completion tasks when using domain-specific LMs combined with KG embeddings, but required domain finetuning.",
            "limitations": "Requires field-specific finetuning and domain adaptation; not directly domain-agnostic.",
            "counterpoint": null,
            "uuid": "e7719.4",
            "source_info": {
                "paper_title": "What's In Your Field? Mapping Scientific Research with Knowledge Graphs and Large Language Models",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "AI Scientist",
            "name_full": "The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery",
            "brief_description": "A proposed framework and demonstration toward autonomous systems that generate scientific hypotheses and carry out open-ended discovery workflows using advanced AI components including language models.",
            "citation_title": "The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery.",
            "mention_or_use": "mention",
            "paper_title": "The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery.",
            "authors": "Chris Lu et al.",
            "year": 2024,
            "method_name": "AI Scientist (automated discovery pipeline)",
            "method_description": "A vision and prototype work that leverages generative models (including LLMs) and automation components to propose and iteratively test scientific hypotheses in an open-ended pipeline.",
            "input_type": "varied scientific data and literature (details not specified in citing paper)",
            "output_type": "generated hypotheses and proposed discovery actions (automated scientific proposals)",
            "prompting_technique": null,
            "model_name": null,
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": "Positioned as initial progress toward autonomous scientific discovery; details are in the cited preprint (not expanded in the citing paper).",
            "limitations": "Broad challenges of automation, evaluation of generated hypotheses, and reliability/hallucination of LLM outputs remain open.",
            "counterpoint": null,
            "uuid": "e7719.5",
            "source_info": {
                "paper_title": "What's In Your Field? Mapping Scientific Research with Knowledge Graphs and Large Language Models",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "GPT-4 Discovery Study",
            "name_full": "The Impact of Large Language Models on Scientific Discovery: a Preliminary Study using GPT-4",
            "brief_description": "A preliminary study exploring how GPT-4 can assist scientific discovery tasks and what impacts LLMs may have on research workflows.",
            "citation_title": "The Impact of Large Language Models on Scientific Discovery: a Preliminary Study using GPT-4.",
            "mention_or_use": "mention",
            "paper_title": "The Impact of Large Language Models on Scientific Discovery: a Preliminary Study using GPT-4.",
            "authors": "Microsoft Research AI4Science and Microsoft Azure Quantum",
            "year": 2023,
            "method_name": "GPT-4 preliminary study for scientific discovery",
            "method_description": "Uses GPT-4 to perform and evaluate a variety of tasks related to supporting scientific discovery, assessing capabilities and limitations in research assistance.",
            "input_type": "scientific text and tasks (study-dependent)",
            "output_type": "analyses, generated hypotheses, task-specific outputs from GPT-4",
            "prompting_technique": null,
            "model_name": "GPT-4",
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": "Preliminary insights into strengths and failure modes of GPT-4 when applied to scientific tasks; specifics are in the cited report.",
            "limitations": "Study highlights hallucination and reliability concerns when using LLMs for scientific inference.",
            "counterpoint": null,
            "uuid": "e7719.6",
            "source_info": {
                "paper_title": "What's In Your Field? Mapping Scientific Research with Knowledge Graphs and Large Language Models",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Interesting Scientific Idea Generation Using Knowledge Graphs and LLMs: Evaluations with 100 Research Group Leaders.",
            "rating": 2,
            "sanitized_title": "interesting_scientific_idea_generation_using_knowledge_graphs_and_llms_evaluations_with_100_research_group_leaders"
        },
        {
            "paper_title": "Knowledge Graph in Astronomical Research with Large Language Models: Quantifying Driving Forces in Interdisciplinary Scientific Discovery.",
            "rating": 2,
            "sanitized_title": "knowledge_graph_in_astronomical_research_with_large_language_models_quantifying_driving_forces_in_interdisciplinary_scientific_discovery"
        },
        {
            "paper_title": "PaperQA: Retrieval-Augmented Generative Agent for Scientific Research.",
            "rating": 2,
            "sanitized_title": "paperqa_retrievalaugmented_generative_agent_for_scientific_research"
        },
        {
            "paper_title": "Scientific Language Models for Biomedical Knowledge Base Completion: An Empirical Study.",
            "rating": 2,
            "sanitized_title": "scientific_language_models_for_biomedical_knowledge_base_completion_an_empirical_study"
        },
        {
            "paper_title": "The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery.",
            "rating": 2,
            "sanitized_title": "the_ai_scientist_towards_fully_automated_openended_scientific_discovery"
        },
        {
            "paper_title": "The Impact of Large Language Models on Scientific Discovery: a Preliminary Study using GPT-4.",
            "rating": 2,
            "sanitized_title": "the_impact_of_large_language_models_on_scientific_discovery_a_preliminary_study_using_gpt4"
        }
    ],
    "cost": 0.01563925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>What's In Your Field? Mapping Scientific Research with Knowledge Graphs and LLMs
29 May 2025</p>
<p>Abhipsha Das abhipsha.das@nyu.edu 
Polymathic AI</p>
<p>Flatiron Institute</p>
<p>Nicholas Lourie 
Polymathic AI</p>
<p>New York University</p>
<p>Siavash Golkar 
Polymathic AI</p>
<p>New York University</p>
<p>Mariel Pettee 
Polymathic AI</p>
<p>Lawrence Berkeley National Laboratory</p>
<p>What's In Your Field? Mapping Scientific Research with Knowledge Graphs and LLMs
29 May 202553D0D06EF33E949974E5F358664A809BarXiv:2503.09894v2[cs.CL]
The scientific literature's exponential growth makes it increasingly challenging to navigate and synthesize knowledge across disciplines.Large language models (LLMs) are powerful tools for understanding scientific text, but they fail to capture detailed relationships across large bodies of work.Unstructured approaches, like retrieval augmented generation, can sift through such corpora to recall relevant facts; however, when millions of facts influence the answer, unstructured approaches become cost prohibitive.Structured representations offer a natural complement-enabling systematic analysis across the whole corpus.Recent work enhances LLMs with unstructured or semistructured representations of scientific concepts; to complement this, we try extracting structured representations using LLMs.By combining LLMs' semantic understanding with a schema of scientific concepts, we prototype a system that answers precise questions about the literature as a whole.Our schema applies across scientific fields and we extract concepts from it using only 20 manually annotated abstracts.To demonstrate the system, we extract concepts from 30,000 papers on arXiv spanning astrophysics, fluid dynamics, and evolutionary biology.The resulting database highlights emerging trends and, by visualizing the knowledge graph, offers new ways to explore the ever-growing landscape of scientific knowledge.Demo: abby101/surveyor-0 on HF Spaces.</p>
<p>Introduction</p>
<p>Consider a researcher seeking to build a multimodal foundation model for astrophysics.They might begin by asking: What are the most important data modalities to support-the most common § Video: YouTube Link ones in the field?How would such a researcher go about answering these questions today?</p>
<p>One might hope that LLMs could easily answer such a question, but while they have created unprecedented opportunities for accelerating scientific discovery, they struggle to aggregate reliable statistics and generate systematic analyses across the breadth of scientific literature.Manually reviewing papers or consulting domain experts are not scalable approaches when there are thousands of papers to investigate.</p>
<p>Most current approaches rely on unstructured methods like retrieval augmented generation (RAG) [9,11,3].While these methods excel at broad information retrieval and synthesis, they make it difficult to analyze specific patterns in research across large bodies of literature.The limitations become particularly evident when researchers try to understand how research in a field evolves.They need to track new instruments, identify research problems that require methodological innovation, and understand how theoretical models get validated across disciplines.Unstructured representations struggle to systematically capture these relationships.</p>
<p>Some efforts explore semistructured representations such as: keyphrase extractions based on statistical patterns [5] and LLM-based concept extraction combined with constructing vector-similaritybased knowledge graphs [15].While valuable, these methods typically treat all concepts uniformly without distinguishing between their functional roles in scientific work, or rely on semantic similarity of concepts using embedding models.This limits the utility of extracted knowledge when a researcher needs a quantitative analysis.</p>
<p>To address this challenge, we introduce a novel approach using LLMs that extracts categorized concepts from scientific papers using a general schema covering key research entities like objects, datasets, methods, and modalities.Our system combines structured knowledge representation with an interactive query interface to enable researchers to analyze methodological patterns, track research evolution, and understand relationships between different aspects of scientific work at scale.We visualize the extracted structured information using knowledge graphs that provide key insights into concept co-occurrence in scientific research.The main contributions of this work are: 1) a generic schema for categorizing scientific concepts across different fields, 2) a scalable LLM-based extraction pipeline to mine concepts from papers, 3) an interactive system for querying scientific information, 4) informative knowledge graphs built from the extracted concepts to represent scientific fields.</p>
<p>Method</p>
<p>Our system consists of three key components: a schema defining different kinds of scientific concepts, a pipeline for prompting LLMs to extract these concepts, and a database to store this structured knowledge for efficient queries and analysis.</p>
<p>Schema</p>
<p>Through an iterative discussion process of example selection, comparing manual annotations between the authors, and examining scientific papers across different domains, we developed an annotation schema capturing nine fundamental categories of scientific concepts: models, tasks, datasets, fields, modalities, methods, objects, properties, and instruments.In designing the schema, we aimed for categories that apply across scientific disciplines.This schema intentionally uses broad category definitions that are immediately understandable to scientists without requiring study of specialized class or type of data/observations with similar or the same structure method:</p>
<p>approach, technique or procedure to complete a task object:</p>
<p>entity that can be studied property:</p>
<p>quantitative or qualitative descriptor, or an inherent attribute of an entity, data, modality or method instrument: device or system used for making measurements Table 1: Definitions for a schema of scientific concepts.</p>
<p>and intricate taxonomies.We opted for coarser categories to avoid the ambiguity and complexity that arises when making subtle distinctions between closely related concepts, even though this means some concepts could be described by multiple tags.</p>
<p>Rather than implementing complex disambiguation tests, our simple tagging schema allows us to maintain scalability; however, there is always a trade-off between coverage and precision.</p>
<p>Original Sentence</p>
<p>We present an analysis of a new Australia Telescope Compact Array (ATCA) radiocontinuum observation of supernova remnant (SNR) G1.9+0.3, which at an age of 181±25 years is the youngest known in the Galaxy.</p>
<p>Tagged Sentence</p>
<p>We present an analysis of a new <dataset> <instrument>Australia Telescope Compact Array (ATCA)</instrument> <modality>radio-continuum</modality> observation</dataset> of <object>supernova remnant (SNR) G1.9+0.3 </object>, which at an <property>age</property> of 181±25 years is the youngest known in the <object>Galaxy</object>.</p>
<p>We implemented our extraction pipeline using the open-source Llama-3 70B Instruct model [4], employing few-shot learning to guide concept extraction.For our prompt optimization experiments, we manually annotated 20 papers, using 3 demonstration papers for few-shot examples and the remaining 17 as a development set to iteratively refine the prompts.Above is an example of the manual annotation process sentence-by-sentence.</p>
<p>The pipeline processes the language content sentence-by-sentence using manually annotated examples to demonstrate the target structure (see Fig. 2).</p>
<p>Pipeline</p>
<p>To optimize extraction reliability, we conducted systematic prompt engineering experiments on the manually annotated set, varying: (i) number and selection of few-shot examples, (ii) structure and ordering of the prompt, (iii) granularity of input text (sentence vs. paragraph), (iv) format of extracted concepts (JSON vs. human-readable).</p>
<p>To guide this iteration, we used a comprehensive set of metrics calculated on our development set, including: precision, recall and F-1 scores, for exact matches.In addition, we also considered the processing time and efficiency of the different approaches.During the annotation process, we found that even simple and broad concepts, such as a modality, encounter ambiguities when applied across scientific fields.As a result, it is likely that no method can achieve complete agreement with the development set annotations.Rather than as an absolute benchmark, we used the development set as a directional signal-a way to see if a given change improved the extraction process.Ultimately, our optimized prompt configuration consisting of instruction, schema and  from each of the 3 demonstration papers) shows promising results.The final results on our development set were: precision of 44% ± 12% and recall of 31% ± 11% in human-readable response format, with processing times averaging around 2.8 seconds per sentence.When using JSON output format, we observed similar precision levels with slightly higher recall rates of 40% ± 12% and processing times of around 4 seconds per sentence.While some noise persisted in the extracted data, the results were sufficient to explore using such structured knowledge from the scientific literature in order to discover relationships and systematically analyze complex statistical questions.</p>
<p>Database</p>
<p>The extracted concepts and their relationships are stored in a SQL database for scientific analysis queries which enables fast computation of aggregate statistics.The SQL database contains 2 tables: papers and predictions (see Fig. 1).The papers table contains metadata, raw text, author and category information about the papers, and the predictions table contains the information about the tagged concepts, the tag type and the papers they come from.Storing both the extracted information and relevant metadata about where the extractions came from facilitates analyses such as the evolution of methodological approaches over time or the adoption patterns of new experimental techniques across fields.</p>
<p>Demo</p>
<p>Visualization</p>
<p>To explore the relationships between scientific concepts, we built a dynamic visualization using force-directed graph layouts.In this representation, nodes represent individual scientific concepts V (e.g., specific methods, instruments, or objects of study), while edges represent co-occurrences within the same paper E in a graph G = (V, E).A physics-based spring layout algorithm determines the spatial arrangement, with frequently cooccurring concepts drawn closer together.</p>
<p>Our visualization system supports interactive exploration through: 1. Tag-type filtering to focus on specific concept categories (e.g., only methods or instruments), 2. Node highlighting to emphasize specific concepts and their immediate connections, 3. Depth-based exploration to reveal n-hop neighborhoods around concepts of interest, 4. Dynamic force-directed layout updates to reflect filtered subgraphs.</p>
<p>This graph-based approach enables both targeted investigation of specific concept relationships and broader analysis of methodological patterns across domains.For example, researchers can identify clusters of related experimental techniques, trace the adoption of methods across different subfields, or visualize isolated clusters of objects to understand how they are studied.</p>
<p>Query Interface</p>
<p>The query interface supports SQL queries for scientific concept exploration, with predefined queries demonstrating use cases like modality distribution analysis and temporal trends.The interface allows researchers to ask increasingly sophisticated questions by leveraging the structured database.For example, a researcher building an astrophysics foundation model could analyze most-used modalities, examine their current coverage, track usage trends over 5 years, and estimate coverage gains from adding new modalities.This approach helps scientists make data-driven research decisions that would be difficult to achieve through other means.</p>
<p>Results</p>
<p>Dataset.We collected the titles and abstracts from 30,000 articles from arXiv comprising 10,000 papers each from astrophysics, fluid dynamics and evolutionary biology, in order test across a breadth of scientific disciplines.Titles and abstracts (i.e.article metadata) were used instead of the full text in order to optimize processing efficiency while maintaining representativeness, since the titles and abstracts of papers are likely to be more information dense than the papers' bodies.After setting aside 20 astrophysics papers for prompt development, we used the optimized prompts refined through this development process to extract concepts from the remaining 9,980 astrophysics papers (from the original 10,000), as well as all 10,000 papers from each of the other two fields (fluid dynamics and evolutionary biology).Our final extractions and subsequent knowledge graph visualizations include results for these 29,980 papers.</p>
<p>Graph-based Exploration</p>
<p>Fig. 3 demonstrates the interconnected nature of scientific concepts through co-occurrence knowledge graphs from our analyzed domains.†</p>
<p>Demonstrating Example Queries</p>
<p>We examine several key questions to demonstrate the interface's capability for both exploratory research and targeted investigation in scientific discovery.</p>
<p>Related Work</p>
<p>Large language models (LLMs) [2,7,13,16,4] have recently demonstrated remarkable capabilities in language tasks, particularly through advances in prompting strategies [2,18].These advances have inspired building LLM-based pipelines to engage with complex scientific discovery tasks [8,1,10].Despite these advancements, the problem of hallucinations in LLMs persists [6].</p>
<p>To overcome this, a popular approach is to augment LLMs with unstructured, external knowledge [11,9], but while RAG excels in retrieving broad context information at scale, it is limited in providing precise information and avenues for systematic analysis, which can be efficiently realized through structured knowledge representations.Some prior work applies semistructured knowledge representations to the scientific literature, such as Gu and Krenn [5]'s SciMuse system which combines LLMs with the RAKE algorithm to extract concepts as keyword phrases.[15] extended this to astronomy, using LLMs to extract concepts from scientific texts and construct knowledge graphs, grouping the concepts with a vectorbased semantic similarity.In biomedicine, [12] integrated domain-specific language models with knowledge graph embeddings, showing improved performance but requiring field-specific finetuning.</p>
<p>Our work differs from these approaches by providing a domain-agnostic framework that combines LLM-powered semantic understanding with queryable structured knowledge curation, and enhanced by graph visualizations.Unlike previous methods, our approach introduces generalizable categorization schemes that enable cross-domain concept extraction, relationship mapping, and question answering.</p>
<p>Discussion</p>
<p>Our tool enables quantitative analysis of research methodologies across scientific domains by organizing concepts into distinct categories like methods, instruments, and data modalities, allowing researchers to systematically investigate patterns that would be difficult to discover through traditional literature review or citation analysis.The combination of SQL queries and graph visualization proves especially valuable for exploring methodological connections.For instance, answering how similar mathematical models get applied across different fields is hard with existing approaches but readily solvable by our system.</p>
<p>While the system shows promise, its current extraction precision leaves room for improvement.At times, the LLM can struggle to distinguish specific named entities (like "Melbourne wind tunnel") from generic concepts (like "wind-tunnel data"), introducing noise into the extracted relationships.Future work could address these limitations through improved prompting strategies, post-training of models by gathering insights from domain experts, and extracting more sophisticated relationships between concepts beyond co-occurrence.</p>
<p>Conclusion</p>
<p>This work demonstrates how combining LLMs with structured knowledge representation can enable systematic analysis of scientific literature.Our four key contributions are: a domain-agnostic schema for categorizing scientific concepts, a scalable LLM-based extraction pipeline, a queryable interactive system, and informative knowledge graphs built from the extracted concepts.The results show that even with modest extraction accuracy, our approach can reveal valuable insights about cross-disciplinary connections and research evolution that would be difficult to discover through traditional literature review, opening up new possibilities for navigating scientific research.support provided by the Simons Foundation and Schmidt Sciences, LLC.</p>
<p>A Comparison of Keyword Extraction Methods</p>
<p>A.1 Methodology</p>
<p>In this appendix we compare two approaches to scientific concept extraction: Rapid Automatic Keyword Extraction (RAKE) [14] and our method.RAKE is an unsupervised statistical method based on word frequency and co-occurrence, while our schema-based approach leverages Llama-3 70B for semantic understanding.We randomly sampled 300 scientific papers (100 each from astrophysics, fluid dynamics, and evolutionary biology) from our dataset and applied RAKE extractions on the title and abstract text, performed using the rake-nltk library [17].The extracted keywords were then compared to those generated by our method.Our approach consistently extracted fewer concepts than RAKE but organized them into semantic categories that reveal their functional roles within the scientific discourse.In table 3, we see that across all domains, object was the predominant concept type, followed by property.</p>
<p>A.2 Quantitative Comparison</p>
<p>A.3 Qualitative Analysis</p>
<p>A.3.1 Astrophysics</p>
<p>A Detailed Analysis of a Magnetic Island Observed by WISPR on Parker Solar Probe</p>
<p>We present the identification and physical analysis of a possible magnetic island feature seen in white-light images observed by the Wide-field Imager for Solar Probe (WISPR) on board the Parker Solar Probe (Parker).The island is imaged by WISPR during Parker's second solar encounter on 2019 April 06, when Parker was 38 solar radii from the Sun center.We report that the average velocity and acceleration of the feature are approximately 334 km s and -0.64 m s-2.The kinematics of the island feature, coupled with its direction of propagation, indicate that the island is likely entrained in the slow solar wind.The island is elliptical in shape with a density deficit in its center, suggesting the presence of a magnetic guide field.We argue that this feature is consistent with the formation of this island via reconnection in the current sheet of the streamer.The feature's aspect ratio (calculated as the ratio of its minor to major axis) evolves from an elliptical to a more circular shape that approximately doubles during its propagation through WISPR's field of view.The island is not distinct in other white-light observations from the Solar and Heliospheric Observatory (SOHO) and the Solar Terrestrial Relations Observatory (STEREO) coronagraphs, suggesting that this is a comparatively faint heliospheric feature and that viewing perspective and WISPR's enhanced sensitivity are key to observing the magnetic island.</p>
<p>RAKE Keywords</p>
<p>[(score, "keyword") * count] (17.67, "possible magnetic island feature seen"), (13.71, "solar terrestrial relations observatory"), (13.33, "comparatively faint heliospheric feature"), (9.00, "2019 april 06"), (8.71, "slow solar wind"), (8.71, "second solar encounter"), (8.71, "38 solar radii"), (8.50, "light images observed"), (8.50, "approximately 334 km"), (8.33, "magnetic guide field"), (8.00, "island via reconnection"), (6.96, "parker solar probe"), (6.00, "heliospheric observatory"), (5.33, "magnetic island"), (5.21, "solar probe"), (4.50, "light observations"), (4.50, "approximately doubles"), (4.33, "island feature"), (4.00, "viewing perspective"), (4.00, "physical analysis"), (4.00, "major axis"), (4.00, "likely entrained"), (4.00, "field imager"), (4.00, "enhanced sensitivity"), (4.00, "density deficit"), (4.00, "current sheet"), (4.00, "average velocity"), (3.75, "parker ()"), (3.50, "sun center"), (3.50, "circular shape"), (3.50, "aspect ratio"), (2.71, "solar"), (2.33, "feature") * 3, (2.00, "island") * 4, (2.00, "field"), (1.75, "parker") * 2, (1.50, "shape"), (1.50, "ratio"), (1.50, "center"), (1.00, "wispr") * 4, (1.00, "wide"), (1.00, "white") * 2, (1.00, "view"), (1.00, "suggesting") * 2, (1.00, "streamer"), (1.00, "stereo"), (1.00, "soho"), (1.00, "report"), (1.00, "propagation") * 2, (1.00, "present"), (1.00, "presence"), (1.00, "observing"), (1.00, "minor"), (1.00, "kinematics"), (1.00, "key"), (1.00, "indicate"), (1.00, "imaged"), (1.00, "identification"), (1.00, "formation"), (1.00, "evolves"), (1.00, "elliptical") * 2, (1.00, "distinct"), (1.00, "direction"), (1.00, "coupled"), (1.00, "coronagraphs"), (1.00, "consistent"), (1.00, "calculated"), (1.00, "board"), (1.00, "argue"), (1.00, "acceleration"), (1.00, "64"), (1.00, "2"), (1.00, "0")</p>
<p>Concepts (by type):</p>
<p>• object: Parker Solar Probe, magnetic island feature, white-light images, island, Parker, Sun, feature, island feature, slow solar wind, streamer, WISPR's field of view, magnetic island, heliospheric feature</p>
<p>• instrument: WISPR, Wide-field Imager for Solar Probe (WISPR), Parker Solar Probe (Parker), Solar and Heliospheric Observatory (SOHO), Solar Terrestrial Relations Observatory (STEREO)</p>
<p>• property: solar radii, velocity, acceleration, density deficit, shape, aspect ratio, faint</p>
<p>The RAKE extraction produces a flat list of keywords with associated scores.In contrast, our approach organizes concepts by semantic role, such as instruments (WISPR, SOHO), objects (Sun, slow solar wind), and their properties (solar radii, acceleration).</p>
<p>A.3.2 Fluid Dynamics</p>
<p>Approximation of sea surface velocity field by fitting surrogate two-dimensional flow to scattered measurements</p>
<p>In this paper, a rapid approximation method is introduced to estimate the sea surface velocity field based on scattered measurements.The method uses a simplified two-dimensional flow model as a surrogate model, which mimics the real submesoscale flow.The proposed approach treats the interpolation of the flow velocities as an optimization problem, aiming to fit the flow model to the scattered measurements.To ensure consistency between the simulated velocity field and the measured values, the boundary conditions in the numerical simulations are adjusted during the optimization process.Additionally, the relevance of quantity and quality of the scattered measurements is assessed, emphasizing the importance of the measurement locations within the domain as well as explaining how these measurements contribute to the accuracy and reliability of the sea surface velocity field approximation.The proposed methodology has been successfully tested in both synthetic and real-world scenarios, leveraging measurements obtained from Global Positioning System (GPS) drifters and high-frequency (HF) radar systems.The adaptability of this approach for different domains, measurement types, and conditions implies that it is suitable for real-world submesoscale scenarios where only an approximation of the sea surface velocity field is sufficient.</p>
<p>RAKE Keywords</p>
<p>[(score, "keyword") * count] (22.83, "sea surface velocity field based"), (20.83, "sea surface velocity field approximation"), (17.83, "sea surface velocity field"), (11.50, "simulated velocity field"), (9.00, "global positioning system"), (8.50, "rapid approximation method"), (8.50, "measurement locations within"), (8.20, "leveraging measurements obtained"), (8.00, "world submesoscale scenarios"), (7.83, "dimensional flow model"), (7.50, "proposed approach treats"), (7.17, "real submesoscale flow"), (5.00, "world scenarios"), (4.83, "flow model"), (4.50, "proposed methodology"), (4.50, "method uses"), (4.50, "measurement types"), (4.50, "flow velocities"), (4.33, "surrogate model"), (4.20, "scattered measurements") * 3, (4.20, "measurements contribute"), (4.00, "successfully tested"), (4.00, "simplified two"), (4.00, "radar systems"), (4.00, "optimization process"), (4.00, "optimization problem"), (4.00, "numerical simulations"), (4.00, "measured values"), (4.00, "ensure consistency"), (4.00, "different domains"), (4.00, "conditions implies"), (4.00, "boundary conditions"), (3.00, "approximation"), (2.00, "approach"), (1.67, "real") * 2, (1.00, "well"), (1.00, "synthetic"), (1.00, "suitable"), (1.00, "sufficient"), (1.00, "reliability"), (1.00, "relevance"), (1.00, "quantity"), (1.00, "quality"), (1.00, "paper"), (1.00, "mimics"), (1.00, "introduced"), (1.00, "interpolation"), (1.00, "importance"), (1.00, "high"), (1.00, "hf"), (1.00, "gps"), (1.00, "frequency"), (1.00, "fit"), (1.00, "explaining"), (1.00, "estimate"), (1.00, "emphasizing"), (1.00, "drifters"), (1.00, "domain"), (1.00, "assessed"), (1.00, "aiming"), (1.00, "adjusted"), (1.00, "additionally"), (1.00, "adaptability"), (1.00, "accuracy") Concepts (by type): Our method's extraction effectively distinguishes between physical objects of study (porous media), models (two-dimensional flow model), methodological approaches (rapid approximation method), and instruments (Global Positioning System (GPS) drifters).</p>
<p>A.3.3 Evolutionary Biology</p>
<p>Complexity-stability relationships in disordered dynamical systems</p>
<p>Robert May famously used random matrix theory to predict that large, complex systems cannot admit stable fixed points.However, this general conclusion is not always supported by empirical observation: from cells to biomes, biological systems are large, complex and, often, stable.In this paper, we revisit May's argument in light of recent developments in both ecology and random matrix theory.Using a non-linear generalization of the competitive Lotka-Volterra model, we show that there are, in fact, two kinds of complexity-stability relationships in disordered dynamical systems: if self-interactions grow faster with density than cross-interactions, complexity is destabilizing; but if cross-interactions grow faster than self-interactions, complexity is stabilizing.</p>
<p>RAKE Keywords</p>
<p>[(score, "keyword") * count] (40.50, "robert may famously used random matrix theory"), (40.00, "complex systems cannot admit stable fixed points"), (15.00, "random matrix theory"), (10.00, "disordered dynamical systems"), (8.00, "interactions grow faster") * 2, (6.50, "revisit may"), (6.00, "biological systems"), (4.00, "volterra model"), (4.00, "two kinds"), (4.00, "stable"), (4.00, "stability relationships"), (4.00, "recent developments"), (4.00, "linear generalization"), (4.00, "general conclusion"), (4.00, "empirical observation"), (4.00, "complex"), (4.00, "competitive lotka"), (4.00, "always supported"), (2.00, "interactions") * 2, (1.00, "using"), (1.00, "stabilizing"), (1.00, "show"), (1.00, "self") * 2, (1.00, "predict"), (1.00, "paper"), (1.00, "often"), (1.00, "non"), (1.00, "light"), (1.00, "large") * 2, (1.00, "however"), (1.00, "fact"), (1.00, "ecology"), (1.00, "destabilizing"), (1.00, "density"), (1.00, "cross") * 2, (1.00, "complexity") * 3, (1.00, "cells"), (1.00, "biomes"), (1.00, "argument")</p>
<p>Concepts (by type):</p>
<p>• object: disordered dynamical systems, systems, cells, biomes, biological systems</p>
<p>• property: complexity-stability relationships, large, complex, stable, complexity-stability, density</p>
<p>• method: random matrix theory</p>
<p>• field: ecology</p>
<p>• model: non-linear generalization of the competitive Lotka-Volterra model Our method's extraction captures biological entities (cells, biomes, systems) and their properties, while also identifying the specific models and methods used.</p>
<p>A.4 Insights From The Comparison</p>
<p>RAKE is efficient, language-independent, and quantitatively ranks keyword importance, but it lacks semantic depth, extracts fragmented phrases, and does not distinguish between concept types.Our method, while computationally more demanding and occasionally prone to concept hallucination, provides structured semantic categorization, generates coherent concepts, and captures domainspecific nuances more effectively.</p>
<p>Both methods introduce some noise, though RAKE produces significantly more.The structured semantic representation in our approach offers a more meaningful and organized summary compared to RAKE's flat keyword list, making it more useful for domain experts.</p>
<p>"Figure 1 :
1
Figure 1: Illustration of the structured concept extraction pipeline: i) the corpus used, ii) running optimized prompt on full corpus, iii) storing model's outputs and corpus metadata in SQL database.</p>
<p>9 few-shot examples (3 sentences with annotated extractions Illustration: Prefix + Prompt The following schema is provided to tag the title and abstract of a given scientific paper as shown in the examples: $SCHEMA Sentence: This magnetic field strength implies a minimum total energy of the synchrotron radiation of E min ≈ 1.8×10 48 ergs.Extractions: property: magnetic field strength, energy object: synchrotron radiation ... (Total 9 few-shot examples) ... Sentence: We present HATNet observations of XO-5b, confirming its planetary nature based on evidence beyond that described in the announcement of Burke et al Ground Truth Tags: dataset: HATNet observations instrument: HATNet object: XO-5b Predicted Tags: dataset: HATNet observations object: XO-5b, planetary nature</p>
<p>Figure 2 :
2
Figure 2: Expanded prompt illustration with schema and few-shot examples, along with the sentence to predict tags for.</p>
<p>(a) Analysis of galaxy images: various galactic entities, measurement objects, clouds, instruments, and properties.(b) COVID-19 cluster analysis: geographic distribution of pandemic research and immune health terms indicating research focus areas.(c) Fluid dynamics clusters centered around "flow," branching into related flow and turbulence-based physical phenomena.(d) Astrophysical objects and phenomena spanning multiple scales, from individual stars and binaries to galactic structures and clusters.</p>
<p>Figure 3 :
3
Figure 3: Co-occurrence graphs: astrophysics (a, d), epidemiology (b), fluid dynamics (c).</p>
<p>Table 2
2
reveals significant differences in the average count of extracted concepts by each method across domains for the subset of 300 papers.
DomainRAKE Ours Ratio(avg)(avg) (avg)Astrophysics69.533.82.05Fluid Dynamics70.332.62.15Evolutionary Biology67.729.72.28Overall69.232.12.16</p>
<p>Table 2 :
2
Average number of extracted concepts by domain
DomainOur method: concept typesAstrophysicsobject (56.1%), property (24.7%),instrument (5.4%), method (3.6%),modality (3.5%), model (3.1%), task(1.5%), field (1.0%), dataset (0.6%)Fluid Dynamicsobject (44.7%), property (31.0%),method (10.0%), model (5.3%), modal-ity (3.3%), field (1.9%), task (1.9%),instrument (1.2%), dataset (0.5%)Evolutionary Biology object (51.2%), property (25.7%),model (9.2%), method (5.4%), task(3.3%), field (2.5%), modality (1.7%),dataset (0.4%), instrument (0.2%)</p>
<p>Table 3 :
3
Distribution of concept types by domain</p>
<p>†  We use the d3-force library; see documentation for more information on the spring-layout implementation.
AcknowledgmentsThe computations in this work were, in part, run at facilities supported by the Scientific Computing Core at the Flatiron Institute, a division of the Simons Foundation.Polymathic AI acknowledgeshttps://github.com/chiral-carbon/kgfor-science.§
arXiv:2311.07361Microsoft Research AI4Science and Microsoft Azure Quantum. 2023. The Impact of Large Language Models on Scientific Discovery: a Preliminary Study using GPT-4. Preprint. </p>
<p>others. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, arXiv:2005.14165Language Models are Few-Shot Learners. 202012Preprint</p>
<p>Retrieval-Augmented Generation for Large Language Models: A Survey. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, Haofen Wang, arXiv:2312.109972024Preprint</p>
<p>Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, and 542 others. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, arXiv:2407.21783The Llama 3 Herd of Models. 2024Preprint</p>
<p>Interesting Scientific Idea Generation Using Knowledge Graphs and LLMs: Evaluations with 100 Research Group Leaders. Xuemei Gu, Mario Krenn, arXiv:2405.170442024Preprint</p>
<p>A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, Ting Liu, 10.1145/3703155ACM Transactions on Information Systems. 2024</p>
<p>Large Language Models are Zero-Shot Reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, arXiv:2205.119162023Preprint</p>
<p>Predicting research trends with semantic and neural networks with an application in quantum physics. Mario Krenn, Anton Zeilinger, 10.1073/pnas.1914370116Proceedings of the National Academy of Sciences. 11742020</p>
<p>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen Tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela, arXiv:2005.114012021Preprint</p>
<p>Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha, arXiv:2408.06292The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery. 2024Preprint</p>
<p>PaperQA: Retrieval-Augmented Generative Agent for Scientific Research. Jakub Lála, O' Odhran, Aleksandar Donoghue, Sam Shtedritski, Samuel G Cox, Andrew D Rodriques, White, arXiv:2312.075592023Preprint</p>
<p>Scientific Language Models for Biomedical Knowledge Base Completion: An Empirical Study. Rahul Nadkarni, David Wadden, Iz Beltagy, Noah A Smith, Hannaneh Hajishirzi, Tom Hope, arXiv:2106.097002021Preprint</p>
<p>. Josh Openai, Steven Achiam, Sandhini Adler, Lama Agarwal, Ilge Ahmad, Florencia Akkaya, Diogo Leoni Aleman, Janko Almeida, Sam Altenschmidt, Shyamal Altman, Anadkat, arXiv:2303.08774Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff BelgumPreprintand 262 others. 2024. GPT-4</p>
<p>Automatic keyword extraction from individual documents. Stuart J Rose, Dave W Engel, Nick Cramer, Wendy Cowley, 2010</p>
<p>Knowledge Graph in Astronomical Research with Large Language Models: Quantifying Driving Forces in Interdisciplinary Scientific Discovery. Zechang Sun, Yuan-Sen, Yaobo Ting, Nan Liang, Song Duan, Zheng Huang, Cai, arXiv:2406.013912024Preprint</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, arXiv:2307.09288Wenyin Fu, and 49 others. 2023. Llama 2: Open Foundation and Fine-Tuned Chat Models. Preprint</p>
<p>rake-nltk: Python implementation of the rapid automatic keyword extraction algorithm using nltk. B V Vishwas, 2017</p>
<p>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, arXiv:2201.119032023Preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7570 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7570</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7570</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-139.html">extraction-schema-139</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <p><strong>Paper ID:</strong> paper-5b7f5488c380cf5085a5dd93e993ad293b225eee</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/5b7f5488c380cf5085a5dd93e993ad293b225eee" target="_blank">One Fits All: Power General Time Series Analysis by Pretrained LM</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> The results demonstrate that pre-trained models on natural language or images can lead to a comparable or state-of-the-art performance in all main time series analysis tasks, as illustrated in Figure 1.</p>
                <p><strong>Paper Abstract:</strong> Although we have witnessed great success of pre-trained models in natural language processing (NLP) and computer vision (CV), limited progress has been made for general time series analysis. Unlike NLP and CV where a unified model can be used to perform different tasks, specially designed approach still dominates in each time series analysis task such as classification, anomaly detection, forecasting, and few-shot learning. The main challenge that blocks the development of pre-trained model for time series analysis is the lack of a large amount of data for training. In this work, we address this challenge by leveraging language or CV models, pre-trained from billions of tokens, for time series analysis. Specifically, we refrain from altering the self-attention and feedforward layers of the residual blocks in the pre-trained language or image model. This model, known as the Frozen Pretrained Transformer (FPT), is evaluated through fine-tuning on all major types of tasks involving time series. Our results demonstrate that pre-trained models on natural language or images can lead to a comparable or state-of-the-art performance in all main time series analysis tasks, as illustrated in Figure 1. We also found both theoretically and empirically that the self-attention module behaviors similarly to principle component analysis (PCA), an observation that helps explains how transformer bridges the domain gap and a crucial step towards understanding the universality of a pre-trained transformer.The code is publicly available at https://github.com/DAMO-DI-ML/One_Fits_All.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7570.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7570.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT2-FPT (anomaly)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-2 Backbone Frozen Pretrained Transformer for Time-Series Anomaly Detection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This paper applies a frozen, pretrained GPT-2 transformer backbone (FPT) fine-tuned with a lightweight head and positional/normalization tuning to detect anomalies in multivariate time series via reconstruction error; it outperforms prior SOTA on average F1 across five standard industrial/time-series anomaly datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 (GPT2)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only transformer backbone (GPT-2) used in a frozen pretrained form where self-attention and feedforward layers are frozen; embedding layer, positional embeddings and layer-norm are fine-tuned and a small output head is trained.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_approach</strong></td>
                            <td>Fine-tuning of a frozen pretrained transformer backbone (FPT) with a lightweight head; anomalies detected using classical reconstruction-error scoring (reconstruction-based anomaly detection).</td>
                        </tr>
                        <tr>
                            <td><strong>prompt_template</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Fine-tuned on the standard time-series anomaly datasets using the same reconstruction-error experimental setting as TimesNet; datasets used: SMD, MSL, SMAP, SWaT, PSM. (Paper does not provide additional synthetic-anomaly generation or labeled-tabular anomaly supervision details.)</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Multivariate numeric time series (temporal tabular/time-series data sequences)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>SMD, MSL, SMAP, SWaT, PSM (standard time-series anomaly detection benchmarks cited in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Per-dataset and average F1-score (percentage); reconstruction-error followed by thresholding evaluated with F1.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>GPT2(6) FPT average F1 = 86.72%; per-dataset F1 — SMD: 86.89%, MSL: 82.45%, SMAP: 72.88%, SWaT: 94.23%, PSM: 97.13%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to TimesNet (reproduced) average F1 = 85.24% (GPT2(6) beats TimesNet by 1.48 percentage points absolute / ~1.7% relative as reported). Other baselines reported per-dataset F1 in the same table (PatchTST, ETSformer, FEDformer, LightTS, DLinear, Stationary, Autoformer, Pyraformer, Anomaly Transformer (reconstruction-only), Informer, Reformer). Example per-dataset TimesNet F1s: SMD 84.61%, MSL 81.84%, SMAP 69.39%, SWaT 93.02%, PSM 97.34%.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Fine-tuning on the target anomaly datasets (reconstruction-based); not zero-shot/few-shot in the anomaly experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>The paper does not report detailed failure-mode analyses for anomaly detection; small per-dataset differences exist (e.g., PSM F1: GPT2(6) 97.13% vs TimesNet 97.34% where GPT2 is marginally lower), but overall average performance is reported higher. No explicit discussion of high false-positive regimes or domain-specific failure cases for anomaly detection is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Reported training/inference microbenchmarks: GPT-2(6)-768 variant reported (Table 9) training time per step ~0.104s and inference time per batch ~0.054s on a 32G V100 GPU (batch from ETTh2, batch size 128). The paper also reports that the learnable (trainable) parameters are a small fraction of total model parameters (GPT-2(6) has learnable parameters proportion reported as 4.6% and GPT-2(3) as 6.12% in the 'Training Params Percentages' column); the 'Training Params' entry for GPT-2 variants in Table 9 is listed as ~4M (this table reports the number of trainable parameters used in fine-tuning rather than total pre-trained model size).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'One Fits All: Power General Time Series Analysis by Pretrained LM', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Anomaly transformer: Time series anomaly detection with association discrepancy <em>(Rating: 2)</em></li>
                <li>TimesNet: Temporal 2D-variation modeling for general time series analysis <em>(Rating: 2)</em></li>
                <li>Frozen pretrained transformers as universal computation engines <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7570",
    "paper_id": "paper-5b7f5488c380cf5085a5dd93e993ad293b225eee",
    "extraction_schema_id": "extraction-schema-139",
    "extracted_data": [
        {
            "name_short": "GPT2-FPT (anomaly)",
            "name_full": "GPT-2 Backbone Frozen Pretrained Transformer for Time-Series Anomaly Detection",
            "brief_description": "This paper applies a frozen, pretrained GPT-2 transformer backbone (FPT) fine-tuned with a lightweight head and positional/normalization tuning to detect anomalies in multivariate time series via reconstruction error; it outperforms prior SOTA on average F1 across five standard industrial/time-series anomaly datasets.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 (GPT2)",
            "model_description": "Decoder-only transformer backbone (GPT-2) used in a frozen pretrained form where self-attention and feedforward layers are frozen; embedding layer, positional embeddings and layer-norm are fine-tuned and a small output head is trained.",
            "model_size": null,
            "anomaly_detection_approach": "Fine-tuning of a frozen pretrained transformer backbone (FPT) with a lightweight head; anomalies detected using classical reconstruction-error scoring (reconstruction-based anomaly detection).",
            "prompt_template": null,
            "training_data": "Fine-tuned on the standard time-series anomaly datasets using the same reconstruction-error experimental setting as TimesNet; datasets used: SMD, MSL, SMAP, SWaT, PSM. (Paper does not provide additional synthetic-anomaly generation or labeled-tabular anomaly supervision details.)",
            "data_type": "Multivariate numeric time series (temporal tabular/time-series data sequences)",
            "dataset_name": "SMD, MSL, SMAP, SWaT, PSM (standard time-series anomaly detection benchmarks cited in the paper)",
            "evaluation_metric": "Per-dataset and average F1-score (percentage); reconstruction-error followed by thresholding evaluated with F1.",
            "performance": "GPT2(6) FPT average F1 = 86.72%; per-dataset F1 — SMD: 86.89%, MSL: 82.45%, SMAP: 72.88%, SWaT: 94.23%, PSM: 97.13%.",
            "baseline_comparison": "Compared to TimesNet (reproduced) average F1 = 85.24% (GPT2(6) beats TimesNet by 1.48 percentage points absolute / ~1.7% relative as reported). Other baselines reported per-dataset F1 in the same table (PatchTST, ETSformer, FEDformer, LightTS, DLinear, Stationary, Autoformer, Pyraformer, Anomaly Transformer (reconstruction-only), Informer, Reformer). Example per-dataset TimesNet F1s: SMD 84.61%, MSL 81.84%, SMAP 69.39%, SWaT 93.02%, PSM 97.34%.",
            "zero_shot_or_few_shot": "Fine-tuning on the target anomaly datasets (reconstruction-based); not zero-shot/few-shot in the anomaly experiments.",
            "limitations_or_failure_cases": "The paper does not report detailed failure-mode analyses for anomaly detection; small per-dataset differences exist (e.g., PSM F1: GPT2(6) 97.13% vs TimesNet 97.34% where GPT2 is marginally lower), but overall average performance is reported higher. No explicit discussion of high false-positive regimes or domain-specific failure cases for anomaly detection is provided.",
            "computational_cost": "Reported training/inference microbenchmarks: GPT-2(6)-768 variant reported (Table 9) training time per step ~0.104s and inference time per batch ~0.054s on a 32G V100 GPU (batch from ETTh2, batch size 128). The paper also reports that the learnable (trainable) parameters are a small fraction of total model parameters (GPT-2(6) has learnable parameters proportion reported as 4.6% and GPT-2(3) as 6.12% in the 'Training Params Percentages' column); the 'Training Params' entry for GPT-2 variants in Table 9 is listed as ~4M (this table reports the number of trainable parameters used in fine-tuning rather than total pre-trained model size).",
            "uuid": "e7570.0",
            "source_info": {
                "paper_title": "One Fits All: Power General Time Series Analysis by Pretrained LM",
                "publication_date_yy_mm": "2023-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Anomaly transformer: Time series anomaly detection with association discrepancy",
            "rating": 2
        },
        {
            "paper_title": "TimesNet: Temporal 2D-variation modeling for general time series analysis",
            "rating": 2
        },
        {
            "paper_title": "Frozen pretrained transformers as universal computation engines",
            "rating": 2
        }
    ],
    "cost": 0.014659,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>One Fits All: Power General Time Series Analysis by Pretrained LM</h1>
<p>Tian Zhou<em> Peisong Niu</em> Xue Wang* Liang Sun Rong Jin ${ }^{\dagger}$<br>{tian.zt,niupeisong.nps,xue.w,liang.sun, jinrong.jr}@alibaba-inc.com</p>
<h4>Abstract</h4>
<p>Although we have witnessed great success of pre-trained models in natural language processing (NLP) and computer vision (CV), limited progress has been made for general time series analysis. Unlike NLP and CV where a unified model can be used to perform different tasks, specially designed approach still dominates in each time series analysis task such as classification, anomaly detection, forecasting, and few-shot learning. The main challenge that blocks the development of pre-trained model for time series analysis is the lack of a large amount of data for training. In this work, we address this challenge by leveraging language or CV models, pre-trained from billions of tokens, for time series analysis. Specifically, we refrain from altering the self-attention and feedforward layers of the residual blocks in the pre-trained language or image model. This model, known as the Frozen Pretrained Transformer (FPT), is evaluated through fine-tuning on all major types of tasks involving time series. Our results demonstrate that pre-trained models on natural language or images can lead to a comparable or state-of-the-art performance in all main time series analysis tasks, as illustrated in Figure 1. We also found both theoretically and empirically that the self-attention module behaviors similarly to principle component analysis (PCA), an observation that helps explains how transformer bridges the domain gap and a crucial step towards understanding the universality of a pre-trained transformer. The code is publicly available at https://github.com/DAMO-DI-ML/One_Fits_All.</p>
<h2>1 Introduction</h2>
<p>Time series analysis is a fundamental problem Hyndman \&amp; Athanasopoulos (2021) that has played an important role in many real-world applications Wen et al. (2022), such as retail sales forecasting Böse et al. (2017); Courty \&amp; Li (1999) , imputation of missing data for economic time series Friedman (1962) anomaly detection for industrial maintenance Gao et al. (2020), and classification of time series from various domain Ismail Fawaz et al. (2019). Numerous statistical and machine learning methods have been developed for time series analysis in the past. Inspired by its great success in natural language processing and computer vision Vaswani et al. (2017); Devlin et al. (2019); Dosovitskiy et al. (2021); Rao et al. (2021), transformer has been introduced to various time series tasks with promising results Wen et al. (2023), especially for time series forecasting Lim et al. (2021); Zhou et al. (2022, 2021); Wu et al. (2021); Nie et al. (2022).
We have recently witnessed the rapid development of foundation models in NLP. The key idea is to pre-train a large language model from billions of tokens to facilitate model training for downstream tasks, particularly when we have a few, sometimes even zero, labeled instances. Another advantage of</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Model performance comparison on various tasks.</p>
<p>foundation models is that they provide a unified framework for handling diverse tasks, which contrasts conventional wisdom where each task requires a specially designed algorithm. However, so far, little progress has been made to exploit pre-trained or foundation models for time series analysis. One main challenge is the lack of the large amount of data to train a foundation model for time series analysis. The largest data sets for time series analysis is less than 10GB <em>Godahewa et al. (2021)</em>, which is much smaller than that for NLP. To address this challenge, we propose to leverage pre-trained language models for general time series analysis. Our approach provides a unified framework for diverse time series tasks, such as classification, anomaly detection, forecasting, and few-shot or zero-shot learning. As shown in Figure 1, using the same backbone, our approach performs either on-par or better than the state-of-the-art methods for all main time series analysis tasks. Besides extensive empirical studies, we also investigate why a transformer model pre-trained from the language domain can be adapted to time series analysis with almost no change. Our analysis indicates that the self-attention modules in the pre-trained transformer acquire the ability to perform certain non-data-dependent operations through training. These operations are closely linked to principal component analysis over the input patterns. We believe it is this generic function performed by the self-attention module that allows trained transformer models to be so-called universal compute engine <em>Lu et al. (2022)</em> or general computation calculator <em>Giannou et al. (2023)</em>. We support our claims by conducting an empirical investigation of the resemblance in model behaviors when self-attention is substituted with PCA, and by providing a theoretical analysis of their correlation.</p>
<p>Here we summarize our key contributions as follows:</p>
<ol>
<li>We propose a unified framework that uses a frozen pre-trained language model to achieve a SOTA or comparable performance in all major types of time series analysis tasks supported by thorough and extensive experiments, including time series classification, short/long-term forecasting, imputation, anomaly detection, few-shot and zero-sample forecasting.</li>
<li>We found, both theoretically and empirically, that self-attention performs a function similar to PCA, which helps explain the universality of transformer models.</li>
<li>We demonstrate the universality of our approach by exploring a pre-trained transformer model from another backbond model (BERT) or modality (computer vision) to power the time series forecasting.</li>
</ol>
<p>The remainder of this paper is structured as follows. Section 2 briefly summarizes the related work. Section 3 presents the proposed detailed model structure. In Section 4, we conduct a thorough and extensive evaluation of the performance of cross-modality time series analysis using our proposed method in seven main time series analysis tasks compared to various SOTA baseline models. Section 5</p>
<p>presents various ablation studies, and Section 6 demonstrates the universality of our proposed method using pre-trained models with another structure or pre-trained from another modality. In Section 7, we provide a theoretical explanation of the connection between self-attention and PCA. Finally, in Section 8, we discuss our results and future directions. Due to space limit, more extensive discussion of related work, experimental results, and theoretical analysis are provided in the Appendix.</p>
<h1>2 Related Work</h1>
<p>In this section, we provide short reviews of literature in the areas of time series analysis, in-modality transfer learning, and cross-modality knowledge transfer learning. We postpone the discussion of works for end-to-end time series analysis to Appendix B, due to the limited space.</p>
<p>In-modality Transfer Learning through pre-trained models In recent years, a large number of research works have verified the effectiveness of the pre-trained model from NLP, CV to Vision-and-Language (VL). Latest studies for NLP focus on learning contextual word embeddings for downstream tasks. With the increase of computing power, the very deep transformer models have shown powerful representation ability in various language tasks. Among them, BERT Devlin et al. (2019) uses transformer encoders and employs masked language modeling task that aims to recover the random masked tokens within a text. OpenAI proposed GPT Radford \&amp; Narasimhan (2018) that trains transformer decoders on a large language corpus and then fine-tunes on task-specific data. GPT2 Radford et al. (2019) is trained on larger datasets with much more parameters and can be transferred to various downstream tasks. Since transformer models can adapt to various inputs, the idea of pre-training can also be well adapted to visual tasks. DEiT Touvron et al. (2021) proposed a teacher-student strategy for transformers with convolution neural networks (CNNs) as the teacher model and achieves competitive performance. BEiT Bao et al. (2022) converts images as visual tokens and successfully uses the BERT model in CV. However, because of the insufficient training sample, there is little research on pre-trained models on general time series analysis that cover all major tasks like CV or NLP domain.</p>
<p>Cross-modality knowledge transfer Since transformers can handle different modal tasks through tokenizing the inputs to embeddings, it is also an interesting topic whether the transformers have universal representation ability and can be used for transferring between various domains. The VL pre-trained model VLMo Bao et al. (2021) proposed a stagewise pre-training strategy that utilizes frozen attention blocks pre-trained by image-only data to train the language expert. One of the most related works which transfer knowledge from a pre-trained language model to other domains is Lu et al. (2022), which studies the strong performance of a frozen pre-trained language model (LM) compared to an end-to-end transformer alternative learned from other domains' data. Another relative work for knowledge transfer to the time series is the Voice2series Yang et al. (2021), which leverages a pre-trained speech processing model for time series classification and achieves superior performance. To the best of our knowledge, no previous research has investigated cross-modality knowledge transfer for the time series forecasting task, let alone general time series analysis.</p>
<h2>3 Methodology</h2>
<h3>3.1 Model Structure</h3>
<p>The architecture we employ is depicted in Figure 2. We utilize parameters from NLP pretrained transformer models for time series analysis, with a focus on the GPT2 model Radford et al. (2019). We also experiment with other models, such as BERT Devlin et al. (2019) and BEiT Bao et al. (2022), to further demonstrate that the universal performance of cross-domain knowledge transfer exists in a wide range of pre-trained models.</p>
<p>Frozen Pretrained Block Our architecture retains the positional embedding layers and self-attention blocks from the pre-trained models. As self-attention layers and FFN (Feedforward Neural Networks) contain the majority of learned knowledge from pre-trained language models, we opt to freeze the self-attention blocks while fine-tuning.</p>
<p>Positional Embeddings and Layer Normalization To enhance downstream tasks with minimal effort, we fine-tune the positional embeddings and layer normalization layer, which is considered a</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Model architecture. Pre-trained parameters are transferred to the time series forecasting tasks. Self-attention and Feedforward layers in the transformer blocks are frozen while only the embedding layer, normalization layers, and output layer require training.
standard practiceLu et al. (2022); Houlsby et al. (2019). As a result, we retrain these components during fine-tuning.</p>
<p>Input Embedding Given our goal of applying the NLP pre-trained model to various tasks and a new modality, we must redesign and train the input embedding layer. This layer is responsible for projecting the time-series data to the required dimensions of the specific pre-trained model. To accomplish this, we use linear probing, which also reduces the number of parameters required for training.</p>
<p>Normalization Data normalization is crucial for pre-trained models across various modalities. In addition to the layer norm utilized in pre-trained LM, we also incorporate a simple data normalization block, reverse instance norm Kim et al. (2022), to further facilitate knowledge transfer. This normalization block simply normalizes the input time series using mean and variance, and then adds them back to the output.</p>
<p>Patching To extract local semantic information, we utilize patching Nie et al. (2022) by aggregating adjacent time steps to form a single patch-based token. Patching enables a significant increase in the input historical time horizon while maintaining the same token length and reducing information redundancy for transformer models. In our architecture, we apply patching after instance normalization.</p>
<h1>4 Main Time Series Analysis Tasks</h1>
<p>Our proposed method excels in various downstream time series analysis tasks through fine-tuning. To demonstrate the effectiveness of our approach, we conduct extensive experiments on major types of downstream tasks, including time series classification, anomaly detection, imputation, short/long-term forecasting and few-shot/zero-shot forecasting. To ensure a fair comparison, we use GPT2-backbone FPT and adhere to the experimental settings of TimesNet Wu et al. (2023). Due to the space limit, only the summarized results are presented below except zero-shot forecasting. Full experimental results of the other six downstream tasks can be found in Appendix D.3, D.2, D.7, H.6, H.7, H.8, H. 9 respectively.</p>
<p>Baselines We select representative baselines and cite their results from Wu et al. (2023), which includes the most recent and quite extensive empirical studies of time series. The baselines include CNN-based models: TimesNet Wu et al. (2023); MLP-based models: LightTS Zhang et al. (2022) and DLinear Zeng et al. (2023); Transformer-based models: Reformer Kitaev et al. (2020), Informer Zhou et al. (2021), Autoformer Wu et al. (2021), FEDformer Zhou et al. (2022), Nonstationary Transformer Liu et al. (2022), ETSformer Woo et al. (2022), PatchTST Nie et al. (2022). Besides, N-HiTS Challu et al. (2022) and N-BEATS Oreshkin et al. (2019) are used for short-term forecasting. Anomaly Transformer Xu et al. (2021) is used for anomaly detection. XGBoost Chen \&amp; Guestrin (2016), Rocket Dempster et al. (2020), LSTNet Lai et al. (2018), LSSL Gu et al. (2021),</p>
<p>Pyraformer Liu et al. (2021), TCN Franceschi et al. (2019) and Flowformer Huang et al. (2022) are used for classification.</p>
<h1>4.1 Main Results</h1>
<p>Overall, as shown in Figure 1, GPT2-backbone FPT outperforms other models in most tasks, including long/short-term forecasting, classification, anomaly detection, imputation, and fow-shot/zero-short forecasting. This confirms that time series tasks can also take advantage of cross-modality transferred knowledge. In the following, we use GPT2(K) to represent GPT2-backbone with first K Layers.</p>
<h3>4.2 Imputation</h3>
<p>Setups We conduct experiments on six popular real-world datasets, including 4 ETT datasets Zhou et al. (2021) (ETTh1, ETTh2, ETTm1, ETTm2), Electricity and Weather, where the data-missing is common. Following the settings of TimesNet, different random mask ratios ( $\left.{12.5 \%, 25 \%, 37.5 \%,\right.$ $50 \%$ }) of time points are selected for the evaluation on various proportions of missing data.
Results The results are shown in Table 1 that GPT2(3) FPT achieves the best performance on most datasets. Particularly, compared to the previous SOTA TimesNet, GPT2(3) FPT yields a relative $\mathbf{1 1 . 5 \%}$ MSE reduction on ETTh1, and a $\mathbf{4 . 1 \%}$ MSE reduction on average on six benchmark datasets. It verifies that the proposed method can also effectively mine temporal patterns of incomplete time series.</p>
<p>Table 1: Imputation task. We randomly mask ${12.5 \%, 25 \%, 37.5 \%, 50 \%}$ time points of 96 -length time series. The results are averaged from 4 different mask ratios. Black: best, Red: second best. Appendix H. 8 shows the full results.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Methods</th>
<th style="text-align: center;">GPT2(3)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">TimesNet</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">PatchTST</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ETSformer</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">LightTS</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">DLinear</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">FEDformer</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Stationary</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Autoformer</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Informer</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Reformer</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
</tr>
<tr>
<td style="text-align: center;">ETTm1</td>
<td style="text-align: center;">0.028</td>
<td style="text-align: center;">0.103</td>
<td style="text-align: center;">0.027</td>
<td style="text-align: center;">0.107</td>
<td style="text-align: center;">0.047</td>
<td style="text-align: center;">0.140</td>
<td style="text-align: center;">0.120</td>
<td style="text-align: center;">0.253</td>
<td style="text-align: center;">0.104</td>
<td style="text-align: center;">0.218</td>
<td style="text-align: center;">0.093</td>
<td style="text-align: center;">0.206</td>
<td style="text-align: center;">0.062</td>
<td style="text-align: center;">0.177</td>
<td style="text-align: center;">0.036</td>
<td style="text-align: center;">0.126</td>
<td style="text-align: center;">0.051</td>
<td style="text-align: center;">0.150</td>
<td style="text-align: center;">0.071</td>
<td style="text-align: center;">0.188</td>
<td style="text-align: center;">0.055</td>
<td style="text-align: center;">0.166</td>
</tr>
<tr>
<td style="text-align: center;">ETTm2</td>
<td style="text-align: center;">0.021</td>
<td style="text-align: center;">0.084</td>
<td style="text-align: center;">0.022</td>
<td style="text-align: center;">0.088</td>
<td style="text-align: center;">0.029</td>
<td style="text-align: center;">0.102</td>
<td style="text-align: center;">0.208</td>
<td style="text-align: center;">0.327</td>
<td style="text-align: center;">0.046</td>
<td style="text-align: center;">0.151</td>
<td style="text-align: center;">0.096</td>
<td style="text-align: center;">0.208</td>
<td style="text-align: center;">0.101</td>
<td style="text-align: center;">0.215</td>
<td style="text-align: center;">0.026</td>
<td style="text-align: center;">0.099</td>
<td style="text-align: center;">0.029</td>
<td style="text-align: center;">0.105</td>
<td style="text-align: center;">0.156</td>
<td style="text-align: center;">0.292</td>
<td style="text-align: center;">0.157</td>
<td style="text-align: center;">0.280</td>
</tr>
<tr>
<td style="text-align: center;">ETTh1</td>
<td style="text-align: center;">0.069</td>
<td style="text-align: center;">0.173</td>
<td style="text-align: center;">0.078</td>
<td style="text-align: center;">0.187</td>
<td style="text-align: center;">0.115</td>
<td style="text-align: center;">0.224</td>
<td style="text-align: center;">0.202</td>
<td style="text-align: center;">0.329</td>
<td style="text-align: center;">0.284</td>
<td style="text-align: center;">0.373</td>
<td style="text-align: center;">0.201</td>
<td style="text-align: center;">0.306</td>
<td style="text-align: center;">0.117</td>
<td style="text-align: center;">0.246</td>
<td style="text-align: center;">0.094</td>
<td style="text-align: center;">0.201</td>
<td style="text-align: center;">0.103</td>
<td style="text-align: center;">0.214</td>
<td style="text-align: center;">0.161</td>
<td style="text-align: center;">0.279</td>
<td style="text-align: center;">0.122</td>
<td style="text-align: center;">0.245</td>
</tr>
<tr>
<td style="text-align: center;">ETTh2</td>
<td style="text-align: center;">0.048</td>
<td style="text-align: center;">0.141</td>
<td style="text-align: center;">0.049</td>
<td style="text-align: center;">0.146</td>
<td style="text-align: center;">0.065</td>
<td style="text-align: center;">0.163</td>
<td style="text-align: center;">0.367</td>
<td style="text-align: center;">0.436</td>
<td style="text-align: center;">0.119</td>
<td style="text-align: center;">0.250</td>
<td style="text-align: center;">0.142</td>
<td style="text-align: center;">0.259</td>
<td style="text-align: center;">0.163</td>
<td style="text-align: center;">0.279</td>
<td style="text-align: center;">0.053</td>
<td style="text-align: center;">0.152</td>
<td style="text-align: center;">0.055</td>
<td style="text-align: center;">0.156</td>
<td style="text-align: center;">0.337</td>
<td style="text-align: center;">0.452</td>
<td style="text-align: center;">0.234</td>
<td style="text-align: center;">0.352</td>
</tr>
<tr>
<td style="text-align: center;">ECL</td>
<td style="text-align: center;">0.090</td>
<td style="text-align: center;">0.207</td>
<td style="text-align: center;">0.092</td>
<td style="text-align: center;">0.210</td>
<td style="text-align: center;">0.072</td>
<td style="text-align: center;">0.183</td>
<td style="text-align: center;">0.214</td>
<td style="text-align: center;">0.339</td>
<td style="text-align: center;">0.131</td>
<td style="text-align: center;">0.262</td>
<td style="text-align: center;">0.132</td>
<td style="text-align: center;">0.260</td>
<td style="text-align: center;">0.130</td>
<td style="text-align: center;">0.259</td>
<td style="text-align: center;">0.100</td>
<td style="text-align: center;">0.218</td>
<td style="text-align: center;">0.101</td>
<td style="text-align: center;">0.225</td>
<td style="text-align: center;">0.222</td>
<td style="text-align: center;">0.328</td>
<td style="text-align: center;">0.200</td>
<td style="text-align: center;">0.313</td>
</tr>
<tr>
<td style="text-align: center;">Weather</td>
<td style="text-align: center;">0.031</td>
<td style="text-align: center;">0.056</td>
<td style="text-align: center;">0.030</td>
<td style="text-align: center;">0.054</td>
<td style="text-align: center;">0.034</td>
<td style="text-align: center;">0.055</td>
<td style="text-align: center;">0.076</td>
<td style="text-align: center;">0.171</td>
<td style="text-align: center;">0.055</td>
<td style="text-align: center;">0.117</td>
<td style="text-align: center;">0.052</td>
<td style="text-align: center;">0.110</td>
<td style="text-align: center;">0.099</td>
<td style="text-align: center;">0.203</td>
<td style="text-align: center;">0.032</td>
<td style="text-align: center;">0.059</td>
<td style="text-align: center;">0.031</td>
<td style="text-align: center;">0.057</td>
<td style="text-align: center;">0.045</td>
<td style="text-align: center;">0.104</td>
<td style="text-align: center;">0.038</td>
<td style="text-align: center;">0.087</td>
</tr>
<tr>
<td style="text-align: center;">Average</td>
<td style="text-align: center;">0.047</td>
<td style="text-align: center;">0.127</td>
<td style="text-align: center;">0.049</td>
<td style="text-align: center;">0.132</td>
<td style="text-align: center;">0.060</td>
<td style="text-align: center;">0.144</td>
<td style="text-align: center;">0.197</td>
<td style="text-align: center;">0.309</td>
<td style="text-align: center;">0.123</td>
<td style="text-align: center;">0.228</td>
<td style="text-align: center;">0.119</td>
<td style="text-align: center;">0.224</td>
<td style="text-align: center;">0.112</td>
<td style="text-align: center;">0.229</td>
<td style="text-align: center;">0.056</td>
<td style="text-align: center;">0.142</td>
<td style="text-align: center;">0.061</td>
<td style="text-align: center;">0.151</td>
<td style="text-align: center;">0.165</td>
<td style="text-align: center;">0.273</td>
<td style="text-align: center;">0.134</td>
<td style="text-align: center;">0.240</td>
</tr>
</tbody>
</table>
<h3>4.3 Time Series Classification</h3>
<p>Setups To evaluate the model's capacity for high-level representation learning, we employ sequence-level classification. Specifically, we follow the same setting as TimesNet: For classification, 10 multivariate UEA classification datasets Bagnall et al. (2018) are selected for evaluation, including gesture, action, audio recognition medical diagnosis and other practical tasks.
Results As shown in Figure 3, GPT2(6) FPT achieves an average accuracy of $74.00 \%$, surpassing all baselines including TimesNet ( $73.60 \%$ ). Specifically, compared to recent published patch-transformer-based models Nie et al. (2022) , GPT2(6) FPT surpasses it by a large margin $\mathbf{9 . 0 \%}$ which shows the prior NLP transfer knowledge can indeed help in time series representation.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Model comparison in classification. The results are averaged from 10 subsets of UEA. Appendix H. 6 shows the full results.</p>
<h3>4.4 Time Series Anomaly Detection</h3>
<p>Setups Detecting anomalies in time series is vital in industrial applications, ranging from health monitoring to space \&amp; earth exploration. We compare models on five commonly used datasets, including</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>SMDSu et al. (2019), MSLHundman et al. (2018), SMAPHundman et al. (2018), SWaTMathur \&amp; Tippenhauer (2016) and PSMAbdulaal et al. (2021). To perform a fair comparison, only the classical reconstruction error is used for all baseline models to the make the setting the same as TimesNet.</p>
<p>Results Table 2 demonstrates that GPT2(6) FPT also achieves the best performance with the averaged F1-score $\mathbf{8 6 . 7 2 \%}$, surpassing previous SOTA method TimesNet by $\mathbf{1 . 7 \%}$. Thus, in addition to its proficiency in representing complete sequences for classification purposes, GPT2(6) FPT is capable of detecting infrequent anomalies within time series.
Table 2: Anomaly detection task. We calculate the F1-score (as \%) for each dataset. <em>. in the Transformers indicates the name of $</em>$ former. Black: best, Red: second best. Appendix H. 7 shows the full results.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Methods</th>
<th style="text-align: center;">GPT2(6) <br> Ours</th>
<th style="text-align: center;">TimesNet*</th>
<th style="text-align: center;">PatchTS.</th>
<th style="text-align: center;">ETS.</th>
<th style="text-align: center;">FED.</th>
<th style="text-align: center;">LightTS</th>
<th style="text-align: center;">DLinear</th>
<th style="text-align: center;">Stationary</th>
<th style="text-align: center;">Auto.</th>
<th style="text-align: center;">Pyra.</th>
<th style="text-align: center;">Anomaly.**</th>
<th style="text-align: center;">In.</th>
<th style="text-align: center;">Re.</th>
<th style="text-align: center;">LogTrans.</th>
<th style="text-align: center;">Trans.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">SMD</td>
<td style="text-align: center;">86.89</td>
<td style="text-align: center;">84.61</td>
<td style="text-align: center;">84.62</td>
<td style="text-align: center;">83.13</td>
<td style="text-align: center;">85.08</td>
<td style="text-align: center;">82.53</td>
<td style="text-align: center;">77.10</td>
<td style="text-align: center;">84.72</td>
<td style="text-align: center;">85.11</td>
<td style="text-align: center;">83.04</td>
<td style="text-align: center;">85.49</td>
<td style="text-align: center;">81.65</td>
<td style="text-align: center;">75.32</td>
<td style="text-align: center;">76.21</td>
<td style="text-align: center;">79.56</td>
</tr>
<tr>
<td style="text-align: center;">MSL</td>
<td style="text-align: center;">82.45</td>
<td style="text-align: center;">81.84</td>
<td style="text-align: center;">78.70</td>
<td style="text-align: center;">85.03</td>
<td style="text-align: center;">78.57</td>
<td style="text-align: center;">78.95</td>
<td style="text-align: center;">84.88</td>
<td style="text-align: center;">77.50</td>
<td style="text-align: center;">79.05</td>
<td style="text-align: center;">84.86</td>
<td style="text-align: center;">83.31</td>
<td style="text-align: center;">84.06</td>
<td style="text-align: center;">84.40</td>
<td style="text-align: center;">79.57</td>
<td style="text-align: center;">78.68</td>
</tr>
<tr>
<td style="text-align: center;">SMAP</td>
<td style="text-align: center;">72.88</td>
<td style="text-align: center;">69.39</td>
<td style="text-align: center;">68.82</td>
<td style="text-align: center;">69.50</td>
<td style="text-align: center;">70.76</td>
<td style="text-align: center;">69.21</td>
<td style="text-align: center;">69.26</td>
<td style="text-align: center;">71.09</td>
<td style="text-align: center;">71.12</td>
<td style="text-align: center;">71.09</td>
<td style="text-align: center;">71.18</td>
<td style="text-align: center;">69.92</td>
<td style="text-align: center;">70.40</td>
<td style="text-align: center;">69.97</td>
<td style="text-align: center;">69.70</td>
</tr>
<tr>
<td style="text-align: center;">SWaT</td>
<td style="text-align: center;">94.23</td>
<td style="text-align: center;">93.02</td>
<td style="text-align: center;">85.72</td>
<td style="text-align: center;">84.91</td>
<td style="text-align: center;">93.19</td>
<td style="text-align: center;">93.33</td>
<td style="text-align: center;">87.52</td>
<td style="text-align: center;">79.88</td>
<td style="text-align: center;">92.74</td>
<td style="text-align: center;">91.78</td>
<td style="text-align: center;">83.10</td>
<td style="text-align: center;">81.43</td>
<td style="text-align: center;">82.80</td>
<td style="text-align: center;">80.52</td>
<td style="text-align: center;">80.37</td>
</tr>
<tr>
<td style="text-align: center;">PSM</td>
<td style="text-align: center;">97.13</td>
<td style="text-align: center;">97.34</td>
<td style="text-align: center;">96.08</td>
<td style="text-align: center;">91.76</td>
<td style="text-align: center;">97.23</td>
<td style="text-align: center;">97.15</td>
<td style="text-align: center;">93.55</td>
<td style="text-align: center;">97.29</td>
<td style="text-align: center;">93.29</td>
<td style="text-align: center;">82.08</td>
<td style="text-align: center;">79.40</td>
<td style="text-align: center;">77.10</td>
<td style="text-align: center;">73.61</td>
<td style="text-align: center;">76.74</td>
<td style="text-align: center;">76.07</td>
</tr>
<tr>
<td style="text-align: center;">Average</td>
<td style="text-align: center;">86.72</td>
<td style="text-align: center;">85.24</td>
<td style="text-align: center;">82.79</td>
<td style="text-align: center;">82.87</td>
<td style="text-align: center;">84.97</td>
<td style="text-align: center;">84.23</td>
<td style="text-align: center;">82.46</td>
<td style="text-align: center;">82.08</td>
<td style="text-align: center;">84.26</td>
<td style="text-align: center;">82.57</td>
<td style="text-align: center;">80.50</td>
<td style="text-align: center;">78.83</td>
<td style="text-align: center;">77.31</td>
<td style="text-align: center;">76.60</td>
<td style="text-align: center;">76.88</td>
</tr>
</tbody>
</table>
<ul>
<li>We reproduce the results of TimesNet by https://github.com/thuml/Time-Series-Library.
${ }^{<em> </em>}$ We replace the joint criterion in Anomaly Transformer with reconstruction error for fair comparison.</li>
</ul>
<h1>4.5 Long-term Forecasting</h1>
<p>Setups Eight popular real-world benchmark datasets Wu et al. (2023), including Weather, Traffic , Electricity, ILI , and 4 ETT datasets (ETTh1, ETTh2, ETTm1, ETTm2), are used for long-term forecasting evaluation. Additional information regarding the discussion on the input length setting can be found in the appendix H. 10 .
Results As shown in Table 3, GPT2(6) FPT achieves comparable performance with PatchTST and outperforms other baselines. Specifically, compared with recent published SOTA method TimesNet, GPT2(6) FPT yields a relative $\mathbf{9 . 3 \%}$ average MSE reduction.</p>
<p>Table 3: Long-term forecasting task. All the results are averaged from 4 different prediction lengths, that is [24, $36,48,60]$ for ILI and ${96,192,336,720}$ for the others. Black: best, Red: second best. Appendix D. 3 shows the full results.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Methods</th>
<th style="text-align: center;">GPT2(6)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">TimesNet</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ETSformer</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">LightTS</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">DLinear</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">FEDformer</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">PatchTST</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Stationary</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Autoformer</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Informer</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Reformer</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MSE MAE</td>
</tr>
<tr>
<td style="text-align: center;">Weather</td>
<td style="text-align: center;">0.237</td>
<td style="text-align: center;">0.279</td>
<td style="text-align: center;">0.259</td>
<td style="text-align: center;">0.287</td>
<td style="text-align: center;">0.271</td>
<td style="text-align: center;">0.334</td>
<td style="text-align: center;">0.261</td>
<td style="text-align: center;">0.312</td>
<td style="text-align: center;">0.249</td>
<td style="text-align: center;">0.300</td>
<td style="text-align: center;">0.309</td>
<td style="text-align: center;">0.360</td>
<td style="text-align: center;">0.225</td>
<td style="text-align: center;">0.264</td>
<td style="text-align: center;">0.288</td>
<td style="text-align: center;">0.314</td>
<td style="text-align: center;">0.338</td>
<td style="text-align: center;">0.382</td>
<td style="text-align: center;">0.634</td>
<td style="text-align: center;">0.548</td>
<td style="text-align: center;">0.803</td>
</tr>
<tr>
<td style="text-align: center;">ETTh1</td>
<td style="text-align: center;">0.427</td>
<td style="text-align: center;">0.426</td>
<td style="text-align: center;">0.458</td>
<td style="text-align: center;">0.450</td>
<td style="text-align: center;">0.542</td>
<td style="text-align: center;">0.510</td>
<td style="text-align: center;">0.491</td>
<td style="text-align: center;">0.479</td>
<td style="text-align: center;">0.423</td>
<td style="text-align: center;">0.437</td>
<td style="text-align: center;">0.440</td>
<td style="text-align: center;">0.460</td>
<td style="text-align: center;">0.413</td>
<td style="text-align: center;">0.439</td>
<td style="text-align: center;">0.570</td>
<td style="text-align: center;">0.537</td>
<td style="text-align: center;">0.496</td>
<td style="text-align: center;">0.487</td>
<td style="text-align: center;">1.040</td>
<td style="text-align: center;">0.795</td>
<td style="text-align: center;">1.029</td>
</tr>
<tr>
<td style="text-align: center;">ETTh2</td>
<td style="text-align: center;">0.346</td>
<td style="text-align: center;">0.394</td>
<td style="text-align: center;">0.414</td>
<td style="text-align: center;">0.427</td>
<td style="text-align: center;">0.439</td>
<td style="text-align: center;">0.452</td>
<td style="text-align: center;">0.602</td>
<td style="text-align: center;">0.543</td>
<td style="text-align: center;">0.431</td>
<td style="text-align: center;">0.447</td>
<td style="text-align: center;">0.437</td>
<td style="text-align: center;">0.449</td>
<td style="text-align: center;">0.330</td>
<td style="text-align: center;">0.379</td>
<td style="text-align: center;">0.526</td>
<td style="text-align: center;">0.516</td>
<td style="text-align: center;">0.450</td>
<td style="text-align: center;">0.459</td>
<td style="text-align: center;">4.431</td>
<td style="text-align: center;">1.729</td>
<td style="text-align: center;">6.736</td>
</tr>
<tr>
<td style="text-align: center;">ETTm1</td>
<td style="text-align: center;">0.352</td>
<td style="text-align: center;">0.383</td>
<td style="text-align: center;">0.400</td>
<td style="text-align: center;">0.406</td>
<td style="text-align: center;">0.429</td>
<td style="text-align: center;">0.425</td>
<td style="text-align: center;">0.435</td>
<td style="text-align: center;">0.437</td>
<td style="text-align: center;">0.357</td>
<td style="text-align: center;">0.378</td>
<td style="text-align: center;">0.448</td>
<td style="text-align: center;">0.452</td>
<td style="text-align: center;">0.351</td>
<td style="text-align: center;">0.387</td>
<td style="text-align: center;">0.481</td>
<td style="text-align: center;">0.456</td>
<td style="text-align: center;">0.588</td>
<td style="text-align: center;">0.517</td>
<td style="text-align: center;">0.961</td>
<td style="text-align: center;">0.734</td>
<td style="text-align: center;">0.799</td>
</tr>
<tr>
<td style="text-align: center;">ETTm2</td>
<td style="text-align: center;">0.266</td>
<td style="text-align: center;">0.326</td>
<td style="text-align: center;">0.291</td>
<td style="text-align: center;">0.333</td>
<td style="text-align: center;">0.293</td>
<td style="text-align: center;">0.342</td>
<td style="text-align: center;">0.409</td>
<td style="text-align: center;">0.436</td>
<td style="text-align: center;">0.267</td>
<td style="text-align: center;">0.334</td>
<td style="text-align: center;">0.305</td>
<td style="text-align: center;">0.349</td>
<td style="text-align: center;">0.255</td>
<td style="text-align: center;">0.315</td>
<td style="text-align: center;">0.306</td>
<td style="text-align: center;">0.347</td>
<td style="text-align: center;">0.327</td>
<td style="text-align: center;">0.371</td>
<td style="text-align: center;">1.410</td>
<td style="text-align: center;">0.810</td>
<td style="text-align: center;">1.479</td>
</tr>
<tr>
<td style="text-align: center;">ILI</td>
<td style="text-align: center;">1.925</td>
<td style="text-align: center;">0.903</td>
<td style="text-align: center;">2.139</td>
<td style="text-align: center;">0.931</td>
<td style="text-align: center;">2.497</td>
<td style="text-align: center;">1.004</td>
<td style="text-align: center;">7.382</td>
<td style="text-align: center;">2.003</td>
<td style="text-align: center;">2.169</td>
<td style="text-align: center;">1.041</td>
<td style="text-align: center;">2.847</td>
<td style="text-align: center;">1.144</td>
<td style="text-align: center;">1.443</td>
<td style="text-align: center;">0.798</td>
<td style="text-align: center;">2.077</td>
<td style="text-align: center;">0.914</td>
<td style="text-align: center;">3.006</td>
<td style="text-align: center;">1.161</td>
<td style="text-align: center;">5.137</td>
<td style="text-align: center;">1.544</td>
<td style="text-align: center;">4.724</td>
</tr>
<tr>
<td style="text-align: center;">ECL</td>
<td style="text-align: center;">0.167</td>
<td style="text-align: center;">0.263</td>
<td style="text-align: center;">0.192</td>
<td style="text-align: center;">0.295</td>
<td style="text-align: center;">0.208</td>
<td style="text-align: center;">0.323</td>
<td style="text-align: center;">0.229</td>
<td style="text-align: center;">0.329</td>
<td style="text-align: center;">0.166</td>
<td style="text-align: center;">0.263</td>
<td style="text-align: center;">0.214</td>
<td style="text-align: center;">0.327</td>
<td style="text-align: center;">0.161</td>
<td style="text-align: center;">0.253</td>
<td style="text-align: center;">0.193</td>
<td style="text-align: center;">0.296</td>
<td style="text-align: center;">0.227</td>
<td style="text-align: center;">0.338</td>
<td style="text-align: center;">0.311</td>
<td style="text-align: center;">0.397</td>
<td style="text-align: center;">0.338</td>
</tr>
<tr>
<td style="text-align: center;">Traffic</td>
<td style="text-align: center;">0.414</td>
<td style="text-align: center;">0.294</td>
<td style="text-align: center;">0.620</td>
<td style="text-align: center;">0.336</td>
<td style="text-align: center;">0.621</td>
<td style="text-align: center;">0.396</td>
<td style="text-align: center;">0.622</td>
<td style="text-align: center;">0.392</td>
<td style="text-align: center;">0.434</td>
<td style="text-align: center;">0.295</td>
<td style="text-align: center;">0.610</td>
<td style="text-align: center;">0.376</td>
<td style="text-align: center;">0.390</td>
<td style="text-align: center;">0.264</td>
<td style="text-align: center;">0.624</td>
<td style="text-align: center;">0.340</td>
<td style="text-align: center;">0.628</td>
<td style="text-align: center;">0.370</td>
<td style="text-align: center;">0.764</td>
<td style="text-align: center;">0.416</td>
<td style="text-align: center;">0.741</td>
</tr>
<tr>
<td style="text-align: center;">Average</td>
<td style="text-align: center;">0.516</td>
<td style="text-align: center;">0.407</td>
<td style="text-align: center;">0.596</td>
<td style="text-align: center;">0.433</td>
<td style="text-align: center;">0.662</td>
<td style="text-align: center;">0.473</td>
<td style="text-align: center;">1.303</td>
<td style="text-align: center;">0.616</td>
<td style="text-align: center;">0.562</td>
<td style="text-align: center;">0.436</td>
<td style="text-align: center;">0.701</td>
<td style="text-align: center;">0.489</td>
<td style="text-align: center;">0.446</td>
<td style="text-align: center;">0.386</td>
<td style="text-align: center;">0.633</td>
<td style="text-align: center;">0.465</td>
<td style="text-align: center;">0.757</td>
<td style="text-align: center;">0.511</td>
<td style="text-align: center;">1.836</td>
<td style="text-align: center;">0.871</td>
<td style="text-align: center;">2.081</td>
</tr>
</tbody>
</table>
<h3>4.6 Short-term Forecasting</h3>
<p>Setups To fully evaluate different algorithms in forecasting tasks, we also conduct short-term forecasting (with relatively short forecasting horizon) experiments on M4 Makridakis et al. (2018), contains marketing data of various frequencies.
Results The results in Table 4 show that the performance of GPT2-backbone (6) FPT is superior to advanced Transformer-based and MLP-based models, and comparable to TimesNet and N-BEATS.</p>
<h3>4.7 Few-shot Forecasting</h3>
<p>The large language model (LLM) has demonstrated remarkable performance in both few-shot and zero-shot learning settings Brown et al. (2020); OpenAI (2023). It can be argued that few-shot and</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 4: Short-term forecasting task on M4. The prediction lengths are in [6, 48] and results are weighted averaged from several datasets under different sample intervals. Black: best, Red: second best. Appendix H. 9 shows the full results.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Methods</th>
<th style="text-align: center;">GPT2(6)</th>
<th style="text-align: center;">TimesNet</th>
<th style="text-align: center;">PatchTST</th>
<th style="text-align: center;">N-HiTS</th>
<th style="text-align: center;">N-BEATS</th>
<th style="text-align: center;">ETSformer</th>
<th style="text-align: center;">LightTS</th>
<th style="text-align: center;">DLinear</th>
<th style="text-align: center;">FEDformer</th>
<th style="text-align: center;">Stationary</th>
<th style="text-align: center;">Autoformer</th>
<th style="text-align: center;">Informer</th>
<th style="text-align: center;">Reformer</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SMAPE</td>
<td style="text-align: center;">11.991</td>
<td style="text-align: center;">$\mathbf{1 1 . 8 2 9}$</td>
<td style="text-align: center;">12.059</td>
<td style="text-align: center;">11.927</td>
<td style="text-align: center;">11.851</td>
<td style="text-align: center;">14.718</td>
<td style="text-align: center;">13.525</td>
<td style="text-align: center;">13.639</td>
<td style="text-align: center;">12.840</td>
<td style="text-align: center;">12.780</td>
<td style="text-align: center;">12.909</td>
<td style="text-align: center;">14.086</td>
<td style="text-align: center;">18.200</td>
</tr>
<tr>
<td style="text-align: left;">MASE</td>
<td style="text-align: center;">1.600</td>
<td style="text-align: center;">$\mathbf{1 . 5 8 5}$</td>
<td style="text-align: center;">1.623</td>
<td style="text-align: center;">1.613</td>
<td style="text-align: center;">1.599</td>
<td style="text-align: center;">2.408</td>
<td style="text-align: center;">2.111</td>
<td style="text-align: center;">2.095</td>
<td style="text-align: center;">1.701</td>
<td style="text-align: center;">1.756</td>
<td style="text-align: center;">1.771</td>
<td style="text-align: center;">2.718</td>
<td style="text-align: center;">4.223</td>
</tr>
<tr>
<td style="text-align: left;">OWA</td>
<td style="text-align: center;">0.861</td>
<td style="text-align: center;">$\mathbf{0 . 8 5 1}$</td>
<td style="text-align: center;">0.869</td>
<td style="text-align: center;">0.861</td>
<td style="text-align: center;">0.855</td>
<td style="text-align: center;">1.172</td>
<td style="text-align: center;">1.051</td>
<td style="text-align: center;">1.051</td>
<td style="text-align: center;">0.918</td>
<td style="text-align: center;">0.930</td>
<td style="text-align: center;">0.939</td>
<td style="text-align: center;">1.230</td>
<td style="text-align: center;">1.775</td>
</tr>
</tbody>
</table>
<p>zero-shot learning also represent the ultimate tasks for a universal time series forecasting model. To extensively evaluate the representation power of the GPT2(6) for time series analysis, we conduct experiments under few-shot and zero-shot learning settings.
Similar to traditional experimental settings, each time series is split into three parts: training data, validation data, and test data. For few-shot learning, only a certain percentage ( $10 \%, 5 \%$ ) timesteps of training data are used.
The results of $10 \%$ few-shot learning are shown in Table 5. Compared to TimesNet, DLinear, PatchTST and other methods, GPT2(6) FPT achieves the best performance. Traditionally, CNNbased and single MLP-based models are considered more data-efficient for training and suitable for few-shot learning methods. In comparison to convolution-based TimesNet and MLP-based DLinear models, GPT2(6) FPT demonstrates a relative average MSE reduction of $\mathbf{3 3 . 3 \%}$ and $\mathbf{1 3 . 5 \%}$ respectively. We add a comparison with traditional algorithms (ETS, ARIMA, NaiveDrift) in the Appendix D. 5 as well, and GTP2(6)FPT also surpass all those traditional methods.</p>
<p>Table 5: Few-shot learning task on $10 \%$ data. All the results are averaged from 4 different prediction lengths ( ${96,192,336,720})$. Black: best, Red: second best. Appendix D. 2 shows the detailed results of $10 \%$ and $5 \%$ data.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Methods</th>
<th style="text-align: center;">GPT2(6)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">TimesNet</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">DLinear</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">FEDformer</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">PatchTST</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Autoformer</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Stationary</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ETSformer</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">LightTS</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Informer</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Reformer</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Weather</td>
<td style="text-align: center;">0.238</td>
<td style="text-align: center;">0.275</td>
<td style="text-align: center;">0.279</td>
<td style="text-align: center;">0.301</td>
<td style="text-align: center;">0.301</td>
<td style="text-align: center;">0.283</td>
<td style="text-align: center;">0.284</td>
<td style="text-align: center;">0.324</td>
<td style="text-align: center;">0.241</td>
<td style="text-align: center;">0.279</td>
<td style="text-align: center;">0.300</td>
<td style="text-align: center;">0.342</td>
<td style="text-align: center;">0.318</td>
<td style="text-align: center;">0.322</td>
<td style="text-align: center;">0.317</td>
<td style="text-align: center;">0.359</td>
<td style="text-align: center;">0.289</td>
<td style="text-align: center;">0.322</td>
<td style="text-align: center;">0.597</td>
<td style="text-align: center;">0.494</td>
<td style="text-align: center;">0.545</td>
</tr>
<tr>
<td style="text-align: center;">ETTb1</td>
<td style="text-align: center;">0.590</td>
<td style="text-align: center;">0.524</td>
<td style="text-align: center;">0.869</td>
<td style="text-align: center;">0.628</td>
<td style="text-align: center;">0.691</td>
<td style="text-align: center;">0.599</td>
<td style="text-align: center;">0.638</td>
<td style="text-align: center;">0.561</td>
<td style="text-align: center;">0.633</td>
<td style="text-align: center;">0.542</td>
<td style="text-align: center;">0.701</td>
<td style="text-align: center;">0.596</td>
<td style="text-align: center;">0.914</td>
<td style="text-align: center;">0.639</td>
<td style="text-align: center;">1.179</td>
<td style="text-align: center;">0.833</td>
<td style="text-align: center;">1.375</td>
<td style="text-align: center;">0.877</td>
<td style="text-align: center;">1.199</td>
<td style="text-align: center;">0.808</td>
<td style="text-align: center;">1.249</td>
</tr>
<tr>
<td style="text-align: center;">ETTb2</td>
<td style="text-align: center;">0.397</td>
<td style="text-align: center;">0.421</td>
<td style="text-align: center;">0.479</td>
<td style="text-align: center;">0.465</td>
<td style="text-align: center;">0.608</td>
<td style="text-align: center;">0.538</td>
<td style="text-align: center;">0.466</td>
<td style="text-align: center;">0.475</td>
<td style="text-align: center;">0.415</td>
<td style="text-align: center;">0.431</td>
<td style="text-align: center;">0.488</td>
<td style="text-align: center;">0.499</td>
<td style="text-align: center;">0.461</td>
<td style="text-align: center;">0.454</td>
<td style="text-align: center;">0.893</td>
<td style="text-align: center;">0.713</td>
<td style="text-align: center;">2.655</td>
<td style="text-align: center;">1.159</td>
<td style="text-align: center;">3.871</td>
<td style="text-align: center;">1.512</td>
<td style="text-align: center;">3.485</td>
</tr>
<tr>
<td style="text-align: center;">ETTm1</td>
<td style="text-align: center;">0.464</td>
<td style="text-align: center;">0.441</td>
<td style="text-align: center;">0.676</td>
<td style="text-align: center;">0.537</td>
<td style="text-align: center;">0.411</td>
<td style="text-align: center;">0.429</td>
<td style="text-align: center;">0.721</td>
<td style="text-align: center;">0.605</td>
<td style="text-align: center;">0.501</td>
<td style="text-align: center;">0.466</td>
<td style="text-align: center;">0.802</td>
<td style="text-align: center;">0.628</td>
<td style="text-align: center;">0.797</td>
<td style="text-align: center;">0.577</td>
<td style="text-align: center;">0.979</td>
<td style="text-align: center;">0.714</td>
<td style="text-align: center;">0.970</td>
<td style="text-align: center;">0.704</td>
<td style="text-align: center;">1.192</td>
<td style="text-align: center;">0.820</td>
<td style="text-align: center;">1.425</td>
</tr>
<tr>
<td style="text-align: center;">ETTm2</td>
<td style="text-align: center;">0.293</td>
<td style="text-align: center;">0.335</td>
<td style="text-align: center;">0.319</td>
<td style="text-align: center;">0.353</td>
<td style="text-align: center;">0.316</td>
<td style="text-align: center;">0.368</td>
<td style="text-align: center;">0.463</td>
<td style="text-align: center;">0.488</td>
<td style="text-align: center;">0.296</td>
<td style="text-align: center;">0.343</td>
<td style="text-align: center;">1.341</td>
<td style="text-align: center;">0.930</td>
<td style="text-align: center;">0.332</td>
<td style="text-align: center;">0.366</td>
<td style="text-align: center;">0.447</td>
<td style="text-align: center;">0.487</td>
<td style="text-align: center;">0.987</td>
<td style="text-align: center;">0.755</td>
<td style="text-align: center;">3.369</td>
<td style="text-align: center;">1.439</td>
<td style="text-align: center;">3.977</td>
</tr>
<tr>
<td style="text-align: center;">ECL</td>
<td style="text-align: center;">0.176</td>
<td style="text-align: center;">0.269</td>
<td style="text-align: center;">0.323</td>
<td style="text-align: center;">0.392</td>
<td style="text-align: center;">0.180</td>
<td style="text-align: center;">0.280</td>
<td style="text-align: center;">0.346</td>
<td style="text-align: center;">0.428</td>
<td style="text-align: center;">0.180</td>
<td style="text-align: center;">0.269</td>
<td style="text-align: center;">0.431</td>
<td style="text-align: center;">0.478</td>
<td style="text-align: center;">0.443</td>
<td style="text-align: center;">0.479</td>
<td style="text-align: center;">0.659</td>
<td style="text-align: center;">0.617</td>
<td style="text-align: center;">0.441</td>
<td style="text-align: center;">0.488</td>
<td style="text-align: center;">1.194</td>
<td style="text-align: center;">0.890</td>
<td style="text-align: center;">0.965</td>
</tr>
<tr>
<td style="text-align: center;">Traffic</td>
<td style="text-align: center;">0.440</td>
<td style="text-align: center;">0.309</td>
<td style="text-align: center;">0.951</td>
<td style="text-align: center;">0.535</td>
<td style="text-align: center;">0.496</td>
<td style="text-align: center;">0.371</td>
<td style="text-align: center;">0.663</td>
<td style="text-align: center;">0.425</td>
<td style="text-align: center;">0.430</td>
<td style="text-align: center;">0.305</td>
<td style="text-align: center;">0.749</td>
<td style="text-align: center;">0.446</td>
<td style="text-align: center;">1.453</td>
<td style="text-align: center;">0.815</td>
<td style="text-align: center;">1.913</td>
<td style="text-align: center;">0.936</td>
<td style="text-align: center;">1.247</td>
<td style="text-align: center;">0.684</td>
<td style="text-align: center;">1.534</td>
<td style="text-align: center;">0.811</td>
<td style="text-align: center;">1.550</td>
</tr>
<tr>
<td style="text-align: center;">Average</td>
<td style="text-align: center;">0.371</td>
<td style="text-align: center;">0.367</td>
<td style="text-align: center;">0.556</td>
<td style="text-align: center;">0.458</td>
<td style="text-align: center;">0.429</td>
<td style="text-align: center;">0.409</td>
<td style="text-align: center;">0.511</td>
<td style="text-align: center;">0.472</td>
<td style="text-align: center;">0.385</td>
<td style="text-align: center;">0.376</td>
<td style="text-align: center;">0.687</td>
<td style="text-align: center;">0.559</td>
<td style="text-align: center;">0.674</td>
<td style="text-align: center;">0.522</td>
<td style="text-align: center;">0.912</td>
<td style="text-align: center;">0.665</td>
<td style="text-align: center;">1.137</td>
<td style="text-align: center;">0.712</td>
<td style="text-align: center;">1.850</td>
<td style="text-align: center;">0.967</td>
<td style="text-align: center;">1.888</td>
</tr>
</tbody>
</table>
<h1>4.8 Zero-shot forecasting</h1>
<p>This task is used to evaluate the cross datasets adaption ability of our proposed algorithm, i.e. how well a model is able to perform on dataset $A$ (without any training data from $A$ ) when it is trained from dataset $B$.
The results are summarized in Table 6. The GPT2(6) FPT model consistently outperforms all recent state-of-the-art transformer and MLP-based time series forecasting methods. Compared to recently published state-of-the-art MLP-based method Dlinear, convolution-based method Timesnet, and transformer-based method Patchtst, GPT2(6)FPT demonstrates a relative average metric reduction of $\mathbf{1 3 . 1 \%}, \mathbf{1 3 . 6 \%}$ and $\mathbf{7 . 3 \%}$, respectively. Also, the proposed method is comparable to N-BEATS without any meta-learning design and outperforms N-BEATS in the ELECTR dataset. We attribute this to the knowledge transfer capability from the FPT model.</p>
<h2>5 Ablations</h2>
<p>In this section, we conduct several ablations on model selection and effectiveness of pre-training. The detailed results are shown in Appendix H. We introduce several variants, GPT2(0) FPT, GPT2(6) without freezing and GPT2(6) without pre-training.
Model Selection We separately analyze the number of GPT2 layers and the fine-tuning parameters selection. The results in Appendix H show that GPT2 with 6-layers is a sound choice compared to full or few layers and partially freezing can avoid catastrophic forgetting, enabling fine-tuning without overfitting.</p>
<p>Table 6: Zero-shot learning results. Dataset-specific metrics aggregated over each dataset. A lower value indicates better performance. The source dataset of M3, Tourism, Electricity are M4. For M4, the source data for N-BEATS is FRED, and M3 for other models. Black: best, Red: second best, Violet: third best. Appendix D. 7 shows full results.</p>
<table>
<thead>
<tr>
<th>Methods</th>
<th>M4</th>
<th>M3</th>
<th>TOURISM</th>
<th>ELECTR</th>
<th>Average</th>
</tr>
</thead>
<tbody>
<tr>
<td>Metric</td>
<td>sMAPE</td>
<td>sMAPE</td>
<td>MAPE</td>
<td>$N D=100$</td>
<td></td>
</tr>
<tr>
<td>N-BEATS</td>
<td>$\mathbf{1 1 . 7 0}$</td>
<td>$\mathbf{1 2 . 4 4}$</td>
<td>$\mathbf{1 8 . 8 2}$</td>
<td>$\mathbf{1 7 . 8}$</td>
<td>$\mathbf{1 5 . 1 9}$</td>
</tr>
<tr>
<td>DLinear</td>
<td>15.33</td>
<td>14.03</td>
<td>28.51</td>
<td>17.6</td>
<td>18.86</td>
</tr>
<tr>
<td>TimesNet</td>
<td>13.55</td>
<td>14.17</td>
<td>28.84</td>
<td>19.3</td>
<td>18.96</td>
</tr>
<tr>
<td>PatchTST</td>
<td>$\mathbf{1 3 . 2 2}$</td>
<td>$\mathbf{1 3 . 0 6}$</td>
<td>27.10</td>
<td>$\mathbf{1 7 . 3}$</td>
<td>$\mathbf{1 7 . 6 7}$</td>
</tr>
<tr>
<td>ETSformer</td>
<td>27.74</td>
<td>16.03</td>
<td>180.40</td>
<td>44.2</td>
<td>67.09</td>
</tr>
<tr>
<td>LightTS</td>
<td>13.62</td>
<td>17.90</td>
<td>66.99</td>
<td>19.6</td>
<td>29.52</td>
</tr>
<tr>
<td>Stationary</td>
<td>13.32</td>
<td>15.29</td>
<td>43.75</td>
<td>22.0</td>
<td>23.59</td>
</tr>
<tr>
<td>FEDformer</td>
<td>15.04</td>
<td>13.53</td>
<td>31.55</td>
<td>18.4</td>
<td>19.63</td>
</tr>
<tr>
<td>Autoformer</td>
<td>20.02</td>
<td>15.87</td>
<td>40.39</td>
<td>33.9</td>
<td>27.54</td>
</tr>
<tr>
<td>Informer</td>
<td>19.04</td>
<td>15.82</td>
<td>35.82</td>
<td>21.2</td>
<td>22.97</td>
</tr>
<tr>
<td>Reformer</td>
<td>14.09</td>
<td>13.37</td>
<td>$\mathbf{2 5 . 4 8}$</td>
<td>21.6</td>
<td>18.63</td>
</tr>
<tr>
<td>GPT2(6)</td>
<td>$\mathbf{1 3 . 1 2}$</td>
<td>$\mathbf{1 3 . 0 6}$</td>
<td>$\mathbf{2 2 . 1 4}$</td>
<td>$\mathbf{1 7 . 2}$</td>
<td>$\mathbf{1 6 . 3 8}$</td>
</tr>
</tbody>
</table>
<p>Effectiveness of Pre-training The results are shown in Table 7, GPT2(6) FPT outperforms both GPT2(0) FPT and GPT2-random-initialized, suggesting that GPT2 with pre-training parameters can achieve improvement on times series tasks. Besides, GPT2(6) FPT performs better than GPT2unfrozen, demonstrating that partially freezing also helps. Also, results in Appendix H. 2 show that random initialized GPT2(6) with freezing performs poorly and the pre-trained knowledge is instrumental for time series tasks.</p>
<p>Table 7: Ablation study on 10\% data. All the results are averaged from 4 different prediction lengths. No Freeze represents GPT2(6) without freezing, No Pretrain represents GPT2(6) without pre-training. Black: best.</p>
<table>
<thead>
<tr>
<th>Methods</th>
<th>GPT2(6)</th>
<th></th>
<th>GPT2(0)</th>
<th></th>
<th>No Freeze</th>
<th></th>
<th>No Pretrain</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>MSE</td>
<td>MAE</td>
<td>MSE</td>
<td>MAE</td>
<td>MSE</td>
<td>MAE</td>
<td>MSE</td>
<td>MAE</td>
</tr>
<tr>
<td>Weather</td>
<td>$\mathbf{0 . 2 3 7}$</td>
<td>$\mathbf{0 . 2 7 0}$</td>
<td>0.263</td>
<td>0.297</td>
<td>0.273</td>
<td>0.302</td>
<td>0.277</td>
<td>0.305</td>
</tr>
<tr>
<td>ETTh1</td>
<td>$\mathbf{0 . 4 2 7}$</td>
<td>$\mathbf{0 . 4 2 6}$</td>
<td>0.874</td>
<td>0.647</td>
<td>0.753</td>
<td>0.596</td>
<td>1.326</td>
<td>0.743</td>
</tr>
<tr>
<td>ETTh2</td>
<td>$\mathbf{0 . 3 4 6}$</td>
<td>$\mathbf{0 . 3 9 4}$</td>
<td>0.666</td>
<td>0.559</td>
<td>0.447</td>
<td>0.451</td>
<td>0.502</td>
<td>0.479</td>
</tr>
</tbody>
</table>
<h1>6 Exploring Transfer Learning from others: The Unexceptional Nature of GPT2-based-FPT</h1>
<p>We also present experiments on BERT-backbond FPT Devlin et al. (2019) model and the imagepretrained BEiT-backbone FPT model Bao et al. (2022) to illustrate the generality of pre-trained models for cross-domain knowledge transferring. The results in Table 8 demonstrate that the ability of knowledge transfer is not exclusive to GPT2-based pre-trained language models. Subsequently, our theoretical analysis will shed light on the universality of this phenomenon.</p>
<p>Table 8: Results of frozen pretrained transformer variants on 5\% ETTh2 and ETTm2. All the results are averaged from 4 different prediction lengths. Black: best. Appendix H. 5 shows the full results.</p>
<p>| Methods | GPT2(6) | | BERT(6) | | BEiT(6) | | DLinear | | PatchTST | | FEDformer | | Autoformer | |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| | MSE | MAE | MSE | MAE | MSE | MAE | MSE | MAE | MSE | MAE | MSE | MAE | MSE |
| ETTh2 | $\mathbf{0 . 4 0 0}$ | $\mathbf{0 . 4 3 3}$ | 0.452 | 0.451 | 0.459 | 0.454 | 0.827 | 0.615 | 0.439 | 0.448 | 0.441 | 0.457 | 0.470 | 0.489 |
| ETTm2 | $\mathbf{0 . 3 0 8}$ | $\mathbf{0 . 3 4 6}$ | 0.318 | 0.357 | 0.315 | 0.357 | 0.399 | 0.426 | 0.314 | 0.352 | 0.381 | 0.404 | 0.388 | 0.433 |</p>
<h2>7 Training/Inferencing Cost</h2>
<p>Analysis of computational cost is helpful for investigating the practicality of the LLM-based model. The results can be found in table 9. Each baseline model comes in two variants, featuring model</p>
<p>Table 9: Training parameters and Training/Inference Cost Comparison</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Training Params</th>
<th>Training Params Percentages</th>
<th>Training Time for 1 step(s)</th>
<th>Inference Time for 1 Batch(s)</th>
</tr>
</thead>
<tbody>
<tr>
<td>FEDformer-32</td>
<td>44k</td>
<td>100</td>
<td>0.889</td>
<td>0.170</td>
</tr>
<tr>
<td>TimesNet-32</td>
<td>2M</td>
<td>100</td>
<td>0.747</td>
<td>0.302</td>
</tr>
<tr>
<td>PatchTST-32</td>
<td>543K</td>
<td>100</td>
<td>0.043</td>
<td>0.022</td>
</tr>
<tr>
<td>FEDformer-768</td>
<td>33M</td>
<td>100</td>
<td>0.208</td>
<td>0.056</td>
</tr>
<tr>
<td>TimesNet-768</td>
<td>42M</td>
<td>100</td>
<td>5.723</td>
<td>2.162</td>
</tr>
<tr>
<td>PatchTST-768</td>
<td>20M</td>
<td>100</td>
<td>0.457</td>
<td>0.123</td>
</tr>
<tr>
<td>GPT-2(3)-768</td>
<td>4M</td>
<td>6.12</td>
<td>0.093</td>
<td>0.032</td>
</tr>
<tr>
<td>GPT-2(6)-768</td>
<td>4M</td>
<td>4.6</td>
<td>0.104</td>
<td>0.054</td>
</tr>
</tbody>
</table>
<p>hidden dimensions of 32 and 768, which align with GPT-2's specifications. Furthermore, the majority of the baseline models consist of three layers. We assessed the computational cost using a batch from ETTh2 (with a batch size of 128) on a 32G V100 GPU.</p>
<p>The results indicate that GPT-2(3) has substantially enhanced time efficiency and reduced parameter quantity compared to baselines with the same model dimension. This was a surprise since we initially anticipated that this large language model might be slower. However, we surmise that the efficient optimization of huggingface's GPT model implementation primarily accounts for such a significant improvement in time costs. Furthermore, GPT-2(3) and GPT-2(6) demonstrate a mere 6.12% and 4.60% proportion of learnable parameters among the overall parameter size, respectively.</p>
<h2>8 Towards Understanding the Universality of Transformer: Connecting Self-Attention with PCA</h2>
<p>The observation, i.e. we can directly use a trained LM for time series forecasting without having to modify its model, makes us believe that the underlying model is doing something very generic and independent from texts despite it being trained from text data. Our analysis aims to show that part of this generic function can be related to PCA, as minimizing the gradient with respect to the self-attention layer seems to do something similar to PCA. In this section, we take the first step towards revealing the generality of self-attention by connecting the self-attention with principal component analysis (PCA). Moreover, when coming the question of why fine-tuning is restricted to the embedding layer and layer norm, following our hypothesis that the pre-trained LM as a whole performs something generic, partially fine-tuning any of its components may break the generic function and lead to relatively poor performance for time series analysis.</p>
<p>For each layer, we calculate and perform statistical analysis of the pairwise token similarity values. Specifically, we denote each output feature map with shape of $(b,n,d)$, where $b$ is the batch size, $n$ is the number of tokens, and $d$ is the dimension of each token feature. We calculate the cosine similarity, and the resulting pairwise similarity matrix of shape $(b,n,n)$. Next we count the number of occurrences of similarity values within each interval as a simple statistical analysis.</p>
<p>Our analysis is motivated by the observation that the within-layer token similarity increases with deeper layers in transformer. We report the layer-wise average token cosine similarity on ETTh2 dataset in Figure 4 (a, c), where we mix weights from pre-trained LM with weights randomly sampled from Gaussian distribution. Here we summarize our observations: a) in a randomly initialed GPT2 (6) model, the token similarity is low among all layers (0.1 − 0.2); b) when gradually switched to the pretrained GPT2 model, the token similarity significantly increases in the deep layers and eventually reaches more than 0.9 in the last layer. One potential explanation for the increasing token similarity is that all the token vectors are projected into the low-dimensional top eigenvector space of input patterns. To verify this idea, we further conduct experiments where we replace the self-attention module with PCA and find token similarity patterns remain unchanged according to Figure 4 (b), which further justifies the potential connection between PCA and self-attention.</p>
<p>To build the theoretical connection between PCA and self-attention, we first analyze the gradient structure of self-attention. Let $X = (x_1, \ldots, x_N)^{\top} \in \mathbb{R}^{N \times D}$ be the input pattern, and let $f(X) = (f_1(X), \ldots, f_N(x))^{\top} : \mathbb{R}^{N \times D} \mapsto \mathbb{R}^{N \times D}$ be the function for self-attention, i.e., $f_i(X) = \operatorname{softmax}(XAX^\top)X$ where $A = W_QW_K^\top \in \mathbb{R}^{D \times D}$.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: (a, c) The performance and token similarity within samples with respect to each layer with different random mixed ratio. Pre-trained parameters are mixed with random initial parameters according to certain proportions. (b) Token similarity within samples when replacing the attention with PCA.</p>
<p>Lemma 8.1. Let the Jacobian $J=\left[\frac{\partial f_{i}(X)}{\partial x_{j}}\right]<em 2="2">{i, j=1}^{N}$ represent the gradient $f(X)$ w.r.t the input pattern, then we have $|J|</em> \leq|A|<em i="1">{2} \sum</em>+\Delta$ where}^{N}\left(P_{i, i}+\frac{1}{2}\right)\left|x_{i}-\sum_{j=1}^{N} P_{i, j} x_{j}\right|^{2</p>
<p>$$
\Delta=|A|<em _neq="\neq" i="i" j="j">{2} \sum</em>+\frac{|A|}^{N} P_{i, j}\left|x_{j}-\sum_{k=1}^{N} P_{i, k} x_{k}\right|^{2<em j="1">{2}}{2} \sum</em>
$$}^{N}\left|x_{i}\right|^{2} \quad \text { and } \quad P_{i, j}=\frac{\exp \left(x_{i}^{\top} A x_{j}\right)}{\sum_{k=1}^{N} \exp \left(x_{i}^{\top} A x_{k}\right)</p>
<p>This lemma reveals an important gradient structure of $J$. The proof of essentially follows the analysis in Kim et al. (2021), and we include it in Appendix G for completeness.
Using the gradient structure revealed in Lemma 8.1, we can connect self-attention with PCA. In order to minimize the norm of gradient $|J|<em i="1">{2}$, we essentially need to make $\sum</em>$.
The theorem below shows that $A$ minimizing the objective $\sum_{i=1}^{N}\left|x_{i}-X^{\top} X A x_{i}\right|^{2}$ contains the largest $m$ eigenvectors of $X^{\top} X$ where $m$ is the rank of $A$.
Theorem 1. Let $W_{Q}$ and $W_{K}$ be matrices of size $D \times m$. Let $\lambda_{1} \geq \lambda_{2} \geq \ldots \geq \lambda_{D}$ be the eigenvalues of $X^{\top} X$ ranked in descending order, and let $v_{i} \in \mathbb{R}^{D}, i=1, \ldots, D$ be the corresponding eigenvectors. The optimal solution $A^{*}$ that minimizes $\sum_{i=1}^{N}\left|x_{i}-X^{\top} X A x_{i}\right|^{2}$ is given by $A=$ $\sum_{i=1}^{m} \frac{1}{\lambda_{i}} v_{i} v_{i}^{\top}$.
The proof of Theorem 1 can be found in Appendix G. Following Theorem 1, through the training of pushing gradient to zero, self-attention learns to perform a function closely related to PCA.}^{N}\left|x_{i}-\sum_{j=1}^{N} P_{i, j} x_{j}\right|^{2}$ small. When $A$ is small and all the input patterns are centered at 0 (i.e. $\sum_{i=1}^{N} x_{i}=0$ ), we have $\sum_{i=1}^{N}\left|x_{i}-X^{\top} P_{i,}\right|^{2} \approx \sum_{i=1}^{N}\left|x_{i}-X^{\top} X A x_{i}\right|^{2</p>
<h1>9 Conclusions</h1>
<p>In this paper, we developed a foundation model for time series analysis, based on pre-trained model from NLP or CV, that can (a) facilitate the model training for downstream tasks, and (b) provide unified framework for diverse time series analysis tasks. Our empirical studies show that the proposed method performs on par or better than the state-of-the-art approaches on almost all time series tasks. We also examine the universality of transformer by connecting self-attention with PCA, an important step towards understanding how generative models work in practice. On the other hand, we do recognize some limitations of our work: the zero-shot performance of our approach is still behind N -beat on several datasets, and our analysis of the generality of transformer is still in the early stage. Moving forward, we plan to improve the performance of our approach by exploiting the parameter efficient fine-tuning approaches which usually introduce additional structures into the pre-trained model for better adaption. To better understand the universality of transformer, we also plan to examine it from the viewpoint of n-gram language model, an approach that is taken by Elhage et al. (2021); Olsson et al. (2022). In Appendix F, we include our initial analysis along this direction.</p>
<h1>Acknowledgement</h1>
<p>We would like to express our sincere gratitude to Ziqing Ma, Qingsong Wen, Mengni Ye, and Tao Yao for their valuable suggestions and proofreading assistance throughout the development of this paper. Their insightful feedback and attention to detail greatly improved the quality and clarity of our work.</p>
<h2>References</h2>
<p>Abdulaal, A., Liu, Z., and Lancewicki, T. Practical approach to asynchronous multivariate time series anomaly detection and localization. In Proceedings of the 27th ACM SIGKDD conference on knowledge discovery \&amp; data mining, pp. 2485-2494, 2021.</p>
<p>Bagnall, A., Dau, H. A., Lines, J., Flynn, M., Large, J., Bostrom, A., Southam, P., and Keogh, E. The uea multivariate time series classification archive, 2018. arXiv preprint arXiv:1811.00075, 2018.</p>
<p>Bao, H., Wang, W., Dong, L., Liu, Q., Mohammed, O. K., Aggarwal, K., Som, S., and Wei, F. Vlmo: Unified vision-language pre-training with mixture-of-modality-experts. arXiv preprint arXiv:2111.02358, 2021.</p>
<p>Bao, H., Dong, L., Piao, S., and Wei, F. BEit: BERT pre-training of image transformers. In International Conference on Learning Representations, 2022.</p>
<p>Böse, J.-H., Flunkert, V., Gasthaus, J., Januschowski, T., Lange, D., Salinas, D., Schelter, S., Seeger, M., and Wang, Y. Probabilistic demand forecasting at scale. Proceedings of the VLDB Endowment, 10(12):1694-1705, 2017.</p>
<p>Box, G. E. and Jenkins, G. M. Some recent advances in forecasting and control. Journal of the Royal Statistical Society. Series C (Applied Statistics), 17(2):91-109, 1968.</p>
<p>Box, G. E. and Pierce, D. A. Distribution of residual autocorrelations in autoregressive-integrated moving average time series models. Journal of the American statistical Association, 65(332): $1509-1526,1970$.</p>
<p>Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T. J., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. ArXiv, abs/2005.14165, 2020.</p>
<p>Challu, C., Olivares, K. G., Oreshkin, B. N., Garza, F., Mergenthaler, M., and Dubrawski, A. N-hits: Neural hierarchical interpolation for time series forecasting. arXiv preprint arXiv:2201.12886, 2022.</p>
<p>Chen, T. and Guestrin, C. Xgboost: A scalable tree boosting system. KDD '16, 2016.
Chung, J., Gulcehre, C., Cho, K., and Bengio, Y. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.</p>
<p>Courty, P. and Li, H. Timing of seasonal sales. The Journal of Business, 72(4):545-572, 1999.
Dempster, A., Petitjean, F., and Webb, G. I. ROCKET: Exceptionally fast and accurate time series classification using random convolutional kernels. Data Mining and Knowledge Discovery, 34(5): $1454-1495,2020$.</p>
<p>Devlin, J., Chang, M., Lee, K., and Toutanova, K. BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), Minneapolis, MN, USA, June 2-7, 2019, pp. 4171-4186, 2019.</p>
<p>Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An image is worth 16x16 words: Transformers for image recognition at scale. In 9th International Conference on Learning Representations (ICLR), Austria, May 3-7, 2021, 2021.</p>
<p>Elhage, N., Nanda, N., Olsson, C., Henighan, T., Joseph, N., Mann, B., Askell, A., Bai, Y., Chen, A., Conerly, T., DasSarma, N., Drain, D., Ganguli, D., Hatfield-Dodds, Z., Hernandez, D., Jones, A., Kernion, J., Lovitt, L., Ndousse, K., Amodei, D., Brown, T., Clark, J., Kaplan, J., McCandlish, S., and Olah, C. A mathematical framework for transformer circuits. Transformer Circuits Thread, 2021. https://transformer-circuits.pub/2021/framework/index.html.</p>
<p>Franceschi, J.-Y., Dieuleveut, A., and Jaggi, M. Unsupervised scalable representation learning for multivariate time series. Advances in neural information processing systems, 32, 2019.</p>
<p>Friedman, M. The interpolation of time series by related series. J. Amer. Statist. Assoc, 1962.
Gao, J., Song, X., Wen, Q., Wang, P., Sun, L., and Xu, H. RobustTAD: Robust time series anomaly detection via decomposition and convolutional neural networks. KDD Workshop on Mining and Learning from Time Series (KDD-MileTS'20), 2020.</p>
<p>Giannou, A., Rajput, S., Sohn, J.-y., Lee, K., Lee, J. D., and Papailiopoulos, D. Looped Transformers as Programmable Computers. arXiv e-prints, art. arXiv:2301.13196, January 2023. doi: 10.48550/ arXiv. 2301.13196 .</p>
<p>Godahewa, R., Bergmeir, C., Webb, G. I., Hyndman, R. J., and Montero-Manso, P. Monash time series forecasting archive. In Neural Information Processing Systems Track on Datasets and Benchmarks, 2021.</p>
<p>Gu, A., Goel, K., and Ré, C. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396, 2021.</p>
<p>Hochreiter, S. and Schmidhuber, J. Long short-term memory. Neural computation, 9(8):1735-1780, 1997.</p>
<p>Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A., Attariyan, M., and Gelly, S. Parameter-efficient transfer learning for nlp. In International Conference on Machine Learning, pp. 2790-2799. PMLR, 2019.</p>
<p>Huang, Z., Shi, X., Zhang, C., Wang, Q., Cheung, K. C., Qin, H., Dai, J., and Li, H. Flowformer: A transformer architecture for optical flow. In Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XVII, pp. 668-685. Springer, 2022.</p>
<p>Hundman, K., Constantinou, V., Laporte, C., Colwell, I., and Soderstrom, T. Detecting spacecraft anomalies using lstms and nonparametric dynamic thresholding. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery \&amp; data mining, pp. 387-395, 2018.</p>
<p>Hyndman, R. and Athanasopoulos, G. Forecasting: Principles and Practice. OTexts, Australia, 3rd edition, 2021.</p>
<p>Ismail Fawaz, H., Forestier, G., Weber, J., Idoumghar, L., and Muller, P.-A. Deep learning for time series classification: a review. Data Mining and Knowledge Discovery, 33(4):917-963, 2019.</p>
<p>Kim, H., Papamakarios, G., and Mnih, A. The lipschitz constant of self-attention. In International Conference on Machine Learning, pp. 5562-5571. PMLR, 2021.</p>
<p>Kim, T., Kim, J., Tae, Y., Park, C., Choi, J.-H., and Choo, J. Reversible instance normalization for accurate time-series forecasting against distribution shift. In International Conference on Learning Representations, 2022.</p>
<p>Kitaev, N., Kaiser, L., and Levskaya, A. Reformer: The efficient transformer. arXiv preprint arXiv:2001.04451, 2020.</p>
<p>Lacoste-Julien, S., Schmidt, M., and Bach, F. A simpler approach to obtaining an o (1/t) convergence rate for the projected stochastic subgradient method. arXiv preprint arXiv:1212.2002, 2012.</p>
<p>Lai, G., Chang, W.-C., Yang, Y., and Liu, H. Modeling long-and short-term temporal patterns with deep neural networks. In The 41st international ACM SIGIR conference on research \&amp; development in information retrieval, pp. 95-104, 2018.</p>
<p>Lim, B., Arık, S. Ö., Loeff, N., and Pfister, T. Temporal fusion transformers for interpretable multi-horizon time series forecasting. International Journal of Forecasting, 2021.</p>
<p>Liu, S., Yu, H., Liao, C., Li, J., Lin, W., Liu, A. X., and Dustdar, S. Pyraformer: Low-complexity pyramidal attention for long-range time series modeling and forecasting. In International conference on learning representations, 2021.</p>
<p>Liu, Y., Wu, H., Wang, J., and Long, M. Non-stationary transformers: Exploring the stationarity in time series forecasting. In Advances in Neural Information Processing Systems, 2022.</p>
<p>Lu, K., Grover, A., Abbeel, P., and Mordatch, I. Frozen pretrained transformers as universal computation engines. Proceedings of the AAAI Conference on Artificial Intelligence, 36(7): 7628-7636, Jun. 2022.</p>
<p>Makridakis, S., Spiliotis, E., and Assimakopoulos, V. The m4 competition: Results, findings, conclusion and way forward. International Journal of Forecasting, 34(4):802-808, 2018.</p>
<p>Mathur, A. P. and Tippenhauer, N. O. Swat: A water treatment testbed for research and training on ics security. In 2016 international workshop on cyber-physical systems for smart water networks (CySWater), pp. 31-36. IEEE, 2016.</p>
<p>Nie, Y., Nguyen, N. H., Sinthong, P., and Kalagnanam, J. A time series is worth 64 words: Long-term forecasting with transformers. ArXiv, abs/2211.14730, 2022.</p>
<p>Olsson, C., Elhage, N., Nanda, N., Joseph, N., DasSarma, N., Henighan, T., Mann, B., Askell, A., Bai, Y., Chen, A., Conerly, T., Drain, D., Ganguli, D., Hatfield-Dodds, Z., Hernandez, D., Johnston, S., Jones, A., Kernion, J., Lovitt, L., Ndousse, K., Amodei, D., Brown, T., Clark, J., Kaplan, J., McCandlish, S., and Olah, C. In-context learning and induction heads. Transformer Circuits Thread, 2022. https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html.</p>
<p>OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023.
Oreshkin, B. N., Carpov, D., Chapados, N., and Bengio, Y. N-beats: Neural basis expansion analysis for interpretable time series forecasting. arXiv preprint arXiv:1905.10437, 2019.</p>
<p>Oreshkin, B. N., Carpov, D., Chapados, N., and Bengio, Y. Meta-learning framework with applications to zero-shot time-series forecasting. In Proceedings of the AAAI Conference on Artificial Intelligence, number 10, pp. 9242-9250, 2021.</p>
<p>Radford, A. and Narasimhan, K. Improving language understanding by generative pre-training. 2018.
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language models are unsupervised multitask learners. 2019.</p>
<p>Rao, Y., Zhao, W., Zhu, Z., Lu, J., and Zhou, J. Global filter networks for image classification. Advances in Neural Information Processing Systems (NeurIPS), 34, 2021.</p>
<p>Su, Y., Zhao, Y., Niu, C., Liu, R., Sun, W., and Pei, D. Robust anomaly detection for multivariate time series through stochastic recurrent neural network. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery \&amp; data mining, pp. 2828-2837, 2019.</p>
<p>Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and Jégou, H. Training data-efficient image transformers \&amp; distillation through attention. In International Conference on Machine Learning, pp. 10347-10357. PMLR, 2021.</p>
<p>Vardi, G., Yehudai, G., and Shamir, O. On the optimal memorization power of relu neural networks. arXiv preprint arXiv:2110.03187, 2021.</p>
<p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser Lukasz, and Polosukhin, I. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017.</p>
<p>Wang, P., Wang, X., Wang, F., Lin, M., Chang, S., Li, H., and Jin, R. Kvt: k-nn attention for boosting vision transformers. In Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXIV, pp. 285-302. Springer, 2022.</p>
<p>Wang, T. and Isola, P. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. ArXiv, abs/2005.10242, 2020.</p>
<p>Wen, Q., Yang, L., Zhou, T., and Sun, L. Robust time series analysis and applications: An industrial perspective. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 4836-4837, 2022.</p>
<p>Wen, Q., Zhou, T., Zhang, C., Chen, W., Ma, Z., Yan, J., and Sun, L. Transformers in time series: A survey. In International Joint Conference on Artificial Intelligence(IJCAI), 2023.</p>
<p>Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., von Platen, P., Ma, C., Jernite, Y., Plu, J., Xu, C., Scao, T. L., Gugger, S., Drame, M., Lhoest, Q., and Rush, A. M. Transformers: State-of-theart natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 38-45, Online, October 2020. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/2020. emnlp-demos. 6 .</p>
<p>Woo, G., Liu, C., Sahoo, D., Kumar, A., and Hoi, S. Etsformer: Exponential smoothing transformers for time-series forecasting. arXiv preprint arXiv:2202.01381, 2022.</p>
<p>Wu, H., Xu, J., Wang, J., and Long, M. Autoformer: Decomposition transformers with autocorrelation for long-term series forecasting. In Advances in Neural Information Processing Systems (NeurIPS), pp. 101-112, 2021.</p>
<p>Wu, H., Hu, T., Liu, Y., Zhou, H., Wang, J., and Long, M. Timesnet: Temporal 2d-variation modeling for general time series analysis. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=ju_Uqw3840q.</p>
<p>Xu, J., Wu, H., Wang, J., and Long, M. Anomaly transformer: Time series anomaly detection with association discrepancy. arXiv preprint arXiv:2110.02642, 2021.</p>
<p>Yang, C.-H. H., Tsai, Y.-Y., and Chen, P.-Y. Voice2series: Reprogramming acoustic models for time series classification. In International Conference on Machine Learning, pp. 11808-11819, 2021.</p>
<p>Yun, C., Chang, Y.-W., Bhojanapalli, S., Rawat, A. S., Reddi, S., and Kumar, S. O (n) connections are expressive enough: Universal approximability of sparse transformers. Advances in Neural Information Processing Systems, 33:13783-13794, 2020.</p>
<p>Zeng, A., Chen, M., Zhang, L., and Xu, Q. Are transformers effective for time series forecasting? 2023.</p>
<p>Zhang, T., Zhang, Y., Cao, W., Bian, J., Yi, X., Zheng, S., and Li, J. Less is more: Fast multivariate time series forecasting with light sampling-oriented mlp structures. arXiv preprint arXiv:2207.01186, 2022.</p>
<p>Zhou, H., Zhang, S., Peng, J., Zhang, S., Li, J., Xiong, H., and Zhang, W. Informer: Beyond efficient transformer for long sequence time-series forecasting. In Proceedings of AAAI, 2021.</p>
<p>Zhou, T., Ma, Z., Wen, Q., Wang, X., Sun, L., and Jin, R. FEDformer: Frequency enhanced decomposed transformer for long-term series forecasting. In Proc. 39th International Conference on Machine Learning (ICML 2022), 2022.</p>
<h1>A Visualization</h1>
<p>In order to clarify the representation ability more clearly, Figure 5 provides showcases of imputation, long-term forecasting and few-shot forecasting. Especially for few-shot learning, GPT2(6) can accurately forecast, while TimesNet and DLinear fail in this task.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Visualization of imputation, long-term forecasting and few-shot forecasting.</p>
<h2>B Related Works</h2>
<p>We have presented a novel general time series analysis model in this paper, and to the best of our knowledge, there has been limited work on similar comprehensive methods for time series analysis. The most closely related field is time series forecasting, where transformer models have gained widespread popularity. Therefore, our focus in this related work will primarily be on introducing the end-to-end time series forecasting method.</p>
<p>Time series forecasting models can be roughly divided into three categories, ranging from the classic ARIMA models to the most recent transformer models. The first generation of well-discussed models can be dated back to auto-regressive family, such as ARIMA Box \&amp; Jenkins (1968); Box \&amp; Pierce (1970) that follows the Markov process and recursively execute sequential forecasting. However, it is limited to stationary sequences while most time series is non-stationary. Additionally, with the bloom of deep neural networks, recurrent neural networks (RNNs), such as LSTM Hochreiter \&amp; Schmidhuber (1997) and GRU Chung et al. (2014), were designed for sequential tasks. Yet the recurrent model is inefficient for training and long-term dependencies are still under resolved.</p>
<p>Recently, transformer models have achieve great progress in NLP Vaswani et al. (2017); Devlin et al. (2019); Radford et al. (2019) and CV Dosovitskiy et al. (2021); Bao et al. (2022) tasks. Also, a large amount of transformer models are proposed to apply to time series forecasting Wen et al. (2023). In the following, we briefly introduce several representative algorithms. Informer Zhou et al. (2021)</p>
<p>proposes a probability sparse attention mechanism to deal with long-term dependencies. Autoformer Wu et al. (2021) introduces a decomposition transformer architecture and replaces the attention module with an Auto-Correlation mechanism. FEDformer Zhou et al. (2022) uses Fourier enhanced structure to improve computational efficiency and achieves linear complexity. Similar to patching in ViT Dosovitskiy et al. (2021), PatchTST Nie et al. (2022) employs segmentation of time series that divide a sequence into patches to increase input length and reduce information redundancy. Besides, a simple MLP-based model DLinear Zeng et al. (2023) outperforms most transformer models and it validates channel-independence works well in time series forecasting. Recently, TimesNet Wu et al. (2023) has treated time series as a 2D signal and utilized a convolution-based inception net backbone to function as a comprehensive time series analysis model. This work is closely related to our tasks in this paper.</p>
<h1>C Dataset Details</h1>
<p>In this section, we separately summarize dataset details long/short-term forecasting and few-shot/zeroshot forecasting.</p>
<p>Datasets of Long-term Forecasting and Few-shot Learning The details of datasets are shown as follows: 1) ETT datasets Zhou et al. (2021) contain electricity load of various resolutions (ETTh \&amp; ETTm) from two electricity stations. 2) Weather contains 21 meteorological indicators of Germany within 1 year; 3) Illness contains the influenza-like illness patients in the United States; 4) Electricity dataset contains the electricity consumption; 5) Traffic dataset contains the occupation rate of freeway system across the State of California. Table 10 summarizes details of feature statistics.
Similar to PatchTST Nie et al. (2022), Exchange is not contained. Zeng et al. (2023) shows that simply repeating the last value in the look-back window can outperform or be comparable to the best results. Also, ILI is not used for few-shot learning for the limited quantity that is hard to follow the definition of few-shot.</p>
<p>Table 10: Dataset details of few-shot learning.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">Length</th>
<th style="text-align: center;">Dimension</th>
<th style="text-align: center;">Frequency</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ETTh</td>
<td style="text-align: center;">17420</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">1 hour</td>
</tr>
<tr>
<td style="text-align: left;">ETTm</td>
<td style="text-align: center;">69680</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">15 min</td>
</tr>
<tr>
<td style="text-align: left;">Weather</td>
<td style="text-align: center;">52696</td>
<td style="text-align: center;">22</td>
<td style="text-align: center;">10 min</td>
</tr>
<tr>
<td style="text-align: left;">ILI</td>
<td style="text-align: center;">966</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">7 days</td>
</tr>
<tr>
<td style="text-align: left;">Electricity</td>
<td style="text-align: center;">26304</td>
<td style="text-align: center;">321</td>
<td style="text-align: center;">1 hour</td>
</tr>
<tr>
<td style="text-align: left;">Traffic</td>
<td style="text-align: center;">17544</td>
<td style="text-align: center;">862</td>
<td style="text-align: center;">1 hour</td>
</tr>
</tbody>
</table>
<p>Datasets of Short-term Forecasting and Zero-shot Learning The details of short-term forecasting and zero-shot learning datasets are shown as follows: 1) M4 is a large and diverse dataset that contains time series of various frequencies and fields, including business, financial and economic forecasting; 2) M3 is smaller than M4, but also contains time series from diverse domains and frequencies; 3) TOURISM is the dataset of tourism activities with different frequencies and contains a much higher fraction of erratic series compared with M4; 4) ELECTR represents the electricity usage monitoring of 370 customers over three years. Table 6 summarizes details of the datasets and zero-shot mapping between source and target.</p>
<h2>D Experimental Details</h2>
<p>All the deep learning networks are implemented in PyTorch and trained on NVIDIA V100 32GB GPUs. We use the pre-trained models from Wolf et al. (2020) for experiments. For few-shot learning, an early stopping counter is employed to stop the training process after three epochs if no loss degradation on the valid set is observed. Plus, we convert the multivariate data into univariate data. Specifically, we treat each feature of the sequence as a single time series. This is mainly for memory efficiency after patching of GPT2(6) and previous works, DLinear and PatchTST, have proved the effectiveness of channel-independence.</p>
<p>Table 11: Datasets and mapping details of zero-shot learning.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Mapping</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Length</td>
<td style="text-align: center;">Horizon</td>
<td style="text-align: center;">M4</td>
<td style="text-align: center;">M3</td>
</tr>
<tr>
<td style="text-align: center;">M3 Yearly</td>
<td style="text-align: center;">645</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">Yearly</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">M3 Quarterly</td>
<td style="text-align: center;">756</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">Quarterly</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">M3 Monthly</td>
<td style="text-align: center;">1428</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">Monthly</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">M3 Others</td>
<td style="text-align: center;">174</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">Monthly</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">M4 Yearly</td>
<td style="text-align: center;">23000</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Yearly</td>
</tr>
<tr>
<td style="text-align: center;">M4 Quarterly</td>
<td style="text-align: center;">24000</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Quarterly</td>
</tr>
<tr>
<td style="text-align: center;">M4 Monthly</td>
<td style="text-align: center;">48000</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Monthly</td>
</tr>
<tr>
<td style="text-align: center;">M4 Weekly</td>
<td style="text-align: center;">359</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Monthly</td>
</tr>
<tr>
<td style="text-align: center;">M4 Daily</td>
<td style="text-align: center;">4227</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Monthly</td>
</tr>
<tr>
<td style="text-align: center;">M4 Hourly</td>
<td style="text-align: center;">414</td>
<td style="text-align: center;">48</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Monthly</td>
</tr>
<tr>
<td style="text-align: center;">TOURISM Yearly</td>
<td style="text-align: center;">518</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">Yearly</td>
<td style="text-align: center;">Yearly</td>
</tr>
<tr>
<td style="text-align: center;">TOURISM Quarterly</td>
<td style="text-align: center;">427</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">Quarterly</td>
<td style="text-align: center;">Quarterly</td>
</tr>
<tr>
<td style="text-align: center;">TOURISM Monthly</td>
<td style="text-align: center;">366</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">Monthly</td>
<td style="text-align: center;">Monthly</td>
</tr>
<tr>
<td style="text-align: center;">ELECTR</td>
<td style="text-align: center;">1311</td>
<td style="text-align: center;">168</td>
<td style="text-align: center;">Hourly</td>
<td style="text-align: center;">Monthly</td>
</tr>
</tbody>
</table>
<h1>D. 1 Accuracy Metrics</h1>
<p>For long-term/short-term forecasting and few-shot forecasting, we use mean square error (MSE) and mean absolute error (MAE) as metrics. For zero-shot learning, mean absolute percentage error (MAPE) is used for TOURISM; symmetric MAPE (sMAPE) is used for M3 and M4; normalized deviation (ND) is used for ELECTR. All experiments are repeated 3 times and the mean of the metrics is used in the final results.</p>
<h2>D. 2 Detailed Definition and Results for Few-shot and Long-term Forecasting</h2>
<p>Task Definition Since Zeng et al. (2023) and Nie et al. (2022) have verified that channel-independence works well for time series datasets, we treat each multivariate series as multiple independent univariate series. Similar to traditional experimental settings, each time series is split into three parts: training data, validation data, and test data. For the few-shot forecasting task, only a certain percentage ( $5 \%$, $10 \%$ ) timesteps of training data are used, and the other two parts remain unchanged. The evaluation metrics remain the same as for classic multivariate time series forecasting. We repeat this experiment 3 times and report the average metrics in the following experiments.
Detail Experiment Tables for Few-shot Time-Series Forecasting in Table 12 and Table 13</p>
<p>Table 12: Few-shot learning results on 5% data. We use prediction length $O \in{96,192,336,720}$. A lower MSE indicates better performance, and the best results are highlighted in bold. ' - ' means that 5% time series is not sufficient to constitute a training set.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Methods</th>
<th style="text-align: center;">GPT2(6)</th>
<th style="text-align: center;">GPT2(0)</th>
<th style="text-align: center;">DLinear</th>
<th style="text-align: center;">PatchTST</th>
<th style="text-align: center;">TimesNet</th>
<th style="text-align: center;">FEDformer</th>
<th style="text-align: center;">Autoformer</th>
<th style="text-align: center;">Stationary</th>
<th style="text-align: center;">ETSformer</th>
<th style="text-align: center;">LightTS</th>
<th style="text-align: center;">Informer</th>
<th style="text-align: center;">Reformer</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Metric</td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
</tr>
<tr>
<td style="text-align: center;">96</td>
<td style="text-align: center;">0.175</td>
<td style="text-align: center;">0.230</td>
<td style="text-align: center;">0.191</td>
<td style="text-align: center;">0.243</td>
<td style="text-align: center;">0.184</td>
<td style="text-align: center;">0.242</td>
<td style="text-align: center;">0.171</td>
<td style="text-align: center;">0.224</td>
<td style="text-align: center;">0.207</td>
<td style="text-align: center;">0.253</td>
<td style="text-align: center;">0.229</td>
<td style="text-align: center;">0.309</td>
</tr>
<tr>
<td style="text-align: center;">192</td>
<td style="text-align: center;">0.227</td>
<td style="text-align: center;">0.276</td>
<td style="text-align: center;">0.244</td>
<td style="text-align: center;">0.289</td>
<td style="text-align: center;">0.228</td>
<td style="text-align: center;">0.283</td>
<td style="text-align: center;">0.230</td>
<td style="text-align: center;">0.277</td>
<td style="text-align: center;">0.272</td>
<td style="text-align: center;">0.307</td>
<td style="text-align: center;">0.265</td>
<td style="text-align: center;">0.317</td>
</tr>
<tr>
<td style="text-align: center;">336</td>
<td style="text-align: center;">0.286</td>
<td style="text-align: center;">0.322</td>
<td style="text-align: center;">0.303</td>
<td style="text-align: center;">0.332</td>
<td style="text-align: center;">0.279</td>
<td style="text-align: center;">0.322</td>
<td style="text-align: center;">0.294</td>
<td style="text-align: center;">0.326</td>
<td style="text-align: center;">0.313</td>
<td style="text-align: center;">0.328</td>
<td style="text-align: center;">0.353</td>
<td style="text-align: center;">0.392</td>
</tr>
<tr>
<td style="text-align: center;">720</td>
<td style="text-align: center;">0.366</td>
<td style="text-align: center;">0.379</td>
<td style="text-align: center;">0.391</td>
<td style="text-align: center;">0.393</td>
<td style="text-align: center;">0.364</td>
<td style="text-align: center;">0.388</td>
<td style="text-align: center;">0.384</td>
<td style="text-align: center;">0.387</td>
<td style="text-align: center;">0.400</td>
<td style="text-align: center;">0.385</td>
<td style="text-align: center;">0.391</td>
<td style="text-align: center;">0.394</td>
</tr>
<tr>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">0.263</td>
<td style="text-align: center;">0.301</td>
<td style="text-align: center;">0.282</td>
<td style="text-align: center;">0.314</td>
<td style="text-align: center;">0.263</td>
<td style="text-align: center;">0.308</td>
<td style="text-align: center;">0.269</td>
<td style="text-align: center;">0.303</td>
<td style="text-align: center;">0.298</td>
<td style="text-align: center;">0.318</td>
<td style="text-align: center;">0.309</td>
<td style="text-align: center;">0.353</td>
</tr>
<tr>
<td style="text-align: center;">96</td>
<td style="text-align: center;">0.543</td>
<td style="text-align: center;">0.506</td>
<td style="text-align: center;">0.825</td>
<td style="text-align: center;">0.638</td>
<td style="text-align: center;">0.547</td>
<td style="text-align: center;">0.503</td>
<td style="text-align: center;">0.557</td>
<td style="text-align: center;">0.519</td>
<td style="text-align: center;">0.892</td>
<td style="text-align: center;">0.625</td>
<td style="text-align: center;">0.593</td>
<td style="text-align: center;">0.529</td>
</tr>
<tr>
<td style="text-align: center;">192</td>
<td style="text-align: center;">0.748</td>
<td style="text-align: center;">0.580</td>
<td style="text-align: center;">1.220</td>
<td style="text-align: center;">0.778</td>
<td style="text-align: center;">0.720</td>
<td style="text-align: center;">0.604</td>
<td style="text-align: center;">0.711</td>
<td style="text-align: center;">0.570</td>
<td style="text-align: center;">0.940</td>
<td style="text-align: center;">0.665</td>
<td style="text-align: center;">0.652</td>
<td style="text-align: center;">0.563</td>
</tr>
<tr>
<td style="text-align: center;">336</td>
<td style="text-align: center;">0.754</td>
<td style="text-align: center;">0.595</td>
<td style="text-align: center;">1.852</td>
<td style="text-align: center;">0.965</td>
<td style="text-align: center;">0.984</td>
<td style="text-align: center;">0.727</td>
<td style="text-align: center;">0.816</td>
<td style="text-align: center;">0.619</td>
<td style="text-align: center;">0.945</td>
<td style="text-align: center;">0.653</td>
<td style="text-align: center;">0.731</td>
<td style="text-align: center;">0.594</td>
</tr>
<tr>
<td style="text-align: center;">720</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">0.681</td>
<td style="text-align: center;">0.560</td>
<td style="text-align: center;">1.299</td>
<td style="text-align: center;">0.793</td>
<td style="text-align: center;">0.750</td>
<td style="text-align: center;">0.611</td>
<td style="text-align: center;">0.694</td>
<td style="text-align: center;">0.569</td>
<td style="text-align: center;">0.925</td>
<td style="text-align: center;">0.647</td>
<td style="text-align: center;">0.658</td>
<td style="text-align: center;">0.562</td>
</tr>
<tr>
<td style="text-align: center;">96</td>
<td style="text-align: center;">0.376</td>
<td style="text-align: center;">0.421</td>
<td style="text-align: center;">0.551</td>
<td style="text-align: center;">0.507</td>
<td style="text-align: center;">0.442</td>
<td style="text-align: center;">0.456</td>
<td style="text-align: center;">0.401</td>
<td style="text-align: center;">0.421</td>
<td style="text-align: center;">0.409</td>
<td style="text-align: center;">0.420</td>
<td style="text-align: center;">0.390</td>
<td style="text-align: center;">0.424</td>
</tr>
<tr>
<td style="text-align: center;">192</td>
<td style="text-align: center;">0.418</td>
<td style="text-align: center;">0.441</td>
<td style="text-align: center;">0.765</td>
<td style="text-align: center;">0.610</td>
<td style="text-align: center;">0.617</td>
<td style="text-align: center;">0.542</td>
<td style="text-align: center;">0.452</td>
<td style="text-align: center;">0.455</td>
<td style="text-align: center;">0.483</td>
<td style="text-align: center;">0.464</td>
<td style="text-align: center;">0.457</td>
<td style="text-align: center;">0.465</td>
</tr>
<tr>
<td style="text-align: center;">336</td>
<td style="text-align: center;">0.408</td>
<td style="text-align: center;">0.439</td>
<td style="text-align: center;">0.767</td>
<td style="text-align: center;">0.614</td>
<td style="text-align: center;">1.424</td>
<td style="text-align: center;">0.849</td>
<td style="text-align: center;">0.464</td>
<td style="text-align: center;">0.469</td>
<td style="text-align: center;">0.499</td>
<td style="text-align: center;">0.479</td>
<td style="text-align: center;">0.477</td>
<td style="text-align: center;">0.483</td>
</tr>
<tr>
<td style="text-align: center;">720</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">0.400</td>
<td style="text-align: center;">0.433</td>
<td style="text-align: center;">0.694</td>
<td style="text-align: center;">0.577</td>
<td style="text-align: center;">0.827</td>
<td style="text-align: center;">0.615</td>
<td style="text-align: center;">0.439</td>
<td style="text-align: center;">0.448</td>
<td style="text-align: center;">0.463</td>
<td style="text-align: center;">0.454</td>
<td style="text-align: center;">0.441</td>
<td style="text-align: center;">0.457</td>
</tr>
<tr>
<td style="text-align: center;">96</td>
<td style="text-align: center;">0.386</td>
<td style="text-align: center;">0.405</td>
<td style="text-align: center;">0.582</td>
<td style="text-align: center;">0.512</td>
<td style="text-align: center;">0.332</td>
<td style="text-align: center;">0.374</td>
<td style="text-align: center;">0.399</td>
<td style="text-align: center;">0.414</td>
<td style="text-align: center;">0.606</td>
<td style="text-align: center;">0.518</td>
<td style="text-align: center;">0.628</td>
<td style="text-align: center;">0.544</td>
</tr>
<tr>
<td style="text-align: center;">192</td>
<td style="text-align: center;">0.440</td>
<td style="text-align: center;">0.438</td>
<td style="text-align: center;">0.632</td>
<td style="text-align: center;">0.536</td>
<td style="text-align: center;">0.358</td>
<td style="text-align: center;">0.390</td>
<td style="text-align: center;">0.441</td>
<td style="text-align: center;">0.436</td>
<td style="text-align: center;">0.681</td>
<td style="text-align: center;">0.539</td>
<td style="text-align: center;">0.666</td>
<td style="text-align: center;">0.566</td>
</tr>
<tr>
<td style="text-align: center;">336</td>
<td style="text-align: center;">0.485</td>
<td style="text-align: center;">0.459</td>
<td style="text-align: center;">0.767</td>
<td style="text-align: center;">0.584</td>
<td style="text-align: center;">0.402</td>
<td style="text-align: center;">0.416</td>
<td style="text-align: center;">0.499</td>
<td style="text-align: center;">0.467</td>
<td style="text-align: center;">0.786</td>
<td style="text-align: center;">0.597</td>
<td style="text-align: center;">0.807</td>
<td style="text-align: center;">0.628</td>
</tr>
<tr>
<td style="text-align: center;">720</td>
<td style="text-align: center;">0.577</td>
<td style="text-align: center;">0.499</td>
<td style="text-align: center;">1.334</td>
<td style="text-align: center;">0.742</td>
<td style="text-align: center;">0.511</td>
<td style="text-align: center;">0.489</td>
<td style="text-align: center;">0.767</td>
<td style="text-align: center;">0.587</td>
<td style="text-align: center;">0.796</td>
<td style="text-align: center;">0.593</td>
<td style="text-align: center;">0.822</td>
<td style="text-align: center;">0.633</td>
</tr>
<tr>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">0.472</td>
<td style="text-align: center;">0.450</td>
<td style="text-align: center;">0.828</td>
<td style="text-align: center;">0.593</td>
<td style="text-align: center;">0.400</td>
<td style="text-align: center;">0.417</td>
<td style="text-align: center;">0.526</td>
<td style="text-align: center;">0.476</td>
<td style="text-align: center;">0.717</td>
<td style="text-align: center;">0.561</td>
<td style="text-align: center;">0.730</td>
<td style="text-align: center;">0.592</td>
</tr>
<tr>
<td style="text-align: center;">96</td>
<td style="text-align: center;">0.199</td>
<td style="text-align: center;">0.280</td>
<td style="text-align: center;">0.282</td>
<td style="text-align: center;">0.347</td>
<td style="text-align: center;">0.236</td>
<td style="text-align: center;">0.326</td>
<td style="text-align: center;">0.206</td>
<td style="text-align: center;">0.288</td>
<td style="text-align: center;">0.220</td>
<td style="text-align: center;">0.299</td>
<td style="text-align: center;">0.229</td>
<td style="text-align: center;">0.320</td>
</tr>
<tr>
<td style="text-align: center;">192</td>
<td style="text-align: center;">0.256</td>
<td style="text-align: center;">0.316</td>
<td style="text-align: center;">0.346</td>
<td style="text-align: center;">0.383</td>
<td style="text-align: center;">0.306</td>
<td style="text-align: center;">0.373</td>
<td style="text-align: center;">0.264</td>
<td style="text-align: center;">0.324</td>
<td style="text-align: center;">0.311</td>
<td style="text-align: center;">0.361</td>
<td style="text-align: center;">0.394</td>
<td style="text-align: center;">0.361</td>
</tr>
<tr>
<td style="text-align: center;">336</td>
<td style="text-align: center;">0.318</td>
<td style="text-align: center;">0.353</td>
<td style="text-align: center;">0.429</td>
<td style="text-align: center;">0.427</td>
<td style="text-align: center;">0.380</td>
<td style="text-align: center;">0.423</td>
<td style="text-align: center;">0.334</td>
<td style="text-align: center;">0.367</td>
<td style="text-align: center;">0.338</td>
<td style="text-align: center;">0.366</td>
<td style="text-align: center;">0.378</td>
<td style="text-align: center;">0.427</td>
</tr>
<tr>
<td style="text-align: center;">720</td>
<td style="text-align: center;">0.460</td>
<td style="text-align: center;">0.436</td>
<td style="text-align: center;">0.751</td>
<td style="text-align: center;">0.568</td>
<td style="text-align: center;">0.674</td>
<td style="text-align: center;">0.583</td>
<td style="text-align: center;">0.454</td>
<td style="text-align: center;">0.432</td>
<td style="text-align: center;">0.509</td>
<td style="text-align: center;">0.465</td>
<td style="text-align: center;">0.523</td>
<td style="text-align: center;">0.510</td>
</tr>
<tr>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">0.308</td>
<td style="text-align: center;">0.346</td>
<td style="text-align: center;">0.452</td>
<td style="text-align: center;">0.431</td>
<td style="text-align: center;">0.399</td>
<td style="text-align: center;">0.426</td>
<td style="text-align: center;">0.314</td>
<td style="text-align: center;">0.352</td>
<td style="text-align: center;">0.344</td>
<td style="text-align: center;">0.372</td>
<td style="text-align: center;">0.381</td>
<td style="text-align: center;">0.404</td>
</tr>
<tr>
<td style="text-align: center;">96</td>
<td style="text-align: center;">0.143</td>
<td style="text-align: center;">0.241</td>
<td style="text-align: center;">0.147</td>
<td style="text-align: center;">0.246</td>
<td style="text-align: center;">0.150</td>
<td style="text-align: center;">0.251</td>
<td style="text-align: center;">0.145</td>
<td style="text-align: center;">0.244</td>
<td style="text-align: center;">0.315</td>
<td style="text-align: center;">0.389</td>
<td style="text-align: center;">0.235</td>
<td style="text-align: center;">0.322</td>
</tr>
<tr>
<td style="text-align: center;">192</td>
<td style="text-align: center;">0.159</td>
<td style="text-align: center;">0.255</td>
<td style="text-align: center;">0.163</td>
<td style="text-align: center;">0.260</td>
<td style="text-align: center;">0.163</td>
<td style="text-align: center;">0.263</td>
<td style="text-align: center;">0.163</td>
<td style="text-align: center;">0.260</td>
<td style="text-align: center;">0.318</td>
<td style="text-align: center;">0.396</td>
<td style="text-align: center;">0.247</td>
<td style="text-align: center;">0.341</td>
</tr>
<tr>
<td style="text-align: center;">336</td>
<td style="text-align: center;">0.179</td>
<td style="text-align: center;">0.274</td>
<td style="text-align: center;">0.182</td>
<td style="text-align: center;">0.278</td>
<td style="text-align: center;">0.175</td>
<td style="text-align: center;">0.278</td>
<td style="text-align: center;">0.183</td>
<td style="text-align: center;">0.281</td>
<td style="text-align: center;">0.340</td>
<td style="text-align: center;">0.415</td>
<td style="text-align: center;">0.267</td>
<td style="text-align: center;">0.356</td>
</tr>
<tr>
<td style="text-align: center;">720</td>
<td style="text-align: center;">0.233</td>
<td style="text-align: center;">0.323</td>
<td style="text-align: center;">0.239</td>
<td style="text-align: center;">0.329</td>
<td style="text-align: center;">0.219</td>
<td style="text-align: center;">0.311</td>
<td style="text-align: center;">0.233</td>
<td style="text-align: center;">0.323</td>
<td style="text-align: center;">0.635</td>
<td style="text-align: center;">0.613</td>
<td style="text-align: center;">0.318</td>
<td style="text-align: center;">0.394</td>
</tr>
<tr>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">0.178</td>
<td style="text-align: center;">0.273</td>
<td style="text-align: center;">0.182</td>
<td style="text-align: center;">0.278</td>
<td style="text-align: center;">0.176</td>
<td style="text-align: center;">0.275</td>
<td style="text-align: center;">0.181</td>
<td style="text-align: center;">0.277</td>
<td style="text-align: center;">0.402</td>
<td style="text-align: center;">0.453</td>
<td style="text-align: center;">0.266</td>
<td style="text-align: center;">0.353</td>
</tr>
<tr>
<td style="text-align: center;">96</td>
<td style="text-align: center;">0.419</td>
<td style="text-align: center;">0.298</td>
<td style="text-align: center;">0.468</td>
<td style="text-align: center;">0.354</td>
<td style="text-align: center;">0.427</td>
<td style="text-align: center;">0.304</td>
<td style="text-align: center;">0.404</td>
<td style="text-align: center;">0.286</td>
<td style="text-align: center;">0.854</td>
<td style="text-align: center;">0.492</td>
<td style="text-align: center;">0.670</td>
<td style="text-align: center;">0.421</td>
</tr>
<tr>
<td style="text-align: center;">192</td>
<td style="text-align: center;">0.434</td>
<td style="text-align: center;">0.305</td>
<td style="text-align: center;">0.479</td>
<td style="text-align: center;">0.352</td>
<td style="text-align: center;">0.447</td>
<td style="text-align: center;">0.315</td>
<td style="text-align: center;">0.412</td>
<td style="text-align: center;">0.294</td>
<td style="text-align: center;">0.894</td>
<td style="text-align: center;">0.517</td>
<td style="text-align: center;">0.653</td>
<td style="text-align: center;">0.405</td>
</tr>
<tr>
<td style="text-align: center;">336</td>
<td style="text-align: center;">0.449</td>
<td style="text-align: center;">0.313</td>
<td style="text-align: center;">0.477</td>
<td style="text-align: center;">0.345</td>
<td style="text-align: center;">0.478</td>
<td style="text-align: center;">0.333</td>
<td style="text-align: center;">0.439</td>
<td style="text-align: center;">0.310</td>
<td style="text-align: center;">0.853</td>
<td style="text-align: center;">0.471</td>
<td style="text-align: center;">0.707</td>
<td style="text-align: center;">0.445</td>
</tr>
<tr>
<td style="text-align: center;">720</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">0.434</td>
<td style="text-align: center;">0.305</td>
<td style="text-align: center;">0.474</td>
<td style="text-align: center;">0.350</td>
<td style="text-align: center;">0.450</td>
<td style="text-align: center;">0.317</td>
<td style="text-align: center;">0.418</td>
<td style="text-align: center;">0.296</td>
<td style="text-align: center;">0.867</td>
<td style="text-align: center;">0.493</td>
<td style="text-align: center;">0.676</td>
<td style="text-align: center;">0.423</td>
</tr>
<tr>
<td style="text-align: center;">Average</td>
<td style="text-align: center;">0.377</td>
<td style="text-align: center;">0.375</td>
<td style="text-align: center;">0.575</td>
<td style="text-align: center;">0.465</td>
<td style="text-align: center;">0.441</td>
<td style="text-align: center;">0.413</td>
<td style="text-align: center;">0.392</td>
<td style="text-align: center;">0.383</td>
<td style="text-align: center;">0.552</td>
<td style="text-align: center;">0.464</td>
<td style="text-align: center;">0.483</td>
<td style="text-align: center;">0.445</td>
</tr>
</tbody>
</table>
<p>Table 13: Few-shot learning results on 10% data. We use prediction length $O \in{96,192,336,720}$. A lower MSE indicates better performance, and the best results are highlighted in bold. '-' means that 10% time series is not sufficient to constitute a training set.</p>
<p>| Methods | GPT2(6) | GPT2(0) | DLinear | PatchTST | TimesNet | FEDformer | Autoformer | Stationary | ETSformer | LightTS | Informer | Reformer |
| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |
| Metric | MSE | MAE | MSE | MAE | MSE | MAE | MSE | MAE | MSE | MAE | MSE | MAE | MSE | MAE | MSE | MAE | MSE | MAE | MSE | MAE |
| 96 | 0.163 | 0.215 | 0.190 | 0.240 | 0.171 | 0.224 | 0.165 | 0.215 | 0.184 | 0.230 | 0.188 | 0.253 | 0.221 | 0.297 | 0.192 | 0.234 | 0.199 | 0.272 | 0.217 | 0.269 | 0.374 | 0.401 | 0.335 | 0.380 |
| 192 | 0.210 | 0.254 | 0.243 | 0.284 | 0.215 | 0.263 | 0.210 | 0.257 | 0.245 | 0.283 | 0.250 | 0.304 | 0.270 | 0.322 | 0.269 | 0.295 | 0.279 | 0.332 | 0.259 | 0.304 | 0.552 | 0.478 | 0.522 | 0.462 |
| 336 | 0.256 | 0.292 | 0.270 | 0.305 | 0.258 | 0.299 | 0.259 | 0.297 | 0.305 | 0.321 | 0.312 | 0.346 | 0.320 | 0.351 | 0.370 | 0.357 | 0.356 | 0.386 | 0.303 | 0.334 | 0.724 | 0.541 | 0.715 | 0.535 |
| 720 | 0.321 | 0.339 | 0.348 | 0.359 | 0.320 | 0.346 | 0.332 | 0.346 | 0.381 | 0.371 | 0.387 | 0.393 | 0.390 | 0.396 | 0.441 | 0.405 | 0.437 | 0.448 | 0.377 | 0.382 | 0.739 | 0.558 | 0.611 | 0.500 |
| Avg. | 0.238 | 0.275 | 0.263 | 0.297 | 0.241 | 0.283 | 0.242 | 0.279 | 0.279 | 0.301 | 0.284 | 0.324 | 0.300 | 0.342 | 0.318 | 0.323 | 0.318 | 0.360 | 0.289 | 0.322 | 0.597 | 0.495 | 0.546 | 0.469 |
| 96 | 0.458 | 0.456 | 0.601 | 0.536 | 0.492 | 0.495 | 0.516 | 0.485 | 0.861 | 0.628 | 0.512 | 0.499 | 0.613 | 0.552 | 0.918 | 0.639 | 1.112 | 0.806 | 1.298 | 0.838 | 1.179 | 0.792 | 1.184 | 0.790 |
| 192 | 0.570 | 0.516 | 0.709 | 0.587 | 0.565 | 0.538 | 0.598 | 0.524 | 0.797 | 0.593 | 0.624 | 0.555 | 0.722 | 0.598 | 0.915 | 0.629 | 1.155 | 0.823 | 1.322 | 0.854 | 1.199 | 0.806 | 1.295 | 0.850 |
| 336 | 0.608 | 0.535 | 0.801 | 0.635 | 0.721 | 0.622 | 0.657 | 0.550 | 0.941 | 0.648 | 0.691 | 0.574 | 0.750 | 0.619 | 0.939 | 0.644 | 1.179 | 0.832 | 1.347 | 0.870 | 1.202 | 0.811 | 1.294 | 0.854 |
| 720 | 0.725 | 0.591 | 1.385 | 0.831 | 0.986 | 0.743 | 0.762 | 0.610 | 0.877 | 0.641 | 0.728 | 0.614 | 0.721 | 0.616 | 0.887 | 0.645 | 1.273 | 0.874 | 1.534 | 0.947 | 1.217 | 0.825 | 1.223 | 0.838 |
| Avg. | 0.590 | 0.525 | 0.874 | 0.647 | 0.691 | 0.600 | 0.633 | 0.542 | 0.869 | 0.628 | 0.639 | 0.561 | 0.702 | 0.596 | 0.915 | 0.639 | 1.180 | 0.834 | 1.375 | 0.877 | 1.199 | 0.809 | 1.249 | 0.833 |
| 96 | 0.331 | 0.374 | 0.539 | 0.495 | 0.357 | 0.411 | 0.353 | 0.389 | 0.378 | 0.409 | 0.382 | 0.416 | 0.413 | 0.451 | 0.389 | 0.411 | 0.678 | 0.619 | 2.022 | 1.006 | 3.837 | 1.508 | 3.788 | 1.533 |
| 192 | 0.402 | 0.411 | 0.675 | 0.555 | 0.569 | 0.519 | 0.403 | 0.414 | 0.490 | 0.467 | 0.478 | 0.474 | 0.474 | 0.477 | 0.473 | 0.455 | 0.785 | 0.666 | 2.329 | 1.104 | 3.856 | 1.513 | 3.552 | 1.483 |
| 336 | 0.406 | 0.433 | 0.718 | 0.580 | 0.671 | 0.572 | 0.426 | 0.441 | 0.537 | 0.494 | 0.504 | 0.501 | 0.547 | 0.543 | 0.507 | 0.480 | 0.839 | 0.694 | 2.453 | 1.122 | 3.952 | 1.526 | 3.395 | 1.526 |
| 720 | 0.449 | 0.464 | 0.732 | 0.605 | 0.824 | 0.648 | 0.477 | 0.480 | 0.510 | 0.491 | 0.499 | 0.509 | 0.516 | 0.523 | 0.477 | 0.472 | 1.273 | 0.874 | 3.816 | 1.407 | 3.842 | 1.503 | 3.205 | 1.401 |
| Avg. | 0.397 | 0.421 | 0.666 | 0.559 | 0.605 | 0.538 | 0.415 | 0.431 | 0.479 | 0.465 | 0.466 | 0.475 | 0.488 | 0.499 | 0.462 | 0.455 | 0.894 | 0.713 | 2.655 | 1.160 | 3.872 | 1.513 | 3.485 | 1.486 |
| 96 | 0.390 | 0.404 | 0.610 | 0.508 | 0.352 | 0.392 | 0.410 | 0.419 | 0.583 | 0.501 | 0.578 | 0.518 | 0.774 | 0.614 | 0.761 | 0.568 | 0.911 | 0.688 | 0.921 | 0.682 | 1.162 | 0.785 | 1.442 | 0.847 |
| 192 | 0.429 | 0.423 | 0.666 | 0.540 | 0.382 | 0.412 | 0.437 | 0.434 | 0.630 | 0.528 | 0.617 | 0.546 | 0.754 | 0.592 | 0.781 | 0.574 | 0.955 | 0.703 | 0.957 | 0.701 | 1.172 | 0.793 | 1.444 | 0.862 |
| 336 | 0.469 | 0.439 | 0.895 | 0.615 | 0.419 | 0.434 | 0.476 | 0.454 | 0.725 | 0.568 | 0.998 | 0.775 | 0.869 | 0.677 | 0.803 | 0.587 | 0.991 | 0.719 | 0.998 | 0.716 | 1.227 | 0.908 | 1.450 | 0.866 |
| 720 | 0.569 | 0.498 | 0.916 | 0.646 | 0.490 | 0.477 | 0.681 | 0.556 | 0.769 | 0.549 | 0.693 | 0.579 | 0.810 | 0.630 | 0.844 | 0.581 | 1.062 | 0.747 | 1.007 | 0.719 | 1.207 | 0.797 | 1.366 | 0.850 |
| Avg. | 0.464 | 0.441 | 0.772 | 0.577 | 0.411 | 0.429 | 0.501 | 0.466 | 0.677 | 0.537 | 0.722 | 0.605 | 0.802 | 0.628 | 0.797 | 0.578 | 0.980 | 0.714 | 0.971 | 0.705 | 1.192 | 0.821 | 1.426 | 0.856 |
| 96 | 0.188 | 0.269 | 0.283 | 0.344 | 0.213 | 0.303 | 0.191 | 0.274 | 0.212 | 0.285 | 0.291 | 0.399 | 0.352 | 0.454 | 0.229 | 0.308 | 0.331 | 0.430 | 0.813 | 0.688 | 3.203 | 1.407 | 4.195 | 1.628 |
| 192 | 0.251 | 0.309 | 0.353 | 0.384 | 0.278 | 0.345 | 0.252 | 0.317 | 0.270 | 0.323 | 0.307 | 0.379 | 0.694 | 0.691 | 0.291 | 0.343 | 0.400 | 0.464 | 1.008 | 0.768 | 3.112 | 1.387 | 4.042 | 1.601 |
| 336 | 0.307 | 0.346 | 0.420 | 0.422 | 0.338 | 0.385 | 0.306 | 0.353 | 0.323 | 0.353 | 0.543 | 0.559 | 2.408 | 1.407 | 0.348 | 0.376 | 0.469 | 0.498 | 1.031 | 0.775 | 3.255 | 1.421 | 3.963 | 1.585 |
| 720 | 0.426 | 0.417 | 0.553 | 0.491 | 0.436 | 0.440 | 0.433 | 0.427 | 0.474 | 0.449 | 0.712 | 0.614 | 1.913 | 1.166 | 0.461 | 0.438 | 0.589 | 0.557 | 1.096 | 0.791 | 3.909 | 1.543 | 3.711 | 1.532 |
| Avg. | 0.293 | 0.335 | 0.402 | 0.410 | 0.316 | 0.368 | 0.296 | 0.343 | 0.320 | 0.353 | 0.463 | 0.488 | 1.342 | 0.930 | 0.332 | 0.366 | 0.447 | 0.487 | 0.987 | 0.756 | 3.370 | 1.440 | 3.978 | 1.587 |
| 96 | 0.139 | 0.237 | 0.142 | 0.240 | 0.150 | 0.253 | 0.140 | 0.238 | 0.299 | 0.373 | 0.231 | 0.323 | 0.261 | 0.348 | 0.420 | 0.466 | 0.599 | 0.587 | 0.350 | 0.425 | 1.259 | 0.919 | 0.993 | 0.784 |
| 192 | 0.156 | 0.252 | 0.158 | 0.254 | 0.164 | 0.264 | 0.160 | 0.255 | 0.305 | 0.379 | 0.261 | 0.356 | 0.338 | 0.406 | 0.411 | 0.459 | 0.620 | 0.598 | 0.376 | 0.448 | 1.160 | 0.873 | 0.938 | 0.753 |
| 336 | 0.175 | 0.270 | 0.175 | 0.271 | 0.181 | 0.282 | 0.180 | 0.276 | 0.319 | 0.391 | 0.360 | 0.445 | 0.410 | 0.474 | 0.434 | 0.473 | 0.662 | 0.619 | 0.428 | 0.485 | 1.157 | 0.872 | 0.925 | 0.745 |
| 720 | 0.233 | 0.317 | 0.230 | 0.315 | 0.223 | 0.321 | 0.241 | 0.323 | 0.369 | 0.426 | 0.530 | 0.585 | 0.715 | 0.685 | 0.510 | 0.521 | 0.757 | 0.664 | 0.611 | 0.597 | 1.203 | 0.898 | 1.004 | 0.790 |
| Avg. | 0.176 | 0.269 | 0.176 | 0.270 | 0.180 | 0.280 | 0.180 | 0.273 | 0.323 | 0.392 | 0.346 | 0.427 | 0.431 | 0.478 | 0.444 | 0.480 | 0.660 | 0.617 | 0.441 | 0.489 | 1.195 | 0.891 | 0.965 | 0.768 |
| 96 | 0.414 | 0.297 | 0.478 | 0.368 | 0.419 | 0.298 | 0.403 | 0.289 | 0.719 | 0.416 | 0.639 | 0.400 | 0.672 | 0.405 | 1.412 | 0.802 | 1.643 | 0.855 | 1.157 | 0.636 | 1.557 | 0.821 | 1.527 | 0.815 |
| 192 | 0.426 | 0.301 | 0.481 | 0.363 | 0.434 | 0.305 | 0.415 | 0.296 | 0.748 | 0.428 | 0.637 | 0.416 | 0.727 | 0.424 | 1.419 | 0.806 | 1.641 | 0.854 | 1.207 | 0.661 | 1.454 | 0.765 | 1.538 | 0.817 |
| 336 | 0.434 | 0.303 | 0.488 | 0.365 | 0.449 | 0.313 | 0.426 | 0.304 | 0.853 | 0.471 | 0.655 | 0.427 | 0.749 | 0.454 | 1.443 | 0.815 | 1.711 | 0.878 | 1.334 | 0.713 | 1.521 | 0.812 | 1.550 | 0.819 |
| 720 | 0.487 | 0.337 | 0.537 | 0.386 | 0.484 | 0.336 | 0.474 | 0.331 | 1.485 | 0.825 | 0.722 | 0.456 | 0.847 | 0.499 | 1.539 | 0.837 | 2.660 | 1.157 | 1.292 | 0.726 | 1.605 | 0.846 | 1.588 | 0.833 |
| Avg. | 0.440 | 0.310 | 0.496 | 0.371 | 0.447 | 0.313 | 0.430 | 0.305 | 0.951 | 0.535 | 0.663 | 0.425 | 0.749 | 0.446 | 1.453 | 0.815 | 1.914 | 0.936 | 1.248 | 0.684 | 1.534 | 0.811 | 1.551 | 0.821 |
| Average | 0.371 | 0.367 | 0.521 | 0.447 | 0.413 | 0.401 | 0.385 | 0.376 | 0.556 | 0.458 | 0.511 | 0.472 | 0.687 | 0.559 | 0.674 | 0.522 | 0.912 | 0.665 | 1.137 | 0.712 | 1.850 | 0.967 | 1.888 | 0.974 |</p>
<h1>D. 3 Long-term Time-series Forecasting</h1>
<p>Here we investigate whether our architecture performs consistently well with more training data. Thus, we follow the classical experiment settings of Nie et al. (2022) and conduct experiments on full data. The results are shown in Table 14. Overall, GPT2(6) FPT achieves comparable performance to PatchTST, Dlinear and outperforms other baselines by a large margin. Compared with the second best transformer-based baseline method FEDformer, GPT2(6) FPT yields an overall $\mathbf{1 8 . 7 \%}$ relatively MSE reduction. It verifies the effectiveness of NLP pretrained model in time series forecasting, not limited to the few-shot setting.</p>
<h2>Detail Experiment Table for Long-term Time-Series Forecasting in table 14</h2>
<p>Table 14: Full results on full data. We use prediction length $O \in{96,192,336,720}$ for ILI and $O \in{24,36,48,60}$ for others. A lower MSE indicates better performance. Black: best, Red: second best.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Methods</th>
<th style="text-align: center;">GPT2(6)</th>
<th style="text-align: center;">GPT2(0)</th>
<th style="text-align: center;">DLinear</th>
<th style="text-align: center;">PatchTST</th>
<th style="text-align: center;">TimesNet</th>
<th style="text-align: center;">FEDformer</th>
<th style="text-align: center;">Autoformer</th>
<th style="text-align: center;">Stationary</th>
<th style="text-align: center;">ETSformer</th>
<th style="text-align: center;">LightTS</th>
<th style="text-align: center;">Informer</th>
<th style="text-align: center;">Reformer</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Metric</td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
</tr>
<tr>
<td style="text-align: center;">96</td>
<td style="text-align: center;">0.162</td>
<td style="text-align: center;">0.212</td>
<td style="text-align: center;">0.181</td>
<td style="text-align: center;">0.232</td>
<td style="text-align: center;">0.176</td>
<td style="text-align: center;">0.237</td>
<td style="text-align: center;">0.149</td>
<td style="text-align: center;">0.198</td>
<td style="text-align: center;">0.172</td>
<td style="text-align: center;">0.220</td>
<td style="text-align: center;">0.217</td>
<td style="text-align: center;">0.296</td>
</tr>
<tr>
<td style="text-align: center;">192</td>
<td style="text-align: center;">0.204</td>
<td style="text-align: center;">0.248</td>
<td style="text-align: center;">0.222</td>
<td style="text-align: center;">0.266</td>
<td style="text-align: center;">0.220</td>
<td style="text-align: center;">0.282</td>
<td style="text-align: center;">0.194</td>
<td style="text-align: center;">0.241</td>
<td style="text-align: center;">0.219</td>
<td style="text-align: center;">0.261</td>
<td style="text-align: center;">0.276</td>
<td style="text-align: center;">0.336</td>
</tr>
<tr>
<td style="text-align: center;">336</td>
<td style="text-align: center;">0.254</td>
<td style="text-align: center;">0.286</td>
<td style="text-align: center;">0.270</td>
<td style="text-align: center;">0.299</td>
<td style="text-align: center;">0.265</td>
<td style="text-align: center;">0.319</td>
<td style="text-align: center;">0.245</td>
<td style="text-align: center;">0.282</td>
<td style="text-align: center;">0.280</td>
<td style="text-align: center;">0.306</td>
<td style="text-align: center;">0.339</td>
<td style="text-align: center;">0.380</td>
</tr>
<tr>
<td style="text-align: center;">720</td>
<td style="text-align: center;">0.326</td>
<td style="text-align: center;">0.337</td>
<td style="text-align: center;">0.338</td>
<td style="text-align: center;">0.345</td>
<td style="text-align: center;">0.333</td>
<td style="text-align: center;">0.362</td>
<td style="text-align: center;">0.314</td>
<td style="text-align: center;">0.334</td>
<td style="text-align: center;">0.365</td>
<td style="text-align: center;">0.359</td>
<td style="text-align: center;">0.403</td>
<td style="text-align: center;">0.428</td>
</tr>
<tr>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">0.237</td>
<td style="text-align: center;">0.270</td>
<td style="text-align: center;">0.252</td>
<td style="text-align: center;">0.285</td>
<td style="text-align: center;">0.248</td>
<td style="text-align: center;">0.300</td>
<td style="text-align: center;">0.225</td>
<td style="text-align: center;">0.264</td>
<td style="text-align: center;">0.259</td>
<td style="text-align: center;">0.287</td>
<td style="text-align: center;">0.309</td>
<td style="text-align: center;">0.360</td>
</tr>
<tr>
<td style="text-align: center;">96</td>
<td style="text-align: center;">0.376</td>
<td style="text-align: center;">0.397</td>
<td style="text-align: center;">0.422</td>
<td style="text-align: center;">0.428</td>
<td style="text-align: center;">0.375</td>
<td style="text-align: center;">0.399</td>
<td style="text-align: center;">0.370</td>
<td style="text-align: center;">0.399</td>
<td style="text-align: center;">0.384</td>
<td style="text-align: center;">0.402</td>
<td style="text-align: center;">0.376</td>
<td style="text-align: center;">0.419</td>
</tr>
<tr>
<td style="text-align: center;">192</td>
<td style="text-align: center;">0.416</td>
<td style="text-align: center;">0.418</td>
<td style="text-align: center;">0.466</td>
<td style="text-align: center;">0.450</td>
<td style="text-align: center;">0.405</td>
<td style="text-align: center;">0.416</td>
<td style="text-align: center;">0.413</td>
<td style="text-align: center;">0.421</td>
<td style="text-align: center;">0.436</td>
<td style="text-align: center;">0.429</td>
<td style="text-align: center;">0.420</td>
<td style="text-align: center;">0.448</td>
</tr>
<tr>
<td style="text-align: center;">336</td>
<td style="text-align: center;">0.442</td>
<td style="text-align: center;">0.433</td>
<td style="text-align: center;">0.488</td>
<td style="text-align: center;">0.464</td>
<td style="text-align: center;">0.439</td>
<td style="text-align: center;">0.443</td>
<td style="text-align: center;">0.422</td>
<td style="text-align: center;">0.436</td>
<td style="text-align: center;">0.491</td>
<td style="text-align: center;">0.469</td>
<td style="text-align: center;">0.459</td>
<td style="text-align: center;">0.465</td>
</tr>
<tr>
<td style="text-align: center;">720</td>
<td style="text-align: center;">0.477</td>
<td style="text-align: center;">0.456</td>
<td style="text-align: center;">0.485</td>
<td style="text-align: center;">0.478</td>
<td style="text-align: center;">0.472</td>
<td style="text-align: center;">0.490</td>
<td style="text-align: center;">0.447</td>
<td style="text-align: center;">0.466</td>
<td style="text-align: center;">0.521</td>
<td style="text-align: center;">0.500</td>
<td style="text-align: center;">0.506</td>
<td style="text-align: center;">0.507</td>
</tr>
<tr>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">0.427</td>
<td style="text-align: center;">0.426</td>
<td style="text-align: center;">0.465</td>
<td style="text-align: center;">0.455</td>
<td style="text-align: center;">0.422</td>
<td style="text-align: center;">0.437</td>
<td style="text-align: center;">0.413</td>
<td style="text-align: center;">0.430</td>
<td style="text-align: center;">0.458</td>
<td style="text-align: center;">0.450</td>
<td style="text-align: center;">0.440</td>
<td style="text-align: center;">0.460</td>
</tr>
<tr>
<td style="text-align: center;">96</td>
<td style="text-align: center;">0.285</td>
<td style="text-align: center;">0.342</td>
<td style="text-align: center;">0.318</td>
<td style="text-align: center;">0.368</td>
<td style="text-align: center;">0.289</td>
<td style="text-align: center;">0.353</td>
<td style="text-align: center;">0.274</td>
<td style="text-align: center;">0.336</td>
<td style="text-align: center;">0.340</td>
<td style="text-align: center;">0.374</td>
<td style="text-align: center;">0.358</td>
<td style="text-align: center;">0.397</td>
</tr>
<tr>
<td style="text-align: center;">192</td>
<td style="text-align: center;">0.354</td>
<td style="text-align: center;">0.389</td>
<td style="text-align: center;">0.383</td>
<td style="text-align: center;">0.407</td>
<td style="text-align: center;">0.383</td>
<td style="text-align: center;">0.418</td>
<td style="text-align: center;">0.339</td>
<td style="text-align: center;">0.379</td>
<td style="text-align: center;">0.402</td>
<td style="text-align: center;">0.414</td>
<td style="text-align: center;">0.429</td>
<td style="text-align: center;">0.439</td>
</tr>
<tr>
<td style="text-align: center;">336</td>
<td style="text-align: center;">0.373</td>
<td style="text-align: center;">0.407</td>
<td style="text-align: center;">0.406</td>
<td style="text-align: center;">0.427</td>
<td style="text-align: center;">0.448</td>
<td style="text-align: center;">0.465</td>
<td style="text-align: center;">0.329</td>
<td style="text-align: center;">0.380</td>
<td style="text-align: center;">0.452</td>
<td style="text-align: center;">0.452</td>
<td style="text-align: center;">0.496</td>
<td style="text-align: center;">0.487</td>
</tr>
<tr>
<td style="text-align: center;">720</td>
<td style="text-align: center;">0.406</td>
<td style="text-align: center;">0.441</td>
<td style="text-align: center;">0.420</td>
<td style="text-align: center;">0.446</td>
<td style="text-align: center;">0.605</td>
<td style="text-align: center;">0.551</td>
<td style="text-align: center;">0.379</td>
<td style="text-align: center;">0.422</td>
<td style="text-align: center;">0.462</td>
<td style="text-align: center;">0.468</td>
<td style="text-align: center;">0.463</td>
<td style="text-align: center;">0.474</td>
</tr>
<tr>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">0.354</td>
<td style="text-align: center;">0.394</td>
<td style="text-align: center;">0.381</td>
<td style="text-align: center;">0.412</td>
<td style="text-align: center;">0.431</td>
<td style="text-align: center;">0.446</td>
<td style="text-align: center;">0.330</td>
<td style="text-align: center;">0.379</td>
<td style="text-align: center;">0.414</td>
<td style="text-align: center;">0.427</td>
<td style="text-align: center;">0.437</td>
<td style="text-align: center;">0.449</td>
</tr>
<tr>
<td style="text-align: center;">96</td>
<td style="text-align: center;">0.292</td>
<td style="text-align: center;">0.346</td>
<td style="text-align: center;">0.330</td>
<td style="text-align: center;">0.372</td>
<td style="text-align: center;">0.299</td>
<td style="text-align: center;">0.343</td>
<td style="text-align: center;">0.290</td>
<td style="text-align: center;">0.342</td>
<td style="text-align: center;">0.338</td>
<td style="text-align: center;">0.375</td>
<td style="text-align: center;">0.379</td>
<td style="text-align: center;">0.419</td>
</tr>
<tr>
<td style="text-align: center;">192</td>
<td style="text-align: center;">0.332</td>
<td style="text-align: center;">0.372</td>
<td style="text-align: center;">0.371</td>
<td style="text-align: center;">0.394</td>
<td style="text-align: center;">0.335</td>
<td style="text-align: center;">0.365</td>
<td style="text-align: center;">0.332</td>
<td style="text-align: center;">0.369</td>
<td style="text-align: center;">0.374</td>
<td style="text-align: center;">0.387</td>
<td style="text-align: center;">0.426</td>
<td style="text-align: center;">0.441</td>
</tr>
<tr>
<td style="text-align: center;">336</td>
<td style="text-align: center;">0.366</td>
<td style="text-align: center;">0.394</td>
<td style="text-align: center;">0.398</td>
<td style="text-align: center;">0.409</td>
<td style="text-align: center;">0.369</td>
<td style="text-align: center;">0.386</td>
<td style="text-align: center;">0.366</td>
<td style="text-align: center;">0.392</td>
<td style="text-align: center;">0.410</td>
<td style="text-align: center;">0.411</td>
<td style="text-align: center;">0.445</td>
<td style="text-align: center;">0.459</td>
</tr>
<tr>
<td style="text-align: center;">720</td>
<td style="text-align: center;">0.417</td>
<td style="text-align: center;">0.421</td>
<td style="text-align: center;">0.454</td>
<td style="text-align: center;">0.440</td>
<td style="text-align: center;">0.425</td>
<td style="text-align: center;">0.421</td>
<td style="text-align: center;">0.416</td>
<td style="text-align: center;">0.420</td>
<td style="text-align: center;">0.478</td>
<td style="text-align: center;">0.450</td>
<td style="text-align: center;">0.543</td>
<td style="text-align: center;">0.490</td>
</tr>
<tr>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">0.352</td>
<td style="text-align: center;">0.383</td>
<td style="text-align: center;">0.388</td>
<td style="text-align: center;">0.403</td>
<td style="text-align: center;">0.357</td>
<td style="text-align: center;">0.378</td>
<td style="text-align: center;">0.351</td>
<td style="text-align: center;">0.380</td>
<td style="text-align: center;">0.400</td>
<td style="text-align: center;">0.406</td>
<td style="text-align: center;">0.448</td>
<td style="text-align: center;">0.452</td>
</tr>
<tr>
<td style="text-align: center;">96</td>
<td style="text-align: center;">0.173</td>
<td style="text-align: center;">0.262</td>
<td style="text-align: center;">0.192</td>
<td style="text-align: center;">0.281</td>
<td style="text-align: center;">0.167</td>
<td style="text-align: center;">0.269</td>
<td style="text-align: center;">0.165</td>
<td style="text-align: center;">0.255</td>
<td style="text-align: center;">0.187</td>
<td style="text-align: center;">0.267</td>
<td style="text-align: center;">0.203</td>
<td style="text-align: center;">0.287</td>
</tr>
<tr>
<td style="text-align: center;">192</td>
<td style="text-align: center;">0.229</td>
<td style="text-align: center;">0.301</td>
<td style="text-align: center;">0.245</td>
<td style="text-align: center;">0.317</td>
<td style="text-align: center;">0.224</td>
<td style="text-align: center;">0.303</td>
<td style="text-align: center;">0.220</td>
<td style="text-align: center;">0.292</td>
<td style="text-align: center;">0.249</td>
<td style="text-align: center;">0.309</td>
<td style="text-align: center;">0.269</td>
<td style="text-align: center;">0.328</td>
</tr>
<tr>
<td style="text-align: center;">336</td>
<td style="text-align: center;">0.286</td>
<td style="text-align: center;">0.341</td>
<td style="text-align: center;">0.302</td>
<td style="text-align: center;">0.352</td>
<td style="text-align: center;">0.281</td>
<td style="text-align: center;">0.342</td>
<td style="text-align: center;">0.274</td>
<td style="text-align: center;">0.329</td>
<td style="text-align: center;">0.321</td>
<td style="text-align: center;">0.351</td>
<td style="text-align: center;">0.325</td>
<td style="text-align: center;">0.366</td>
</tr>
<tr>
<td style="text-align: center;">720</td>
<td style="text-align: center;">0.378</td>
<td style="text-align: center;">0.401</td>
<td style="text-align: center;">0.399</td>
<td style="text-align: center;">0.408</td>
<td style="text-align: center;">0.397</td>
<td style="text-align: center;">0.421</td>
<td style="text-align: center;">0.362</td>
<td style="text-align: center;">0.385</td>
<td style="text-align: center;">0.408</td>
<td style="text-align: center;">0.403</td>
<td style="text-align: center;">0.421</td>
<td style="text-align: center;">0.415</td>
</tr>
<tr>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">0.266</td>
<td style="text-align: center;">0.326</td>
<td style="text-align: center;">0.284</td>
<td style="text-align: center;">0.339</td>
<td style="text-align: center;">0.267</td>
<td style="text-align: center;">0.333</td>
<td style="text-align: center;">0.255</td>
<td style="text-align: center;">0.315</td>
<td style="text-align: center;">0.291</td>
<td style="text-align: center;">0.333</td>
<td style="text-align: center;">0.305</td>
<td style="text-align: center;">0.349</td>
</tr>
<tr>
<td style="text-align: center;">24</td>
<td style="text-align: center;">2.063</td>
<td style="text-align: center;">0.881</td>
<td style="text-align: center;">2.723</td>
<td style="text-align: center;">1.099</td>
<td style="text-align: center;">2.215</td>
<td style="text-align: center;">1.081</td>
<td style="text-align: center;">1.319</td>
<td style="text-align: center;">0.754</td>
<td style="text-align: center;">2.317</td>
<td style="text-align: center;">0.934</td>
<td style="text-align: center;">3.228</td>
<td style="text-align: center;">1.260</td>
</tr>
<tr>
<td style="text-align: center;">36</td>
<td style="text-align: center;">1.868</td>
<td style="text-align: center;">0.892</td>
<td style="text-align: center;">2.027</td>
<td style="text-align: center;">0.966</td>
<td style="text-align: center;">1.963</td>
<td style="text-align: center;">0.963</td>
<td style="text-align: center;">1.430</td>
<td style="text-align: center;">0.834</td>
<td style="text-align: center;">1.972</td>
<td style="text-align: center;">0.920</td>
<td style="text-align: center;">2.679</td>
<td style="text-align: center;">1.080</td>
</tr>
<tr>
<td style="text-align: center;">48</td>
<td style="text-align: center;">1.790</td>
<td style="text-align: center;">0.884</td>
<td style="text-align: center;">2.206</td>
<td style="text-align: center;">1.022</td>
<td style="text-align: center;">2.130</td>
<td style="text-align: center;">1.024</td>
<td style="text-align: center;">1.553</td>
<td style="text-align: center;">0.815</td>
<td style="text-align: center;">2.238</td>
<td style="text-align: center;">0.940</td>
<td style="text-align: center;">2.622</td>
<td style="text-align: center;">1.078</td>
</tr>
<tr>
<td style="text-align: center;">60</td>
<td style="text-align: center;">1.979</td>
<td style="text-align: center;">0.957</td>
<td style="text-align: center;">1.976</td>
<td style="text-align: center;">0.983</td>
<td style="text-align: center;">2.368</td>
<td style="text-align: center;">1.096</td>
<td style="text-align: center;">1.470</td>
<td style="text-align: center;">0.788</td>
<td style="text-align: center;">2.027</td>
<td style="text-align: center;">0.928</td>
<td style="text-align: center;">2.857</td>
<td style="text-align: center;">1.157</td>
</tr>
<tr>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">1.925</td>
<td style="text-align: center;">0.963</td>
<td style="text-align: center;">2.233</td>
<td style="text-align: center;">1.017</td>
<td style="text-align: center;">2.169</td>
<td style="text-align: center;">1.041</td>
<td style="text-align: center;">1.443</td>
<td style="text-align: center;">0.797</td>
<td style="text-align: center;">2.139</td>
<td style="text-align: center;">0.931</td>
<td style="text-align: center;">2.847</td>
<td style="text-align: center;">1.144</td>
</tr>
<tr>
<td style="text-align: center;">96</td>
<td style="text-align: center;">0.139</td>
<td style="text-align: center;">0.238</td>
<td style="text-align: center;">0.138</td>
<td style="text-align: center;">0.234</td>
<td style="text-align: center;">0.140</td>
<td style="text-align: center;">0.237</td>
<td style="text-align: center;">0.129</td>
<td style="text-align: center;">0.222</td>
<td style="text-align: center;">0.168</td>
<td style="text-align: center;">0.272</td>
<td style="text-align: center;">0.193</td>
<td style="text-align: center;">0.308</td>
</tr>
<tr>
<td style="text-align: center;">192</td>
<td style="text-align: center;">0.153</td>
<td style="text-align: center;">0.251</td>
<td style="text-align: center;">0.152</td>
<td style="text-align: center;">0.247</td>
<td style="text-align: center;">0.153</td>
<td style="text-align: center;">0.249</td>
<td style="text-align: center;">0.157</td>
<td style="text-align: center;">0.240</td>
<td style="text-align: center;">0.184</td>
<td style="text-align: center;">0.289</td>
<td style="text-align: center;">0.201</td>
<td style="text-align: center;">0.315</td>
</tr>
<tr>
<td style="text-align: center;">336</td>
<td style="text-align: center;">0.169</td>
<td style="text-align: center;">0.266</td>
<td style="text-align: center;">0.168</td>
<td style="text-align: center;">0.263</td>
<td style="text-align: center;">0.169</td>
<td style="text-align: center;">0.267</td>
<td style="text-align: center;">0.163</td>
<td style="text-align: center;">0.259</td>
<td style="text-align: center;">0.198</td>
<td style="text-align: center;">0.300</td>
<td style="text-align: center;">0.214</td>
<td style="text-align: center;">0.329</td>
</tr>
<tr>
<td style="text-align: center;">720</td>
<td style="text-align: center;">0.206</td>
<td style="text-align: center;">0.297</td>
<td style="text-align: center;">0.207</td>
<td style="text-align: center;">0.295</td>
<td style="text-align: center;">0.203</td>
<td style="text-align: center;">0.301</td>
<td style="text-align: center;">0.197</td>
<td style="text-align: center;">0.290</td>
<td style="text-align: center;">0.220</td>
<td style="text-align: center;">0.320</td>
<td style="text-align: center;">0.246</td>
<td style="text-align: center;">0.355</td>
</tr>
<tr>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">0.167</td>
<td style="text-align: center;">0.263</td>
<td style="text-align: center;">0.166</td>
<td style="text-align: center;">0.259</td>
<td style="text-align: center;">0.166</td>
<td style="text-align: center;">0.263</td>
<td style="text-align: center;">0.161</td>
<td style="text-align: center;">0.252</td>
<td style="text-align: center;">0.192</td>
<td style="text-align: center;">0.295</td>
<td style="text-align: center;">0.214</td>
<td style="text-align: center;">0.327</td>
</tr>
<tr>
<td style="text-align: center;">96</td>
<td style="text-align: center;">0.388</td>
<td style="text-align: center;">0.282</td>
<td style="text-align: center;">0.390</td>
<td style="text-align: center;">0.272</td>
<td style="text-align: center;">0.410</td>
<td style="text-align: center;">0.282</td>
<td style="text-align: center;">0.360</td>
<td style="text-align: center;">0.249</td>
<td style="text-align: center;">0.593</td>
<td style="text-align: center;">0.321</td>
<td style="text-align: center;">0.587</td>
<td style="text-align: center;">0.366</td>
</tr>
<tr>
<td style="text-align: center;">192</td>
<td style="text-align: center;">0.407</td>
<td style="text-align: center;">0.290</td>
<td style="text-align: center;">0.403</td>
<td style="text-align: center;">0.276</td>
<td style="text-align: center;">0.423</td>
<td style="text-align: center;">0.287</td>
<td style="text-align: center;">0.379</td>
<td style="text-align: center;">0.256</td>
<td style="text-align: center;">0.617</td>
<td style="text-align: center;">0.336</td>
<td style="text-align: center;">0.604</td>
<td style="text-align: center;">0.373</td>
</tr>
<tr>
<td style="text-align: center;">336</td>
<td style="text-align: center;">0.412</td>
<td style="text-align: center;">0.294</td>
<td style="text-align: center;">0.413</td>
<td style="text-align: center;">0.280</td>
<td style="text-align: center;">0.436</td>
<td style="text-align: center;">0.296</td>
<td style="text-align: center;">0.392</td>
<td style="text-align: center;">0.264</td>
<td style="text-align: center;">0.629</td>
<td style="text-align: center;">0.336</td>
<td style="text-align: center;">0.621</td>
<td style="text-align: center;">0.383</td>
</tr>
<tr>
<td style="text-align: center;">720</td>
<td style="text-align: center;">0.450</td>
<td style="text-align: center;">0.312</td>
<td style="text-align: center;">0.447</td>
<td style="text-align: center;">0.298</td>
<td style="text-align: center;">0.466</td>
<td style="text-align: center;">0.315</td>
<td style="text-align: center;">0.432</td>
<td style="text-align: center;">0.286</td>
<td style="text-align: center;">0.640</td>
<td style="text-align: center;">0.350</td>
<td style="text-align: center;">0.626</td>
<td style="text-align: center;">0.382</td>
</tr>
<tr>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">0.414</td>
<td style="text-align: center;">0.294</td>
<td style="text-align: center;">0.413</td>
<td style="text-align: center;">0.281</td>
<td style="text-align: center;">0.433</td>
<td style="text-align: center;">0.295</td>
<td style="text-align: center;">0.390</td>
<td style="text-align: center;">0.263</td>
<td style="text-align: center;">0.620</td>
<td style="text-align: center;">0.336</td>
<td style="text-align: center;">0.610</td>
<td style="text-align: center;">0.376</td>
</tr>
<tr>
<td style="text-align: center;">Average</td>
<td style="text-align: center;">0.516</td>
<td style="text-align: center;">0.407</td>
<td style="text-align: center;">0.573</td>
<td style="text-align: center;">0.0.431</td>
<td style="text-align: center;">0.562</td>
<td style="text-align: center;">0.436</td>
<td style="text-align: center;">0.446</td>
<td style="text-align: center;">0.386</td>
<td style="text-align: center;">0.596</td>
<td style="text-align: center;">0.433</td>
<td style="text-align: center;">0.701</td>
<td style="text-align: center;">0.489</td>
</tr>
</tbody>
</table>
<h1>D. 4 Mean and STD for Few-shot Learning</h1>
<p>Table 15 lists both mean and STD for GPT2(6), DLinear and PatchTST with 3 runs on 5\% ETTh2 and ETTm2. The results show a small variance in performance of GPT2(6) that represents the stability of GPT2(6).</p>
<p>Table 15: A subset of results showing both Mean and STD on 5\% datasets.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Methods</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">GPT2-backbone(6 Layers)</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Metric</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">96</td>
<td style="text-align: center;">$0.376 \pm 0.0072$</td>
<td style="text-align: center;">$0.421 \pm 0.0054$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">192</td>
<td style="text-align: center;">$0.418 \pm 0.0013$</td>
<td style="text-align: center;">$0.441 \pm 0.0014$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">336</td>
<td style="text-align: center;">$0.408 \pm 0.0006$</td>
<td style="text-align: center;">$0.439 \pm 0.0002$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">720</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">96</td>
<td style="text-align: center;">$0.199 \pm 0.0040$</td>
<td style="text-align: center;">$0.280 \pm 0.0042$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">192</td>
<td style="text-align: center;">$0.256 \pm 0.0030$</td>
<td style="text-align: center;">$0.316 \pm 0.0017$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">336</td>
<td style="text-align: center;">$0.318 \pm 0.0046$</td>
<td style="text-align: center;">$0.353 \pm 0.0032$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">720</td>
<td style="text-align: center;">$0.460 \pm 0.0132$</td>
<td style="text-align: center;">$0.436 \pm 0.0066$</td>
</tr>
</tbody>
</table>
<h2>D. 5 Comparison with Traditional Methods on Few-shot Learning</h2>
<p>Since deep learning methods are more advantageous than traditional methods when applied to large datasets. For few-shot learning, traditional methods should also consider. The results are shown in Table 16 that GPT2(6) also achieves best performance.</p>
<p>Table 16: Comparison with traditional methods.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Methods</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">GPT2(6) 5\%</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">GPT2(6) 10\%</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ETS</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ARIMA</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">NaiveDrift</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Metric</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">96</td>
<td style="text-align: center;">0.376</td>
<td style="text-align: center;">0.421</td>
<td style="text-align: center;">0.331</td>
<td style="text-align: center;">0.374</td>
<td style="text-align: center;">2.954</td>
<td style="text-align: center;">0.742</td>
<td style="text-align: center;">0.481</td>
<td style="text-align: center;">0.443</td>
<td style="text-align: center;">0.764</td>
<td style="text-align: center;">0.561</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">192</td>
<td style="text-align: center;">0.418</td>
<td style="text-align: center;">0.441</td>
<td style="text-align: center;">0.402</td>
<td style="text-align: center;">0.411</td>
<td style="text-align: center;">10.226</td>
<td style="text-align: center;">1.212</td>
<td style="text-align: center;">0.585</td>
<td style="text-align: center;">0.495</td>
<td style="text-align: center;">1.560</td>
<td style="text-align: center;">0.785</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">96</td>
<td style="text-align: center;">0.386</td>
<td style="text-align: center;">0.405</td>
<td style="text-align: center;">0.390</td>
<td style="text-align: center;">0.404</td>
<td style="text-align: center;">52.237</td>
<td style="text-align: center;">2.689</td>
<td style="text-align: center;">0.693</td>
<td style="text-align: center;">0.547</td>
<td style="text-align: center;">1.539</td>
<td style="text-align: center;">0.913</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">192</td>
<td style="text-align: center;">0.440</td>
<td style="text-align: center;">0.438</td>
<td style="text-align: center;">0.429</td>
<td style="text-align: center;">0.423</td>
<td style="text-align: center;">186.445</td>
<td style="text-align: center;">4.654</td>
<td style="text-align: center;">0.710</td>
<td style="text-align: center;">0.557</td>
<td style="text-align: center;">2.869</td>
<td style="text-align: center;">1.215</td>
</tr>
</tbody>
</table>
<h2>D. 6 Baselines with Instance Normalization</h2>
<p>Instance normalization Kim et al. (2022) is a plug-in for time series for distribution shift. Most baselines, such as Autoformer and FEDformer are not equipped with instance normalization. Thus, for a fair comparison, we add the experiment, as in Table 17, for baselines w/o instance normalization and GPT(6) can also perform superior.</p>
<p>Table 17: Comparison on 5\% data. Autoformer and FEDformer are equiped with instance normalization.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Methods</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">GPT2(6)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">PatchTST</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">DLinear</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Autoformer</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Autoformer(Revin)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">FEDformer</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">FEDformer(Revin)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Metric</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MAE</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">96</td>
<td style="text-align: center;">0.199</td>
<td style="text-align: center;">0.280</td>
<td style="text-align: center;">0.206</td>
<td style="text-align: center;">0.288</td>
<td style="text-align: center;">0.236</td>
<td style="text-align: center;">0.326</td>
<td style="text-align: center;">0.232</td>
<td style="text-align: center;">0.322</td>
<td style="text-align: center;">0.224</td>
<td style="text-align: center;">0.300</td>
<td style="text-align: center;">0.229</td>
<td style="text-align: center;">0.320</td>
<td style="text-align: center;">0.223</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">192</td>
<td style="text-align: center;">0.256</td>
<td style="text-align: center;">0.316</td>
<td style="text-align: center;">0.264</td>
<td style="text-align: center;">0.324</td>
<td style="text-align: center;">0.306</td>
<td style="text-align: center;">0.373</td>
<td style="text-align: center;">0.291</td>
<td style="text-align: center;">0.357</td>
<td style="text-align: center;">0.296</td>
<td style="text-align: center;">0.343</td>
<td style="text-align: center;">0.294</td>
<td style="text-align: center;">0.361</td>
<td style="text-align: center;">0.288</td>
</tr>
</tbody>
</table>
<h2>D. 7 Detailed Definition and Results of Zero-shot Learning</h2>
<p>Task Definition Each experiment contains two distinct datasets, source, and target datasets. The source dataset is used to train the model and then forecasts without fine-tuning in the target dataset. The target dataset is split into non-overlapping historical and test sequences. We use the historical sequence as input to the model, and the obtained output is used to calculate errors with the test sequences. Besides meta-learning-based models like N-BEATS, evaluated models' parameters are not allowed any adjustment using the forecasting phase. Also, same as Oreshkin et al. (2021), each data set adopts a specific metric (M4: sMAPE; M3: sMAPE; TOURISM: MAPE; ELECTR: ND)</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>http://pems.dot.ca.gov
https://gis.cdc.gov/grasp/fluview/fluportaldashboard.html&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
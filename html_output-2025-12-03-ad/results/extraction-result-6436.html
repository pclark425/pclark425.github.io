<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6436 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6436</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6436</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-126.html">extraction-schema-126</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <p><strong>Paper ID:</strong> paper-266573802</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2312.17080v4.pdf" target="_blank">MR-GSM8K: A Meta-Reasoning Benchmark for Large Language Model Evaluation</a></p>
                <p><strong>Paper Abstract:</strong> In this work, we introduce a novel evaluation paradigm for Large Language Models (LLMs) that compels them to transition from a traditional question-answering role, akin to a student, to a solution-scoring role, akin to a teacher. This paradigm, focusing on"reasoning about reasoning,"hence termed meta-reasoning, shifts the emphasis from result-oriented assessments, which often neglect the reasoning process, to a more comprehensive evaluation that effectively distinguishes between the cognitive capabilities of different models. By applying this paradigm in the GSM8K dataset, we have developed the MR-GSM8K benchmark. Our extensive analysis includes several state-of-the-art models from both open-source and commercial domains, uncovering fundamental deficiencies in their training and evaluation methodologies. Notably, while models like Deepseek-v2 and Claude3-Sonnet closely competed with GPT-4 in GSM8K, their performance disparities expanded dramatically in MR-GSM8K, with differences widening to over 20 absolute points, underscoring the significant challenge posed by our meta-reasoning approach.</p>
                <p><strong>Cost:</strong> 0.025</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6436.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6436.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MR-GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Meta-Reasoning GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark that converts GSM8K math problems into question+solution pairs and tasks LLMs with 'reasoning about reasoning': judge solution correctness, identify the first erroneous step, and explain the error reason; includes original, Program-of-Thought (code) and reversed reasoning variants.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Derived from the GSM8K dataset; solutions were generated (≈50% correct) using MetaMath-7B and then manually annotated by experts; no model pretraining data described here.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MR-GSM8K (derived from GSM8K)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step grade-school arithmetic word problems; correctness verification, error-localization, and error-explanation</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language question paired with a candidate step-by-step solution (question–solution pair); evaluator must output correctness (binary), first-error-step (step index) and an error reason (free-form).</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>GSM8K / grade-school multi-step word problems including forward, Program-of-Thought (code) variants and reversed (backwards) reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Models presented with zero-shot and few-shot scoring prompts (zero-shot primary; few-shot used 3-shot with example composition often 2 correct + 1 incorrect); evaluation prompts shown in paper (zero-shot 'act as grade school teacher' format).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>MR-Score (weighted combination of: MCC for solution-correctness binary classification, ACC_step for first-error-step accuracy, ACC_reason for correct error-reason), plus per-task metrics (TPR/TNR, MCC, ACC).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Benchmark defines MR-Score; reported per-model MR-Score (zero-shot and 3-shot) in Table 2 (examples: GPT-4-Turbo MR-Score ≈ 50.5 (k=0) / 53.0 (k=3); Llama3-70B ≈ 34.2; Deepseek-v2-236B ≈ 34.1; Qwen1.5-72B ≈ 23.3; Phi3-3.8B ≈ 22.9; GPT-3.5-Turbo ≈ 22.6 / 17.9; Claude3-Sonnet ≈ 23.5 / 20.8).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>No mechanistic probing (attention/activation/logit-lens) reported; analyses are behavioral: comparison of TPR/TNR under few-shot, error-step localization accuracy, and human-vs-GPT-4 auto-labeler agreement. Dataset authors note they used GPT-4 as an automated error-reason judge and cross-validated against manual labels.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Paper documents model failure modes when performing arithmetic verification: sensitivity to few-shot example distribution (bias toward labels in demonstrations), inability to generalize beyond training response formats, the 'reversal curse' (treating A=B and B=A as non-equivalent), ontological/unit errors (misusing units or object properties), insensitivity to simple numerical computation errors, hallucinated/incorrect error explanations, and over-rejection or over-acceptance tendencies after fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Performance does not strictly scale with parameter count; several smaller models outperform much larger ones (e.g., Phi3-3.8B beating bigger 7B/72B models in this task), and Llama3-70B outperformed Deepseek-v2-236B; authors conclude data quality, distillation and instruction tuning matter as much as model size.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MR-GSM8K: A Meta-Reasoning Benchmark for Large Language Model Evaluation', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6436.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6436.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MR-Score</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Meta-Reasoning Score (MR-Score)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A composite metric for the MR-GSM8K tasks combining (1) MCC for binary solution correctness, (2) accuracy of first-error-step localization, and (3) accuracy of error-reason explanation, weighted to reflect task difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MR-GSM8K evaluation metric</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>meta-reasoning evaluation (judge candidate solutions)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Scoring of question+solution pairs with three subtasks</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Designed to be harder than stem GSM8K by requiring diagnosis of incorrect reasoning; intended for grade-school math problems</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Used in evaluation pipeline; per-model evaluation output compared against annotated ground truth; also compared manual labeling vs. GPT-4 auto-labeling</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Composite MR-Score built from: MCC (binary correctness), ACC_step (first-error-step accuracy), ACC_reason (error-reason accuracy); MR-Score normalized and weighted empirically.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported MR-Scores per model (see MR-GSM8K entry); manual vs. GPT-4 auto-labeler MR-Score comparisons reported in Table 4 showing close agreement for top models (e.g., GPT-4 MR-Score by manual vs. GPT-4 auto-labeler ~0.495 vs 0.512 in a subset analysis).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Design rationale: MCC chosen to counteract trivial bias (models that always say correct/incorrect), accuracy used for step and reason tasks; authors analyze auto-labeler (GPT-4) confusion matrix vs manual labels.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Authors note MR-Score can be targeted for optimization (Goodhart's law) and that auto-labeling of error-reasons is imperfect (false positives/negatives), limiting fully automatic scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Not applicable (metric design), but authors tuned weights empirically to maximize cross-model discrimination.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MR-GSM8K: A Meta-Reasoning Benchmark for Large Language Model Evaluation', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6436.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6436.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4-Turbo (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source large language model from OpenAI evaluated as an automatic judge and as an evaluated model on MR-GSM8K; it achieves the best reported MR-Score in the paper and is used as a proxy auto-labeler for error reasons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>transformer (decoder-only, OpenAI family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified in this paper (general web/code/token data and instruction-tuning typical of OpenAI models; paper does not assert specifics).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MR-GSM8K (derived from GSM8K)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step grade-school arithmetic word problems verification (correctness classification, first-error-step identification, error-reason generation)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language question paired with a proposed step-by-step solution (scoring task).</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>GSM8K-level multi-step problems including Program-of-Thought and reversed variants</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot scoring prompt (act as grade school math teacher) and few-shot (3-shot) evaluated; temperature=0 for inference; GPT-4 also used to synthesize in-domain diagnostic training data for fine-tuning experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>MR-Score (composite), MCC for correctness, ACC_step, ACC_reason, plus TPR/TNR for solution-correctness</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Top-performing model in paper: MR-Score ≈ 50.5 (zero-shot) and ≈ 53.0 (3-shot) as reported in Table 2; high MCC and the best ACC_step/ACC_reason among evaluated models.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Behavioral analyses only: GPT-4 sometimes fails to generalize equivalence (reversal curse) and can miss simple arithmetic mistakes; GPT-4 was also used as a near-human auto-evaluator (confusion matrix vs. manual labels reported). No activation/attention probing provided.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Reversal curse (mis-evaluating equivalent equations), occasional insensitivity to trivial arithmetic errors, and provide erroneous error explanations in some examples; limited true understanding of fabricated errors when asked to generate diagnostic training data (GPT-4 sometimes fails to fabricate valid errors).</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>N/A within paper (single closed-source model); authors note GPT-4's high performance relative to other models but do not analyze scaling within GPT-4 variants beyond k=0/k=3 prompting differences.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MR-GSM8K: A Meta-Reasoning Benchmark for Large Language Model Evaluation', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6436.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6436.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-Turbo (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A widely used OpenAI chat model evaluated on MR-GSM8K; shows moderate-to-low MR-Score and notable weaknesses in error localization/explanation relative to GPT-4-Turbo.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>transformer (decoder-only, OpenAI family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MR-GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step arithmetic word-problem verification and diagnosis</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language question paired with candidate solution</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>GSM8K-level</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot and few-shot (3-shot) scoring prompts; temperature=0</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>MR-Score, MCC, ACC_step, ACC_reason; TPR/TNR reported for solution correctness task</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>MR-Score reported ≈ 22.6 (k=0) and ≈ 17.9 (k=3) indicating mediocre performance and sometimes degradation with few-shot examples (per Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Behavioral: shows increased TPR but decreased TNR under few-shot (models become biased by composition of examples); no mechanistic probing.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Susceptible to few-shot distribution bias (shifts to class frequencies in demonstrations), insensitivity to calculation errors, and poor error-reason explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Within this paper, standard sized GPT-3.5 shows lower MR-Score than GPT-4-Turbo; few-shot did not uniformly help and sometimes worsened results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MR-GSM8K: A Meta-Reasoning Benchmark for Large Language Model Evaluation', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6436.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6436.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude-3-Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude 3 Sonnet (Anthropic)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Claude-3 family chat model evaluated on MR-GSM8K; mid-ranked performance with notable hallucinated or incorrect error analyses in qualitative examples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude3-Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>transformer (Anthropic Claude family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MR-GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step arithmetic verification & diagnosis</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language question + candidate solution</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>GSM8K-level</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot and few-shot (3-shot); temperature=0</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>MR-Score, MCC, ACC_step, ACC_reason</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>MR-Score approximately 23.5 (k=0) and 20.8 (k=3) per Table 2.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Behavioral observations: provided hallucinated error reasons in examples and made unit mistakes; no internal activation analyses provided.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Hallucinated or incorrect error explanations, unit/ontological errors (misunderstanding of object properties), and calculation mistakes ignored in diagnosis.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Not discussed beyond reported comparative performance among Claude variants and other closed-source models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MR-GSM8K: A Meta-Reasoning Benchmark for Large Language Model Evaluation', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6436.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6436.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude-3-Haiku</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude 3 Haiku (Anthropic)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A member of the Claude-3 family measured on MR-GSM8K; shows lower MR-Score compared to Claude3-Sonnet and GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude3-Haiku</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>transformer (Anthropic Claude family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MR-GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step arithmetic verification & diagnosis</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language question + candidate solution</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>GSM8K-level</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot and few-shot; temperature=0</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>MR-Score, MCC, ACC_step, ACC_reason</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>MR-Score reported ≈ 15.3 (k=0) and ≈ 4.9 (k=3) in Table 2 (showing unstable/falling few-shot behavior).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Behavioral only: fluctuations in few-shot performance and bias effects; no representational analyses provided.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Susceptibility to few-shot distribution bias and degraded MR-Score under some few-shot contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>N/A within paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MR-GSM8K: A Meta-Reasoning Benchmark for Large Language Model Evaluation', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6436.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6436.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama3-70B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 3 70B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large open-source chat model evaluated on MR-GSM8K; among open-source models it performed closest to GPT-4 but still substantially lower in MR-Score.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama3-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>Llama family (decoder-only transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified in this paper (standard Llama3 pretraining presumed); paper does not enumerate training corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MR-GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step arithmetic verification & diagnosis</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language question + candidate solution</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>GSM8K-level</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot and few-shot; temperature=0</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>MR-Score, MCC, ACC_step, ACC_reason</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>MR-Score ≈ 34.2 (k=0) per Table 2 (substantially below GPT-4 but highest among many open-source models).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Behavioral: Llama3-70B better follows evaluation instructions than many specialized math models; no internal mechanistic probes reported.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Still shows arithmetic insensitivity, step-misidentification for many incorrect solutions, and limited holistic reasoning across solution-space variations.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>As a 70B model, it outperformed some larger (236B) models examined, indicating scale alone is not determinant.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MR-GSM8K: A Meta-Reasoning Benchmark for Large Language Model Evaluation', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6436.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6436.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Deepseek-v2-236B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deepseek-v2 236B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A very large open-source model (236B) included in evaluations; achieved mid-to-high MR-Score but was outperformed by some 70B models, indicating non-monotonic scaling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Deepseek-v2-236B</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>transformer (open-source scaling family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>236B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MR-GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step arithmetic verification & diagnosis</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language question + candidate solution</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>GSM8K-level</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot and few-shot; temperature=0</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>MR-Score, MCC, ACC_step, ACC_reason</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>MR-Score ≈ 34.1 (k=0) per Table 2; authors note Llama3-70B outperformed Deepseek-v2-236B.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Behavioral only: authors infer deficits from MR-Score and error analyses; no internal probing.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Overfitting-like behaviors, inability to assess reasoning outside familiar response formats, arithmetic insensitivity noted in case studies.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Performance not strictly increasing with parameters; 236B model not uniformly superior to smaller 70B models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MR-GSM8K: A Meta-Reasoning Benchmark for Large Language Model Evaluation', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6436.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6436.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Phi3-3.8B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Phi-3 3.8B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A small chat model (≈3.8B) that outperformed many larger models on MR-GSM8K, illustrating the importance of pretraining/data quality and distillation techniques over raw parameter count.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Phi3-3.8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>transformer (small chat model family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3.8B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MR-GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step arithmetic verification & diagnosis</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language question + candidate solution</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>GSM8K-level</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot and few-shot; temperature=0</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>MR-Score, MCC, ACC_step, ACC_reason</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>MR-Score reported ≈ 22.9 (k=0) and competitive with larger medium models per Table 2.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Behavioral: demonstrates that small well-distilled models can outperform larger ones; no internal mechanism analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Typical arithmetic verification failures: mis-localized errors, reasoning superficiality, sensitivity to few-shot example composition.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Counterexample to pure size-scaling: small model achieves strong performance due to training/data quality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MR-GSM8K: A Meta-Reasoning Benchmark for Large Language Model Evaluation', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6436.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e6436.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen-1.5-72B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen 1.5 / 72B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large open-source family (Qwen) evaluated on MR-GSM8K; achieves mid-range MR-Score relative to other large models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen1.5-72B</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>transformer (Qwen family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>72B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MR-GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step arithmetic verification & diagnosis</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language question + candidate solution</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>GSM8K-level</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot and few-shot; temperature=0</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>MR-Score, MCC, ACC_step, ACC_reason</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>MR-Score ≈ 23.3 (k=0) per Table 2.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Behavioral only: compared across models; no internal probing.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Similar to others: few-shot bias, arithmetic insensitivity, and trouble generalizing to reversed and POT variants.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Moderate performance for its size; again underscores that parameter count is not the only determiner.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MR-GSM8K: A Meta-Reasoning Benchmark for Large Language Model Evaluation', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6436.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e6436.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WizardMath-v1.1-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>WizardMath v1.1 7B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A specialized math-focused 7B model (open-source) that performed poorly on MR-GSM8K due to overfitting to GSM8K response format and inability to follow the scoring instruction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>WizardMath-v1.1-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>open-source fine-tuned math model</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Fine-tuned on math datasets (GSM8K-like), but exact data specifics not given in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MR-GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step arithmetic verification & diagnosis (scoring)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language question + candidate solution</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>GSM8K-level</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot and few-shot (few-shot harmed performance), temperature=0</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>MR-Score, MCC, ACC_step, ACC_reason</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>MR-Score very low (≈ 0.1 as reported), with near 100% TPR but near-zero TNR indicating over-acceptance or format overfitting.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Behavioral: model strongly reverts to question-answering format and fails to follow scoring instructions even with few-shot; no internal representation analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Overfitting to answer-generation format, inability to act as a scorer, very low error-localization and explanation accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Specialized fine-tuning on narrow math datasets led to brittle behavior under new evaluation paradigm.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MR-GSM8K: A Meta-Reasoning Benchmark for Large Language Model Evaluation', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6436.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e6436.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MetaMath-70B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MetaMath 70B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A math-specialized 70B open-source model included in evaluations that performed poorly on MR-GSM8K, likely due to overfitting to familiar solution formats and inability to switch to scoring role.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MetaMath-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>math-specialized transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Finetuned on math problem datasets (details not specified in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MR-GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step arithmetic verification & diagnosis</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language question + candidate solution</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>GSM8K-level</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot and few-shot; temperature=0</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>MR-Score, MCC, ACC_step, ACC_reason</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>MR-Score ≈ 0.0 in Table 2 (failed to adapt to evaluator role).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Behavioral: specialized fine-tuning led to reverting to QA behavior; no internal mechanistic analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Overfitting to GSM8K response format; inability to follow new scoring instruction, poor generalization to reversed and POT variants.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Despite large size, poor MR performance due to training objective mismatch.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MR-GSM8K: A Meta-Reasoning Benchmark for Large Language Model Evaluation', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6436.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e6436.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Deepseek-Math-7B-RL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deepseek-Math 7B RL</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source math-specialized model with RL fine-tuning that better complied with scoring instructions and had moderate MR performance compared to other specialized models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Deepseek-Math-7B-RL</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>math-specialized transformer (RL-finetuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Fine-tuned for math tasks and RL training (details not specified in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MR-GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step arithmetic verification & diagnosis</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language question + candidate solution</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>GSM8K-level</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot and few-shot; temperature=0</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>MR-Score, MCC, ACC_step, ACC_reason</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>MR-Score ≈ 11.6 (k=0) per Table 2 (decent among specialized models).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Behavioral: better instruction compliance than several other specialized models; no internal mechanistic probes.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Still limited in fully diagnosing error reasons and localizing first errors across diverse incorrect-solution types.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Fine-tuning method and instruction compliance matter more than raw size for this task.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MR-GSM8K: A Meta-Reasoning Benchmark for Large Language Model Evaluation', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6436.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e6436.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>llaMA2-70B-MR (finetuned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>llama-2-70B finetuned with diagnostic MR data (llama2-70B-MR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An in-domain fine-tuned model: Llama2-70B-base was fine-tuned with GPT-4-generated diagnostic incorrect solutions (5k incorrect + 4k correct) using QLoRA; showed minor MR improvements but still low absolute diagnostic accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>llama2-70B-MR</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>Llama 2 transformer (decoder-only), finetuned</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>GSM8K training set augmented with GPT-4-generated diagnostic incorrect solutions (5k incorrect, 4k correct); fine-tuned with QLoRA.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MR-GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step arithmetic verification & diagnosis</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language question + candidate solution</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>GSM8K-level</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot scoring; finetuned on in-domain diagnostics instead of or in addition to few-shot prompts; temperature=0 at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>MR-Score, TPR/TNR, MCC, ACC_step, ACC_reason</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>After fine-tuning: TPR ≈ 31.74%, TNR ≈ 73.49%, MCC ≈ 5.8%, ACC_step ≈ 20.79%, ACC_reason ≈ 6.29%, MR-Score ≈ 10.5% (reported in Section B).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Behavioral: fine-tuned model tended to over-reject solutions and was less likely to accept; of correctly-diagnosed cases many involved POT and reversed types despite training only on original problems. No internal mechanism probing.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Low absolute rates of correct diagnosis for incorrect solutions (esp. error reasons), tendency to over-reject, limited improvement from simple in-domain fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Fine-tuning with targeted diagnostic examples improves some metrics but does not yield high diagnostic accuracy; demonstrates difficulty of the MR task beyond dataset augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MR-GSM8K: A Meta-Reasoning Benchmark for Large Language Model Evaluation', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks <em>(Rating: 2)</em></li>
                <li>The reversal curse: Llms trained on "a is b" fail to learn "b is a." <em>(Rating: 2)</em></li>
                <li>Measuring mathematical problem solving with the math dataset <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6436",
    "paper_id": "paper-266573802",
    "extraction_schema_id": "extraction-schema-126",
    "extracted_data": [
        {
            "name_short": "MR-GSM8K",
            "name_full": "Meta-Reasoning GSM8K",
            "brief_description": "A benchmark that converts GSM8K math problems into question+solution pairs and tasks LLMs with 'reasoning about reasoning': judge solution correctness, identify the first erroneous step, and explain the error reason; includes original, Program-of-Thought (code) and reversed reasoning variants.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_family": null,
            "model_size": null,
            "training_data_description": "Derived from the GSM8K dataset; solutions were generated (≈50% correct) using MetaMath-7B and then manually annotated by experts; no model pretraining data described here.",
            "benchmark_name": "MR-GSM8K (derived from GSM8K)",
            "task_type": "multi-step grade-school arithmetic word problems; correctness verification, error-localization, and error-explanation",
            "problem_format": "Natural-language question paired with a candidate step-by-step solution (question–solution pair); evaluator must output correctness (binary), first-error-step (step index) and an error reason (free-form).",
            "difficulty_level": "GSM8K / grade-school multi-step word problems including forward, Program-of-Thought (code) variants and reversed (backwards) reasoning",
            "prompting_method": "Models presented with zero-shot and few-shot scoring prompts (zero-shot primary; few-shot used 3-shot with example composition often 2 correct + 1 incorrect); evaluation prompts shown in paper (zero-shot 'act as grade school teacher' format).",
            "performance_metric": "MR-Score (weighted combination of: MCC for solution-correctness binary classification, ACC_step for first-error-step accuracy, ACC_reason for correct error-reason), plus per-task metrics (TPR/TNR, MCC, ACC).",
            "performance_value": "Benchmark defines MR-Score; reported per-model MR-Score (zero-shot and 3-shot) in Table 2 (examples: GPT-4-Turbo MR-Score ≈ 50.5 (k=0) / 53.0 (k=3); Llama3-70B ≈ 34.2; Deepseek-v2-236B ≈ 34.1; Qwen1.5-72B ≈ 23.3; Phi3-3.8B ≈ 22.9; GPT-3.5-Turbo ≈ 22.6 / 17.9; Claude3-Sonnet ≈ 23.5 / 20.8).",
            "internal_analysis": "No mechanistic probing (attention/activation/logit-lens) reported; analyses are behavioral: comparison of TPR/TNR under few-shot, error-step localization accuracy, and human-vs-GPT-4 auto-labeler agreement. Dataset authors note they used GPT-4 as an automated error-reason judge and cross-validated against manual labels.",
            "failure_modes": "Paper documents model failure modes when performing arithmetic verification: sensitivity to few-shot example distribution (bias toward labels in demonstrations), inability to generalize beyond training response formats, the 'reversal curse' (treating A=B and B=A as non-equivalent), ontological/unit errors (misusing units or object properties), insensitivity to simple numerical computation errors, hallucinated/incorrect error explanations, and over-rejection or over-acceptance tendencies after fine-tuning.",
            "scaling_trend": "Performance does not strictly scale with parameter count; several smaller models outperform much larger ones (e.g., Phi3-3.8B beating bigger 7B/72B models in this task), and Llama3-70B outperformed Deepseek-v2-236B; authors conclude data quality, distillation and instruction tuning matter as much as model size.",
            "uuid": "e6436.0",
            "source_info": {
                "paper_title": "MR-GSM8K: A Meta-Reasoning Benchmark for Large Language Model Evaluation",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "MR-Score",
            "name_full": "Meta-Reasoning Score (MR-Score)",
            "brief_description": "A composite metric for the MR-GSM8K tasks combining (1) MCC for binary solution correctness, (2) accuracy of first-error-step localization, and (3) accuracy of error-reason explanation, weighted to reflect task difficulty.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_family": null,
            "model_size": null,
            "training_data_description": null,
            "benchmark_name": "MR-GSM8K evaluation metric",
            "task_type": "meta-reasoning evaluation (judge candidate solutions)",
            "problem_format": "Scoring of question+solution pairs with three subtasks",
            "difficulty_level": "Designed to be harder than stem GSM8K by requiring diagnosis of incorrect reasoning; intended for grade-school math problems",
            "prompting_method": "Used in evaluation pipeline; per-model evaluation output compared against annotated ground truth; also compared manual labeling vs. GPT-4 auto-labeling",
            "performance_metric": "Composite MR-Score built from: MCC (binary correctness), ACC_step (first-error-step accuracy), ACC_reason (error-reason accuracy); MR-Score normalized and weighted empirically.",
            "performance_value": "Reported MR-Scores per model (see MR-GSM8K entry); manual vs. GPT-4 auto-labeler MR-Score comparisons reported in Table 4 showing close agreement for top models (e.g., GPT-4 MR-Score by manual vs. GPT-4 auto-labeler ~0.495 vs 0.512 in a subset analysis).",
            "internal_analysis": "Design rationale: MCC chosen to counteract trivial bias (models that always say correct/incorrect), accuracy used for step and reason tasks; authors analyze auto-labeler (GPT-4) confusion matrix vs manual labels.",
            "failure_modes": "Authors note MR-Score can be targeted for optimization (Goodhart's law) and that auto-labeling of error-reasons is imperfect (false positives/negatives), limiting fully automatic scoring.",
            "scaling_trend": "Not applicable (metric design), but authors tuned weights empirically to maximize cross-model discrimination.",
            "uuid": "e6436.1",
            "source_info": {
                "paper_title": "MR-GSM8K: A Meta-Reasoning Benchmark for Large Language Model Evaluation",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "GPT-4-Turbo",
            "name_full": "GPT-4-Turbo (OpenAI)",
            "brief_description": "A closed-source large language model from OpenAI evaluated as an automatic judge and as an evaluated model on MR-GSM8K; it achieves the best reported MR-Score in the paper and is used as a proxy auto-labeler for error reasons.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4-Turbo",
            "model_family": "transformer (decoder-only, OpenAI family)",
            "model_size": "not specified in paper",
            "training_data_description": "Not specified in this paper (general web/code/token data and instruction-tuning typical of OpenAI models; paper does not assert specifics).",
            "benchmark_name": "MR-GSM8K (derived from GSM8K)",
            "task_type": "multi-step grade-school arithmetic word problems verification (correctness classification, first-error-step identification, error-reason generation)",
            "problem_format": "Natural-language question paired with a proposed step-by-step solution (scoring task).",
            "difficulty_level": "GSM8K-level multi-step problems including Program-of-Thought and reversed variants",
            "prompting_method": "Zero-shot scoring prompt (act as grade school math teacher) and few-shot (3-shot) evaluated; temperature=0 for inference; GPT-4 also used to synthesize in-domain diagnostic training data for fine-tuning experiments.",
            "performance_metric": "MR-Score (composite), MCC for correctness, ACC_step, ACC_reason, plus TPR/TNR for solution-correctness",
            "performance_value": "Top-performing model in paper: MR-Score ≈ 50.5 (zero-shot) and ≈ 53.0 (3-shot) as reported in Table 2; high MCC and the best ACC_step/ACC_reason among evaluated models.",
            "internal_analysis": "Behavioral analyses only: GPT-4 sometimes fails to generalize equivalence (reversal curse) and can miss simple arithmetic mistakes; GPT-4 was also used as a near-human auto-evaluator (confusion matrix vs. manual labels reported). No activation/attention probing provided.",
            "failure_modes": "Reversal curse (mis-evaluating equivalent equations), occasional insensitivity to trivial arithmetic errors, and provide erroneous error explanations in some examples; limited true understanding of fabricated errors when asked to generate diagnostic training data (GPT-4 sometimes fails to fabricate valid errors).",
            "scaling_trend": "N/A within paper (single closed-source model); authors note GPT-4's high performance relative to other models but do not analyze scaling within GPT-4 variants beyond k=0/k=3 prompting differences.",
            "uuid": "e6436.2",
            "source_info": {
                "paper_title": "MR-GSM8K: A Meta-Reasoning Benchmark for Large Language Model Evaluation",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "GPT-3.5-Turbo",
            "name_full": "GPT-3.5-Turbo (OpenAI)",
            "brief_description": "A widely used OpenAI chat model evaluated on MR-GSM8K; shows moderate-to-low MR-Score and notable weaknesses in error localization/explanation relative to GPT-4-Turbo.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-Turbo",
            "model_family": "transformer (decoder-only, OpenAI family)",
            "model_size": "not specified in paper",
            "training_data_description": "Not specified in this paper.",
            "benchmark_name": "MR-GSM8K",
            "task_type": "multi-step arithmetic word-problem verification and diagnosis",
            "problem_format": "Natural-language question paired with candidate solution",
            "difficulty_level": "GSM8K-level",
            "prompting_method": "Zero-shot and few-shot (3-shot) scoring prompts; temperature=0",
            "performance_metric": "MR-Score, MCC, ACC_step, ACC_reason; TPR/TNR reported for solution correctness task",
            "performance_value": "MR-Score reported ≈ 22.6 (k=0) and ≈ 17.9 (k=3) indicating mediocre performance and sometimes degradation with few-shot examples (per Table 2).",
            "internal_analysis": "Behavioral: shows increased TPR but decreased TNR under few-shot (models become biased by composition of examples); no mechanistic probing.",
            "failure_modes": "Susceptible to few-shot distribution bias (shifts to class frequencies in demonstrations), insensitivity to calculation errors, and poor error-reason explanations.",
            "scaling_trend": "Within this paper, standard sized GPT-3.5 shows lower MR-Score than GPT-4-Turbo; few-shot did not uniformly help and sometimes worsened results.",
            "uuid": "e6436.3",
            "source_info": {
                "paper_title": "MR-GSM8K: A Meta-Reasoning Benchmark for Large Language Model Evaluation",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Claude-3-Sonnet",
            "name_full": "Claude 3 Sonnet (Anthropic)",
            "brief_description": "A Claude-3 family chat model evaluated on MR-GSM8K; mid-ranked performance with notable hallucinated or incorrect error analyses in qualitative examples.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Claude3-Sonnet",
            "model_family": "transformer (Anthropic Claude family)",
            "model_size": "not specified in paper",
            "training_data_description": "Not specified in this paper.",
            "benchmark_name": "MR-GSM8K",
            "task_type": "multi-step arithmetic verification & diagnosis",
            "problem_format": "Natural-language question + candidate solution",
            "difficulty_level": "GSM8K-level",
            "prompting_method": "Zero-shot and few-shot (3-shot); temperature=0",
            "performance_metric": "MR-Score, MCC, ACC_step, ACC_reason",
            "performance_value": "MR-Score approximately 23.5 (k=0) and 20.8 (k=3) per Table 2.",
            "internal_analysis": "Behavioral observations: provided hallucinated error reasons in examples and made unit mistakes; no internal activation analyses provided.",
            "failure_modes": "Hallucinated or incorrect error explanations, unit/ontological errors (misunderstanding of object properties), and calculation mistakes ignored in diagnosis.",
            "scaling_trend": "Not discussed beyond reported comparative performance among Claude variants and other closed-source models.",
            "uuid": "e6436.4",
            "source_info": {
                "paper_title": "MR-GSM8K: A Meta-Reasoning Benchmark for Large Language Model Evaluation",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Claude-3-Haiku",
            "name_full": "Claude 3 Haiku (Anthropic)",
            "brief_description": "A member of the Claude-3 family measured on MR-GSM8K; shows lower MR-Score compared to Claude3-Sonnet and GPT-4.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Claude3-Haiku",
            "model_family": "transformer (Anthropic Claude family)",
            "model_size": "not specified in paper",
            "training_data_description": "Not specified in this paper.",
            "benchmark_name": "MR-GSM8K",
            "task_type": "multi-step arithmetic verification & diagnosis",
            "problem_format": "Natural-language question + candidate solution",
            "difficulty_level": "GSM8K-level",
            "prompting_method": "Zero-shot and few-shot; temperature=0",
            "performance_metric": "MR-Score, MCC, ACC_step, ACC_reason",
            "performance_value": "MR-Score reported ≈ 15.3 (k=0) and ≈ 4.9 (k=3) in Table 2 (showing unstable/falling few-shot behavior).",
            "internal_analysis": "Behavioral only: fluctuations in few-shot performance and bias effects; no representational analyses provided.",
            "failure_modes": "Susceptibility to few-shot distribution bias and degraded MR-Score under some few-shot contexts.",
            "scaling_trend": "N/A within paper.",
            "uuid": "e6436.5",
            "source_info": {
                "paper_title": "MR-GSM8K: A Meta-Reasoning Benchmark for Large Language Model Evaluation",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Llama3-70B",
            "name_full": "Llama 3 70B",
            "brief_description": "A large open-source chat model evaluated on MR-GSM8K; among open-source models it performed closest to GPT-4 but still substantially lower in MR-Score.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama3-70B",
            "model_family": "Llama family (decoder-only transformer)",
            "model_size": "70B",
            "training_data_description": "Not specified in this paper (standard Llama3 pretraining presumed); paper does not enumerate training corpora.",
            "benchmark_name": "MR-GSM8K",
            "task_type": "multi-step arithmetic verification & diagnosis",
            "problem_format": "Natural-language question + candidate solution",
            "difficulty_level": "GSM8K-level",
            "prompting_method": "Zero-shot and few-shot; temperature=0",
            "performance_metric": "MR-Score, MCC, ACC_step, ACC_reason",
            "performance_value": "MR-Score ≈ 34.2 (k=0) per Table 2 (substantially below GPT-4 but highest among many open-source models).",
            "internal_analysis": "Behavioral: Llama3-70B better follows evaluation instructions than many specialized math models; no internal mechanistic probes reported.",
            "failure_modes": "Still shows arithmetic insensitivity, step-misidentification for many incorrect solutions, and limited holistic reasoning across solution-space variations.",
            "scaling_trend": "As a 70B model, it outperformed some larger (236B) models examined, indicating scale alone is not determinant.",
            "uuid": "e6436.6",
            "source_info": {
                "paper_title": "MR-GSM8K: A Meta-Reasoning Benchmark for Large Language Model Evaluation",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Deepseek-v2-236B",
            "name_full": "Deepseek-v2 236B",
            "brief_description": "A very large open-source model (236B) included in evaluations; achieved mid-to-high MR-Score but was outperformed by some 70B models, indicating non-monotonic scaling.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Deepseek-v2-236B",
            "model_family": "transformer (open-source scaling family)",
            "model_size": "236B",
            "training_data_description": "Not specified in this paper.",
            "benchmark_name": "MR-GSM8K",
            "task_type": "multi-step arithmetic verification & diagnosis",
            "problem_format": "Natural-language question + candidate solution",
            "difficulty_level": "GSM8K-level",
            "prompting_method": "Zero-shot and few-shot; temperature=0",
            "performance_metric": "MR-Score, MCC, ACC_step, ACC_reason",
            "performance_value": "MR-Score ≈ 34.1 (k=0) per Table 2; authors note Llama3-70B outperformed Deepseek-v2-236B.",
            "internal_analysis": "Behavioral only: authors infer deficits from MR-Score and error analyses; no internal probing.",
            "failure_modes": "Overfitting-like behaviors, inability to assess reasoning outside familiar response formats, arithmetic insensitivity noted in case studies.",
            "scaling_trend": "Performance not strictly increasing with parameters; 236B model not uniformly superior to smaller 70B models.",
            "uuid": "e6436.7",
            "source_info": {
                "paper_title": "MR-GSM8K: A Meta-Reasoning Benchmark for Large Language Model Evaluation",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Phi3-3.8B",
            "name_full": "Phi-3 3.8B",
            "brief_description": "A small chat model (≈3.8B) that outperformed many larger models on MR-GSM8K, illustrating the importance of pretraining/data quality and distillation techniques over raw parameter count.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Phi3-3.8B",
            "model_family": "transformer (small chat model family)",
            "model_size": "3.8B",
            "training_data_description": "Not specified in this paper.",
            "benchmark_name": "MR-GSM8K",
            "task_type": "multi-step arithmetic verification & diagnosis",
            "problem_format": "Natural-language question + candidate solution",
            "difficulty_level": "GSM8K-level",
            "prompting_method": "Zero-shot and few-shot; temperature=0",
            "performance_metric": "MR-Score, MCC, ACC_step, ACC_reason",
            "performance_value": "MR-Score reported ≈ 22.9 (k=0) and competitive with larger medium models per Table 2.",
            "internal_analysis": "Behavioral: demonstrates that small well-distilled models can outperform larger ones; no internal mechanism analysis.",
            "failure_modes": "Typical arithmetic verification failures: mis-localized errors, reasoning superficiality, sensitivity to few-shot example composition.",
            "scaling_trend": "Counterexample to pure size-scaling: small model achieves strong performance due to training/data quality.",
            "uuid": "e6436.8",
            "source_info": {
                "paper_title": "MR-GSM8K: A Meta-Reasoning Benchmark for Large Language Model Evaluation",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Qwen-1.5-72B",
            "name_full": "Qwen 1.5 / 72B",
            "brief_description": "A large open-source family (Qwen) evaluated on MR-GSM8K; achieves mid-range MR-Score relative to other large models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Qwen1.5-72B",
            "model_family": "transformer (Qwen family)",
            "model_size": "72B",
            "training_data_description": "Not specified in this paper.",
            "benchmark_name": "MR-GSM8K",
            "task_type": "multi-step arithmetic verification & diagnosis",
            "problem_format": "Natural-language question + candidate solution",
            "difficulty_level": "GSM8K-level",
            "prompting_method": "Zero-shot and few-shot; temperature=0",
            "performance_metric": "MR-Score, MCC, ACC_step, ACC_reason",
            "performance_value": "MR-Score ≈ 23.3 (k=0) per Table 2.",
            "internal_analysis": "Behavioral only: compared across models; no internal probing.",
            "failure_modes": "Similar to others: few-shot bias, arithmetic insensitivity, and trouble generalizing to reversed and POT variants.",
            "scaling_trend": "Moderate performance for its size; again underscores that parameter count is not the only determiner.",
            "uuid": "e6436.9",
            "source_info": {
                "paper_title": "MR-GSM8K: A Meta-Reasoning Benchmark for Large Language Model Evaluation",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "WizardMath-v1.1-7B",
            "name_full": "WizardMath v1.1 7B",
            "brief_description": "A specialized math-focused 7B model (open-source) that performed poorly on MR-GSM8K due to overfitting to GSM8K response format and inability to follow the scoring instruction.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "WizardMath-v1.1-7B",
            "model_family": "open-source fine-tuned math model",
            "model_size": "7B",
            "training_data_description": "Fine-tuned on math datasets (GSM8K-like), but exact data specifics not given in this paper.",
            "benchmark_name": "MR-GSM8K",
            "task_type": "multi-step arithmetic verification & diagnosis (scoring)",
            "problem_format": "Natural-language question + candidate solution",
            "difficulty_level": "GSM8K-level",
            "prompting_method": "Zero-shot and few-shot (few-shot harmed performance), temperature=0",
            "performance_metric": "MR-Score, MCC, ACC_step, ACC_reason",
            "performance_value": "MR-Score very low (≈ 0.1 as reported), with near 100% TPR but near-zero TNR indicating over-acceptance or format overfitting.",
            "internal_analysis": "Behavioral: model strongly reverts to question-answering format and fails to follow scoring instructions even with few-shot; no internal representation analysis.",
            "failure_modes": "Overfitting to answer-generation format, inability to act as a scorer, very low error-localization and explanation accuracy.",
            "scaling_trend": "Specialized fine-tuning on narrow math datasets led to brittle behavior under new evaluation paradigm.",
            "uuid": "e6436.10",
            "source_info": {
                "paper_title": "MR-GSM8K: A Meta-Reasoning Benchmark for Large Language Model Evaluation",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "MetaMath-70B",
            "name_full": "MetaMath 70B",
            "brief_description": "A math-specialized 70B open-source model included in evaluations that performed poorly on MR-GSM8K, likely due to overfitting to familiar solution formats and inability to switch to scoring role.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "MetaMath-70B",
            "model_family": "math-specialized transformer",
            "model_size": "70B",
            "training_data_description": "Finetuned on math problem datasets (details not specified in this paper).",
            "benchmark_name": "MR-GSM8K",
            "task_type": "multi-step arithmetic verification & diagnosis",
            "problem_format": "Natural-language question + candidate solution",
            "difficulty_level": "GSM8K-level",
            "prompting_method": "Zero-shot and few-shot; temperature=0",
            "performance_metric": "MR-Score, MCC, ACC_step, ACC_reason",
            "performance_value": "MR-Score ≈ 0.0 in Table 2 (failed to adapt to evaluator role).",
            "internal_analysis": "Behavioral: specialized fine-tuning led to reverting to QA behavior; no internal mechanistic analyses.",
            "failure_modes": "Overfitting to GSM8K response format; inability to follow new scoring instruction, poor generalization to reversed and POT variants.",
            "scaling_trend": "Despite large size, poor MR performance due to training objective mismatch.",
            "uuid": "e6436.11",
            "source_info": {
                "paper_title": "MR-GSM8K: A Meta-Reasoning Benchmark for Large Language Model Evaluation",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Deepseek-Math-7B-RL",
            "name_full": "Deepseek-Math 7B RL",
            "brief_description": "An open-source math-specialized model with RL fine-tuning that better complied with scoring instructions and had moderate MR performance compared to other specialized models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Deepseek-Math-7B-RL",
            "model_family": "math-specialized transformer (RL-finetuned)",
            "model_size": "7B",
            "training_data_description": "Fine-tuned for math tasks and RL training (details not specified in this paper).",
            "benchmark_name": "MR-GSM8K",
            "task_type": "multi-step arithmetic verification & diagnosis",
            "problem_format": "Natural-language question + candidate solution",
            "difficulty_level": "GSM8K-level",
            "prompting_method": "Zero-shot and few-shot; temperature=0",
            "performance_metric": "MR-Score, MCC, ACC_step, ACC_reason",
            "performance_value": "MR-Score ≈ 11.6 (k=0) per Table 2 (decent among specialized models).",
            "internal_analysis": "Behavioral: better instruction compliance than several other specialized models; no internal mechanistic probes.",
            "failure_modes": "Still limited in fully diagnosing error reasons and localizing first errors across diverse incorrect-solution types.",
            "scaling_trend": "Fine-tuning method and instruction compliance matter more than raw size for this task.",
            "uuid": "e6436.12",
            "source_info": {
                "paper_title": "MR-GSM8K: A Meta-Reasoning Benchmark for Large Language Model Evaluation",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "llaMA2-70B-MR (finetuned)",
            "name_full": "llama-2-70B finetuned with diagnostic MR data (llama2-70B-MR)",
            "brief_description": "An in-domain fine-tuned model: Llama2-70B-base was fine-tuned with GPT-4-generated diagnostic incorrect solutions (5k incorrect + 4k correct) using QLoRA; showed minor MR improvements but still low absolute diagnostic accuracy.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "llama2-70B-MR",
            "model_family": "Llama 2 transformer (decoder-only), finetuned",
            "model_size": "70B",
            "training_data_description": "GSM8K training set augmented with GPT-4-generated diagnostic incorrect solutions (5k incorrect, 4k correct); fine-tuned with QLoRA.",
            "benchmark_name": "MR-GSM8K",
            "task_type": "multi-step arithmetic verification & diagnosis",
            "problem_format": "Natural-language question + candidate solution",
            "difficulty_level": "GSM8K-level",
            "prompting_method": "Zero-shot scoring; finetuned on in-domain diagnostics instead of or in addition to few-shot prompts; temperature=0 at inference.",
            "performance_metric": "MR-Score, TPR/TNR, MCC, ACC_step, ACC_reason",
            "performance_value": "After fine-tuning: TPR ≈ 31.74%, TNR ≈ 73.49%, MCC ≈ 5.8%, ACC_step ≈ 20.79%, ACC_reason ≈ 6.29%, MR-Score ≈ 10.5% (reported in Section B).",
            "internal_analysis": "Behavioral: fine-tuned model tended to over-reject solutions and was less likely to accept; of correctly-diagnosed cases many involved POT and reversed types despite training only on original problems. No internal mechanism probing.",
            "failure_modes": "Low absolute rates of correct diagnosis for incorrect solutions (esp. error reasons), tendency to over-reject, limited improvement from simple in-domain fine-tuning.",
            "scaling_trend": "Fine-tuning with targeted diagnostic examples improves some metrics but does not yield high diagnostic accuracy; demonstrates difficulty of the MR task beyond dataset augmentation.",
            "uuid": "e6436.13",
            "source_info": {
                "paper_title": "MR-GSM8K: A Meta-Reasoning Benchmark for Large Language Model Evaluation",
                "publication_date_yy_mm": "2023-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
            "rating": 2,
            "sanitized_title": "program_of_thoughts_prompting_disentangling_computation_from_reasoning_for_numerical_reasoning_tasks"
        },
        {
            "paper_title": "The reversal curse: Llms trained on \"a is b\" fail to learn \"b is a.\"",
            "rating": 2,
            "sanitized_title": "the_reversal_curse_llms_trained_on_a_is_b_fail_to_learn_b_is_a"
        },
        {
            "paper_title": "Measuring mathematical problem solving with the math dataset",
            "rating": 2,
            "sanitized_title": "measuring_mathematical_problem_solving_with_the_math_dataset"
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 1,
            "sanitized_title": "training_verifiers_to_solve_math_word_problems"
        }
    ],
    "cost": 0.02501875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>MR-GSM8K: A Meta-Reasoning Benchmark for Large Language Model Evaluation</p>
<p>Zhongshen Zeng zszeng23@cse.cuhk.edu.hk 
Pengguang Chen pgchen@cse.cuhk.edu.hk 
Shu Liu sliu@smartmore.com 
Smartmore Co 
Ltd 
Haiyun Jiang haiyunjiang@tencent.com 
Jiaya Jia </p>
<p>Chinese University of HongKong</p>
<p>Chinese University of Hong Kong</p>
<p>Tencent AI Lab</p>
<p>Chinese University of Hong Kong</p>
<p>MR-GSM8K: A Meta-Reasoning Benchmark for Large Language Model Evaluation
2BFA17D05EE693A2C4911D7B6AE5F5B8
In this work, we introduce a novel evaluation paradigm for Large Language Models (LLMs) that compels them to transition from a traditional question-answering role, akin to a student, to a solution-scoring role, akin to a teacher.This paradigm, focusing on "reasoning about reasoning," hence termed meta-reasoning, shifts the emphasis from result-oriented assessments, which often neglect the reasoning process, to a more comprehensive evaluation that effectively distinguishes between the cognitive capabilities of different models.By applying this paradigm in the GSM8K dataset, we have developed the MR-GSM8K benchmark.Our extensive analysis includes several state-of-the-art models from both open-source and commercial domains, uncovering fundamental deficiencies in their training and evaluation methodologies.Notably, while models like Deepseek-v2 and Claude3-Sonnet closely competed with GPT-4 in GSM8K, their performance disparities expanded dramatically in MR-GSM8K, with differences widening to over 20 absolute points, underscoring the significant challenge posed by our meta-reasoning approach.</p>
<p>Introduction</p>
<p>Pretrained on trillions of tokens and equipped with billions of parameters, today's large language models [25,1,33] are capable of generating coherent texts and achieving super-human performances in many tasks [8,15].With the aim of differentiating cognitive abilities among models, math questions are often chosen as a proxy evaluation task.However, despite the complexity and diversity of these math problems, recent state-of-the-art LLMs [25,38,13] have achieved accuracy rates exceeding 80% [20] on multi-step math reasoning datasets like GSM8K [10].</p>
<p>Upon detailed examination of the design principles and objectives of current math datasets, we identified several key shortcomings.Firstly, the majority of these datasets focus on result-oriented metrics, scoring accuracy based solely on the final answer, without considering the underlying reasoning process.With the emergence of the chain of thought methodology [34] and its derivative techniques [9,36] as the de facto standard for evaluating reasoning processes, we argue that the resultdriven evaluation method may be insufficient for a comprehensive assessment of intended cognitive and reasoning capabilities.Secondly, a recent study [26] suggests that some LLMs, which achieved state-of-the-art performances in GSM8K and MATH [15] benchmarks, demonstrate unexpectedly low performance when facing newly released Hungarian high school exams.This raises concerns about data contamination and potential overfitting to the benchmarks, and it challenges the efficacy of these benchmarks in differentiating model capabilities.In response to these identified limitations, we introduced a novel paradigm that shifts the role of the evaluated model from a question-answering student to a solution-scoring teacher.Specifically, instead of delivering potential solutions to given questions, which may be subject to data contamination issues, the evaluated models are now presented with question-solution pairs and tasked with determining solution correctness, identifying potential first error steps, and providing reasons for errors.This paradigm shift challenges the evaluated models to engage in meta-reasoning about different reasoning processes, a concept we term "meta-reasoning" in this paper.</p>
<p>Following this design principle, we have developed a new benchmark named Meta-Reasoning-GSM8k (MR-GSM8k) and proposed a novel metric called MR-Score.Our benchmark, characterized by instances manually labeled by experts and rigorously reviewed, serves as a robust tool for both qualitative and quantitative assessments of language models.Our findings indicate that most stateof-the-art models demonstrate a significant performance decline in this more nuanced assessment.As demonstrated in Figure -1, although state-of-the-art models exhibit comparable performance in GSM8K, there is considerable variance in their effectiveness on our benchmark, with discrepancies up to more than tenfold.</p>
<p>We argue that our evaluation paradigm not only introduces a metric that more effectively differentiates based on the reasoning process over mere computational outcomes, but it also exposes fundamental deficiencies within current evaluation and training methodologies.First, as detailed in Section-5, our experiments revealed that specialized math models struggle to generalize their reasoning abilities to our new paradigm, regardless of whether they are directed by specific instructions or through few-shot in-context learning.Second, we observed that contrary to the common belief that larger models inherently perform better in reasoning tasks, smaller chat models such as Phi-3-Mini [14] can surpass those twenty times their size.This underscores the significance of data quality and techniques like knowledge distillation in enhancing model performance.Third, our empirical findings indicate that current models still engage in superficial mathematical reasoning, exhibiting flaws such as a lack of ontological understanding [32], susceptibility to the reversal curse [6], and inadequate global comprehension of solution spaces.</p>
<p>In conclusion, our paper significantly contributes to the field in the following ways:</p>
<p>• We introduce a novel evaluation principle, the accompanying open-source benchmark MR-GSM8k, and the metric MR-Score.</p>
<p>• We demonstrate the effective transformation of an existing benchmark (e.g., GSM8K) and how such modification can lead to robust evaluation against potential overfitting and data contamination.</p>
<p>• We conduct comprehensive experiments on an array of state-of-the-art models using the MR-GSM8k benchmark, highlighting critical shortcomings in current training and evaluation paradigms.</p>
<p>• Through analysis of cognitive levels and examination of holistic coverage on the solution space, we emphasize the need for benchmarks that go beyond surface-level evaluations, fostering more sophisticated and nuanced AI development.</p>
<p>Related Works</p>
<p>Complex reasoning tasks, such as math problems, have long been recognized as effective proxies for gauging the cognitive abilities of language models [29,17,30,27,23,15,10].These tasks require the ability to understand symbols and text, dissect problems into logically connected sub-problems, combine results, and synthesize final solutions.They engage cognitive functions such as pattern induction, formula recall, deductive rule application, and abstract symbolic reasoning.</p>
<p>GSM8K [10] and MATH [15] have been prominent benchmarks for evaluating the math reasoning capabilities of large language models (LLMs) in recent years.The chain of thought approach, proposed by [34], addresses multi-step reasoning tasks by breaking them down into manageable steps.</p>
<p>Stanford Alpaca [31] has popularized the knowledge distillation method of cloning abilities from ChatGPT [24] by generating related QA pairs.WizardMath [20] further refined this distillation by specifying the difficulties of the QA pairs in the generation process.Mammoth [39] combines chain of thought and program of thought, finetuning its models with answers generated by GPT-4 [25] in either natural or code language.MetaMath [38] broadens the variety of question types by introducing forward/backward reasoning variations.</p>
<p>Despite significant advancements in math reasoning, evidence suggests that large language models may not fully master reasoning or even understand their own outputs.For example, [12] found that LLMs fail to generalize to questions of varying complexity from their training data.[2] demonstrated that, despite occasional analytical brilliance, GPT-4 is still severely limited in its reasoning capabilities.</p>
<p>Similarly, [16] and [37] have shown that ChatGPT struggles to judge the correctness of math problem solutions.However, our work focuses on constructing a qualitative and quantitative evaluation framework and discusses the evaluation principles and deficiencies of the current training paradigm in greater depth.</p>
<p>3 Dataset Construction</p>
<p>Construction</p>
<p>Question Types: The MR-GSM8k consists of three distinct types of questions.The first type includes original GSM8K instances, sampled directly from the dataset [10].The second type modifies GSM8K questions to include prompts requesting code solutions, as explored in [39].The third type, termed reversed reasoning [38], involves concealing one of the inputs and asking for solutions that compute the missing input using the provided original answer.These variations were selected for their potential to broaden the scope of reasoning methodologies in LLMs.The "Program of Thought" approach, which includes code solutions, was proposed by [21] and has been empirically validated as an effective framework for math reasoning [39,13].Reversed reasoning, a method that tests whether models can effectively learn backward relationships, has been recently emphasized in studies such as [5], presenting additional challenges for these systems.</p>
<p>Solution:</p>
<p>For each question collected, we used MetaMath-7B [38] with a temperature setting of 1 to generate step-by-step solutions.We found this setting results in outputs with decent accuracy and nuanced mistakes that we would like the annotators and evaluated models to figure out.Intentionally, we targeted a result accuracy of approximately 50% for the collected question-solution pairs, ensuring a balance between correct outcomes and those arising from flawed reasoning processes.</p>
<h3>Instruction:</h3>
<p>• An electronics seller bought 5 phones for $700 each and gives the seller $4000 in dollar bills.How much will the seller give back in change?Let's write a program.</p>
<h3>Instruction:</h3>
<p>• An electronics seller bought 5 phones for $700 each and gives the seller $4000 in dollar bills.How much will the seller give back in change?</p>
<h3>Instruction: Figure 2: Structure of the MR-GSM8k benchmark and its evaluation paradigm.</h3>
<p>Annotation</p>
<p>For each collected question-solution pair, a panel of selected annotators was recruited to review each question-solution pair for its reasoning process and determine the following fields:</p>
<p>Solution Correctness: Solutions that yield a final output differing from the established ground truth are automatically marked as incorrect.However, in cases where the solution's final output aligns with the ground truth, annotators are tasked with reviewing the entire reasoning path.Their objective is to ascertain whether the correct output is the result of a logical and sensible reasoning process.</p>
<p>First Error</p>
<p>Step: This attribute is applicable for solutions with either an unmatched final output or a matched final output underpinned by flawed reasoning.Annotators identify the initial step in the reasoning process where the logic deviates from correctness.In line with the approach of [18], we dissected GSM8K solutions into discrete steps, each marked by a newline character, and indexed them accordingly.Each step is then categorized as positive, neutral, or negative.Positive and neutral steps represent stages in the reasoning process where the correct final output remains attainable, whereas negative steps indicate a divergence from the path leading to the correct solution.</p>
<p>Error Analysis: Beyond identifying the first erroneous step, annotators are also responsible for conducting an in-depth analysis of the reasoning that led to the error.This involves an examination of the solution's reasoning flow, focusing on the cause of the initial error and what the correct line of reasoning should have been at that juncture.This error analysis is subsequently compared against the reasoning errors identified by the evaluated models during testing, to assess their accuracy and validity.</p>
<p>Quality Control</p>
<p>Annotators: Our panel of annotators is both well-trained and highly selective.Each candidate is required to thoroughly review our annotation manual (see Appendix-D for details) and watch a demonstration video.Following this, candidates undergo a trial labeling process using a small, Annotation Procedure: Every question in the MR-GSM8K undergoes multiple levels of scrutiny before being used in the evaluation process.Initially, each question is labeled twice by different annotators to ensure consistency.Any discrepancies in labeling, particularly regarding solution correctness or the identification of the first error step, are flagged and reviewed by a quality control supervisor.Furthermore, 50 percent of the labeled questions are randomly selected for a second round of quality control to identify and eliminate any inconsistent error steps or reasons.In the final stage of quality control, approximately 10 percent of the questions are manually inspected by the authors to ensure accuracy.This rigorous process ensures that any questions with incorrect error steps or reasoning are rectified before inclusion in the final dataset.</p>
<p>Dataset Statistics</p>
<p>Table-1 presents the statistics of MR-GSM8K, illustrating the distribution of correct and incorrect solutions across the three different types of questions.It is noteworthy that the reversed question type exhibits a significantly longer average question length due to its construction methodology.This type of question, due to its complex nature, also tends to have longer solution lengths as indicated in the table.Conversely, questions classified under the Program-of-Thought category, which typically require code solutions, have the shortest solution lengths, reflecting the concise and succinct nature of coding language.Despite these variations in average solution lengths and step counts, the average position of the first error step remains remarkably consistent across all question types.</p>
<p>Evaluation Process &amp; Metric</p>
<p>Evaluation As illustrated in Figure-2, instead of simply solving a given question, the evaluated models are now presented with a question and a paired potential solution to score (e.g., the upper light blue part in the figure).Specifically, the evaluated model (e.g., the lower white part) is required to predict the correctness of the solution.If the solution is deemed incorrect, the model must further identify the first-error-step and explain the error-reason.The solution-correctness and firsterror-step are scored automatically based on comparison with manual annotations.Only when the evaluated model correctly identifies an incorrect solution and its first-error-step will its error-reason be further examined, either manually or automatically by models (see our discussions on error reason examination in the limitation section of Appendix-A).</p>
<p>Metrics To provide a unified and normalized score reflecting the overall competence of the evaluated model, we propose a novel metric named MR-Score, consisting of three sub-metrics.The first sub-metric is the Matthews Correlation Coefficient (MCC, [22]) for binary classification of solutioncorrectness.
M CC = T P × T N − F P × F N (T P + F P ) × (T P + F N ) × (T N + F P ) × (T N + F N )(1)
Here, TP, TN, FP, and FN represent true positive, true negative, false positive, and false negative, respectively.The MCC score ranges from -1 to +1, where -1 indicates total disagreement between prediction and observation, 0 suggests performance near random, and +1 denotes perfect prediction.In the context of this paper, negative values are interpreted as no better than random guesses, and 0 is set as the cut-off threshold for normalization purposes.</p>
<p>The second metric is the accuracy of the first-error-step prediction:
ACC step = N correct_first_error_step N incorrect_sols(2)
The third metric calculates the accuracy of identifying both the correct first-error-step and the error-reason:
ACC reason = N correct_error_reason N incorrect_sols(3)
MR-Score is a weighted combination of these three metrics:
M R-Score = w 1 * max(0, M CC) + w 2 * ACC step + w 3 * ACC reason(4)
The weights w 1 , w 2 , and w 3 are chosen empirically to maximize differentiation between model performances by taking the difficulties of each task into account.For an extended discussion on the design of MR-Score, please refer to Appendix-C.</p>
<p>Experiments</p>
<p>Experiment Setup</p>
<p>To evaluate the performance of different language models on our benchmark, we selected models from diverse backgrounds.These models vary greatly in size, ranging from a few billion parameters, such as Qwen-v1.5-1.8B[3], to 70 billion parameters like Llama3-70B [33], and up to 236 billion parameters as seen in Deepseek-v2-236B [7].Additionally, to contrast performances between models fine-tuned from general instructions and those specialized in math problems, we included representative math models from the open-source community, such as WizardMath-v1.1-7B[20], MAmmoTH-70B [39], DeepseekMath-7B-RL [28], and MetaMath-70B [38].Furthermore, to explore the differences between commercial and open-source models, we included models from the OpenAI GPT family [24] and the Anthropic Claude-3 series [1].</p>
<p>Each model was evaluated under a zero-shot setting to assess their ability to follow instructions and their mathematical reasoning capabilities.Given that some evaluated models are not fine-tuned for general instruction following, we also tested each model under a few-shot setting to leverage their in-context learning abilities for understanding mathematical reasoning (see our prompts in Figure-4 and 5 in the Appendix).To ensure reproducibility and minimize variance, the inference temperature was set to zero across all models.</p>
<p>Experiment Results</p>
<p>Our evaluation results are presented in Table-2, where tasks 1, 2, and 3 correspond to determining solution correctness, the first error step, and the error reason, respectively.For Task 1, we also report the true positive rate and true negative rate.Key observations from our study are as follows:</p>
<p>Overall Performance: As depicted in the table, GPT-4-Turbo significantly outperforms all other models across both open-source and closed-source domains.Among the open-source models, Llama3-70B exhibits the closest performance to GPT-4, yet it still lags by more than 12 absolute points in MR-Score.In the small to medium model size category, Phi3-3.8Boutshines others, even surpassing the scores of Claude3-Haiku.Notably, most specialized models we evaluated failed to adapt to our evaluation paradigm and systematically underperformed compared to the generalized chat models.Despite similar levels of success on GSM8K, as illustrated in Figure -1, all models tested show a significant drop in performance in our benchmark, resulting in a much wider differentiation in scores.</p>
<p>Performance by Model Size: Contrary to the common belief that larger models inherently possess greater capabilities, our findings challenge this notion.Specifically, Phi3-3.8Bperformed substantially better than other models at the 7B level and achieved comparable performance with Qwen1.5-72B, which is approximately twenty times larger.A similar trend is observed between Llama3-70B and Deepseek-v2-236B, with Llama3-70B outperforming the latter in both zero-shot and few-shot settings.These results suggest that while model size is an important factor in reasoning ability, the quality of pretraining data and the application of data synthesis techniques such as knowledge distillation may also play crucial roles.</p>
<p>Specialized Math Models:</p>
<p>Within the open-source community, a multitude of models are dedicated to math reasoning, employing various fine-tuning techniques and datasets.Despite this, most models failed significantly in our benchmark.Specifically, WizardMath-v1.1-7B and MetaMath-70B appeared overfitted to the GSM8K response format and were unable to adhere to our evaluation instructions, both with and without few-shot demonstrations.Conversely, Deepseek-Math-7B-RL and MAmmoTH-70B managed to comprehend our complex evaluation instructions and achieved decent performance.</p>
<p>Few Shot Demonstrations: Given that the MR-GSM8K benchmark poses significant challenges in complex instruction-following in addition to mathematical reasoning, we explored whether providing few-shot demonstrations could enhance the performance of specialized models.However, as indicated in Table-2, the few-shot setting proved detrimental to all tested models.Although Deepseek-Math-7B-RL and MAmmoTH-70B demonstrated decent performance in the zero-shot setting, they struggled to adhere to the desired task instructions in the few-shot setting, reverting to the question-answering paradigm when presented with extended context and demonstrations.Similarly, few-shot examples did not aid WizardMath-v1.1-7B and MetaMath-70B in adhering to the expected scoring and reasoning format.These specialized models exhibited a strong tendency to revert to the question-answering paradigm, an issue not as prevalent in general chat models.This tendency suggests that fine-tuning on a narrowly focused dataset, often sampled or augmented from specific math datasets like GSM8K, may lead to overfitting to a particular input/output data distribution, resulting in only a superficial mastery of mathematical reasoning.Despite the close similarity between MR-GSM8K and the datasets these models were exposed to, their underperformance highlights a critical shortcoming in the generalization capabilities of reasoning abilities developed through specialized fine-tuning.</p>
<p>For general chat models, the impact of few-shot demonstrations varied significantly across models, even within the same family, with outcomes ranging from slight improvements to notable deteriorations.No consistent pattern emerged, indicating that improvements do not uniformly correlate with model size or initial benchmark performance.</p>
<p>In Context Learning Bias: An intriguing outcome from our few-shot experiments is the significant impact this setting had on the models' propensity to score solutions as correct or incorrect.Analyzing  6 Discussion</p>
<p>Case Studies on Reasoning Failures</p>
<p>In Appendix-D, we provide several case studies of responses generated by evaluated models.These examples help illuminate various reasoning errors that are often obscured in simpler benchmarks such as GSM8K.First, models frequently exhibit what is known as the "reversal curse," where they mistakenly claim that "A equals to B" is not equivalent to "B equals to A" [5].Second, inconsistencies and errors in quantity unit usage by the models suggest a fundamental lack of ontological understanding regarding the properties of the quantities they manipulate.Third, many models display insensitivity to numerical computation errors, highlighting an inherent weakness in arithmetic processing by language models.These observations underscore the critical gaps in model training and evaluation, pointing to an over-reliance on correct solution paths that neglects the broader context and complexity of real-world reasoning.</p>
<p>What Is the Significance of Reason About Reasoning?</p>
<p>In this paper, we have demonstrated that simply observing computation results is insufficient to uncover the cognitive depth of evaluated models.Equally important is the validity and logic of the reasoning process employed by these models.For a model to successfully diagnose solution correctness, it must infer the correct result and also engage in counterfactual reasoning along different reasoning paths, actively examining the conditions and assumptions made at various steps.Success in this paradigm is unlikely without a holistic understanding and robust mastery of the underlying concepts.Thus, the "reason about reasoning" paradigm emerges as a critical meta-evaluative tool.</p>
<p>Another key significance of this paradigm is its capability to transform any existing benchmark into a more robust and holistic assessment tool.As highlighted by [4] and [35], data contamination issues are becoming increasingly prevalent and elusive to detect.Our paradigm not only facilitates modifications to existing benchmarks but also demonstrates robustness against potential data contamination, as evidenced by our experiments across a wide array of state-of-the-art LLMs.</p>
<p>Conclusion</p>
<p>Throughout this paper, we have explored the inadequacies of prevalent math reasoning benchmarks and introduced a pioneering evaluation paradigm that compels models to engage in meta-reasoning.</p>
<p>Our empirical findings demonstrate that this novel paradigm enables our benchmark to effectively differentiate between models and uncover their various deficiencies.This differentiation has been particularly evident in the performance struggles of state-of-the-art language models when confronted with our benchmark, revealing significant shortcomings in current training methodologies.</p>
<p>These revelations underscore the need for a critical reevaluation of existing training and evaluation practices in the realm of large language models.By advocating for the widespread adoption of our "reason about reasoning" evaluation paradigm, we encourage researchers to adapt and broaden other reasoning benchmarks similarly.Such transformation is vital not only for a more rigorous assessment of LLMs but also for fostering a deeper and more holistic understanding of these models' capabilities.In this study, we proposed the MR-Score as a unified and normalized metric to evaluate the language models.The MR-Score consists of three sub-metrics, with the error reason being just one of the evaluation criteria.Similar to translation tasks, where expressions in one language may correspond to many variations in another, it is challenging to develop an automatic evaluator that scores the</p>
<p>Pos</p>
<p>Prompt for Few-Shot Scoring</p>
<p>Act as a grade school math teacher and score the following problem solution.</p>
<p>Your task involves three parts: Figure 5: This is the few shot prompt we used to evaluate all the models error reason perfectly.Despite these limitations, this does not undermine the arguments we present, nor does it affect the cognitive deficiencies unveiled by this metric.To the best of our knowledge, GPT-4 has been the most popular choice for serving as an automatic evaluator across different metrics [40,19].In Appendix-C, we empirically demonstrate that GPT-4 can serve as a decent automatic evaluator, with the final MR-Score based on its labeling results closely matching those of manual labeling.</p>
<p>Limitations of the Meta Reasoning Evaluation Paradigm and MR-GSM8k Dataset</p>
<p>Reflecting on Goodhart's law, which states that "When a measure becomes a target, it ceases to be a good measure," it's evident that the "reason about reasoning" paradigm is not immune to this phenomenon.This paradigm, like any other, can be targeted for optimization.However, our evaluation paradigm presents a greater challenge to overfitting compared to others, due to its demand for a comprehensive understanding within a broad error space, as illustrated in Section-B.</p>
<p>On the other hand, the meta-reasoning evaluation framework in MR-GSM8K, while innovative, is not without its limitations.Firstly, its applicability may be restricted when it comes to subjects that are inherently holistic or creative in nature, such as humanities or sociology.These subjects often require a comprehensive understanding and modification (e.g.essay writing), which can be challenging to break down into specific, sequential reasoning steps and corrections.Secondly, MR-GSM8K is currently confined to questions in English.This could potentially limit the scope of reasoning challenges that can be explored, as different languages may present unique cognitive and linguistic hurdles.Lastly, the analysis and correction of errors in the reasoning steps are currently based on solutions generated by MetaMath-7B model only.It's important to note that different LLMs and different individuals, may exhibit distinct reasoning and error patterns.Therefore, it would be beneficial to broaden the spectrum of solutions analyzed, incorporating a more diverse range of LLMs and even human responses.This would not only enhance the robustness of the evaluation framework but also provide a more nuanced understanding of the reasoning processes at play.</p>
<p>Models</p>
<p>Step</p>
<p>B In Domain Finetuning</p>
<p>Given the challenges posed by the novel "reason about reasoning" task paradigm, we explored how much targeted task-specific training data could enhance the performance of current state-of-the-art models on this task.We considered augmenting the GSM8K training set with diagnostics data in a similar format.However, due to the labor-intensive nature of manual annotation, we opted for a more feasible approach using an expert-designed procedure where GPT-4 generates the training data based solely on the original GSM8K problems, excluding any Program of Thought (POT) or reversed transformations.</p>
<p>This process involved presenting GPT-4 with a question and its correct solution, then instructing it to introduce an error at a randomly chosen step and complete the solution accordingly.The step-by-step analysis was subsequently generated, focusing on the fabricated error.Despite GPT-4's modest 40+% accuracy in correctly identifying incorrect solutions in the test set, this procedure successfully generated accurate diagnostic training data with over 90 percent accuracy, as verified by expert annotation on a sub-sampled set.This is largely due to the expert designed procedure (Figure -7) that greatly lowered the difficulties for instruction following.Note, occasionally GPT4 will fail to fabricate a valid error due to the lack of true understanding of errors (e.g., switch the fraction 8/3 to 2 and 2/3 then claims this is an error).</p>
<p>Prompt for creating DiagGSM8K training data by</p>
<p>Hi ChatGPT, I need your assistance in creating a math dataset that focuses on annotated incorrect solutions.The dataset is intended for grade school math.For our base model, we used llama-2-70B-base, consistent with the approach of other open-source SOTA math models.We merged the GSM8K training set with the GPT-4 generated diagnostic data, consisting of 5k incorrect solutions and 4k correct solutions.For fine-tuning, we employed the Qlora method [11], maintaining the same hyperparameters as used for MetaMath-70B.The evaluated results indicate a 31.74%true positive rate and 73.49% true negative rate, which lead to a 5.8% score in MCC.The accuracy for first-error-step and error-reason is 20.79% and 6.29% respectively.The MR-Score for this finetuned model is 10.5%.</p>
<p>Notably, the fine-tuned Llama2 model demonstrated a distinct tendency from that of GPT3.5 and other open-source models; it was less inclined to accept solutions uncritically, tending instead to over-reject solutions regardless of their correctness.As depicted in Figure -8, of the 99 questions where the model accurately predicted both correctness and the first error step, a significant portion involved questions with POT and reversed reasoning types.This is particularly noteworthy given that the model was trained exclusively on original questions.</p>
<p>Caution is necessary when interpreting the outcomes of in-domain fine-tuning.Although the finetuned model achieved improved results, it is important to recognize that the overall number of correct diagnoses for incorrect solutions remains relatively low (e.g., 6.29%).This underscores the challenging nature of our MR-GSM8k benchmark, where effective diagnosis across diverse solution spaces requires a comprehensive understanding of the problem.Consequently, simple fine-tuning strategies may not yield substantial improvements in performance.</p>
<p>C Design thinking of MR-Score</p>
<p>The MR-Score is consist of three sub-metrics corresponds to the three sequential reasoning subtasks.For the first solution correctness prediction, we empirically noticed that most of the evaluated language models tend to either blindly classify the given solution as correct or incorrect, exemplified by the low true-positive/true-negative rate in Table-2.Therefore, we chose the MCC score instead of metrics like F1 or Balanced-Accuracy due to its value range.The models that have high true-positive rate but low true negative rate will have near zero under the MCC metric and vice versa.For the second and third tasks of locating first error step and elucidating error reason, we chose the simple accuracy metric.One of the reason is that locating the first error step is a multi-class classification problem and it is difficult to have large prediction bias while at the same time scores high accuracy.</p>
<p>Similarly, the explaining error reason task is a free-form generation task that requires substantial understanding and a simple accuracy metric is enough to categorize the model behavior.</p>
<p>As to the weights given to the three metrics, they are crafted by considering the task complexity and the difference between manual labelling and auto labelling results of the error reason.As discussed in Section-A, we chose the GPT-4-Turbo as our proxy evaluator and Table-4 is the results of autolabelling vs our expert manual-labelling.It is clear that the final MR-Score calculated from manual labelling VS auto labelling are very close to each other, exhibiting the potential of GPT4 to serve as a delegate evaluator for our task.</p>
<p>Table-3 displays the confusion matrix based on GPT4's labelling of all the error reasons.Notably, GPT4 is able to achieve 82% of overall accuracy despite a substantial higher false positive rate than false negative rate.However, we still encourage large tech companies, who have the resources to bear manual labelling costs, to release open-source manual labelling results when publishing findings using MR-GSM8k for the best of rigorousness.</p>
<p>D Annotation Manuals, Examples and Case Studies</p>
<p>MR-GSM8K Annotation Manual</p>
<p>Annotation Manual This annotation task involves annotating math in grade school level.Specifically, for each math problem and its solution process, it is necessary to judge whether it is correct or not.</p>
<p>If there is an error in the solution process, the erroneous step number and the reason for the error must be annotated.</p>
<p>Below is the format description of the annotation data.Problem: The "problem" part provides the math problem.</p>
<p>ground-truth-solution: "ground-truth-solution" refers to the reference standard answer for the math problem.For some problems, the solution process in this reference answer may be incorrect, but their final answers are correct.Nevertheless, please read this reference answer carefully and ensure you understand the problem before judging the annotated solution process.Report any problem if you believe the question is incorrect or missing any condition.</p>
<p>model-output-steps "model-output-steps" is the answer output by the language model and the object we need to annotate.Read each step of this solution process carefully and judge whether the thought process is correct.If there is an error, identify at which step the error begins.Note each step can be categorized as positive, neutral, or negative.Positive and neutral steps represent stages in the reasoning process where the correct final output remains attainable, whereas negative steps indicate a divergence from the path leading to the correct solution.We need to find the first step where an error occurs.Reasons for errors include, but are not limited to, calculation errors (e.g., 11*11=133 is a calculation error), formula errors (such as multiplying speed by days instead of hours, which is a unit error), misunderstanding errors (such as not understanding the problem and making nonsensical statements), or missing positive/negative signs.If a step is unnecessary, redundant, or repetitive, but does not directly cause an error or prevent reaching the final result, do not judge this step as an error.Note: (It was found that the solution steps of ten problems are not correctly segmented.If encountered, please manually segment them into Step 1, Step 2, etc., in the same format before annotating.)</p>
<p>model-output-answer-correctness "model-output-answer-correctness" compares the language model's output with the standard answer.Its value can be "correct" or "wrong."Even if some answers are marked as "correct," the solution process may still be incorrect, so the solution steps must still be judged step by step.Generally, this option does not need modification.If modification is necessary, please report the UUID number and keep a record.</p>
<p>model-output-solution-correctness "model-output-solution-correctness" is the first field we need to annotate.If "model-output-answer-correctness" is "correct," you need to judge the solution process.If it is confirmed that the correct result is reached through the correct steps, this column should be marked "correct;" otherwise, mark "wrong."If "model-output-answer-correctness" is "wrong," this column defaults to "wrong."</p>
<p>model-output-solution-first-error-step "model-output-solution-first-error-step" is the second field we need to annotate.If the solution process in the first column is correct, then this column and the next column for the reason of the error should be filled with "N/A."If the solution process is incorrect, identify the first erroneous step and fill in this column with Arabic numerals "1", "2", "3", etc.</p>
<p>model-output-solution-first-error-reason "model-output-solution-first-error-reason" is the third column that needs annotation.If the solution process is correct, fill in "N/A;" if incorrect, specify the reason for the first error in detail.Describe the erroneous process, what misunderstanding led to the incorrect inference, then provide the correct answer, rather than just stating the misunderstanding followed by the correct answer.The purpose of annotating reasons for errors is mainly for later evaluation of some open-source and closed-source models, like ChatGPT, GPT4, to assess their accuracy in identifying erroneous steps and reasons, and to compare their provided reasons for errors with those marked by humans.So, when explaining the reason for an error, assume you are explaining to another person why it is wrong, and the explanation should be as comprehensive as possible.Since there might be multiple perspectives to judge the model's reasons for errors.All possible viewpoints should be mentioned.The ultimate goal is for another annotator, seeing your reasons for error and the model's reasons, to be able to quickly compare and understand whether the model has correctly identified the reasons for the error.</p>
<p>-Some examples of how to write reasons for errors: "According to the problem, John was stuck in traffic for the first two hours and did not immediately turn around to head home.A misunderstanding of the total time allocation for the return journey affected the calculation for each subsequent time segment.""Because Raymond is 6 years older than Samantha, and Samantha is currently 31 years old, Raymond should be 37 years old (31+6), not 25 years old .The model misunderstood the age difference.""The step incorrectly set up the equation.If the running speed is 300/x miles per hour, then the walking speed is one-quarter of the running speed, so the total distance traveled in six hours would be (300/x)<em>2 for running plus (300/x)</em>(1/4)*4 for walking, and their sum should be 18 miles."</p>
<p>This is the partial annotation manual we provide to the annotators.In the full annotation manual, we have an extra section including several examples detailing frequent asked questions, as well as cases for desired and undesired annotation results.</p>
<p>uuid: f25b796f-bc99-49c7-a6cd-ac5bb98412ce question: There are 6 girls the park.If there are twice the number of boys in the park, how many kids are in the park? ground truth solution: There are 6 girls x 2 boys/girl = 6*2=12, 12 boys in the park.In total there are 6 girls + 12 boys = 6+12=18,18 kids in the park #### 18", ground truth answer: 18 model output steps:</p>
<p>Step 1: There are twice the number of boys in the park, so there are 2 boys/girl * 6 girls = 2*6=12, 12 boys in the park.</p>
<p>Step 2: In total there are 12 boys + 6 girls = 12+6=18,18 kids in the park.</p>
<p>Step ----------------------------------uuid: d4ec7076-6dcb-48e3-ba5b-b29d2adc483f question: If Chester eats 3 eggs a day for 30 days and then increases it to 5 eggs a day for 30 days, how many dozens of eggs will Chester need for 60 days?ground truth solution: He starts off eating 3 eggs a day for 30 days for a total of 3<em>30=90 eggs.Then he increases it to 5 eggs a day for 30 days for a total of 5</em>30 = 150 eggs.All total he will eat 90+150 = 240 eggs.There are 12 eggs in 1 dozen and he will 240 eggs which is 240/12 = 20 dozen eggs.#### 20", ground truth answer: 20 model output steps:</p>
<p>Step 1: months eating 3 eggs=2.</p>
<p>Step 2: increased eggs=5</p>
<p>Step 3: days eating 5 eggs=30</p>
<p>Step 4: months eating 5 eggs=6</p>
<p>Step 5: total days=60</p>
<p>Step 6: total months=6+2  -----------------------------------uuid: 18eb1c66-ede5-4504-8831-a3154eae6938 question: Tom gets X car washes a month.If each car wash costs 15 how much does he pay in a year?If we know the answer to the above question is 720, what is the value of unknown variable 'X' ?ground truth solution: Since there are 12 months a year and the total amount Tom paid for is known to be 720, then the monthly pay is 720/12=60.Given that each car wash costs 15 dollars, we know that the car Tom washes a month equals to 60/15=4.#### 4", ground truth answer: 4 model output steps:</p>
<p>Step 1: We know that Tom gets X car washes a month.</p>
<p>Step 2: Each car wash costs 15.</p>
<p>Step 3: To find out how much Tom pays in a year, we need to multiply the monthly cost by the number of months in a year.</p>
<p>Step 4: So, the total cost for a year is 15*X * 12 = 180X.</p>
<p>Step 5: We are given that the total cost for a year is 720, so we can write: 180X = 720.</p>
<p>Step 6: Dividing both sides of the equation by 180, we get: X = 4.</p>
<p>Step 7: The value of X is 4. 3 Case Studies of Response from Evaluated Models question: two sports coaches shopping together.The baseball coach bought x new baseballs for $3 each.The basketball coach bought 8 new basketballs for $14 each.How much more did the basketball coach spend than the baseball coach?If we know the answer of the above question is 85, what is the value of unknown variable x? model output steps:</p>
<p>Step 1: The baseball coach bought x new baseballs for $3 each, so the total cost for the baseballs is 3x.</p>
<p>Step 2: The basketball coach bought 8 new basketballs for $14 each, so the total cost for the basketballs is 8 * $14 = $112.</p>
<p>Step 3: To find out how much more the basketball coach spent than the baseball coach, we need to subtract the cost of the baseballs from the cost of the basketballs: $112 -3x.</p>
<p>Step 4: We are given that the difference in cost is $85, so we can write: $112 -3x = $85.</p>
<p>Step 5: Simplifying the left side, we get: -$85 = 3x.</p>
<p>Step 6: Subtracting $85 from we get: $27 = 3x.Error Analysis by Claude-3-Sonnet: The student incorrectly adds the no-show fraction (1/4) to the gift bags needed instead of subtracting it from the invited guests before multiplying by the required gift bags per guest (0.75).This error likely stems from a misunderstanding of how to account for the no-show guests in the calculation.</p>
<p>Above are the response from the evaluated models.A number of errors can be observed in their responses.For the first response we sampled, GPT-4-Turbo fail to realize that the two equations it is referring to are actually equivalent, thus demonstrating the typical 'reversal curse'.For the second example, GPT-4-Turbo fail to capture the obvious calculation error despite the simplicity of the problem.For the last response, Claude-3-Sonnet hallucinate on the error reason and provide a correction with yet another unit error, demonstrating a lack of ontological understanding on the objects involved in the calculation.</p>
<p>Original 65%</p>
<p>POT 12%</p>
<p>Backward 23%</p>
<p>Problem Types Solutions Llama2-70B Correctly Identified Original POT Backward</p>
<p>Figure 1 :
1
Figure 1: Model performances on GSM8K measured in accuracy versus performances on MR-GSM8K measured in MR-Score.Note that some models score near zero in our benchmark, highlighting the limitations of current training and evaluation paradigms.</p>
<p>the true positive rate (TPR) and true negative rate (TNR) in the solution correctness task, we empirically observed that most models exhibited an increase in TPR while concurrently showing a reduction in TNR under the three-shot setting.This trend can likely be attributed to the composition of our few-shot examples, which included two correct solutions and one incorrect solution, suggesting that the distribution of correctness within these examples may influence model predictions.To verify our conjecture, we conducted an ablation study on the number of correct solutions and the result shows strong correlation with our proposal (see Figure-6in Appendix for more details).This susceptibility of language models to the distribution of few-shot examples highlights a fundamental flaw in the current reasoning paradigms: rather than making scoring decisions based purely on reasoning, the models appear to be swayed by the few-shot examples.This influence might also account for the generally worse performance observed in the few-shot experiments compared to the zero-shot settings, where such biases are absent.</p>
<p>Figure 3 :
3
Figure 3: This figure aims to illustrate the fundamental limitations of the current training and evaluation paradigms for LLMs.The left side of the figure demonstrates that utilizing only correct solutions limits coverage of the solution space despite various data augmentation techniques.The right side depicts potential solutions that might contain incorrect steps or reach the final computation result through an incorrect reasoning path.Models trained exclusively on correct reasoning paths often fail to assess the validity or discern nuanced differences of alternative reasoning paths for the same problem.This highlights a critical gap in the training paradigm, where models exhibit basic imitation skills but lack a deep understanding of the underlying logical rules, leading to a superficial grasp of reasoning processes.</p>
<p>Table 3 :
3
This confusion matrix represents the accuracy of GPT4-Turbo-1106 in assessing 1668 incorrect solutions that were correctly identified with the right error step.The task for GPT4-Turbo-1106 was to evaluate the correctness of the error reason provided by the evaluated model, in comparison with the actual ground truth labelled by expert.'Pos' and 'Neg' represent the ground truth correctness of the provided explanation, while 'Pred-Pos' and 'Pred-Neg' indicate GPT4's prediction about the correctness.Prompt for Zero-Shot ScoringAct as a grade school math teacher and score the following problem solution.Question: {data['question']} Student Solution: {sol_steps} Your task involves three parts: 1. <strong>Step-by-step Evaluation:</strong> Go through the student solution carefully and identify key errors and potential misunderstandings that led to the incorrect solution.2. <strong>Final Judgement:</strong> Provide an overall judgement on the correctness of the student's solution.3. <strong>First Error Step:</strong> If the solution is incorrect, generate the step number where the first error occurs, otherwise generate N/A here 4. <strong>Error Analysis:</strong> If the solution is incorrect, analyse the cause and reasons for the first error step, otherwise generate N/A here Here's the format I want: Step-by-step Evaluation: [Provide a step by step examination of the student solution and identify key errors and misunderstandings here.]Final Judgement: [Insert only <strong>correct</strong> or <strong>wrong</strong> here] First Error Step: [Insert either N/A or the step number where the first error occurs] Error Analysis: [Insert either N/A or the analysis of error in the first error among solution steps] Please follow this format without any additional introductory or concluding statements.</p>
<p>Figure 4 :
4
Figure 4: This is the zero shot prompt we used to evaluate all the models</p>
<p>Figure 6 :
6
Figure6: This is how the true positive and true negative rates change with the number of correct solutions in the 3-shot demonstrations.Note for the smaller models like Phi-3 and Llama3-8B, their susceptibility trend is opposite with that of Llama3-70B.</p>
<p>Understand the Question and Solution: Ensure you comprehend the question and the correct solution.Fabricate Error Step and Reason: Randomly select a step and come up with an error that is both realistic and commonly observed among students such as calculation mistake, conceptual misunderstanding, or incorrect formula application etc. Develop the Incorrect Solution: Base on the error step and error reason, rewrite the solution, ensuring they are internally consistent but incorrect due to the initial error.<strong>Do not mention anything about incorrectness or deliberate error in the error step in your solution</strong>.Independent Analysis: Provide a step-by-step analysis of the solution, including the error, as if encountering the problem for the first time.Do not reference the fact that the error was intentionally created.Summarize the Error: Based on your analysis, summarize the error reason for the first incorrect step.Format: Error Step: [Identify a step number where the first error in the correct solution should occur.Any step from either the beginning, middle or the end will work.]Error Reason: [Describe the nature of the error] Incorrect Solution: [Present the solution, starting correctly but deviating post error step.DO NOT MENTION ANYTHING ABOUT THE INCORRECTNESS.]Independent Step-by-Step Analysis: [Analyse the solution independently, as if unaware of the intentional error] Independent Error Reason: [Conclude with a summary of the perceived error reason]Please adhere strictly to this format, avoiding any additional introductory or concluding remarks.</p>
<p>Figure 7 :
7
Figure 7: This is the prompt we used for GPT4 to create in-domain training data.</p>
<p>3: #### 18 model output answer correctness: correct model output solution correctness: correct model output solution first error step: N/A model output solution first error reason: N/A question type: original</p>
<p>Step 7 :
7
total eggs months=months eating 3 eggs<em>30+months eating 5 eggs</em>days eating 5 eggs+total months Step 8: total dozen eggs=total eggs months/12 Step 9: print(total dozen eggs) model output answer correctness: wrong model output solution correctness: wrong model output solution first error step: 1 model output solution first error reason: The solution assumes the months eating three eggs are 2 which is unsupported since there is only 30 days for eating three eggs per day.question type: Program of Thought</p>
<p>Step 8: 4 Step 9 :
49
The answer is: 4 model output answer correctness: correct model output solution correctness: correct model output solution first error step: N/A model output solution first error reason: N/A question type: Reversed Above are the annotated samples extracted from the MR-GSM8K dataset, with one question from each type.The same problems can be found in the MR-GSM8K.jsonfile in our open sourced repository.</p>
<p>Step 7 :
7
Dividing both sides by 3, we get: $9 = x.Final Judgement by GPT-4-Turbo: wrong First Error Step by GPT-4-Turbo: step 4 Error Analysis by GPT-4-Turbo: The student incorrectly simplified the equation in Step 4. The correct equation should be 3x = $112 -$85, not $112 -3x = $85.This error led to the incorrect calculation in Step 5.----------------------------------question: Trinity sells magazines at 11/8 of the price she bought the magazines.If she bought the magazines at $72, what is her profit?model output steps: Step 1: If Trinity bought the magazines at $72, then she sells them at 11/8 * $72 = $90 Step 2: Her profit is the selling price minus the buying price, so her profit is $90 -$72 = $18 Step 3: #### 18 Final Judgement by GPT-4-Turbo: Correct First Error Step by GPT-4-Turbo: N/A Error Analysis by GPT-4-Turbo: N/A ----------------------------------question: Christina is planning a birthday party and needs .75gift bags per invited guest, because 1/4 of attendees don't show up.She invited 16 friends.Gift bags are $2 each.How much will she spend?model output steps: Step 1: no show = 1/4 Step 2: invited guests = 16 Step 3: gift bags needed = invited guests * 0.75 Step 4: total gift bags = no show + gift bags needed Step 5: total cost = total gift bags * 2 Step 6: print(total cost) Final Judgement by Claude-3-Sonnet: Wrong First Error Step by Claude-3-Sonnet: Step 4</p>
<p>Figure 8 :
8
Figure 8: Problem types of incorrect solutions for which the llama2-70B-MR model has successfully identified both the error step and error reason.Note that the training set only included solutions from original problems.</p>
<p>•</p>
<p>An electronics seller bought 5 phones for $700 each and gives the seller $X in dollar bills.How much will the seller give back in change?If we know the answer to the above question is 500,</p>
<p>what is the value of unknown variable 'X'?
InputQuestion### Response:### Response:### Response:• "Step 1: total_amount_paid = 5 * 700",• "Step 1: The seller bought 5 phones for• "Step 1: The seller bought 5 phones for• "Step 2: total_amount_given = 4000",5<em>700 = $&lt;&lt;5</em>700=3500&gt;&gt;3500.",$700 each, so…….",• "Step 3: change = total_amount_given -• "Step 2: …….",• "Step 2: The seller gives the seller $X intotal_amount_paid",• "Step 3:……",dollar bills, so ……",• "Step 4: print(change)"• "Step 4: The seller will give back $3500-• "Step 3: ……",Output$4000 = $&lt;&lt;3500-4000=500&gt;&gt;500 in change."• "Step 4: Solving for X, we get: X = 4000.", • "Step 5: The value of X is 4000.",Solution•GPT4-GradingOutput</p>
<p>Final Judgement: Wrong • First Error Step: Step 3 • Error Analysis: The student
POT GSM8KOriginal GSM8KREVERSED GSM8K• Final Judgement: Correct• Final Judgement: Correct• First Error Step: N/A• First Error Step: N/Ahas• Error Analysis: N/A• Error Analysis: N/Amisunderstood the concept ofcalculating change. It is not possible toget a negative change. Instead, thestudent should have checked if thetotal amount given was less than thetotal amount required and thencalculated the additional amountrequired.</p>
<p>Table 1 :
1
MR-GSM8K statistics: The first two columns are the numbers of correct and incorrect solutions.The following two columns hold the lengths of questions and solutions, which are measured in number of words.The last two column shows the averaged solution steps and first error steps.
Question Types Correct Incorrect Q Length S Length S Steps First Error StepsOriginal69372546.9100.57.3.4POT11310945.134.96.53.9Reverse62273762.4157.011.34.5Total1428157151.597.58.33.9hold-out dataset. Recruitment decisions are based on performance during this trial. Additionally, fourquality control supervisors, including the first author, are designated to address queries and monitorthe labeling quality throughout the project.</p>
<p>Table 2 :
2
Evaluation results on MR-GSM8K
ModelTask1-TPR k=0 k=3Task1-TNR k=0 k=3Task1-MCC k=0 k=3Task2-Accy k=0 k=3Task3-Accy k=0 k=3MR-Score k=0 k=3Open-Source SmallQwen-1.8B21.8 33.30.13.90.0.0.0.40.0.0. 0.1Phi3-3.8B11.3 62.698.5 72.620.435.432.9 26.318.0 13.922.9 21.9Open-Source MediumDeepseek-Math-7B-RL 77.32.452.30.430.40.9.80.15.10.111.6 0.1WizardMath-v1.1-7B 99.36.70.50.60.00.00.30.20.30.10.2 0.1Llama3-8B3.2 40.998.3 80.35.123.129.1 23.315.0 11.617.2 17.4Open-Source LargeMAmmoTH-70B88.0 89.823.12.814.60.03.90.31.80.35.0 0.2MetaMath-70B7.80.00.30.00.00.00.10.00.00.00.0 0.0Llama3-70B67.6 89.383.0 66.051.356.438.9 33.532.7 25.738.3 34.2Qwen1.5-72B83.7 87.757.1 52.442.042.519.1 23.113.5 15.820.9 23.3Deepseek-v2-236B60.1 88.287.2 61.549.451.226.8 32.423.8 28.329.8 34.1Closed-Source LLMsClaude3-Haiku70.4 99.051.78.122.516.717.22.311.31.815.3 4.9GPT-3.5-Turbo16.3 59.793.8 65.716.225.530.6 21.020.3 13.022.6 17.9Claude3-Sonnet35.1 88.489.8 44.830.036.525.2 18.819.9 15.623.5 20.8GPT-4-Turbo69.5 83.091.8 84.263.367.248.8 51.746.3 48.150.5 53.0</p>
<p>Table 4 :
4
Comparison of the manual labelling results and GPT4-Turbo-1106 labelling results.Step column shows the number that each evaluated models successfully located the first error steps among incorrect solutions.Step+Reason/M stands for the manual labelling results of the error reasons where its first error step is correct.Step+Reason/A corresponds to the labelling results of GPT4-Turbo-1106.llama2-70B-MR are llama2-70B model finetuned on the GSM8k training set and its meta-reasoning augmentation by GPT4.
Step+Reason/M Step+Reason/A MR-Score/M MR-Score/AGPT4823/1573677/1573732/15730.4950.512Claude2331/1573185/1573224/15730.1910.203llama2-70B-MR327/157399/1573139/15730.1050.118GPT3.5179/157373/157373/15730.0970.097MetaMath-70B22/15736/15737/15730.0130.013Mammoth-70B4/15731/15732/15730.0120.012WizardMath-70B6/15731/15731/15730.0010.001
Preprint. Under review.</p>
<p>Introducing claude. Anthropic, 2023</p>
<p>GPT-4 can't reason. Konstantine Arkoudas, 10.48550/arXiv.2308.037622023</p>
<p>. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, Tianhang Zhu, 10.48550/arXiv.2309.16609Qwen technical report. 2023</p>
<p>Leak, cheat, repeat: Data contamination and evaluation malpractices in closed-source llms. Simone Balloccu, ' Patr, Mateusz Schmidtov'a, Ondvrej Lango, Duvsek, Annual Meeting of the Association for Computational Linguistics. 2024</p>
<p>The reversal curse: Llms trained on "a is b" fail to learn "b is a. Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, Owain Evans, ArXiv, abs/2309.122882023262083829</p>
<p>The reversal curse: Llms trained on "a is b" fail to learn "b is a. Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, Owain Evans, 10.48550/arXiv.2309.122882023</p>
<p>Deepseek LLM: scaling open-source language models with longtermism. Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, Huazuo Gao, Kaige Gao, Wenjun Gao, Ruiqi Ge, Kang Guan, Daya Guo, Jianzhong Guo, Guangbo Hao, Zhewen Hao, Ying He, Wenjie Hu, Panpan Huang, Erhang Li, Guowei Li, Jiashi Li, Yao Li, Y K Li, Wenfeng Liang, Fangyun Lin, Alex X Liu, Bo Liu, Wen Liu, Xiaodong Liu, Xin Liu, Yiyuan Liu, Haoyu Lu, Shanghao Lu, Fuli Luo, Shirong Ma, Xiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu, Hui Qu, Tongzheng Ren, Zehui Ren, Chong Ruan, Zhangli Sha, Zhihong Shao, Junxiao Song, Xuecheng Su, Jingxiang Sun, Yaofeng Sun, Minghui Tang, Bingxuan Wang, Peiyi Wang, Shiyu Wang, Yaohui Wang, Yongji Wang, Tong Wu, Y Wu, Xin Xie, Zhenda Xie, Ziwei Xie, Yiliang Xiong, Hanwei Xu, R X Xu, Yanhong Xu, Dejian Yang, Yuxiang You, Shuiping Yu, Xingkai Yu, B Zhang, Haowei Zhang, Lecong Zhang, Liyue Zhang, Mingchuan Zhang, Minghua Zhang, Wentao Zhang, Yichao Zhang, Chenggang Zhao, Yao Zhao, Shangyan Zhou, Shunfeng Zhou, Qihao Zhu, Yuheng Zou, 10.48550/arXiv.2401.029542024</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, John A Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuan-Fang Li, Scott M Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang, ArXiv, abs/2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023257663729</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Wenhu Chen, Xueguang Ma, Xinyi Wang, William W Cohen, ArXiv, abs/2211.125882022</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, 2021</p>
<p>Qlora: Efficient finetuning of quantized llms. Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer, ArXiv, abs/2305.143142023</p>
<p>Faith and fate: Limits of transformers on compositionality. Nouha Dziri, Ximing Lu, Melanie Sclar, Lorraine Xiang, Liwei Li, Bill Yuchen Jian, Peter Lin, Chandra West, Bhagavatula, Le Ronan, Jena D Bras, Soumya Hwang, Sean Sanyal, Xiang Welleck, Allyson Ren, Zaïd Ettinger, Yejin Harchaoui, Choi, ArXiv, abs/2305.186542023258967391</p>
<p>Tora: A tool-integrated reasoning agent for mathematical problem solving. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan, Weizhu Chen, ArXiv, abs/2309.174522023</p>
<p>Textbooks are all you need. Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César, Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo De Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, Sébastien Bubeck, 10.48550/arXiv.2306.11644Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li2023</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, 2021</p>
<p>Large language models cannot self-correct reasoning yet. Jie Huang, Xinyun Chen, Swaroop Mishra, Steven Huaixiu, Adams Wei Zheng, Xinying Yu, Denny Song, Zhou, ArXiv, abs/2310.017982023</p>
<p>Mawps: A math word problem repository. Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, Hannaneh Hajishirzi, North American Chapter. the Association for Computational Linguistics20162228719</p>
<p>Let's verify step by step. Vineet Hunter Lightman, Yura Kosaraju, Harrison Burda, John Edwards ; Leike, Ilya Schulman, Karl Sutskever, Cobbe, ArXiv, abs/2305.20050Jan. 2023258987659Bowen Baker, Teddy Lee</p>
<p>Alignbench: Benchmarking chinese alignment of large language models. Xiao Liu, Xuanyu Lei, Sheng-Ping Wang, Yue Huang, Zhuoer Feng, Bosi Wen, Jiale Cheng, Pei Ke, Yifan Xu, Weng Tam, Xiaohan Zhang, Lichao Sun, Hongning Wang, Jing Zhang, Minlie Huang, Yuxiao Dong, Jie Tang, ArXiv, abs/2311.187432023</p>
<p>Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, Dongmei Zhang, arXiv:2308.09583Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. 2023arXiv preprint</p>
<p>Language models of code are few-shot commonsense learners. Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, Graham Neubig, ArXiv, abs/2210.071282022</p>
<p>Comparison of the predicted and observed secondary structure of t4 phage lysozyme. Brian W Matthews, Biochimica et biophysica acta. 2445966731975</p>
<p>A diverse corpus for evaluating and developing english math word problem solvers. Shen-Yun, Chao-Chun Miao, Keh-Yih Liang, Su, Annual Meeting of the Association for Computational Linguistics. 2020220047831</p>
<p>Optimizing language models for dialogue. Openai, Chatgpt, 2022</p>
<p>ArXiv, abs/2303.08774Gpt-4 technical report. 2023257532815OpenAI</p>
<p>Testing language models on a held-out high school national finals exam. Keiran Paster, 2023</p>
<p>Generative language modeling for automated theorem proving. Stanislas Polu, Ilya Sutskever, ArXiv, abs/2009.033932020221535103</p>
<p>Deepseekmath: Pushing the limits of mathematical reasoning in open language models. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y K Li, Y Wu, Daya Guo, 10.48550/arXiv.2402.033002024</p>
<p>Computers and thought: A practical introduction to artificial intelligence. Mike Sharples, David C Hogg, Chris Hutchinson, Steve Torrance, David J Young, Proceedings of the Conference on Innovative Applications of Artificial Intelligence. the Conference on Innovative Applications of Artificial Intelligence198961039423</p>
<p>A promising path towards autoformalization and general artificial intelligence. Christian Szegedy, International Conference on Intelligent Computer Mathematics. 2020</p>
<p>Stanford alpaca: An instruction-following llama model. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, 2023</p>
<p>Veritymath: Advancing mathematical reasoning by self-verification through unit consistency. Vernon Toh, Ratish Puduppully, Nancy F Chen, ArXiv, abs/2311.071722023</p>
<p>Llama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.092882023arXiv preprint</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Huai Hsin Chi, F Xia, Quoc Le, Denny Zhou, ArXiv, abs/2201.119032022246411621</p>
<p>Rethinking benchmark and contamination for language models with rephrased samples. Shuo Yang, Wei-Lin Chiang, Lianmin Zheng, Joseph E Gonzalez, Ion Stoica, ArXiv, abs/2311.048502023</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, ArXiv, abs/2305.106012023258762525</p>
<p>Three questions concerning the use of large language models to facilitate mathematics learning. An-Zi Yen, Wei-Ling Hsu, ArXiv, abs/2310.136152023</p>
<p>Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, Weiyang Liu, Metamath, arXiv:2309.12284Bootstrap your own mathematical questions for large language models. 2023arXiv preprint</p>
<p>Mammoth: Building math generalist models through hybrid instruction tuning. Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen, ArXiv, abs/2309.056532023</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P Xing, Haotong Zhang, Joseph Gonzalez, Ion Stoica, ArXiv, abs/2306.056852023</p>            </div>
        </div>

    </div>
</body>
</html>
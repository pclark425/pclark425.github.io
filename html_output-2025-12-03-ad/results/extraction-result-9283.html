<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9283 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9283</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9283</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-163.html">extraction-schema-163</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <p><strong>Paper ID:</strong> paper-278293860</p>
                <p><strong>Paper Title:</strong> Cost, Complexity, and Efficacy of Prompt Engineering Techniques for Large Language Models</p>
                <p><strong>Paper Abstract:</strong> This research investigates the impact of various prompt engineering techniques on the length, cost, complexity, and accuracy of responses from large language models (LLMs). By comparing direct prompting with zero-shot, few-shot, and chain-of-thought (CoT) meth-ods on tasks like GSM8K and creative writing, I analyze the trade-offs between token usage and response quality. Results show that while zero-shot CoT prompting is highly effective and cost-efficient, other meth-ods like Least-to-Most and Tree-of-Thought add significant length and complexity without proportional accuracy gains. Additionally, I discuss the financial implications, finding that GPT-4’s unique pricing structure narrows the cost difference between manual/few-shot and zero-shot methods. Complexity analysis reveals that more intricate prompts often lead to convoluted outputs, challenging human review and implementation. Our findings guide the selection of prompt engineering strategies to optimize both performance and resource utilization.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9283.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9283.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-Shot CoT (original)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-Shot Chain-of-Thought Prompting (original: "Let's think step by step")</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting format that elicits intermediate reasoning by appending a short instruction (e.g. "Let's think step by step") to the task, causing the model to produce stepwise reasoning without providing example chains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large Language Models are Zero-Shot Reasoners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4; text-DaVinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K (math word problems); Creative writing (2-paragraph passage with specified ending sentences)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>GSM8K: elementary-level math word problems; Creative writing: generate a coherent two-paragraph passage that ends with given sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot: append a short instruction to elicit intermediate reasoning. Example wording used in experiments: "Let's think step by step." For creative writing adapted to: "Plan step-by-step before writing the passage."</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to direct prompting (no special instruction), few-shot prompting, manual chain-of-thought, APE-improved zero-shot CoT, Self-Refine, Least-to-Most, Tree-of-Thought.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>GSM8K: substantial and statistically significant accuracy gains relative to direct prompting (improvement described as 'substantial gains' and significant at 95% level); Creative writing: improved inter-paragraph cosine similarity and modest improvements in inter-sentence similarity and human-preferred coherence (statistically significant for some comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Outperforms direct prompting and other more complex methods per token-cost (zero-shot CoT is described as 'cheap and extremely effective'); often comparable or superior to manual/few-shot CoT; APE-improved zero-shot CoT was not more effective than the simple wording in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Described qualitatively as 'substantial gains' on GSM8K and the most effective technique per-token and per-cent spent; effect sizes decreased as base model improved (gains smaller on newer models). No single uniform numeric effect size reported in main text for all comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Hypothesized that LLMs have many examples of step-by-step worked solutions in their training data, so simple stepwise instructions map to familiar patterns; the brevity and familiarity of the phrasing may be well-matched to the models' pretraining distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Prompt phrasing: "Let's think step by step." For creative writing adapted to planning wording. Evaluated on GPT-4 (GPT-4-0613) and text-DaVinci-003 via OpenAI API. Statistical tests: McNemar's tests for GSM8K, paired t-tests for creative-writing metrics; costs computed using Nov 11, 2023 pricing (text-davinci-003: $0.02/1k tokens; GPT-4 input $0.03/1k, output $0.06/1k). Zero-shot methods increased token usage moderately (1–2× of direct prompting) and thus kept costs low while improving accuracy/coherence.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cost, Complexity, and Efficacy of Prompt Engineering Techniques for Large Language Models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9283.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9283.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>APE Zero-Shot CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automatic Prompt Engineer (APE) - Improved Zero-Shot Chain-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A zero-shot Chain-of-Thought variant discovered via automated prompt-search/engineering that uses an alternative wording intended to maximize stepwise reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4; text-DaVinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K; Creative writing (2-paragraph)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>GSM8K: elementary math word problems withheld from model training; Creative writing: produce two-paragraph passages that end in specified sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot with APE wording. The optimal APE prompt found in prior automated testing used wording: "Let's work this out in a step-by-step way to be sure I have the right answer." For creative writing adapted to: "Plan step-by-step before writing the passage to be sure I have a correct and coherent answer."</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to original Zero-Shot CoT wording ("Let's think step by step"), direct prompting, few-shot, manual CoT, Self-Refine, Least-to-Most, Tree-of-Thought.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Generally effective; improves coherence metrics on creative writing and accuracy on GSM8K, but in this paper APE-improved zero-shot CoT was not more effective than the simple "Let's think step by step" phrasing.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>No consistent advantage over the simpler zero-shot CoT phrasing in this study; both zero-shot variants outperform direct prompting and other complex methods on average.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>No clear numeric superiority vs original zero-shot CoT reported; described as not more effective in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Additional wording aiming to ensure correctness/coherence did not yield measurable improvement beyond the simple, familiar phrasing—supporting the idea that concise, familiar stepwise prompts align well with LLM pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>APE wording tested exactly as above for both tasks; same models and testing procedure as other methods. Statistical significance assessed at 95% level; APE variant didn't outperform original zero-shot CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cost, Complexity, and Efficacy of Prompt Engineering Techniques for Large Language Models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9283.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9283.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Few-Shot (examples without reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Few-Shot Prompting (example QA pairs provided, no reasoning chains)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompt format that provides a small number of question–answer examples before the target question, but does not include worked-out reasoning steps.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language Models are Few-Shot Learners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4; text-DaVinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K; Creative writing (2-paragraph)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>GSM8K: math word problems; Creative writing: produce passages that satisfy ending constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Manual few-shot: a few examples of Q&A (answers without intermediate reasoning) are included in the prompt prior to the test question. For creative writing, few-shot examples included exemplar passage(s).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to zero-shot CoT (original and APE), manual few-shot CoT (worked examples), direct prompting, iterative methods (Self-Refine, Tree-of-Thought), Least-to-Most.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>GSM8K: ineffective and sometimes harmful (reduced accuracy) compared to direct prompting and especially compared to chain-of-thought prompting; Creative writing: provided some improvements in coherence for older model (text-davinci-003) and helped formatting/readability, but gains are small and have decreased for newer models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>On GSM8K, worse than zero-shot CoT and sometimes worse than direct prompting. On creative writing, slightly better than direct prompting for older model, but smaller or no gains on GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Described qualitatively as harmful on GSM8K and modestly beneficial for creative writing on older models; numeric effect sizes not consistently reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Few-shot examples can supply formatting/readability priors and subjective targets (e.g., passage style), which helps for generative tasks and older models, but providing only final answers without reasoning may mislead models on reasoning tasks (GSM8K) where explicit chains are important.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Few-shot examples were included in the prompt; conversation/input lengths are substantially larger due to the examples (input length often many times the length of the question/answer). Cost increases accordingly. The creative-writing few-shot used exemplar passages; GSM8K few-shot used example Q/A pairs without reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cost, Complexity, and Efficacy of Prompt Engineering Techniques for Large Language Models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9283.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9283.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Manual CoT (few-shot CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Few-Shot (Manual) Chain-of-Thought Prompting (worked-out example reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Provision of a few examples where each example includes the question, a worked-out chain-of-thought (reasoning steps), and the final answer, to demonstrate desired multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4; text-DaVinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K; Creative writing</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>GSM8K: elementary math word problems; Creative writing: writing coherent short passages ending with provided sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Few-shot examples that include explicit reasoning steps for each example (worked-out chains) provided in prompt before target question.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared against zero-shot CoT, APE Zero-Shot CoT, few-shot without reasoning, direct prompting, iterative techniques, Least-to-Most.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Provides notable accuracy improvements on GSM8K relative to direct prompting in many contexts, but in this study zero-shot CoT achieved comparable or better accuracy-cost tradeoff; manual CoT adds substantial tokens (long input) and complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Comparable to zero-shot CoT on some tasks; less cost-effective and often no better than simple zero-shot phrasing. For creative writing, manual CoT/few-shot examples can help demonstrate format and coherence but gains have shrunk for newer models.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Gains exist but are often smaller per added token than zero-shot CoT; exact numeric effect sizes not consistently reported across model/task pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Worked examples explicitly model desired multi-step behavior and can teach structure, but many models already have such patterns in pretraining so short zero-shot cues can suffice; manual CoT length/cost is a drawback.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Manual few-shot CoT prompts include several full Q+chain+answer examples. Input-length and token costs are high because of included exemplars—few-shot prompt input lengths are often multiple times the original question length.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cost, Complexity, and Efficacy of Prompt Engineering Techniques for Large Language Models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9283.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9283.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Least-to-Most</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Least-to-Most Prompting (decompose into subproblems then solve sequentially)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A few-shot/chain-of-thought method that demonstrates breaking a problem into simpler subproblems and solving them in sequence, designed to improve multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Least-to-Most Prompting Enables Complex Reasoning in Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4; text-DaVinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K; Creative writing</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>GSM8K: math word problems; Creative writing: generate coherent passages with specified endings.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Few-shot examples plus instructions encouraging decomposition into subproblems and sequential solving; model output may include explicit step numbering and subproblem solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to direct prompting, zero-shot CoT, manual CoT, Self-Refine, Tree-of-Thought, few-shot without reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Adds many reasoning steps and substantially increases conversation length and token cost; accuracy gains on GSM8K are present but not proportional to the added token/complexity cost and often smaller than those from zero-shot CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Often outperformed by zero-shot CoT when considering per-token or per-cost improvements; in creative writing, added complexity did not translate into better human-preferred coherence and sometimes harmed readability/compliance.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Large increases in tokens/steps with only modest accuracy gains; paper reports that Least-to-Most adds the most length among CoT methods but without proportional accuracy gains.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Decomposition increases explicit reasoning steps which can help on complex problems, but the very long structured outputs increase cost and risk of distraction or failure to follow instructions; this makes the method less cost-effective on studied tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Least-to-Most prompts caused dramatic increases in linebreaks/steps and conversation length. Complexity metrics (line breaks, '1.,2.' markers) were notably higher for Least-to-Most. Task compliance suffered for many methods including Least-to-Most, particularly on older models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cost, Complexity, and Efficacy of Prompt Engineering Techniques for Large Language Models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9283.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9283.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Refine</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Refine Prompting (iterative refinement with self-feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative method where the model first produces an answer, then is prompted to critique or provide feedback on its response and refine it, potentially repeating this loop.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-Refine: Iterative Refinement with Self-Feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4; text-DaVinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K; Creative writing</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>GSM8K: math word problems; Creative writing: produce coherent two-paragraph passages with specified endings.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Iterative prompting: initial generation followed by one or more critique/refinement prompts; final score measured after all refinement steps completed.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to direct prompting, zero-shot CoT (original and APE), few-shot (with/without CoT), Least-to-Most, Tree-of-Thought.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Variable. On GSM8K, Self-Refine generally performed well; with GPT-4 it could bring results up to the level of chain-of-thought methods in some cases. However, overall Self-Refine and Tree-of-Thought methods largely do not lead to consistent improvement and are highly variable in length and cost.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Sometimes matches chain-of-thought performance (notably with GPT-4), but overall less reliable and more costly; benefits are task- and model-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Variable; occasionally matches CoT gains on GPT-4 for GSM8K, but no consistent, robust advantage across tasks and models.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Iterative feedback can correct earlier mistakes but increases conversation length and complexity; success depends on the model's ability to self-criticize accurately, which is inconsistent across tasks and model versions.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Self-Refine implemented as iterative steps until refinement completion. Observed high variability in conversation lengths and token costs; task compliance issues detected, particularly for older models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cost, Complexity, and Efficacy of Prompt Engineering Techniques for Large Language Models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9283.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9283.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tree-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree of Thought Prompting (searching/backtracking across multiple generated branches)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative, decision-tree style prompting approach in which the model expands multiple candidate steps/ideas, evaluates branches, and may backtrack to explore alternate solutions before finalizing an answer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tree of Thoughts: Deliberate Problem Solving with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4; text-DaVinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K; Creative writing</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>GSM8K: math word problems; Creative writing: produce coherent two-paragraph passages ending with specified sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Iterative tree traversal where multiple candidate steps/ideas are generated and evaluated; backtracking is allowed, producing multiple rounds of generated material before a final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to direct prompting, zero-shot CoT, APE CoT, few-shot, Least-to-Most, Self-Refine.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>High variability, frequently very long conversations and token costs; overall did not produce proportional accuracy/quality gains for the tested tasks and often added complexity that made human review difficult; rarely the best-performing method.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Often inferior in cost-effectiveness to zero-shot CoT; added complexity and length without commensurate quality/accuracy improvement across tested tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Large increases in tokens and number of steps; effect on accuracy/quality was inconsistent and often not positive when accounting for added cost and complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Tree traversal and backtracking require finding and evaluating multiple candidate branches—such search behavior is expensive in tokens and may not correspond to patterns commonly seen in pretraining data, hence models struggle to exploit it reliably for these tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Tree-of-Thought runs produced highly variable conversation lengths (sometimes massive), large numbers of linebreaks/steps, and higher implementation difficulty; human-ease-of-review ratings were low due to convoluted outputs. Significant non-compliance with prompt instructions noted, especially for older models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cost, Complexity, and Efficacy of Prompt Engineering Techniques for Large Language Models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9283.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9283.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Direct Prompting (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-Shot Direct Prompting (no specialized techniques)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline format consisting of giving the task/question directly to the model without special instructions, example chains, or iterative procedures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4; text-DaVinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K; Creative writing</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>GSM8K: elementary math word problems; Creative writing: two-paragraph passage generation with specified endings.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Directly present the question/task and request a response; no additional reasoning cues, examples, or iterative refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Used as the baseline comparator for all other presentation formats.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Strong baseline performance especially for more modern models (GPT-4). For GSM8K, baseline accuracy has improved over time making gains from prompt engineering smaller; for creative writing baseline coherence is lower than the best chain-of-thought variants but sometimes comparable in human preference.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>All methods compared against direct prompting; Zero-shot CoT often significantly outperforms direct prompting, while few-shot without reasoning can be worse for reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Direct prompting is shortest and cheapest; as base models become stronger, the room for improvement from elaborate prompting shrinks, reducing benefit/cost ratio of advanced methods.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Used as control across experiments. Direct prompting yields the shortest conversations (lowest token cost). The paper reports that prompt engineering typically increases tokens by 1–10× relative to direct prompting (zero-shot 1–2×; few-shot and iterative methods much higher).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cost, Complexity, and Efficacy of Prompt Engineering Techniques for Large Language Models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models <em>(Rating: 2)</em></li>
                <li>Least-to-Most Prompting Enables Complex Reasoning in Large Language Models <em>(Rating: 2)</em></li>
                <li>Large Language Models are Zero-Shot Reasoners <em>(Rating: 2)</em></li>
                <li>Tree of Thoughts: Deliberate Problem Solving with Large Language Models <em>(Rating: 2)</em></li>
                <li>Self-Refine: Iterative Refinement with Self-Feedback <em>(Rating: 2)</em></li>
                <li>Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data <em>(Rating: 2)</em></li>
                <li>Language Models are Few-Shot Learners <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9283",
    "paper_id": "paper-278293860",
    "extraction_schema_id": "extraction-schema-163",
    "extracted_data": [
        {
            "name_short": "Zero-Shot CoT (original)",
            "name_full": "Zero-Shot Chain-of-Thought Prompting (original: \"Let's think step by step\")",
            "brief_description": "A prompting format that elicits intermediate reasoning by appending a short instruction (e.g. \"Let's think step by step\") to the task, causing the model to produce stepwise reasoning without providing example chains.",
            "citation_title": "Large Language Models are Zero-Shot Reasoners",
            "mention_or_use": "use",
            "model_name": "GPT-4; text-DaVinci-003",
            "model_size": null,
            "task_name": "GSM8K (math word problems); Creative writing (2-paragraph passage with specified ending sentences)",
            "task_description": "GSM8K: elementary-level math word problems; Creative writing: generate a coherent two-paragraph passage that ends with given sentences.",
            "presentation_format": "Zero-shot: append a short instruction to elicit intermediate reasoning. Example wording used in experiments: \"Let's think step by step.\" For creative writing adapted to: \"Plan step-by-step before writing the passage.\"",
            "comparison_format": "Compared to direct prompting (no special instruction), few-shot prompting, manual chain-of-thought, APE-improved zero-shot CoT, Self-Refine, Least-to-Most, Tree-of-Thought.",
            "performance": "GSM8K: substantial and statistically significant accuracy gains relative to direct prompting (improvement described as 'substantial gains' and significant at 95% level); Creative writing: improved inter-paragraph cosine similarity and modest improvements in inter-sentence similarity and human-preferred coherence (statistically significant for some comparisons).",
            "performance_comparison": "Outperforms direct prompting and other more complex methods per token-cost (zero-shot CoT is described as 'cheap and extremely effective'); often comparable or superior to manual/few-shot CoT; APE-improved zero-shot CoT was not more effective than the simple wording in this paper.",
            "format_effect_size": "Described qualitatively as 'substantial gains' on GSM8K and the most effective technique per-token and per-cent spent; effect sizes decreased as base model improved (gains smaller on newer models). No single uniform numeric effect size reported in main text for all comparisons.",
            "explanation_or_hypothesis": "Hypothesized that LLMs have many examples of step-by-step worked solutions in their training data, so simple stepwise instructions map to familiar patterns; the brevity and familiarity of the phrasing may be well-matched to the models' pretraining distribution.",
            "null_or_negative_result": false,
            "experimental_details": "Prompt phrasing: \"Let's think step by step.\" For creative writing adapted to planning wording. Evaluated on GPT-4 (GPT-4-0613) and text-DaVinci-003 via OpenAI API. Statistical tests: McNemar's tests for GSM8K, paired t-tests for creative-writing metrics; costs computed using Nov 11, 2023 pricing (text-davinci-003: $0.02/1k tokens; GPT-4 input $0.03/1k, output $0.06/1k). Zero-shot methods increased token usage moderately (1–2× of direct prompting) and thus kept costs low while improving accuracy/coherence.",
            "uuid": "e9283.0",
            "source_info": {
                "paper_title": "Cost, Complexity, and Efficacy of Prompt Engineering Techniques for Large Language Models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "APE Zero-Shot CoT",
            "name_full": "Automatic Prompt Engineer (APE) - Improved Zero-Shot Chain-of-Thought",
            "brief_description": "A zero-shot Chain-of-Thought variant discovered via automated prompt-search/engineering that uses an alternative wording intended to maximize stepwise reasoning.",
            "citation_title": "Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data",
            "mention_or_use": "use",
            "model_name": "GPT-4; text-DaVinci-003",
            "model_size": null,
            "task_name": "GSM8K; Creative writing (2-paragraph)",
            "task_description": "GSM8K: elementary math word problems withheld from model training; Creative writing: produce two-paragraph passages that end in specified sentences.",
            "presentation_format": "Zero-shot with APE wording. The optimal APE prompt found in prior automated testing used wording: \"Let's work this out in a step-by-step way to be sure I have the right answer.\" For creative writing adapted to: \"Plan step-by-step before writing the passage to be sure I have a correct and coherent answer.\"",
            "comparison_format": "Compared to original Zero-Shot CoT wording (\"Let's think step by step\"), direct prompting, few-shot, manual CoT, Self-Refine, Least-to-Most, Tree-of-Thought.",
            "performance": "Generally effective; improves coherence metrics on creative writing and accuracy on GSM8K, but in this paper APE-improved zero-shot CoT was not more effective than the simple \"Let's think step by step\" phrasing.",
            "performance_comparison": "No consistent advantage over the simpler zero-shot CoT phrasing in this study; both zero-shot variants outperform direct prompting and other complex methods on average.",
            "format_effect_size": "No clear numeric superiority vs original zero-shot CoT reported; described as not more effective in the experiments.",
            "explanation_or_hypothesis": "Additional wording aiming to ensure correctness/coherence did not yield measurable improvement beyond the simple, familiar phrasing—supporting the idea that concise, familiar stepwise prompts align well with LLM pretraining.",
            "null_or_negative_result": false,
            "experimental_details": "APE wording tested exactly as above for both tasks; same models and testing procedure as other methods. Statistical significance assessed at 95% level; APE variant didn't outperform original zero-shot CoT.",
            "uuid": "e9283.1",
            "source_info": {
                "paper_title": "Cost, Complexity, and Efficacy of Prompt Engineering Techniques for Large Language Models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Few-Shot (examples without reasoning)",
            "name_full": "Few-Shot Prompting (example QA pairs provided, no reasoning chains)",
            "brief_description": "Prompt format that provides a small number of question–answer examples before the target question, but does not include worked-out reasoning steps.",
            "citation_title": "Language Models are Few-Shot Learners",
            "mention_or_use": "use",
            "model_name": "GPT-4; text-DaVinci-003",
            "model_size": null,
            "task_name": "GSM8K; Creative writing (2-paragraph)",
            "task_description": "GSM8K: math word problems; Creative writing: produce passages that satisfy ending constraints.",
            "presentation_format": "Manual few-shot: a few examples of Q&A (answers without intermediate reasoning) are included in the prompt prior to the test question. For creative writing, few-shot examples included exemplar passage(s).",
            "comparison_format": "Compared to zero-shot CoT (original and APE), manual few-shot CoT (worked examples), direct prompting, iterative methods (Self-Refine, Tree-of-Thought), Least-to-Most.",
            "performance": "GSM8K: ineffective and sometimes harmful (reduced accuracy) compared to direct prompting and especially compared to chain-of-thought prompting; Creative writing: provided some improvements in coherence for older model (text-davinci-003) and helped formatting/readability, but gains are small and have decreased for newer models.",
            "performance_comparison": "On GSM8K, worse than zero-shot CoT and sometimes worse than direct prompting. On creative writing, slightly better than direct prompting for older model, but smaller or no gains on GPT-4.",
            "format_effect_size": "Described qualitatively as harmful on GSM8K and modestly beneficial for creative writing on older models; numeric effect sizes not consistently reported in the paper.",
            "explanation_or_hypothesis": "Few-shot examples can supply formatting/readability priors and subjective targets (e.g., passage style), which helps for generative tasks and older models, but providing only final answers without reasoning may mislead models on reasoning tasks (GSM8K) where explicit chains are important.",
            "null_or_negative_result": true,
            "experimental_details": "Few-shot examples were included in the prompt; conversation/input lengths are substantially larger due to the examples (input length often many times the length of the question/answer). Cost increases accordingly. The creative-writing few-shot used exemplar passages; GSM8K few-shot used example Q/A pairs without reasoning.",
            "uuid": "e9283.2",
            "source_info": {
                "paper_title": "Cost, Complexity, and Efficacy of Prompt Engineering Techniques for Large Language Models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Manual CoT (few-shot CoT)",
            "name_full": "Few-Shot (Manual) Chain-of-Thought Prompting (worked-out example reasoning)",
            "brief_description": "Provision of a few examples where each example includes the question, a worked-out chain-of-thought (reasoning steps), and the final answer, to demonstrate desired multi-step reasoning.",
            "citation_title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
            "mention_or_use": "use",
            "model_name": "GPT-4; text-DaVinci-003",
            "model_size": null,
            "task_name": "GSM8K; Creative writing",
            "task_description": "GSM8K: elementary math word problems; Creative writing: writing coherent short passages ending with provided sentences.",
            "presentation_format": "Few-shot examples that include explicit reasoning steps for each example (worked-out chains) provided in prompt before target question.",
            "comparison_format": "Compared against zero-shot CoT, APE Zero-Shot CoT, few-shot without reasoning, direct prompting, iterative techniques, Least-to-Most.",
            "performance": "Provides notable accuracy improvements on GSM8K relative to direct prompting in many contexts, but in this study zero-shot CoT achieved comparable or better accuracy-cost tradeoff; manual CoT adds substantial tokens (long input) and complexity.",
            "performance_comparison": "Comparable to zero-shot CoT on some tasks; less cost-effective and often no better than simple zero-shot phrasing. For creative writing, manual CoT/few-shot examples can help demonstrate format and coherence but gains have shrunk for newer models.",
            "format_effect_size": "Gains exist but are often smaller per added token than zero-shot CoT; exact numeric effect sizes not consistently reported across model/task pairs.",
            "explanation_or_hypothesis": "Worked examples explicitly model desired multi-step behavior and can teach structure, but many models already have such patterns in pretraining so short zero-shot cues can suffice; manual CoT length/cost is a drawback.",
            "null_or_negative_result": false,
            "experimental_details": "Manual few-shot CoT prompts include several full Q+chain+answer examples. Input-length and token costs are high because of included exemplars—few-shot prompt input lengths are often multiple times the original question length.",
            "uuid": "e9283.3",
            "source_info": {
                "paper_title": "Cost, Complexity, and Efficacy of Prompt Engineering Techniques for Large Language Models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Least-to-Most",
            "name_full": "Least-to-Most Prompting (decompose into subproblems then solve sequentially)",
            "brief_description": "A few-shot/chain-of-thought method that demonstrates breaking a problem into simpler subproblems and solving them in sequence, designed to improve multi-step reasoning.",
            "citation_title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models",
            "mention_or_use": "use",
            "model_name": "GPT-4; text-DaVinci-003",
            "model_size": null,
            "task_name": "GSM8K; Creative writing",
            "task_description": "GSM8K: math word problems; Creative writing: generate coherent passages with specified endings.",
            "presentation_format": "Few-shot examples plus instructions encouraging decomposition into subproblems and sequential solving; model output may include explicit step numbering and subproblem solutions.",
            "comparison_format": "Compared to direct prompting, zero-shot CoT, manual CoT, Self-Refine, Tree-of-Thought, few-shot without reasoning.",
            "performance": "Adds many reasoning steps and substantially increases conversation length and token cost; accuracy gains on GSM8K are present but not proportional to the added token/complexity cost and often smaller than those from zero-shot CoT.",
            "performance_comparison": "Often outperformed by zero-shot CoT when considering per-token or per-cost improvements; in creative writing, added complexity did not translate into better human-preferred coherence and sometimes harmed readability/compliance.",
            "format_effect_size": "Large increases in tokens/steps with only modest accuracy gains; paper reports that Least-to-Most adds the most length among CoT methods but without proportional accuracy gains.",
            "explanation_or_hypothesis": "Decomposition increases explicit reasoning steps which can help on complex problems, but the very long structured outputs increase cost and risk of distraction or failure to follow instructions; this makes the method less cost-effective on studied tasks.",
            "null_or_negative_result": true,
            "experimental_details": "Least-to-Most prompts caused dramatic increases in linebreaks/steps and conversation length. Complexity metrics (line breaks, '1.,2.' markers) were notably higher for Least-to-Most. Task compliance suffered for many methods including Least-to-Most, particularly on older models.",
            "uuid": "e9283.4",
            "source_info": {
                "paper_title": "Cost, Complexity, and Efficacy of Prompt Engineering Techniques for Large Language Models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Self-Refine",
            "name_full": "Self-Refine Prompting (iterative refinement with self-feedback)",
            "brief_description": "An iterative method where the model first produces an answer, then is prompted to critique or provide feedback on its response and refine it, potentially repeating this loop.",
            "citation_title": "Self-Refine: Iterative Refinement with Self-Feedback",
            "mention_or_use": "use",
            "model_name": "GPT-4; text-DaVinci-003",
            "model_size": null,
            "task_name": "GSM8K; Creative writing",
            "task_description": "GSM8K: math word problems; Creative writing: produce coherent two-paragraph passages with specified endings.",
            "presentation_format": "Iterative prompting: initial generation followed by one or more critique/refinement prompts; final score measured after all refinement steps completed.",
            "comparison_format": "Compared to direct prompting, zero-shot CoT (original and APE), few-shot (with/without CoT), Least-to-Most, Tree-of-Thought.",
            "performance": "Variable. On GSM8K, Self-Refine generally performed well; with GPT-4 it could bring results up to the level of chain-of-thought methods in some cases. However, overall Self-Refine and Tree-of-Thought methods largely do not lead to consistent improvement and are highly variable in length and cost.",
            "performance_comparison": "Sometimes matches chain-of-thought performance (notably with GPT-4), but overall less reliable and more costly; benefits are task- and model-dependent.",
            "format_effect_size": "Variable; occasionally matches CoT gains on GPT-4 for GSM8K, but no consistent, robust advantage across tasks and models.",
            "explanation_or_hypothesis": "Iterative feedback can correct earlier mistakes but increases conversation length and complexity; success depends on the model's ability to self-criticize accurately, which is inconsistent across tasks and model versions.",
            "null_or_negative_result": true,
            "experimental_details": "Self-Refine implemented as iterative steps until refinement completion. Observed high variability in conversation lengths and token costs; task compliance issues detected, particularly for older models.",
            "uuid": "e9283.5",
            "source_info": {
                "paper_title": "Cost, Complexity, and Efficacy of Prompt Engineering Techniques for Large Language Models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Tree-of-Thought",
            "name_full": "Tree of Thought Prompting (searching/backtracking across multiple generated branches)",
            "brief_description": "An iterative, decision-tree style prompting approach in which the model expands multiple candidate steps/ideas, evaluates branches, and may backtrack to explore alternate solutions before finalizing an answer.",
            "citation_title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
            "mention_or_use": "use",
            "model_name": "GPT-4; text-DaVinci-003",
            "model_size": null,
            "task_name": "GSM8K; Creative writing",
            "task_description": "GSM8K: math word problems; Creative writing: produce coherent two-paragraph passages ending with specified sentences.",
            "presentation_format": "Iterative tree traversal where multiple candidate steps/ideas are generated and evaluated; backtracking is allowed, producing multiple rounds of generated material before a final answer.",
            "comparison_format": "Compared to direct prompting, zero-shot CoT, APE CoT, few-shot, Least-to-Most, Self-Refine.",
            "performance": "High variability, frequently very long conversations and token costs; overall did not produce proportional accuracy/quality gains for the tested tasks and often added complexity that made human review difficult; rarely the best-performing method.",
            "performance_comparison": "Often inferior in cost-effectiveness to zero-shot CoT; added complexity and length without commensurate quality/accuracy improvement across tested tasks.",
            "format_effect_size": "Large increases in tokens and number of steps; effect on accuracy/quality was inconsistent and often not positive when accounting for added cost and complexity.",
            "explanation_or_hypothesis": "Tree traversal and backtracking require finding and evaluating multiple candidate branches—such search behavior is expensive in tokens and may not correspond to patterns commonly seen in pretraining data, hence models struggle to exploit it reliably for these tasks.",
            "null_or_negative_result": true,
            "experimental_details": "Tree-of-Thought runs produced highly variable conversation lengths (sometimes massive), large numbers of linebreaks/steps, and higher implementation difficulty; human-ease-of-review ratings were low due to convoluted outputs. Significant non-compliance with prompt instructions noted, especially for older models.",
            "uuid": "e9283.6",
            "source_info": {
                "paper_title": "Cost, Complexity, and Efficacy of Prompt Engineering Techniques for Large Language Models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Direct Prompting (baseline)",
            "name_full": "Zero-Shot Direct Prompting (no specialized techniques)",
            "brief_description": "Baseline format consisting of giving the task/question directly to the model without special instructions, example chains, or iterative procedures.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4; text-DaVinci-003",
            "model_size": null,
            "task_name": "GSM8K; Creative writing",
            "task_description": "GSM8K: elementary math word problems; Creative writing: two-paragraph passage generation with specified endings.",
            "presentation_format": "Directly present the question/task and request a response; no additional reasoning cues, examples, or iterative refinement.",
            "comparison_format": "Used as the baseline comparator for all other presentation formats.",
            "performance": "Strong baseline performance especially for more modern models (GPT-4). For GSM8K, baseline accuracy has improved over time making gains from prompt engineering smaller; for creative writing baseline coherence is lower than the best chain-of-thought variants but sometimes comparable in human preference.",
            "performance_comparison": "All methods compared against direct prompting; Zero-shot CoT often significantly outperforms direct prompting, while few-shot without reasoning can be worse for reasoning tasks.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Direct prompting is shortest and cheapest; as base models become stronger, the room for improvement from elaborate prompting shrinks, reducing benefit/cost ratio of advanced methods.",
            "null_or_negative_result": false,
            "experimental_details": "Used as control across experiments. Direct prompting yields the shortest conversations (lowest token cost). The paper reports that prompt engineering typically increases tokens by 1–10× relative to direct prompting (zero-shot 1–2×; few-shot and iterative methods much higher).",
            "uuid": "e9283.7",
            "source_info": {
                "paper_title": "Cost, Complexity, and Efficacy of Prompt Engineering Techniques for Large Language Models",
                "publication_date_yy_mm": "2025-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models",
            "rating": 2,
            "sanitized_title": "leasttomost_prompting_enables_complex_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Large Language Models are Zero-Shot Reasoners",
            "rating": 2,
            "sanitized_title": "large_language_models_are_zeroshot_reasoners"
        },
        {
            "paper_title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Self-Refine: Iterative Refinement with Self-Feedback",
            "rating": 2,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data",
            "rating": 2,
            "sanitized_title": "automatic_prompt_augmentation_and_selection_with_chainofthought_from_labeled_data"
        },
        {
            "paper_title": "Language Models are Few-Shot Learners",
            "rating": 2,
            "sanitized_title": "language_models_are_fewshot_learners"
        }
    ],
    "cost": 0.0167455,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>International Journal on Science and Technology (IJSAT)</p>
<p>Milind Cherukuri cherukurimilind@gmail.com 
International Journal on Science and Technology (IJSAT)
2229-76777226C9C3DE9F022C788CEBDB91018AD6
This research investigates the impact of various prompt en-gineering techniques on the length, cost, complexity, and accuracy of responses from large language models (LLMs).By comparing direct prompting with zero-shot, few-shot, and chain-of-thought (CoT) meth-ods on tasks like GSM8K and creative writing, I analyze the trade-offs between token usage and response quality.Results show that while zero-shot CoT prompting is highly effective and costefficient, other meth-ods like Least-to-Most and Tree-of-Thought add significant length and complexity without proportional accuracy gains.Additionally, I discuss the financial implications, finding that GPT-4's unique pricing structure narrows the cost difference between manual/few-shot and zero-shot methods.Complexity analysis reveals that more intricate prompts often lead to convoluted outputs, challenging human review and implementa-tion.Our findings guide the selection of prompt engineering strategies to optimize both performance and resource utilization.</p>
<p>Introduction</p>
<p>Prompt engineering, the practice of developing specialized prompts and queries to improve the performance of Large Language Models, is a prominent topic of interest in the NLP community and among the general public.The practice is believed to allow for improvements in LLM performance in a variety of domains without investment in underlying training [21].It is not, however, without its critics.Some commentators believe that the practice will become irrelevant as models grow larger and more powerful, becoming more capable of directly inter-preting a user's intent without error [4].Others question the need for specialized professionals or training to attain minimal improvements, which are often not repeatable across contexts [13].</p>
<p>Despite such controversy, it is difficult to find empirical analyses of the trade-off between costs and performance benefits associated with advanced prompting.Papers introducing new prompting techniques often only include performance benchmarks concerning the techniques' efficacy, typically within a limited do-main.Some authors briefly mention problems associated with involved prompt-ing, such as the human time and effort induced by increased complexity and limitations on creativity and randomness [5], and others suggest the automation of prompting to avoid some of these costs [14].It is known that token costs, degradation of quality with increased prompt context lengths, and the uncertain nature of performance gains are all important practical considerations [36].How-ever, the extent of these issues for various techniques, so as to enable standardized comparison between them and with a control baseline (no special prompting), has not been (to my knowledge) quantified.</p>
<p>The quality, length, and complexity of LLM responses have been analyzed within several individual task and technique domains.Some research with GPT-3 series models on math and non-math reasoning tasks suggests the addition of length and complexity through the introduction of extra reasoning steps for both input prompts and output responses improves accuracy when using chain-of-thought prompting techniques [37].Effects are on the magnitude of several points of accuracy per added step, with generally low costs as long as prompt examples are selected carefully.Uniform improvements from complex chain-of-thought prompting are not fully accepted in the literature however -other work has noted a tendency for the method to lead to worse performance on simple questions [38].</p>
<p>Complexity has also been studied within the context of prompting for sum-marization and story-generation tasks.The Chain of Density prompting tech-nique seeks to optimize the named entity density of LLM-generated summaries through choice from a series of repeated, increasingly dense iterations [27].Hu-man preferences tend to align with 0.1 -0.15 named entities per token, a point near the middle of the usual sequence of generations, demonstrating the exis-tence of a tradeoff between informativeness and clarity.At the same time, other work has shown is difficult to control language model output complexity, mean-ing that the choice of specific techniques is important.Research demonstrates current models are not yet able to achieve compliance with desired readability instructions for the tasks of story generation, simplification, and summarization.However, a small amount of improvement is achievable through careful prompt word choice and the use of few-shot examples [28,29].</p>
<p>This paper uses several metrics to evaluate the benefits and drawbacks of prompt engineering methods systematically and analyzes the tradeoffs inher-ent in their application to standardized data.Such an assessment is valuable on several dimensions.Beyond quantifiably testing the practicality of prompt engineering as a whole, it can be used to compare the performance of different approaches.It is useful in a world where so many competing techniques are avail-able.I also provide a newly constructed dataset summarizing the wide variety of existing methods and data on their popularity as measured by Semantic Scholar citations, which may be useful for future surveys of the field.Next, I offer a new look at these prompting methods in a period long after their introduction.The current environment is one in which far greater capabilities are native to underlying foundation models.Finally, I introduce and adapt some useful mea-sures of accuracy, quality, length, and complexity, such as inter-paragraph cosine similarity and the ratio of interaction length with prompting to the length of an accepted human-generated answer, to the challenge of LLM evaluation.</p>
<p>Data</p>
<p>To evaluate performance, I attempt to use tasks that are general-purpose, close to real-world applications, and standardized in the literature.To this end, I selected the GSM8K dataset, a collection of elementary-level math word problems [?], and a creative writing task involving the generation of a coherent two-paragraph passage with random, predetermined ending sentences for each paragraph.The original creative writing task in [12] uses four paragraphs and sentences.How-ever, this is too difficult for older models and the manual production of good answer demonstrations -I take the first two sentences for each original question.These tasks carry several key benefits.They cover both the mathematical and linguistic domains -two types of tasks that form the foundation of the standardized testing of humans.GSM8K is studied in the majority of the pa-pers introducing techniques used in this paper.It is among the most common datasets used in the far larger list of papers I initially surveyed.Text and story generation is a widespread foundational task in NLP.Importantly, these sets of tasks are known to be free of data contamination.The GSM8K test set has been intentionally withheld from the training of OpenAI's models [6].The creative writing task was released only in 2023, and the code and data provided with the associated paper include only questions and not LLM responses. 1 I performed the analysis on one cutting-edge model and one older model from closer to the time that these techniques were introduced.This provides a picture of the changing costs and benefits of advanced prompting, a trend that may even be extrapolated into the future if current LLM scaling laws continue to hold.As the most widely used models and the ones behind much original work in the field, I select two models from the OpenAI series: GPT-4 (June 13th, 2023 version: 'GPT-4-0613') and text-DaVinci-003.All conversations were conducted programmatically via the OpenAI API.</p>
<p>Prompting Methods Assessed</p>
<p>The following methods were selected based on their popularity and ease of im-plementation.They are listed in order of initial paper release/discovery date below (according to Semantic Scholarcitation publication dates below may be for revisions).</p>
<p>-Few-Shot Prompting: The prompter provides a few examples of questions and correct answers before the main question/task.Notably featured in [30]. 1 It is also relatively simple to find random sentence generators online (such as https: //www.thewordfinder.com/random-sentence-generator/),which would work for this task -at the cost of comparability with the results of [12].In any case, the 2-sentence task represents a modification of the original, and the test overall makes use of subjective human evaluations of the coherence of generated stories, limiting comparisons anyway -so perhaps testing the task on more truly novel pairings is a promising direction for future work.</p>
<p>-Few-Shot (Manual) Chain-of-Thought Prompting: The model provides worked examples of answers in which the reasoning steps are written out.[17] Note, however, that such steps are not explicitly planned out or mentioned, as is the case in least-to-most prompting.-Least-to-Most Prompting: The model is given few-shot examples that demon-strate how first to break down the task into smaller and simpler subproblems and then solve them sequentially.[8] This is a few-shot and chain-of-thought method.</p>
<p>-Zero-Shot Chain of Thought Prompting (Original): Initial advances in the chain of thought prompting to improve reasoning were achieved by simply including the following before the question/task: "Let's think step by step."[9] For the creative writing task, the prompt following the question/task is adapted to: "Plan stepby-step before writing the passage." 2 -Zero-Shot Chain of Thought Prompting (Automatic Prompt Engineer): Au-tomated testing has indicated that an optimal zero-shot Chain of Thought prompt is "Let's work this out in a stepby-step way to be sure I have the right answer."[23] For the creative writing task, the prompt following the question/task is adapted to: "Plan step-by-step before writing the passage to be sure I have a correct and coherent answer." 3 -Self-Refine Prompting: The model produces an initial response and then is prompted for feedback, which it uses for refinement.[11] This is an iterative method.</p>
<p>-Tree of Thought Prompting: The language model traverses a tree of deci-sions -choosing among multiple steps or ideas it has generated to conclude.Backtracking is possible.[12] This is an iterative method.-Zero-Shot Control Baseline/Direct Prompting: This method consists of just providing the question/task directly.</p>
<p>Analyses</p>
<p>I provide performance scores as well as summary statistics (mean, standard deviation) of the metrics discussed below for each prompting method by model by question/task type.Statistical inference is uncommon in the prompting literature -most papers simply report performance scores. 4However, in this work, I do check for significant differences in metrics between prompting methods and the direct prompting baseline, reporting at the 95% level.For GSM8K accuracy</p>
<p>In initial experiments, this modification was found necessary to elicit any step-by-step behavior from the model.Again, in initial experiments, this modification was found necessary to elicit any step-by-step behavior from the model.Effect sizes and sample sizes are usually sufficiently large to merit confidence in the statistical significance of results.All paired t-tests comparing prompting methods and human responses were significant in the one paper I did find with inference, [28].</p>
<p>scores, I performed McNemar's tests. 5For sample means of creative writing scores and other metrics, I perform paired t-tests. 6</p>
<p>Quality and Accuracy</p>
<p>Results concerning the accuracy and/or quality of each method can be found in Table 1.For GSM8K problems, I report correct/incorrect accuracy at the point a technique has been fully implemented (the end of the tree of thought or, after all, Self-Refinement, etc.).Chain-ofthought methods (including Least-to-Most) provide substantial gains for this task when implemented both manually and with a zero-shot approach. 7Few-shot prompting -implemented in this paper as the provision of questions and answers without reasoning -is ineffective and even harmful.The iterative Self-Refine and Tree-of-Thought methods are generally well suited to this task, with the exception of Self-Refine checking when imple-mented with GPT-4, which can bring results up to the level of chain-of-thought methods.My initial analysis of creating writing coherence consisted of the author's assessment of generated passage coherence (on a scale of 1 to 10, 1 being incoherent and 10 being very coherent) based on several guidelines -unconnected ideas or abrupt changes in characters or settings received low scores, and opposite cases received high scores.Grader coherence scores are inherently noisy, as noted in [14], but I take several measures to alleviate this problem.</p>
<p>In this and other analyses, I provide adjustments for scores accounting for whether or not the task constraints were followed -whether responses did, indeed, contain two paragraphs with specified ending sentences.10 For human-preferred scores, adjustments for non-compliance give the outcomes in the case in which noncompliant responses are reduced to the lowest possible score.</p>
<p>Figure 1 shows the number of conversations where each method was one of the most preferred.It appears to be difficult to attain any gains from prompt engineering on this task, at least as far as human preferences for coherence are concerned.The provision of few-shot examples indicating preferences seems to be a minimally consistently promising approach, while other, more complex methods may be detrimental.</p>
<p>For my main results on creative writing, however, I introduce more objective measures of passage coherence that give a good sense of the concept on various levels -a novel approach for this creative writing task.To assess local coherence between sentences, I compute the average cosine similarity between consecutive sentence-level BERT embeddings (all-distilroberta-v1) [22], [23].</p>
<p>For passage with n sentences indexed by i, each with embedding si, average inter-sentence cosine similarity is given by n−1 First, I limit reported results for these grades to information concerning which method was preferred for each model and task question comprising a set of ending sentences.This limits demands on the data to only the ordinal ranking of methods.Second, I check the consistency of my methodology and results by soliciting a secondary opinion.I fine-tuned GPT-3.5 to learn my methodology for grading passage coherence (see Appendix Section ?? for prompts) -a difficult task, given the slippery nature of coherence as a concept and the wide variety of LLM responses and associated formats resulting from different models and methods.8 I created two fine-tuned models, each trained and validated on a randomly selected 50% fold of responses (stratified by method) and human coherence scores.I deployed each model on the other 50% fold.The results demonstrate the consistency and learnability of human preferences, for 76.5% of model-sentence pairings, human and automated scores agree on at least one of the methods as among For a more global measure, I compute embeddings for each paragraph of the LLM response by averaging the sentence embeddings and then the average cosine similarity between these paragraph embeddings for consecutive paragraphs. 11I believe this custom method best captures success on this task, as outlined in the initial prompt and question, and it produced logical results (see Appendix Section ?? for examples to aid in interpretation) that also had the highest correlation with human scores of any metric. 12It is my preferred metric for the rest of this paper.For passage with p paragraphs indexed by j each with number of sentences lj, comprised of sentences indexed by i each with embedding sj,i, average inter-paragraph cosine similarity is given by the top preferred.Simulations using empirical probabilities of scores under independence indicate that this would occur in only 21.84% of cases by chance. 9  5 The same questions are administered for each prompting method, so there is a dependence that this paired test accounts for.6 Some other metrics remain, such as the token cost of performance or change in performance divided by change in tokens for each method versus direct prompting.Bootstrapping confidence intervals for this sort of value seems possible, but the dependent nature of the data poses challenges.7 Surprisingly, APE-Improved Zero-Shot CoT is not more effective than the simple "Let's Think Step by Step approach, as was the case in [20].8 GPT-4 has been prompted to produce scalar scores for this task [14], but I generally found its grading to be inconsistent, even with few-shot prompts providing a few examples included.9 Contact the author for simulation details. 10 I additionally tried to use GPT-4 to assess adherence to the original instructions -to check if the exact sentences specified in the prompt were used.In my initial experiments -in contrast to [14] -I found that GPT-4 was not able to do this, repeatedly missing deviations of one or a few words, even when told to perform the check in a step-by-step manner carefully!I instead implemented simple logic using regular expressions, paired with a small amount of manual cleaning, and recommend this approach for future work.11 Other structural methods for assessing global coherence were considered but not implemented due to their limited additional value and overall feasibility for short, 2-paragraph passages.12 Contact the author for details on correlations.These measures are again reported in Table I.It isn't easy to attain much improvement in inter-sentence cosine similarity, particularly when accounting for task compliance, but chain-of-thought methods and the APE Zero-Shot CoT method (adapted to include an extra request for correctness and coherence for this task) perform well.Self-refinement also appears to be somewhat helpful.Gains from prompting are larger for inter-paragraph cosine similarity, and chain-of-thought methods (including least-to-most) tend to fairly consistently outperform, with some improvement coming from simple few-shot prompting on older models.Task compliance rates are low for both new and old models, suggesting careful adherence to complex instructions is an area with room for significant improvement in future models and methods.</p>
<p>No prompt engineering method demonstrates substantial improvement in task compliance, and prompting often leads to actively worse performance -potentially distracting models or adding additional complication.</p>
<p>Do larger/more modern models benefit more from prompt engineering, or are the techniques becoming obsolete?Gains from prompting on GSM8K questions have fallen as default performance on the task has improved dramatically.The creative writing task, however, is not a solved problem, and yet the gains from prompting still appear to be larger for older models -though task compliance also seems to suffer more under almost all techniques.Earlier evidence demonstrated that gains from few-shot learning increased with model scale -but this paper does not seem to support proof that the trend has held.[16] Few-shot benefits have, for the most part, decreased or flatlined with model improvement.</p>
<p>One might expect more recent prompt engineering techniques (as measured by paper release/publication date) to be more powerful and useful, but the evidence shows this is not the case.Chain-of-thought prompting, popularized throughout mid-late 2022 (and including least-to-most prompting) seems to have been the most significant innnovation not only for math-based reasoning tasks as might be expected, but also for the language-based creative writing challenge.The culmination of chain-ofthought improvements in zero-shotting is able to attain comparable and sometimes superior results relative the provision of manual/few-shot chain-of-thought examples.Few-shotting does still have its uses, however, particularly for creative writing tasks and when working with older models, where the construction of a coherent passage (a potentially subjective target) can be demonstrated.Complex iterative techniques such as Self-Refinement and traversal of the tree of thoughts, both introduced in 2023, largely do not lead to improvement (or attain less improvement than other methods), at least for these tasks.They may be appropriate only for some more specialized applications and can only be successfully applied for the most recent models.13 13Inspection of results found significant non-compliance with prompt instructions for these methods, especially with older models.</p>
<p>I can see that a method's performance seems fairly repeatable across tasks, but what about the variability of performance within a task?Very high accuracy/compliance scores indicate consistent performance and are present for only a few models and methods-perhaps only GPT-4 with one of the chain of thought methods can be relied upon, and only for the GSM8K task.Though inter-sentence results are consistent, variability in inter-paragraph cosine similarity is moderately large, and it is not entirely clear whether any method is sufficiently reliable.</p>
<p>Grader coherence scores are inherently noisy, as noted in [12], but I take several measures to alleviate this problem.First, I limit reported results for these grades to information concerning which method was preferred for each model and task question comprising a set of ending sentences.This limits de-mands on the data to only the ordinal ranking of methods.Second, I check the consistency of my methodology and results by soliciting a secondary opinion.I finetuned GPT-3.5 to learn my methodology for grading passage coherence -a difficult task, given the slippery nature of coherence as a concept and the wide variety of LLM responses and associated formats resulting from different models and methods. 8I created two finetuned models, each trained and vali-dated on a randomly selected 50% fold of responses (stratified by method) and human coherence scores.I deployed each model on the other 50% fold.The results demonstrate the consistency and learnability of human preferences, for of model-sentence pairings, human and automated scores agree on at least one of the methods as among the top preferred.Simulations using empirical probabili-ties of scores under independence indicate that this would occur in only of cases by chance. 9  8 GPT-4 has been prompted to produce scalar scores for this task [12], but I generally found its grading to be inconsistent, even with few-shot prompts providing a few examples included. 9Contact the author for simulation details.</p>
<p>In this and other analyses, I provide adjustments for scores accounting for whether or not the task constraints were followed -whether responses did, indeed, contain two paragraphs with specified ending sentences. 10For human-preferred scores, adjustments for non-compliance give the outcomes in the case in which non-compliant responses are reduced to the lowest possible score.</p>
<p>Figure 1 shows the number of conversations where each method was one of the most preferred.It appears to be difficult to attain any gains from prompt engineering on this task, at least as far as human preferences for coherence are concerned.The provision of few-shot examples indicating preferences seems to be a minimally consistently promising approach, while other, more complex methods may be detrimental.</p>
<p>For my main results on creative writing, however, I introduce more objective measures of passage coherence that give a good sense of the concept on various levels -a novel approach for this creative writing task.To assess local coherence between sentences, I compute the average cosine similarity between consecutive sentence-level BERT embeddings (all-distilroberta-v1) [39,40].</p>
<p>For passage with n sentences indexed by i, each with embedding si, average inter-sentence cosine similarity is given by For a more global measure, I compute embeddings for each paragraph of the LLM response by averaging the sentence embeddings and then the average cosine similarity between these paragraph embeddings for consecutive paragraphs. 11I believe this custom method best captures success on this task, as outlined in the initial prompt and question, and it produced logical results for examples to aid in interpretation) that also had the highest correlation with human scores of any metric. 12It is my preferred metric for the rest of this paper.</p>
<p>For passage with p paragraphs indexed by j each with number of sentences lj, comprised of sentences indexed by i each with embedding sj,i, average inter-paragraph cosine similarity is given by 10 I additionally tried to use GPT-4 to assess adherence to the original instructions -to check if the exact sentences specified in the prompt were used.In my initial experiments -in contrast to [12] -I found that GPT-4 was not able to do this, repeatedly missing deviations of one or a few words, even when told to perform the check in a step-by-step manner carefully!I instead implemented simple logic using regular expressions, paired with a small amount of manual cleaning, and recommend this approach for future work. 11Other structural methods for assessing global coherence were considered but not implemented due to their limited additional value and overall feasibility for short, 2-paragraph passages. 12Contact the author for details on correlations.</p>
<p>For both cosine similarity measures, I adjust for task compliance by removing non-compliant conversations.</p>
<p>These measures are again reported in Table 1.It isn't easy to attain much improvement in intersentence cosine similarity, particularly when accounting for task compliance, but chain-of-thought methods and the APE Zero-Shot CoT method (adapted to include an extra request for correctness and coherence for this task) perform well.Self-refinement also appears to be somewhat helpful.Gains from prompting are larger for inter-paragraph cosine similarity, and chain-of-thought methods (including least-to-most) tend to fairly consistently outper-form, with some improvement coming from simple few-shot prompting on older models.Task compliance rates are low for both new and old models, suggesting careful adherence to complex instructions is an area with room for significant improvement in future models and methods.No prompt engineering method demonstrates substantial improvement in task compliance, and prompting often leads to actively worse performance -potentially distracting models or adding additional complication.Do larger/more modern models benefit more from prompt engineering, or are the techniques becoming obsolete?Gains from prompting on GSM8K questions have fallen as default performance on the task has improved dramatically.The creative writing task, however, is not asolved problem, and yet the gains from prompting still appear to be larger for older modelsthough task compliance also seems to suffer more under almost all techniques.Earlier evidence demonstrated that gains from few-shot learning increased with model scale -but this paper does not seem to support proof that the trend has held.[30] Few-shot benefits have, for the most part, decreased or flatlined with model improvement.</p>
<p>One might expect more recent prompt engineering techniques (as measured by paper release/publication date) to be more powerful and useful, but the evidence shows this is not the case.Chain-of-thought prompting, popularized throughout mid-late 2022 (and including least-tomost prompting) seems to have been the most significant innnovation not only for math-based reasoning tasks as might be expected, but also for the language-based creative writing challenge.The culmination of chain-of-thought improvements in zero-shotting is able to attain comparable and sometimes superior results relative the provision of manual/few-shot chain-ofthought examples.Few-shotting does still have its uses, however, particularly for creative writing tasks and when working with older models, where the construction of a coherent passage (a potentially sub-jective target) can be demonstrated.Complex iterative techniques such as Self-Refinement and traversal of the tree of thoughts, both introduced in 2023, largely do not lead to improvement (or attain less improvement than other methods), at least for these tasks.They may be appropriate only for some more specialized applications and can only be successfully applied for the most recent models. 13I can see that a method's performance seems fairly repeatable across tasks, but what about the variability of performance within a task?Very high ac- 13 Inspection of results found significant non-compliance with prompt instructions for these methods, especially with older models.</p>
<p>curacy/compliance scores indicate consistent performance and are present for only a few models and methods-perhaps only GPT-4 with one of the chain of thought methods can be relied upon, and only for the GSM8K task.Though inter-sentence results are consistent, variability in interparagraph cosine sim-ilarity is moderately large, and it is not entirely clear whether any method is sufficiently reliable.</p>
<p>Length</p>
<p>Table II contains results on the length and cost of conver-sations.I begin with an analysis of the length of the entire interaction in tokens (using OpenAI's tiktoken tokenizer for the appropriate model).</p>
<p>Prompt engineering generally requires anywhere from 1-10 times as many tokens as direct prompting, which ends up having downstream effects for the amount of human effort and direct financial costs of tokens.Zero-shot methods show moderate increases in length -factors of only 1-2 times that of direct prompting.Self-Refine and Tree-of-Thought methods exhibit massive amounts of variability across questions and models that can lead to very long conversations -likely a result of differences in LLM decisions and backtracking.Few-shot methods generally have long lengths as a result of the space occupied by examples -often, many times, that of the actual question and results.This is clear from the results on input length, which are the vast majority of overall length for these methods -unlike the results for others.</p>
<p>For the GSM8K task, I can compare the length of con-versations for each method with the length of the provided question + answer in Figure 2, providing another perspective as to the extent to which prompt engineering can "stretch out" interactions.For text-davinci-003, direct prompting generally produces LLM conversations and responses somewhat shorter than provided answers, but zero-shot chain-ofthought prompt-ing brings lengths up into the expected range.Other methods typically add large multiples.For GPT-4, direct responses are already near the expected length and all forms of prompting add more tokens.</p>
<p>In Table II, the cost of conversations was computed using rates as of November 11, 2023 -2 cents per 1000 tokens for text-davinci-003, and 3 cents per 1000 tokens (input), 6 cents per 1000 tokens (output) for GPT-4.Prompting is the difference between spending a fraction of a penny on every conservation to several cents or more.Zero-shot methods do not increase costs much at all, and GPT-4's relative discount on input tokens makes manual/few-shot methods closer in expense to zero-shot ones, though there are still some differences.Similar to the results on length, the iterative Self-Refine and Tree-of-Thought methods have variable conversation costs.Figure 3 considers the average change in accuracy or quality divided by the change in tokens between the prompt engineering technique and the direct prompting baseline.Is any stretching of output adding value/improving accuracy/quality?For GSM8K, the results are in percentage points per additional token.Zero-shot chain-ofthought prompting (particularly with the original think step-by-step prompt) is cheap and extremely effective, and it outperforms techniques introduced earlier and later.The extra details and length of Least-to-Most and Manual Chain-of-Thought prompting do not provide the same benefit.Improve-ments in creative writing cosine similarity are harder to come by -note that changes are per 1,000 additional tokens, longer than most conversations ever last.However, I can still see that Zero-Shot Chain-of-Thought prompting is the most effective, with some benefit for the provision of examples using few-shot methods with the older text-davinci-003 model.</p>
<p>AQP E − AQB TokensP E − TokensB</p>
<p>Figure 4 repeats the same calculation, but per additional cent of financial cost and with different y-axis scales.</p>
<p>AQP E − AQB CostP E − CostB</p>
<p>The unique price structures for GPT-4 make this plot different, and the new y-axis scale provides additional insight -though one should be careful with extrapolation given the small cost (fraction of to a few cents) of most conversa-tions (these plots should be interpreted in the context of earlier tables for any given practical decision).The intricacies of pricing structure reduce the relative gains on GPT-4 and level out comparisons between methods.On older mod-els, prompting and zero-shotprompting are very likely cost-effective, but newer models have changed this calculation.</p>
<p>Complexity</p>
<p>Other metrics beyond just length may be of interest -for example, in under-standing how methods work, or when considering the important task of human review/checking of LLM responses.In Table ??, I report information on the com-plexity of responses.For both tasks, I examine the number of reasoning steps -linebreaks, sentences (NLTK sentence tokenizer), and strings "step i" [37] and "1.", "2.", "3.", etc. in the response. 14Due to the intricacies of results for various methods, none of these measures provides a full picture of complexity on its own, but their combination is more insightful.Here I report meaningful results based on knowledge of the underlying data.</p>
<p>For the GSM8K task, which often lacks standard formatting, drawing con-clusions from the number of linebreaks is difficult, though I can clearly see that prompting tends to add lines.By combining this information with the number of sentences (which may have its problems regarding tokenization accuracy), I get a clearer picture.All prompting methods, except for manual few-shot prompting (which encourages the model just to state a number directly), add line breaks or sentences.The long structures of methods such as Least-to-Most and Tree-of-Thought prompting add the most.For the creative writing task, data on the number of line breaks shows that prompting adds a significant amount of complexity through additional steps, with multiple chainof-thought and Tree-of-Thought methods adding the most.Considering the number of sentences, there is some doubt about this conclusion.Still, for Creative Writing, sentences likely mostly correspond to passage rather than planning length -an introduction of additional noise that makes them less useful as a metric.Despite the usage of the appearance of language such as "step i" as a metric in prior literature, it appears sparsely in my data, even for zero-shot methods without formatting.My new metric of the number of "1.", "2.", etc. is more useful.The written planning of explicit steps elucidated by Least-to-Most and other chain-of-thought meth-ods on GSM8K is clear, and there is also some planning for many methods in creative writing.</p>
<p>For the creative writing task, I also examine the sentence length (using NLTK word and sentence tokenizers) in the response and the Flesch Reading Ease (implemented via the textstat Python package) of the reaction [31,32].Most prompting methods produce slightly shorter sentences, except for the Tree-of-Thoughts, which makes longer ones -potentially through inclinations of the model to choose more complex paths and drafts of passages.Higher FRE scores indicate that the material is easier to read.When few-shot examples are provided for the manual prompting methods and Least-to-Most prompting, scores and readability increase somewhat relative to direct prompting (a modest effect, far 14 Sentences seem to be a better measure of complexity than just periods as were used in prior work (decimals, abbreviations, etc. present challenges, though the problems can be mitigated somewhat with regex).Semicolons were also considered, but in experiments, these did not appear unless models were specifically prompted towards including them."step i" comes from [37], but "1.", "2." etc. are novel metrics, to the best of my knowledge.textstat Python package) of the reaction [24], [25].Most prompting methods produce slightly shorter sentences, except for the Tree-of-Thoughts, which makes longer ones -potentially through inclinations of the model to choose more complex paths and drafts of passages.Higher FRE scores indicate that the material is easier to read.When few-shot examples are provided for the manual prompting methods and Least-to-Most prompting, scores and readability increase somewhat relative to direct prompting (a modest effect, far less than a grade level).Still, for the other methods, they remain similar or fall.Controlling readability may be difficult, but in line with the literature, it is clear that few-shot examples can provide a starting point.[12] Table IV compares a selected set of the metrics above in responses to provided answers and prompts.GSM8K responses are compared to the provided answer.Manual few-shot prompting leads to short and simple responses by design.Other methods generally introduce more reasoning steps and sentences than the provided answers -Least-to-Most prompting, in particular, shows a dramatic increase.The iterative, choice-based setup of Tree-of-Thought prompting is again notable in the addition of steps.For the creative writing task, sentence length is generally somewhat longer in the responses relative to the prompts for few-shot methods (data on the other techniques, which do not contain examples, is not as interpretable).For text-davinci-003, the few-shot responses are somewhat more readable or roughly in line with the readability of the prompts, but for GPT-4 they are less so -a concerning trend for the prospect of controlling readability in future.</p>
<p>The final components of my analysis of complexity are human-provided ratings of the ease of review and difficulty of implementation for each method.For ease of review, each technique was rated in Appendix Section ??.These results concur with the finding that Least-to-Most and Tree-of-Thought prompting add complexity, while few-shot methods often decrease it.Though chain-of-thought reasoning may add steps, these evaluations note the fact that they are usually not too difficult to follow.On the other hand, steps from least to most and Tree-of-Thought prompting are often difficult to piece back together.An additional note is that the provision of examples in few-shot prompting can, aside from readability, help with the formatting of responses.Appendix Section ?? offers some observations on the difficulty of implementing each method.Zero-shot methods are the easiest to implement.Fewshot methods require a few examples -though providing chains of reasoning can be tricky.Iterative Self-Refine and Tree-of-Thought methods are complicated and can require specialized skills and time investment.</p>
<p>Length or Complexity?</p>
<p>As is the case in the study of the chain-of-thought setting of [37], are any gains in performance coming from reasoning steps as opposed to length in tokens?A generalized answer to this question across methods is central to our understand-ing of if and how prompt engineering works.</p>
<p>Relationships may be complex.For language understanding tasks, prompt length has been found to improve model performance, with optimality achieved somewhere in the range of 20-100 tokens.Evidence shows that in longer conversations, models may get distracted, go off on tangents, or get stuck in a loop repeating themselves (to the extent some platforms have imposed length limitations).Accounting for non-linearity and the relation of length with reasoning steps to understand these phenomena may be helpful.</p>
<p>Table V implements regressions (with quadratic terms to model non-linearity) controlling for length and complexity to begin to address this question.To limit collinearity with my preferred length variable of thousands of tokens, I selected the number of steps/ideas as my desired complexity metric.I summed the "step i" and "1.", "2.", etc. measures into this single measure to improve data quality.For GSM8K, increases in linear conversation length do tend to improve performance, but the far larger squared term indicates that further increases are detrimental beyond a certain point.The coefficients are fairly small, being per thousand tokens, and less significant than those for the number of reasoning steps/ideas.Reasoning steps are more clearly linearly beneficial for performance, and their quadratic term is small.Stars indicate significance at the 95% level.Does increased length or complexity limit creativity and randomness?Table V also includes regression results on the creative writing task, which may be helpful -coherence requires creativity.Length and complexity (more steps/ideas and lower FRE scores) actually increase coherence, and the quadratic terms are weak.This comes at the cost of decreased task compliance, which follows the opposite relationship.</p>
<p>CONCLUSION</p>
<p>This broad standardized and quantitative evaluation of the trade-offs behind prompt engineering has revealed a fair amount of worthwhile benefit for a selected set of techniques.Chain-of-thought prompting outperforms other techniques and direct prompting on both math and language-based tasks, and it can be implemented quite cheaply and easily in a zero-shot manner.Why does chain-of-thought prompting do so well?One plausible explanation is that LLMs have many examples loosely following the method in their training data and paired with correct answers.It is easy to find examples of reasoning problems worked out step-by-step in humanity's collective corpus of text, but ostensibly more difficult to think of examples where a tree of possible decisions is considered or where there is text laying out a conversation of responses, feedback, and refinement.In a similar vein, LLMs may struggle with the creative writing task -even with prompt engineeringand with compliance with task instructions (to an alarming degree) as it is a unique task that is difficult to find examples for.Though the skills behind it and the task of maximizing passage coherence are generalizable and well-studied, the highly random nature of sentences used and the need to follow instructions exactly make for a challenge.Gains on this task from prompting are statistically significant but small and variable and limited to coherence rather than compliance.Aside from being a somewhat task-specific question, the overall efficacy of prompting has indeed shifted over time, with performance improvements decreasing along with en-hancements in base models.Prompting is also a length and terms of accuracy and quality per additional token and cent spent on prompting have clearly fallen.In addition to increasing the length of responses, prompting introduces additional complexity -reasoning steps and structure-and worsens readability (though sentence length may fall somewhat).Steps and complexity are not always a serious problem for human review, especially if done in a structured, ordered manner (Chain-of-Thought methods) -though this issue and complexity of implementation are real issues for the Tree-of-Thought (and, to a lesser extent, Self-Refine and, Least-to-Most) method.Few-shot prompting is indeed a promising way to slightly reduce readability problems, in addition to improving formatting and the alignment of responses to human preferences.Though gains relative to direct prompting are somewhat larger, responsiveness to the readability of few-shot example passages has fallen for newer models.Finally, this paper tested generalized models concerning the relationship between length and complexity and accu-racy/quality -offering further insight into the drivers of performance.On GSM8K, reasoning steps are indeed generally more important than length -which is also affected by a strong, negative quadratic term.For creative writing, coherence, length, and complexity improve cosine similarity but decrease task compliance.A chain of concise, organized, and well-chosen steps (or in a few cases, examples) can still bring out moderate gains from prompting, though real challenges and drawbacks remain.</p>
<p>Fig. 1 :
1
Fig. 1: Human-Preferred Method by Model -Creative Writing Coherence -Ad-just for Non-Compliance</p>
<p>Fig. 2 :Fig. 3 :
23
Fig. 2: Average Length vs. Provided Length -GSM8K</p>
<p>Fig. 4 :
4
Gains Per Cent v. Direct Prompting (a) GSM8K (b) Creative Writing</p>
<p>logit regressions, the average of marginal effects are reported, and a binary indicator for the model (text-davinci-003 or GPT-4) is included.Standard errors in parentheses.For linear models, fixed effects for the model and task question are included, and standard errors are clustered by task question by method.</p>
<p>TABLE I :
I
Mean and Standard Deviation of Accuracy/Quality Scores
Adjusted)GPT-4 0.3490.355<em>0.35</em>0.3440.3560.3510.3480.334(0.058(0.069(0.06)(0.059(0.065(0.059(0.061(0.046)))))))Task Metric Average Inter-Paragraph Cosine SimilarityModel Text-Davin ci-003Manu al Few-Shot 0.433<em> (0.177 )Manu al CoT (Jan 0.401 (0.191 )Least-to-Most (May 0.414 (0.168 )Zero-Shot CoT (May 0.357 (0.182 )APE Zero-Shot CoT 0.389 (0.183 )Self-Refine (Mar 2023) 0.34 (0.204 )Tree-of-Thoug ht 0.224 (0.143 )Direct Promp ting 0.371 (0.178 )(Complian(May2022)2022)2022)(Nov(Mayce2020)2022)2023)Adjusted)GPT-4 0.366</em>0.4230.4040.463<em>0.4490.3980.4550.42(0.154(0.15)(0.159(0.155(0.159(0.158(0.143(0.157GSM8 KAccuracy Text-Davin ci-003 Complianc ci-e Text-Davin0.18 0.6</em> 0.58<em> 0.62</em> 0.49<em> 0.2 ) ) ) ) ) 0.43 0.19</em> 0.25<em> 0.43 0.44 0.32</em> 0.04<em> 0.5 ) ) 0.23 0.23GPT-4 0.49</em> 0.93<em> 0.95</em> 0.95<em> 0.93</em> 0.89<em> 0.4</em> 0.73 003CreatiAverageText-GPT-4 0.63 0.51 0.52 0.57 0.56 0.48 0.26<em> 0.56 0.364 0.345 0.357 0.366 0.382</em> 0.369 0.357 0.363veInter-Davin(0.063(0.061(0.059(0.077(0.077(0.064(0.096(0.064WritinSentenceci-))))))))gCosine003SimilarityGPT-4 0.3460.35<em>0.3410.347</em>0.35<em>0.356</em>0.351<em>0.333(0.062(0.066(0.061(0.057(0.066(0.06)(0.061(0.049)))))))AverageText-0.476</em>0.479<em>0.48</em>0.41<em>0.42</em>0.3660.430.359Inter-Davin(0.163(0.161(0.162(0.192(0.192(0.179(0.222(0.178Paragraphci-))))))))Cosine003SimilarityGPT-4 0.386<em>0.4220.4220.465</em>0.464<em>0.4120.4470.42(0.146(0.147(0.151(0.151(0.148(0.161(0.145(0.158))))))))Average Inter-Sentence Cosine SimilarityText-Davin ci-0030.363 (0.064 )0.351 (0.063 )0.369 (0.059 )0.386 (0.076 )0.391</em> (0.072 )0.393 (0.054 )0.397 (0.104 )0.368 (0.072 )(Compliance
Stars indicate a significant difference from the direct prompting baseline from McNemar's tests at the 95% level for GSM8K accuracy scores and creative writing task compliance scores and paired t-tests at the 95% level for all other metrics.</p>
<p>Table 2 :
2
Conversation-and input-lengths and token-costs for prompting strate-gies on GSM8K and creative-writing tasks.Asterisks (<em>) mark the best result per model per metric.
Task MetricModel Manu alManu alLeast-to-Zero-ShotAPE Zero-Self-RefineTree-of-Direct PrompFew-CoTMostCoTShotThougtingShotCoThtGSM8 KConversati on LengthText-Davin ci-533.94 * (21.16722.7</em> (35.46 9)272.27 * (49.82166.02 * (47.20181.76 * (54.99125.99 * (36.17248.8<em> (116.2 45)87.06 (42.02 3)0032)4)4)5)5)GPT-4 579.69850.04332.36223.88243.93344.86764.72146.69</em><strong><em>*</em><em>(79.54(21.03(50.96(58.19(59.14(58.87(116.2(312.33)6))6)8)9)43)61)Input LengthText-Davin ci-531.48 * (21.04655.48 * (21.04158.48 * (21.0467.48</em> (21.04 3)80.48<em> (21.04 3)99.48</em> (21.04 3)137.68 * (77.5259.48 (21.04 3)0033)3)3)3)GPT-4 567.5<em>741.5</em>181.5<em>80.5</em>93.5<em>167.7</em>407.4272.5(21.01(21.01(21.01(21.01(21.01(38.21<em>(21.012)2)2)2)2)5)(154.32)53)Conversati on CostText-Davin ci-0.011</em> (0.0)0.014<em> (0.001 )0.005</em> (0.001 )0.003<em> (0.001 )0.004</em> (0.001 )0.003<em> (0.001 )0.005</em> (0.002 )0.002 (0.001 )003GPT-4 0.018<em>0.029</em>0.014<em>0.011</em>0.012<em>0.016</em>0.034<em>0.007(0.001(0.003(0.003(0.003(0.003(0.006(0.014(0.004))))))))Creati ve WritinConversati on LengthText-Davin ci-695.41 * (31.51960.51 * (36.441025.0 2</em> (34.55256.53 * (46.98274.04 * (58.59382.19 * (141.5916.81 * (157.0200.41 (30.56 5)g0037)7)9)2)3)96)42)GPT-4 751.04968.751091.4459.98470.72520.061325.1337.33</strong>1***<em>7</em>(43.73(35.27(55.61(46.79(49.62(53.05(210.2(158.58)2)1)4)8)7)2)59)
IJSAT25022584Volume 16, Issue 2, April-June 2025 14Stars indicate a significant difference from the direct prompting baseline from paired t-tests at the 95% level.</p>
<p>., etc.
Text-Davin ci-0.04<em> (0.2)2.54</em> (2.63)6.96<em> (3.06)1.84</em> (2.62)2.33<em> (3.24)0.5 (0.99)1.0 (0.0)0.72 (1.77)003GPT-4 0.04</em>2.8<em>6.95</em>1.991.95<em>2.36</em>3.5<em>1.28(0.2)(3.7)(3.36)(3.63)(3.21)(4.39)(4.23)(2.98)CreatiNumber ofText-1.07</em>6.01<em>7.03</em>4.67<em>4.37</em>3.1<em>11.29</em>0.98veLinebreaksDavin(0.26)(0.5)(0.3)(2.47)(2.97)(1.54)(2.75)(0.25)Writinci-g003GPT-4 2.01<em>3.69</em>7.5<em>10.93</em>10.77<em>4.37</em>18.54<em>2.08(0.17)(2.5)(1.55)(2.38)(2.97)(2.36)(5.19)(0.27)Number of SentencesText-Davin ci-10.08</em> (1.86)15.8<em> (1.72)17.82</em> (1.88)10.03<em> (2.38)10.12</em> (2.48)13.54<em> (5.37)31.37</em> (6.9)7.6 (1.33)003GPT-4 10.25<em>11.9314.76</em>15.92<em>15.43</em>15.97<em>39.42</em>11.32(1.89)(2.7)(2.45)(3.26)(3.9)(7.73)(6.18)(2.27)Number ofStep1,Step 2, etc.</p>
<p>., etc.
GPT-4 -1.68 0.82 5.91 0.16 0.47 2.44 5.35 -1.17Difference in Step Step 2, etc. 1, Number ofText-Davin ci-003 GPT-4 0.0 0.01 (0.1) Text-003 ci-Davin 0.02.9<em> (0.46) 0.94</em> 0.02.99<em> (0.1) 2.49</em> 0.00.85<em> (1.18) 3.58</em> 0.32 0.56 0.0 0.51<em> (0.89) 0.63</em> (0.79) 2.81<em> 0.8</em>2.84<em> (0.92) 3.93</em> 2.00.0 (0.0) 0.0 0.0(Response(0.0)(1.35)(0.7)(2.36)(2.68)(1.02)(1.48)(0.0)s Sentence Provided Length Answer)-Text-Davin ci-16.24 (2.31)14.4<em> (1.54)13.14</em> (1.36)15.38<em> (3.24)16.08 (3.6)14.63</em> (2.08)18.94<em> (2.97)16.41 (2.46)003 GPT-4 0.00.00.00.00.39 0.00.22 0.0DifferenceGPT-4 17.85</em> Text-17.8<em>17.61</em>18.6419.7617.38<em>20.78</em>19.11inDavin(2.45) -1.24 1.26 5.68 0.56 1.05 -0.78 -0.28 -0.56 (2.71) (2.42) (3.69) (4.85) (2.0) (2.67) (2.6)Flesch Number ofText-ci-76.66<em>74.175.9572.39</em>71.93<em>73.6966.68</em>74.73Reading 1., 2., etc.Davin 003(7.58)(6.22)(5.7)(8.54)(8.89)(7.86)(8.17)(8.24)Ease (Responseci-Score s-003Provided Answer)GPT-4 67.84<em> (7.43)67.76</em> (7.14)67.37<em> (5.97)59.95</em> (7.08)60.74<em> (6.69)62.37 (7.16)57.57</em> (6.62)63.78 (7.16)GPT-4 -1.24 1.52 5.67 0.71 0.67 1.08 2.22 0.0Stars indicate a significant difference from the direct prompting baseline from paired t-tests at the 95% level. TABLE IV: Differences of Complexity Metrics Difference Text-Creati (Response g 003 Length Writin ci-Sentence ve in Davin 4.54 2.82 1.64s-Prompts)Task Metric Ease Score Difference Reading in FleschModel Manu al Few-Shot 003 GPT-4 3.37 2.62 2.14 Manu al CoT Least-to-Most Text-ci-Davin 1.66 -0.14 2.07Zero-Shot CoTAPE Zero-Shot CoTSelf-RefineTree-of-Thoug htDirect Promp ting(Responses-GSM8 KPrompts) Difference in Number of Sentences (ResponseText-GPT-4 -7.29 -5.56 -6.87 Davin -1.68 2.28 9.63 1.98 1.7 ci-003 TABLE V: Regression Results-0.43 5.73 -1.19s Provided Answer) Model Conversati -on Length (ThousandConversati on Length (ThousandNumber of Steps/IdeasNumber of Steps/Idea s SquaredFlesch Reading EaseFlesch Reading Ease
Volume 16, Issue 2, April-June 2025</p>
<p>What is prompt tuning?. K Martineau, Feb. 2021</p>
<p>I have a strong suspicion that "prompt engineering" is not going to be a big deal in the long-term &amp; prompt engineer is not the job of the future AI gets easier. You can already see in Midjourney how basic prompts went from complex in v3 to easy in v4. Ethan Mollick, [ , Same with ChatGPT to Bing. Feb. 2023</p>
<p>Prompt engineering: is being an AI 'whisperer' the job of the future or a short-lived fad?. C Shackell, Aug. 2023</p>
<p>cost multiplier, especially for few-shot and iterative/complex. </p>
<p>prompt-engineering-is-being-an-ai-whisperer-the-job-of-the-future-or-a-short-lived-fad-6. methods. Putting these facts together and further accounting for the differing cost structures for newer models. the gains in</p>
<p>AI Prompt Engineering Isn't the Future. O A Acar, section: Technology and analytics. Jun. 2023</p>
<p>AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts. T Wu, M Terry, C J Cai, 10.1145/3491102.3517582CHI Conference on Human Factors in Computing Systems. New Orleans LA USAACMApr. 2022</p>
<p>Active Prompting with Chain-of-Thought for Large Language Models. S Diao, P Wang, Y Lin, T Zhang, arXiv:2302.12246May 2023</p>
<p>Prompt Engineering for Large Language Models. A Gao, Jul. 2023Rochester, NY</p>
<p>Complexity-Based Prompting for Multi-Step Reasoning. Y Fu, H Peng, A Sabharwal, P Clark, T Khot, arXiv:2210.00720Jan. 2023</p>
<p>Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data. K Shum, S Diao, T Zhang, arXiv:2302.12822Feb. 2023</p>
<p>From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting. G Adams, A Fabbri, F Ladhak, E Lehman, N Elhadad, arXiv:2309.04269Sep. 2023</p>
<p>ChatGPT vs Human-authored Text: Insights into Controllable Text Summarization and Sentence Style Transfer. D Pu, V Demberg, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational LinguisticsJul. 2023, 144Student Research Workshop)</p>
<p>Flesch or Fumble? Evaluating Readability Standard Alignment of Instruction-Tuned Language Models. J M Imperial, H T Madabushi, arXiv:2309.05454Sep. 2023</p>
<p>. K Cobbe, V Kosaraju, M Bavarian, M Chen, H Jun, L Kaiser, </p>
<p>. M Plappert, J Tworek, J Hilton, R Nakano, C Hesse, </p>
<p>Training Verifiers to Solve Math Word Problems. J Schulman, arXiv:2110.14168Nov. 2021</p>
<p>. Online, </p>
<p>. S Yao, D Yu, J Zhao, I Shafran, T L Griffiths, Y Cao, </p>
<p>Tree of Thoughts: Deliberate Problem Solving with Large Language Models. K Narasimhan, arXiv:2305.10601May 2023</p>
<p>GPT-4 Technical Report. Openai, arXiv:2303.08774Mar. 2023</p>
<p>. T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P , </p>
<p>. P Neelakantan, G Shyam, A Sastry, S Askell, Agarwal, </p>
<p>. G Herbert-Voss, T Krueger, R Henighan, A Child, D M Ramesh, J Ziegler, C Wu, C Winter, M Hesse, E Chen, M Sigler, Litwin, </p>
<p>. S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, </p>
<p>Language Models are Few-Shot Learners. D Sutskever, Amodei, arXiv:2005.14165Jul. 2020</p>
<p>. J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E H Chi, </p>
<p>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. Q V Le, D Zhou, </p>
<p>. D Zhou, N Schärli, L Hou, J Wei, N Scales, X Wang, </p>
<p>Least-to-Most Prompting Enables Complex Reasoning in Large Language Models. D Schuurmans, C Cui, O Bousquet, Q Le, E Chi, arXiv:2205.10625Apr. 2023</p>
<p>Large Language Models are Zero-Shot Reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, arXiv:2205.11916Jan. 2023</p>
<p>Large Language Models are Human-Level Prompt Engineers. Y Zhou, A I Muresanu, Z Han, K Paster, S Pitis, H Chan, J Ba ; Madaan, N Tandon, P Gupta, S Hallinan, L Gao, S Wiegreffe, Sep. 2022</p>
<p>. U Alon, N Dziri, S Prabhumoye, Y Yang, S Gupta, B P Majumder, </p>
<p>Self-Refine: Iterative Refinement with Self-Feedback. K Hermann, S Welleck, A Yazdanbakhsh, P Clark, arXiv:2303.17651May 2023</p>
<p>An introduction to latent semantic analysis. T K Landauer, P W Foltz, D Laham, 10.1080/01638539809545028Discourse Processes. Jan. 199825</p>
<p>sentence-transformers/all-distilroberta-v1 • Hugging Face. </p>
<p>How to Write Plain English. R Flesch, Jul. 2016</p>
<p>textstat: Calculate statistical features from text. S B Aggarwal, Chaitanya , </p>
<p>The Power of Scale for Parameter-Efficient Prompt Tuning. B Lester, R Al-Rfou, N Constant, arXiv:2104.08691Sep. 2021</p>
<p>. F Shi, X Chen, K Misra, N Scales, D Dohan, E H Chi, </p>
<p>Large Language Models Can Be Easily Distracted by Irrelevant Context. N Schärli, D Zhou, Proceedings of the 40th International Conference on Machine Learning. PMLR. the 40th International Conference on Machine Learning. PMLRJul. 202322744</p>
<p>Microsoft limits Bing chat exchanges and conversation lengths after 'creepy' interactions with some users. J Mann, </p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5522 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5522</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5522</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-114.html">extraction-schema-114</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <p><strong>Paper ID:</strong> paper-259213474</p>
                <p><strong>Paper Title:</strong> Large language models are universal biomedical simulators</p>
                <p><strong>Paper Abstract:</strong> Computational simulation of biological processes can be a valuable tool in accelerating biomedical research, but usually requires extensive domain knowledge and manual adaptation. Recently, large language models (LLMs) such as GPT-4 have proven surprisingly successful for a wide range of tasks by generating human language at a very large scale. Here we explore the potential of leveraging LLMs as simulators of biological systems. We establish proof-of-concept of a text-based simulator, SimulateGPT, that uses LLM reasoning . We demonstrate good prediction performance for various biomedical applications, without requiring explicit domain knowledge or manual tuning. LLMs thus enable a new class of versatile and broadly applicable biological simulators. This text-based simulation paradigm is well-suited for modeling and understanding complex living systems that are difficult to describe with physics-based first-principles simulation, but for which extensive knowledge and context is available as written text.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5522.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5522.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SimulateGPT (GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SimulateGPT: a GPT-4 based text‑based biomedical simulator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>SimulateGPT is a method that uses GPT-4 in a structured, stepwise prompting format to produce multi-step, text-based simulations of biological systems across molecular, cellular, organ, and organismal levels, generating intermediate steps (level, facts, entities, assumptions, consequence, probability, explanation, novelty) and a final outcome.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive large language model developed by OpenAI, trained on very large text corpora (including scientific literature and web content) to predict next tokens and capable of chain-of-thought style reasoning when prompted; accessed via OpenAI API (April–May 2023) with temperature=0 and max length=2048 for experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Broad biomedical sciences (molecular biology, cellular biology, immunology, oncology, physiology, translational medicine)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Text-based stepwise simulation of biological scenarios (e.g., cancer progression and prognosis, treatment response in sepsis, trained immunity experiments, mouse in vivo perturbations), producing intermediate mechanistic steps and final outcome predictions (classification and regression).</td>
                        </tr>
                        <tr>
                            <td><strong>accuracy_metric</strong></td>
                            <td>Varied by task: human expert Likert-scale rankings for qualitative scenarios; classification metrics (precision, recall, accuracy) for gene essentiality; error and correlation coefficients for progression-free survival regression; statistical tests (Wilcoxon signed-rank) used for expert comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Overall: SimulateGPT outperformed direct inference prompting across four blinded expert-evaluated qualitative scenarios (mouse immunology, trained immunity, sepsis, glioblastoma) (no single numeric aggregate provided). Gene essentiality: accuracy = 86% on a class-balanced dataset with high precision and recall. Direct inference baseline accuracy = 64% for gene essentiality. Colorectal progression-free survival (PFS): SimulateGPT produced significantly smaller prediction error and higher correlation coefficients than direct inference (exact numeric values not reported); tendency to underestimate variance in both methods. Variants: low-complexity variant worsened expert ratings but improved PFS prediction; enforcing >=5 simulation steps improved PFS prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Prompt structure (stepwise SimulateGPT vs direct inference), number of simulation steps (more steps improved some quantitative predictions), prompt calibration (few-shot examples for colorectal PFS), model stochasticity/settings (temperature=0, deterministic sampling), representation complexity (low-complexity vs full schema), availability/access to ground-truth data (used to avoid training-data leakage), high intrinsic variance of clinical datasets (e.g., glioblastoma) limiting predictability.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_factors</strong></td>
                            <td>Controlled comparisons: SimulateGPT vs direct inference across the same tasks showing consistent improvement in outcome prediction; gene-essentiality experiment compared to DepMap ground truth showing 86% accuracy for SimulateGPT vs 64% for direct inference; colorectal PFS calibrated by few-shot examples improved predictive performance; experiments with low-complexity prompt and enforced >=5-step prompt variants showed systematic changes (low-complexity: worse expert ratings but better PFS numeric error; >=5 steps: improved PFS). Expert evaluation (blinded, 3 postdocs) and statistical testing (Wilcoxon signed-rank) used to support claims.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Mixed evaluation: (1) Expert-based blinded surveys (three domain experts, 7-level Likert scale) assessing correctness, explanation, and completeness for four qualitative scenarios; comparisons between SimulateGPT and direct inference were performed and aggregated (worse/same/better counts and Wilcoxon signed-rank tests). (2) Data-driven validation against ground truth datasets: DepMap CRISPR-based essentiality lists for gene essentiality classification (precision, recall, accuracy on a held-out balanced subset); colorectal cancer PFS regression compared to aggregated clinical groups downloaded from cBioPortal with metrics reported as error and correlation coefficients (exact metric names not always specified). (3) Internal ablations: low-complexity vs full prompt variants and enforced-step variants.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Explanations from SimulateGPT were not consistently ranked better than direct inference and were sometimes not self-contained because intermediate step content was required to understand the final explanation; both SimulateGPT and direct inference tended to underestimate variance in clinical outcome predictions; low-complexity prompt decreased expert-perceived quality despite numeric improvements for some predictions; high-variance clinical datasets (e.g., glioblastoma) limit predictive accuracy and some genotype–outcome relationships remained unpredictable (linear classifiers performed at chance); GPT-4 was not specifically trained as a biomedical simulator and may recall incorrect facts; potential for training-data leakage was considered and controlled but cannot be fully excluded.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Direct comparisons performed between SimulateGPT and 'direct inference' prompting with same underlying GPT-4 model; SimulateGPT outperformed direct inference on outcome predictions across qualitative scenarios and on gene essentiality classification; low-complexity SimulateGPT and high-complexity (>=5 steps) variants were compared, showing trade-offs between expert-rated quality and numeric prediction accuracy; some baseline statistical models (linear classifiers) were mentioned as failing on glioblastoma.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use structured stepwise prompting to guide LLM simulation (include levels, facts, entities, assumptions, consequences, probabilities); enforce multi-step simulations (more than default steps can improve quantitative prediction); calibrate outputs with few-shot examples for regression tasks; evaluate with both domain experts and ground-truth datasets; consider ensemble/self-consistency (multiple runs) to quantify variation; augment LLMs with external retrieval/knowledge and enable code/math execution where needed; incorporate real-world experimental feedback and fine-tuning on domain data to improve fidelity.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5522.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5522.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gene essentiality (SimulateGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot gene essentiality prediction in cancer cell lines using SimulateGPT (GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>SimulateGPT was applied to predict whether individual genes are essential for cancer cell survival (binary classification) in specific cancer cell lines, evaluated against large-scale experimental ground truth from DepMap.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive LLM (GPT-4) used with the SimulateGPT stepwise prompting method to produce mechanistic steps and a final binary essential/non-essential outcome.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Molecular and cellular biology / cancer genomics</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Zero-shot binary classification of gene essentiality for cancer cell lines (predict whether knocking out a gene is essential for cell survival).</td>
                        </tr>
                        <tr>
                            <td><strong>accuracy_metric</strong></td>
                            <td>Precision, recall, accuracy on a class-balanced dataset (ground truth from DepMap CRISPR data).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Accuracy = 86% on the class-balanced dataset (SimulateGPT); direct inference baseline accuracy = 64% (baseline biased toward non-essential predictions). High precision and recall reported for SimulateGPT (exact precision/recall numbers not listed in main text but stated as 'high').</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Prompt structure (SimulateGPT stepwise prompting vs direct inference), class balance in evaluation dataset, and possibly the presence of domain-specific knowledge in GPT-4's training data.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_factors</strong></td>
                            <td>Direct comparison to direct inference baseline using the same GPT-4 model and evaluation against DepMap ground-truth lists; SimulateGPT substantially outperformed direct inference (86% vs 64%). The paper reports that direct inference exhibited a bias toward predicting non-essential.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Data-driven validation: Selected 25 broadly essential and 25 broadly non-essential genes from DepMap (public release) randomly sampled (seeded), and predictions compared to ground truth; metrics computed included precision, recall, and accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Limited to a relatively small randomly sampled test set (n=50); class-balanced test set may not reflect real-world class imbalance; potential unknown leakage if GPT-4 had seen DepMap or related data during pretraining (authors tried to control for this by choosing data accessible only via domain repositories and by comparing to direct inference).</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared directly to direct inference prompting on same tasks; SimulateGPT outperformed direct inference on accuracy and reduced bias toward non-essential predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use stepwise structured prompts for classification tasks in molecular biology; validate against experimental ground truth datasets (e.g., DepMap); be mindful of class balance and baseline biases when evaluating LLM-based classifiers.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5522.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5522.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Colorectal PFS (SimulateGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Progression-free survival prediction in colorectal cancer patients using SimulateGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>SimulateGPT produced regression-style predictions of progression-free survival (PFS) in months for groups of colorectal cancer patients based on aggregated clinical presentation, calibrated via few-shot examples from the dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4 accessed via API, prompted with SimulateGPT stepwise schema; prompts included four few-shot calibration examples drawn from the same dataset to reduce variance.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Clinical oncology / translational medicine</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Predict progression-free survival (in months) under optimal standard-of-care for grouped clinical presentations (regression task).</td>
                        </tr>
                        <tr>
                            <td><strong>accuracy_metric</strong></td>
                            <td>Prediction error (unspecified exact metric; reported as 'smaller error') and correlation coefficients between predicted and observed PFS; confidence intervals (standard error) shown in figures.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>SimulateGPT achieved substantially smaller prediction error and higher correlation coefficients than direct inference prompting; both methods tended to underestimate variance (predictions closer to the median). Exact numeric RMSE or correlation values are not provided in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Few-shot calibration with four example outcomes improved calibration; number of simulation steps (more steps via low-complexity variant or enforcing >=5 steps affected performance); prompt structure (SimulateGPT vs direct inference); dataset heterogeneity and variance.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_factors</strong></td>
                            <td>Authors performed experiments: (a) baseline SimulateGPT with default step behavior, (b) low-complexity variant (produced more steps) which improved numeric PFS prediction but worsened expert-rated simulation quality, and (c) a forced >=5-step variant which further improved performance. Few-shot calibration was explicitly used to reduce dataset variance.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Data-driven comparison against ground truth aggregated clinical groups derived from a colorectal cancer study on cBioPortal (n=18 held-out groups simulated); comparison metrics included error and correlation coefficients; visualized with confidence intervals and statistical significance markers in figures.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Both SimulateGPT and direct inference underestimated the variance in PFS across patients (regression outputs biased toward the median). Exact numeric performance metrics are not fully reported in text; small number of groups (n=18) for evaluation may limit generalizability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared SimulateGPT vs direct inference; compared SimulateGPT default vs low-complexity variant vs high-complexity (>=5-step) enforced variant. Low-complexity variant reduced expert ratings but improved numeric PFS prediction; enforcing additional steps improved numeric accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Calibrate regression prompts with few-shot examples from the target dataset; experiment with forcing a minimum number of reasoning/simulation steps; evaluate both human expert judgments and quantitative error/correlation metrics; be cautious of tendency to predict toward the median and consider methods to increase predicted variance (e.g., stochastic sampling, ensembles).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5522.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5522.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qualitative biomedical scenarios (SimulateGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Blinded expert-evaluated qualitative biomedical simulations: mouse immunology, trained immunity, sepsis treatment, glioblastoma</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>SimulateGPT was evaluated on four qualitative biomedical scenarios (in vivo mouse cyanide and melanoma experiments, trained immunity experiments, sepsis treatment decision support, and glioblastoma mutation effects on survival) via blinded expert Likert-scale surveys to assess correctness, explanation, and completeness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4 used with structured SimulateGPT prompts to produce multi-step mechanistic simulations and final outcomes for each scenario.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Immunology, toxicology, translational medicine, neuro-oncology</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Qualitative stepwise simulation to predict final outcomes and provide mechanistic explanations for: (1) mouse cyanide poisoning and melanoma (YUMM1.7) experiments, (2) trained immunity phenotypes after primary stimuli (beta-glucan, low-dose LPS, high-dose LPS), (3) sepsis treatment recommendations based on HLA-DR and ferritin levels, (4) glioblastoma mutation-group survival deviation from median.</td>
                        </tr>
                        <tr>
                            <td><strong>accuracy_metric</strong></td>
                            <td>Blinded human expert Likert-scale assessments (7-level) aggregated into worse/same/better comparisons; Wilcoxon signed-rank tests for paired comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>SimulateGPT outperformed direct inference prompting on outcome prediction across all four qualitative scenarios according to blinded expert rankings (exact numeric scores not reported in main text). Explanations were not ranked better than direct inference.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Structured stepwise prompting (SimulateGPT) improved outcome prediction relative to direct inference; complexity of simulation output affected human-rated quality (low-complexity variant performed worse by experts); number of steps and prompt design influenced results (forced >=5 steps improved some metrics). Dataset heterogeneity (glioblastoma) reduced predictability.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_factors</strong></td>
                            <td>Blinded evaluation by three postdoctoral biologists on nine Likert-scale statements per scenario; aggregated comparisons between SimulateGPT and direct inference with statistical testing; additional experiments comparing SimulateGPT vs low-complexity variant and high-complexity variant for sepsis and colorectal tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Three domain experts (postdocs) blinded to condition evaluated outputs using a 9-statement questionnaire on a 7-level Likert scale; comparisons focused on outcome and explanation; results aggregated into percentages and tested with two-sided Wilcoxon signed-rank tests.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Explanations from SimulateGPT were sometimes fragmented across simulation steps and therefore not self-contained, leading to no advantage over direct inference for explanation quality; small number of expert evaluators (n=3) may limit robustness; some scenarios (glioblastoma) constrained by high variance and limited predictive signal.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Direct inference prompting (same GPT-4 model) served as baseline. SimulateGPT outperformed it on outcomes but not on final explanation self-containedness. Low-complexity vs full SimulateGPT and high-complexity enforced-step variants compared in related experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>For qualitative expert-facing simulations, use full structured SimulateGPT prompts to maximize outcome prediction quality; ensure final explanations are self-contained (e.g., include step summaries) to improve human interpretability; collect blinded expert evaluations; consider trade-offs between textual simplicity and numeric predictive performance.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5522.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5522.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Conway's Game of Life (proof-of-concept)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Emulation of Conway's Game of Life dynamics using GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 was prompted with the explicit update rules of Conway's Game of Life and successfully simulated the 4-step cycle of a 'glider' pattern without manual adaptation, demonstrating the capacity to perform deterministic stepwise state transitions when given explicit rules.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4 prompted with explicit rules and asked to output each generation including LaTeX matrix visualizations; deterministic sampling (temperature=0) used.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Computational simulation / complex systems (toy example used to validate stepwise simulation capability)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Simulate discrete cellular automaton state transitions (Game of Life) over multiple generations given explicit rules and an initial grid configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>accuracy_metric</strong></td>
                            <td>Exact match of generated subsequent grid states to deterministic rule-based ground truth (visualized as LaTeX matrices); qualitative correctness over a 4-step glider cycle.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>GPT-4 simulated the entire glider cycle correctly for 4 steps (reported successful proof-of-concept). No numeric accuracy beyond correctness reported.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Explicitness of rules provided in prompt (authors supplied update rules and required LaTeX format), deterministic sampling (temperature=0), and constrained task structure (finite 4x4 grid).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_factors</strong></td>
                            <td>Experimental setup described: system prompt contained the rules and formatting instructions; GPT-4 produced correct subsequent generations without further intervention, contrasted with prior reports that some neural networks struggle with Game of Life.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Deterministic rule-based evaluation by checking whether produced grids matched expected next-generation states (rendered LaTeX shown in figures).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>This was a toy deterministic task and does not imply general capability across stochastic or highly quantitative physical/chemical simulations; success likely depended on being provided the explicit rules and constrained grid size and format.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Authors contrasted GPT-4's success on this task with literature reporting that some neural networks struggle to learn Game of Life dynamics, using the task as a simple proof-of-concept for stepwise simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>When simulating rule-based discrete systems with LLMs, provide explicit rules and constrained formatting instructions; use deterministic decoding (temperature=0) for reproducible outputs.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>GPT-4 Technical Report <em>(Rating: 2)</em></li>
                <li>Large Language Models Encode Clinical Knowledge <em>(Rating: 2)</em></li>
                <li>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models <em>(Rating: 1)</em></li>
                <li>Self-Consistency Improves Chain of Thought Reasoning in Language Models <em>(Rating: 2)</em></li>
                <li>Augmented Language Models: a Survey <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5522",
    "paper_id": "paper-259213474",
    "extraction_schema_id": "extraction-schema-114",
    "extracted_data": [
        {
            "name_short": "SimulateGPT (GPT-4)",
            "name_full": "SimulateGPT: a GPT-4 based text‑based biomedical simulator",
            "brief_description": "SimulateGPT is a method that uses GPT-4 in a structured, stepwise prompting format to produce multi-step, text-based simulations of biological systems across molecular, cellular, organ, and organismal levels, generating intermediate steps (level, facts, entities, assumptions, consequence, probability, explanation, novelty) and a final outcome.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Autoregressive large language model developed by OpenAI, trained on very large text corpora (including scientific literature and web content) to predict next tokens and capable of chain-of-thought style reasoning when prompted; accessed via OpenAI API (April–May 2023) with temperature=0 and max length=2048 for experiments.",
            "model_size": null,
            "scientific_subdomain": "Broad biomedical sciences (molecular biology, cellular biology, immunology, oncology, physiology, translational medicine)",
            "simulation_task": "Text-based stepwise simulation of biological scenarios (e.g., cancer progression and prognosis, treatment response in sepsis, trained immunity experiments, mouse in vivo perturbations), producing intermediate mechanistic steps and final outcome predictions (classification and regression).",
            "accuracy_metric": "Varied by task: human expert Likert-scale rankings for qualitative scenarios; classification metrics (precision, recall, accuracy) for gene essentiality; error and correlation coefficients for progression-free survival regression; statistical tests (Wilcoxon signed-rank) used for expert comparisons.",
            "reported_accuracy": "Overall: SimulateGPT outperformed direct inference prompting across four blinded expert-evaluated qualitative scenarios (mouse immunology, trained immunity, sepsis, glioblastoma) (no single numeric aggregate provided). Gene essentiality: accuracy = 86% on a class-balanced dataset with high precision and recall. Direct inference baseline accuracy = 64% for gene essentiality. Colorectal progression-free survival (PFS): SimulateGPT produced significantly smaller prediction error and higher correlation coefficients than direct inference (exact numeric values not reported); tendency to underestimate variance in both methods. Variants: low-complexity variant worsened expert ratings but improved PFS prediction; enforcing &gt;=5 simulation steps improved PFS prediction.",
            "factors_affecting_accuracy": "Prompt structure (stepwise SimulateGPT vs direct inference), number of simulation steps (more steps improved some quantitative predictions), prompt calibration (few-shot examples for colorectal PFS), model stochasticity/settings (temperature=0, deterministic sampling), representation complexity (low-complexity vs full schema), availability/access to ground-truth data (used to avoid training-data leakage), high intrinsic variance of clinical datasets (e.g., glioblastoma) limiting predictability.",
            "evidence_for_factors": "Controlled comparisons: SimulateGPT vs direct inference across the same tasks showing consistent improvement in outcome prediction; gene-essentiality experiment compared to DepMap ground truth showing 86% accuracy for SimulateGPT vs 64% for direct inference; colorectal PFS calibrated by few-shot examples improved predictive performance; experiments with low-complexity prompt and enforced &gt;=5-step prompt variants showed systematic changes (low-complexity: worse expert ratings but better PFS numeric error; &gt;=5 steps: improved PFS). Expert evaluation (blinded, 3 postdocs) and statistical testing (Wilcoxon signed-rank) used to support claims.",
            "evaluation_method": "Mixed evaluation: (1) Expert-based blinded surveys (three domain experts, 7-level Likert scale) assessing correctness, explanation, and completeness for four qualitative scenarios; comparisons between SimulateGPT and direct inference were performed and aggregated (worse/same/better counts and Wilcoxon signed-rank tests). (2) Data-driven validation against ground truth datasets: DepMap CRISPR-based essentiality lists for gene essentiality classification (precision, recall, accuracy on a held-out balanced subset); colorectal cancer PFS regression compared to aggregated clinical groups downloaded from cBioPortal with metrics reported as error and correlation coefficients (exact metric names not always specified). (3) Internal ablations: low-complexity vs full prompt variants and enforced-step variants.",
            "limitations_or_failure_cases": "Explanations from SimulateGPT were not consistently ranked better than direct inference and were sometimes not self-contained because intermediate step content was required to understand the final explanation; both SimulateGPT and direct inference tended to underestimate variance in clinical outcome predictions; low-complexity prompt decreased expert-perceived quality despite numeric improvements for some predictions; high-variance clinical datasets (e.g., glioblastoma) limit predictive accuracy and some genotype–outcome relationships remained unpredictable (linear classifiers performed at chance); GPT-4 was not specifically trained as a biomedical simulator and may recall incorrect facts; potential for training-data leakage was considered and controlled but cannot be fully excluded.",
            "comparisons": "Direct comparisons performed between SimulateGPT and 'direct inference' prompting with same underlying GPT-4 model; SimulateGPT outperformed direct inference on outcome predictions across qualitative scenarios and on gene essentiality classification; low-complexity SimulateGPT and high-complexity (&gt;=5 steps) variants were compared, showing trade-offs between expert-rated quality and numeric prediction accuracy; some baseline statistical models (linear classifiers) were mentioned as failing on glioblastoma.",
            "recommendations_or_best_practices": "Use structured stepwise prompting to guide LLM simulation (include levels, facts, entities, assumptions, consequences, probabilities); enforce multi-step simulations (more than default steps can improve quantitative prediction); calibrate outputs with few-shot examples for regression tasks; evaluate with both domain experts and ground-truth datasets; consider ensemble/self-consistency (multiple runs) to quantify variation; augment LLMs with external retrieval/knowledge and enable code/math execution where needed; incorporate real-world experimental feedback and fine-tuning on domain data to improve fidelity.",
            "uuid": "e5522.0"
        },
        {
            "name_short": "Gene essentiality (SimulateGPT)",
            "name_full": "Zero-shot gene essentiality prediction in cancer cell lines using SimulateGPT (GPT-4)",
            "brief_description": "SimulateGPT was applied to predict whether individual genes are essential for cancer cell survival (binary classification) in specific cancer cell lines, evaluated against large-scale experimental ground truth from DepMap.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Autoregressive LLM (GPT-4) used with the SimulateGPT stepwise prompting method to produce mechanistic steps and a final binary essential/non-essential outcome.",
            "model_size": null,
            "scientific_subdomain": "Molecular and cellular biology / cancer genomics",
            "simulation_task": "Zero-shot binary classification of gene essentiality for cancer cell lines (predict whether knocking out a gene is essential for cell survival).",
            "accuracy_metric": "Precision, recall, accuracy on a class-balanced dataset (ground truth from DepMap CRISPR data).",
            "reported_accuracy": "Accuracy = 86% on the class-balanced dataset (SimulateGPT); direct inference baseline accuracy = 64% (baseline biased toward non-essential predictions). High precision and recall reported for SimulateGPT (exact precision/recall numbers not listed in main text but stated as 'high').",
            "factors_affecting_accuracy": "Prompt structure (SimulateGPT stepwise prompting vs direct inference), class balance in evaluation dataset, and possibly the presence of domain-specific knowledge in GPT-4's training data.",
            "evidence_for_factors": "Direct comparison to direct inference baseline using the same GPT-4 model and evaluation against DepMap ground-truth lists; SimulateGPT substantially outperformed direct inference (86% vs 64%). The paper reports that direct inference exhibited a bias toward predicting non-essential.",
            "evaluation_method": "Data-driven validation: Selected 25 broadly essential and 25 broadly non-essential genes from DepMap (public release) randomly sampled (seeded), and predictions compared to ground truth; metrics computed included precision, recall, and accuracy.",
            "limitations_or_failure_cases": "Limited to a relatively small randomly sampled test set (n=50); class-balanced test set may not reflect real-world class imbalance; potential unknown leakage if GPT-4 had seen DepMap or related data during pretraining (authors tried to control for this by choosing data accessible only via domain repositories and by comparing to direct inference).",
            "comparisons": "Compared directly to direct inference prompting on same tasks; SimulateGPT outperformed direct inference on accuracy and reduced bias toward non-essential predictions.",
            "recommendations_or_best_practices": "Use stepwise structured prompts for classification tasks in molecular biology; validate against experimental ground truth datasets (e.g., DepMap); be mindful of class balance and baseline biases when evaluating LLM-based classifiers.",
            "uuid": "e5522.1"
        },
        {
            "name_short": "Colorectal PFS (SimulateGPT)",
            "name_full": "Progression-free survival prediction in colorectal cancer patients using SimulateGPT",
            "brief_description": "SimulateGPT produced regression-style predictions of progression-free survival (PFS) in months for groups of colorectal cancer patients based on aggregated clinical presentation, calibrated via few-shot examples from the dataset.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "GPT-4 accessed via API, prompted with SimulateGPT stepwise schema; prompts included four few-shot calibration examples drawn from the same dataset to reduce variance.",
            "model_size": null,
            "scientific_subdomain": "Clinical oncology / translational medicine",
            "simulation_task": "Predict progression-free survival (in months) under optimal standard-of-care for grouped clinical presentations (regression task).",
            "accuracy_metric": "Prediction error (unspecified exact metric; reported as 'smaller error') and correlation coefficients between predicted and observed PFS; confidence intervals (standard error) shown in figures.",
            "reported_accuracy": "SimulateGPT achieved substantially smaller prediction error and higher correlation coefficients than direct inference prompting; both methods tended to underestimate variance (predictions closer to the median). Exact numeric RMSE or correlation values are not provided in the text.",
            "factors_affecting_accuracy": "Few-shot calibration with four example outcomes improved calibration; number of simulation steps (more steps via low-complexity variant or enforcing &gt;=5 steps affected performance); prompt structure (SimulateGPT vs direct inference); dataset heterogeneity and variance.",
            "evidence_for_factors": "Authors performed experiments: (a) baseline SimulateGPT with default step behavior, (b) low-complexity variant (produced more steps) which improved numeric PFS prediction but worsened expert-rated simulation quality, and (c) a forced &gt;=5-step variant which further improved performance. Few-shot calibration was explicitly used to reduce dataset variance.",
            "evaluation_method": "Data-driven comparison against ground truth aggregated clinical groups derived from a colorectal cancer study on cBioPortal (n=18 held-out groups simulated); comparison metrics included error and correlation coefficients; visualized with confidence intervals and statistical significance markers in figures.",
            "limitations_or_failure_cases": "Both SimulateGPT and direct inference underestimated the variance in PFS across patients (regression outputs biased toward the median). Exact numeric performance metrics are not fully reported in text; small number of groups (n=18) for evaluation may limit generalizability.",
            "comparisons": "Compared SimulateGPT vs direct inference; compared SimulateGPT default vs low-complexity variant vs high-complexity (&gt;=5-step) enforced variant. Low-complexity variant reduced expert ratings but improved numeric PFS prediction; enforcing additional steps improved numeric accuracy.",
            "recommendations_or_best_practices": "Calibrate regression prompts with few-shot examples from the target dataset; experiment with forcing a minimum number of reasoning/simulation steps; evaluate both human expert judgments and quantitative error/correlation metrics; be cautious of tendency to predict toward the median and consider methods to increase predicted variance (e.g., stochastic sampling, ensembles).",
            "uuid": "e5522.2"
        },
        {
            "name_short": "Qualitative biomedical scenarios (SimulateGPT)",
            "name_full": "Blinded expert-evaluated qualitative biomedical simulations: mouse immunology, trained immunity, sepsis treatment, glioblastoma",
            "brief_description": "SimulateGPT was evaluated on four qualitative biomedical scenarios (in vivo mouse cyanide and melanoma experiments, trained immunity experiments, sepsis treatment decision support, and glioblastoma mutation effects on survival) via blinded expert Likert-scale surveys to assess correctness, explanation, and completeness.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "GPT-4 used with structured SimulateGPT prompts to produce multi-step mechanistic simulations and final outcomes for each scenario.",
            "model_size": null,
            "scientific_subdomain": "Immunology, toxicology, translational medicine, neuro-oncology",
            "simulation_task": "Qualitative stepwise simulation to predict final outcomes and provide mechanistic explanations for: (1) mouse cyanide poisoning and melanoma (YUMM1.7) experiments, (2) trained immunity phenotypes after primary stimuli (beta-glucan, low-dose LPS, high-dose LPS), (3) sepsis treatment recommendations based on HLA-DR and ferritin levels, (4) glioblastoma mutation-group survival deviation from median.",
            "accuracy_metric": "Blinded human expert Likert-scale assessments (7-level) aggregated into worse/same/better comparisons; Wilcoxon signed-rank tests for paired comparisons.",
            "reported_accuracy": "SimulateGPT outperformed direct inference prompting on outcome prediction across all four qualitative scenarios according to blinded expert rankings (exact numeric scores not reported in main text). Explanations were not ranked better than direct inference.",
            "factors_affecting_accuracy": "Structured stepwise prompting (SimulateGPT) improved outcome prediction relative to direct inference; complexity of simulation output affected human-rated quality (low-complexity variant performed worse by experts); number of steps and prompt design influenced results (forced &gt;=5 steps improved some metrics). Dataset heterogeneity (glioblastoma) reduced predictability.",
            "evidence_for_factors": "Blinded evaluation by three postdoctoral biologists on nine Likert-scale statements per scenario; aggregated comparisons between SimulateGPT and direct inference with statistical testing; additional experiments comparing SimulateGPT vs low-complexity variant and high-complexity variant for sepsis and colorectal tasks.",
            "evaluation_method": "Three domain experts (postdocs) blinded to condition evaluated outputs using a 9-statement questionnaire on a 7-level Likert scale; comparisons focused on outcome and explanation; results aggregated into percentages and tested with two-sided Wilcoxon signed-rank tests.",
            "limitations_or_failure_cases": "Explanations from SimulateGPT were sometimes fragmented across simulation steps and therefore not self-contained, leading to no advantage over direct inference for explanation quality; small number of expert evaluators (n=3) may limit robustness; some scenarios (glioblastoma) constrained by high variance and limited predictive signal.",
            "comparisons": "Direct inference prompting (same GPT-4 model) served as baseline. SimulateGPT outperformed it on outcomes but not on final explanation self-containedness. Low-complexity vs full SimulateGPT and high-complexity enforced-step variants compared in related experiments.",
            "recommendations_or_best_practices": "For qualitative expert-facing simulations, use full structured SimulateGPT prompts to maximize outcome prediction quality; ensure final explanations are self-contained (e.g., include step summaries) to improve human interpretability; collect blinded expert evaluations; consider trade-offs between textual simplicity and numeric predictive performance.",
            "uuid": "e5522.3"
        },
        {
            "name_short": "Conway's Game of Life (proof-of-concept)",
            "name_full": "Emulation of Conway's Game of Life dynamics using GPT-4",
            "brief_description": "GPT-4 was prompted with the explicit update rules of Conway's Game of Life and successfully simulated the 4-step cycle of a 'glider' pattern without manual adaptation, demonstrating the capacity to perform deterministic stepwise state transitions when given explicit rules.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "GPT-4 prompted with explicit rules and asked to output each generation including LaTeX matrix visualizations; deterministic sampling (temperature=0) used.",
            "model_size": null,
            "scientific_subdomain": "Computational simulation / complex systems (toy example used to validate stepwise simulation capability)",
            "simulation_task": "Simulate discrete cellular automaton state transitions (Game of Life) over multiple generations given explicit rules and an initial grid configuration.",
            "accuracy_metric": "Exact match of generated subsequent grid states to deterministic rule-based ground truth (visualized as LaTeX matrices); qualitative correctness over a 4-step glider cycle.",
            "reported_accuracy": "GPT-4 simulated the entire glider cycle correctly for 4 steps (reported successful proof-of-concept). No numeric accuracy beyond correctness reported.",
            "factors_affecting_accuracy": "Explicitness of rules provided in prompt (authors supplied update rules and required LaTeX format), deterministic sampling (temperature=0), and constrained task structure (finite 4x4 grid).",
            "evidence_for_factors": "Experimental setup described: system prompt contained the rules and formatting instructions; GPT-4 produced correct subsequent generations without further intervention, contrasted with prior reports that some neural networks struggle with Game of Life.",
            "evaluation_method": "Deterministic rule-based evaluation by checking whether produced grids matched expected next-generation states (rendered LaTeX shown in figures).",
            "limitations_or_failure_cases": "This was a toy deterministic task and does not imply general capability across stochastic or highly quantitative physical/chemical simulations; success likely depended on being provided the explicit rules and constrained grid size and format.",
            "comparisons": "Authors contrasted GPT-4's success on this task with literature reporting that some neural networks struggle to learn Game of Life dynamics, using the task as a simple proof-of-concept for stepwise simulation.",
            "recommendations_or_best_practices": "When simulating rule-based discrete systems with LLMs, provide explicit rules and constrained formatting instructions; use deterministic decoding (temperature=0) for reproducible outputs.",
            "uuid": "e5522.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "GPT-4 Technical Report",
            "rating": 2,
            "sanitized_title": "gpt4_technical_report"
        },
        {
            "paper_title": "Large Language Models Encode Clinical Knowledge",
            "rating": 2,
            "sanitized_title": "large_language_models_encode_clinical_knowledge"
        },
        {
            "paper_title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
            "rating": 1,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Augmented Language Models: a Survey",
            "rating": 1,
            "sanitized_title": "augmented_language_models_a_survey"
        }
    ],
    "cost": 0.014159,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Large language models are universal biomedical simulators</p>
<p>Moritz Schaefer 
Institute of Artificial Intelligence
Center for Medical Data Science
Medical University of Vienna
ViennaAustria</p>
<p>CeMM Research Center for Molecular Medicine
Austrian Academy of Sciences
ViennaAustria</p>
<p>Stephan Reichl 
Institute of Artificial Intelligence
Center for Medical Data Science
Medical University of Vienna
ViennaAustria</p>
<p>CeMM Research Center for Molecular Medicine
Austrian Academy of Sciences
ViennaAustria</p>
<p>Rob Ter Horst 
CeMM Research Center for Molecular Medicine
Austrian Academy of Sciences
ViennaAustria</p>
<p>Adele M Nicolas 
CeMM Research Center for Molecular Medicine
Austrian Academy of Sciences
ViennaAustria</p>
<p>Thomas Krausgruber 
Institute of Artificial Intelligence
Center for Medical Data Science
Medical University of Vienna
ViennaAustria</p>
<p>CeMM Research Center for Molecular Medicine
Austrian Academy of Sciences
ViennaAustria</p>
<p>Francesco Piras 
CeMM Research Center for Molecular Medicine
Austrian Academy of Sciences
ViennaAustria</p>
<p>Peter Stepper 
CeMM Research Center for Molecular Medicine
Austrian Academy of Sciences
ViennaAustria</p>
<p>Christoph Bock 
Institute of Artificial Intelligence
Center for Medical Data Science
Medical University of Vienna
ViennaAustria</p>
<p>CeMM Research Center for Molecular Medicine
Austrian Academy of Sciences
ViennaAustria</p>
<p>Matthias Samwald matthias.samwald@meduniwien.ac.at&amp;christoph.bock@meduniwien.ac.at 
Institute of Artificial Intelligence
Center for Medical Data Science
Medical University of Vienna
ViennaAustria</p>
<p>Large language models are universal biomedical simulators
1 * These authors contributed equally § Correspondence:
Computational simulation of biological processes can be a valuable tool in accelerating biomedical research, but usually requires extensive domain knowledge and manual adaptation. Recently, large language models (LLMs) such as GPT-4 have proven surprisingly successful for a wide range of tasks by generating human language at a very large scale. Here we explore the potential of leveraging LLMs as simulators of biological systems. We establish proof-of-concept of a text-based simulator, SimulateGPT, that uses LLM reasoning. We demonstrate good prediction performance for various biomedical applications, without requiring explicit domain knowledge or manual tuning. LLMs thus enable a new class of versatile and broadly applicable biological simulators. This text-based simulation paradigm is well-suited for modeling and understanding complex living systems that are difficult to describe with physics-based first-principles simulation, but for which extensive knowledge and context is available as written text.Main TextLarge language models (LLMs) have an impressive ability to use human language generation for problem solving, for example to answer complex questions and to build arguments step-by-step 1-3 . LLMs generate text auto-regressively, by incrementally predicting the next text token from preceding text 4 . Despite the simplicity of this underlying process, LLMs can solve tasks across diverse domains including medicine and biology, exceeding human experts on certain tasks 1,5-8 . Novel methods such as chain-ofthought reasoning further enhance their capabilities by imitating complex, causal reasoning patterns 9,10 .Here we describe, implement, and evaluate a new simulation paradigm based on LLMs. Our approach takes LLMs beyond imitating human writing and thinking. It employs LLMs to simulate biological systems in a qualitative, text-based manner without explicit domain knowledge or manual tuning.Computational modeling of molecular, cellular, and physiological biological processes can be used to gain scientific insights, guide experimental research, and potentially facilitate personalized medicine[11][12][13]. LLM-based simulation may complement existing simulation paradigms, in particular by exploiting the large implicit knowledgebase of LLMs and the versatile sequential model that LLMs operate on.This study provides proof-of-concept of a text-based biological simulator based on GPT-4 1 , evaluates this system in diverse biomedical scenarios, and outlines a roadmap for systematic development and application of LLMs as artificial intelligence (AI) simulators of complex biological processes(Fig. 1a).</p>
<p>Development and evaluation of the SimulateGPT method for LLM-based biological simulation</p>
<p>We first demonstrate the feasibility of LLM-based biological simulation in a simple and well-defined test case, by emulating Conway's Game of Life 14 in GPT-4. In contrast to neural networks, which have been reported to struggle with this task 15 , GPT-4 was able to simulate the "glider" pattern's entire cycle using the provided update rules without manual adaption or dedicated training (Fig. 1b, Supp. Text 1).</p>
<p>Beyond this simple proof-of-concept for GPT-4 based biological simulation, we anticipate that GPT-4 may be particularly useful for qualitative, text-based simulation of complex biological processes that are difficult or impossible to model based on their physical and chemical foundations. GPT-4 contains extensive relevant knowledge, as it has been trained on massive text corpora that appear to include a large percentage of the scientific literature in biology and medicine, as well as websites and discussions about various scientific and non-scientific topics. We found that GPT-4 can be prompted to infer meaningful estimates about cancer progression of cancer patients (Supp. Text 2,3). We thus hypothesized that the comprehensive knowledge incorporated in LLMs could be leveraged as an implicit rule set for simulating systems in biomedical, translational and life sciences disciplines. GPT-4 may thus take on a role similar to human expert panels evaluating biological scenarios, while avoiding some of their pitfalls.</p>
<p>We developed the SimulateGPT method of using GPT-4 as a biomedical simulator. This method enforces stepwise simulation, where each step is composed of a reasoning structure designed to facilitate simulation across multiple levels of biological organization (Supp. Text 4, Fig. 1c). Initial tests showed that our method provided meaningful explanations and probabilities and was generally able to provide correct scientific references to substantiate its reasoning (Fig. 1c, Supp. Text 5). To systematically evaluate the performance of SimulateGPT in terms of its ability to simulate complex biomedical scenarios in a stepwise manner toward outcome predictions, we compared its performance to direct inference prompting (Supp. Text 2) through expert-based and data-driven validations (Fig. 1d).</p>
<p>First, we evaluated SimulateGPT's ability to simulate biological processes in areas of established scientific knowledge through blinded expert surveys with Likert scale questions covering the correctness, explanation, and completeness of the simulation. We formulated four scenarios spanning different levels of complexity (Supp. Text 6): (i) In vivo mouse experiments with known outcomes; (ii) experiments exploring trained immunity, a recent and less well understood immunological concept of immune memory in innate cells; (iii) reasoning about novel treatment decision support based on limited clinical parameters in sepsis; and (iv) tumor mutation effects on overall survival in glioblastoma patients.</p>
<p>For each scenario, the simulation seeks to establish a specific outcome (e.g., progression-free survival of individual patients with colorectal cancer patients, as in Fig. 1c) along with an explanation of the path toward the simulated outcome. We simulated between one and three conditions for each scenario, and the performance was ranked by postdoctoral biologists in a blinded manner. SimulateGPT outperformed conventional GTP-4 based reasoning on outcome prediction in all four scenarios (Fig. 2a, Supp. Fig  S1a). In contrast, the explanations provided by SimulateGPT were not ranked better than those of direct inference prompting (Supp. Fig S1b). This is likely because SimulateGPT's final explanation was often not self-contained, as some of its parts were already provided by preceding simulation steps (Fig. 1c).</p>
<p>Application and extension of the SimulateGPT method to classification and regression tasks</p>
<p>LLMs excel in verbal reasoning tasks but struggle with structured predictions. We hypothesized that the structure that SimulateGPT imposes on LLM reasoning helps address this weakness and thereby improves the performance also for classification and regression tasks. We simulated two scenarios with outcomes for which ground truth data was publicly available for validation, but only accessible through domain-specific data repositories, thus making it highly unlikely that such information was incorporated into the training process of GPT-4. To further control for the possibility of data leakage during GPT-4's training, we evaluated the performance against conventional GTP-4 based reasoning.</p>
<p>In the first scenario, we focused on molecular and cellular mechanisms by simulating the essentiality of individual genes for cancer cell survival framed as a zero-shot binary classification task. The ground truth was provided by large-scale experimental data for the corresponding cancer cell lines. Simu-lateGPT exhibited high precision and recall, and achieved an accuracy of 86% on the class-balanced dataset (Fig. 2c). In contrast, direct inference exhibited a strong bias toward genes being predicted as non-essential, resulting in an accuracy of only 64% (Fig. 2b, Table S1).</p>
<p>In the second scenario, we predicted the expected progression-free survival of patients with colorectal cancer, given their clinical presentation (Fig. 1c, Supp. Text 5). To reduce the considerable variation across cancer datasets, we calibrated our model by few-shot learning with four example outcomes from the test dataset as part of the prompt. Both SimulateGPT and direct inference showed a tendency to underestimate variance in the dataset (i.e., on average they predicted progression-free survival closer to the median than suggested by the data). Nevertheless, SimulateGPT achieved substantially smaller error and increased correlation coefficients compared to direct inference (Fig. 2c, Supp. Table 2).</p>
<p>Investigating our simulation results, we noted that SimulateGPT typically produces three steps before concluding. We wanted to see whether more steps would improve the overall performance. To this end, we derived a low-complexity variant of SimulateGPT with reduced output features (Supp. Text 7), which generated more steps, presumably due to GPT-4's tendency to generate responses of similar length.</p>
<p>The low-complexity variant of SimulateGPT performed worse according to our experts (Fig. 2d), but progression-free survival prediction significantly improved (Fig. 2e). To validate these results, we modified SimulateGPT to enforce at least five simulation steps (Supp. Text 8). This modification led to improved performance (Fig. 2f), adding further support for the use of stepwise simulation in SimulateGPT.</p>
<p>Collectively, our experiments show good predictive performance of SimulateGPT across a diverse range of biomedical scenarios, suggesting that LLMs can be configured as explainable simulators that improve on complex outcome prediction over models that do not follow the step-by-step simulation implemented in SimulateGPT. Our method is readily applicable to a broad range of processes and applications in biology and medicine, facilitating the exploration and prediction of scenarios with minimal configuration. </p>
<p>Structured text-based reasoning using LLMs as a new paradigm for scientific simulation</p>
<p>The results achieved are remarkable given that GPT-4 was not specifically trained on biomedical data nor to operate as a scientific simulator. Nevertheless, this proof-of-concept study can only constitute an initial step toward the promising new research area of LLM-powered biological simulation. We propose ten components that will jointly enable universal biomedical simulation using LLMs. 1) Interactivity: Enabling follow-up dialogs after initial simulations to ask clarifying questions or to re-run simulations with hypothetical "what if" and "zoom in" scenarios at intermediary and terminal states.</p>
<p>2) Knowledge augmentation: Expanding the model's knowledge and reducing risk of LLMs generating incorrect output by allowing dynamic retrieval of information from external sources such as biomedical literature or knowledge graphs 16 .</p>
<p>3) Self-consistency: Running models multiple times with induced stochastic variation, in order to establish consensus results and quantify existing variation 17 . 4) Built-in mathematics and programming: Enabling the model to include mathematical models or run programming code, for example for numeric simulations as well as inline quantitative modeling 16 . 5) Self-reflection: Instructing the model to critique and revise its own simulation results to iteratively improve simulation quality 18 . 6) Bifurcation: Simulations may reach bifurcation points with multiple trajectories of varying probabilities, which could be modeled by splitting autoregressive generation into separate streams. 7) Reverse simulation: Running simulations backward in time to identify causes of an observed state. 8) Multi-modality: LLMs are increasingly capable of using multimodal data, allowing for seamless integration for example of imaging data into simulations. 9) Fine-tuning of simulation models: Creating new and optimized models by fine-tuning existing LLMs on data from biomedical experiments could greatly improve simulation capabilities. 10) Real-world feedback: Simulation is often most powerful when it guides further experimentation, and feeding the results of validation experiments back into the simulations can anchor their intermediary states to reality, enhancing accuracy and simulation breadth.</p>
<p>In conclusion, we envision that the SimulateGPT paradigm will be an important building block of future AI-augmented research infrastructures for biology, medicine, and potentially other scientific fields.</p>
<p>Online Methods</p>
<p>The SimulateGPT method</p>
<p>SimulateGPT (Supp. Text 4) leverages GPT-4 to create a text-based simulator of biological processes. It employs a structured approach to guide GPT-4 in generating a step-by-step simulation of biological processes based on the input parameters provided. SimulateGPT consists of the following main components, driving GPT-4 toward detailed and logically consistent output:</p>
<ol>
<li>input parameters corresponding to the user request 2. a flexible number of simulation steps 3. a final conclusion, comprising an outcome and an explanation Each simulation step is divided into level, facts, entities, assumptions, consequence, probability, explanation, and novelty. To use SimulateGPT effectively, users provide a biological or medical scenario as a starting point for the simulation. Optionally, a perturbation can be included or implied in the input. Users can further describe the type of outcome they are interested in. To increase the novelty of concepts used in the simulation, users can add the phrase "Focus on more novelty" to their prompt.</li>
</ol>
<p>GPT-4 was accessed for SimulateGPT experiments described in this paper in April and May 2023 via their Playground and the API via the langchain library. Default parameters were kept, except for the temperature (0) and the 'Maximum length' (2048). Our simulator prompts were provided as system prompts, whereas the simulation scenarios were provided as user prompts.</p>
<p>Simulating Conway's Game of Life</p>
<p>The simulation rules were described as system prompts, which further included instructions to output each completed configuration in a LaTeX-visualizable matrix format. The human_prompt contained the initial configuration of the grid, in our example the glider. We simulated a complete cycle of the glider (4 steps) without providing intermediary user input. The rendered LaTeX is shown in Fig. 1b.</p>
<p>Application scenario 1: Mouse immunology (qualitative)</p>
<p>Preclinical mouse models and in vivo experiments constitute a cornerstone in the field of biomedical and translational research. Nevertheless, they can be extremely challenging (time, labor, and animal welfare), and according to the 3Rs of animal research (replacement, reduction, and refinement) the reduction of animal experiments, due to ethical considerations, has always been an important objective in biomedical research and motivation for simulations. Immunology is one the most complex biological fields with pronounced organism-wide reciprocal cellular interplay and crosstalk. Advances in understanding immunology are directly translatable to address diseases like cancer, inflammatory and autoimmune disorders, which affect a large proportion of society. To test simple in vivo experimental setups in basic biology and immunology with well-established outcomes we simulated a wild-type mouse model subjected to two different perturbations: injection of cyanide as a chemical toxin 19 and transplantation of YUMM1.7 melanoma cancer cell lines 20 . We chose these two in vivo perturbations as cyanide poisoning and melanoma tumor progression experiments have been well described in literature with a bulk of robust in vivo scientific findings. We chose a prompt describing the most minimal experimental setup and instructions to report the "most relevant final outcome" (Supp. Text 6).</p>
<p>Application scenario 2: Trained immunity (qualitative)</p>
<p>Trained immunity, the concept of immunological memory in innate immune cells, has emerged over the last decade. Two phenotypes are described in the literature, after a primary stimulus: (i) training, characterized by an enhanced immune response upon re-challenge with a secondary stimulus (i.e., unspecific); and (ii) tolerance, characterized by a suppressed immune response upon re-challenge with the primary stimulus 21 . To determine if the (conservative) simulator correctly applies modern immunological concepts to more complex experimental setups, we devised three complementary scenarios. They only differed in their primary stimulus, which should result in innate memory. Concretely, we used beta-glucan for training, low-dose LPS for training, and high-dose LPS for tolerance (Supp. Text 6).</p>
<p>Application scenario 3: Sepsis treatment (qualitative)</p>
<p>Sepsis is a systemic inflammatory response syndrome accompanied with multiple organ dysfunctions 22 . It is a leading cause of death among intensive care patients with treatment options including anti-inflammatory agents and immunomodulators 23 . Sepsis therapies are limited with emerging personalized-medicine centered research attempting to bring forward novel therapies, diagnostic and prognostic markers by patients substratification into two distinct groups: (i) immunoparalysis, characterized by low HLA-DR expression on monocytes; and (ii) hyperinflammation, characterized by high HLA-DR expression on monocytes and increased circulating ferritin concentrations 24 . To test the simulator's reasoning on guiding novel treatment options based on only two reported clinical parameters, we used two scenarios describing the clinical features of each patient group (immunoparalysis and hyperinflammation) in a complementary manner followed by a concrete question for recommended treatment (Supp. Text 6).</p>
<p>Application scenario 4: Glioblastoma (qualitative)</p>
<p>Glioblastoma clinical data was downloaded from TCGA via the web portal for the CPTAC-3 study 25 and enriched with methylation data for the MGMT gene from cBioPortal. Genotypes for the tumors of each clinical data point were further obtained using the TCGA API. Only cases with a valid death date were considered and genotype information was only kept for the top 10 mutated genes in glioblastoma (derived from TCGA). Cases were grouped based on their binary mutation profile (gene mutated or not for the top-10 most frequently altered genes), leading to ten mutation groups with two or more cases each. We simulated each group's genotype using SimulateGPT towards an estimate of overall survival, to be provided as the months-deviation from the 15 month median glioblastoma survival derived from the dataset. Given the heterogeneous nature of this cancer type and the high variance in the dataset (we found linear classifiers to be unable to predict survival rates better than random), we selected the genotype-scenario for which the simulation outcome coincided best with the ground-truth outcome and which comprised at least four cases and asked for expert-assessment.</p>
<p>Application scenario 5: Colorectal cancer (quantitative)</p>
<p>Patient and sample data from a colorectal cancer study 26 were downloaded from cBioPortal. To account for patient-to-patient variation, the data was grouped by (carcinomatosis, tumor stage, tumor site, differentiation grade, cancer type) and aggregated (mean age, median progression-free survival). Groups of common clinical presentation were filtered to contain at least four cases. From these groups, we picked four diverse ones that we provided as few-shot examples in the prompt, to calibrate the model outcome.</p>
<p>The remaining groups (n=18) were simulated toward progression-free survival (in months) as outcome.</p>
<p>Application scenario 6: Gene essentiality in cancer cell-lines (quantitative)</p>
<p>We randomly selected 25 genes that are broadly essential and 25 genes that are broadly non-essential in cancer cell lines, based on gene lists extracted from DepMap (https://depmap.org) 27 . Specifically, we downloaded the essential gene list "CRISPRInferredCommonEssentials.csv" (n=1856), and the nonessential gene list "AchillesNonessentialControls.csv" (n=781) from DepMap Public release 22Q4. We randomly subsetted these gene lists to 25 genes each using the R function "sample" (random seed=42).</p>
<p>Expert evaluation</p>
<p>The qualitative evaluation of the simulation outcomes of application scenarios 1-4 was performed by three domain experts (postdoctoral researchers) that were blinded and not otherwise involved in the study design. We designed a questionnaire consisting of 9 statements, to be judged using a 7-level Likert scale, of which 7 were chosen based on previous studies in the field 28 , one was added to ask for novelty, and one to give experts the chance to judge their qualifications to evaluate this topic (Supp. Text 9). To ensure comparability and rigor we only compared the two coinciding fields (by design) of the direct inference and SimulateGPT system namely, outcome and explanation, in a blinded/randomized fashion. Additionally, we let them evaluate the full simulation output of SimulateGPT and low complexity system (Supp. Text 7) for the sepsis treatment experiments. The results were computationally unblinded and quantified using a direct comparison between the direct inference and SimulateGPT, yielding a count for worse, same, or better for each question and scenario, depending on the nature of the question. Thereby, we were able to account for inter-expert variation and potential biases due to the unbalanced number of cases per experiment. The final result was aggregated into percentages and visualized using stacked bar plots per experiment, across experiments, and per question across experiments ( Fig.  2a/d, Supp. Fig S1). Statistical analysis was performed for each direct comparison group (each row in all bar plots) using a two-sided Wilcoxon signed-rank test on the relative comparison data. Exact pvalues for all tested comparisons are provided with our code repository.</p>
<p>Code &amp; Data availability </p>
<p>Competing interests</p>
<p>CB is a cofounder and scientific advisor of Aelian Biotechnology and Neurolentech. The other authors declare no competing interests. Figure S1 Comprehensive expert evaluation results a, Simulation outcome expert evaluation as in Fig. 2a </p>
<p>Supplementary Material</p>
<p>Supplementary Text 1: Conway's Game of Life "glider" simulation</p>
<p>GPT-4 SYSTEM PROMPT</p>
<p>As a large language model, simulate and describe the evolution of a given configuration in Conway's Game of Life for one generation. Conway's Game of Life takes place on a finite 4x4 grid of square cells where each cell can be either alive or dead and dead cells surrounding it. The rules for each generation are as follows:</p>
<p>1 In addition, provide the new generation also in a LaTex bracketed matrix format. This is purely for providing feedback to the user and should not be used to simulate downstream simulations. Provide the latex using an empty space for dead and \blacksquare for alive cells as follows:
\begin{bmatrix} &amp; \blacksquare &amp; &amp; \ ... \end{bmatrix}
Following this entire description, simulate as many generations as indicated by the user. </p>
<p>GPT-4 USER PROMPT</p>
<p>Supplementary Text 2: GPT-4 prompt for direct inference</p>
<p>GPT-4 SYSTEM PROMPT</p>
<p>You are a science-based, comprehensive and mechanistic AI system with knowledge of biomedical processes across all levels of biology: molecular, cellular, organ, and organism. Your input consists of parameters describing a biomedical scenario and an outcome request. Based on the input, you infer the requested outcome. Format your response in YAML, following this schema: conclusion: outcome: <Infer the final outcome based on the provided input.> explanation: <Offer a good explanation as evidence that supports the final outcome.></p>
<p>Supplementary Text 3: Example simulation for colorectal cancer patient prognosis</p>
<p>GPT-4 USER PROMPT</p>
<p>Consider a 60 year old patient who is diagnosed with stage IV colon adenocarcinoma on the right side of the colon. The tumor exhibits moderately differentiated differentiation histology, has metastasized and shows no signs of carcinomatosis.</p>
<p>Given the general effectiveness of colon adenocarcinoma therapies, explore how the disease progresses under an optimal standard of care treatment plan, focusing on the molecular and higher-level processes impacting tumor progression. As the final outcome, report the expected progression-free survival in months.</p>
<p>GPT-4 OUTPUT</p>
<p>conclusion: outcome: 8-12 months explanation: The patient is diagnosed with stage IV colon adenocarcinoma, which indicates that the cancer has metastasized to distant organs. The moderately differentiated histology suggests that the tumor cells are somewhat similar to normal cells, which may make them more responsive to treatment than poorly differentiated cells. However, the advanced stage of the cancer and the patient's age may limit the effectiveness of treatment. Under an optimal standard of care treatment plan, which may include surgery, chemotherapy, and targeted therapy, the expected progression-free survival for stage IV colon cancer patients is typically between 8-12 months. This estimate takes into account the molecular and higher-level processes impacting tumor progression, such as the ability of cancer cells to evade the immune system, develop resistance to therapy, and establish new blood vessels to support their growth.</p>
<p>Supplementary Text 4: GPT-4 prompt for stepwise simulation used by SimulateGPT</p>
<p>GPT-4 SYSTEM PROMPT</p>
<p>You are a science-based, comprehensive and mechanistic simulator of biomedical processes across all levels of biology: molecular, cellular, organ, and organism.</p>
<p>Your input consists of simulation parameters. Based on the input, you simulate all relevant processes that unfold step-by-step until a final outcome can be directly inferred from the simulation Simulation rules: -Begin the simulation at the level of biology matching the input best.</p>
<p>-Ensure that each step logically informs the next step.</p>
<p>-Use as many steps as necessary.</p>
<p>-Conclude the simulation with a final outcome, once it can be directly inferred from the simulation steps.</p>
<p>Aim for an informative level of detail. Ensure that every step logically follows up on all previous steps and that processes in subsequent steps are informed by previous steps. Format your response in YAML, following this schema:</p>
<p>parameters:</p>
<p>-<first relevant parameter for simulation> -<second relevant parameter for simulation> -... simulation:</p>
<p>-step: 1 level: <Indicate the level of biology of this step.> facts: <Provide a comprehensive overview of facts about the entities and processes you are considering, including facts that are not stated in the query. Attempt to include gene regulation, protein interactions, cell types, tissue functions, and organ functions that might influence the step and its consequences. Avoid repeating any facts that you already provided. Mention facts that might become relevant later. Provide references for all facts you list at the end only using the provided structure and indicate it using [1].> entities: <Enumerate all involved entities such as genes, proteins, cell types, tissues, organs, etc. by their name.> assumptions: <Integrate the stated facts and previous consequences into assumptions about the current step.> consequence: <Generate the most probable consequence, given the facts, assumptions, and any previously generated consequences. Given the level of biology of this step, provide fitting specifics like different types of entities (e.g. small molecules, RNA, DNA, proteins, metabolites, cell types, tissues, organs) and processes (e.g., interactions, pathways, biological processes, biochemical reactions, tissue function, organ function). For example: decreased \<gene X> expression by \<cell type X>, increased \<metabolite X> levels in the blood, changed \<morphology X> in \<tissue X>, increased \<organ X> function, systemic \<phenotype X>, etc.> probability: <The probability of the consequence happening on a scale of 0 to 100.> explanation: <Offer a good explanation as evidence that supports the listed consequence.> novelty: <How novel or unconventional is your reasoning on a scale of 0 to 100.> -step: 2 ... conclusion: outcome: <Infer the final outcome from the last simulation step.> explanation: <Offer a good explanation as evidence that supports the final outcome.> references: "[1]": "<authors> <year> <journal> <title of the publication>" " [2]": "<authors> <year> <journal> <title of the publication>" …</p>
<p>Supplementary Text 5: Example SimulateGPT simulation for colorectal cancer prognosis</p>
<p>GPT-4 USER PROMPT (same as in Supp. Text 3)</p>
<p>Consider a 60 year old patient who is diagnosed with stage IV colon adenocarcinoma on the right side of the colon. The tumor exhibits moderately differentiated differentiation histology, has metastasized and shows no signs of carcinomatosis.</p>
<p>Given the general effectiveness of colon adenocarcinoma therapies, explore how the disease progresses under an optimal standard of care treatment plan, focusing on the molecular and higher-level processes impacting tumor progression. As the final outcome, report the expected progression-free survival in months.</p>
<p>GPT-4 OUTPUT</p>
<p>parameters:</p>
<p>-age: 60 -diagnosis: stage IV colon adenocarcinoma -tumor_location: right side of the colon -differentiation_histology: moderately differentiated -metastasis: present -carcinomatosis: absent -treatment: optimal standard of care simulation: -step: 1 level: molecular facts: The standard of care for stage IV colon adenocarcinoma typically includes chemotherapy, targeted therapy, and immunotherapy [1]. Chemotherapy drugs, such as 5-fluorouracil (5-FU), oxaliplatin, and irinotecan, target rapidly dividing cancer cells [2]. Targeted therapies, like bevacizumab and cetuximab, inhibit specific molecular pathways involved in tumor growth and progression [3]. Immunotherapies, such as pembrolizumab, enhance the immune system's ability to recognize and destroy cancer cells [4]. entities: 5-fluorouracil, oxaliplatin, irinotecan, bevacizumab, cetuximab, pembrolizumab assumptions: The patient receives the optimal standard of care treatment, which includes a combination of chemotherapy, targeted therapy, and immunotherapy.</p>
<p>consequence: Decreased tumor cell proliferation and increased cancer cell death. probability: 80 explanation: The combination of chemotherapy, targeted therapy, and immunotherapy is expected to effectively target and kill cancer cells, reducing tumor growth and progression. novelty: 0 -step: 2 level: cellular facts: Tumor cells can develop resistance to chemotherapy and targeted therapies through various mechanisms, such as drug efflux, DNA repair, and alterations in drug targets [5]. Immunotherapies can also become less effective due to immune evasion by cancer cells [6]. entities: drug resistance, immune evasion assumptions: The patient's cancer cells may develop resistance to the administered treatments over time.</p>
<p>consequence: Reduced treatment effectiveness and potential tumor progression. probability: 60 explanation: The development of drug resistance and immune evasion can lead to reduced treatment effectiveness, allowing the tumor to progress despite the optimal standard of care.</p>
<p>novelty: 0 -step: 3 level: organ facts: Metastatic colon cancer can spread to other organs, such as the liver, lungs, and peritoneum [7]. The presence of metastasis can complicate treatment and negatively impact prognosis [8].</p>
<p>entities: liver, lungs, peritoneum assumptions: The patient's cancer has already metastasized, which may affect the treatment response and prognosis.</p>
<p>consequence: Increased difficulty in achieving complete tumor control and potential for further metastasis.</p>
<p>probability: 70 explanation: The presence of metastasis increases the complexity of treatment and the likelihood of further metastatic spread, which can negatively impact the patient's prognosis.</p>
<p>novelty: 0 conclusion: outcome: The expected progression-free survival is approximately 12 months. explanation: The optimal standard of care treatment plan, including chemotherapy, targeted therapy, and immunotherapy, is expected to initially control tumor growth and progression. However, the development of drug resistance, immune evasion, and the presence of metastasis may eventually lead to treatment failure and disease progression. Based on the simulation, the expected progression-free survival for this patient is approximately 12 months [9]. </p>
<p>GPT-4 USER PROMPTS</p>
<p>Mouse immunology: Cyanide</p>
<p>A wild type mouse is injected with Cyanide. Report the most relevant final outcome.</p>
<p>YUMM_1_7</p>
<p>A wild type mouse is injected with YUMM 1.7. Report the most relevant final outcome. </p>
<p>Sepsis treatment: sepsis_hyperinflammation</p>
<p>A patient with sepsis has high ferritin levels and a high percentage of CD45/CD14-monocytes that express HLA-DR. What treatment would be recommended? sepsis_immunoparalysis A patient with sepsis has low ferritin levels and a low percentage of CD45/CD14-monocytes that express HLA-DR. What treatment would be recommended?</p>
<p>Supplementary Text 7: GPT-4 prompt used by low-complexity SimulateGPT simulation</p>
<p>You are a science-based, comprehensive and mechanistic simulator of biomedical processes across all levels of biology: molecular, cellular, organ, and organism.</p>
<p>Your input consists of simulation parameters. Based on the input, you simulate all relevant processes that unfold step-by-step until a final outcome can be directly inferred from the simulation Simulation rules: -Begin the simulation at the level of biology matching the input best.</p>
<p>-Ensure that each step logically informs the next step.</p>
<p>-Use as many steps as necessary.</p>
<p>-Conclude the simulation with a final outcome, once it can be directly inferred from the simulation steps. Aim for an informative level of detail. Ensure that every step logically follows up on all previous steps and that processes in subsequent steps are informed by previous steps. Format your response in YAML, following this schema: simulation:</p>
<p>-step: 1 level: <Indicate the level of biology of this step.> consequence: <Generate the most probable consequence, given any previously generated consequences. Given the level of biology of this step, provide fitting specifics like different types of entities (e.g. small molecules, RNA, DNA, proteins, metabolites, cell types, tissues, organs) and processes (e.g., interactions, pathways, biological processes, biochemical reactions, tissue function, organ function).> probability: <The probability of the consequence happening on a scale of 0 to 100.> explanation: <Offer a good explanation as evidence that supports the listed consequence.> -step: 2 ... conclusion: outcome: <Infer the final outcome from the last simulation step.> explanation: <Offer a good explanation as evidence that supports the final outcome.></p>
<p>Supplementary Text 8: GPT-4 prompt used by high-complexity SimulateGPT simulation</p>
<p>You are a science-based, comprehensive and mechanistic simulator of biomedical processes across all levels of biology: molecular, cellular, organ, and organism.</p>
<p>Your input consists of simulation parameters. Based on the input, you simulate all relevant processes that unfold step-by-step until a final outcome can be directly inferred from the simulation Simulation rules: -Begin the simulation at the level of biology matching the input best.</p>
<p>-Ensure that each step logically informs the next step.</p>
<p>-Use at least 5 steps and as many more as necessary.</p>
<p>-Conclude the simulation with a final outcome, once it can be directly inferred from the simulation steps.</p>
<p>Aim for an informative level of detail. Ensure that every step logically follows up on all previous steps and that processes in subsequent steps are informed by previous steps. Format your response in YAML, following this schema:</p>
<p>parameters: -<first relevant parameter for simulation> -<second relevant parameter for simulation> -... simulation:</p>
<p>-step: 1 level: <Indicate the level of biology of this step.> facts: <Provide a comprehensive overview of facts about the entities and processes you are considering, including facts that are not stated in the query. Attempt to include gene regulation, protein interactions, cell types, tissue functions, and organ functions that might influence the step and its consequences. Avoid repeating any facts that you already provided. Mention facts that might become relevant later. Provide references for all facts you list at the end only using the provided structure and indicate it using [1].> entities: <Enumerate all involved entities such as genes, proteins, cell types, tissues, organs, etc. by their name.> assumptions: <Integrate the stated facts and previous consequences into assumptions about the current step.> consequence: <Generate the most probable consequence, given the facts, assumptions, and any previously generated consequences. Given the level of biology of this step, provide fitting specifics like different types of entities (e.g. small molecules, RNA, DNA, proteins, metabolites, cell types, tissues, organs) and processes (e.g., interactions, pathways, biological processes, biochemical reactions, tissue function, organ function). For example: decreased \<gene X> expression by \<cell type X>, increased \<metabolite X> levels in the blood, changed \<morphology X> in \<tissue X>, increased \<organ X> function, systemic \<phenotype X>, etc.> probability: <The probability of the consequence happening on a scale of 0 to 100.> explanation: <Offer a good explanation as evidence that supports the listed consequence.> novelty: <How novel or unconventional is your reasoning on a scale of 0 to 100.> -step: 2 ... </p>
<p>Supplementary Text 9: Expert evaluation questions</p>
<p>Figure 1 .
1Implementation and testing of a universal biological simulator based on GPT-4. a, Schematic overview of SimulateGPT, a GPT-4 based biological simulator. b, Depiction of the use of GPT-4 in a simulation of Conway's Game of Life. c, Example prompts and output of SimulateGPT. d, Overview of the evaluation methods used to validate SimulateGPT.</p>
<p>Figure 2
2Qualitative and quantitative evaluation of SimulateGPT. a, Expert evaluation of the simulation for four biomedical scenarios, comparing SimulateGPT to conventional GTP-4 based reasoning. b, Performance of SimulateGPT in predicting broadly essential genes in cancer cell lines. c, Performance of SimulateGPT in predicting progression-free survival in patients with colorectal cancer. d, Expert evaluation of simulation results of the sepsis treatment experiment using SimulateGPT versus 'low complexity SimulateGPT'. e-f, Performance of SimulateGPT in predicting progression-free survival in colorectal cancer patients with the low complexity simulation (panel e) and high complexity simulation with a minimum of 5 simulation steps (panel f). Confidence intervals represent standard error. Significance: * = P ≤ 0.05, ** = P ≤ 0.01. *** = P ≤ 0.001.</p>
<p>-
Level: The level of biology addressed in this step (e.g., molecular, cellular, organ, organism) -Facts: An overview of relevant facts, including gene regulation, protein interactions, etc. -Entities: A list of all entities in the step, such as genes, proteins, cell types, tissues, and organs -Assumptions: Integration of stated facts and previous consequences into assumptions about the current step -Consequence: The most probable consequence based on facts, assumptions, and previously generated consequences, including specific entities and processes -Probability: The likelihood of the consequence happening, on a scale of 0 to 100 -Explanation: A well-reasoned explanation supporting the listed consequence -Novelty: A rating of how novel or unconventional the reasoning is, on a scale of 0 to 100</p>
<p>the eight adjacent cells, horizontally, vertically, and diagonally. For each generation, describe how you simulate the new grid in excruciating detail: For each cell in the grid (i, j) indicate all its neighbors with their states, count the number of living ones (just count the number of listed neighbors with state 'live'). Then, state the current state of the current cell of interest and derive its next generation's state following the rules stated above. Simulate each cell as just described in the format (<i,j>)$[[<neighbors>]#COUNT=<num-ber_of_alive_neighbors>]$$$oldstate=<new_state> where <neighbors> is a list of all neighbors and their states, as in "(0,1):dead|alive, ...". Finally, after the new states of all cells have been computed, print the new generation as indicated by the provided initial generation (Generation 0).</p>
<p>": "Van Cutsem E, Cervantes A, Nordlinger B, Arnold D; ESMO Guidelines Working Group. 2014. Ann Oncol. ESMO consensus guidelines for the management of patients with metastatic colorectal cancer." "[2]": "Longley DB, Harkin DP, Johnston PG. 2003. Nat Rev Cancer. 5-fluorouracil: mechanisms of action and clinical strategies." "[3]": "Cunningham D, Humblet Y, Siena S, et al. 2004. N Engl J Med. Cetuximab monotherapy and cetuximab plus irinotecan in irinotecan-refractory metastatic colorectal cancer." "[4]": "Le DT, Uram JN, Wang H, et al. 2015. N Engl J Med. PD-1 Blockade in Tumors with Mismatch-Repair Deficiency." "[5]": "Holohan C, Van Schaeybroeck S, Longley DB, Johnston PG. 2013. Nat Rev Cancer. Cancer drug resistance: an evolving paradigm." "[6]": "Sharma P, Hu-Lieskovan S, Wargo JA, Ribas A. 2017. Cell. Primary, Adaptive, and Acquired Resistance to Cancer Immunotherapy." "[7]": "Manfredi S, Lepage C, Hatem C, Coatmeur O, Faivre J, Bouvier AM. 2006. Br J Cancer. Epidemiology and management of liver metastases from colorectal cancer." "[8]": "Van Cutsem E, Köhne CH, Láng I, et al. 2011. J Clin Oncol. Cetuximab Plus Irinotecan, Fluorouracil, and Leucovorin As First-Line Treatment for Metastatic Colorectal Cancer: Updated Analysis of Overall Survival According to Tumor KRAS and BRAF Mutation Status." "[9]": "Heinemann V, von Weikersthal LF, Decker T, et al. 2014. Lancet Oncol. FOLFIRI plus cetuximab versus FOLFIRI plus bevacizumab as first-line treatment for patients with metastatic colorectal cancer (FIRE-3): a randomised, open-label, phase 3 trial." Supplementary Text 6: Expert-evaluated scenarios</p>
<p>-
The output reflects the consensus in the scientific and clinical community. -The output contains evidence of correct recall of knowledge. -The output contains evidence of incorrect recall of knowledge. -The output contains evidence of correct reasoning steps. -The output contains evidence of incorrect reasoning steps. -The output contains content it shouldn't contain. -The output omits content it shouldn't omit. -The answer contains (novel) concepts that are not typically known in the scientific and clinical community. -I felt qualified to evaluate this topic.</p>
<p>Singhal, K. et al. Large Language Models Encode Clinical Knowledge. arXiv (2022) doi:10.48550/arxiv.2212.13138. original draft, visualization. AN: expertise, validation. FP: expertise, validation. TK: expertise, validation. PS: Investigation, Resources. CB: supervision, funding acquisition, writing -review &amp; editing, MaS: conceptualization, supervision, funding acquisition, project administration, writing -original draft.All relevant data, including full text simulations, expert validation and code to reproduce our results are 
available from the following repository: https://github.com/OpenBioLink/SimulateGPT </p>
<p>A runnable version of SimulateGPT is available through Google Colab (for SimulateGPT to use GPT-4 
securely, the user needs to provide a personal OpenAI API key that allows for GPT-4 access): 
https://colab.research.google.com/github/OpenBioLink/SimulateGPT/blob/main/SimulateGPT.ipynb 
20. 
Meeth, K., Wang, J. X., Micevic, G., Damsky, W. &amp; Bosenberg, M. W. The YUMM lines: a se-
ries of congenic mouse melanoma cell lines with defined genetic alterations. Pigment Cell Mela-
noma Res. 29, 590-597 (2016). </p>
<ol>
<li>
<p>Netea, M. G. et al. Defining trained immunity and its role in health and disease. Nat. Rev. Im-
munol. 20, 375-388 (2020). </p>
</li>
<li>
<p>Hotchkiss, R. S. et al. Sepsis and septic shock. Nat. Rev. Dis. Primers 2, 16045 (2016). </p>
</li>
<li>
<p>Jarczak, D., Kluge, S. &amp; Nierhaus, A. Sepsis-Pathophysiology and Therapeutic Concepts. 
Front Med (Lausanne) 8, 628302 (2021). </p>
</li>
<li>
<p>Leventogiannis, K. et al. Toward personalized immunotherapy in sepsis: The PROVIDE ran-
domized clinical trial. Cell Rep. Med. 3, 100817 (2022). </p>
</li>
<li>
<p>Wang, L.-B. et al. Proteogenomic and metabolomic characterization of human glioblastoma. 
Cancer Cell 39, 509-528.e20 (2021). </p>
</li>
<li>
<p>Mondaca, S. et al. Specific mutations in APC, but not alterations in DNA damage response, 
associate with outcomes of patients with metastatic colorectal cancer. Gastroenterology 159, 
1975-1978.e4 (2020). </p>
</li>
<li>
<p>DepMap, B. DepMap 22Q4 Public. Figshare (2022) doi:10.6084/m9.figshare.21637199.v2. </p>
</li>
<li>
<p>Author contributions </p>
</li>
</ol>
<p>MoS: project administration, investigation, formal analysis, methodology, software, writing -original 
draft, validation. SR: project administration, investigation, formal analysis, methodology, writing -original 
draft, validation, resources. RtH: project administration, investigation, formal analysis, methodology, 
writing -</p>
<p>Trained immunity :
immunitytolerance_LPSA mouse is injected with high-dose LPS. What is the innate immune response upon rechallenge with high-dose LPS after 90 days? Focus on novelty.training_LPSA mouse is injected with low-dose LPS. What is the innate immune response upon rechallenge with high-dose LPS after 90 days? Focus on novelty. training_betaglucan A mouse is injected with beta-glucan. What is the innate immune response upon rechallenge with high-dose LPS after 90 days? Focus on novelty.</p>
<p>. Openai, 10.48550/arxiv.2303.08774GPT-4 Technical ReportOpenAI. GPT-4 Technical Report. arXiv (2023) doi:10.48550/arxiv.2303.08774.</p>
<p>A Chowdhery, 10.48550/arxiv.2204.02311Scaling Language Modeling with Pathways. arXiv (2022). Chowdhery, A. et al. PaLM: Scaling Language Modeling with Pathways. arXiv (2022) doi:10.48550/arxiv.2204.02311.</p>
<p>. V Sanh, 10.48550/arxiv.2110.08207Multitask Prompted Training Enables Zero-Shot Task Generalization. arXivSanh, V. et al. Multitask Prompted Training Enables Zero-Shot Task Generalization. arXiv (2021) doi:10.48550/arxiv.2110.08207.</p>
<p>Attention is all you need. A Vaswani, 10.48550/arxiv.1706.03762Vaswani, A. et al. Attention is all you need. arXiv (2017) doi:10.48550/arxiv.1706.03762.</p>
<p>H Nori, N King, S M Mckinney, D Carignan, E Horvitz, 10.48550/arxiv.2303.13375Capabilities of GPT-4 on Medical Challenge Problems. arXiv. Nori, H., King, N., McKinney, S. M., Carignan, D. &amp; Horvitz, E. Capabilities of GPT-4 on Medi- cal Challenge Problems. arXiv (2023) doi:10.48550/arxiv.2303.13375.</p>
<p>Towards Expert-Level Medical Question Answering with Large Language Models. K Singhal, 10.48550/arxiv.2305.09617Singhal, K. et al. Towards Expert-Level Medical Question Answering with Large Language Models. arXiv (2023) doi:10.48550/arxiv.2305.09617.</p>
<p>Can large language models reason about medical questions?. V Liévin, C E Hother, O Winther, 10.48550/arxiv.2207.08143Liévin, V., Hother, C. E. &amp; Winther, O. Can large language models reason about medical ques- tions? arXiv (2022) doi:10.48550/arxiv.2207.08143.</p>
<p>Emergent autonomous scientific research capabilities of large language models. D A Boiko, R Macknight, G Gomes, 10.48550/arxiv.2304.05332arXivBoiko, D. A., MacKnight, R. &amp; Gomes, G. Emergent autonomous scientific research capabilities of large language models. arXiv (2023) doi:10.48550/arxiv.2304.05332.</p>
<p>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. J Wei, 10.48550/arxiv.2201.11903Wei, J. et al. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. arXiv (2022) doi:10.48550/arxiv.2201.11903.</p>
<p>Causal Reasoning and Large Language Models: Opening a New Frontier for Causality. E Kıcıman, R Ness, A Sharma, C Tan, 10.48550/arxiv.2305.00050Kıcıman, E., Ness, R., Sharma, A. &amp; Tan, C. Causal Reasoning and Large Language Models: Opening a New Frontier for Causality. arXiv (2023) doi:10.48550/arxiv.2305.00050.</p>
<p>The challenges of in silico biology. B Palsson, Nat. Biotechnol. 18Palsson, B. The challenges of in silico biology. Nat. Biotechnol. 18, 1147-1150 (2000).</p>
<p>The virtual physiological human: ten years after. M Viceconti, P Hunter, Annu. Rev. Biomed. Eng. 18Viceconti, M. &amp; Hunter, P. The virtual physiological human: ten years after. Annu. Rev. Bio- med. Eng. 18, 103-123 (2016).</p>
<p>F Pappalardo, G Russo, F M Tshinanu, M Viceconti, silico clinical trials: concepts and early adoptions. 20Pappalardo, F., Russo, G., Tshinanu, F. M. &amp; Viceconti, M. In silico clinical trials: concepts and early adoptions. Brief. Bioinformatics 20, 1699-1708 (2019).</p>
<p>. M Gardner, Mathematical Games. Sci. Am. 223Gardner, M. Mathematical Games. Sci. Am. 223, 120-123 (1970).</p>
<p>It's Hard for Neural Networks To Learn the. J M Springer, G T Kenyon, 10.48550/arxiv.2009.01398Game of Life. arXiv. Springer, J. M. &amp; Kenyon, G. T. It's Hard for Neural Networks To Learn the Game of Life. arXiv (2020) doi:10.48550/arxiv.2009.01398.</p>
<p>G Mialon, 10.48550/ar-xiv.2302.07842Augmented Language Models: a Survey. arXiv. Mialon, G. et al. Augmented Language Models: a Survey. arXiv (2023) doi:10.48550/ar- xiv.2302.07842.</p>
<p>Self-Consistency Improves Chain of Thought Reasoning in Language Models. X Wang, 10.48550/arxiv.2203.11171Wang, X. et al. Self-Consistency Improves Chain of Thought Reasoning in Language Models. arXiv (2022) doi:10.48550/arxiv.2203.11171.</p>
<p>Reflexion: an autonomous agent with dynamic memory and self-reflection. N Shinn, B Labash, A Gopinath, 10.48550/arxiv.2303.11366arXivShinn, N., Labash, B. &amp; Gopinath, A. Reflexion: an autonomous agent with dynamic memory and self-reflection. arXiv (2023) doi:10.48550/arxiv.2303.11366.</p>
<p>Cyanide toxicokinetics: the behavior of cyanide, thiocyanate and 2-amino-2-thiazoline-4-carboxylic acid in multiple animal models. R K Bhandari, J. Anal. Toxicol. 38Bhandari, R. K. et al. Cyanide toxicokinetics: the behavior of cyanide, thiocyanate and 2- amino-2-thiazoline-4-carboxylic acid in multiple animal models. J. Anal. Toxicol. 38, 218-225 (2014).</p>
<p>Glioblastoma survival. Glioblastoma survival:</p>
<p>As final outcome, provide two values, first indicating whether the patient's survival is more likely to be 'increased' or 'decreased' relative to the median overall glioblastoma survival (15 months), and second, the number of months by which you estimate the survival to deviate from that median. Focus on novelty and ignore the inherent uncertainty of this scenario to provide bold estimates based on your knowledge. Identified mutations: PTEN: mutations found EGFR: mutations found MGMT methylation status: high TP53: no mutations found PIK3CA: no mutations found NF1: no mutations found MUC16: no mutations found RB1: no mutations found conclusion: outcome: <Infer the final outcome from the last simulation step.> explanation: <Offer a good explanation as evidence that supports the final outcome. Consider a patient diagnosed with a glioblastoma with genetic alterations, as indicated below. Explore the tumor progression under standard of care with genotype-informed treatment (if applicable). > references: "[1]": "<authors> <year> <journal> <title of the publication>" "[2]": "<authors> <year> <journal> <title of the publication>" …Consider a patient diagnosed with a glioblastoma with genetic alterations, as indicated below. Explore the tumor progression under standard of care with genotype-informed treat- ment (if applicable). As final outcome, provide two values, first indicating whether the patient's survival is more likely to be 'increased' or 'decreased' relative to the median overall glioblastoma survival (15 months), and second, the number of months by which you estimate the survival to deviate from that median. Focus on novelty and ignore the inher- ent uncertainty of this scenario to provide bold estimates based on your knowledge. Identified mutations: PTEN: mutations found EGFR: mutations found MGMT methylation status: high TP53: no mutations found PIK3CA: no mutations found NF1: no mutations found MUC16: no mutations found RB1: no mutations found conclusion: outcome: <Infer the final outcome from the last simulation step.> explanation: <Offer a good explanation as evidence that supports the final outcome.> references: "[1]": "<authors> <year> <journal> <title of the publication>" "[2]": "<authors> <year> <journal> <title of the publication>" …</p>            </div>
        </div>

    </div>
</body>
</html>
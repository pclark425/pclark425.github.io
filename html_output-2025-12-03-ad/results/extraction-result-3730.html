<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3730 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3730</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3730</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-88.html">extraction-schema-88</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-f4bc22fd898250f4668112232324e81943a2aaeb</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/f4bc22fd898250f4668112232324e81943a2aaeb" target="_blank">Probing neural language models for understanding of words of estimative probability</a></p>
                <p><strong>Paper Venue:</strong> STARSEM</p>
                <p><strong>Paper TL;DR:</strong> This study gauges the competency of neural language processing models in accurately capturing the consensual probability level associated with each WEP, and develops a dataset based on WEP-focused probabilistic reasoning to assess if language models can logically process WEP compositions.</p>
                <p><strong>Paper Abstract:</strong> Words of Estimative Probability (WEP) are phrases used to express the plausibility of a statement. Examples include terms like \textit{probably, maybe, likely, doubt, unlikely}, and \textit{impossible}. Surveys have shown that human evaluators tend to agree when assigning numerical probability levels to these WEPs. For instance, the term \textit{highly likely} equates to a median probability of $0.90{\pm}0.08$ according to a survey by \citet{fagen-ulmschneider}.In this study, our focus is to gauge the competency of neural language processing models in accurately capturing the consensual probability level associated with each WEP. Our first approach is utilizing the UNLI dataset \cite{chen-etal-2020-uncertain}, which links premises and hypotheses with their perceived joint probability $p$. From this, we craft prompts in the form: "[\textsc{Premise}]. [\textsc{Wep}], [\textsc{Hypothesis}].” This allows us to evaluate whether language models can predict if the consensual probability level of a WEP aligns closely with $p$.In our second approach, we develop a dataset based on WEP-focused probabilistic reasoning to assess if language models can logically process WEP compositions. For example, given the prompt "[\textsc{EventA}] \textit{is likely}. [\textsc{EventB}] \textit{is impossible}.”, a well-functioning language model should not conclude that [\textsc{EventA$\&$B}] is likely.Through our study, we observe that both tasks present challenges to out-of-the-box English language models. However, we also demonstrate that fine-tuning these models can lead to significant and transferable improvements.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3730.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3730.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-2 (Generative Pretrained Transformer 2)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A causal transformer language model used in this paper to score likelihoods of concatenated premise and hypothesis texts; applied zero-shot to compare valid vs invalid WEP verbalizations by likelihood.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models are unsupervised multitask learners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT2 (base)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Causal Transformer language model (GPT-2 base) with ~127M parameters, pretrained as a next-token predictor on large unlabeled text corpora (Radford et al., 2019).</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_task</strong></td>
                            <td>Estimate the plausibility/likelihood of a hypothesis given a premise by computing text likelihood for verbalizations that incorporate Words of Estimative Probability (WEP); used in WEP-UNLI and WEP-Reasoning tasks to decide whether a WEP verbalization matches an annotated probability p (binary valid/invalid verbalization).</td>
                        </tr>
                        <tr>
                            <td><strong>method_of_probability_estimation</strong></td>
                            <td>Zero-shot scoring via text likelihood (concatenate PREMISE and verbalized HYPOTHESIS), using average likelihood per token and length-normalization; also explored calibrated scores (dividing combined score by score of verbalized hypothesis alone) as described in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>WEP-UNLI (derived from UNLI / SNLI with annotated joint probability p) and synthetic WEP-Reasoning datasets (1-hop and 2-hop) built from bAbI fact templates and ProbLog reasoning to compute ground-truth hypothesis probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Binary classification accuracy (valid vs invalid verbalization). Reported test accuracies (mean ± std): WEP-Reasoning (1 hop): 50.1 ± 0.0; WEP-Reasoning (2 hops): 50.0 ± 0.0; WEP-UNLI: 45.6 ± 0.0.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Compared to human baseline (accuracy ~97.0/93.5/89.5 on 1-hop/2-hop/UNLI). GPT2 performs at or near chance on the constructed WEP tasks and substantially worse than humans and some fine-tuned models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Zero-shot causal likelihood from GPT2 failed to distinguish valid vs invalid WEP verbalizations (near-chance accuracy); raw likelihood is sensitive to length and tokenization so authors applied length-normalization and calibration but results remained poor. The model was not fine-tuned on WEP tasks in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_findings</strong></td>
                            <td>Off-the-shelf GPT2 (base) provides little useful signal for mapping WEP verbalizations to their consensual numerical probabilities or reasoning about composed probabilistic statements; fine-tuning (on RoBERTa) was required to achieve high performance in these tasks. The paper does not attempt forecasting real-world events or scientific discoveries, focusing instead on mapping textual WEP to probabilities and compositional probabilistic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Probing neural language models for understanding of words of estimative probability', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3730.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3730.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoBERTa</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoBERTa (Robustly optimized BERT pretraining approach)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A masked-language transformer encoder used to score masked-likelihood (pseudo log-likelihood) of premise+hypothesis pairs and evaluated zero-shot and after fine-tuning on WEP datasets; shows better zero-shot performance than GPT2 and strong gains when fine-tuned.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>RoBERTa: A Robustly Optimized BERT Pretraining Approach</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa (base)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A masked-language Transformer encoder (RoBERTa-base) with ~123M parameters pretrained with robust optimization of BERT-style objectives (Liu et al., 2019); used both as a likelihood scorer (pseudo log-likelihood) and as a base for fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_task</strong></td>
                            <td>Same WEP verbalization validity (binary) tasks: decide whether a WEP-based verbalization correctly matches an underlying numerical probability p (WEP-UNLI) and assess compositional probabilistic reasoning (WEP-Reasoning 1-hop and 2-hop).</td>
                        </tr>
                        <tr>
                            <td><strong>method_of_probability_estimation</strong></td>
                            <td>Zero-shot scoring using masked-language-model-based pseudo-likelihoods, with length-normalization and calibration applied; additionally used as the base model for fine-tuning in a multiple-choice setup (predict logits for valid vs invalid verbalization and optimize cross-entropy).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>WEP-UNLI (derived from UNLI / SNLI) and synthetic WEP-Reasoning (1-hop and 2-hop) datasets constructed from bAbI fact templates with ProbLog-derived ground-truth probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Binary classification accuracy (valid vs invalid verbalization). Zero-shot RoBERTa likelihood reported: WEP-Reasoning (1 hop): 63.4 ± 0.0; WEP-Reasoning (2 hops): 63.2 ± 0.0; WEP-UNLI: 53.2 ± 0.0. After fine-tuning (multiple variants) RoBERTa achieves much higher accuracies (see next entry for fine-tuned variants).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Zero-shot RoBERTa outperforms GPT2 zero-shot and RoBERTa-MNLI zero-shot on some tasks but still trails human baseline (human: ~97.0/93.5/89.5). Fine-tuned RoBERTa models substantially outperform zero-shot models on corresponding tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Although better than GPT2 zero-shot, RoBERTa zero-shot still shows limited understanding of WEP semantics and composition. Performance depends on scoring normalization and calibration; without fine-tuning, RoBERTa struggles particularly on UNLI-derived examples.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_findings</strong></td>
                            <td>RoBERTa provides a stronger zero-shot signal than GPT2 for mapping WEP verbalizations to probabilities, but fine-tuning on targeted WEP datasets yields large and transferable improvements. The study shows that supervision (fine-tuning) enables the model to learn WEP-consensus mappings and compositional probabilistic reasoning that are not present in pretraining alone.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Probing neural language models for understanding of words of estimative probability', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3730.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3730.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoBERTa-MNLI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoBERTa fine-tuned on MNLI (natural language inference)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A RoBERTa model fine-tuned on the MNLI entailment dataset, used zero-shot as an entailment scorer to evaluate whether WEP verbalizations are more plausible (entailment) than distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A broad-coverage challenge corpus for sentence understanding through inference</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa-MNLI</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>RoBERTa-base model fine-tuned on the Multi-Genre Natural Language Inference (MNLI) dataset to predict entailment/contradiction/neutral labels; used here zero-shot to score plausibility of premise-hypothesis pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_task</strong></td>
                            <td>Assess whether a WEP-verbalized hypothesis is more plausible/entailed given a premise than an incorrect (distractor) WEP verbalization—used as a proxy for mapping WEP to consensual probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>method_of_probability_estimation</strong></td>
                            <td>Zero-shot entailment scoring (use entailment likelihoods/logits from the MNLI fine-tuned model) to compare valid vs invalid WEP verbalizations.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>WEP-UNLI and WEP-Reasoning (1-hop and 2-hop) evaluation sets constructed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Binary classification accuracy (valid vs invalid verbalization). Reported test accuracies (mean ± std): WEP-Reasoning (1 hop): 49.2 ± 5.4; WEP-Reasoning (2 hops): 41.7 ± 4.2; WEP-UNLI: 54.6 ± 3.7.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Performance is near chance on compositional reasoning tasks and only marginally better than chance on UNLI; it underperforms human baseline and in many cases underperforms the RoBERTa likelihood zero-shot scorer.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>MNLI fine-tuning does not appear to transfer strong WEP composition or probability-mapping abilities; MNLI contains some instances of 'probably' but lacks structured WEP composition coverage, limiting zero-shot transfer to WEP tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_findings</strong></td>
                            <td>An NLI-fine-tuned encoder (RoBERTa-MNLI) is not sufficient by itself to capture consensual WEP probability judgments or compositional probabilistic reasoning in these probes; specialized fine-tuning on WEP datasets yields much larger gains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Probing neural language models for understanding of words of estimative probability', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3730.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3730.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoBERTa+WEP-finetuned</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoBERTa fine-tuned on the WEP probes (WEP-Reasoning / WEP-UNLI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>RoBERTa-base models fine-tuned on the paper's WEP-targeted datasets (WEP-Reasoning 1-hop, WEP-Reasoning 2-hop, and WEP-UNLI) in a multiple-choice setup; fine-tuning yields substantial improvements and some cross-task transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa (fine-tuned on WEP-Reasoning or WEP-UNLI)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>RoBERTa-base (~123M) further fine-tuned for 3 epochs with sequence length 256, lr=2e-5, batch size 16 in a multiple-choice classification setup where the model predicts which of two verbalizations (valid vs invalid) is correct.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_task</strong></td>
                            <td>Binary classification (valid vs invalid WEP verbalization) across three tasks: WEP-Reasoning (1-hop), WEP-Reasoning (2-hop), and WEP-UNLI; also evaluated for cross-dataset transfer (fine-tune on one probe, test on others).</td>
                        </tr>
                        <tr>
                            <td><strong>method_of_probability_estimation</strong></td>
                            <td>Fine-tuned discriminative multiple-choice classification: present PREMISE + verbalized HYPOTHESIS (valid or distractor) and train model to score the valid verbalization higher; scoring uses logits and softmax over the two choices. Training uses length-normalization and calibration methods found effective on validation.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>WEP-Reasoning synthetic datasets (1-hop and 2-hop) generated from bAbI facts and ProbLog-inferred probabilities (5k examples, 10%/10% validation/test splits) and WEP-UNLI derived from UNLI/SNLI pairs (55k train, 3k/3k val/test).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Binary classification accuracy (test). Reported test accuracies (mean ± std) depending on fine-tuning target and test set: - RoBERTa+WEP-Reasoning (1 hop) fine-tuning: WEP-Reasoning 1-hop 97.8 ± 0.4; WEP-Reasoning 2-hop 81.6 ± 1.3; WEP-UNLI 61.2 ± 0.4. - RoBERTa+WEP-Reasoning (2 hops) fine-tuning: WEP-Reasoning 1-hop 85.0 ± 1.6; WEP-Reasoning 2-hop 91.1 ± 0.1; WEP-UNLI 62.3 ± 1.7. - RoBERTa+WEP-UNLI fine-tuning: WEP-Reasoning 1-hop 62.4 ± 0.4; WEP-Reasoning 2-hop 64.3 ± 0.1; WEP-UNLI 84.4 ± 0.5.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Fine-tuned RoBERTa models reach or exceed human baseline on the specific WEP-Reasoning 1-hop case (97.8 vs human 97.0) and substantially improve over zero-shot RoBERTa and GPT2. Fine-tuning on one probe transfers partially to other probes but best performance is achieved on the probe used for fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Fine-tuning is required to obtain good performance; models still show reduced accuracy when evaluated on probing tasks different from their fine-tuning target (transfer is partial). The study focuses on mapping WEP and compositional reasoning in synthetic settings; it does not attempt real-world forecasting of scientific discoveries or events. Also, construction choices (e.g., sampling WEP medians, restricting fact overlap) simplify the reasoning setting and may not reflect all naturalistic complexities.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_findings</strong></td>
                            <td>Supervised fine-tuning on targeted WEP datasets yields large and in several cases transferable improvements in mapping WEP to consensual probabilities and in compositional probabilistic reasoning; this suggests WEP handling can be improved by incorporating such supervision into text-encoder training. However, pretraining alone (even with large LMs) does not reliably encode consensual numerical meanings of WEP or their logical compositions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Probing neural language models for understanding of words of estimative probability', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>Uncertain natural language inference <em>(Rating: 2)</em></li>
                <li>Mapping probability word problems to executable representations <em>(Rating: 2)</em></li>
                <li>Solving probability problems in natural language <em>(Rating: 1)</em></li>
                <li>Problog: A probabilistic prolog and its application in link discovery <em>(Rating: 1)</em></li>
                <li>Perception of probability words <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3730",
    "paper_id": "paper-f4bc22fd898250f4668112232324e81943a2aaeb",
    "extraction_schema_id": "extraction-schema-88",
    "extracted_data": [
        {
            "name_short": "GPT2",
            "name_full": "GPT-2 (Generative Pretrained Transformer 2)",
            "brief_description": "A causal transformer language model used in this paper to score likelihoods of concatenated premise and hypothesis texts; applied zero-shot to compare valid vs invalid WEP verbalizations by likelihood.",
            "citation_title": "Language models are unsupervised multitask learners",
            "mention_or_use": "use",
            "model_name": "GPT2 (base)",
            "model_description": "Causal Transformer language model (GPT-2 base) with ~127M parameters, pretrained as a next-token predictor on large unlabeled text corpora (Radford et al., 2019).",
            "prediction_task": "Estimate the plausibility/likelihood of a hypothesis given a premise by computing text likelihood for verbalizations that incorporate Words of Estimative Probability (WEP); used in WEP-UNLI and WEP-Reasoning tasks to decide whether a WEP verbalization matches an annotated probability p (binary valid/invalid verbalization).",
            "method_of_probability_estimation": "Zero-shot scoring via text likelihood (concatenate PREMISE and verbalized HYPOTHESIS), using average likelihood per token and length-normalization; also explored calibrated scores (dividing combined score by score of verbalized hypothesis alone) as described in the paper.",
            "dataset_or_benchmark": "WEP-UNLI (derived from UNLI / SNLI with annotated joint probability p) and synthetic WEP-Reasoning datasets (1-hop and 2-hop) built from bAbI fact templates and ProbLog reasoning to compute ground-truth hypothesis probabilities.",
            "performance_metrics": "Binary classification accuracy (valid vs invalid verbalization). Reported test accuracies (mean ± std): WEP-Reasoning (1 hop): 50.1 ± 0.0; WEP-Reasoning (2 hops): 50.0 ± 0.0; WEP-UNLI: 45.6 ± 0.0.",
            "comparison_to_baselines": "Compared to human baseline (accuracy ~97.0/93.5/89.5 on 1-hop/2-hop/UNLI). GPT2 performs at or near chance on the constructed WEP tasks and substantially worse than humans and some fine-tuned models.",
            "limitations_or_challenges": "Zero-shot causal likelihood from GPT2 failed to distinguish valid vs invalid WEP verbalizations (near-chance accuracy); raw likelihood is sensitive to length and tokenization so authors applied length-normalization and calibration but results remained poor. The model was not fine-tuned on WEP tasks in the experiments.",
            "notable_findings": "Off-the-shelf GPT2 (base) provides little useful signal for mapping WEP verbalizations to their consensual numerical probabilities or reasoning about composed probabilistic statements; fine-tuning (on RoBERTa) was required to achieve high performance in these tasks. The paper does not attempt forecasting real-world events or scientific discoveries, focusing instead on mapping textual WEP to probabilities and compositional probabilistic reasoning.",
            "uuid": "e3730.0",
            "source_info": {
                "paper_title": "Probing neural language models for understanding of words of estimative probability",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "RoBERTa",
            "name_full": "RoBERTa (Robustly optimized BERT pretraining approach)",
            "brief_description": "A masked-language transformer encoder used to score masked-likelihood (pseudo log-likelihood) of premise+hypothesis pairs and evaluated zero-shot and after fine-tuning on WEP datasets; shows better zero-shot performance than GPT2 and strong gains when fine-tuned.",
            "citation_title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
            "mention_or_use": "use",
            "model_name": "RoBERTa (base)",
            "model_description": "A masked-language Transformer encoder (RoBERTa-base) with ~123M parameters pretrained with robust optimization of BERT-style objectives (Liu et al., 2019); used both as a likelihood scorer (pseudo log-likelihood) and as a base for fine-tuning.",
            "prediction_task": "Same WEP verbalization validity (binary) tasks: decide whether a WEP-based verbalization correctly matches an underlying numerical probability p (WEP-UNLI) and assess compositional probabilistic reasoning (WEP-Reasoning 1-hop and 2-hop).",
            "method_of_probability_estimation": "Zero-shot scoring using masked-language-model-based pseudo-likelihoods, with length-normalization and calibration applied; additionally used as the base model for fine-tuning in a multiple-choice setup (predict logits for valid vs invalid verbalization and optimize cross-entropy).",
            "dataset_or_benchmark": "WEP-UNLI (derived from UNLI / SNLI) and synthetic WEP-Reasoning (1-hop and 2-hop) datasets constructed from bAbI fact templates with ProbLog-derived ground-truth probabilities.",
            "performance_metrics": "Binary classification accuracy (valid vs invalid verbalization). Zero-shot RoBERTa likelihood reported: WEP-Reasoning (1 hop): 63.4 ± 0.0; WEP-Reasoning (2 hops): 63.2 ± 0.0; WEP-UNLI: 53.2 ± 0.0. After fine-tuning (multiple variants) RoBERTa achieves much higher accuracies (see next entry for fine-tuned variants).",
            "comparison_to_baselines": "Zero-shot RoBERTa outperforms GPT2 zero-shot and RoBERTa-MNLI zero-shot on some tasks but still trails human baseline (human: ~97.0/93.5/89.5). Fine-tuned RoBERTa models substantially outperform zero-shot models on corresponding tasks.",
            "limitations_or_challenges": "Although better than GPT2 zero-shot, RoBERTa zero-shot still shows limited understanding of WEP semantics and composition. Performance depends on scoring normalization and calibration; without fine-tuning, RoBERTa struggles particularly on UNLI-derived examples.",
            "notable_findings": "RoBERTa provides a stronger zero-shot signal than GPT2 for mapping WEP verbalizations to probabilities, but fine-tuning on targeted WEP datasets yields large and transferable improvements. The study shows that supervision (fine-tuning) enables the model to learn WEP-consensus mappings and compositional probabilistic reasoning that are not present in pretraining alone.",
            "uuid": "e3730.1",
            "source_info": {
                "paper_title": "Probing neural language models for understanding of words of estimative probability",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "RoBERTa-MNLI",
            "name_full": "RoBERTa fine-tuned on MNLI (natural language inference)",
            "brief_description": "A RoBERTa model fine-tuned on the MNLI entailment dataset, used zero-shot as an entailment scorer to evaluate whether WEP verbalizations are more plausible (entailment) than distractors.",
            "citation_title": "A broad-coverage challenge corpus for sentence understanding through inference",
            "mention_or_use": "use",
            "model_name": "RoBERTa-MNLI",
            "model_description": "RoBERTa-base model fine-tuned on the Multi-Genre Natural Language Inference (MNLI) dataset to predict entailment/contradiction/neutral labels; used here zero-shot to score plausibility of premise-hypothesis pairs.",
            "prediction_task": "Assess whether a WEP-verbalized hypothesis is more plausible/entailed given a premise than an incorrect (distractor) WEP verbalization—used as a proxy for mapping WEP to consensual probabilities.",
            "method_of_probability_estimation": "Zero-shot entailment scoring (use entailment likelihoods/logits from the MNLI fine-tuned model) to compare valid vs invalid WEP verbalizations.",
            "dataset_or_benchmark": "WEP-UNLI and WEP-Reasoning (1-hop and 2-hop) evaluation sets constructed in this paper.",
            "performance_metrics": "Binary classification accuracy (valid vs invalid verbalization). Reported test accuracies (mean ± std): WEP-Reasoning (1 hop): 49.2 ± 5.4; WEP-Reasoning (2 hops): 41.7 ± 4.2; WEP-UNLI: 54.6 ± 3.7.",
            "comparison_to_baselines": "Performance is near chance on compositional reasoning tasks and only marginally better than chance on UNLI; it underperforms human baseline and in many cases underperforms the RoBERTa likelihood zero-shot scorer.",
            "limitations_or_challenges": "MNLI fine-tuning does not appear to transfer strong WEP composition or probability-mapping abilities; MNLI contains some instances of 'probably' but lacks structured WEP composition coverage, limiting zero-shot transfer to WEP tasks.",
            "notable_findings": "An NLI-fine-tuned encoder (RoBERTa-MNLI) is not sufficient by itself to capture consensual WEP probability judgments or compositional probabilistic reasoning in these probes; specialized fine-tuning on WEP datasets yields much larger gains.",
            "uuid": "e3730.2",
            "source_info": {
                "paper_title": "Probing neural language models for understanding of words of estimative probability",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "RoBERTa+WEP-finetuned",
            "name_full": "RoBERTa fine-tuned on the WEP probes (WEP-Reasoning / WEP-UNLI)",
            "brief_description": "RoBERTa-base models fine-tuned on the paper's WEP-targeted datasets (WEP-Reasoning 1-hop, WEP-Reasoning 2-hop, and WEP-UNLI) in a multiple-choice setup; fine-tuning yields substantial improvements and some cross-task transfer.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "RoBERTa (fine-tuned on WEP-Reasoning or WEP-UNLI)",
            "model_description": "RoBERTa-base (~123M) further fine-tuned for 3 epochs with sequence length 256, lr=2e-5, batch size 16 in a multiple-choice classification setup where the model predicts which of two verbalizations (valid vs invalid) is correct.",
            "prediction_task": "Binary classification (valid vs invalid WEP verbalization) across three tasks: WEP-Reasoning (1-hop), WEP-Reasoning (2-hop), and WEP-UNLI; also evaluated for cross-dataset transfer (fine-tune on one probe, test on others).",
            "method_of_probability_estimation": "Fine-tuned discriminative multiple-choice classification: present PREMISE + verbalized HYPOTHESIS (valid or distractor) and train model to score the valid verbalization higher; scoring uses logits and softmax over the two choices. Training uses length-normalization and calibration methods found effective on validation.",
            "dataset_or_benchmark": "WEP-Reasoning synthetic datasets (1-hop and 2-hop) generated from bAbI facts and ProbLog-inferred probabilities (5k examples, 10%/10% validation/test splits) and WEP-UNLI derived from UNLI/SNLI pairs (55k train, 3k/3k val/test).",
            "performance_metrics": "Binary classification accuracy (test). Reported test accuracies (mean ± std) depending on fine-tuning target and test set: - RoBERTa+WEP-Reasoning (1 hop) fine-tuning: WEP-Reasoning 1-hop 97.8 ± 0.4; WEP-Reasoning 2-hop 81.6 ± 1.3; WEP-UNLI 61.2 ± 0.4. - RoBERTa+WEP-Reasoning (2 hops) fine-tuning: WEP-Reasoning 1-hop 85.0 ± 1.6; WEP-Reasoning 2-hop 91.1 ± 0.1; WEP-UNLI 62.3 ± 1.7. - RoBERTa+WEP-UNLI fine-tuning: WEP-Reasoning 1-hop 62.4 ± 0.4; WEP-Reasoning 2-hop 64.3 ± 0.1; WEP-UNLI 84.4 ± 0.5.",
            "comparison_to_baselines": "Fine-tuned RoBERTa models reach or exceed human baseline on the specific WEP-Reasoning 1-hop case (97.8 vs human 97.0) and substantially improve over zero-shot RoBERTa and GPT2. Fine-tuning on one probe transfers partially to other probes but best performance is achieved on the probe used for fine-tuning.",
            "limitations_or_challenges": "Fine-tuning is required to obtain good performance; models still show reduced accuracy when evaluated on probing tasks different from their fine-tuning target (transfer is partial). The study focuses on mapping WEP and compositional reasoning in synthetic settings; it does not attempt real-world forecasting of scientific discoveries or events. Also, construction choices (e.g., sampling WEP medians, restricting fact overlap) simplify the reasoning setting and may not reflect all naturalistic complexities.",
            "notable_findings": "Supervised fine-tuning on targeted WEP datasets yields large and in several cases transferable improvements in mapping WEP to consensual probabilities and in compositional probabilistic reasoning; this suggests WEP handling can be improved by incorporating such supervision into text-encoder training. However, pretraining alone (even with large LMs) does not reliably encode consensual numerical meanings of WEP or their logical compositions.",
            "uuid": "e3730.3",
            "source_info": {
                "paper_title": "Probing neural language models for understanding of words of estimative probability",
                "publication_date_yy_mm": "2022-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2
        },
        {
            "paper_title": "Uncertain natural language inference",
            "rating": 2
        },
        {
            "paper_title": "Mapping probability word problems to executable representations",
            "rating": 2
        },
        {
            "paper_title": "Solving probability problems in natural language",
            "rating": 1
        },
        {
            "paper_title": "Problog: A probabilistic prolog and its application in link discovery",
            "rating": 1
        },
        {
            "paper_title": "Perception of probability words",
            "rating": 1
        }
    ],
    "cost": 0.0116807,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Probing neural language models for understanding of words of estimative probability</h1>
<p>Damien Sileo ${ }^{1}$ and Marie-Francine Moens ${ }^{2}$<br>${ }^{1}$ Univ. Lille, Inria, CNRS, Centrale Lille, UMR 9189 - CRIStAL, F-59000 Lille, France<br>${ }^{2}$ Department of Computer Science, KU Leuven, Belgium<br>damien.sileo@inria.fr</p>
<h4>Abstract</h4>
<p>Words of Estimative Probability (WEP) are phrases used to express the plausibility of a statement. Examples include terms like probably, maybe, likely, doubt, unlikely, and impossible. Surveys have shown that human evaluators tend to agree when assigning numerical probability levels to these WEPs. For instance, the term highly likely equates to a median probability of $0.90 \pm 0.08$ according to a survey by Fagen-Ulmschneider (2015). In this study, our focus is to gauge the competency of neural language processing models in accurately capturing the consensual probability level associated with each WEP. Our first approach is utilizing the UNLI dataset (Chen et al., 2020), which links premises and hypotheses with their perceived joint probability $p$. From this, we craft prompts in the form: "[PREMISE]. [WEP], [HYPOTHESIS]." This allows us to evaluate whether language models can predict if the consensual probability level of a WEP aligns closely with $p$. In our second approach, we develop a dataset based on WEP-focused probabilistic reasoning to assess if language models can logically process WEP compositions. For example, given the prompt "[EVENTA] is likely. [EVENTB] is impossible.", a wellfunctioning language model should not conclude that [EVENTA\&amp;B] is likely. Through our study, we observe that both tasks present challenges to out-of-the-box English language models. However, we also demonstrate that fine-tuning these models can lead to significant and transferable improvements.</p>
<h2>1 Introduction</h2>
<p>Expression of uncertainty is an important part of communication. Formal statistics are the rigorous way to quantify uncertainty but do not fit all communication styles. Words of estimative probability (WEP) such as maybe and believe are adverbs or verbs that are informal alternatives. Kent (1964) noted the importance of clarifying WEP meaning
for intelligence analysis in the Central Intelligence Agency, and provided guidelines for mapping WEP to numerical probabilities. Several studies then measured the human perceptions of probability words and discovered some agreement with Kent (1964)'s guidelines. In this work, we use the scale derived from a survey (Fagen-Ulmschneider, 2015), which is the largest and most recent WEP perception survey available. 123 participants were asked to label WEP with numerical probabilities. We use the median of the participant answers to assign a consensual value to each WEP. Associated probabilities for the 19 WEP we use are available in Appendix A, table 2.</p>
<p>Here, we assess whether neural language models learn the consensual probability judgment of WEP from language modeling pretraining. We develop datasets and a methodology to probe neural language model understanding of WEP. The first dataset leverages previously annotated probability scores between a premise and a hypothesis, in order to measure a language model's ability to capture the agreement between numerical probabilities and WEP-expressed probabilities. The second dataset is based on compositions of facts with WEP-expressed probabilities, and measures verbal probabilistic reasoning in language models.</p>
<p>Our contributions are as follows: (i) two datasets and methods to measure understanding of WEP; and (ii) evaluation of the ability of neural language models (GPT2, RoBERTa-trained on MNLI) to tackle WEP-related problems, showing that off-the-shelf models are very little influenced by them, even though fine-tuning on our constructed datasets quickly leads to high accuracies. The code and generated datasets are publicly available ${ }^{1}$</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>2 Related work</h2>
<p>Our work probes a particular aspect of language understanding. We do not analyze the inside of the models (Rogers et al., 2020). We focus on the models' ability to perform controlled tasks (Naik et al., 2018; Richardson et al., 2020) involving WEP. WEP were studied in the context of intelligence analysis and linguistics, our work is the first to look at them through natural language processing (NLP) models. Our study also pertains to NLP analyses of logical reasoning and probability problems, and to uncertainty in natural language inference tasks.</p>
<p>Linguistics study of WEP Kent (1964)'s seminal work was the first to link WEP and numerical probability estimates, with intelligence analysis motivations (Dhami and Mandel, 2021) and a prescriptivist approach. This inspired further quantifications of human perceptions of WEP, in the context of medical reports (O'Brien, 1989; Ott, 2021) and weather reports (Lenhardt et al., 2020). FagenUlmschneider (2015) proposed the largest survey up to date with 123 participants about generaldomain WEP perception.</p>
<p>Logical and probabilistic reasoning Another strand of work probes NLP text encoders capabilities, notably reasoning abilities. Weston et al. (2015) probed understanding of specific problems like negation, spatial and temporal reasoning with the bAbI dataset. Richardson et al. (2020) probe understanding of first-order logic reasoning, Sileo and Lernould (2023) probe epistemic logic reasoning. Our work is the first to address probabilistic logic, alongside Dries et al. (2017); Suster et al. (2021) who construct a dataset of natural language probability problems, e.g., "A bag has 4 white and 8 blue marbles. You pull out one marble and it is blue. You pull out another marble, what is the probability of it being white?". They also rely on the ProbLog solver (De Raedt et al., 2007), but focus on numeric probability problems. By contrast, our work targets WEP, and textual probabilistic logical reasoning.</p>
<p>Natural language inference, uncertainty, modality, evidentiality Uncertainty was also studied in the context of natural language inference tasks. Zhou et al. (2022) study the disagreement across annotators when labeling entailment relationships. Zhang et al. (2017) annotate graded entailment with 5 probability levels, and the UNLI dataset (Chen
et al., 2020) go further by annotating numerical probabilities. Our work also pertains to the study of modality (Palmer, 1992; Saurí et al., 2006) and more particularly evidentiality (Su et al., 2010), but where previous work focused on WEP.</p>
<h2>3 Probing WEP understanding</h2>
<h3>3.1 Verbalization and distractor generation</h3>
<p>Our goal is to measure the understanding of WEP. One requirement of WEP understanding is capturing the consensual probability level. To test that, we use contexts (PREMISE) paired with a conclusions (HYPOTHESIS). The likelihood of a conclusion, $p$, depends on the associated context. One example from UNLI (Chen et al., 2020), which annotates that, is (A man in a white shirt taking a picture, A man takes a picture, 1.0).</p>
<p>We convert a triplet (PremiSE, Hypothesis, $p$ ) to the following verbalization:</p>
<p>$$
\text { PREMISE. } T_{p}(\text { HypothESIS })
$$</p>
<p>where $T_{p}$ is a text template assigned to the probability $p$. To select a template, we find the WEP whose associated median probability (see table 2) is the closest to $p$. We then use handcrafted templates to construct a modal sentence from the selected WEP and the hypothesis, e.g., "It is certain that a man takes a picture". Table 3 in appendix B displays the templates that we associate with each WEP.</p>
<p>We also generate an invalid verbalization by randomly selecting an incorrect WEP (a WEP whose consensual probability differs from $p$ by at least $40 \%)^{2}$, e.g., It is unlikely that a man takes a picture. We hypothesize that language models and entailment recognition models should give a higher score (respectively likelihood and entailment probability) to the correct valid verbalization than to the invalid verbalization of $p$.</p>
<h3>3.2 WEP-UNLI: probability/WEP matching</h3>
<p>The UNLI dataset annotates (PREMISE, HYPOTHESIS) pairs from the SNLI dataset (Bowman et al., 2015) with joint probability scores $p$, totaling 55 k training examples, $3 \mathrm{k} / 3 \mathrm{k}$ validation/test examples. We use these examples to generate WEPunderstanding dataset with verbalization validity prediction as shown in the previous subsection.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: WEP-reasoning task constructions, with 2 hops. We sample randomly concrete facts $f a c t_{i}$ and probabilities $p_{i}$ then build modal sentences with verbalization templates. We randomly sample logical operators to compose the modal sentences from the previous rounds to construct a premise, then a hypothesis, and we use a probabilistic soft logic solver to compute the hypothesis probability. We then correctly and incorrectly verbalize this probability. This process generates data for the task of probability verbalization validity. 1 hop reasoning skips the second round: fact 7 and fact 8 are sampled from ${$ factA, factB, factC $}$</p>
<h3>3.3 WEP-Reasoning: WEP compositions</h3>
<p>Here, our goal is to assess models' ability to reason over combinations of probabilistic statements. We construct synthetic (PREMISE, HYPOTHESIS, $p$ ) examples from random factoids extracted from the bAbI dataset (Weston et al., 2015). Figure 1 illustrates the construction of WEP-reasoning examples:</p>
<p>We randomly sample initial facts and associated probability levels, and we verbalize them with the previously mentioned templates from Table 3 (Round 1). We further compose them with randomly sampled logical operators (and, or, xor). We then generate a hypothesis with logical combinations of the previous round. Finally, we feed the constructed premise and hypothesis to a probabilistic soft reasoning engine in order to derive the likelihood of the hypothesis given the premise. We rely on the ProbLog (De Raedt et al., 2007) reasoner which implements Dantsin (1992) semantics.</p>
<p>To evaluate different complexities of reasoning, we propose two variants: 2-hop reasoning, where facts in Round 2 combine facts from Round 1, and the final hypothesis combines facts from Round 2. and 1-hop reasoning where facts from the hypothesis combine Round 1 facts (Round 2 is skipped).</p>
<p>Since we want to sample more than two facts and we cannot a priori use text from the UNLI dataset,
because UNLI only provides entailment likelihood for specific pairs. Combining several sentences could cause unaccounted interference. Therefore, we sample subject/verb/object factoids from the bAbI (Weston et al., 2015) datasets instead, which is built with handwritten arbitrary factoids such as John went to the kitchen. To sample multiple factoids, we prevent any overlap of concepts (verb, subject, object) between any pair of facts to make the facts independent of one another.</p>
<p>We sample probability levels from the list of medians of all WEP to prevent sampling the levels that too distant from a known WEP. When we assign a WEP to a probability level, we assume that the correct semantics is the consensual one, but humans differs slightly from this consensus. Still, when adding random perturbations of $20 \%$ to sampled $p_{1 \ldots 6}$, the hypothesis probability is perturbed by less than $40 \%$ for $98 \%$ of examples.</p>
<p>We generate 5 k examples using the template depicted in Figure 1, and use $10 \% / 10 \%$ of the data for the validation/test splits. Appendix C shows the distribution of correct WEP for each dataset.</p>
<h2>4 Experiments</h2>
<p>We conduct verbalization validity prediction (binary classification task of WEP correctness detection between two candidates) under two settings.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">WEP-Reasoning (1 hop)</th>
<th style="text-align: left;">WEP-Reasoning (2 hops)</th>
<th style="text-align: left;">WEP-UNLI</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Chance</td>
<td style="text-align: left;">50.0</td>
<td style="text-align: left;">50.0</td>
<td style="text-align: left;">50.0</td>
</tr>
<tr>
<td style="text-align: left;">Human baseline</td>
<td style="text-align: left;">$97.0 \pm 1.0$</td>
<td style="text-align: left;">$\mathbf{9 3 . 5 \pm 1 . 5}$</td>
<td style="text-align: left;">$\mathbf{8 9 . 5 \pm 2 . 5}$</td>
</tr>
<tr>
<td style="text-align: left;">GPT2 likelihood zero-shot</td>
<td style="text-align: left;">$50.1 \pm 0.0$</td>
<td style="text-align: left;">$50.0 \pm 0.0$</td>
<td style="text-align: left;">$45.6 \pm 0.0$</td>
</tr>
<tr>
<td style="text-align: left;">RoBERTa likelihood zero-shot</td>
<td style="text-align: left;">$63.4 \pm 0.0$</td>
<td style="text-align: left;">$63.2 \pm 0.0$</td>
<td style="text-align: left;">$53.2 \pm 0.0$</td>
</tr>
<tr>
<td style="text-align: left;">RoBERTa-MNLI zero-shot</td>
<td style="text-align: left;">$49.2 \pm 5.4$</td>
<td style="text-align: left;">$41.7 \pm 4.2$</td>
<td style="text-align: left;">$54.6 \pm 3.7$</td>
</tr>
<tr>
<td style="text-align: left;">RoBERTa+WEP-Reasoning (1 hop) fine-tuning</td>
<td style="text-align: left;">$\mathbf{9 7 . 8 \pm 0 . 4}$</td>
<td style="text-align: left;">$81.6 \pm 1.3$</td>
<td style="text-align: left;">$61.2 \pm 0.4$</td>
</tr>
<tr>
<td style="text-align: left;">RoBERTa+WEP-Reasoning (2 hops) fine-tuning</td>
<td style="text-align: left;">$85.0 \pm 1.6$</td>
<td style="text-align: left;">$\mathbf{9 1 . 1 \pm 0 . 1}$</td>
<td style="text-align: left;">$62.3 \pm 1.7$</td>
</tr>
<tr>
<td style="text-align: left;">RoBERTa+WEP-UNLI fine-tuning</td>
<td style="text-align: left;">$62.4 \pm 0.4$</td>
<td style="text-align: left;">$64.3 \pm 0.1$</td>
<td style="text-align: left;">$\mathbf{8 4 . 4 \pm 0 . 5}$</td>
</tr>
</tbody>
</table>
<p>Table 1: Test accuracy percentage of different models over the 3 WEP-understanding tasks. The last three rows display the accuracy when fine-tuning on each task, and transferability of the fine-tuned model outside the diagonal.</p>
<h3>4.1 Zero-shot models</h3>
<p>We use off-the-shelf language models to assign likelihood scores to a context and its conclusion. We evaluate the rate at which valid verbalization is scored higher than invalid verbalization. We refine the scores by also considering the average likelihood per token (Brown et al., 2020; Schick and Schütze, 2021) and calibrated scores (Brown et al., 2020; Zhao et al., 2021) where we divide the score of a PREMISE. $T_{p}$ (HYPOTHESIS). by the score of $T_{p}$ (HYPOTHESIS). We evaluate the normalized, length-normalized, and calibrated likelihood on the validation sets of each dataset and select the most accurate method for each dataset and model.</p>
<p>We also consider a pretrained natural language inference model, which is trained to predict entailment scores between a context and a conclusion.</p>
<p>GPT2 We use the pretrained GPT2 base version with 127 M parameters (Radford et al., 2019), which is a causal language model trained to estimate text likelihood. We concatenate the premise and hypothesis and compute their likelihood as a plausibility score.
RoBERTa We also use the pretrained RoBERTa base model with 123 M parameters (Liu et al., 2019) to score the masked language modeling likelihood of the premise/hypothesis pair.
RoBERTa-MNLI We fine-tune RoBERTa on the MNLI entailment detection dataset (Williams et al., 2018) with standard hyperparameters (see the following subsection).
Human baseline To establish human baseline performance on the constructed dataset, we had two NLP researchers annotate 100 examples randomly sampled from the test set of each dataset, with a multiple-choice question answering setting.</p>
<p>Overall inter-annotator agreement is relatively high, with a Fleiss's $\kappa$ of 0.70/0.68/0.71 for WEP Reasoning 1 hop, 2 hops and WEP-UNLI respectively.</p>
<h3>4.2 Fine-tuning and transfer across probes</h3>
<p>We fine-tune RoBERTa-base models on our datasets, using standard (Mosbach et al., 2021) hyperparameters ${ }^{3}$ ( 3 epochs, sequence length of 256 , learning rate of $2.10^{-5}$ batch size of 16 . We use length-normalization with GPT2 likelihood and calibration with RoBERTa likelihood as they worked best on the validation sets.). We use a multiple-choice-question answering setup (we predict logit scores for the valid and invalid verbalization, combine their score with a softmax, then optimize the likelihood of the valid verbalization). The same format is applied to all tasks, so we can also study the transfer of capacities acquired during fine-tuning of each probe, for instance, between probability matching and compositional reasoning.</p>
<h3>4.3 Results and discussion</h3>
<p>Table 1 shows the results of our experiments. The very low accuracy of causal and masked language models (first two rows) demonstrates how challenging the WEP-understanding tasks are.</p>
<p>RoBERTa fine-tuned on MNLI dataset performs better than chance for WEP-UNLI. MNLI contains 814 instances of probably in the MNLI dataset, but we found little to no evidence of WEP compositions among them, which can explain the results.</p>
<p>Finally, fine-tuning on the dataset of a particular probe leads to high test accuracy on the associated test set. More surprisingly, fine-tuning on one dataset also causes substantial accuracy gain on other probes. This suggests that our datasets can</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>be incorporated in text encoder training in order to improve WEP handling.</p>
<h2>5 Conclusion</h2>
<p>We investigated WEP understanding in neural language models with new datasets and experiments, showing that WEP processing is challenging but helped by supervision which leads to transferable improvement. Future work could extract WEP probability scales from the UNLI dataset as an alternative to human perception surveys, but our work suggests that this requires language modeling progress.</p>
<h2>6 Acknowledgements</h2>
<p>This work is part of the CALCULUS project, which is funded by the ERC Advanced Grant H2020-ERC-2017 ADG $788506^{4}$.</p>
<h2>References</h2>
<p>Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632-642, Lisbon, Portugal. Association for Computational Linguistics.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.</p>
<p>Tongfei Chen, Zhengping Jiang, Adam Poliak, Keisuke Sakaguchi, and Benjamin Van Durme. 2020. Uncertain natural language inference. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8772-8779, Online. Association for Computational Linguistics.</p>
<p>Eugene Dantsin. 1992. Probabilistic logic programs and their semantics. In Logic Programming, pages 152164, Berlin, Heidelberg. Springer Berlin Heidelberg.</p>
<p>Luc De Raedt, Angelika Kimmig, and Hannu Toivonen. 2007. Problog: A probabilistic prolog and its application in link discovery. In IJCAI, volume 7, pages 2462-2467. Hyderabad.</p>
<p>Mandeep K Dhami and David R Mandel. 2021. Words or numbers? communicating probability in intelligence analysis. American Psychologist, 76(3):549.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Anton Dries, Angelika Kimmig, Jesse Davis, Vaishak Belle, and Luc de Raedt. 2017. Solving probability problems in natural language. In Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI-17, pages 3981-3987.</p>
<p>Wade Fagen-Ulmschneider. 2015. Perception of probability words.</p>
<p>Sherman Kent. 1964. Words of estimative probability. Studies in intelligence, 8(4):49-65.</p>
<p>Emily D Lenhardt, Rachael N Cross, Makenzie J Krocak, Joseph T Ripberger, Sean R Ernst, Carol L Silva, and Hank C Jenkins-Smith. 2020. How likely is that chance of thunderstorms? a study of how national weather service forecast offices use words of estimative probability and what they mean to the public. Journal of Operational Meteorology, 8(5).</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.</p>
<p>Marius Mosbach, Maksym Andriushchenko, and Dietrich Klakow. 2021. On the stability of fine-tuning bert: Misconceptions, explanations, and strong baselines. In International Conference on Learning Representations.</p>
<p>Aakanksha Naik, Abhilasha Ravichander, Norman Sadeh, Carolyn Rose, and Graham Neubig. 2018. Stress test evaluation for natural language inference. In Proceedings of the 27th International Conference on Computational Linguistics, pages 2340-2353, Santa Fe, New Mexico, USA. Association for Computational Linguistics.</p>
<p>B J O'Brien. 1989. Words or numbers? the evaluation of probability expressions in general practice. The Journal of the Royal College of General Practitioners, 39 320:98-100.</p>
<p>Douglas E Ott. 2021. Words representing numeric probabilities in medical writing are ambiguous and misinterpreted. JSLS: Journal of the Society of Laparoscopic \&amp; Robotic Surgeons, 25(3).
F.R. Palmer. 1992. Words and worlds; on the linguistic analysis of modality. (european university studies, series xiv, vol. 191): Richard matthews, frankfurt am main/bern/ new york/paris, peter lang, 1991. 310 pp. sfr 76.00 (pb.). Lingua, 88(1):87-90.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.</p>
<p>Kyle Richardson, Hai Hu, Lawrence Moss, and Ashish Sabharwal. 2020. Probing natural language inference models through semantic fragments. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 8713-8721.</p>
<p>Anna Rogers, Olga Kovaleva, and Anna Rumshisky. 2020. A primer in BERTology: What we know about how BERT works. Transactions of the Association for Computational Linguistics, 8:842-866.</p>
<p>Roser Saurí, Marc Verhagen, and James Pustejovsky. 2006. Annotating and recognizing event modality in text. In Proceedings of the Nineteenth International Florida Artificial Intelligence Research Society Conference, Melbourne Beach, Florida, USA, May 11-13, 2006, pages 333-339. AAAI Press.</p>
<p>Timo Schick and Hinrich Schütze. 2021. Exploiting cloze-questions for few-shot text classification and natural language inference. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 255-269, Online. Association for Computational Linguistics.</p>
<p>Damien Sileo and Antoine Lernould. 2023. Mindgames: Targeting theory of mind in large language models with dynamic epistemic modal logic. arXiv preprint arXiv:2305.03353.</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615.</p>
<p>Qi Su, Chu-Ren Huang, and Kai-yun Chen. 2010. Evidentiality for text trustworthiness detection. In Proceedings of the 2010 Workshop on NLP and Linguistics: Finding the Common Ground, pages 1017, Uppsala, Sweden. Association for Computational Linguistics.</p>
<p>Simon Suster, Pieter Fivez, Pietro Totis, Angelika Kimmig, Jesse Davis, Luc de Raedt, and Walter Daelemans. 2021. Mapping probability word problems to executable representations. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3627-3640, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M Rush, Bart Van Merriënboer, Armand Joulin, and Tomas Mikolov. 2015. Towards ai-complete question answering: A set of prerequisite toy tasks. arXiv preprint arXiv:1502.05698.</p>
<p>Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112-1122. Association for Computational Linguistics.</p>
<p>Sheng Zhang, Rachel Rudinger, Kevin Duh, and Benjamin Van Durme. 2017. Ordinal common-sense inference. Transactions of the Association for Computational Linguistics, 5:379-395.</p>
<p>Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. In Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 12697-12706. PMLR.</p>
<p>Xiang Zhou, Yixin Nie, and Mohit Bansal. 2022. Distributed nli: Learning to predict human opinion distributions for language reasoning. In Findings of the Association for Computational Linguistics: ACL 2022. Association for Computational Linguistics.</p>
<h1>A Associated probabilities</h1>
<table>
<thead>
<tr>
<th style="text-align: left;">WEP</th>
<th style="text-align: left;">Median probability judgment</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">certain</td>
<td style="text-align: left;">$100^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: left;">almost certain</td>
<td style="text-align: left;">$95.0 \pm 10.9$</td>
</tr>
<tr>
<td style="text-align: left;">highly likely</td>
<td style="text-align: left;">$90.0 \pm 8.4$</td>
</tr>
<tr>
<td style="text-align: left;">very good chance</td>
<td style="text-align: left;">$80.0 \pm 10.8$</td>
</tr>
<tr>
<td style="text-align: left;">we believe</td>
<td style="text-align: left;">$75.0 \pm 15.0$</td>
</tr>
<tr>
<td style="text-align: left;">likely</td>
<td style="text-align: left;">$70.0 \pm 11.3$</td>
</tr>
<tr>
<td style="text-align: left;">probably</td>
<td style="text-align: left;">$70.0 \pm 12.9$</td>
</tr>
<tr>
<td style="text-align: left;">probable</td>
<td style="text-align: left;">$70.0 \pm 14.7$</td>
</tr>
<tr>
<td style="text-align: left;">better than even</td>
<td style="text-align: left;">$60.0 \pm 9.1$</td>
</tr>
<tr>
<td style="text-align: left;">about even</td>
<td style="text-align: left;">$50.0 \pm 4.9$</td>
</tr>
<tr>
<td style="text-align: left;">probably not</td>
<td style="text-align: left;">$25.0 \pm 14.4$</td>
</tr>
<tr>
<td style="text-align: left;">we doubt</td>
<td style="text-align: left;">$20.0 \pm 16.9$</td>
</tr>
<tr>
<td style="text-align: left;">unlikely</td>
<td style="text-align: left;">$20.0 \pm 15.0$</td>
</tr>
<tr>
<td style="text-align: left;">little chance</td>
<td style="text-align: left;">$10.0 \pm 12.2$</td>
</tr>
<tr>
<td style="text-align: left;">chances are slight</td>
<td style="text-align: left;">$10.0 \pm 10.9$</td>
</tr>
<tr>
<td style="text-align: left;">improbable</td>
<td style="text-align: left;">$10.0 \pm 17.5$</td>
</tr>
<tr>
<td style="text-align: left;">highly unlikely</td>
<td style="text-align: left;">$5.0 \pm 17.3$</td>
</tr>
<tr>
<td style="text-align: left;">almost no chance</td>
<td style="text-align: left;">$2.0 \pm 17.0$</td>
</tr>
<tr>
<td style="text-align: left;">impossible</td>
<td style="text-align: left;">$0^{\dagger}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Median probability percentage associated to words of estimative probability according to (FagenUlmschneider, 2015). First and last words ( $\dagger$ ) are taken from (Kent, 1964).</p>
<h2>B WEP verbalization template</h2>
<table>
<thead>
<tr>
<th style="text-align: left;">WEP</th>
<th style="text-align: left;">Verbalization template</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">about even</td>
<td style="text-align: left;">chances are about even that [FACT]</td>
</tr>
<tr>
<td style="text-align: left;">almost certain</td>
<td style="text-align: left;">it is almost certain that [FACT]</td>
</tr>
<tr>
<td style="text-align: left;">almost no chance</td>
<td style="text-align: left;">there is almost no chance that [FACT]</td>
</tr>
<tr>
<td style="text-align: left;">better than even</td>
<td style="text-align: left;">there is a better than even chance that [FACT]</td>
</tr>
<tr>
<td style="text-align: left;">certain</td>
<td style="text-align: left;">it is certain that [FACT]</td>
</tr>
<tr>
<td style="text-align: left;">chances are slight</td>
<td style="text-align: left;">chances are slight that [FACT]</td>
</tr>
<tr>
<td style="text-align: left;">highly likely</td>
<td style="text-align: left;">it is highly likely that [FACT]</td>
</tr>
<tr>
<td style="text-align: left;">highly unlikely</td>
<td style="text-align: left;">it is highly unlikely that [FACT]</td>
</tr>
<tr>
<td style="text-align: left;">impossible</td>
<td style="text-align: left;">it is impossible that [FACT]</td>
</tr>
<tr>
<td style="text-align: left;">improbable</td>
<td style="text-align: left;">it is improbable that [FACT]</td>
</tr>
<tr>
<td style="text-align: left;">likely</td>
<td style="text-align: left;">it is likely that [FACT]</td>
</tr>
<tr>
<td style="text-align: left;">little chance</td>
<td style="text-align: left;">there is little chance that [FACT]</td>
</tr>
<tr>
<td style="text-align: left;">probable</td>
<td style="text-align: left;">it is probable that [FACT]</td>
</tr>
<tr>
<td style="text-align: left;">probably</td>
<td style="text-align: left;">it is probably the case that [FACT]</td>
</tr>
<tr>
<td style="text-align: left;">probably not</td>
<td style="text-align: left;">it is probably not the case that [FACT]</td>
</tr>
<tr>
<td style="text-align: left;">unlikely</td>
<td style="text-align: left;">it is unlikely that [FACT]</td>
</tr>
<tr>
<td style="text-align: left;">very good chance</td>
<td style="text-align: left;">there is a very good chance that [FACT]</td>
</tr>
<tr>
<td style="text-align: left;">we believe</td>
<td style="text-align: left;">we believe that [FACT]</td>
</tr>
<tr>
<td style="text-align: left;">we doubt</td>
<td style="text-align: left;">we doubt that [FACT]</td>
</tr>
</tbody>
</table>
<p>Table 3: Templates used to convert a fact and a WEP expressed uncertainty into a modal sentence.</p>
<h1>C WEP frequencies on the generated datasets</h1>
<table>
<thead>
<tr>
<th style="text-align: left;">WEP-reasoning</th>
<th style="text-align: center;">(1 hop)</th>
<th style="text-align: left;">WEP-Reasoning</th>
<th style="text-align: center;">(2 hops)</th>
<th style="text-align: left;">WEP-USNLI</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">WEP</td>
<td style="text-align: center;">frequency</td>
<td style="text-align: left;">WEP</td>
<td style="text-align: center;">frequency</td>
<td style="text-align: left;">WEP</td>
<td style="text-align: center;">frequency</td>
</tr>
<tr>
<td style="text-align: left;">about even</td>
<td style="text-align: center;">11.1</td>
<td style="text-align: left;">impossible</td>
<td style="text-align: center;">13.2</td>
<td style="text-align: left;">impossible</td>
<td style="text-align: center;">25.6</td>
</tr>
<tr>
<td style="text-align: left;">probably not</td>
<td style="text-align: center;">9.7</td>
<td style="text-align: left;">about even</td>
<td style="text-align: center;">10.8</td>
<td style="text-align: left;">better than even</td>
<td style="text-align: center;">10.7</td>
</tr>
<tr>
<td style="text-align: left;">better than even</td>
<td style="text-align: center;">7.7</td>
<td style="text-align: left;">probably not</td>
<td style="text-align: center;">9.0</td>
<td style="text-align: left;">certain</td>
<td style="text-align: center;">7.2</td>
</tr>
<tr>
<td style="text-align: left;">we believe</td>
<td style="text-align: center;">7.1</td>
<td style="text-align: left;">highly unlikely</td>
<td style="text-align: center;">8.2</td>
<td style="text-align: left;">about even</td>
<td style="text-align: center;">6.9</td>
</tr>
<tr>
<td style="text-align: left;">highly likely</td>
<td style="text-align: center;">6.4</td>
<td style="text-align: left;">almost no chance</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: left;">almost certain</td>
<td style="text-align: center;">6.7</td>
</tr>
<tr>
<td style="text-align: left;">certain</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: left;">better than even</td>
<td style="text-align: center;">6.6</td>
<td style="text-align: left;">highly likely</td>
<td style="text-align: center;">6.0</td>
</tr>
<tr>
<td style="text-align: left;">highly unlikely</td>
<td style="text-align: center;">5.9</td>
<td style="text-align: left;">we believe</td>
<td style="text-align: center;">4.3</td>
<td style="text-align: left;">very good chance</td>
<td style="text-align: center;">5.9</td>
</tr>
<tr>
<td style="text-align: left;">almost no chance</td>
<td style="text-align: center;">5.8</td>
<td style="text-align: left;">highly likely</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: left;">almost no chance</td>
<td style="text-align: center;">5.0</td>
</tr>
<tr>
<td style="text-align: left;">impossible</td>
<td style="text-align: center;">5.3</td>
<td style="text-align: left;">very good chance</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: left;">we believe</td>
<td style="text-align: center;">4.1</td>
</tr>
<tr>
<td style="text-align: left;">almost certain</td>
<td style="text-align: center;">5.1</td>
<td style="text-align: left;">we doubt</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: left;">highly unlikely</td>
<td style="text-align: center;">4.1</td>
</tr>
<tr>
<td style="text-align: left;">very good chance</td>
<td style="text-align: center;">4.7</td>
<td style="text-align: left;">improbable</td>
<td style="text-align: center;">3.9</td>
<td style="text-align: left;">probably not</td>
<td style="text-align: center;">3.4</td>
</tr>
<tr>
<td style="text-align: left;">chances are slight</td>
<td style="text-align: center;">3.6</td>
<td style="text-align: left;">chances are slight</td>
<td style="text-align: center;">3.9</td>
<td style="text-align: left;">likely</td>
<td style="text-align: center;">2.5</td>
</tr>
<tr>
<td style="text-align: left;">little chance</td>
<td style="text-align: center;">3.5</td>
<td style="text-align: left;">unlikely</td>
<td style="text-align: center;">3.6</td>
<td style="text-align: left;">probable</td>
<td style="text-align: center;">2.4</td>
</tr>
<tr>
<td style="text-align: left;">probable</td>
<td style="text-align: center;">3.2</td>
<td style="text-align: left;">little chance</td>
<td style="text-align: center;">3.5</td>
<td style="text-align: left;">probably</td>
<td style="text-align: center;">2.4</td>
</tr>
<tr>
<td style="text-align: left;">unlikely</td>
<td style="text-align: center;">3.1</td>
<td style="text-align: left;">almost certain</td>
<td style="text-align: center;">2.9</td>
<td style="text-align: left;">unlikely</td>
<td style="text-align: center;">1.5</td>
</tr>
<tr>
<td style="text-align: left;">likely</td>
<td style="text-align: center;">3.1</td>
<td style="text-align: left;">certain</td>
<td style="text-align: center;">2.7</td>
<td style="text-align: left;">little chance</td>
<td style="text-align: center;">1.5</td>
</tr>
<tr>
<td style="text-align: left;">probably</td>
<td style="text-align: center;">3.0</td>
<td style="text-align: left;">likely</td>
<td style="text-align: center;">2.5</td>
<td style="text-align: left;">chances are slight</td>
<td style="text-align: center;">1.5</td>
</tr>
<tr>
<td style="text-align: left;">we doubt</td>
<td style="text-align: center;">2.9</td>
<td style="text-align: left;">probable</td>
<td style="text-align: center;">2.4</td>
<td style="text-align: left;">improbable</td>
<td style="text-align: center;">1.4</td>
</tr>
<tr>
<td style="text-align: left;">improbable</td>
<td style="text-align: center;">2.9</td>
<td style="text-align: left;">probably</td>
<td style="text-align: center;">2.2</td>
<td style="text-align: left;">we doubt</td>
<td style="text-align: center;">1.4</td>
</tr>
</tbody>
</table>
<p>Table 4: Validation set frequency of WEP in the correct answer of each dataset (percentages).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ https://calculus-project.eu/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
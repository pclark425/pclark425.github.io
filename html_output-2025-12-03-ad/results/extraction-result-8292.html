<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8292 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8292</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8292</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-152.html">extraction-schema-152</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-254877753</p>
                <p><strong>Paper Title:</strong> <a href="https://aclanthology.org/2023.findings-acl.67.pdf" target="_blank">Towards Reasoning in Large Language Models: A Survey</a></p>
                <p><strong>Paper Abstract:</strong> Reasoning is a fundamental aspect of human intelligence that plays a crucial role in activities such as problem solving, decision making, and critical thinking. In recent years, large language models (LLMs) have made significant progress in natural language processing, and there is observation that these models may exhibit reasoning abilities when they are sufficiently large. However, it is not yet clear to what extent LLMs are capable of reasoning. This paper provides a comprehensive overview of the current state of knowledge on reasoning in LLMs, including techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, findings and implications of previous research in this field, and suggestions on future directions. Our aim is to provide a detailed and up-to-date review of this topic and stimulate meaningful discussion and future work.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8292.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8292.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompting method that includes intermediate natural-language reasoning steps (rationales) in few-shot demonstrations or elicits them via a phrase like 'Let's think step by step' to encourage LLMs to produce explicit multi-step reasoning prior to the final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain of thought prompting elicits reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Large language models (e.g., GPT-3 175B, PaLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer LLMs pretrained on large text corpora; CoT applied via prompting without changing model parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['chain-of-thought (explicit natural-language rationales)', 'zero-shot CoT (phrase-elicited reasoning)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>CoT inserts ⟨input, chain-of-thought, output⟩ exemplars in the prompt to teach the model to generate intermediate steps; Zero-shot-CoT uses a short instruction (e.g., 'Let's think step by step') to elicit a rationale without few-shot exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compared standard prompting (direct answer) vs CoT prompting; Zero-shot-CoT compared with few-shot CoT; various studies report ablations on exemplar choice, ordering, and relevance as part of CoT analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Arithmetic, symbolic, and commonsense reasoning benchmarks (e.g., GSM8K, Math, MathQA, CSQA, StrategyQA, Last Letter Concatenation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Survey reports CoT often substantially improves few-shot performance on arithmetic, symbolic, and commonsense tasks (qualitative description); no single numerical aggregate given in survey (original papers report large relative gains on tasks like GSM8K for sufficiently large models).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>CoT elicits step-by-step outputs and can dramatically improve performance and out-of-distribution robustness for large models; generated rationales can still be incorrect/incoherent and ordering/relevance of steps matter.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Chain-of-thought prompting reliably elicits multi-step reasoning-like outputs and improves downstream task accuracy for large LLMs, but it does not prove the model is 'reasoning' in the human sense; exemplar choice and rationale quality strongly affect outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Reasoning in Large Language Models: A Survey', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8292.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8292.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-shot-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot Chain-of-Thought (Zero-shot-CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Simple prompting technique that elicits reasoning without few-shot exemplars by appending a phrase such as 'Let's think step by step' to the query, enabling the model to produce intermediate rationale in zero-shot settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models are zero-shot reasoners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Large language models (e.g., GPT-3 variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer LLMs used with a single-shot/zero-shot instruction to prompt reasoning behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['zero-shot chain-of-thought prompting']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>A short natural-language instruction appended to the input nudges the model to emit a chain of reasoning before answering, avoiding the need for few-shot exemplar rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compared Zero-shot-CoT to few-shot CoT and standard prompting across benchmarks; assessed effect of the single eliciting phrase versus exemplar-based rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Various reasoning tasks including arithmetic and commonsense benchmarks (as above)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Survey states Zero-shot-CoT can elicit reasoning and improve performance without exemplars for large models; specific numeric comparisons are reported in original papers but not enumerated in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Zero-shot-CoT provides a simple way to get CoT-like behavior; effectiveness depends on model scale (works better for very large models) and does not remove quality issues in generated rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>A simple phrase can elicit rationales in large LLMs and improve performance, demonstrating that prompting design (not just exemplars) affects whether a model emits stepwise reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Reasoning in Large Language Models: A Survey', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8292.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8292.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency (rationale exploration via diverse sampling)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Decoding/selection strategy that samples many diverse chains-of-thought and selects the final answer by marginalizing/voting over them to improve robustness and accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-consistency improves chain of thought reasoning in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Large language models (e.g., PaLM, GPT-3 family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLMs decoding using stochastic sampling to produce multiple reasoning traces, followed by aggregation over outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['diverse rationale sampling', 'voting / marginalization over rationales']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Instead of greedy decoding (single rationale), the method samples a set of diverse rationales using temperature/top-k sampling and selects the most consistent final answer (majority vote or marginalization over sampled chains).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Direct comparison of greedy CoT (single rationale) vs sampling multiple rationales then aggregating (self-consistency); some studies also vary exemplar sets to increase diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Arithmetic and multi-step reasoning datasets (e.g., GSM8K and other CoT-evaluated tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Survey reports self-consistency improves over greedy CoT decoding (qualitative summary); original work demonstrates consistent accuracy gains by aggregating diverse sampled rationales but survey does not give exact numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Sampling diverse rationales and aggregating reduces sensitivity to individual flawed rationales, increases robustness and accuracy, and leverages multiple ways of reasoning to reach correct answer.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Diverse reasoning traces combined via self-consistency lead to stronger performance than relying on a single greedy rationale, indicating benefit from exploring multiple reasoning methods/paths.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Reasoning in Large Language Models: A Survey', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8292.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8292.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Auto-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automatic Chain-of-Thought Prompting (Auto-CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Automatic exemplar construction method that clusters dataset questions and uses a representative question per cluster to generate rationales, with an emphasis on constructing diverse exemplars to improve CoT prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automatic chain of thought prompting in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs used as generators for exemplars (e.g., GPT-family models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses an LLM to automatically generate rationales for representative instances derived from clustering; exemplars then used in few-shot CoT prompts for target questions.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['rationale exemplar automation', 'diverse exemplar selection']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Questions are clustered; for each cluster a representative question's rationale is generated (via Zero-shot-CoT) and included as a CoT exemplar, promoting diversity among examples shown in the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Empirical analyses comparing exemplar construction strategies; study highlights that increasing diversity of exemplars improves downstream CoT performance.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Various CoT-evaluated reasoning tasks (survey-level description)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Survey notes Auto-CoT analysis shows making exemplars diverse is important for eliciting better rationales and improved performance; no numeric results provided in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Diverse exemplars lead to better rationales from the model and improved few-shot reasoning performance; exemplar choice is a critical factor in CoT effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Diversity in the exemplars used for CoT prompting is a key factor that improves the model's reasoning outputs and final accuracy; automatic exemplar construction can exploit this.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Reasoning in Large Language Models: A Survey', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8292.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8292.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Rationale Verification</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rationale Verification (trained or model-based verifiers)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Approach that verifies candidate rationales/solutions—either via a separately trained verifier model or by using the LLM itself—to select the most likely correct solution among generated candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Training verifiers to solve math word problems</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs + external verifier models or LLM self-verification (e.g., GPT family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pipeline where generation of candidate rationale+answer pairs is followed by scoring and selection using a verifier classifier or a separate LLM pass.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['external trained verifier', 'LLM self-verification']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Generate multiple candidate rationales and solutions, then apply a verifier (trained on rationale-answer correctness) or ask LLM to judge/verify candidates, selecting highest-scoring solution.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Comparisons noted between greedy selection, voting/self-consistency, and verifier-based selection; Weng et al. propose using LLMs as verifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Math word problems and arithmetic reasoning datasets (e.g., GSM8K, MathQA)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Survey summarizes that verifier augmentation can improve final-answer selection and reduce errors from incorrect rationales; specific numbers not reported in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Verifiers reduce the impact of incorrect reasoning traces by scoring candidates, improving reliability; LLM-based verifiers offer an alternative to training separate classifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Verifying candidate rationales and answers improves robustness and accuracy over naive single-rationale decoding; combining diverse generation with verification yields benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Reasoning in Large Language Models: A Survey', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8292.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8292.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Least-to-Most</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Least-to-Most Prompting (problem decomposition)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Decomposition technique that breaks complex problems into an ordered sequence of simpler subproblems and solves them stepwise, passing answers of earlier subproblems to later ones.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Least-to-most prompting enables complex reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs used in multi-pass prompting (e.g., GPT-family, PaLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses prompting to decompose and solve subproblems over multiple forward passes, facilitating compositional/complex reasoning without changing model parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['problem decomposition / least-to-most', 'decomposed prompting / dynamic least-to-most', 'successive prompting']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Decompose the original problem into smaller subquestions; solve each subquestion sequentially with the model, using previous answers as context for subsequent subproblems.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compared monolithic CoT (single pass) vs decomposition methods (multiple passes) and dynamic selection of exemplars; pressures on compositional generalization were evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Compositional semantic parsing, complex multi-step reasoning tasks, and some table/semantic parsing tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Survey reports decomposition methods help with compositional generalization and solving complex tasks where monolithic CoT struggles; numeric results are left to cited works.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Decomposition (least-to-most and variants) helps LLMs handle compositional/complex reasoning by structuring the solution process; some methods run multiple forward passes while others attempt single-pass decomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Problem decomposition increases tractability and performance on complex tasks; divide-and-conquer approaches complement CoT and can outperform single-pass rationales on compositional problems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Reasoning in Large Language Models: A Survey', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8292.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8292.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Rationale Engineering</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rationale Engineering (refinement, exploration, verification)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An umbrella of techniques for creating, exploring, and validating rationales (exemplar design, complexity tuning, automated exemplar selection, diverse sampling, and verification) to more effectively elicit and use reasoning from LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>General LLMs (GPT-3, Codex, PaLM, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Rationale engineering is applied at the prompting/decoding stage and may use auxiliary models or the LLM itself for refinement and verification; no single model architecture required.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['rationale refinement (complexity-based prompting, algorithmic prompting)', 'rationale exploration (diverse sampling, voting)', 'rationale verification (verifiers or LLM self-checking)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Refinement: design better exemplars (complexity, thoroughness); Exploration: sample many rationales, use voting/self-consistency; Verification: score and select among candidates via trained verifier or LLM-based checks.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Survey references studies that vary exemplar complexity and diversity (Fu et al., Zhang Auto-CoT), sampling strategies (self-consistency), and verifier use (Cobbe et al., Weng et al.) to compare performance.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Arithmetic, commonsense, symbolic reasoning benchmarks (GSM8K, CSQA, Last Letter Concatenation, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Multiple cited works report that better exemplars, greater exemplar diversity, and diverse rationale sampling plus verification improve accuracy and robustness; survey does not list unified numeric results.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Rationale engineering shows that careful design and exploration of rationales—especially increasing diversity—improves solution correctness and robustness; however, bad or irrelevant rationales can still mislead models.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Systematic engineering of rationales (refinement, exploration, verification) is crucial: diverse exemplars and sampled rationales plus verification yield better and more robust problem solving than single static rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Reasoning in Large Language Models: A Survey', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8292.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8292.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>STaR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Taught Reasoner (STaR) / Bootstrapping</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Bootstrapping technique where an LLM is iteratively finetuned on its own generated rationales that lead to correct answers, enabling the model to self-improve its reasoning capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>STar: Bootstrapping reasoning with reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs fine-tuned on self-generated rationales (e.g., GPT variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Start from a pretrained LLM, generate chain-of-thought rationales and correct outputs, then finetune the model on the subset of its outputs that are correct, repeating iteratively.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['bootstrapping / self-improvement via generated rationales']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Generate CoT rationales and answers; select those that yield correct answers; finetune the model on this self-generated training data to improve future reasoning generation.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Pipelines compare base model performance vs model finetuned on its own correct rationales across iterations; survey also cites work on self-consistency supporting self-improvement without supervised data.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Reasoning benchmarks targeted in STaR and follow-ups (math/logic datasets described in cited works)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Survey summarizes that bootstrapping can improve reasoning ability and that iterations produce improved models; exact numbers referenced to original STaR paper.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Self-improvement via bootstrapping helps models generate better rationales over iterations, but relies on being able to identify correct outputs to train on and may propagate subtle biases/artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Bootstrapping from a model's own correct rationales can enhance performance without external supervised data, especially when combined with strategies that identify high-quality rationales (e.g., self-consistency).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Reasoning in Large Language Models: A Survey', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8292.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8292.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reasoning-Enhanced Training</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reasoning-Enhanced Pretraining / Finetuning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Improving LLM reasoning by pretraining/finetuning on datasets containing reasoning-heavy content (e.g., code, SQL, scientific/math corpora), which can boost downstream reasoning when combined with CoT prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Solving quantitative reasoning problems with language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs pretrained or continually pretrained with reasoning-style data (e.g., code, SQL, scientific text)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Modify pretraining or continual pretraining data to include structured reasoning examples (programs, SQL, math/science text) and/or finetune on CoT-labeled datasets to improve reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['finetuning on reasoning datasets', 'continual pretraining with code/SQL/scientific data']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Train or fine-tune LLMs on corpora that include formalized reasoning or programmatic data (code, SQL, mathematical content) and/or CoT-labeled examples to induce better reasoning capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Studies compare standard pretrained models vs those pretrained/finetuned on reasoning-rich corpora and evaluate with CoT prompting, showing improved generalization especially on longer or numeric reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Quantitative reasoning, numerical and logical reasoning tasks, and other CoT-evaluated benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Survey reports that models trained on reasoning-rich data (code, scientific/math corpora) achieve better performance with CoT prompting and generalize better to longer problems; precise metrics are in cited works.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Incorporating structured reasoning data into training increases the model's capacity to perform multi-step and numeric reasoning, and complements prompting techniques (CoT) for better end-task performance.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Training data that includes reasoning-like structure (e.g., code/SQL/math/science) and finetuning with CoT examples can improve LLM reasoning, but such training changes model capabilities rather than only eliciting latent abilities via prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Reasoning in Large Language Models: A Survey', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Large language models are zero-shot reasoners <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Automatic chain of thought prompting in large language models <em>(Rating: 2)</em></li>
                <li>Complexity-based prompting for multi-step reasoning <em>(Rating: 2)</em></li>
                <li>Least-to-most prompting enables complex reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 2)</em></li>
                <li>STar: Bootstrapping reasoning with reasoning <em>(Rating: 2)</em></li>
                <li>Solving quantitative reasoning problems with language models <em>(Rating: 2)</em></li>
                <li>Are large pre-trained language models leaking your personal information? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8292",
    "paper_id": "paper-254877753",
    "extraction_schema_id": "extraction-schema-152",
    "extracted_data": [
        {
            "name_short": "CoT",
            "name_full": "Chain-of-Thought Prompting",
            "brief_description": "Prompting method that includes intermediate natural-language reasoning steps (rationales) in few-shot demonstrations or elicits them via a phrase like 'Let's think step by step' to encourage LLMs to produce explicit multi-step reasoning prior to the final answer.",
            "citation_title": "Chain of thought prompting elicits reasoning in large language models",
            "mention_or_use": "mention",
            "model_name": "Large language models (e.g., GPT-3 175B, PaLM)",
            "model_description": "Autoregressive transformer LLMs pretrained on large text corpora; CoT applied via prompting without changing model parameters.",
            "reasoning_methods": [
                "chain-of-thought (explicit natural-language rationales)",
                "zero-shot CoT (phrase-elicited reasoning)"
            ],
            "reasoning_methods_description": "CoT inserts ⟨input, chain-of-thought, output⟩ exemplars in the prompt to teach the model to generate intermediate steps; Zero-shot-CoT uses a short instruction (e.g., 'Let's think step by step') to elicit a rationale without few-shot exemplars.",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Compared standard prompting (direct answer) vs CoT prompting; Zero-shot-CoT compared with few-shot CoT; various studies report ablations on exemplar choice, ordering, and relevance as part of CoT analyses.",
            "task_or_benchmark": "Arithmetic, symbolic, and commonsense reasoning benchmarks (e.g., GSM8K, Math, MathQA, CSQA, StrategyQA, Last Letter Concatenation)",
            "performance_results": "Survey reports CoT often substantially improves few-shot performance on arithmetic, symbolic, and commonsense tasks (qualitative description); no single numerical aggregate given in survey (original papers report large relative gains on tasks like GSM8K for sufficiently large models).",
            "qualitative_findings": "CoT elicits step-by-step outputs and can dramatically improve performance and out-of-distribution robustness for large models; generated rationales can still be incorrect/incoherent and ordering/relevance of steps matter.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Chain-of-thought prompting reliably elicits multi-step reasoning-like outputs and improves downstream task accuracy for large LLMs, but it does not prove the model is 'reasoning' in the human sense; exemplar choice and rationale quality strongly affect outcomes.",
            "uuid": "e8292.0",
            "source_info": {
                "paper_title": "Towards Reasoning in Large Language Models: A Survey",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Zero-shot-CoT",
            "name_full": "Zero-shot Chain-of-Thought (Zero-shot-CoT)",
            "brief_description": "Simple prompting technique that elicits reasoning without few-shot exemplars by appending a phrase such as 'Let's think step by step' to the query, enabling the model to produce intermediate rationale in zero-shot settings.",
            "citation_title": "Large language models are zero-shot reasoners",
            "mention_or_use": "mention",
            "model_name": "Large language models (e.g., GPT-3 variants)",
            "model_description": "Autoregressive transformer LLMs used with a single-shot/zero-shot instruction to prompt reasoning behavior.",
            "reasoning_methods": [
                "zero-shot chain-of-thought prompting"
            ],
            "reasoning_methods_description": "A short natural-language instruction appended to the input nudges the model to emit a chain of reasoning before answering, avoiding the need for few-shot exemplar rationales.",
            "reasoning_diversity": "similar",
            "reasoning_diversity_experimental_setup": "Compared Zero-shot-CoT to few-shot CoT and standard prompting across benchmarks; assessed effect of the single eliciting phrase versus exemplar-based rationales.",
            "task_or_benchmark": "Various reasoning tasks including arithmetic and commonsense benchmarks (as above)",
            "performance_results": "Survey states Zero-shot-CoT can elicit reasoning and improve performance without exemplars for large models; specific numeric comparisons are reported in original papers but not enumerated in the survey.",
            "qualitative_findings": "Zero-shot-CoT provides a simple way to get CoT-like behavior; effectiveness depends on model scale (works better for very large models) and does not remove quality issues in generated rationales.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "A simple phrase can elicit rationales in large LLMs and improve performance, demonstrating that prompting design (not just exemplars) affects whether a model emits stepwise reasoning.",
            "uuid": "e8292.1",
            "source_info": {
                "paper_title": "Towards Reasoning in Large Language Models: A Survey",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Self-Consistency",
            "name_full": "Self-Consistency (rationale exploration via diverse sampling)",
            "brief_description": "Decoding/selection strategy that samples many diverse chains-of-thought and selects the final answer by marginalizing/voting over them to improve robustness and accuracy.",
            "citation_title": "Self-consistency improves chain of thought reasoning in language models",
            "mention_or_use": "mention",
            "model_name": "Large language models (e.g., PaLM, GPT-3 family)",
            "model_description": "LLMs decoding using stochastic sampling to produce multiple reasoning traces, followed by aggregation over outputs.",
            "reasoning_methods": [
                "diverse rationale sampling",
                "voting / marginalization over rationales"
            ],
            "reasoning_methods_description": "Instead of greedy decoding (single rationale), the method samples a set of diverse rationales using temperature/top-k sampling and selects the most consistent final answer (majority vote or marginalization over sampled chains).",
            "reasoning_diversity": "diverse",
            "reasoning_diversity_experimental_setup": "Direct comparison of greedy CoT (single rationale) vs sampling multiple rationales then aggregating (self-consistency); some studies also vary exemplar sets to increase diversity.",
            "task_or_benchmark": "Arithmetic and multi-step reasoning datasets (e.g., GSM8K and other CoT-evaluated tasks)",
            "performance_results": "Survey reports self-consistency improves over greedy CoT decoding (qualitative summary); original work demonstrates consistent accuracy gains by aggregating diverse sampled rationales but survey does not give exact numbers.",
            "qualitative_findings": "Sampling diverse rationales and aggregating reduces sensitivity to individual flawed rationales, increases robustness and accuracy, and leverages multiple ways of reasoning to reach correct answer.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Diverse reasoning traces combined via self-consistency lead to stronger performance than relying on a single greedy rationale, indicating benefit from exploring multiple reasoning methods/paths.",
            "uuid": "e8292.2",
            "source_info": {
                "paper_title": "Towards Reasoning in Large Language Models: A Survey",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Auto-CoT",
            "name_full": "Automatic Chain-of-Thought Prompting (Auto-CoT)",
            "brief_description": "Automatic exemplar construction method that clusters dataset questions and uses a representative question per cluster to generate rationales, with an emphasis on constructing diverse exemplars to improve CoT prompting.",
            "citation_title": "Automatic chain of thought prompting in large language models",
            "mention_or_use": "mention",
            "model_name": "LLMs used as generators for exemplars (e.g., GPT-family models)",
            "model_description": "Uses an LLM to automatically generate rationales for representative instances derived from clustering; exemplars then used in few-shot CoT prompts for target questions.",
            "reasoning_methods": [
                "rationale exemplar automation",
                "diverse exemplar selection"
            ],
            "reasoning_methods_description": "Questions are clustered; for each cluster a representative question's rationale is generated (via Zero-shot-CoT) and included as a CoT exemplar, promoting diversity among examples shown in the prompt.",
            "reasoning_diversity": "diverse",
            "reasoning_diversity_experimental_setup": "Empirical analyses comparing exemplar construction strategies; study highlights that increasing diversity of exemplars improves downstream CoT performance.",
            "task_or_benchmark": "Various CoT-evaluated reasoning tasks (survey-level description)",
            "performance_results": "Survey notes Auto-CoT analysis shows making exemplars diverse is important for eliciting better rationales and improved performance; no numeric results provided in survey.",
            "qualitative_findings": "Diverse exemplars lead to better rationales from the model and improved few-shot reasoning performance; exemplar choice is a critical factor in CoT effectiveness.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Diversity in the exemplars used for CoT prompting is a key factor that improves the model's reasoning outputs and final accuracy; automatic exemplar construction can exploit this.",
            "uuid": "e8292.3",
            "source_info": {
                "paper_title": "Towards Reasoning in Large Language Models: A Survey",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Rationale Verification",
            "name_full": "Rationale Verification (trained or model-based verifiers)",
            "brief_description": "Approach that verifies candidate rationales/solutions—either via a separately trained verifier model or by using the LLM itself—to select the most likely correct solution among generated candidates.",
            "citation_title": "Training verifiers to solve math word problems",
            "mention_or_use": "mention",
            "model_name": "LLMs + external verifier models or LLM self-verification (e.g., GPT family)",
            "model_description": "Pipeline where generation of candidate rationale+answer pairs is followed by scoring and selection using a verifier classifier or a separate LLM pass.",
            "reasoning_methods": [
                "external trained verifier",
                "LLM self-verification"
            ],
            "reasoning_methods_description": "Generate multiple candidate rationales and solutions, then apply a verifier (trained on rationale-answer correctness) or ask LLM to judge/verify candidates, selecting highest-scoring solution.",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Comparisons noted between greedy selection, voting/self-consistency, and verifier-based selection; Weng et al. propose using LLMs as verifiers.",
            "task_or_benchmark": "Math word problems and arithmetic reasoning datasets (e.g., GSM8K, MathQA)",
            "performance_results": "Survey summarizes that verifier augmentation can improve final-answer selection and reduce errors from incorrect rationales; specific numbers not reported in the survey.",
            "qualitative_findings": "Verifiers reduce the impact of incorrect reasoning traces by scoring candidates, improving reliability; LLM-based verifiers offer an alternative to training separate classifiers.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Verifying candidate rationales and answers improves robustness and accuracy over naive single-rationale decoding; combining diverse generation with verification yields benefits.",
            "uuid": "e8292.4",
            "source_info": {
                "paper_title": "Towards Reasoning in Large Language Models: A Survey",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Least-to-Most",
            "name_full": "Least-to-Most Prompting (problem decomposition)",
            "brief_description": "Decomposition technique that breaks complex problems into an ordered sequence of simpler subproblems and solves them stepwise, passing answers of earlier subproblems to later ones.",
            "citation_title": "Least-to-most prompting enables complex reasoning in large language models",
            "mention_or_use": "mention",
            "model_name": "LLMs used in multi-pass prompting (e.g., GPT-family, PaLM)",
            "model_description": "Uses prompting to decompose and solve subproblems over multiple forward passes, facilitating compositional/complex reasoning without changing model parameters.",
            "reasoning_methods": [
                "problem decomposition / least-to-most",
                "decomposed prompting / dynamic least-to-most",
                "successive prompting"
            ],
            "reasoning_methods_description": "Decompose the original problem into smaller subquestions; solve each subquestion sequentially with the model, using previous answers as context for subsequent subproblems.",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Compared monolithic CoT (single pass) vs decomposition methods (multiple passes) and dynamic selection of exemplars; pressures on compositional generalization were evaluated.",
            "task_or_benchmark": "Compositional semantic parsing, complex multi-step reasoning tasks, and some table/semantic parsing tasks",
            "performance_results": "Survey reports decomposition methods help with compositional generalization and solving complex tasks where monolithic CoT struggles; numeric results are left to cited works.",
            "qualitative_findings": "Decomposition (least-to-most and variants) helps LLMs handle compositional/complex reasoning by structuring the solution process; some methods run multiple forward passes while others attempt single-pass decomposition.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Problem decomposition increases tractability and performance on complex tasks; divide-and-conquer approaches complement CoT and can outperform single-pass rationales on compositional problems.",
            "uuid": "e8292.5",
            "source_info": {
                "paper_title": "Towards Reasoning in Large Language Models: A Survey",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Rationale Engineering",
            "name_full": "Rationale Engineering (refinement, exploration, verification)",
            "brief_description": "An umbrella of techniques for creating, exploring, and validating rationales (exemplar design, complexity tuning, automated exemplar selection, diverse sampling, and verification) to more effectively elicit and use reasoning from LLMs.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "General LLMs (GPT-3, Codex, PaLM, etc.)",
            "model_description": "Rationale engineering is applied at the prompting/decoding stage and may use auxiliary models or the LLM itself for refinement and verification; no single model architecture required.",
            "reasoning_methods": [
                "rationale refinement (complexity-based prompting, algorithmic prompting)",
                "rationale exploration (diverse sampling, voting)",
                "rationale verification (verifiers or LLM self-checking)"
            ],
            "reasoning_methods_description": "Refinement: design better exemplars (complexity, thoroughness); Exploration: sample many rationales, use voting/self-consistency; Verification: score and select among candidates via trained verifier or LLM-based checks.",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Survey references studies that vary exemplar complexity and diversity (Fu et al., Zhang Auto-CoT), sampling strategies (self-consistency), and verifier use (Cobbe et al., Weng et al.) to compare performance.",
            "task_or_benchmark": "Arithmetic, commonsense, symbolic reasoning benchmarks (GSM8K, CSQA, Last Letter Concatenation, etc.)",
            "performance_results": "Multiple cited works report that better exemplars, greater exemplar diversity, and diverse rationale sampling plus verification improve accuracy and robustness; survey does not list unified numeric results.",
            "qualitative_findings": "Rationale engineering shows that careful design and exploration of rationales—especially increasing diversity—improves solution correctness and robustness; however, bad or irrelevant rationales can still mislead models.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Systematic engineering of rationales (refinement, exploration, verification) is crucial: diverse exemplars and sampled rationales plus verification yield better and more robust problem solving than single static rationales.",
            "uuid": "e8292.6",
            "source_info": {
                "paper_title": "Towards Reasoning in Large Language Models: A Survey",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "STaR",
            "name_full": "Self-Taught Reasoner (STaR) / Bootstrapping",
            "brief_description": "Bootstrapping technique where an LLM is iteratively finetuned on its own generated rationales that lead to correct answers, enabling the model to self-improve its reasoning capabilities.",
            "citation_title": "STar: Bootstrapping reasoning with reasoning",
            "mention_or_use": "mention",
            "model_name": "LLMs fine-tuned on self-generated rationales (e.g., GPT variants)",
            "model_description": "Start from a pretrained LLM, generate chain-of-thought rationales and correct outputs, then finetune the model on the subset of its outputs that are correct, repeating iteratively.",
            "reasoning_methods": [
                "bootstrapping / self-improvement via generated rationales"
            ],
            "reasoning_methods_description": "Generate CoT rationales and answers; select those that yield correct answers; finetune the model on this self-generated training data to improve future reasoning generation.",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Pipelines compare base model performance vs model finetuned on its own correct rationales across iterations; survey also cites work on self-consistency supporting self-improvement without supervised data.",
            "task_or_benchmark": "Reasoning benchmarks targeted in STaR and follow-ups (math/logic datasets described in cited works)",
            "performance_results": "Survey summarizes that bootstrapping can improve reasoning ability and that iterations produce improved models; exact numbers referenced to original STaR paper.",
            "qualitative_findings": "Self-improvement via bootstrapping helps models generate better rationales over iterations, but relies on being able to identify correct outputs to train on and may propagate subtle biases/artifacts.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Bootstrapping from a model's own correct rationales can enhance performance without external supervised data, especially when combined with strategies that identify high-quality rationales (e.g., self-consistency).",
            "uuid": "e8292.7",
            "source_info": {
                "paper_title": "Towards Reasoning in Large Language Models: A Survey",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Reasoning-Enhanced Training",
            "name_full": "Reasoning-Enhanced Pretraining / Finetuning",
            "brief_description": "Improving LLM reasoning by pretraining/finetuning on datasets containing reasoning-heavy content (e.g., code, SQL, scientific/math corpora), which can boost downstream reasoning when combined with CoT prompting.",
            "citation_title": "Solving quantitative reasoning problems with language models",
            "mention_or_use": "mention",
            "model_name": "LLMs pretrained or continually pretrained with reasoning-style data (e.g., code, SQL, scientific text)",
            "model_description": "Modify pretraining or continual pretraining data to include structured reasoning examples (programs, SQL, math/science text) and/or finetune on CoT-labeled datasets to improve reasoning.",
            "reasoning_methods": [
                "finetuning on reasoning datasets",
                "continual pretraining with code/SQL/scientific data"
            ],
            "reasoning_methods_description": "Train or fine-tune LLMs on corpora that include formalized reasoning or programmatic data (code, SQL, mathematical content) and/or CoT-labeled examples to induce better reasoning capabilities.",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Studies compare standard pretrained models vs those pretrained/finetuned on reasoning-rich corpora and evaluate with CoT prompting, showing improved generalization especially on longer or numeric reasoning.",
            "task_or_benchmark": "Quantitative reasoning, numerical and logical reasoning tasks, and other CoT-evaluated benchmarks",
            "performance_results": "Survey reports that models trained on reasoning-rich data (code, scientific/math corpora) achieve better performance with CoT prompting and generalize better to longer problems; precise metrics are in cited works.",
            "qualitative_findings": "Incorporating structured reasoning data into training increases the model's capacity to perform multi-step and numeric reasoning, and complements prompting techniques (CoT) for better end-task performance.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Training data that includes reasoning-like structure (e.g., code/SQL/math/science) and finetuning with CoT examples can improve LLM reasoning, but such training changes model capabilities rather than only eliciting latent abilities via prompting.",
            "uuid": "e8292.8",
            "source_info": {
                "paper_title": "Towards Reasoning in Large Language Models: A Survey",
                "publication_date_yy_mm": "2022-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 2,
            "sanitized_title": "large_language_models_are_zeroshot_reasoners"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Automatic chain of thought prompting in large language models",
            "rating": 2,
            "sanitized_title": "automatic_chain_of_thought_prompting_in_large_language_models"
        },
        {
            "paper_title": "Complexity-based prompting for multi-step reasoning",
            "rating": 2,
            "sanitized_title": "complexitybased_prompting_for_multistep_reasoning"
        },
        {
            "paper_title": "Least-to-most prompting enables complex reasoning in large language models",
            "rating": 2,
            "sanitized_title": "leasttomost_prompting_enables_complex_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 2,
            "sanitized_title": "training_verifiers_to_solve_math_word_problems"
        },
        {
            "paper_title": "STar: Bootstrapping reasoning with reasoning",
            "rating": 2,
            "sanitized_title": "star_bootstrapping_reasoning_with_reasoning"
        },
        {
            "paper_title": "Solving quantitative reasoning problems with language models",
            "rating": 2,
            "sanitized_title": "solving_quantitative_reasoning_problems_with_language_models"
        },
        {
            "paper_title": "Are large pre-trained language models leaking your personal information?",
            "rating": 1,
            "sanitized_title": "are_large_pretrained_language_models_leaking_your_personal_information"
        }
    ],
    "cost": 0.01799025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Towards Reasoning in Large Language Models: A Survey</p>
<p>Jie Huang 
Department of Computer Science
University of Illinois at Urbana-Champaign</p>
<p>Kevin Chen 
Department of Computer Science
University of Illinois at Urbana-Champaign</p>
<p>Chuan Chang kcchang@illinois.edu 
Department of Computer Science
University of Illinois at Urbana-Champaign</p>
<p>Towards Reasoning in Large Language Models: A Survey
CA8B2B910B2B9C57AD835F2672E95E7E
Reasoning is a fundamental aspect of human intelligence that plays a crucial role in activities such as problem solving, decision making, and critical thinking.In recent years, large language models (LLMs) have made significant progress in natural language processing, and there is observation that these models may exhibit reasoning abilities when they are sufficiently large.However, it is not yet clear to what extent LLMs are capable of reasoning.This paper provides a comprehensive overview of the current state of knowledge on reasoning in LLMs, including techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, findings and implications of previous research in this field, and suggestions on future directions.Our aim is to provide a detailed and up-to-date review of this topic and stimulate meaningful discussion and future work. 1</p>
<p>Introduction</p>
<p>Reasoning is a cognitive process that involves using evidence, arguments, and logic to arrive at conclusions or make judgments.It plays a central role in many intellectual activities, such as problem solving, decision making, and critical thinking.The study of reasoning is important in fields like psychology (Wason and Johnson-Laird, 1972), philosophy (Passmore, 1961), and computer science (Huth and Ryan, 2004), as it helps individuals make decisions, solve problems, and think critically.</p>
<p>Recently, large language models (LLMs) (Brown et al., 2020;Chowdhery et al., 2022;Chung et al., 2022;OpenAI, 2022, inter alia) such as Chat-GPT have made significant advancements in natural language processing and related fields.It has been shown that these models exhibit emergent behaviors, including the ability to "reason", when they are large enough (Wei et al., 2022a).For example, by providing the models with "chain of thoughts", i.e., reasoning exemplars, or a simple prompt "Let's think step by step", these models are able to answer questions with explicit reasoning steps (Wei et al., 2022b;Kojima et al., 2022), e.g., "all whales are mammals, all mammals have kidneys; therefore, all whales have kidneys."This has sparked considerable interest in the community since reasoning ability is a hallmark of human intelligence that is frequently considered missed in current artificial intelligence systems (Marcus, 2020;Russin et al., 2020;Mitchell, 2021;Bommasani et al., 2021).</p>
<p>However, despite the strong performance of LLMs on certain reasoning tasks, it remains unclear whether LLMs are actually reasoning and to what extent they are capable of reasoning.For example, Kojima et al. (2022) claim that "LLMs are decent zero-shot reasoners (p.1)", while Valmeekam et al. (2022) conclude that "LLMs are still far from achieving acceptable performance on common planning/reasoning tasks which pose no issues for humans to do (p.2)."This limitation is also stated by Wei et al. (2022b):</p>
<p>"we qualify that although chain of thought emulates the thought processes of human reasoners, this does not answer whether the neural network is actually reasoning (p.9)."Therefore, in this paper, we aim to provide a comprehensive overview and engage in an insightful discussion on the current state of knowledge on this fast-evolving topic.We initiate our exploration with a clarification of the concept of reasoning ( §2).Subsequently, we turn our attention to the techniques for enhancing/eliciting reasoning in LLMs ( §3), the methods and benchmarks for evaluating reasoning in LLMs ( §4), and the key findings and implications in this field ( §5).Finally, we reflect on and discuss the current state of the field ( §6). 2 What is Reasoning?</p>
<p>Reasoning is the process of thinking about something in a logical and systematic way, using evidence and past experiences to reach a conclusion or make a decision (Wason and Johnson-Laird, 1972;Wason, 1968;Galotti, 1989;Fagin et al., 2004;McHugh and Way, 2018).Reasoning involves making inferences, evaluating arguments, and drawing logical conclusions based on available information.Although "reasoning" is a term that is commonly used in literature and daily life, it is also an abstract concept that can refer to many things.To help the reader better understand this concept, we summarize several main categories of reasoning that are commonly recognized: Deductive reasoning.Deductive reasoning is a type of reasoning in which a conclusion is drawn based on the truth of the premises.In deductive reasoning, the conclusion must necessarily follow from the premises, meaning that if the premises are true, the conclusion must also be true.For example:</p>
<p>• Premise: All mammals have kidneys.</p>
<p>• Premise: All whales are mammals.</p>
<p>• Conclusion: All whales have kidneys.</p>
<p>Inductive reasoning.Inductive reasoning is a type of reasoning in which a conclusion is drawn based on observations or evidence.The conclusion is likely to be true based on the available evidence, but it is not necessarily certain.For example:</p>
<p>• Observation: Every time we see a creature with wings, it is a bird.• Observation: We see a creature with wings.</p>
<p>• Conclusion: The creature is likely to be a bird.</p>
<p>Abductive reasoning.Abductive reasoning is a type of reasoning in which a conclusion is drawn based on the best explanation for a given set of observations.The conclusion is the most likely explanation based on the available evidence, but it is not necessarily certain.For example:</p>
<p>• Observation: The car cannot start and there is a puddle of liquid under the engine.• Conclusion: The most likely explanation is that the car has a leak in the radiator.</p>
<p>Other types of reasoning include analogical reasoning, which involves making comparisons between two or more things in order to make inferences or arrive at conclusions; causal reasoning, which involves identifying and understanding the causes and effects of events or phenomena; and probabilistic reasoning, which involves making decisions or arriving at conclusions based on the likelihood or probability of certain outcomes.</p>
<p>Formal Reasoning vs Informal Reasoning.Formal reasoning is a systematic and logical process that follows a set of rules and principles, often used in mathematics and logic.Informal reasoning is a less structured approach that relies on intuition, experience, and common sense to draw conclusions and solve problems, and is often used in everyday life.Formal reasoning is more structured and reliable, while informal reasoning is more adaptable and open-ended, but may also be less reliable.We refer the reader to Galotti (1989); Bronkhorst et al. (2020) for a detailed distinction between them.</p>
<p>Reasoning in Language Models.The concept of reasoning in language models has been around for some time, but there is not a clear definition of what it entails.In the literature, the term "reasoning" is often used to refer to informal reasoning, although it is not always explicitly stated that it is informal (Cobbe et al., 2021;Wei et al., 2022b, inter alia).Different forms of reasoning may be used depending on the task, benchmark, or method being used, e.g., deductive reasoning (Cobbe et al., 2021;Creswell et al., 2022;Han et al., 2022b, inter alia), inductive reasoning (Yang et al., 2022;Misra et al., 2022, inter alia) or abductive reasoning (Wiegreffe et al., 2022;Lampinen et al., 2022;Jung et al., 2022, inter alia).In this paper, we encompass various forms of reasoning, with a particular focus on "informal deductive reasoning" in large language models since it is a widely used form in which the conclusion is guaranteed to be true as long as the premises are true.</p>
<p>Towards Reasoning in Large Language Models</p>
<p>Reasoning, particularly multi-step reasoning, is often seen as a weakness in language models and other NLP models (Bommasani et al., 2021;Rae et al., 2021;Valmeekam et al., 2022).Recent research has suggested that reasoning ability may emerge in language models at a certain scale, such as models with over 100 billion parameters (Wei et al., 2022a,b;Cobbe et al., 2021).In this paper, we follow Wei et al. (2022a) in considering reasoning as an ability that is rarely present in smallscale models like GPT-2 (Radford et al., 2019) and BERT (Devlin et al., 2019), and therefore focus on techniques applicable to improving or eliciting "reasoning"2 in LLMs such as GPT-3 (Brown et al., 2020) and PaLM (Chowdhery et al., 2022).</p>
<p>Fully Supervised Finetuning</p>
<p>Before discussing reasoning in large language models, it is worth mentioning there is research working on eliciting/improving reasoning in small language models through fully supervised finetuning on specific datasets.For example, Rajani et al.</p>
<p>(2019) finetune a pretrained GPT model (Radford et al., 2018) to generate rationales that explain model predictions with the built CoS-E dataset, and find that models trained with explanations perform better on commonsense question answering tasks (Talmor et al., 2019).Talmor et al. (2020) train RoBERTa (Liu et al., 2019)  There are two major limitations of fully supervised finetuning.First, it requires a dataset containing explicit reasoning, which can be difficult and time-consuming to create.Additionally, the model is only trained on a specific dataset, which limits its application to a specific domain and may result in the model relying on artifacts in the training data rather than actual reasoning to make predictions.</p>
<p>Prompting &amp; In-Context Learning</p>
<p>Large language models such as GPT-3 (Brown et al., 2020) have demonstrated remarkable fewshot performance across a variety of tasks through in-context learning.These models can be prompted with a question and a few ⟨input, output⟩ exemplars to potentially solve a problem through "reasoning", either implicitly or explicitly.However, research has shown that these models still fall short when it comes to tasks that require multiple steps of reasoning to solve (Bommasani et al., 2021;Rae et al., 2021;Valmeekam et al., 2022).This may be due to a lack of exploration into the full capabilities of these models, as recent studies have suggested.</p>
<p>Chain of Thought and Its Variants</p>
<p>To encourage LLMs to engage in reasoning rather than simply providing answers directly, we may guide LLMs to generate "reasoning" explicitly.One approach for doing this is chain-of-thought prompting, proposed by Wei et al. (2022b).This approach involves providing a few examples of "chain of thought" (CoT), which are intermediate natural language reasoning steps, in the prompt to LLMs (Figure 2).Specifically, in CoT prompting, ⟨input, output⟩ demonstrations are replaced with ⟨input, chain of thought, output⟩ triples, e.g., "[input] Roger has 5 tennis balls.He buys 2 more cans of tennis balls.Each can has 3 tennis balls.How many tennis balls does he have now?[chain of thought] Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls.5 + 6 = 11.[output] The answer is 11."In this way, given a target question, the model learns to generate explicit ratio- nale before producing the final answer.Experimental results show that this simple idea can improve LLMs' few-shot performance on arithmetic, symbolic, and commonsense reasoning tasks, sometimes to a striking degree.</p>
<p>There are several variants of chain-of-thought prompting that have been proposed in the literature, in a different form or to solve a specific problem.</p>
<p>Different Form: Kojima et al. (2022) introduce Zero-shot-CoT, in which LLMs are simply prompted with the phrase "Let's think step by step" after the input, in order to elicit reasoning without the need for few-shot demonstrations.Madaan et al. (2022); Gao et al. (2022); Chen et al. (2022) find that LLMs trained with code, e.g., Codex (Chen et al., 2021), can achieve better performance on reasoning tasks by framing reasoning as code generation.Wang et al. (2022a) propose to iteratively prompt chain of thought.He et al. (2023) attempt to retrieve external knowledge in CoT to improve faithfulness of reasoning.</p>
<p>Specific Problem/Setting: Before chain of thought, Nye et al. (2022) also try to use intermediate computations, named "scratchpads", to improve language models' reasoning performance in both finetuning and few-shot regimes, with a particular focus on programs.Shi et al. (2022) attempt to solve multilingual reasoning tasks with CoT in the native language, CoT in English (regardless of the problem language), and CoT in English (with the problem translated to English).Chen (2022) apply CoT to table-based reasoning, finding that LLMs can achieve strong performance on table tasks with only one exemplar.Prystawski et al. (2022) demonstrate that CoT can improve LLMs' performance on paraphrase selection for metaphors.Lu et al. (2022) apply chain of thought to solve multimodal science questions.</p>
<p>Rationale Engineering</p>
<p>The original version of chain-of-thought prompting, proposed by Wei et al. (2022b), relies on manually crafted examples of intermediate reasoning steps and applies greedy decoding in the generation.Rationale engineering aims to more effectively elicit or utilize reasoning in LLMs.This can be achieved through rationale refinement, which involves creating more effective examples of reasoning steps, or through rationale exploration and rationale verification, which involve exploring and verifying the rationales produced by LLMs.A summary of raltionale engineering is illustrated in Figure 2.</p>
<p>Rationale refinement.The choice of exemplars can significantly affect the few-shot performance of LLMs, as demonstrated in research such as Liu et al. (2022b), which also appears in chain-of-thought prompting.Rationale refinement aims to create and refine rationale examples that are better able to elicit reasoning in LLMs.Fu et al. (2022b) propose complexity-based prompting to create rationales with more reasoning steps.Their experiments show that the performance of LLMs improves with the increased rationale complexity.Similarly, Zhou et al. (2022c) propose algorithmic prompting, which suggests that providing more thorough examples of solutions can help improve reasoning performance on some simple math calculations.Zhang et al. (2022b) design Auto-CoT to automatically construct exemplars by partitioning questions from a given dataset into clusters and then using Zero-Shot-CoT (Kojima et al., 2022) to generate the rationale for a representative question from each cluster.The analysis shows that making exemplars diverse is important in prompting LLMs to produce better rationales.</p>
<p>Rationale exploration.In addition to providing better exemplars, we can allow LLMs to fully explore various ways of reasoning to improve their performance on reasoning tasks, named rationale exploration.Based on the idea that complex problems often admit multiple ways of thinking that can lead to their unique correct answer, Wang et al. (2022c) present a decoding strategy called selfconsistency to improve upon the traditional greedy decoding used in chain-of-thought prompting.This strategy involves sampling a diverse set of rationales, rather than just the greedy one, and selecting the most consistent answer by marginalizing out the sampled rationales.The idea is also used in Fu et al. (2022b) to vote over the top complex rationales.To further improve performance, Li et al. (2022b) suggest providing different demonstrations for each question by sampling exemplars from an exemplar base, in order to increase the diversity of the sampled rationales.</p>
<p>Rationale verification.Ensuring that the rationales produced by LLMs are valid is critical, as incorrect rationales can lead to incorrect final predictions (Ye and Durrett, 2022).To address this issue, the process of rationale verification aims to verify whether the rationales produced by LLMs lead to the correct final answers.Cobbe et al. (2021) propose augmenting LLMs with a trained verifier that assigns a score to each rationale and solution generated by the LLM, selecting the highest-ranked solution as the final answer when solving math word problems.Li et al. (2022b) also use this technique to guide rationale selection, in conjunction with the process of rationale exploration.Different from the above methods that train an external verifier to verify the rationales, Weng et al. (2022) suggest using LLMs themselves as the verifiers.</p>
<p>Problem Decomposition</p>
<p>Chain-of-thought prompting, while effective for eliciting reasoning in LLMs, can struggle with complex tasks, e.g., tasks that require compositional generalization (Lake and Baroni, 2018; Keysers et al., 2020).To solve a complex problem, it is helpful to first break it down into smaller, more manageable subproblems.By solving each of these subproblems, we can effectively solve the complex problem.This technique is called problem decom-position or divide and conquer (Talmor and Berant, 2018;Min et al., 2019;Perez et al., 2020).</p>
<p>Based on this idea, Zhou et al. (2022a) propose least-to-most prompting, which consists of two steps: decomposing the complex problem into subproblems and solving these subproblems in a specific order, with each subproblem being facilitated by the answers obtained from previously solved subproblems.As follow-up work, Drozdov et al. (2022) introduce dynamic least-to-most prompting, which is designed to solve more realistic semantic parsing problems by decomposing the problems with prompting-based syntactic parsing and dynamically selecting exemplars based on the decomposition.In addition, Khot et al. (2022) design decomposed prompting, which breaks down a complex problem into subproblems that can be handled by a shared library of prompting-based LLMs, each specialized in a particular subproblem.Furthermore, Dua et al. (2022) develop successive prompting, which iteratively decomposes a complex problem into a simple problem, with the next subproblem prediction having access to the answers to the previous subproblems.While the above methods decompose or solve compositional questions with multiple forward passes, Press et al. (2022) suggest decomposing and solving the input question in one forward pass using CoT prompting.Overall, these techniques show promise for helping LLMs to solve complex tasks by decomposing the problem into more manageable subproblems.</p>
<p>Others</p>
<p>There are other techniques that have been developed to facilitate reasoning in LLMs for specific tasks or settings.For instance, Creswell et al. (2022); Creswell and Shanahan (2022) introduce a selection-inference framework that uses LLMs as modules to select and infer reasoning steps from a set of facts that culminate in the final answer.Kazemi et al. ( 2022) suggest using backward chaining, i.e., from goal to the set of facts that support it, instead of forward chaining like Creswell et al. (2022); Creswell and Shanahan (2022).In addition, Jung et al. (2022) propose a method for solving binary questions by prompting LLMs abductively and recursively to rationalize each option.Zhou et al. (2022b) design a technique for performing numerical reasoning on complex numbers by replacing the complex numbers with simple numbers to produce simpler expressions, and then using these expressions to perform calculations on the complex numbers.There are also efforts to distill reasoning from LLMs into smaller models, such as the work by Li et al. (2022a); Shridhar et al. (2022);Magister et al. (2022).Finally, we refer the reader to Dohan et al. (2022)'s position paper on language model cascade, which presents a unifying framework for understanding chain-of-thought prompting and research in this line.</p>
<p>Hybrid Method</p>
<p>While "prompting" techniques can help elicit or better utilize reasoning in large language models to solve reasoning tasks, they do not actually improve the reasoning capabilities of the LLMs themselves, as the parameters of the models remain unchanged.In contrast, the "hybrid approach" aims to simultaneously improve the reasoning capabilities of LLMs and make better use of these models in order to solve complex problems.This approach involves both enhancing the reasoning capabilities of the LLMs and using techniques such as prompting to effectively utilize these capabilities.</p>
<p>Reasoning-Enhanced Training and Prompting</p>
<p>One approach to improving the reasoning capabilities of LLMs is to pretrain or finetune the models on datasets that include "reasoning".Lewkowycz et al. (2022); Taylor et al. (2022) find that LLMs trained on datasets containing scientific and mathematical data can achieve better performance on reasoning tasks like quantitative reasoning problems when using CoT prompting3 .Pi et al. (2022) show that continually pretraining with SQL data can boost the performance of language models, e.g., T5 (Raffel et al., 2020), on natural language reasoning such as numerical reasoning and logical reasoning.finetuning and scratchpad prompting results in a significant improvement in LLMs' ability to generalize to longer problems, while this phenomenon is not observed in the standard fully supervised finetuning paradigm.</p>
<p>Bootstrapping &amp; Self-Improving</p>
<p>Instead of finetuning LLMs on pre-built datasets that include reasoning, there are studies that have explored the idea of using LLMs to self-improve their reasoning abilities through a process known as bootstrapping.One example of this is the Self-Taught Reasoner (STaR) introduced by Zelikman et al. ( 2022), in which a LLM is trained and refined on its own output iteratively.Specifically, with CoT prompting, the model first generates initial rationales.And then, the model is finetuned on rationales that lead to correct answers.This process can be repeated, with each iteration resulting in an improved model that can generate better training data, which in turn leads to further improvements.As a follow-up to this work, Huang et al. (2022a) show that LLMs are able to self-improve their reasoning abilities without the need for supervised data by leveraging the self-consistency of reasoning (Wang et al., 2022c).</p>
<p>Measuring Reasoning in Large Language Models</p>
<p>We summarize methods and benchmarks for evaluating reasoning abilities of LLMs in this section.</p>
<p>End Task Performance</p>
<p>One way to measure reasoning abilities of LLMs is to report their performance, e.g., accuracy, on end tasks that require reasoning.We list some common benchmarks as follows.</p>
<p>Arithmetic Reasoning.Arithmetic reasoning is the ability to understand and apply mathematical concepts and principles in order to solve problems involving arithmetic operations.This involves using logical thinking and mathematical principles to determine the correct course of action when solving mathematical problems.</p>
<p>Representative benchmarks for arithmetic reasoning include GSM8K (Cobbe et al., 2021), Math (Hendrycks et al., 2021), MathQA (Amini et al., 2019), SVAMP (Patel et al., 2021), AS-Div (Miao et al., 2020), AQuA (Ling et al., 2017), and MAWPS (Roy and Roth, 2015).It is worth mentioning that Anil et al. (2022) generate the Parity Datasets and the Boolean Variable Assignment Dataset for analyzing the length generalization capabilities of LLMs ( §3.3.1).</p>
<p>Commonsense Reasoning.Commonsense Reasoning is the use of everyday knowledge and understanding to make judgments and predictions about new situations.It is a fundamental aspect of human intelligence that enables us to navigate our environment, understand others, and make decisions with incomplete information.Benchmarks that can be used for testing commonsense reasoning abilities of LLMs include CSQA (Talmor et al., 2019), StrategyQA (Geva et al., 2021), and ARC (Clark et al., 2018).We refer the reader to Bhargava and Ng ( 2022)'s survey for more work in this domain.</p>
<p>Symbolic Reasoning.Symbolic reasoning is a form of reasoning that involves the manipulation of symbols according to formal rules.In symbolic reasoning, we use abstract symbols to represent concepts and relationships, and then manipulate those symbols according to precise rules in order to draw conclusions or solve problems.Two benchmarks of symbolic reasoning are presented in Wei et al. (2022b), including Last Letter Concatenation and Coin Flip.</p>
<p>Others.In practice, there are many benchmarks that can be used to evaluate reasoning abilities of LLMs (indirectly), as long as the downstream task involves reasoning.BIG-bench (Srivastava et al., 2022), for example, includes over 200 tasks that test a range of reasoning skills, including tasks like Date Understanding, Word Sorting, and Causal Judgement.Other benchmarks, such as SCAN (Lake and Baroni, 2018) and the one proposed by Anil et al. (2022), focus on evaluating generalization ability.LLMs can also be tested on their table reasoning abilities using benchmarks such as WikiTableQA (Pasupat and Liang, 2015), FetaQA (Nan et al., 2022), as suggested by Chen (2022).In addition, there are benchmarks for evaluating LLMs' generative relational reasoning abilities, such as CommonGen (Lin et al., 2020;Liu et al., 2022a) and Open Relation Modeling (Huang et al., 2022b,d).</p>
<p>Analysis on Reasoning</p>
<p>Although LLMs have demonstrated impressive performance on various reasoning tasks, the extent to which their predictions are based on true reasoning or simple heuristics is not always clear.This is because most existing evaluations focus on their accuracy on end tasks, rather than directly assessing their reasoning steps.While some error analysis has been conducted on the generated rationales of LLMs (Wei et al., 2022b;Kojima et al., 2022, inter alia), this analysis has often been limited in depth.</p>
<p>There have been some efforts to develop metrics and benchmarks that enable a more formal/deep analysis of reasoning in LLMs.Golovneva et al. (2022) design ROSCOE, a set of interpretable, detailed step-by-step evaluation metrics covering various perspectives including semantic alignment, logical inference, semantic similarity, and language coherence.Saparov and He (2022) create a synthetic dataset called PrOntoQA that is generated from real or fictional ontologies.Each example in the dataset has a unique proof, which can be converted to simple sentences and back again, allowing for a formal analysis of each reasoning step.Han et al. (2022a) introduce a dataset called FO-LIO to test the first-order logic reasoning capabilities of LLMs.FOLIO contains first-order logic reasoning problems that require models to determine the correctness of conclusions given a set of premises.In addition, Wang et al. (2022b) conduct ablation experiments on CoT and find that LLMs may also perform reasoning while prompting with invalid rationals.Their study also suggests that being relevant to the query and correctly ordering the reasoning steps are important for CoT prompting.</p>
<p>In summary, most existing studies primarily report the performance of the models on downstream reasoning tasks, without a detailed examination of the quality of the rationales produced.This leaves open the question of whether the models are actually able to reason in a way that is similar to human reasoning, or whether they are simply able to achieve good performance on the tasks through other means.Further research is needed to more formally analyze the reasoning abilities of LLMs.</p>
<p>Findings and Implications</p>
<p>In this section, we summarize the important findings and implications of studies on reasoning in large language models.</p>
<p>Reasoning seems an emergent ability of LLMs.Wei et al. (2022a,b); Suzgun et al. (2022) show that reasoning ability appears to emerge only in large language models like GPT-3 175B, as evidenced by significant improvements in performance on reasoning tasks at a certain scale (e.g., 100 billion parameters).This suggests that it may be more effective to utilize large models for general reasoning problems rather than training small models for specific tasks.However, the reason for this emergent ability is not yet fully understood.We refer the reader to Wei et al. (2022a); Fu et al. (2022a) for some potential explanations.</p>
<p>Chain of thought elicits "reasoning" of LLMs.The use of chain-of-thought (CoT) prompts (Wei et al., 2022b) has been shown to improve the performance of LLMs on various reasoning tasks, as demonstrated in the experiments of Wei et al. (2022a,b); Suzgun et al. (2022).Additionally, Saparov and He (2022) ( §4.2) find that, when using CoT prompts, LLMs are able to produce valid individual proof steps, even when the synthetic ontology is fictional or counterfactual.However, they may sometimes choose the wrong steps when multiple options are available, leading to incomplete or incorrect proofs.Moreover, for many reasoning tasks where the performance of standard prompting grows smoothly with model scale, chain-of-thought prompting can lead to dramatic performance improvement.In addition to these benefits, the use of CoT prompts has been shown to improve the out-ofdistribution robustness of LLMs (Wei et al., 2022b;Zhou et al., 2022a;Anil et al., 2022, inter alia), an advantage that is not typically observed with standard prompting or fully supervised finetuning paradigms.</p>
<p>LLMs show human-like content effects on reasoning.According to Dasgupta et al. (2022), LLMs exhibit reasoning patterns that are similar to those of humans as described in the cognitive literature.For example, the models' predictions are influenced by both prior knowledge and abstract reasoning, and their judgments of logical validity are impacted by the believability of the conclusions.These findings suggest that, although language models may not always perform well on reasoning tasks, their failures often occur in situations that are challenging for humans as well.This provides some evidence that language models may "reason" in a way that is similar to human reasoning.</p>
<p>LLMs are still unskilled at complex reasoning.Although LLMs seem to possess impressive reasoning capabilities with the techniques described in §3, they still struggle with more complex reasoning tasks or those involving implicature, according to studies such as Valmeekam et al. (2022); Han et al. (2022a); Ruis et al. (2022).For instance, Valmeekam et al. (2022) find that even in relatively simple commonsense planning domains that humans would have no trouble navigating, LLMs such as GPT-3 (Brown et al., 2020) and BLOOM (Scao et al., 2022) struggle to perform effectively.These findings suggest that existing benchmarks may be too simple to accurately gauge the true reasoning abilities of LLMs, and that more challenging tasks may be needed to fully evaluate their abilities in this regard.</p>
<p>6 Reflection, Discussion, and Future Directions</p>
<p>Why reasoning?Reasoning is the process of thinking about something in a logical and systematic way, and it is a key aspect of human intelligence.By incorporating reasoning capabilities into language models, we can enable them to perform tasks that require more complex and nuanced thinking, such as problem solving, decision making, and planning (Huang et al., 2022e,f;Song et al., 2022).This can improve the performance of these models on downstream tasks and increase their out-ofdistribution robustness (Wei et al., 2022a,b;Suzgun et al., 2022;Zhou et al., 2022a;Anil et al., 2022).</p>
<p>In addition, reasoning can make language models more explainable and interpretable, as it provides explicit rationales for their predictions.</p>
<p>Right task/application?As Valmeekam et al. (2022) point out, current benchmarks may not adequately reflect the reasoning capabilities of LLMs.</p>
<p>In addition, tasks such as solving simple math problems and concatenating letters in strings ( §4.1) are artificial and do not accurately reflect real-world situations.To truly understand the reasoning ability of LLMs, it is important to consider more realistic and meaningful applications such as decision making (Edwards, 1954), legal reasoning (Levi, 2013), and scientific reasoning (Zimmerman, 2000).Our ultimate goal should not be to enable LLMs to solve simple math problems, which can be simply done with other programs.When conducting relevant research, it is essential to ask whether the specific task being tackled is meaningful and whether the proposed method can be generalized to more realistic tasks and applications.</p>
<p>Are language models really able to reason?</p>
<p>There are several indications that LLMs are able to reason, including 1) high performance on various tasks requiring reasoning (Suzgun et al., 2022);</p>
<p>2) the ability to reason step-by-step with chainof-thought prompting (Wei et al., 2022b); and 3) the reflection of human-like content effects on reasoning (Dasgupta et al., 2022).However, these findings are not sufficient to conclude that LLMs can truly reason.For 1), it is not clear whether the models are making predictions based on reasoning or heuristics (Patel et al., 2021).For many existing benchmarks on reasoning, actually, we can design a program with heuristic rules to achieve very high performance.We usually do not think a program relying on heuristic rules is capable of reasoning.</p>
<p>For 2), although the models seem to reason stepby-step, the generated rationales may be incorrect and inconsistent.It is possible that the models are "generating reasoning-like response" rather than "reasoning step-by-step".For 3), while LLMs display some human-like reasoning patterns, this does not necessarily mean that they behave like humans.Additionally, there are several observations that suggest LLMs may not be capable of reasoning: 1) LLMs still struggle with tasks that require complex reasoning (Valmeekam et al., 2022;Han et al., 2022a;Ruis et al., 2022).If LLMs are really decent reasoners, they should handle tasks that can be simply solved by humans through reasoning; 2) LLMs make mistakes in their reasoning, as explained above; 3) #4 The performance of LLMs on downstream tasks has been found to be sensitive to the frequency of certain terms, such as numbers, in the training data (Razeghi et al., 2022;Jung et al., 2022), which would not be expected if the models were solving mathematical problems through reasoning; 4) # Language models have been found to struggle with associating relevant information that they have memorized (Huang et al., 2022c).</p>
<p>Overall, it is still too early to draw a conclusion about the proposed question.In fact, there is also an ongoing debate about whether language models can actually understand language or capture meaning (Bender and Koller, 2020;Li et al., 2021;Manning, 2022;Piantasodi and Hill, 2022).Further in-depth analysis of factors such as training data, model architecture, and optimization objectives is needed, as well as the development of better benchmarks for measuring the reasoning capabilities of LLMs.However, it is clear that the current models are not yet capable of robust reasoning.</p>
<p>Improving reasoning capabilities of LLMs.</p>
<p>While techniques like chain-of-thought prompting (Wei et al., 2022b) may help to elicit reasoning abilities in large language models, they cannot enable the models to solve tasks beyond their current capabilities.To truly enhance reasoning in LLMs, we need to utilize training data, model architecture, and optimization objectives that are designed to encourage reasoning.For example, finetuning a model with a dataset including CoT data has been shown to improve reasoning (Chung et al., 2022), and models can also self-improve through the process of bootstrapping their reasoning (Zelikman et al., 2022;Huang et al., 2022a).There is still much research that needs to be done in this area, and we look forward to future progress in improving reasoning in large language models.</p>
<p>Conclusion</p>
<p>In this paper, we have provided a detailed and upto-date review of the current state of knowledge on reasoning in large language models.We have discussed techniques for improving and eliciting reasoning in LLMs, methods and benchmarks for evaluating reasoning abilities, and the findings and implications of previous studies in this topic.While LLMs have made significant progress in natural language processing and related fields, it remains unclear to what extent they are capable of true reasoning or whether they are simply using memorized patterns and heuristics to solve problems.Further research is needed to fully understand the reasoning abilities of LLMs, improve LLMs' reasoning capabilities, and determine their potential for use in a variety of applications.We hope that this paper will serve as a useful overview of the current state of the field and stimulate further discussion and research on this interesting and important topic.</p>
<p>Limitations</p>
<p>In this paper, we provide an overview of the current state of knowledge on reasoning in large language models.Reasoning is a broad concept that encompasses various forms, making it impractical to summarize all related work in a single paper.Therefore, we focus on deductive reasoning, as it is the most commonly studied in the literature.Other forms of reasoning such as inductive reasoning (Yang et al., 2022;Misra et al., 2022, inter alia) and abductive reasoning (Wiegreffe et al., 2022;Lampinen et al., 2022;Jung et al., 2022, inter alia) may not be discussed in depth.</p>
<p>Additionally, given the rapid evolution and significance of reasoning within large language models, it is crucial to note that new contributions may have emerged in the field concurrent with the writing of this paper.An additional resource to consider is a parallel survey by Qiao et al. (2022), which emphasizes reasoning via language model prompting.Our coverage may not extend to papers released during or after 2023 such as evaluation on Chat-GPT (Bang et al., 2023;Zheng et al., 2023).As such, we recommend readers to check the papers that cite this survey for a more comprehensive and updated understanding of this field.</p>
<p>C2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values?Not applicable.Left blank.</p>
<p>C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run?Not applicable.Left blank.</p>
<p>C4.If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)?Not applicable.Left blank.</p>
<p>D Did you use human annotators (e.g., crowdworkers) or research with human participants?Left blank.</p>
<p>D1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.?Not applicable.Left blank.</p>
<p>D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)?Not applicable.Left blank.</p>
<p>D3. Did you discuss whether and how consent was obtained from people whose data you're using/curating?For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?Not applicable.Left blank.</p>
<p>D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?Not applicable.Left blank.</p>
<p>D5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data?Not applicable.Left blank.</p>
<p>Figure 1 :
1
Figure 1: The structure of the paper.</p>
<p>Figure 2 :
2
Figure 2: An illustration of Chain-of-Thought Prompting and Rationale Engineering, where asterisk (*) denotes the target problem to be solved.</p>
<p>Paperlist can be found at https://github.com/ jeffhj/LM-reasoning.
It is important to note that the term "reasoning" in this paper does not necessarily imply that LLMs are truly capable of reasoning or that they are able to reason in the same way that humans do. We will discuss this issue in more detail in §6.
This may also be true for models trained with code(Chen et al., 2021;Fu et al., 2022a).</p>
<h1>indicates the finding has not been carefully examined in language models with more than 100 billion parameters.</h1>
<p>AcknowledgementsWe would like to thank Jason Wei (OpenAI)  and Denny Zhou (Google DeepMind) for their valuable advice and constructive feedback on this work.This material is based upon work supported by the National Science Foundation IIS 16-19302 and IIS 16-33755, Zhejiang University ZJU Research 083650, IBM-Illinois Center for Cognitive Computing Systems Research (C3SR) and IBM-Illinois Discovery Accelerator Institute (IIDAI), gift grants from eBay and Microsoft Azure, UIUC OVCR CCIL Planning Grant 434S34, UIUC CSBS Small Grant 434C8U, and UIUC New Frontiers Initiative.Any opinions, findings, and conclusions or recommendations expressed in this publication are those of the author(s) and do not necessarily reflect the views of the funding agencies.We wrote part of the appendix with ChatGPT assistance (e.g., to generate an initial description for commonsense reasoning).The generated text is carefully revised and examined by the authors.BDid you use or create scientific artifacts?Not applicable.Left blank.B1. Did you cite the creators of artifacts you used?Not applicable.Left blank.B2. Did you discuss the license or terms for use and / or distribution of any artifacts?Not applicable.Left blank.B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified?For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)?Not applicable.Left blank.B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it?Not applicable.Left blank.B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.?Not applicable.Left blank.B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created?Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results.For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be.Not applicable.Left blank.C Did you run computational experiments?Left blank.C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used?No response.The Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.
MathQA: Towards interpretable math word problem solving with operation-based formalisms. Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, Hannaneh Hajishirzi, 10.18653/v1/N19-1245Proceedings of the 2019 Conference of the North American Chapter. the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American ChapterMinneapolis, MinnesotaAssociation for Computational Linguistics20191Long and Short Papers</p>
<p>Exploring length generalization in large language models. Cem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, Behnam Neyshabur, ArXiv preprint, abs/2207.049012022</p>
<p>A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, ArXiv preprint, abs/2302.040232023</p>
<p>Climbing towards NLU: On meaning, form, and understanding in the age of data. Emily M Bender, Alexander Koller, 10.18653/v1/2020.acl-main.463Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>Commonsense knowledge reasoning and generation with pretrained language models: A survey. Prajjwal Bhargava, Vincent Ng, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2022</p>
<p>On the opportunities and risks of foundation models. Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney Von Arx, Jeannette Michael S Bernstein, Antoine Bohg, Emma Bosselut, Brunskill, abs/2108.07258ArXiv. 2021</p>
<p>Logical reasoning in formal and everyday reasoning tasks. Hugo Bronkhorst, Gerrit Roorda, International Journal of Science and Mathematics Education. 1882020Cor Suhre, and Martin Goedhart</p>
<p>Alec Radford, Ilya Sutskever, and Dario Amodei. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish; NeurIPS2020. 2020. 2020. December 6-12, 2020Language models are few-shot learners</p>
<p>Evaluating large language models trained on code. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, preprint, abs/2107.033742021</p>
<p>Large language models are few (1)-shot table reasoners. Wenhu Chen, abs/2210.067102022ArXiv preprint</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Wenhu Chen, Xueguang Ma, Xinyi Wang, William W Cohen, abs/2211.125882022ArXiv preprint</p>
<p>Palm: Scaling language modeling with pathways. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, ArXiv preprint, abs/2204.023112022</p>
<p>Scaling instruction-finetuned language models. Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, ArXiv preprint, abs/2210.114162022</p>
<p>Think you have solved question answering? try arc, the ai2 reasoning challenge. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, ArXiv preprint, abs/1803.054572018</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, abs/2110.14168ArXiv preprint. 2021</p>
<p>Faithful reasoning using large language models. Antonia Creswell, Murray Shanahan, abs/2208.142712022ArXiv preprint</p>
<p>Selection-inference: Exploiting large language models for interpretable logical reasoning. Antonia Creswell, Murray Shanahan, Irina Higgins, abs/2205.097122022ArXiv preprint</p>
<p>Language models show human-like content effects on reasoning. Ishita Dasgupta, Stephanie Cy Andrew K Lampinen, Antonia Chan, Dharshan Creswell, James L Kumaran, Felix Mcclelland, Hill, preprint, abs/2207.070512022</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>. David Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael Gontijo Lopes, Yuhuai Wu, Henryk Michalewski, Jascha Sohl-Dickstein. et al. 2022. Language model cascades. ArXiv preprint, abs/2207.10342</p>
<p>Compositional semantic parsing with large language models. Andrew Drozdov, Nathanael Schärli, Ekin Akyürek, Nathan Scales, Xinying Song, Xinyun Chen, Olivier Bousquet, Denny Zhou, abs/2209.15003ArXiv preprint. 2022</p>
<p>Successive prompting for decomposing complex questions. Dheeru Dua, Shivanshu Gupta, Sameer Singh, Matt Gardner, abs/2212.040922022ArXiv preprint</p>
<p>The theory of decision making. Ward Edwards, Psychological bulletin. 5143801954</p>
<p>Ronald Fagin, Yoram Joseph Y Halpern, Moshe Moses, Vardi, Reasoning about knowledge. MIT press2004</p>
<p>How does gpt obtain its ability? tracing emergent abilities of language models to their sources. Yao Fu, Hao Peng, Tushar Khot, 2022a</p>
<p>Complexity-based prompting for multi-step reasoning. Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, Tushar Khot, abs/2210.00720ArXiv preprint. 2022b</p>
<p>Approaches to studying formal and everyday reasoning. Kathleen M Galotti, Psychological bulletin. 10533311989</p>
<p>Pal: Program-aided language models. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig, abs/2211.10435ArXiv preprint. 2022</p>
<p>Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, Jonathan Berant, 10.1162/tacl_a_00370Transactions of the Association for Computational Linguistics. 92021</p>
<p>Roscoe: A suite of metrics for scoring step-by-step reasoning. Olga Golovneva, Moya Chen, Spencer Poff, Martin Corredor, Luke Zettlemoyer, Maryam Fazel-Zarandi, Asli Celikyilmaz, abs/2212.07919ArXiv preprint. 2022</p>
<p>Folio: Natural language reasoning with firstorder logic. Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy Sun, Ekaterina Zubova, Yujie Qiao, Matthew Burtell, abs/2209.00840ArXiv preprint. 2022a</p>
<p>Human-like property induction is a challenge for large language models. Simon Jerome, Han , Keith Ransom, Andrew Perfors, Charles Kemp, 2022b</p>
<p>Rethinking with retrieval: Faithful large language model inference. Hangfeng He, Hongming Zhang, Dan Roth, abs/2301.003032023ArXiv preprint</p>
<p>Reasoning with transformer-based models: Deep learning, but shallow reasoning. Chadi Helwe, Chloé Clavel, Fabian M Suchanek, 3rd Conference on Automated Knowledge Base Construction. 2021</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks. the Neural Information Processing Systems Track on Datasets and Benchmarks20211</p>
<p>Large language models can self-improve. Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, Jiawei Han, abs/2210.11610ArXiv preprint. 2022a</p>
<p>Open relation modeling: Learning to define relations between entities. Jie Huang, Kevin Chang, Jinjun Xiong, Wen-Mei Hwu, 10.18653/v1/2022.findings-acl.26Findings of the Association for Computational Linguistics: ACL 2022. Dublin, IrelandAssociation for Computational Linguistics2022b</p>
<p>Are large pre-trained language models leaking your personal information?. Jie Huang, Hanyin Shao, Kevin Chen, -Chuan Chang, Findings of the Association for Computational Linguistics: EMNLP 2022. Abu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022c</p>
<p>DEER: Descriptive knowledge graph for explaining entity relationships. Jie Huang, Kerui Zhu, Kevin Chen-Chuan, Jinjun Chang, Wen-Mei Xiong, Hwu, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022d</p>
<p>Language models as zeroshot planners: Extracting actionable knowledge for embodied agents. Wenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch, Proceedings of the 39th International Conference on Machine Learning. the 39th International Conference on Machine LearningPMLR2022e162</p>
<p>Inner monologue: Embodied reasoning through planning with language models. Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, 2022 Conference on Robot Learning. 2022f</p>
<p>Michael Huth, Mark Ryan, Logic in Computer Science: Modelling and reasoning about systems. Cambridge university press2004</p>
<p>Maieutic prompting: Logically consistent reasoning with recursive explanations. Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra Bhagavatula, Ronan Le Bras, Yejin Choi, The 2022 Conference on Empirical Methods for Natural Language Processing. 2022</p>
<p>Lambada: Backward chaining for automated reasoning in natural language. Najoung Seyed Mehran Kazemi, Deepti Kim, Xin Bhatia, Deepak Xu, Ramachandran, abs/2212.13894ArXiv preprint. 2022</p>
<p>Measuring compositional generalization: A comprehensive method on realistic data. Daniel Keysers, Nathanael Schärli, Nathan Scales, Hylke Buisman, Daniel Furrer, Sergii Kashubin, Nikola Momchev, Danila Sinopalnikov, Lukasz Stafiniak, Tibor Tihon, Dmitry Tsarkov, Xiao Wang, Marc Van Zee, Olivier Bousquet, 8th International Conference on Learning Representations. Addis Ababa, Ethiopia2020. April 26-30, 20202020OpenReview.net</p>
<p>Decomposed prompting: A modular approach for solving complex tasks. Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, Ashish Sabharwal, abs/2210.02406ArXiv preprint. 2022</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in Neural Information Processing Systems. 2022</p>
<p>Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks. M Brenden, Marco Lake, Baroni, Proceedings of the 35th International Conference on Machine Learning, ICML 2018. the 35th International Conference on Machine Learning, ICML 2018Stockholmsmässan, Stockholm, SwedenPMLR2018. July 10-15, 201880of Proceedings of Machine Learning Research</p>
<p>Can language models learn from explanations in context?. Ishita Andrew K Lampinen, Dasgupta, C Y Stephanie, Kory Chan, Michael Henry Matthewson, Antonia Tessler, James L Creswell, Jane X Mcclelland, Felix Wang, Hill, 2022In Findings of the Association for Computational Linguistics: EMNLP 2022</p>
<p>An introduction to legal reasoning. Levi Edward, 2013University of Chicago Press</p>
<p>Theo Gutman-Solo, et al. 2022. Solving quantitative reasoning problems with language models. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag. ArXiv preprint</p>
<p>Implicit representations of meaning in neural language models. Belinda Z Li, Maxwell Nye, Jacob Andreas, 10.18653/v1/2021.acl-long.143Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline. Association for Computational Linguistics20211</p>
<p>Explanations from large language models make small reasoners better. Shiyang Li, Jianshu Chen, Yelong Shen, Zhiyu Chen, Xinlu Zhang, Zekun Li, Hong Wang, Jing Qian, Baolin Peng, Yi Mao, ArXiv preprint, abs/2210.067262022a</p>
<p>On the advance of making language models better reasoners. Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, Weizhu Chen, abs/2206.02336ArXiv preprint. 2022b</p>
<p>CommonGen: A constrained text generation challenge for generative commonsense reasoning. Wangchunshu Bill Yuchen Lin, Ming Zhou, Pei Shen, Chandra Zhou, Yejin Bhagavatula, Xiang Choi, Ren, 10.18653/v1/2020.findings-emnlp.165Findings of the Association for Computational Linguistics: EMNLP 2020. Online. Association for Computational Linguistics2020</p>
<p>Program induction by rationale generation: Learning to solve and explain algebraic word problems. Wang Ling, Dani Yogatama, Chris Dyer, Phil Blunsom, 10.18653/v1/P17-1015Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational Linguistics20171</p>
<p>Dimongen: Diversified generative commonsense reasoning for explaining concept relationships. Chenzhengyi Liu, Jie Huang, Kerui Zhu, Kevin Chen, -Chuan Chang, abs/2212.105452022aArXiv preprint</p>
<p>What makes good in-context examples for GPT-3?. Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, Weizhu Chen, 10.18653/v1/2022.deelio-1.10The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures. Dublin, Ireland and OnlineAssociation for Computational Linguistics2022b. DeeLIO 2022Proceedings of Deep Learning Inside Out</p>
<p>Roberta: A robustly optimized bert pretraining approach. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, ArXiv preprint. 2019. 1907.11692</p>
<p>Learn to explain: Multimodal reasoning via thought chains for science question answering. Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, Ashwin Kalyan, Advances in Neural Information Processing Systems. 2022</p>
<p>Language models of code are few-shot commonsense learners. Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, Graham Neubig, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP)2022</p>
<p>Teaching small language models to reason. Lucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, Aliaksei Severyn, abs/2212.08410ArXiv preprint. 2022</p>
<p>Human language understanding &amp; reasoning. D Christopher, Manning, Daedalus. 15122022</p>
<p>The next decade in ai: four steps towards robust artificial intelligence. Gary Marcus, abs/2002.061772020ArXiv preprint</p>
<p>What is reasoning? Mind. Conor Mchugh, Jonathan Way, 2018127</p>
<p>A diverse corpus for evaluating and developing English math word problem solvers. Chao-Chun Shen-Yun Miao, Keh-Yih Liang, Su, 10.18653/v1/2020.acl-main.92Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>Multi-hop reading comprehension through question decomposition and rescoring. Sewon Min, Victor Zhong, Luke Zettlemoyer, Hannaneh Hajishirzi, 10.18653/v1/P19-1613Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational Linguistics2019</p>
<p>A property induction framework for neural language models. Kanishka Misra, Julia Taylor Rayz, Allyson Ettinger, abs/2205.069102022ArXiv preprint</p>
<p>Abstraction and analogymaking in artificial intelligence. Melanie Mitchell, Annals of the New York Academy of Sciences. 150512021</p>
<p>Caiming Xiong, Dragomir Radev, and Dragomir Radev. Linyong Nan, Chiachun Hsieh, Ziming Mao, Xi Victoria Lin, Neha Verma, Rui Zhang, Wojciech Kryściński, Hailey Schoelkopf, Riley Kong, Xiangru Tang, Mutethia Mutuma, Ben Rosand, Isabel Trindade, Renusree Bandaru, Jacob Cunningham, 10.1162/tacl_a_00446Transactions of the Association for Computational Linguistics. 102022FeTaQA: Free-form table question answering</p>
<p>Show your work: Scratchpads for intermediate computation with language models. Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, Augustus Odena, Deep Learning for Code Workshop. 2022</p>
<p>Chatgpt: Optimizing language models for dialogue. 2022OpenAI</p>
<p>Philosophical reasoning. Panupong Pasupat and Percy Liang. John Arthur, Passmore , 10.3115/v1/P15-1142Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing. Long Papers. the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language ProcessingBeijing, ChinaAssociation for Computational Linguistics1961. 20151Compositional semantic parsing on semi-structured tables</p>
<p>Are NLP models really able to solve simple math word problems?. Arkil Patel, Satwik Bhattamishra, Navin Goyal, 10.18653/v1/2021.naacl-main.168Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesOnline. Association for Computational Linguistics2021</p>
<p>Unsupervised question decomposition for question answering. Ethan Perez, Patrick Lewis, Wen-Tau Yih, Kyunghyun Cho, Douwe Kiela, 10.18653/v1/2020.emnlp-main.713Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>Reasoning like program executors. Xinyu Pi, Qian Liu, Bei Chen, Morteza Ziyadi, Zeqi Lin, Yan Gao, Qiang Fu, Jian-Guang Lou, Weizhu Chen, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP)2022</p>
<p>Meaning without reference in large language models. T Steven, Felix Piantasodi, Hill, abs/2208.029572022ArXiv preprint</p>
<p>Measuring and narrowing the compositionality gap in language models. Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, Mike Lewis, abs/2210.03350ArXiv preprint. 2022</p>
<p>Psychologically-informed chain-of-thought prompts for metaphor understanding in large language models. Ben Prystawski, Paul Thibodeau, Noah Goodman, abs/2209.08141ArXiv preprint. 2022</p>
<p>Reasoning with language model prompting: A survey. Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, Huajun Chen, abs/2212.09597ArXiv preprint. 2022</p>
<p>Improving language understanding by generative pre-training. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, 2018</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 1892019</p>
<p>Scaling language models: Methods, analysis &amp; insights from training gopher. Sebastian Jack W Rae, Trevor Borgeaud, Katie Cai, Jordan Millican, Francis Hoffmann, John Song, Sarah Aslanides, Roman Henderson, Susannah Ring, Young, ArXiv preprint, abs/2112.114462021</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, J. Mach. Learn. Res. 211402020</p>
<p>Explain yourself! leveraging language models for commonsense reasoning. Nazneen Fatema Rajani, Bryan Mccann, Caiming Xiong, Richard Socher, 10.18653/v1/P19-1487Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational Linguistics2019</p>
<p>Impact of pretraining term frequencies on few-shot reasoning. Yasaman Razeghi, Robert L Logan, I V , Matt Gardner, Sameer Singh, abs/2202.07206ArXiv preprint. 2022</p>
<p>Solving general arithmetic word problems. Subhro Roy, Dan Roth, 10.18653/v1/D15-1202Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingLisbon, PortugalAssociation for Computational Linguistics2015</p>
<p>Large language models are not zero-shot communicators. Laura Ruis, Akbir Khan, Stella Biderman, Sara Hooker, Tim Rocktäschel, Edward Grefenstette, abs/2210.14986ArXiv preprint. 2022</p>
<p>Deep learning needs a prefrontal cortex. Jacob Russin, C O' Randall, Yoshua Reilly, Bengio, Work Bridging AI Cogn Sci. 1072020</p>
<p>Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. Abulhair Saparov, He He, abs/2210.01240ArXiv preprint. 2022</p>
<p>Bloom: A 176b-parameter open-access multilingual language model. Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, ArXiv preprint, abs/2211.051002022</p>
<p>Language models are multilingual chain-of-thought reasoners. Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, abs/2210.03057ArXiv preprint. 2022</p>
<p>Distilling multi-step reasoning capabilities of large language models into smaller models via semantic decompositions. Kumar Shridhar, Alessandro Stolfo, Mrinmaya Sachan, abs/2212.001932022ArXiv preprint</p>
<p>Llm-planner: Few-shot grounded planning for embodied agents with large language models. Hee Chan, Jiaman Song, Clayton Wu, Brian M Washington, Wei-Lun Sadler, Yu Chao, Su, ArXiv preprint, abs/2212.040882022</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam Adam R Brown, Aditya Santoro, Adrià Gupta, Garriga-Alonso, 2022ArXiv preprint, abs/2206.04615</p>
<p>Challenging big-bench tasks and whether chain-of-thought can solve them. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Ed H Quoc V Le, Denny Chi, Zhou, ArXiv preprint, abs/2210.092612022</p>
<p>The web as a knowledge-base for answering complex questions. Alon Talmor, Jonathan Berant, 10.18653/v1/N18-1059Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesLouisianaNew Orleans20181Association for Computational Linguistics</p>
<p>CommonsenseQA: A question answering challenge targeting commonsense knowledge. Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, 10.18653/v1/N19-1421Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>Leap-of-thought: Teaching pre-trained models to systematically reason over implicit knowledge. Alon Talmor, Oyvind Tafjord, Peter Clark, Yoav Goldberg, Jonathan Berant, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems. NeurIPS2020. 2020. 2020. December 6-12, 2020</p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, Robert Stojnic, preprint, abs/2211.09085Galactica: A large language model for science. 2022</p>
<p>Large language models still can't plan (a benchmark for llms on planning and reasoning about change). Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, Subbarao Kambhampati, NeurIPS 2022 Foundation Models for Decision Making Workshop. 2022</p>
<p>Iteratively prompt pre-trained language models for chain of thought. Boshi Wang, Xiang Deng, Huan Sun, The 2022 Conference on Empirical Methods for Natural Language Processing. 2022a</p>
<p>Towards understanding chain-of-thought prompting: An empirical study of what matters. Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, Huan Sun, abs/2212.10001ArXiv preprint. 2022b</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Denny Zhou, abs/2203.11171ArXiv preprint. 2022c</p>
<p>Reasoning about a rule. Peter C Wason, Quarterly journal of experimental psychology. 2031968</p>
<p>Psychology of reasoning: Structure and content. Peter Cathcart Wason and Philip Nicholas Johnson-Laird1972Harvard University Press86</p>
<p>Emergent abilities of large language models. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Transactions on Machine Learning Research. 2022a</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed H Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 2022b</p>
<p>Large language models are reasoners with self-verification. Yixuan Weng, Minjun Zhu, Shizhu He, Kang Liu, Jun Zhao, abs/2212.095612022ArXiv preprint</p>
<p>Reframing human-AI collaboration for generating free-text explanations. Sarah Wiegreffe, Jack Hessel, Swabha Swayamdipta, Mark Riedl, Yejin Choi, 10.18653/v1/2022.naacl-main.47Proceedings of the 2022 Conference of the North American Chapter. the 2022 Conference of the North American ChapterSeattle, United StatesAssociation for Computational Linguistics2022</p>
<p>Language models as inductive reasoners. Zonglin Yang, Li Dong, Xinya Du, Hao Cheng, Erik Cambria, Xiaodong Liu, Jianfeng Gao, Furu Wei, abs/2212.10923ArXiv preprint. 2022</p>
<p>The unreliability of explanations in few-shot prompting for textual reasoning. Xi Ye, Greg Durrett, Advances in neural information processing systems. 2022</p>
<p>Alert: Adapting language models to reasoning tasks. Ping Yu, Tianlu Wang, Olga Golovneva, Badr Alkhamissy, Gargi Ghosh, Mona Diab, Asli Celikyilmaz, abs/2212.08286ArXiv preprint. 2022</p>
<p>STar: Bootstrapping reasoning with reasoning. Eric Zelikman, Yuhuai Wu, Jesse Mu, Noah Goodman, Advances in Neural Information Processing Systems. 2022</p>
<p>Opt: Open pre-trained transformer language models. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, ArXiv preprint, abs/2205.010682022a</p>
<p>Automatic chain of thought prompting in large language models. Zhuosheng Zhang, Aston Zhang, Mu Li, Alex Smola, abs/2210.034932022bArXiv preprint</p>
<p>Why does chatgpt fall short in providing truthful answers?. Shen Zheng, Jie Huang, Kevin Chen, -Chuan Chang, ArXiv preprint, abs/2304.105132023</p>
<p>Least-to-most prompting enables complex reasoning in large language models. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, Ed Chi, ArXiv preprint, abs/2205.106252022a</p>
<p>Reflection of thought: Inversely eliciting numerical reasoning in language models via solving linear systems. Fan Zhou, Haoyu Dong, Qian Liu, Zhoujun Cheng, Shi Han, Dongmei Zhang, abs/2210.05075ArXiv preprint. 2022b</p>
<p>Teaching algorithmic reasoning via incontext learning. Hattie Zhou, Azade Nova, Hugo Larochelle, Aaron Courville, Behnam Neyshabur, Hanie Sedghi, 2022cArXiv preprint, abs/2211.09066</p>
<p>The development of scientific reasoning skills. Corinne Zimmerman, Developmental review. 2012000</p>            </div>
        </div>

    </div>
</body>
</html>
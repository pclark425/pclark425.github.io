<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5329 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5329</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5329</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-112.html">extraction-schema-112</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <p><strong>Paper ID:</strong> paper-268041290</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.18225v1.pdf" target="_blank">CogBench: a large language model walks into a psychology lab</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have significantly advanced the field of artificial intelligence. Yet, evaluating them comprehensively remains challenging. We argue that this is partly due to the predominant focus on performance metrics in most benchmarks. This paper introduces CogBench, a benchmark that includes ten behavioral metrics derived from seven cognitive psychology experiments. This novel approach offers a toolkit for phenotyping LLMs' behavior. We apply CogBench to 35 LLMs, yielding a rich and diverse dataset. We analyze this data using statistical multilevel modeling techniques, accounting for the nested dependencies among fine-tuned versions of specific LLMs. Our study highlights the crucial role of model size and reinforcement learning from human feedback (RLHF) in improving performance and aligning with human behavior. Interestingly, we find that open-source models are less risk-prone than proprietary models and that fine-tuning on code does not necessarily enhance LLMs' behavior. Finally, we explore the effects of prompt-engineering techniques. We discover that chain-of-thought prompting improves probabilistic reasoning, while take-a-step-back prompting fosters model-based behaviors.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5329.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5329.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Proprietary large transformer-based language model from OpenAI; evaluated deterministically (temperature=0) in this paper across a battery of seven cognitive-psychology tasks (CogBench).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary OpenAI LLM evaluated via in-context learning (no fine-tuning) with deterministic decoding; listed in paper's model table (uses RLHF), treated as a high-parameter, state-of-the-art model in the benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1760</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>CogBench battery (probabilistic reasoning; horizon; restless bandit; instrumental learning; two-step task; temporal discounting; BART)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>reasoning, decision-making, learning, meta-cognition, temporal discounting, risk</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>A suite of seven canonical cognitive-psychology experiments adapted to text prompts for LLMs: probabilistic belief updating (prior/likelihood weighting), horizon two-armed bandit (directed/random exploration), restless bandit (meta-cognition), instrumental learning (learning rates and optimism bias), two-step task (model-based vs model-free RL), temporal discounting (delay preference), and Balloon Analogue Risk Task (risk-taking).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>GPT-4 achieved human-level performance on most tasks (reported as human-level in 5 out of 6 performance metrics used in main figure); it solved the restless bandit (one of the few models that did), excelled in probabilistic reasoning and instrumental learning, displayed very high model-basedness (reported to significantly surpass human levels), performed strongly on the horizon task (most models including GPT-4 were super-human on horizon), and like all models struggled on the BART (risk task).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human-average used as normalization baseline (value 1 = average human); authors state GPT-4 reached human-level (≈1.0 normalized) in five of six reported performance metrics in the main figure (human data sourced from canonical cognitive studies; exact numeric human scores not enumerated in text).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>GPT-4 matches or exceeds humans on most CogBench performance metrics (human-level in 5/6), and significantly exceeds human levels on model-basedness according to the authors; it is among the few models able to solve the restless bandit task comparably to humans. No per-task p-values for GPT-4 vs humans are provided, but authors report significance-level summaries (e.g., 'GPT-4 significantly surpassing human levels' for model-basedness).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Despite high performance, GPT-4 (like other LLMs) shows behavioral divergences from humans: LLMs overweight priors relative to likelihoods, exhibit strong optimism bias and high learning rates, and often achieve high performance without human-like exploration strategies (exploitation-dominant). GPT-4 still struggles in risk-taking (BART) where models show extreme behaviors. Results are from in-context evaluation (no fine-tuning) with temperature=0, and some comparisons are human-normalized rather than raw-score matched.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CogBench: a large language model walks into a psychology lab', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5329.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5329.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude-1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Anthropic Claude-1</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Proprietary LLM from Anthropic that was evaluated with CogBench and reported to achieve human-level performance on many tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude-1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Anthropic proprietary LLM (listed as using RLAIF in the paper's table) evaluated with deterministic in-context prompts across the CogBench tasks; included in the authors' set of 35 models.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>100</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>CogBench battery (see GPT-4 entry)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>reasoning, decision-making, learning, meta-cognition, temporal discounting, risk</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Same seven cognitive-psychology tasks adapted as text prompts for LLMs (see GPT-4).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Claude-1 is reported alongside GPT-4 as achieving human-level scores in most tasks (human-level in 5 out of 6 performance metrics in the main figure). It is one of the exceptions that performed well on the restless bandit task and demonstrated strong performance in probabilistic reasoning and instrumental learning.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human-average normalized baseline (value 1 corresponds to average human). Authors state Claude-1 reached human-level performance across the majority of tasks (same normalization as for GPT-4); no raw human numeric details provided in text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Claude-1 matched human-level performance across most performance metrics and, like GPT-4, was among the few models able to solve the restless bandit. No per-task statistical tests reported at the individual-model level beyond aggregate descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Although Claude-1 attains high performance, the paper notes general LLM behavioral patterns (e.g., strong prior weighting, optimism bias) that apply to most models. Claude models are reported as RLAIF-trained, and the paper highlights RLHF/RL-like training as associated with more humanlike behavior and improved meta-cognition.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CogBench: a large language model walks into a psychology lab', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5329.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5329.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude-2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Anthropic Claude-2</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Later Anthropic model (Claude-2) included in the evaluation; used in prompt-engineering experiments (CoT, SB) alongside other large models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Anthropic proprietary LLM (listed as RLAIF in the paper); evaluated via the same in-context deterministic protocol; included in both baseline and prompt-engineering comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>200</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>CogBench battery (baseline and with prompt-engineering: chain-of-thought and take-a-step-back)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>reasoning, decision-making (including probabilistic reasoning and model-basedness)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Same battery; additionally used in targeted experiments testing the effect of chain-of-thought (CoT) and take-a-step-back (SB) prompting on probabilistic reasoning (posterior accuracy) and model-basedness.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Claude-2 is among models evaluated with CoT and SB prompting. Aggregated across five evaluated models (GPT-4, PaLM-2 text-bison, Claude-1/2, LLaMA-2), CoT increased posterior accuracy by +9.01% on average and SB by +3.10%; for model-basedness CoT increased it by +64.59% and SB by +118.59% (these model-basedness improvements are aggregated weighted averages across the five models, not per-model breakdowns in text).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human-average normalization used for baseline comparisons in the paper; explicit human numerical values per prompt condition are not provided in the text for Claude-2 specifically.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Claude-2 benefits from prompt-engineering (CoT and SB) in the aggregated analysis, implying improved probabilistic reasoning and model-basedness relative to its own baseline; no per-model statistical tests are reported in the manuscript for Claude-2 alone.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>The CoT and SB effects are reported as aggregated across five large models; per-model variance exists but per-model results are not provided in-text. Authors note automation challenges and potential chaotic outputs for some LLMs when allowing multi-token internal reasoning (token limits imposed).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CogBench: a large language model walks into a psychology lab', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5329.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5329.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>text-davinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI text-davinci-003 (GPT-3 family)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GPT-3–era OpenAI model included in the evaluation; performed well on probabilistic reasoning and instrumental learning and showed comparatively far-sighted temporal discounting behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-3 family model (listed in paper's table), evaluated via in-context prompting with deterministic decoding; included among proprietary models tested.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>170</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>CogBench battery (notably probabilistic reasoning, instrumental learning, temporal discounting, BART)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>reasoning, learning, temporal discounting, risk</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Same battery as above.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>text-davinci-003 is reported among models that excel in probabilistic reasoning and instrumental learning. On temporal discounting it is reported as more far-sighted relative to many models, and it is placed toward the far-sighted end of the temporal-discounting spectrum in the paper's summary.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human-normalized baseline used (1 = human average); paper notes GPT-4 and Claude-1 reach human-level broadly, and that temporal discounting behavior varies across models — GPT-4 aligns with humans while text-davinci-003 is described as far-sighted, but exact human numeric baselines per task are not enumerated.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>text-davinci-003 outperforms many models on some tasks (probabilistic reasoning, instrumental learning) and behaves more far-sighted on temporal discounting than many other LLMs; relative to humans it is sometimes closer and sometimes divergent depending on the metric (no per-task statistical p-values reported in text for this model).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>As with other models, behavioral metrics can diverge from performance metrics (e.g., good performance but non-human strategies). The paper reports broad trends rather than detailed per-model statistics for each task.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CogBench: a large language model walks into a psychology lab', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5329.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5329.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>text-bison (PaLM-2)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM-2 text-bison@002</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Google's PaLM-2 text model (text-bison@002) included in the benchmark; exhibits distinct deviations from other models on exploration and optimism-bias measures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-bison@002</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Google PaLM-2 text model evaluated deterministically via in-context prompts; included among proprietary models.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>340</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>CogBench battery (notably horizon task, optimism bias, temporal discounting)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>exploration/exploitation, learning biases, temporal discounting</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Same seven tasks; horizon task measures directed and random exploration, restless bandit measures meta-cognition, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>text-bison is explicitly reported as an exception: it did not outperform humans on the horizon task (most other models did), and it is also an exception to the general optimism-bias pattern (authors state almost all models exhibit very strong optimism bias except for text-bison). On temporal discounting, text-bison is reported as appearing myopic.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human-normalized baseline is used (value 1 = human average); text-bison underperforms relative to other models on horizon exploration (did not show super-human performance there) and lacks the optimism bias commonly observed in models compared to humans.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>text-bison underperformed relative to the other large models on the horizon task (did not reach the super-human performance most models achieved) and differed from other models by not showing a strong optimism bias; precise numerical comparisons to humans are not provided in-text.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>text-bison's divergence (lack of optimism bias; no super-human horizon performance) is highlighted as an exception in the dataset. The paper does not provide per-task significance tests for text-bison specifically, and sample/simulation differences (e.g., fewer simulations for some tasks) are potential limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CogBench: a large language model walks into a psychology lab', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5329.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5329.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-2-70 vs LLaMA-2-70-chat</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-2-70 and LLaMA-2-70-chat (Meta LLaMA-2 family)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Pair of LLaMA-2 variants (base and chat fine-tuned); paper documents cases where performance is similar but behavioral metrics diverge strongly (e.g., risk-taking, temporal discounting, random exploration).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2-70 & LLaMA-2-70-chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source LLaMA-2 70B parameter model and its -chat fine-tuned version (the -chat variant includes conversational fine-tuning and likely RLHF); both evaluated under deterministic in-context prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>CogBench battery (focus on BART risk-taking, temporal discounting, horizon random exploration)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>risk, temporal discounting, exploration/exploitation</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Same battery (see above); BART measures risk-taking, horizon task measures directed/random exploration, temporal discounting measures delay preference.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>The two models show very similar overall performance across tasks but markedly different behavioral phenotypes: LLaMA-2-70 and LLaMA-2-70-chat have opposite risk-taking behavior on the BART (one extreme vs the other), different temporal-discounting profiles (one myopic vs one far-sighted), and differ in random exploration (LLaMA-2-70-chat exhibits higher-than-human random exploration).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human-average normalization used; although performance scores are similar between the two versions, behavioral metrics diverge from humans in different ways (one variant may match humans on a metric while the other does not). Exact numeric human comparisons per metric not enumerated in text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Despite near-identical performance across tasks, the base LLaMA and its chat-fine-tuned counterpart display qualitatively different behavioral strategies—demonstrating that performance parity does not imply behavioral parity. The chat-fine-tuned (-chat) version displays more exploration in some metrics compared to the base model.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>This within-family contrast highlights that fine-tuning (including RLHF/conversational fine-tuning) can substantially change behavioral phenotypes without large changes in aggregate performance. Authors note that behavioral metrics are necessary to detect such divergences; per-metric statistical tests per model pair are not fully reported in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CogBench: a large language model walks into a psychology lab', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5329.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5329.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Aggregate LLM findings</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Aggregate findings across 35 LLMs evaluated with CogBench</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Summary of overall patterns discovered by applying the CogBench suite to 35 proprietary and open-source LLMs, including effects of size, RLHF, openness, and prompt engineering on performance and behavioral metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Aggregate set of 35 LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Thirty-five LLMs (proprietary and open-source) evaluated in-context with deterministic decoding across the CogBench tasks; features analyzed via multilevel regression (accounting for model families and fine-tuned variants).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (range includes models from ~13 to 1760 in paper table)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>CogBench battery (7 tasks; 10 behavioral metrics)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>reasoning, learning, meta-cognition, exploration, temporal discounting, risk</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Seven canonical cognitive tasks converted to text-based prompts producing both performance metrics (scores to optimize) and behavioral metrics (parameters and regressions capturing strategies).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Key aggregate results: larger models perform better (multi-level regression β = 0.277 ± 0.39, z = 14.1, p < 0.001 for performance); model-basedness also increases with size (β = 0.481 ± 0.22, z = 4.2, p < 0.001). RLHF-like training produces models that are ~2× more similar to human behavior in UMAP visualization and shows an 11.7% average decrease in L2-norm distance to humans; RLHF significantly increases meta-cognition (regression β = 0.461 ± 0.15, z = 5.9, p < 0.001). Open-source models, controlling for other factors, take fewer risks than proprietary models (regression β = −0.612 ± 0.11, z = −11.4, p < 0.001). Code fine-tuning had no detectable positive effect on performance or model-basedness in the regressions.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>All metrics are human-normalized in figures: 0 = random agent, 1 = average human. Authors use human-normalized comparisons throughout; many models reach or exceed 1.0 for some performance metrics, but behavioral metrics typically do not match human profiles across the board.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Summary comparisons: GPT-4 and Claude-1 achieved human-level performance on most tasks (5/6); most models were super-human on the horizon task (except text-bison); restless bandit was challenging for most models except GPT-4 and Claude-1; BART was a challenge for all models (models either never take risks or always risk everything). Behavioral metrics frequently diverged from human behavior even when raw performance was high.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Notable patterns: (1) size and RLHF are robust predictors of more humanlike behavior and higher performance; (2) open-source models, conditional on other features, were less risk-prone than proprietary models (contrary to some expectations about hidden pre-prompts making proprietary models safer); (3) code fine-tuning did not produce the expected behavioral benefits; (4) CoT prompting improved probabilistic reasoning (average +9.01% posterior accuracy) and SB prompting improved model-basedness more strongly (SB +118.59% aggregated increase), though CoT also improved model-basedness (+64.59%)—these prompt-engineering results are aggregated across five models and come with automation and tokenization caveats described by the authors. Limitations: human baselines are provided as normalized averages (raw per-subject stats not given), some aggregated stats are reported without per-model p-values, and proprietary-model metadata is incomplete, which the authors note can limit regression precision.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CogBench: a large language model walks into a psychology lab', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A theory of learning to infer <em>(Rating: 2)</em></li>
                <li>Humans use directed and random exploration to solve the explore-exploit dilemma <em>(Rating: 2)</em></li>
                <li>Model-based influences on humans' choices and striatal prediction errors <em>(Rating: 2)</em></li>
                <li>Behavioural and neural characterization of optimistic reinforcement learning <em>(Rating: 2)</em></li>
                <li>Restless bandit task (Ershadmanesh et al., 2023) <em>(Rating: 2)</em></li>
                <li>Probabilistic reasoning (Dasgupta et al., 2020) <em>(Rating: 2)</em></li>
                <li>Using cognitive psychology to understand gpt-3 <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 1)</em></li>
                <li>Take a step back: Evoking reasoning via abstraction in large language models <em>(Rating: 1)</em></li>
                <li>Evaluation of a behavioral measure of risk taking: the balloon analogue risk task (bart) <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5329",
    "paper_id": "paper-268041290",
    "extraction_schema_id": "extraction-schema-112",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4",
            "brief_description": "Proprietary large transformer-based language model from OpenAI; evaluated deterministically (temperature=0) in this paper across a battery of seven cognitive-psychology tasks (CogBench).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Proprietary OpenAI LLM evaluated via in-context learning (no fine-tuning) with deterministic decoding; listed in paper's model table (uses RLHF), treated as a high-parameter, state-of-the-art model in the benchmark.",
            "model_size": "1760",
            "cognitive_test_name": "CogBench battery (probabilistic reasoning; horizon; restless bandit; instrumental learning; two-step task; temporal discounting; BART)",
            "cognitive_test_type": "reasoning, decision-making, learning, meta-cognition, temporal discounting, risk",
            "cognitive_test_description": "A suite of seven canonical cognitive-psychology experiments adapted to text prompts for LLMs: probabilistic belief updating (prior/likelihood weighting), horizon two-armed bandit (directed/random exploration), restless bandit (meta-cognition), instrumental learning (learning rates and optimism bias), two-step task (model-based vs model-free RL), temporal discounting (delay preference), and Balloon Analogue Risk Task (risk-taking).",
            "llm_performance": "GPT-4 achieved human-level performance on most tasks (reported as human-level in 5 out of 6 performance metrics used in main figure); it solved the restless bandit (one of the few models that did), excelled in probabilistic reasoning and instrumental learning, displayed very high model-basedness (reported to significantly surpass human levels), performed strongly on the horizon task (most models including GPT-4 were super-human on horizon), and like all models struggled on the BART (risk task).",
            "human_baseline_performance": "Human-average used as normalization baseline (value 1 = average human); authors state GPT-4 reached human-level (≈1.0 normalized) in five of six reported performance metrics in the main figure (human data sourced from canonical cognitive studies; exact numeric human scores not enumerated in text).",
            "performance_comparison": "GPT-4 matches or exceeds humans on most CogBench performance metrics (human-level in 5/6), and significantly exceeds human levels on model-basedness according to the authors; it is among the few models able to solve the restless bandit task comparably to humans. No per-task p-values for GPT-4 vs humans are provided, but authors report significance-level summaries (e.g., 'GPT-4 significantly surpassing human levels' for model-basedness).",
            "notable_differences_or_limitations": "Despite high performance, GPT-4 (like other LLMs) shows behavioral divergences from humans: LLMs overweight priors relative to likelihoods, exhibit strong optimism bias and high learning rates, and often achieve high performance without human-like exploration strategies (exploitation-dominant). GPT-4 still struggles in risk-taking (BART) where models show extreme behaviors. Results are from in-context evaluation (no fine-tuning) with temperature=0, and some comparisons are human-normalized rather than raw-score matched.",
            "uuid": "e5329.0",
            "source_info": {
                "paper_title": "CogBench: a large language model walks into a psychology lab",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Claude-1",
            "name_full": "Anthropic Claude-1",
            "brief_description": "Proprietary LLM from Anthropic that was evaluated with CogBench and reported to achieve human-level performance on many tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Claude-1",
            "model_description": "Anthropic proprietary LLM (listed as using RLAIF in the paper's table) evaluated with deterministic in-context prompts across the CogBench tasks; included in the authors' set of 35 models.",
            "model_size": "100",
            "cognitive_test_name": "CogBench battery (see GPT-4 entry)",
            "cognitive_test_type": "reasoning, decision-making, learning, meta-cognition, temporal discounting, risk",
            "cognitive_test_description": "Same seven cognitive-psychology tasks adapted as text prompts for LLMs (see GPT-4).",
            "llm_performance": "Claude-1 is reported alongside GPT-4 as achieving human-level scores in most tasks (human-level in 5 out of 6 performance metrics in the main figure). It is one of the exceptions that performed well on the restless bandit task and demonstrated strong performance in probabilistic reasoning and instrumental learning.",
            "human_baseline_performance": "Human-average normalized baseline (value 1 corresponds to average human). Authors state Claude-1 reached human-level performance across the majority of tasks (same normalization as for GPT-4); no raw human numeric details provided in text.",
            "performance_comparison": "Claude-1 matched human-level performance across most performance metrics and, like GPT-4, was among the few models able to solve the restless bandit. No per-task statistical tests reported at the individual-model level beyond aggregate descriptions.",
            "notable_differences_or_limitations": "Although Claude-1 attains high performance, the paper notes general LLM behavioral patterns (e.g., strong prior weighting, optimism bias) that apply to most models. Claude models are reported as RLAIF-trained, and the paper highlights RLHF/RL-like training as associated with more humanlike behavior and improved meta-cognition.",
            "uuid": "e5329.1",
            "source_info": {
                "paper_title": "CogBench: a large language model walks into a psychology lab",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Claude-2",
            "name_full": "Anthropic Claude-2",
            "brief_description": "Later Anthropic model (Claude-2) included in the evaluation; used in prompt-engineering experiments (CoT, SB) alongside other large models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Claude-2",
            "model_description": "Anthropic proprietary LLM (listed as RLAIF in the paper); evaluated via the same in-context deterministic protocol; included in both baseline and prompt-engineering comparisons.",
            "model_size": "200",
            "cognitive_test_name": "CogBench battery (baseline and with prompt-engineering: chain-of-thought and take-a-step-back)",
            "cognitive_test_type": "reasoning, decision-making (including probabilistic reasoning and model-basedness)",
            "cognitive_test_description": "Same battery; additionally used in targeted experiments testing the effect of chain-of-thought (CoT) and take-a-step-back (SB) prompting on probabilistic reasoning (posterior accuracy) and model-basedness.",
            "llm_performance": "Claude-2 is among models evaluated with CoT and SB prompting. Aggregated across five evaluated models (GPT-4, PaLM-2 text-bison, Claude-1/2, LLaMA-2), CoT increased posterior accuracy by +9.01% on average and SB by +3.10%; for model-basedness CoT increased it by +64.59% and SB by +118.59% (these model-basedness improvements are aggregated weighted averages across the five models, not per-model breakdowns in text).",
            "human_baseline_performance": "Human-average normalization used for baseline comparisons in the paper; explicit human numerical values per prompt condition are not provided in the text for Claude-2 specifically.",
            "performance_comparison": "Claude-2 benefits from prompt-engineering (CoT and SB) in the aggregated analysis, implying improved probabilistic reasoning and model-basedness relative to its own baseline; no per-model statistical tests are reported in the manuscript for Claude-2 alone.",
            "notable_differences_or_limitations": "The CoT and SB effects are reported as aggregated across five large models; per-model variance exists but per-model results are not provided in-text. Authors note automation challenges and potential chaotic outputs for some LLMs when allowing multi-token internal reasoning (token limits imposed).",
            "uuid": "e5329.2",
            "source_info": {
                "paper_title": "CogBench: a large language model walks into a psychology lab",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "text-davinci-003",
            "name_full": "OpenAI text-davinci-003 (GPT-3 family)",
            "brief_description": "A GPT-3–era OpenAI model included in the evaluation; performed well on probabilistic reasoning and instrumental learning and showed comparatively far-sighted temporal discounting behavior.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "text-davinci-003",
            "model_description": "OpenAI GPT-3 family model (listed in paper's table), evaluated via in-context prompting with deterministic decoding; included among proprietary models tested.",
            "model_size": "170",
            "cognitive_test_name": "CogBench battery (notably probabilistic reasoning, instrumental learning, temporal discounting, BART)",
            "cognitive_test_type": "reasoning, learning, temporal discounting, risk",
            "cognitive_test_description": "Same battery as above.",
            "llm_performance": "text-davinci-003 is reported among models that excel in probabilistic reasoning and instrumental learning. On temporal discounting it is reported as more far-sighted relative to many models, and it is placed toward the far-sighted end of the temporal-discounting spectrum in the paper's summary.",
            "human_baseline_performance": "Human-normalized baseline used (1 = human average); paper notes GPT-4 and Claude-1 reach human-level broadly, and that temporal discounting behavior varies across models — GPT-4 aligns with humans while text-davinci-003 is described as far-sighted, but exact human numeric baselines per task are not enumerated.",
            "performance_comparison": "text-davinci-003 outperforms many models on some tasks (probabilistic reasoning, instrumental learning) and behaves more far-sighted on temporal discounting than many other LLMs; relative to humans it is sometimes closer and sometimes divergent depending on the metric (no per-task statistical p-values reported in text for this model).",
            "notable_differences_or_limitations": "As with other models, behavioral metrics can diverge from performance metrics (e.g., good performance but non-human strategies). The paper reports broad trends rather than detailed per-model statistics for each task.",
            "uuid": "e5329.3",
            "source_info": {
                "paper_title": "CogBench: a large language model walks into a psychology lab",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "text-bison (PaLM-2)",
            "name_full": "PaLM-2 text-bison@002",
            "brief_description": "Google's PaLM-2 text model (text-bison@002) included in the benchmark; exhibits distinct deviations from other models on exploration and optimism-bias measures.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "text-bison@002",
            "model_description": "Google PaLM-2 text model evaluated deterministically via in-context prompts; included among proprietary models.",
            "model_size": "340",
            "cognitive_test_name": "CogBench battery (notably horizon task, optimism bias, temporal discounting)",
            "cognitive_test_type": "exploration/exploitation, learning biases, temporal discounting",
            "cognitive_test_description": "Same seven tasks; horizon task measures directed and random exploration, restless bandit measures meta-cognition, etc.",
            "llm_performance": "text-bison is explicitly reported as an exception: it did not outperform humans on the horizon task (most other models did), and it is also an exception to the general optimism-bias pattern (authors state almost all models exhibit very strong optimism bias except for text-bison). On temporal discounting, text-bison is reported as appearing myopic.",
            "human_baseline_performance": "Human-normalized baseline is used (value 1 = human average); text-bison underperforms relative to other models on horizon exploration (did not show super-human performance there) and lacks the optimism bias commonly observed in models compared to humans.",
            "performance_comparison": "text-bison underperformed relative to the other large models on the horizon task (did not reach the super-human performance most models achieved) and differed from other models by not showing a strong optimism bias; precise numerical comparisons to humans are not provided in-text.",
            "notable_differences_or_limitations": "text-bison's divergence (lack of optimism bias; no super-human horizon performance) is highlighted as an exception in the dataset. The paper does not provide per-task significance tests for text-bison specifically, and sample/simulation differences (e.g., fewer simulations for some tasks) are potential limitations.",
            "uuid": "e5329.4",
            "source_info": {
                "paper_title": "CogBench: a large language model walks into a psychology lab",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "LLaMA-2-70 vs LLaMA-2-70-chat",
            "name_full": "LLaMA-2-70 and LLaMA-2-70-chat (Meta LLaMA-2 family)",
            "brief_description": "Pair of LLaMA-2 variants (base and chat fine-tuned); paper documents cases where performance is similar but behavioral metrics diverge strongly (e.g., risk-taking, temporal discounting, random exploration).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-2-70 & LLaMA-2-70-chat",
            "model_description": "Open-source LLaMA-2 70B parameter model and its -chat fine-tuned version (the -chat variant includes conversational fine-tuning and likely RLHF); both evaluated under deterministic in-context prompting.",
            "model_size": "70",
            "cognitive_test_name": "CogBench battery (focus on BART risk-taking, temporal discounting, horizon random exploration)",
            "cognitive_test_type": "risk, temporal discounting, exploration/exploitation",
            "cognitive_test_description": "Same battery (see above); BART measures risk-taking, horizon task measures directed/random exploration, temporal discounting measures delay preference.",
            "llm_performance": "The two models show very similar overall performance across tasks but markedly different behavioral phenotypes: LLaMA-2-70 and LLaMA-2-70-chat have opposite risk-taking behavior on the BART (one extreme vs the other), different temporal-discounting profiles (one myopic vs one far-sighted), and differ in random exploration (LLaMA-2-70-chat exhibits higher-than-human random exploration).",
            "human_baseline_performance": "Human-average normalization used; although performance scores are similar between the two versions, behavioral metrics diverge from humans in different ways (one variant may match humans on a metric while the other does not). Exact numeric human comparisons per metric not enumerated in text.",
            "performance_comparison": "Despite near-identical performance across tasks, the base LLaMA and its chat-fine-tuned counterpart display qualitatively different behavioral strategies—demonstrating that performance parity does not imply behavioral parity. The chat-fine-tuned (-chat) version displays more exploration in some metrics compared to the base model.",
            "notable_differences_or_limitations": "This within-family contrast highlights that fine-tuning (including RLHF/conversational fine-tuning) can substantially change behavioral phenotypes without large changes in aggregate performance. Authors note that behavioral metrics are necessary to detect such divergences; per-metric statistical tests per model pair are not fully reported in the text.",
            "uuid": "e5329.5",
            "source_info": {
                "paper_title": "CogBench: a large language model walks into a psychology lab",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Aggregate LLM findings",
            "name_full": "Aggregate findings across 35 LLMs evaluated with CogBench",
            "brief_description": "Summary of overall patterns discovered by applying the CogBench suite to 35 proprietary and open-source LLMs, including effects of size, RLHF, openness, and prompt engineering on performance and behavioral metrics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Aggregate set of 35 LLMs",
            "model_description": "Thirty-five LLMs (proprietary and open-source) evaluated in-context with deterministic decoding across the CogBench tasks; features analyzed via multilevel regression (accounting for model families and fine-tuned variants).",
            "model_size": "various (range includes models from ~13 to 1760 in paper table)",
            "cognitive_test_name": "CogBench battery (7 tasks; 10 behavioral metrics)",
            "cognitive_test_type": "reasoning, learning, meta-cognition, exploration, temporal discounting, risk",
            "cognitive_test_description": "Seven canonical cognitive tasks converted to text-based prompts producing both performance metrics (scores to optimize) and behavioral metrics (parameters and regressions capturing strategies).",
            "llm_performance": "Key aggregate results: larger models perform better (multi-level regression β = 0.277 ± 0.39, z = 14.1, p &lt; 0.001 for performance); model-basedness also increases with size (β = 0.481 ± 0.22, z = 4.2, p &lt; 0.001). RLHF-like training produces models that are ~2× more similar to human behavior in UMAP visualization and shows an 11.7% average decrease in L2-norm distance to humans; RLHF significantly increases meta-cognition (regression β = 0.461 ± 0.15, z = 5.9, p &lt; 0.001). Open-source models, controlling for other factors, take fewer risks than proprietary models (regression β = −0.612 ± 0.11, z = −11.4, p &lt; 0.001). Code fine-tuning had no detectable positive effect on performance or model-basedness in the regressions.",
            "human_baseline_performance": "All metrics are human-normalized in figures: 0 = random agent, 1 = average human. Authors use human-normalized comparisons throughout; many models reach or exceed 1.0 for some performance metrics, but behavioral metrics typically do not match human profiles across the board.",
            "performance_comparison": "Summary comparisons: GPT-4 and Claude-1 achieved human-level performance on most tasks (5/6); most models were super-human on the horizon task (except text-bison); restless bandit was challenging for most models except GPT-4 and Claude-1; BART was a challenge for all models (models either never take risks or always risk everything). Behavioral metrics frequently diverged from human behavior even when raw performance was high.",
            "notable_differences_or_limitations": "Notable patterns: (1) size and RLHF are robust predictors of more humanlike behavior and higher performance; (2) open-source models, conditional on other features, were less risk-prone than proprietary models (contrary to some expectations about hidden pre-prompts making proprietary models safer); (3) code fine-tuning did not produce the expected behavioral benefits; (4) CoT prompting improved probabilistic reasoning (average +9.01% posterior accuracy) and SB prompting improved model-basedness more strongly (SB +118.59% aggregated increase), though CoT also improved model-basedness (+64.59%)—these prompt-engineering results are aggregated across five models and come with automation and tokenization caveats described by the authors. Limitations: human baselines are provided as normalized averages (raw per-subject stats not given), some aggregated stats are reported without per-model p-values, and proprietary-model metadata is incomplete, which the authors note can limit regression precision.",
            "uuid": "e5329.6",
            "source_info": {
                "paper_title": "CogBench: a large language model walks into a psychology lab",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A theory of learning to infer",
            "rating": 2,
            "sanitized_title": "a_theory_of_learning_to_infer"
        },
        {
            "paper_title": "Humans use directed and random exploration to solve the explore-exploit dilemma",
            "rating": 2,
            "sanitized_title": "humans_use_directed_and_random_exploration_to_solve_the_exploreexploit_dilemma"
        },
        {
            "paper_title": "Model-based influences on humans' choices and striatal prediction errors",
            "rating": 2,
            "sanitized_title": "modelbased_influences_on_humans_choices_and_striatal_prediction_errors"
        },
        {
            "paper_title": "Behavioural and neural characterization of optimistic reinforcement learning",
            "rating": 2,
            "sanitized_title": "behavioural_and_neural_characterization_of_optimistic_reinforcement_learning"
        },
        {
            "paper_title": "Restless bandit task (Ershadmanesh et al., 2023)",
            "rating": 2,
            "sanitized_title": "restless_bandit_task_ershadmanesh_et_al_2023"
        },
        {
            "paper_title": "Probabilistic reasoning (Dasgupta et al., 2020)",
            "rating": 2,
            "sanitized_title": "probabilistic_reasoning_dasgupta_et_al_2020"
        },
        {
            "paper_title": "Using cognitive psychology to understand gpt-3",
            "rating": 2,
            "sanitized_title": "using_cognitive_psychology_to_understand_gpt3"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 1,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Take a step back: Evoking reasoning via abstraction in large language models",
            "rating": 1,
            "sanitized_title": "take_a_step_back_evoking_reasoning_via_abstraction_in_large_language_models"
        },
        {
            "paper_title": "Evaluation of a behavioral measure of risk taking: the balloon analogue risk task (bart)",
            "rating": 2,
            "sanitized_title": "evaluation_of_a_behavioral_measure_of_risk_taking_the_balloon_analogue_risk_task_bart"
        }
    ],
    "cost": 0.018857,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>CogBench: a large language model walks into a psychology lab
28 Feb 2024</p>
<p>Julian Coda-Forno 
Computational Principles of Intelligence Lab
Max Planck Institute for Biological Cybernetics
TübingenGermany</p>
<p>Institute for Human-Centered AI
Helmholtz Compu-tational Health Center
MunichGermany</p>
<p>Marcel Binz 
Computational Principles of Intelligence Lab
Max Planck Institute for Biological Cybernetics
TübingenGermany</p>
<p>Institute for Human-Centered AI
Helmholtz Compu-tational Health Center
MunichGermany</p>
<p>Jane X Wang 
Google DeepMind
LondonUK</p>
<p>Eric Schulz 
Computational Principles of Intelligence Lab
Max Planck Institute for Biological Cybernetics
TübingenGermany</p>
<p>Institute for Human-Centered AI
Helmholtz Compu-tational Health Center
MunichGermany</p>
<p>Coda- </p>
<p>Equal contribution</p>
<p>CogBench: a large language model walks into a psychology lab
28 Feb 2024DAD2E6164DECDD607E9E4633D62C7057arXiv:2402.18225v1[cs.CL]
Large language models (LLMs) have significantly advanced the field of artificial intelligence.Yet, evaluating them comprehensively remains challenging.We argue that this is partly due to the predominant focus on performance metrics in most benchmarks.This paper introduces CogBench, a benchmark that includes ten behavioral metrics derived from seven cognitive psychology experiments.This novel approach offers a toolkit for phenotyping LLMs' behavior.We apply CogBench to 35 LLMs, yielding a rich and diverse dataset.We analyze this data using statistical multilevel modeling techniques, accounting for the nested dependencies among fine-tuned versions of specific LLMs.Our study highlights the crucial role of model size and reinforcement learning from human feedback (RLHF) in improving performance and aligning with human behavior.Interestingly, we find that open-source models are less risk-prone than proprietary models and that fine-tuning on code does not necessarily enhance LLMs' behavior.Finally, we explore the effects of prompt-engineering techniques.We discover that chain-of-thought prompting improves probabilistic reasoning, while take-a-step-back prompting fosters model-based behaviors.</p>
<p>Introduction</p>
<p>Large language models (LLMs) have emerged as a groundbreaking technology, captivating the attention of the scientific community (Bommasani et al., 2021;Binz et al., 2023).Modern LLMs have scaled to remarkable dimensions in both architecture and datasets (Kaplan et al., 2020), revealing a spectrum of capabilities that were previously unimagined (Wei et al., 2022;Brown et al., 2020).Yet, these models also present a significant challenge: their internal workings are largely opaque, making it difficult to fully comprehend their behavior (Tamkin et al., 2021).This lack of understanding fuels ongoing debates about their capabilities and limitations (McCoy et al., 2023;Bubeck et al., 2023).</p>
<p>A notable issue in these discussions is the focus of many benchmarks on performance metrics alone (Burnell et al., 2023).This approach often overlooks the underlying behavioral mechanisms of the models, reducing benchmarks to mere training targets rather than tools for genuine insight, and thus failing to provide a comprehensive measure of the models' abilities (Schaeffer et al., 2023).How can we overcome this problem and make progress toward a better understanding of LLMs' behaviors?</p>
<p>The field of cognitive psychology may offer solutions to these problems.Experiments from cognitive psychology have been used to study human behavior for many decades, and have therefore been extensively validated.Furthermore, they typically focus more on behavioral insights rather than performance metrics alone.Finally, many of these experiments are programmatically generated, minimizing data leakage concerns.Many of these concepts are important to ensure a robust evaluation of an agent's capabilities.However, while there have been studies investigating LLMs on individual tasks from cognitive psychology (Binz &amp; Schulz, 2023;Dasgupta et al., 2022;Hagendorff et al., 2023;Ullman, 2023), no study has evaluated them holistically.</p>
<p>In this paper, we propose CogBench, a novel benchmark consisting of ten behavioral metrics spanning seven cognitive psychology experiments, to fill this gap.We investigate the behaviors of 35 LLMs in total, using our benchmark to not only compare the performance of these models but also apply techniques from computational cognitive modeling to understand the inner workings of their behaviors.</p>
<p>Our results recover the unequivocal importance of size: larger models generally perform better and are more modelbased than smaller models.Our results also show the importance of reinforcement learning from human feedback (RLHF; Christiano et al., 2017) in aligning LLMs with humans: RLHF'ed LLMs behave generally more humanlike and are more accurate in estimating uncertainty.Yet our results also revealed surprising behaviors.First, while open-source models are often believed to be more risky due to the lack of pre-prompts, we find that, holding all else equal, they make less risky decisions than proprietary models.Secondly, while fine-tuning on code is often believed to improve LLMs' behaviors, we find little evidence for this in our benchmarking suite.</p>
<p>Finally, we investigate how chain-of-thought (CoT) (Wei et al., 2023;Kojima et al., 2022) and take-a-step-back (SB) (Zheng et al., 2023a) prompting techniques can influence different behavioral characteristics.Our analysis suggests that CoT is particularly effective at enhancing probabilistic reasoning, while SB proves to be more relevant for promoting model-based behaviors.This showcases insights that can be gained by CogBench also for understanding the effectiveness of these prompt-engineering techniques as well as guiding users in selecting the most suitable promptengineering technique based on the specific context.</p>
<p>Taken together, our experiments show how psychology can offer detailed insights into artificial agents' behavior as we provide an openly accessible1 and challenging benchmark to evaluate LLMs.</p>
<p>Related work</p>
<p>Benchmarking LLMs: As LLMs rapidly evolve, it is critical to assess their capabilities.Numerous benchmarks have emerged to tackle this challenge, evaluating capabilities such as grade school mathematics (Cobbe et al., 2021), general knowledge (Joshi et al., 2017), programming (Chen et al., 2021), reasoning (Collins et al., 2022), among others (Hendrycks et al., 2021).In addition, the Chatbot Arena (Zheng et al., 2023b) provides a platform for comparing AI chatbots, and the Beyond the Imitation Game Benchmark (BIG-bench; Srivastava &amp; authors, 2023) offers a comprehensive evaluation of LLMs across over 200 tasks.</p>
<p>Psychology for LLMs: Our benchmark is part of a new wave of research that uses cognitive psychology to study LLMs (Binz &amp; Schulz, 2023;Dasgupta et al., 2022;Coda-Forno et al., 2023;Ullman, 2023;Hagendorff et al., 2023;Akata et al., 2023;Yax et al., 2023;Chen et al., 2023;Buschoff et al., 2024).The power of this approach lies in its incorporation of tools from cognitive psychology that have been developed and refined over many decades.Instead of focusing solely on how well LLMs perform, this area of research prioritizes describing and characterizing their behavior in terms of underlying mechanisms.This shift in focus helps us understand LLMs in a more meaningful way.</p>
<p>It is important to note that while these works have signifi-cantly contributed to our understanding of LLMs, they have mainly targeted specific behaviors in isolation and did not establish a benchmark providing a standardized evaluation of different models and across a diverse, comprehensive set of tasks and skills.</p>
<p>Methods</p>
<p>CogBench is a benchmark rooted in cognitive psychology for evaluating the behaviors of language models.It incorporates ten metrics derived from seven canonical experiments in the literature on learning and decision-making.These metrics offer a robust measure of wide-ranging behaviors and allow for comparisons with human behavior.In this section, we provide an overview of the models included in our study, followed by brief descriptions of the used cognitive experiments and their respective metrics.Figure 1 displays a visual representation that complements the discussion in this section.</p>
<p>Prompting and summary of included models</p>
<p>We evaluated over 35 different LLMs using our benchmark.This selection includes proprietary models such as Anthropic's Claude-1 and Claude-2 (Anthropic, 2023), Open-AI's GPT-3 (text-davinci-003) and GPT-4 (OpenAI, 2023), and Google's PaLM-2 for text (text-bison@002) (Google, 2023).We also tested open-source models like Mosaic's MPT (MosaicML, 2023), Falcon (Almazrouei et al., 2023), and numerous LLaMA-2 variants (Touvron et al., 2023).</p>
<p>For a full list of the models used, we refer the reader to Appendix A.</p>
<p>It is important to note that all experiments performed in this paper rely entirely on the LLMs' in-context learning abilities and do not involve any form of fine-tuning.We set the temperature parameter to zero, leading to deterministic responses, and retained the default values for all other parameters.</p>
<p>High-level summary of tasks</p>
<p>In the following, we provide a high-level summary of the tasks included in CogBench, alongside their ten behavioral metrics.It is important to highlight that a performance metric can also be obtained for each task.For full descriptions of all tasks and their corresponding metrics, we refer the reader to Appendix B. CogBench consists of the following tasks:</p>
<ol>
<li>
<p>Probabilistic reasoning (Dasgupta et al., 2020): a task that tests how agents update beliefs based on new evidence.They are given a "wheel of fortune" (representing initial prior probabilities) and two urns with different colored ball distributions (representing likelihoods).Upon drawing a ball, agents can revise their belief about the chosen urn, considering both the wheel (prior) and the ball color (evidence).This tests adaptability to different prior/likelihood scenarios by changing the wheel division and ball distributions.Agents have to estimate the probability of the drawn ball's urn.The behavioral choices can be used to estimate an agent's prior and likelihood weightings.Experimentally, people often exhibit a behavior known as system neglect, meaning that they underweight both priors and likelihoods (Massey &amp; Wu, 2005).</p>
</li>
<li>
<p>Horizon task (Wilson et al., 2014): a two-armed bandit task with stationary reward distributions.Agents first observe four reward values of randomly determined options, followed by making either one or six additional choices.We use this task to measure whether an agent uses uncertainty to guide its exploration behavior (directed exploration) and/or whether it injects noise into its policy to explore (random exploration).People are known to rely on a combination of both strategies (Wilson et al., 2014;Brändle et al., 2021).</p>
</li>
<li>
<p>Restless bandit task (Ershadmanesh et al., 2023): a two-armed bandit task with non-stationary reward distributions.There is always one option with a higher average reward.Every few trials a switch between the reward distributions of the two options occurs.Agents furthermore have to indicate after each choice how confident they are in their decisions.We use this task to measure meta-cognition, which indicates whether an agent can assess the quality of its own cognitive abilities.People generally display this ability but its extent is influenced by various internal and external factors (Shekhar &amp; Rahnev, 2021).</p>
</li>
<li>
<p>Instrumental learning (Lefebvre et al., 2017): Agents encounter four two-armed bandit problems in an interleaved order.Each bandit problem is identified by a unique symbol pair.We use this task to investigate how an agent learns.First, we report the learning rate of the agent which is common practice in two-armed bandits.Furthermore, we use it to reveal whether an agent learns more from positive than from negative prediction errors, i.e., whether it has an optimism bias.</p>
</li>
</ol>
<p>People commonly display asymmetric tendencies when updating their beliefs by showing higher learning rates after encountering positive prediction errors compared to negative ones (Palminteri &amp; Lebreton, 2022).</p>
<ol>
<li>
<p>Two-step task (Daw et al., 2011): a reinforcement learning task in which agents have to accumulate as many treasures as possible.Taking an action from a starting state transfers the agent to one out of two second-stage states.In each of these second-stage states, the agent has the choice between two options that probabilistically lead to treasures.Finally, the agent is transferred back to the initial state and the process repeats for a predefined number of rounds.The task experimentally disentangles model-based from model-free reinforcement learning.We therefore use it to measure an agent's model-basedness.Previous studies using this task have shown that people rely on a combination of model-free and model-based reinforcement learning (Daw et al., 2011).</p>
</li>
<li>
<p>Temporal discounting (Ruggeri et al., 2022): Agents have to make a series of choices between two options.Each option is characterized by a monetary outcome and an associated delay until the outcome is received.</p>
</li>
</ol>
<p>We use this task to assess temporal discounting, indicating whether an agent prefers smaller but immediate gains over larger delayed ones.People generally show a preference for immediate gains, although the precise functional form of their discounting is a matter of debate (Cavagnaro et al., 2016).</p>
<ol>
<li>Balloon Analog Risk Task (BART) (Lejuez et al., 2002): Agents have to inflate an imaginary balloon to obtain rewards.They may choose to stop inflating and cashing out all rewards accumulated so far.There is a chance that the balloon pops at any point in time and all rewards will be lost.We use this task to assess risk-taking behavior.Human risk-taking in this task is "significantly correlated with scores on selfreport measures of risk-related constructs and with the self-reported occurrence of real-world risk behaviors" (Lejuez et al., 2002).</li>
</ol>
<p>The cognitive phenotype of LLMs</p>
<p>This section provides the reader with a high-level overview of our benchmark's metrics.From our suite of 7 tasks, we can derive two classes of metrics: 1) performance metrics that represent the score participants aim to optimize, and 2) behavioral metrics measuring how participants complete the task (tasks are typically designed in a way that allows one to disentangle between different types of behavior).Figure 2 visualizes phenotypes for both classes of metrics for seven well-established LLMs. 2 We report the results of all 35 LLMs in Appendix C. The observed differences underscore the practical value and importance of CogBench for evaluating LLMs, offering a more comprehensive assessment than standard performance-based benchmarks alone.</p>
<p>Performance summary</p>
<p>As presented in Figure 2A, in terms of performance, GPT-4 and Claude-1 distinguish themselves, achieving humanlevel scores in most tasks (five out of six). 3 In general, all models demonstrate competence in at least half of the tasks (three out of six).Each of the seven models excels in probabilistic reasoning and instrumental learning.The horizon task sees most models outperforming humans except for text-bison.The restless bandit task poses a challenge for the majority of models, with GPT-4 and Claude-1 being notable exceptions.Finally, the BART proves to be a hurdle for all models.</p>
<p>Differences between behavioral and performance metrics</p>
<p>Figure 2B shows that none of the models exhibit human-like behavior on the majority of behavioral metrics, revealing a complex structure that warrants further exploration.</p>
<p>High performance indicates high meta-cognition and model-basedness: Models that demonstrate satisfactory performance on the restless bandit task exhibit a certain degree of meta-cognition, although not to the same extent as humans.Proprietary models that are capable of solving the two-step task display model-based behavior at least on par with humans, with GPT-4 significantly surpassing human levels.Thus, within the scope of these two tasks, it seems that a model's performance can serve as an indicator of its corresponding behavioral metrics.In this context, meta-cognition and model-basedness appear to emerge as properties of high-performing models.</p>
<p>High performance despite lack of exploration: Interestingly, almost all models (except for text-bison) demonstrate super-human performance on the horizon task.While they exhibit high performance, they still lack exploration (except for LLaMA2-70-chat which exhibits higher-than-human random exploration).This underscores the importance of behavioral metrics in understanding the strategies employed by LLMs.In this case, it appears that LLMs achieve high performance primarily through exploitation without any human-like exploration.</p>
<p>Stronger priors than likelihoods: All models place much more weight on priors than observations, suggesting strong biases that are difficult to alter.Additionally, we can observe a prevalence of optimism bias and high learning rates.Almost all models exhibit a very strong optimism bias (except for text-bison), aligning with the notion that these LLMs harbor strong biases.Low performance but high behavioral variance for risktaking and temporal discounting: Temporal discounting and risk-taking behaviors exhibit high variance among models.While some models, such as text-bison and LLaMA-2-70, appear myopic on the temporal discounting task, others, including text-davinci-003, Claude, and LLaMA-2-70-chat, demonstrate a much more far-sighted approach.GPT-4, interestingly, exhibits behavior akin to humans.For the BART, models are positioned at extreme ends of the risktaking spectrum, i.e., they either never take any risks at all or always risk everything.LLaMA-2-70 and LLaMA-2-70-chat, for example, display the same performance in this task but exhibit opposite risk-taking behavior.This not only indicates a struggle for LLMs to apprehend risks but also underscores the importance of our benchmark.Indeed, it raises questions about what influences a model's behavior.It also highlights how recording only their performance would have overlooked the contrasting risk-taking behavior of the two LLaMA models.</p>
<p>The comparison between LLaMA-2-70 and LLaMA-2-70chat is particularly compelling.Even though LLaMA-2-70-chat is a fine-tuned version of LLaMA-2-70, they exhibit markedly different behavior in risk-taking, temporal discounting, and random exploration.This divergence is intriguing, especially considering their performance on all tasks is relatively similar.This observation sets the stage for the subsequent section, where we will conduct a more comprehensive analysis of how specific features of these models influence their performance and behaviors.</p>
<p>Hypothesis-driven experiments</p>
<p>CogBench provides researchers with the means to explore a broad spectrum of LLMs' behaviors.We have applied CogBench to 35 distinct LLMs.This diversity allows us to test how different aspects of LLMs, such as the number of parameters, the application of Reinforcement Learning from Human Feedback (RLHF), fine-tuning for code, and many more, can impact specific LLMs' performance and behaviors.</p>
<p>The metrics provided by CogBench enable us to perform various analyses to test specific hypotheses of interest.In this section, we formulate and test five hypotheses about different mechanisms in LLMs and how these can affect their behavioral profiles.We use both qualitative, visualizationbased techniques (dimensionality reduction) as well as quantitative analyses (multi-level regression) to test our hypotheses.For all regression analyses, we use the features of LLMs to predict specific behavioral metrics from the benchmark.The multi-level regression approach was chosen because some models are fine-tuned versions of other models.For instance, certain LlaMA models have a -chat version which adds RLHF and conversational fine-tuning, and thus are in the same higher-level group.This approach allows us to account for the hierarchical structure in our data and provides a more nuanced understanding of the behaviors of LLMs.We can isolate the effects of specific features or modifications by comparing models within the same higher-level group.</p>
<p>Hypothesis 1: Does RLHF make LLMs more humanlike?</p>
<p>To evaluate this hypothesis, we used UMAP (McInnes et al., 2020) on the ten behavioral metrics of all LLMs, as illustrated in Figure 3A.Clear separation is evident between LLMs that incorporate RLHF and those that do not.LLMs with RLHF demonstrate behaviors that appear, on average, roughly 2× more similar to human behavior compared to the models without.However, it is important to note that while UMAP space retains some global structure, it is primarily used for visualization purposes.Consequently, we also analyzed the average distances before dimensionality reduction (using normalized feature vectors), observing a 11.7% average decrease in L2-Norm distance for models with RLHF (Figure 3B).Conclusion: Hypothesis is supported.</p>
<p>Hypothesis 2: Does performance increase with the number of parameters, training data, and the inclusion of code?</p>
<p>To answer this question, we used the multi-level regression previously mentioned, focusing on the performance of LLMs.We performed a regression analysis with the average standardized performance scores across all seven tasks as the dependent variable, using LLMs' features as predictors.</p>
<p>We found that the number of parameters indeed had a significant influence on performance (β = 0.277 ± 0.39, z = 14.1, p &lt; 0.001; see Figure 4A).However, the size of the training dataset and the use of code training data did not have a substantial impact.One possible explanation for this could be that the quality of the training data, rather than its sheer volume, plays a more determining role in performance, as well as that larger models also tend to be trained on larger datasets.Conclusion: Hypothesis is partially supported.</p>
<p>Hypothesis 3: Does an increase of parameters, training data, and the inclusion of code increase modelbasedness?</p>
<p>We again used the multi-level regression technique from before, this time focusing on a specific behavioral metric: model-basedness.We found that the number of parameters had a significant positive effect (β = 0.481 ± 0.22, z = 4.2, p &lt; 0.001; see Figure 4B), while the size of the training dataset and the use of code training data did not appear to significantly influence model-basedness.This again suggests that the quality of the data might be more crucial than its quantity when it comes to determining both performance and the emergence of model-based behaviors here.However, identifying which factors constitute 'quality' in the data requires a deeper exploration.This highlights the issue of transparency about data.For a thorough evaluation of how specific data features impact the emergence of behavioral functionalities such as model-basedness, it is essential to be transparent about a model's data and methodologies.Conclusion: Hypothesis is partially supported.</p>
<p>Hypothesis 4: Does RLHF enhance meta-cognition?</p>
<p>To answer this question, we focus our multi-level regression on meta-cognition.Our analysis revealed a strong effect (β = 0.461 ± 0.15, z = 5.9, p &lt; 0.001; see Figure 4C), indicating that RLHF significantly increased meta-cognition in LLMs.This finding underscores the potential of RLHF in enhancing the cognitive capabilities of LLMs.Conclusion: Hypothesis is supported.</p>
<p>Hypothesis 5: Do open-source models take more risks?</p>
<p>The open-source feature could be seen as a proxy for the engineering efforts that proprietary models undergo.There is a growing body of research suggesting that hidden preprompts being one of them, can significantly influence the behavior of LLMs (Liu et al., 2023).They can act as a  form of 'priming' that guides the model's responses, potentially making the model more cautious and less likely to take risks by constraining the model towards safer behaviors.However, our regression analysis suggested otherwise: contrary to expectations, we observed a negative effect (β = −0.612± 0.11, z = −11.4,p &lt; 0.001; see Figure 4D), indicating that proprietary models, which often have hidden pre-prompts, are more likely to take risks.This surprising outcome could be influenced by various factors from different engineering techniques.However, this underscores the limited behavioral evaluation of these techniques.In the subsequent section, we aim to bridge this gap in understanding through an initial exploration into the change of behavior of two standard prompt-engineering techniques.Conclusion: Hypothesis is refuted.</p>
<p>Impact of prompt-engineering</p>
<p>We also explored the impact of prompt-engineering techniques, namely chain-of-thought (CoT) and take-a-stepback (SB) prompting, on the behavior of LLMs.Both techniques are incorporated at the end of a question: Take-a-step-back:</p>
<p>First take a step back and think in the following two steps to answer this:</p>
<p>Step 1) Abstract the key concepts and principles relevant to this question.</p>
<p>Step 2) Use the abstractions to reason through.</p>
<p>Chain-of-thought:</p>
<p>First break down the problem into smaller steps and reason through each step logically.</p>
<p>Their purpose is to stimulate the generation of reasoning steps.These steps serve as an additional context that the LLM can use to elicit better final responses.While these techniques have been shown to enhance performance, it is essential to confirm whether they indeed improve the behaviors they are designed to augment.</p>
<p>We focused on examining two specific behaviors that are hypothesized to improve with the inclusion of reasoning steps.These behaviors are the models' performance in the probabilistic reasoning task and their model-basedness.</p>
<p>We evaluated five specific LLMs: GPT-4, PaLM-2 for text (text-bison@002), Claude-1/2, and LLaMA-2, applying CoT and SB techniques and comparing the outcomes with their base models.The selection of these five models and a limited set of metrics was necessitated by the additional en- gineering effort required to process the outputs when using these techniques.The choice of LLMs aimed at ensuring a diverse representation of established models, considering the complexity of our benchmark tasks and the potential for erratic outputs from smaller LLMs when given the freedom to reason.For a comprehensive explanation of the querying process for these models, please refer to Appendix D.</p>
<p>Our investigation initially focused on probabilistic reasoning, which is a fundamental cognitive ability in decisionmaking.This ability facilitates the optimal integration of new information with pre-existing knowledge.We used the performance metric from the probabilistic reasoning experiment, namely posterior accuracy, which is calculated as one minus the deviation from the Bayes optimal prediction for each task.As depicted in Figure 6A, both CoT and SB techniques generally enhanced probabilistic reasoning compared to their base models, with CoT showing an average increase of 9.01% and SB showing an increase of 3.10%.</p>
<p>Furthermore, we discovered that model-basedness, a critical aspect of reasoning and planning, is significantly augmented by both CoT and SB techniques, as shown in Figure 6B.Specifically, CoT demonstrated a 64.59% increase, while SB showed a substantial increase of 118.59%.</p>
<p>Discussion</p>
<p>We have presented CogBench, a new open-source benchmark for evaluating LLMs.CogBench is rooted in wellestablished experimental paradigms from the cognitive psychology literature, providing a unique set of advantages over traditional LLM benchmarks.First, it is based on tried-andtested experiments whose measures have been extensively validated over many years and shown to capture general cognitive constructs.In addition, unlike standard benchmarks, CogBench does not only focus on performance metrics alone but also comes with behavioral metrics that allow us to gain insights into how a given task is solved.Finally, many of the included problems are procedurally-generated, thereby making it hard to game our benchmark by training on the test set.All our code and analysis will be publicly available, making it easy to use CogBench for the LLM community.</p>
<p>Our analyses yielded several key findings: as expected, RLHF enhanced the human-likeness of LLMs, while the number of parameters improved their performance and model-basedness.However, we also found surprising results.Despite expectations, code fine-tuning did not influence performance or model-basedness and open-source models exhibited less risk-taking behavior.Further, we found CoT prompting to be a promising choice for enhancing probabilistic reasoning.Conversely, SB prompting proved more effective for model-based reasoning.</p>
<p>While these results demonstrate the versatility of our benchmark, our analysis also faces several challenges.For instance, the limited transparency of certain proprietary models poses an issue to our regression analysis because acquiring details about certain models can be difficult or impossible.This lack of transparency could potentially affect the precision of our analysis.It also underscores the need for more transparency to facilitate more thorough and accurate evaluations (LAION, 2024;Binz et al., 2023).</p>
<p>Taken together, our study highlights the importance of behavioral metrics and cognitive modeling in evaluating LLMs and presents a novel benchmark for this purpose.The analysis was preliminary and intended to provide a broad view of how CogBench can be used.The primary aim of this work is to equip the LLM community with new tools, inspired by cognitive science, to evaluate their models more comprehensively.Future work should focus on three areas.First, while cognitive science studies have demonstrated the external validity of the investigated tasks, it is yet to be shown for LLMs.Furthermore, we aim to extend the set of included tasks to cover a broader set of domains.Finally, we plan to properly automate our benchmark, mostly for prompt engineering techniques that were only briefly examined in this study.This could include studying the influence of impersonation (Salewski et al., 2023) • Size of Dataset: This represents the size of the dataset on which the model was trained, expressed in trillions of tokens.</p>
<p>• Context Length: This refers to the length of the context available to the model during its operation.</p>
<p>• Conversational: This indicates whether the model was fine-tuned with conversational datasets.</p>
<p>• Code: This indicates whether the model was fine-tuned with code datasets.</p>
<p>Please note that the selection of features used for our analyses was made based on the best available knowledge of the authors, as some information about certain models can be challenging to obtain.This limitation could potentially impact the precision of the regression analysis.It underscores the need for greater transparency about LLMs to facilitate more thorough evaluations.Upon drawing a ball, participants can revise their belief about the chosen urn, considering both the wheel (prior) and the ball color (evidence).The task allows testing adaptability to different prior/likelihood scenarios by changing the wheel division and ball distributions.Agents have to estimate the probability of the drawn ball's urn.We use this task to estimate an agent's prior and likelihood weightings.In this task, people showed similar weighting between prior and likelihood, both under one.This underweighting is often referred to as system neglect (Massey &amp; Wu, 2005).</p>
<p>B. Comprehensive list &amp; explanation of the cognitive experiments</p>
<p>B.1.2. METHODS</p>
<p>We matched the probabilities used in (Dasgupta et al., 2020) to compare to human data.There they had either an informative likelihood case (P (left urn|red) = 0.7, 0.8 or 0.9) and an informative prior (P (left urn) = 0.5 or 0.6) or vice versa.They also trained humans on this experiment, so we only compared it to data from a human's first trial as we are not interested in learning but in how an LLM weighs its prior and likelihoods by default.The default number of simulations here was 100.</p>
<p>B.1.3. PROMPTS FOR LLMS</p>
<p>Example with informative likelihood</p>
<p>You are participating in an experiment where you are provided with a wheel of fortune and two urns.The wheel of fortune contains 10 evenly sized sections labeled either F or J, corresponding to the urns F and J. Another person will spin the wheel of fortune, select an urn based on the outcome of the spin, and then randomly pick a ball from the selected urn.Your goal is to give your best estimate of the probability of the urn being F after observing the ball drawn from the urn.</p>
<p>Q: The wheel of fortune contains 6 sections labeled F and 4 sections labeled J.The urn F contains (8, 2) and the urn J contains (2, 8) red/blue balls.A red ball was drawn.What is the probability that it was drawn from Urn F? (Give your probability estimate on the scale from 0 to 1 rounded to two decimal places).</p>
<p>A: I estimate the probability of the red ball to be drawn from the urn F to be 0.</p>
<p>B.1.4. METRICS</p>
<p>Performance: Calculated as the posterior accuracy, therefore 1 minus the Bayes optimal.</p>
<p>Behaviours 1 &amp; 2: Prior and likelihood weightings A generalized version of Bayes rule considers prior β 1 and likelihood β 2 weightings to account for biases in Bayesian updating:
P (A|B) ∝ P (B|A) β2 • P (A) β1
For analytical convenience, this model can be reformulated as linear in log-odds.By fitting this model to the data using least squares linear regression, we can obtain the maximum likelihood estimates of the prior and likelihood weightings:
log P (Urn F|Ball) 1 − P (Urn F|Ball) = β 0 + β 1 log P (Urn F) 1 − P (Urn F)
+ β 2 log P (Ball|Urn F) P (Ball|Urn J) -P (Urn F|Ball) is the subjective probability judgment of the urn being 'F' given the ball's color.</p>
<p>-P (Urn F) and P (Ball|Urn F) are the prior probability and likelihood, respectively.</p>
<p>β 1 and β 2 are the prior and likelihood weightings, respectively, which are given as exponents in a generalized version of Bayes' rule to capture specific biases.These two coefficients are the two behavioral metrics we report for this experiment.</p>
<p>β 0 is the intercept term.</p>
<p>B.2. Horizon task (Wilson et al., 2014) -Directed &amp; random exploration
B.2.1. SUMMARY
This task is a two-armed bandit task with stationary reward distributions.Agents first observe four reward values of randomly determined options, followed by making either one or six additional choices.We use this task to measure whether an agent uses uncertainty to guide its exploration behavior (directed exploration) and/or whether it injects noise into its policy to explore (random exploration).People are known to rely on a combination of both strategies when exploring (Wilson et al., 2014;Brändle et al., 2021).</p>
<p>B.2.2. METHODS</p>
<p>We followed the same methods for prompting LLMs as in (Binz &amp; Schulz, 2023).In the Horizon task, two distinct contexts are presented to participants, each differing in their time horizons.Each game involves 4 forced-choice trials, after which participants are given the opportunity to make a single choice (in the horizon 1 scenario) or six consecutive choices (in the horizon 6 scenario).The 4 forced-choice trials either offer one observation from one option and three from the other (unequal information condition), or two observations from each option (equal information condition).</p>
<p>The design of the horizon 1 and horizon 6 scenarios inherently provides a baseline for pure exploitation.Furthermore, the equal and unequal information conditions are designed to differentiate between directed and random exploration by examining the decision made in the first trial.In the equal information condition, a choice is categorized as random exploration if it aligns with the option with the lower average.Conversely, in the unequal information condition, a choice is classified as directed exploration if it aligns with the option that was observed less frequently during the forced-choice trials.</p>
<p>Our default number of simulations was 100.</p>
<p>B.2.3. PROMPTS FOR LLMS</p>
<p>Example with horizon 1 scenario</p>
<p>You are going to a casino that owns two slot machines.You earn money each time you play on one of these machines.</p>
<p>You have received the following amount of dollars when playing in the past: -Machine J delivered 15 dollars.</p>
<p>-Machine F delivered 37 dollars.</p>
<p>-Machine F delivered 28 dollars.</p>
<p>-Machine J delivered 11 dollars.</p>
<p>Your goal is to maximize the sum of received dollars within one additional round.</p>
<p>Q: Which machine do you choose?</p>
<p>A: Machine</p>
<p>B.2.4. METRICS</p>
<p>Performance: Average delivered dollars.</p>
<p>Behaviour 1 -Directed Exploration: This metric is analyzed in the unequal information condition.Here, a regression is performed on the choice variable using three regressors:</p>
<p>• x1 represents the difference in rewards,</p>
<p>• x2 represents the horizon (binary variable), and</p>
<p>• x3 is the interaction term of x1 and x2 (i.e., x1 × x2).</p>
<p>The beta coefficient for x2 (the presence or not of a horizon) is then extracted as the measure of directed exploration.</p>
<p>Behaviour 2 -Random exploration: We follow the same procedure as for the directed exploration but in the equal information condition to measure random exploration.However, in this case, the beta coefficient for x3 (the interaction effect between the difference in rewards and the presence of a horizon) from the regression provides the measure of random exploration.</p>
<p>B.3.Restless bandit task (Ershadmanesh et al., 2023) -Meta-cognition
B.3.1. SUMMARY
This is a two-armed bandit task with non-stationary reward distributions.There is always one option with a higher average reward.Every few trials a switch between the reward distributions of the two options occurs.Agents furthermore have to indicate after each choice how confident they are in their decisions.We use this task to measure meta-cognition, which indicates whether an agent can assess the quality of its own cognitive abilities.People generally display this ability but its extent is influenced by various internal and external factors (Shekhar &amp; Rahnev, 2021).</p>
<p>B.3.2. METHODS</p>
<p>In each trial, LLMs are tasked with choosing between one arm which samples a reward from a normal distribution N (60, 8), while the other arm samples a reward from a N (40, 8).LLMs are informed that the slot machine with the higher average reward changes every 18-22 trials.</p>
<p>Additionally, in each trial, LLMs must express their confidence in their choice on a scale from 0 to 1, as opposed to humans who use a Likert scale.The task is composed of 4 blocks, each containing 18-22 trials, resulting in approximately 80 trials in total.This is in contrast to the human task, which consists of 20 blocks for a total of 400 trials.The decision to limit the number of trials was made due to context size restrictions for some LLMs.</p>
<p>Our default number of simulations was 10.</p>
<p>B.3.3. PROMPTS FOR LLMS</p>
<p>Example for reporting confidence at trial 23 Q: You are going to a casino that owns two slot machines named machine J and F.You earn dollars $ each time you play on one of these machines with one machine always having a higher average $ reward.Every 18 to 22 trials a switch of block takes place and the other slot machine will now give the higher point reward on average.However, you are not told about the change of block.After each choice, you have to indicate how confident you were about your choice being the best on a scale from 0 to 1.The casino includes 4 blocks of 18 to 22 trials, for a total of 80 trials 't'.Your goal is to interact with both machines and optimize your $ as much as possible by identifying the best machine at a given point in time which comes in hand with being attentive to a potential change of block.The rewards will range between 20$ and 80$.</p>
<p>You have received the following amount of $ when playing in the past: t=1: You chose J with a reported confidence of 0.43.It rewarded 54 $. t=2: You chose J with a reported confidence of 0.53.It rewarded 57 $. t=3: You chose J with a reported confidence of 0.88.It rewarded 70 $.... t=17: You chose F with a reported confidence of 0.99.It rewarded 59 $. t=18: You chose F with a reported confidence of 0.44.It rewarded 45 $. t=19: You chose J with a reported confidence of 0.06.It rewarded 61 $. t=20: You chose J with a reported confidence of 0.51.It rewarded 64 $. t=21: You chose J with a reported confidence of 0.37.It rewarded 59 $. t=22: You chose J with a reported confidence of 0.54.It rewarded 42 $.</p>
<p>Q: You are now in trial t=23.Which machine do you choose between machine J and F?(Think carefully remembering that exploration of both machines is required for optimal rewards.Give the answer in the form 'Machine <your choice>'.)</p>
<p>A: Machine F.</p>
<p>Q: How confident are you about your choice being the best on a continuous scale running from 0 representing " 'this was a guess' to 1 representing 'very certain'?(Think carefully and give your answer to two decimal places)</p>
<p>A: On a scale from 0 to 1, I am confident at 0.</p>
<p>B.3.4. METRICS</p>
<p>Performance: Accuracy of choosing the best arm at a given trial.</p>
<p>Behaviour -Meta-cognition: We report the metacognitive sensitivity of a model by reporting the adjusted QSR (Carpenter et al., 2019) defined as
QSR = 1 − (accuracy − scaled confidence) 2
which is a standard metric for metacognitive sensitivity.The scaled confidence is computed as scaled confidence = confidence − lowest reported confidence highest reported confidence − lowest reported confidence Instrumental learning (Lefebvre et al., 2017): LLMs encounter four two-armed bandit problems in an interleaved order.Each bandit problem is identified by a unique symbol pair.We use this task to investigate how an agent learns.First, we report the learning rate of the agent which is common practice in two-armed bandits.Furthermore, we use it to reveal whether an agent learns more from positive than from negative prediction errors, i.e., whether it has an optimism bias.People commonly display asymmetric tendencies when updating their beliefs by showing higher learning rates after encountering positive prediction errors compared to negative ones (Palminteri &amp; Lebreton, 2022).</p>
<p>B.4.2. METHODS</p>
<p>As in (Lefebvre et al., 2017), the task is 4 two-armed bandits of 96 trials (24 per slot machine).Here we randomly sample (without replacement) two letters for each to avoid biases towards a given letter.We used a cover story that involved a gambler visiting different casinos to generate our prompts.This choice has been inspired by similar tasks for human experiments (Gershman, 2018) and LLMs (Binz &amp; Schulz, 2023;Coda-Forno et al., 2024).Our default number of simulations per LLM is 10.</p>
<p>Casinos have the same reward probabilities as in the paper's first experiment: All arms have probabilities P=0.75 or 0.25 of winning 1 dollar and a reciprocal probability (1 -P) of getting nothing.In two casinos, the reward probability was the same for both arms ('symmetric' conditions), and in two other conditions, the reward probability was different across symbols ('asymmetric' conditions).</p>
<p>B.4.3. PROMPTS FOR LLMS</p>
<p>Example for 5th trial</p>
<p>You are going to visit four different casinos (named 1, 2, 3, and 4) 24 times each.Each casino owns two slot machines which all return either 1 or 0 dollars stochastically with different reward probabilities.Your goal is to maximize the sum of received dollars within 96 visits.Behaviour 1 -Learning rate: We fit a Rescorla-Wagner model (Rescorla, 1972) which is standard to retrieve learning rates in two-armed bandits.This model operates under the assumption that decisions are made according to a Softmax function, which takes into account the predicted values of both arms.Each predicted value is updated using ∆V = α× prediction error where ∆V represents the change in value, and α denotes the learning rate.We report the learning rate which minimizes the negative log-likelihood.</p>
<p>Behaviour 2 -Optimism bias: As in (Lefebvre et al., 2017), we retrieve the optimism bias by assuming that there were two different learning rates, one for positive (α + ) and one for negative (α − ) prediction errors, sometimes called the RW ± model.The two learning rates were fit in the same way as for the standard Rescorla-Wagner model and the Optimism bias is computed as α + − α − .This measure provides a quantitative representation of an individual's tendency to learn more from positive outcomes than from negative ones.B.5.Two step task (Daw et al., 2011) -Model-basedness B.5.1.SUMMARY This is a decision-making task in which agents have to accumulate as many treasures as possible.Taking an action from a starting state transfers the agent to one out of two second-stage states.In each of these second-stage states, the agent has the choice between two options that probabilistically lead to treasures.Finally, the agent is transferred back to the initial state and the process repeats for a predefined number of rounds.The task experimentally disentangles model-based from model-free reinforcement learning.We therefore use it to measure an agent's model-basedness.Previous studies using this task have shown that people rely on a combination of model-free and model-based reinforcement learning (Daw et al., 2011).</p>
<p>B.5.2. METHODS</p>
<p>We followed the same methods for LLMs as in (Binz &amp; Schulz, 2023) with a 20-day horizon.Our default number of simulations was 100.</p>
<p>The transition probabilities from the first stage to the chosen second stage are fixed at 70%.The two-step task gauges model-based decision-making by observing how past outcomes influence current choices.If a participant's decisions reflect the previous trial's second-stage state and reward, it suggests model-based decision-making, as they're using a cognitive model of the task.However, if decisions are solely based on the previous trial's first-stage choice and reward, it indicates model-free decision-making.</p>
<p>B.5.3. PROMPTS FOR LLMS</p>
<p>Example on 5th day after choosing planet Y for the first-step of the task.</p>
<p>You will travel to foreign planets in search of treasures.When you visit a planet, you can choose an alien to trade with.The chance of getting treasures from these aliens changes over time.Your goal is to maximize the number of received treasures.</p>
<p>Your previous space travels went as follows: -4 days ago, you boarded the spaceship to planet Y, arrived at planet Y, traded with alien J, and received treasures.The regression is performed with the 'stay probabilities' as the dependent variable, and x1, x2, and x3 as the independent variables.The 'stay probabilities' represent the likelihood of a participant repeating the same first-stage choice on the next trial.We then retrieve the beta parameter for the interaction effect.</p>
<p>In essence, the interaction effect captures how the influence of rewards on stay probabilities changes depending on whether the previous trial involved a common or rare transition.A significant beta parameter for x3 would suggest that the effect of rewards on stay probabilities is not the same for common and rare transitions, indicating the presence of model-based decision-making.</p>
<p>B.6.Temporal discounting (Ruggeri et al., 2022) B.6.1.SUMMARY Agents have to make a series of choices between two options.Each option is characterized by a monetary outcome and an associated delay until the outcome is received.We use this task to assess temporal discounting, indicating whether an agent prefers smaller but immediate gains over larger delayed ones.People generally show a preference for immediate gains, although the precise functional form of their discounting is still a matter of debate (Cavagnaro et al., 2016).</p>
<p>B.6.2. METHODS</p>
<p>This task tests discounting patterns from three baseline scenarios to determine preference for immediate or delayed choices for gains (at two magnitudes) and losses (one).Second, they analyzed the prevalence of all choice anomalies using 4 additional items.Participants responded to 10 to 13 questions, depending on their responses to the initial three sets.Each baseline consisted of five sub-questions.Individuals saw at most three sub-questions depending on the order of their choices.It is worth noting that since this task is the only one which is not procedurally generated, there is only one simulation needed per LLM.Behaviour: We only report one temporal discounting metric as behavioral.We use the same scoring system as in (Ruggeri et al., 2022) where the score is between 0 and 19.</p>
<p>The constructed score corresponded to the ranking of the sub-question where the individual switched from choosing the delayed option towards the sooner option.An individual who always chose the latter option was assigned a score of 0. An individual who always chose the sooner option was assigned a score of 5 (for baselines) and 1 for each of the 4 biases.Adding this score across questions results in a total score of 0 for individuals always choosing the delayed amount and 19 (5+5+5+4) for individuals always choosing the sooner amount.B.7. Balloon Analogue Risk Task (BART) (Lejuez et al., 2002) -Risk B.7.1.SUMMARY Agents have to inflate an imaginary balloon to obtain rewards.They may choose to stop inflating and cashing out all rewards accumulated so far.There is a chance that the balloon pops at any point in time and all rewards will be lost.We use this task to assess risk-taking behavior.Human risk-taking in this task is "significantly correlated with scores on self-report measures of risk-related constructs and with the self-reported occurrence of real-world risk behaviors" (Lejuez et al., 2002).</p>
<p>B.7.2. METHODS</p>
<p>We substitute the colors of the balloons with the letters A, B, and C. We reduce the number of balloons from 15 to 10 per category to avoid context-size limits from some LLMs.Our default number of simulations per LLM is 10.</p>
<p>For the three balloons we sample from the same three distributions as in the paper where probabilities increase when inflating the balloon but start at 1/32, 1/8 and 1/128 per different balloon category.We randomly shuffle the assignment to A, B, and C to avoid biases of LLMs towards certain letters.</p>
<p>B.7.3. PROMPTS FOR LLMS</p>
<p>Example for 5th balloon</p>
<p>In this game, you will encounter 3 different balloons labeled A, B, and C.There will be a total of 10 balloons for each type of balloon.Your goal is to accumulate as many points as possible without popping the balloon.You will be presented with a balloon and given the option to inflate it or not.Each inflation increases the balloon's size and potential points but also carries a risk of the balloon popping.Your task is to decide whether to inflate the balloon or not knowing that a successful inflation adds 1 point from that balloon.Once you decide to stop inflating the balloon, you can no longer earn points from that balloon.If the balloon pops before you stop inflating, you will lose all the points accumulated in that balloon.Your final score will be determined by the total number of points earned across all 30 balloons.Your goal is to maximize your final score.</p>
<p>You observed the following previously where the type of balloon is given in parenthesis: -Balloon 1 (A): You inflated the balloon 1 times for a total of 1 point.It did not explode.</p>
<p>-Balloon 2 (C): You inflated the balloon 4 times for a total of 4 points.It did not explode.</p>
<p>-Balloon 3 (A): You inflated the balloon 7 times for a total of 0 points.It did explode.</p>
<p>-Balloon 4 (C): You inflated the balloon 5 times for a total of 5 points.It did not explode.</p>
<p>-Balloon 5 (A): You inflated the balloon 9 times for a total of 0 points.It did explode.Behaviour: Risk In the paper they report the adjusted risk which is defined as the average number of pumps excluding balloons that exploded.However, this does not take into account edge behaviours which always inflate which is the case for some LLMs and therefore we decided to report the risk as the average number of inflation attempts.</p>
<p>Figure 1 .
1
Figure 1.Overview of approach and methods.CogBench provides open access to seven different cognitive psychology experiments.These experiments are text-based and can be run to evaluate any LLM's behavior.The experiments are submitted to LLMs as textual prompts and the models indicate their choices by completing a given prompt.Past behavior is then concatenated to the prompt and learning is induced via prompt-chaining.We used 35 LLMs in total, including most larger proprietary LLMs as well as many open-source models.</p>
<p>Figure 2 .
2
Figure 2. CogBench results for established LLMs.A: Performance metrics, B: Behavioral metrics.All metrics are human-normalized: a value of zero corresponds to a random agent, while a value of one corresponds to the average human subject (dotted lines).</p>
<p>Figure 3 .
3
Figure3.A: UMAP visualization of the ten behavioral metrics for all LLMs.Each point represents an LLM, with models using RLHF and models without RLHF indicated by different colors.B: Difference in average L2-norm with humans between RLHF models and non-RLHF models.</p>
<p>Figure 4 .
4
Figure 4. Multi-level regressions of LLMs features onto different performance or behavioral metrics.Red bars represent effects included in a hypothesis.A: Regression onto all task performances.B: Regression onto model-basedness.C: Regression onto meta-cognition.D: Regression onto risk taking.*** : p &lt; 0.001, ** : 0.001 ≤ p &lt; 0.01, * : 0.01 ≤ p &lt; 0.05</p>
<p>Figure 5 .
5
Figure5.Difference of chain-of-thoughts and take-a-step-back prompting to baseline models on A: Posterior accuracy, B: Modelbasedness.The aggregated scores are computed using a weighted average of all five models using inverse-variance weighting.</p>
<p>B. 1 .
1
Probabilistic reasoning (Dasgupta et al., 2020) -Prior &amp; likelihood weighting B.1.1.SUMMARY This experiment tests how agents update beliefs based on new evidence.Participants are given a wheel of fortune (representing initial prior probabilities) and two urns with different colored ball distributions (representing likelihoods).</p>
<p>B. 4 .
4
Experiment 2: Instrumental learning(Lefebvre et al., 2017) -Optimism bias &amp; learning rate B.4.1.SUMMARY</p>
<p>You have received the following amount of dollars when playing in the past: -Machine Q in Casino 4 delivered 0.0 dollars.-Machine B in Casino 1 delivered 1.0 dollars.-Machine B in Casino 1 delivered 0.0 dollars.-Machine R in Casino 3 delivered 0.0 dollars.Q: You are now in visit 5 playing in Casino 4. Which machine do you choose between Machine Q and Machine D?(Give the answer in the form "Machine <your choice>").The performance is the average amount of money retrieved by the LLM.</p>
<p>-3 days ago, you boarded the spaceship to planet Y, arrived at planet X, traded with alien D, and received treasures.-2 days ago, you boarded the spaceship to planet Y, arrived at planet Y, traded with alien J, and received junk.-1 day ago, you boarded the spaceship to planet Y, arrived at planet X, traded with alien D, and received treasures.Q: Do you want to take the spaceship to planet X or planet Y? A: Planet Y.You arrive at planet Y. Q: Do you want to trade with alien J or K? Average number of received treasures.It is worth noting that the design of this experiment was done in a way that being model-free or model-based retrieves the same amount of rewards in average.Behaviour -Model-basedness: To retrieve the model-basedness of an agent, we compute a regression using three regressors: • x1 representing rewards, • x2 representing common transitions (binary variable) and • x3 is the interaction term of x1 and x2 (i.e., x1 × x2).</p>
<p>B</p>
<p>.6.3.PROMPTS FOR LLMS Examples for first baseline Q: What do you prefer between the following two options: -Option 1: Receive 500 dollars now. -Option 2: Receive 550 dollars in 12 months.A: I prefer option 2. Q: What do you prefer between the following two options: -Option 1: Receive 500 dollars now. -Option 2: Receive 600 dollars in 12 months.A: I prefer option Examples for 2nd baseline (different magnitude) Q: What do you prefer between the following two options: -Option 1: Receive 5000 dollars now. -Option 2: Receive 5500 dollars in 12 months.A: I prefer option 1. Q: What do you prefer between the following two options: -Option 1: Receive 5000 dollars now. -Option 2: Receive 5100 dollars in 12 months.A: I prefer option 1. "Q: What do you prefer between the following two options: -Option 1: Receive 5000 dollars now. -Option 2: Receive 5050 dollars in 12 months.A: I prefer option Examples for 3rd baseline (loss as opposed to gain) Q: What do you prefer between the following two options: -Option 1: Pay 500 dollars now. -Option 2: Pay 550 dollars in 12 months.A: I prefer option 1 Q: What do you prefer between the following two options: -Option 1: Pay 500 dollars now. -Option 2: Pay 510 dollars in 12 months.A: I prefer option 1 Q: What do you prefer between the following two options: -Option 1: Pay 500 dollars now. -Option 2: Pay 505 dollars in 12 months.A: I prefer option Example for testing present bias Q: What do you prefer between the following two options: -Option 1: Receive 500 dollars in 12 months.-Option 2: Receive 600 dollars in 24 months.A: I prefer option Example for testing subbaddictivity Q: What do you prefer between the following two options: -Option 1: Receive 500 dollars now. -Option 2: Receive 700 dollars in 24 months.A: I prefer option Example for testing delay-speedup asymmetry Q: What do you prefer between the following two options: -Option 1: Receive 500 dollars now. -Option 2: Wait 12 months for the 500 dollars but with an additional 99 dollars.A: I prefer option Example for testing delay-length asymmetry Q: What do you prefer between the following two options: -Option 1: Wait 12 months to receive 600 dollars now. -Option 2: Pay 100 dollars and receive the 600 dollars gain now.A: I prefer option B.6.4.METRICS</p>
<p>Q: You are currently with Balloon 5 which is a balloon of type A. What do you do? (Option 1 for 'skip' or 0 for 'inflate') The performance is the average points across all simulations.</p>
<p>Figure 7 .
7
Figure 7. Behavioral metrics</p>
<p>https://github.com/juliancodaforno/CogBench
A computational phenotype is a collection of mathematically derived parameters that precisely describe individuals across different domains(Patzelt et al., 2018;Montague et al., 2012;Schurr et al., 2023).
It is worth noting that although there are seven experiments, there are only six performance metrics since the temporal discounting experiment's performance metric is used as a behavioral one.
List of LLMs used Model Name No. of Parameters Finetuned LLM Use of RLHF Open Source Size of Dataset Context Length Conversational Code GPT-4 1760 No Yes No 1.56 8 No No text-davinci-003 170 No Yes No 1.37 4 No No text-davinci-002 170 No No No 1.37 4 No No Claude-1 100 No RLAIF No 3.7 100 No No Claude-2 200 No RLAIF No 7.4 100 No No text-bison@002 340 No Yes No 1.4 8 No No Falcon-40b 40 No No Yes 0.54 2 No No Falcon-40b-instruct 40 Falcon-40b No Yes 0.6 2 No No MPT-30b 30 No No Yes 1.76 8 No No MPT-30b-instruct 30 MPT-30b No Yes 1.8 8 No No MPT-30b-chat 30 MPT-30b No Yes 1.8 8 Yes No LLaMA-2-70 70 No No Yes 2 4 No No LLaMA-2-13 13 No No Yes 2 4 No NoC. Full benchmark results for rest of LLMsD. Prompt Engineering techniquesIn both the CoT and SB experiments, we appended specific prompts at the end (details provided below) where the function 'self.formatanswer' was different for each experiment.We imposed a limit of 300 tokens for an LLM.This approach, however, presented some challenges when compared with the standard benchmark analysis, which is designed to output a maximum of one token to ensure a context that enforces a one-token answer.When we permit an LLM to modify the context with a flexible number of tokens, despite our attempts to enforce a maximum word limit, some LLMs do not consistently adhere to this constraint.This flexibility introduces complexity into the process of automating these engineering techniques across different experiments for various types of LLMs.Additionally, some LLMs begin to exhibit chaotic behavior, and once this occurs, it becomes difficult to revert to a controlled state.This phenomenon, known as the 'Waluigi effect'(Nardo, 2024), underscores the challenges of managing the balance between flexibility and control in the design and operation of LLMs.Example for take-a-step-backFirst, take-a-step-back and think in the following two steps to answer this:Step 1) Abstract the key concepts and principles relevant to this question in a maximum of 60 words."Step 2) Use the abstractions to reason through the question in a maximum of 60 words.Finally, give your final answer in the format 'Final answer: {self.formatanswer}<your choice>'.It is very important that you always answer in the right format even if you have no idea or you believe there is not enough information.A:Step 1)Example for chain-of-thoughtFirst break down the problem into smaller steps and reason through each step logically in a maximum of 100 words before giving your final answer in the format 'Final answer: {self.formatanswer}<your choice>'.It is very important that you always answer in the right format even if you have no idea or you believe there is not enough information.A: Let's think step by step:
Playing repeated games with large language models. E Akata, L Schulz, J Coda-Forno, S J Oh, M Bethge, E Schulz, 2023</p>
<p>Falcon-40B: an open large language model with stateof-the-art performance. E Almazrouei, H Alobeidli, A Alshamsi, A Cappelli, R Cojocaru, M Debbah, E Goffinet, D Heslow, J Launay, Q Malartic, B Noune, B Pannier, G Penedo, 2023</p>
<p>Anthropic, Claude, Blog post. 2. 2023</p>
<p>Using cognitive psychology to understand gpt-3. M Binz, E Schulz, Proceedings of the National Academy of Sciences. 1206e22185231202023</p>
<p>How should the advent of large language models affect the practice of science. M Binz, S Alaniz, A Roskies, B Aczel, C T Bergstrom, C Allen, D Schad, D Wulff, J D West, Q Zhang, arXiv:2312.037592023arXiv preprint</p>
<p>On the opportunities and risks of foundation models. R Bommasani, D A Hudson, E Adeli, R Altman, S Arora, S Von Arx, M S Bernstein, J Bohg, A Bosselut, E Brunskill, arXiv:2108.072582021arXiv preprint</p>
<p>Exploration beyond bandits. The drive for knowledge: The science of human information seeking. F Brändle, M Binz, E Schulz, 2021</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in neural information processing systems. 202033</p>
<p>S Bubeck, V Chandrasekaran, R Eldan, J Gehrke, E Horvitz, E Kamar, P Lee, Y T Lee, Y Li, S Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023arXiv preprint</p>
<p>Rethink reporting of evaluation results in ai. R Burnell, W Schellaert, J Burden, T D Ullman, F Martinez-Plumed, J B Tenenbaum, D Rutar, L G Cheke, J Sohl-Dickstein, M Mitchell, Science. 38066412023</p>
<p>L M S Buschoff, E Akata, M Bethge, E Schulz, Visual cognition in multimodal large language models. 2024</p>
<p>Domain-general enhancements of metacognitive ability through adaptive training. J Carpenter, M T Sherman, R A Kievit, A K Seth, H Lau, S M Fleming, Journal of Experimental Psychology: General. 1481512019</p>
<p>On the functional form of temporal discounting: An optimized adaptive test. D R Cavagnaro, G J Aranovich, S M Mcclure, M A Pitt, Myung , J I , Journal of Risk and Uncertainty. 522016</p>
<p>. M Chen, J Tworek, H Jun, Q Yuan, H P De Oliveira Pinto, J Kaplan, H Edwards, Y Burda, N Joseph, G Brockman, A Ray, R Puri, G Krueger, M Petrov, H Khlaaf, G Sastry, P Mishkin, B Chan, S Gray, N Ryder, M Pavlov, A Power, L Kaiser, M Bavarian, C Winter, P Tillet, F P Such, D Cummings, M Plappert, F Chantzis, E Barnes, A Herbert-Voss, W H Guss, A Nichol, A Paino, N Tezak, J Tang, I Babuschkin, S Balaji, S Jain, W Saunders, C Hesse, A N Carr, J Leike, J Achiam, V Misra, E Morikawa, A Radford, M Knight, M Brundage, M Murati, K Mayer, P Welinder, B Mcgrew, D Amodei, S Mccandlish, I Sutskever, Zaremba , 2021Evaluating large language models trained on code</p>
<p>The emergence of economic rationality of gpt. Proceedings of the National Academy of Sciences. Y Chen, T X Liu, Y Shan, S Zhong, 10.1073/pnas1202023</p>
<p>10.1073/pnas.2316205120URL. </p>
<p>Deep reinforcement learning from human preferences. Advances in neural information processing systems. P F Christiano, J Leike, T Brown, M Martic, S Legg, D Amodei, 201730</p>
<p>K Cobbe, V Kosaraju, M Bavarian, M Chen, H Jun, L Kaiser, M Plappert, J Tworek, J Hilton, R Nakano, C Hesse, J Schulman, Training verifiers to solve math word problems. 2021</p>
<p>J Coda-Forno, K Witte, A K Jagadish, M Binz, Z Akata, E Schulz, arXiv:2304.11111Inducing anxiety in large language models increases exploration and bias. 2023arXiv preprint</p>
<p>Meta-in-context learning in large language models. J Coda-Forno, M Binz, Z Akata, M Botvinick, J Wang, E Schulz, Advances in Neural Information Processing Systems. 202436</p>
<p>Structured, flexible, and robust: benchmarking and improving large language models towards more humanlike behavior in out-of-distribution reasoning tasks. K M Collins, C Wong, J Feng, M Wei, J B Tenenbaum, arXiv:2205.057182022arXiv preprint</p>
<p>A theory of learning to infer. I Dasgupta, E Schulz, J B Tenenbaum, S J Gershman, Psychological review. 12734122020</p>
<p>Language models show human-like content effects on reasoning. I Dasgupta, A K Lampinen, S C Chan, A Creswell, D Kumaran, J L Mcclelland, F Hill, arXiv:2207.070512022arXiv preprint</p>
<p>Model-based influences on humans' choices and striatal prediction errors. N D Daw, S J Gershman, B Seymour, P Dayan, R J Dolan, Neuron. 6962011</p>
<p>Meta-cognitive efficiency in learned valuebased choice. S Ershadmanesh, A Gholamzadeh, K Desender, P Dayan, 10.32470/CCN.2023.1570-02023 Conference on Cognitive Computational Neuroscience. 2023</p>
<p>Deconstructing the human algorithms for exploration. S J Gershman, arXiv:2305.10403Google. Palm 2 technical report. 2018. 2023173arXiv preprint</p>
<p>Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in chatgpt. T Hagendorff, S Fabi, M Kosinski, Nature Computational Science. 3102023</p>
<p>Measuring massive multitask language understanding. D Hendrycks, C Burns, S Basart, A Zou, M Mazeika, D Song, J Steinhardt, 2021</p>
<p>Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. M Joshi, E Choi, D S Weld, L Zettlemoyer, 2017</p>
<p>J Kaplan, S Mccandlish, T Henighan, T B Brown, B Chess, R Child, S Gray, A Radford, J Wu, D Amodei, arXiv:2001.08361Scaling laws for neural language models. 2020arXiv preprint</p>
<p>Large language models are zero-shot reasoners. Advances in neural information processing systems. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, 202235</p>
<p>Towards a transparent ai future: The call for less regulatory hurdles on open-source ai in europe. 2024. January 19. 2024LAION</p>
<p>Can language models learn from explanations in context?. A K Lampinen, I Dasgupta, S C Chan, K Matthewson, M H Tessler, A Creswell, J L Mcclelland, J X Wang, F Hill, arXiv:2204.023292022arXiv preprint</p>
<p>Behavioural and neural characterization of optimistic reinforcement learning. G Lefebvre, M Lebreton, F Meyniel, S Bourgeois-Gironde, S Palminteri, Nature Human Behaviour. 14672017</p>
<p>Evaluation of a behavioral measure of risk taking: the balloon analogue risk task (bart). C W Lejuez, 10.1037//1076-898x.8.2.75Journal of experimental psychology. Applied. 822002</p>
<p>Prompting frameworks for large language models: A survey. X Liu, J Wang, J Sun, X Yuan, G Dong, P Di, W Wang, D Wang, 2023</p>
<p>Detecting regime shifts: The causes of under-and overreaction. C Massey, G Wu, Management Science. 5162005</p>
<p>Embers of autoregression: Understanding large language models through the problem they are trained to solve. R T Mccoy, S Yao, D Friedman, M Hardy, T L Griffiths, arXiv:2309.136382023arXiv preprint</p>
<p>Umap: Uniform manifold approximation and projection for dimension reduction. L Mcinnes, J Healy, J Melville, 2020</p>
<p>Computational psychiatry. P R Montague, R J Dolan, K J Friston, P Dayan, Trends in cognitive sciences. 1612012</p>
<p>Introducing mpt-30b: Raising the bar for opensource foundation models. Mosaicml, </p>
<p>The waluigi effect (mega-post). C Nardo, arXiv:2303.08774OpenAI. Gpt-4 technical report. 2024. January 19. 2024. 2023arXiv preprint</p>
<p>The computational roots of positivity and confirmation biases in reinforcement learning. S Palminteri, M Lebreton, Trends in Cognitive Sciences. 2022</p>
<p>Computational phenotyping: using models to understand individual differences in personality, development, and mental illness. E H Patzelt, C A Hartley, S J Gershman, Personality Neuroscience. 1e182018</p>
<p>Classical conditioning ii: current research and theory. R A Rescorla, 197264</p>
<p>The globalizability of temporal discounting. K Ruggeri, A Panin, M Vdovic, B Većkalov, N Abdul-Salaam, J Achterberg, C Akil, J Amatya, K Amatya, T L Andersen, Nature Human Behaviour. 6102022</p>
<p>L Salewski, S Alaniz, I Rio-Torto, E Schulz, Z Akata, arXiv:2305.14930-context impersonation reveals large language models' strengths and biases. 2023arXiv preprint</p>
<p>R Schaeffer, B Miranda, S Koyejo, arXiv:2304.15004Are emergent abilities of large language models a mirage?. 2023arXiv preprint</p>
<p>Dynamic computational phenotyping of human cognition. R Schurr, D Reznik, H Hillman, R Bhui, S J Gershman, 2023</p>
<p>Sources of metacognitive inefficiency. M Shekhar, D Rahnev, Trends in Cognitive Sciences. 2512021</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. A Srivastava, Authors, 2023</p>
<p>Understanding the capabilities, limitations, and societal impact of large language models. A Tamkin, M Brundage, J Clark, D Ganguli, arXiv:2102.025032021arXiv preprint</p>
<p>Llama 2: Open foundation and finetuned chat models. H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, N Bashlykov, S Batra, P Bhargava, S Bhosale, arXiv:2307.092882023arXiv preprint</p>
<p>Large language models fail on trivial alterations to theory-of-mind tasks. T Ullman, arXiv:2302.083992023arXiv preprint</p>
<p>J Wei, Y Tay, R Bommasani, C Raffel, B Zoph, S Borgeaud, D Yogatama, M Bosma, D Zhou, D Metzler, arXiv:2206.07682Emergent abilities of large language models. 2022arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E Chi, Q Le, D Zhou, 2023</p>
<p>Humans use directed and random exploration to solve the explore-exploit dilemma. R C Wilson, A Geana, J M White, E A Ludvig, J D Cohen, Journal of Experimental Psychology: General. 143620742014</p>
<p>Studying and improving reasoning in humans and machines. N Yax, H Anlló, S Palminteri, arXiv:2309.124852023arXiv preprint</p>
<p>Take a step back: Evoking reasoning via abstraction in large language models. H S Zheng, S Mishra, X Chen, H.-T Cheng, E H Chi, Q V Le, D Zhou, 2023a</p>
<p>Judging llm-as-ajudge with mt-bench and chatbot arena. L Zheng, W.-L Chiang, Y Sheng, S Zhuang, Z Wu, Y Zhuang, Z Lin, Z Li, D Li, E P Xing, H Zhang, J E Gonzalez, I Stoica, arXiv:2306.056852023barXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
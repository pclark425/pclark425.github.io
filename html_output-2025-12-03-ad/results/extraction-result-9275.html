<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9275 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9275</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9275</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-163.html">extraction-schema-163</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <p><strong>Paper ID:</strong> paper-258615377</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2305.07004v2.pdf" target="_blank">Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) demonstrate impressive multilingual capability, but their performance varies substantially across different languages. In this work, we introduce a simple yet effective method, called cross-lingual-thought prompting (XLT), to systematically improve the multilingual capability of LLMs. Specifically, XLT is a generic template prompt that stimulates cross-lingual and logical reasoning skills to enhance task performance across languages. We conduct comprehensive evaluations on 7 typical benchmarks related to reasoning, understanding, and generation tasks, covering both high-resource and low-resource languages. Experimental results show that XLT not only remarkably enhances the performance of various multilingual tasks but also significantly reduces the gap between the average performance and the best performance of each task in different languages. Notably, XLT brings over 10 points of average improvement in arithmetic reasoning and open-domain question-answering tasks.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9275.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9275.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>XLT (zero-shot) on MGSM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cross-Lingual-Thought prompting (XLT) — zero-shot on MGSM arithmetic reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of the XLT prompt template (language-independent, includes role assignment, cross-lingual rephrasing to English, CoT-style step instructions, and strict output formatting) in a zero-shot setup on an arithmetic reasoning benchmark (MGSM).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MGSM (arithmetic reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multilingual arithmetic reasoning benchmark (parallel instances across many languages). Metric: accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot: XLT template (English-written template with Role Assigning, Task Inputting, Cross-lingual Thinking that asks the model to rephrase input in English, Task Analyzing, CoT Task Solving (step-by-step), and Output Formatting). Greedy decoding (temperature=0).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Basic Prompt (vanilla English basic prompt) and Chain-of-Thought (CoT) prompting; also Translate-En baseline (translate instance to English then apply basic prompt).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>accuracy: 70.0% (macro average across languages reported for gpt-3.5-turbo under XLT, zero-shot; see Table 7)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>accuracy: 23.3% with Basic Prompt (gpt-3.5-turbo, zero-shot); CoT improved some reasoning scores but still much lower than XLT (CoT reported to help reasoning but not reach XLT levels on MGSM).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+46.7 percentage points accuracy for XLT vs Basic Prompt (gpt-3.5-turbo, zero-shot, MGSM average as reported).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>XLT activates cross-lingual thinking by asking the model to rephrase the input in English (pivot language) and then solve step-by-step; authors hypothesize this lets the LLM leverage stronger English reasoning knowledge and internal CoT-style reasoning, substantially boosting arithmetic reasoning in non-English inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Zero-shot (no exemplars). MGSM test size listed as 250 examples. Greedy decoding (temperature=0). XLT template filled with task meta-information; English-written template even for non-English inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9275.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9275.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>XLT (zero-shot) on MKQA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cross-Lingual-Thought prompting (XLT) — zero-shot on MKQA open-domain QA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>XLT evaluated on MKQA (multilingual open-domain question answering) in zero-shot settings, showing consistent improvements across languages including low-resource languages.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MKQA (multilingual open-domain QA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Open-domain question answering across many languages. Metric: macro-average F1.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot using XLT (same template elements: role, rephrase in English, step-by-step solving, output formatting to one or a few words in target language).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Basic Prompt (vanilla English basic prompt) and few-shot variants; also compared across text-davinci-003.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>macro F1: 26.1 (gpt-3.5-turbo, XLT, zero-shot; see Table 11)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>macro F1: 24.7 (gpt-3.5-turbo, Basic Prompt, zero-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+1.4 F1 points (approx.) for XLT vs Basic Prompt in gpt-3.5-turbo zero-shot on MKQA (macro average reported).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors suggest XLT's cross-lingual rephrasing lets the model leverage English knowledge of open-domain QA and the CoT task-solving encourages structured retrieval and answer formulation; benefits are particularly pronounced for under-resourced languages.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>MKQA test size 6,758 instances; metrics macro-average F1 across target languages; zero-shot setting; greedy decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9275.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9275.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (CoT) prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting (append 'Let's think step-by-step...')</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>CoT prompting asks the model to produce intermediate reasoning steps to solve problems; tested as a baseline against XLT, Basic Prompt, and Translate-En across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MGSM (and other reasoning/understanding tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>CoT evaluated primarily on reasoning benchmarks (MGSM) and also observed on understanding/generation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Append instruction 'Let's think step-by-step and tell me the answer in the end' after the input (CoT). Zero-shot and few-shot variants tested.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to Basic Prompt (no CoT) and XLT (which includes CoT-like instructions as part of a structured template).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Example (text-davinci-003 on MGSM): CoT accuracy: 25.7% vs Basic Prompt 12.5% (numbers reported in Table 1 block for reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>CoT > Basic Prompt for reasoning (e.g., MGSM roughly doubled for text-davinci-003), but CoT is reported to be less effective for understanding and generation tasks compared to XLT.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Example effect: +13.2 percentage points (text-davinci-003 MGSM basic→CoT in zero-shot; Table 1 shows Basic Prompt 12.5 → CoT 25.7).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>CoT elicits intermediate reasoning steps which helps on multi-step arithmetic and reasoning tasks, but CoT alone does not address cross-lingual comprehension and therefore is less helpful on non-reasoning multilingual understanding or generation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>CoT appended to the basic prompt as instruction; tested across the same multilingual benchmarks; greedy decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9275.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9275.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Translate-En baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Translate-to-English then apply English prompt (Translate-En)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Translate multilingual instances into English (via Google Translate in experiments) and apply the basic English prompt; tested as a baseline for leveraging strong English capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multilingual classification/generation benchmarks (applicable to non-generation tasks mainly)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Method translates inputs into English and then runs English prompts; not applied to generation tasks requiring target-language output.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Pre-translate the input into English (external translator) then apply Basic Prompt in English (zero-shot and few-shot where applicable).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared against Basic Prompt, CoT, and XLT.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Translate-En often improves zero-shot performance (example: Table 1 shows Translate-En improving some zero-shot aggregated scores), but exact improvement varies by task.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Authors report Translate-En 'can boost the performance in the zero-shot settings while may not work well in the few-shot settings', and note Translate-En is not applicable to generation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Translating to English lets the LLM leverage stronger English training signal; however translation noise and misalignment with few-shot exemplars can reduce benefits in few-shot setups and cannot be used when the task requires non-English outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Google Translate API used for translation in experiments; Translate-En omitted for generation tasks (e.g., XL-Sum, FLORES).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9275.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9275.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Few-shot demonstration format variants</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Demonstration input-output formatting variants for few-shot learning (Basic+Basic, Basic input + XLT output, XLT+XLT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison of three demonstration construction formats for few-shot learning: (1) Basic prompt inputs with Basic outputs, (2) Basic prompt inputs with XLT-style outputs, and (3) XLT inputs with XLT outputs (authors' chosen method).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MGSM (few-shot demonstration experiments representative)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>How demonstration formatting affects few-shot performance on MGSM; metric: accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>5-shot few-shot. Demonstration variants: (a) Basic input + Basic output, (b) Basic input + XLT-formatted output, (c) XLT input + XLT output (demonstrations created using XLT zero-shot outputs aligned to references).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Direct comparison among the three demonstration styles listed above.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Avg accuracy (MGSM, gpt-3.5-turbo, 5-shot): Basic input + Basic output = 71.9%; Basic input + XLT output = 65.0%; XLT input + XLT output = 72.5% (Table 4 / Table 3 aggregated).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>XLT input + XLT output outperforms Basic input + XLT output by ≈7.5 points and slightly outperforms Basic input + Basic output by ≈0.6 points.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+7.5 percentage points (XLT input+XLT output vs Basic input+XLT output); +0.6 points vs Basic+Basic in MGSM few-shot experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Consistency between zero-shot input-output formatting and few-shot demonstrations matters: demonstrations that match the XLT structured logic (both input and output) better guide the model; mismatched demonstrations (Basic input with XLT outputs) can harm performance.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>5-shot demonstrations (except XL-Sum 3-shot due to input length). Demonstrations were created by sampling dev examples, generating XLT zero-shot outputs, aligning them, and assembling exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9275.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9275.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>XLT instruction ablation & ordering</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ablation and instruction-ordering experiments within XLT (Role Assigning, Cross-lingual Thinking, CoT Task Solving, rephrasing-word variants)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Ablation studies disabling individual instructions and experiments that change instruction order or the rephrasing keyword to evaluate which template components drive XLT's gains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Representative multilingual benchmarks (MGSM, XNLI, FLORES etc. used in ablation tables)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Assess contribution of XLT sub-instructions and robustness to wording/order changes; metrics vary by task (accuracy, BLEU...).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot XLT variants: (a) remove Role Assigning, (b) remove Cross-lingual Thinking, (c) remove CoT Task Solving, (d) change instruction order (swap Role Assigning with Task Inputting/Analysing; swap Cross-lingual Thinking and Task Analyzing), (e) vary rephrasing word ('retell','repeat','translate').</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared against the full XLT template (baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Example: MGSM (gpt-3.5-turbo zero-shot) reported XLT baseline ~79.8 (language-specific slice in Table 2 contexts); ablating Cross-lingual Thinking produced a noticeable drop (examples in Table 2 show drops of several percentage points), and reordering also degraded performance. (Paper reports Cross-lingual Thinking yields the most significant gains.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Removing Cross-lingual Thinking caused the largest performance drop relative to removing Role Assigning or CoT Task Solving; reordering logical instructions decreases performance; word choice for rephrasing affects tasks—'retell' better for certain tasks, 'repeat' better for summarization/translation.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Authors report multi-point decreases when key instructions are removed (example approximations in Table 2 indicate several percentage point drops across tasks; Cross-lingual Thinking produced the largest drop).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Cross-lingual Thinking (explicit rephrasing into English) is the main mechanism that activates English-centric knowledge; Role Assigning helps set behavior and avoiding confusion; CoT Task Solving contributes by enforcing stepwise reasoning; instruction order matters because the template imitates human problem-solving sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Ablations and reorderings reported in Table 2; all ablation experiments performed under zero-shot using gpt-3.5-turbo; rephrasing keyword variants tested (retell/repeat/translate) and reported task-specific sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9275.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9275.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>XLT on other models (compatibility)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>XLT template applied to LLaMA-2-Chat and GPT family models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Tests of XLT on other LLMs to verify generality: text-davinci-003, gpt-3.5-turbo, and LLaMA-2-70b-chat-hf; XLT improves GPT-family models strongly and gives modest gains on LLaMA-2, attributed to LLaMA-2's weaker multi-step instruction following.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2-70b-chat-hf</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MGSM (compatibility exemplar) and other benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Compatibility test: apply XLT to open-source LLaMA-2 chat model to evaluate template generality. Metric: accuracy for MGSM.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot XLT template (same as used for GPT models).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to Basic Prompt on LLaMA-2 (and comparisons to GPT-based model results).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Example: LLaMA-2-70b-chat-hf MGSM average: Basic Prompt ≈34.1% (table reports 34.1 avg for Basic Prompt slice) vs XLT ≈37.3% (Table 7 slice shows improvement but smaller than GPT models).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>XLT provides gains on LLaMA-2 but the magnitude is smaller than with GPT-based models; authors attribute this to LLaMA-2's weaker multi-step instruction-following ability.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+3.2 percentage points (approx.) for XLT vs Basic Prompt on LLaMA-2 (MGSM example from Table 7; gains are modest relative to GPT).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Template is general but model-specific factors matter: models with stronger instruction-following and multi-step reasoning (GPT descendants) benefit more from XLT; LLaMA-2 still benefits but to a lesser degree due to less robust step-by-step instruction following.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Zero-shot; MGSM test size 250; greedy decoding. Authors note limited evaluation budget and that only some model families were tested.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Mega: Multilingual evaluation of generative ai <em>(Rating: 2)</em></li>
                <li>Language models are multilingual chain-of-thought reasoners <em>(Rating: 2)</em></li>
                <li>Chain of Thought Prompting Elicits Reasoning in Large Language Models <em>(Rating: 2)</em></li>
                <li>How good are gpt models at machine translation? a comprehensive evaluation <em>(Rating: 1)</em></li>
                <li>Is chatgpt a good translator? yes with gpt-4 as the engine. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9275",
    "paper_id": "paper-258615377",
    "extraction_schema_id": "extraction-schema-163",
    "extracted_data": [
        {
            "name_short": "XLT (zero-shot) on MGSM",
            "name_full": "Cross-Lingual-Thought prompting (XLT) — zero-shot on MGSM arithmetic reasoning",
            "brief_description": "Evaluation of the XLT prompt template (language-independent, includes role assignment, cross-lingual rephrasing to English, CoT-style step instructions, and strict output formatting) in a zero-shot setup on an arithmetic reasoning benchmark (MGSM).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo",
            "model_size": null,
            "task_name": "MGSM (arithmetic reasoning)",
            "task_description": "Multilingual arithmetic reasoning benchmark (parallel instances across many languages). Metric: accuracy.",
            "presentation_format": "Zero-shot: XLT template (English-written template with Role Assigning, Task Inputting, Cross-lingual Thinking that asks the model to rephrase input in English, Task Analyzing, CoT Task Solving (step-by-step), and Output Formatting). Greedy decoding (temperature=0).",
            "comparison_format": "Basic Prompt (vanilla English basic prompt) and Chain-of-Thought (CoT) prompting; also Translate-En baseline (translate instance to English then apply basic prompt).",
            "performance": "accuracy: 70.0% (macro average across languages reported for gpt-3.5-turbo under XLT, zero-shot; see Table 7)",
            "performance_comparison": "accuracy: 23.3% with Basic Prompt (gpt-3.5-turbo, zero-shot); CoT improved some reasoning scores but still much lower than XLT (CoT reported to help reasoning but not reach XLT levels on MGSM).",
            "format_effect_size": "+46.7 percentage points accuracy for XLT vs Basic Prompt (gpt-3.5-turbo, zero-shot, MGSM average as reported).",
            "explanation_or_hypothesis": "XLT activates cross-lingual thinking by asking the model to rephrase the input in English (pivot language) and then solve step-by-step; authors hypothesize this lets the LLM leverage stronger English reasoning knowledge and internal CoT-style reasoning, substantially boosting arithmetic reasoning in non-English inputs.",
            "null_or_negative_result": false,
            "experimental_details": "Zero-shot (no exemplars). MGSM test size listed as 250 examples. Greedy decoding (temperature=0). XLT template filled with task meta-information; English-written template even for non-English inputs.",
            "uuid": "e9275.0",
            "source_info": {
                "paper_title": "Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "XLT (zero-shot) on MKQA",
            "name_full": "Cross-Lingual-Thought prompting (XLT) — zero-shot on MKQA open-domain QA",
            "brief_description": "XLT evaluated on MKQA (multilingual open-domain question answering) in zero-shot settings, showing consistent improvements across languages including low-resource languages.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo",
            "model_size": null,
            "task_name": "MKQA (multilingual open-domain QA)",
            "task_description": "Open-domain question answering across many languages. Metric: macro-average F1.",
            "presentation_format": "Zero-shot using XLT (same template elements: role, rephrase in English, step-by-step solving, output formatting to one or a few words in target language).",
            "comparison_format": "Basic Prompt (vanilla English basic prompt) and few-shot variants; also compared across text-davinci-003.",
            "performance": "macro F1: 26.1 (gpt-3.5-turbo, XLT, zero-shot; see Table 11)",
            "performance_comparison": "macro F1: 24.7 (gpt-3.5-turbo, Basic Prompt, zero-shot).",
            "format_effect_size": "+1.4 F1 points (approx.) for XLT vs Basic Prompt in gpt-3.5-turbo zero-shot on MKQA (macro average reported).",
            "explanation_or_hypothesis": "Authors suggest XLT's cross-lingual rephrasing lets the model leverage English knowledge of open-domain QA and the CoT task-solving encourages structured retrieval and answer formulation; benefits are particularly pronounced for under-resourced languages.",
            "null_or_negative_result": false,
            "experimental_details": "MKQA test size 6,758 instances; metrics macro-average F1 across target languages; zero-shot setting; greedy decoding.",
            "uuid": "e9275.1",
            "source_info": {
                "paper_title": "Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Chain-of-Thought (CoT) prompting",
            "name_full": "Chain-of-Thought prompting (append 'Let's think step-by-step...')",
            "brief_description": "CoT prompting asks the model to produce intermediate reasoning steps to solve problems; tested as a baseline against XLT, Basic Prompt, and Translate-En across tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "text-davinci-003",
            "model_size": null,
            "task_name": "MGSM (and other reasoning/understanding tasks)",
            "task_description": "CoT evaluated primarily on reasoning benchmarks (MGSM) and also observed on understanding/generation tasks.",
            "presentation_format": "Append instruction 'Let's think step-by-step and tell me the answer in the end' after the input (CoT). Zero-shot and few-shot variants tested.",
            "comparison_format": "Compared to Basic Prompt (no CoT) and XLT (which includes CoT-like instructions as part of a structured template).",
            "performance": "Example (text-davinci-003 on MGSM): CoT accuracy: 25.7% vs Basic Prompt 12.5% (numbers reported in Table 1 block for reasoning).",
            "performance_comparison": "CoT &gt; Basic Prompt for reasoning (e.g., MGSM roughly doubled for text-davinci-003), but CoT is reported to be less effective for understanding and generation tasks compared to XLT.",
            "format_effect_size": "Example effect: +13.2 percentage points (text-davinci-003 MGSM basic→CoT in zero-shot; Table 1 shows Basic Prompt 12.5 → CoT 25.7).",
            "explanation_or_hypothesis": "CoT elicits intermediate reasoning steps which helps on multi-step arithmetic and reasoning tasks, but CoT alone does not address cross-lingual comprehension and therefore is less helpful on non-reasoning multilingual understanding or generation tasks.",
            "null_or_negative_result": null,
            "experimental_details": "CoT appended to the basic prompt as instruction; tested across the same multilingual benchmarks; greedy decoding.",
            "uuid": "e9275.2",
            "source_info": {
                "paper_title": "Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Translate-En baseline",
            "name_full": "Translate-to-English then apply English prompt (Translate-En)",
            "brief_description": "Translate multilingual instances into English (via Google Translate in experiments) and apply the basic English prompt; tested as a baseline for leveraging strong English capabilities.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "text-davinci-003",
            "model_size": null,
            "task_name": "Multilingual classification/generation benchmarks (applicable to non-generation tasks mainly)",
            "task_description": "Method translates inputs into English and then runs English prompts; not applied to generation tasks requiring target-language output.",
            "presentation_format": "Pre-translate the input into English (external translator) then apply Basic Prompt in English (zero-shot and few-shot where applicable).",
            "comparison_format": "Compared against Basic Prompt, CoT, and XLT.",
            "performance": "Translate-En often improves zero-shot performance (example: Table 1 shows Translate-En improving some zero-shot aggregated scores), but exact improvement varies by task.",
            "performance_comparison": "Authors report Translate-En 'can boost the performance in the zero-shot settings while may not work well in the few-shot settings', and note Translate-En is not applicable to generation tasks.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Translating to English lets the LLM leverage stronger English training signal; however translation noise and misalignment with few-shot exemplars can reduce benefits in few-shot setups and cannot be used when the task requires non-English outputs.",
            "null_or_negative_result": null,
            "experimental_details": "Google Translate API used for translation in experiments; Translate-En omitted for generation tasks (e.g., XL-Sum, FLORES).",
            "uuid": "e9275.3",
            "source_info": {
                "paper_title": "Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Few-shot demonstration format variants",
            "name_full": "Demonstration input-output formatting variants for few-shot learning (Basic+Basic, Basic input + XLT output, XLT+XLT)",
            "brief_description": "Comparison of three demonstration construction formats for few-shot learning: (1) Basic prompt inputs with Basic outputs, (2) Basic prompt inputs with XLT-style outputs, and (3) XLT inputs with XLT outputs (authors' chosen method).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo",
            "model_size": null,
            "task_name": "MGSM (few-shot demonstration experiments representative)",
            "task_description": "How demonstration formatting affects few-shot performance on MGSM; metric: accuracy.",
            "presentation_format": "5-shot few-shot. Demonstration variants: (a) Basic input + Basic output, (b) Basic input + XLT-formatted output, (c) XLT input + XLT output (demonstrations created using XLT zero-shot outputs aligned to references).",
            "comparison_format": "Direct comparison among the three demonstration styles listed above.",
            "performance": "Avg accuracy (MGSM, gpt-3.5-turbo, 5-shot): Basic input + Basic output = 71.9%; Basic input + XLT output = 65.0%; XLT input + XLT output = 72.5% (Table 4 / Table 3 aggregated).",
            "performance_comparison": "XLT input + XLT output outperforms Basic input + XLT output by ≈7.5 points and slightly outperforms Basic input + Basic output by ≈0.6 points.",
            "format_effect_size": "+7.5 percentage points (XLT input+XLT output vs Basic input+XLT output); +0.6 points vs Basic+Basic in MGSM few-shot experiments.",
            "explanation_or_hypothesis": "Consistency between zero-shot input-output formatting and few-shot demonstrations matters: demonstrations that match the XLT structured logic (both input and output) better guide the model; mismatched demonstrations (Basic input with XLT outputs) can harm performance.",
            "null_or_negative_result": false,
            "experimental_details": "5-shot demonstrations (except XL-Sum 3-shot due to input length). Demonstrations were created by sampling dev examples, generating XLT zero-shot outputs, aligning them, and assembling exemplars.",
            "uuid": "e9275.4",
            "source_info": {
                "paper_title": "Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "XLT instruction ablation & ordering",
            "name_full": "Ablation and instruction-ordering experiments within XLT (Role Assigning, Cross-lingual Thinking, CoT Task Solving, rephrasing-word variants)",
            "brief_description": "Ablation studies disabling individual instructions and experiments that change instruction order or the rephrasing keyword to evaluate which template components drive XLT's gains.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo",
            "model_size": null,
            "task_name": "Representative multilingual benchmarks (MGSM, XNLI, FLORES etc. used in ablation tables)",
            "task_description": "Assess contribution of XLT sub-instructions and robustness to wording/order changes; metrics vary by task (accuracy, BLEU...).",
            "presentation_format": "Zero-shot XLT variants: (a) remove Role Assigning, (b) remove Cross-lingual Thinking, (c) remove CoT Task Solving, (d) change instruction order (swap Role Assigning with Task Inputting/Analysing; swap Cross-lingual Thinking and Task Analyzing), (e) vary rephrasing word ('retell','repeat','translate').",
            "comparison_format": "Compared against the full XLT template (baseline).",
            "performance": "Example: MGSM (gpt-3.5-turbo zero-shot) reported XLT baseline ~79.8 (language-specific slice in Table 2 contexts); ablating Cross-lingual Thinking produced a noticeable drop (examples in Table 2 show drops of several percentage points), and reordering also degraded performance. (Paper reports Cross-lingual Thinking yields the most significant gains.)",
            "performance_comparison": "Removing Cross-lingual Thinking caused the largest performance drop relative to removing Role Assigning or CoT Task Solving; reordering logical instructions decreases performance; word choice for rephrasing affects tasks—'retell' better for certain tasks, 'repeat' better for summarization/translation.",
            "format_effect_size": "Authors report multi-point decreases when key instructions are removed (example approximations in Table 2 indicate several percentage point drops across tasks; Cross-lingual Thinking produced the largest drop).",
            "explanation_or_hypothesis": "Cross-lingual Thinking (explicit rephrasing into English) is the main mechanism that activates English-centric knowledge; Role Assigning helps set behavior and avoiding confusion; CoT Task Solving contributes by enforcing stepwise reasoning; instruction order matters because the template imitates human problem-solving sequence.",
            "null_or_negative_result": false,
            "experimental_details": "Ablations and reorderings reported in Table 2; all ablation experiments performed under zero-shot using gpt-3.5-turbo; rephrasing keyword variants tested (retell/repeat/translate) and reported task-specific sensitivity.",
            "uuid": "e9275.5",
            "source_info": {
                "paper_title": "Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "XLT on other models (compatibility)",
            "name_full": "XLT template applied to LLaMA-2-Chat and GPT family models",
            "brief_description": "Tests of XLT on other LLMs to verify generality: text-davinci-003, gpt-3.5-turbo, and LLaMA-2-70b-chat-hf; XLT improves GPT-family models strongly and gives modest gains on LLaMA-2, attributed to LLaMA-2's weaker multi-step instruction following.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-2-70b-chat-hf",
            "model_size": "70B",
            "task_name": "MGSM (compatibility exemplar) and other benchmarks",
            "task_description": "Compatibility test: apply XLT to open-source LLaMA-2 chat model to evaluate template generality. Metric: accuracy for MGSM.",
            "presentation_format": "Zero-shot XLT template (same as used for GPT models).",
            "comparison_format": "Compared to Basic Prompt on LLaMA-2 (and comparisons to GPT-based model results).",
            "performance": "Example: LLaMA-2-70b-chat-hf MGSM average: Basic Prompt ≈34.1% (table reports 34.1 avg for Basic Prompt slice) vs XLT ≈37.3% (Table 7 slice shows improvement but smaller than GPT models).",
            "performance_comparison": "XLT provides gains on LLaMA-2 but the magnitude is smaller than with GPT-based models; authors attribute this to LLaMA-2's weaker multi-step instruction-following ability.",
            "format_effect_size": "+3.2 percentage points (approx.) for XLT vs Basic Prompt on LLaMA-2 (MGSM example from Table 7; gains are modest relative to GPT).",
            "explanation_or_hypothesis": "Template is general but model-specific factors matter: models with stronger instruction-following and multi-step reasoning (GPT descendants) benefit more from XLT; LLaMA-2 still benefits but to a lesser degree due to less robust step-by-step instruction following.",
            "null_or_negative_result": false,
            "experimental_details": "Zero-shot; MGSM test size 250; greedy decoding. Authors note limited evaluation budget and that only some model families were tested.",
            "uuid": "e9275.6",
            "source_info": {
                "paper_title": "Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Mega: Multilingual evaluation of generative ai",
            "rating": 2,
            "sanitized_title": "mega_multilingual_evaluation_of_generative_ai"
        },
        {
            "paper_title": "Language models are multilingual chain-of-thought reasoners",
            "rating": 2,
            "sanitized_title": "language_models_are_multilingual_chainofthought_reasoners"
        },
        {
            "paper_title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
            "rating": 2,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "How good are gpt models at machine translation? a comprehensive evaluation",
            "rating": 1,
            "sanitized_title": "how_good_are_gpt_models_at_machine_translation_a_comprehensive_evaluation"
        },
        {
            "paper_title": "Is chatgpt a good translator? yes with gpt-4 as the engine.",
            "rating": 1,
            "sanitized_title": "is_chatgpt_a_good_translator_yes_with_gpt4_as_the_engine"
        }
    ],
    "cost": 0.019334499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting</p>
<p>Haoyang Huang 
Gaoling School of Artificial Intelligence
Renmin University of China</p>
<p>Tianyi Tang 
Gaoling School of Artificial Intelligence
Renmin University of China</p>
<p>Dongdong Zhang 
Gaoling School of Artificial Intelligence
Renmin University of China</p>
<p>Wayne Xin Zhao 
Gaoling School of Artificial Intelligence
Renmin University of China</p>
<p>Ting Song 
Gaoling School of Artificial Intelligence
Renmin University of China</p>
<p>Yan Xia 
Gaoling School of Artificial Intelligence
Renmin University of China</p>
<p>Furu Wei 
Gaoling School of Artificial Intelligence
Renmin University of China</p>
<p>Microsoft Research 
Gaoling School of Artificial Intelligence
Renmin University of China</p>
<p>Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting
4E3992C27992A1B227F572BAABC630BB
Figure 1: Comparing the effectiveness of the Cross-Lingual-Thought prompt versus the baseline basic prompt on 7 representative benchmarks covering 27 languages: (a) Enhancing the multilingual capability of text-davinci-003 under the zero-shot learning, and (b) Narrowing the gap between the average performance and the best performance of each task in different languages.</p>
<p>Introduction</p>
<p>Large language models (LLMs) demonstrate impressive multilingual capability in a wide range of natural language processing tasks, including language generation, knowledge utilization, and complex reasoning (Zhao et al., 2023).Their performance in downstream tasks has been shown to reach or even surpass human-level performance (Brown et al., 2020;Chowdhery et al., 2022;Scao et al., 2022).The capabilities of LLMs stem from the extensive volume of training data they leveraged (Kaplan et al., 2020).The training data for current models is primarily dominated by the English language corpus, but it also encompasses data from other languages, as described in GPT-3 (Brown et al., 2020), PaLM (Chowdhery et al., 2022), andBLOOM (Scao et al., 2022), etc.</p>
<p>There are over 7,000 languages worldwide, with the vast majority being low-resource or extremely low-resource languages (Forkel et al., 2022).Despite the latest GPT-4 model (OpenAI, 2023) demonstrating some generalization capabilities in Request: James decides to run 3 sets of 60-meter sprints three times a week.How many meters does he run in total each week?Output 1. James runs 3 sets of 60-meter sprints, which means he runs 60 x 3 = 180 meters per sprint.2. James runs 3 sets of 180 meters per week, which means he runs 180 x 3 = 540 meters per week.Answer: James runs a total of meters per week.540</p>
<p>Step-by-step answer:  : Overview of our method.Given a request, its associated meta information is filled into the placeholders of the XLT template to form the language-independent prompt, which is fed to the LLM to enhance the generation of responses in the desired format.</p>
<p>multilingual tasks as evaluated on the MMLU benchmark (Hendrycks et al., 2021), it is still the case that LLMs do not have equal capability to handle all languages, leading to imbalanced capability across different languages.Furthermore, several evaluation results (Bang et al., 2023;Jiao et al., 2023;Hendy et al., 2023;Zhu et al., 2023) indicate that large models struggle with understanding and generating non-English languages, particularly in low-resource or extremely low-resource languages.Therefore, to democratize language intelligence and minimize performance gaps in different language, it is essential and meaningful to stimulate and enhance the multilingual capability of models in non-English and low-resource languages.</p>
<p>Intuitively, LLMs can improve multilingual capability by augmenting data (Lin et al., 2022) or fine-tuning models (Chen et al., 2021(Chen et al., , 2022)), but both are computationally expensive.Alternatively, in-context learning with prompts can also boost performance (Brown et al., 2020;Ahuja et al., 2023;Wei et al., 2022c) but is limited to monolingual tasks (Sanh et al., 2022).This work explores a universal in-context learning approach to enhance the multilingual capability of LLMs.We introduce a simple yet effective method, called cross-lingual-thought prompting (XLT), to enable models to handle various natu-ral language processing tasks across different target languages.Our method employs a generic and language-independent prompt, which eliminates the need to update model parameters.Depending on the task input type, cross-lingual-thought prompting guides the large language model to assume the role of an expert in a specific language for a particular task.Given its predefined meta information, XLT directs LLMs to respond logically through a process involving problem understanding, cross-lingual thinking, task analysis, task execution, and output formatting.During this process, our method is designed to stimulate models' cross-lingual and logical reasoning skills, enabling them to respond to input requests regardless of the language.For enhanced performance, few-shot learning can also be employed with our method by providing an LLM-generated response output as a demonstration using cross-lingual-thought prompting zero-shot learning.</p>
<p>We conduct a comprehensive evaluation to verify the effectiveness of XLT across seven representative multilingual benchmarks of natural language reasoning, understanding, and generation tasks.Each benchmark includes multilingual data covering both high-resource and low-resource languages.The experimental results demonstrate that our method can significantly improve the perfor- mance of all benchmarks across languages under both zero-shot and few-shot learning settings.Notably, XLT achieves an average gain of over 10 points on the MGSM and MKQA benchmarks.Furthermore, we observe that our prompting method significantly reduces the gap between the average performance and the best performance of each task in different languages, indicating its potential to democratize language intelligence.</p>
<p>Cross-Lingual-Thought Prompting</p>
<p>Although LLMs are capable of accepting any input and generating responses, users typically structure their requests in the form of prompts to elicit the desired output.The design of these prompts is crucial for achieving optimal performance on downstream tasks, as LLMs are sensitive to the format of the prompts chosen (Zhao et al., 2021).Through a process called instruction tuning (Wei et al., 2022a), models can develop the ability to follow natural language instructions (Wei et al., 2022b), which can reduce their sensitivity to prompt engineering (Wei et al., 2022a).In accordance with the guidelines of the OpenAI cookbook1 , we propose a cross-lingual thought prompting template, denoted as the XLT template.This generic template allows LLMs to respond to requests with cross-lingual thought and supports a wide range of multilingual tasks.</p>
<p>Figure 3 displays the XLT template, with the colored sections representing placeholders.Figure 2 showcases an example of instantiated prompt for the Chinese request.The following section will explain the details of constructing XLT.</p>
<p>Construction of XLT</p>
<p>The XLT template is designed to emulate the process humans employ when handling multilingual tasks.Our template is written in English, as English is the dominant language during LLM pre-training, and existing research indicates that English prompting is more effective for multilingual tasks (Shi et al., 2023).In contrast to the vanilla prompt that only includes a task description, our XLT template aims to elicit multilingual capability through cross-lingual thoughts.This template comprises six logical instructions in sequence.To complete the template, only seven placeholders need to be filled in based on intrinsic knowledge of the task and the request, as depicted in igure 3.</p>
<p>Role Assigning .First, the model receives a role definition that helps establish the model's behavior.This concept is akin to the system role of Chat-GPT2 .To achieve this, we simply need to fulfill the task name with a known category (such as commonsense reasoning or paraphrase identification), along with the language of the task in the task language field.Task Inputting .Second, we explicitly append the request as the task input .The request is basically structured in terms of the task type so as to make sure the model can comprehend it.For example, in the natural language inference task, the two sentence inputs are specified with "premise" and "hypothesis", respectively.</p>
<p>Cross-lingual Thinking .We encourage the model to engage in cross-lingual thought by rephrasing the requested content in English, which is the dominant language used as a pivot language by Shi et al. (2023) andAhuja et al. (2023).Rephrasing the requested content enclosed in the input tag helps the model better understand the request in its native language and knowledge.Our observations suggest that using keywords such as "retell" or "repeat" while rephrasing the content may result in better performance in practice.</p>
<p>Task Analyzing .After rephrasing the task input, we need to complete the task in task goal .This step is comparable to the task description used in conventional prompting methods.In practice, we can get the task information from the literature or seek assistance from ChatGPT to generate effective prompts for solving the task (Jiao et al., 2023).</p>
<p>CoT Task Solving .We then ask the model to follow the instructions and complete the task step by step.Since LLMs exhibit a strong ability to maintain a chain-of-thought (Wei et al., 2022c), we carefully design instructions to guide the model, with the hope that it will respond to our instructions in a step-by-step manner and utilize the intermediate outputs to aid in solving the task.</p>
<p>Output Formatting .Finally, we should regularize the output format of the model to obtain the exact answer.LLMs are utilized in a zero-or few-shot manner, and they tend to generate texts that may not conform to the format of the target answer.Fortunately, LLMs possess a strong ability to follow instructions, and we can define the output format in terms of output type and output constraint .The output type can be a number, index, or text, while the output constraint is optional and determined based on the task requirements.Output constraint may include length limitations, language specifications, and other relevant factors.</p>
<p>XLT for Few-shot Learning</p>
<p>The above construction of XLT can be directly fed to LLMs to yield outputs, which is performed in the zero-shot learning setting.In addition, we also explore incorporating demonstrations into XLT to enable few-shot learning.Different from previous work that just appends model outputs to the corresponding request (Shi et al., 2023) or utilizes a verbalizer to format the output, our method constructs the demonstrations with better formatted model outputs from a step-by-step processing-based XLT.As illustrated in Figure 4, we first sample a few examples from the development set and incorporate the requested parts into XLT.The zero-shot learning is performed over LLM to collect responses that are further aligned with those of the samples.Only response-aligned requests are assembled with the corresponding model responses to form final demonstrations for few-shot learning.In this way, the demonstrations are constructed with rich logical knowledge via XLT, which will cater to the XLT-based generation of new requests.In practice, we can also correct or design the demonstrations for better alignment with the instruction logic.</p>
<p>Experiments</p>
<p>To comprehensively verify the effectiveness of our method on language-independent generality, we evaluate our XLT template on different LLMs covering various natural language processing tasks in multiple languages.</p>
<p>Experimental Setups</p>
<p>Tasks and Benchmarks</p>
<p>We conduct evaluations on seven typical benchmarks related to reasoning, understanding, and generation tasks that can represent different capabilities of LLMs, encompassing both high-resource and low-resource languages.These benchmarks cover 27 different languages, including English (en), German (de), Russian (ru), French (fr), Chinese Simplified (zh), Spanish (es), Japanese (ja), Italian (it), Vietnamese (vi), Turkish (tr), Indonesian (id), Swahili (sw), Arabic (ar), Korean (ko), Greek (el), Thai (th), Bulgarian (bg), Hindi (hi), Estonian (et), Bengali (bn), Tamil (ta), Galician (gl), Urdu (ur), Telugu (te), Javanese (jv), Haitian Creole (ht), and Southern Quechua (qu).In terms of the language distribution statistics in the Common Crawl Monthly Archives3 and the language performance of LLMs (Shi et al., 2023;Ahuja et al., 2023), we have arranged them in the order of language frequency from high-resource to low-resource.In particular, the frequency of some underrepresented languages is even less than 0.1% (e.g., bn, ta, gl, ur, te, jv, ht, qu).</p>
<p>• Reasoning tasks -Arithmetic Reasoning.The MGSM (Shi et al., 2023) (Papineni et al., 2002;Post, 2018) for evaluation.</p>
<p>Among these benchmarks, MGSM, XCOPA, XNLI, PAWS-X, and MKQA are parallel, i.e., the instances are semantics-equivalent across each language.For all benchmarks, we report the results on the test sets using all instances (Table 5), except for XL-Sum and FLORES-200, where we only sample 250 and 200 examples respectively to show the trend of generation performance.In the fewshot setting, we randomly choose examples from the development set if they have, otherwise, we translate the English training set into corresponding languages to construct several examples.</p>
<p>Baselines</p>
<p>Basic Prompt are the vanilla in our experiments that were proposed and suggested in previous work.After determining the prompt, we format each monolingual instance using the English basic prompt.This setting is similar to the monolingual prompting in MEGA (Ahuja et al., 2023).The basic prompts used for the evaluation of each benchmark are listed in Table 5.Note that, we dismiss the baseline using native-language, since MEGA (Ahuja et al., 2023) reveals monolingual prompting is superior to cross-lingual prompting.</p>
<p>Chain-of-Thought (CoT) prompting invokes LLMs to generate a series of intermediate results to solve reasoning tasks (Wei et al., 2022c), which is still effective under multilingual scenarios (Shi et al., 2023).In experiments, we append the instruction "Let's think step-by-step and tell me the answer in the end" after the input to prompt LLMs.</p>
<p>Translate-English leverages the robust capabilities of LLMs in English to tackle multilingual tasks, as suggested by both Shi et al. (2023) andAhuja et al. (2023).This approach translates instances from other languages into English beforehand.In practice, we utilize the Google Translate API to translate examples into English and apply the basic prompt to format them.Note that, we do not apply this method to generation tasks since they require the output in respective language rather English.</p>
<p>XLT utilizes the proposed template consisting of multiple instructions introduced in Section 2. The instantiated XLT templates for each benchmark are listed in Table 6.</p>
<p>In few-shot learning scenarios, for basic prompt, we use the same template as an additional input to the model.For XLT, we provide the exemplars with XLT template inputs and anticipate desirable step-by-step outputs as outlined in Figure 4.In the subsequent evaluation, we apply the 5-shot setting, except for the XL-Sum* experiments, which use the 3-shot setting due to input length constraints.</p>
<p>LLMs</p>
<p>We mainly evaluate two LLMs from the GPT-3.5 series models:</p>
<p>• text-davinci-0034 is trained using instruction tuning and reinforcement learning from human feedback (Ouyang et al., 2022).It can perform a wide range of natural language tasks with satisfactory results.</p>
<p>• gpt-3.5-turbo 4 is optimized for chat based on text-davinci-003 and suitable for traditional NLP tasks.It is the most capable GPT-3.5 model.</p>
<p>To verify the compatibility of our XLT template, we further incorporate LLaMA-2-Chat (Touvron et al., 2023) (Llama-2-70b-chat-hf) as our base models.It is an open-source model that has been trained through supervised fine-tuning and reinforcement learning from human feedback on the base LLaMA 2 model.In addition, we also refer to the existing results from other LLMs, such as code-davinci-002 4 , when the evaluation is comparable.During inference, we employ greedy search (i.e., temperature=0) to generate the LLM responses.We find LLMs have excellent instructionfollowing abilities to respond to our instructions in the given format.Therefore, we just extract the part after "Answer format:" as labels.</p>
<p>Experimental Results</p>
<p>Multilingual Capability.We comprehensively evaluate XLT's performance over seven tasks.The average score of text-davinci-003 is summarized in Figure 1(a) and Table 1, and more details are listed in Appendix A. As for the CoT prompting, it can enhance reasoning tasks while becomes less effective on understanding and generation tasks.In terms of the Translate-En prompting, it can boost the performance in the zero-shot settings while may not work well in the few-shot settings.Overall, compared to the three baseline methods, XLT achieves significant improvements over two LLMs for all tasks on both zero-shot and few-shot settings regardless of the language difference, except for a slight drop on the PAWS-X benchmark in the zero-shot setting.It is noted that XLT achieves remarkable gains of nearly 20 points on average in the MGSM benchmark for the arithmetic reasoning task and around 10 points on average in the MKQA benchmark for the open-domain question answering task.The experiments demonstrates the effectiveness of XLT for empowering LLM with multilingual capability.</p>
<p>As for the compatibility test, we list the results of LLaMA-2-Chat on the MGSM benchmark in Table 7.It is notable that LLaMA 2 can also benefit from our cross-lingual-thought, which further demonstrates the generality of our XLT template.However, the gains of LLaMA-2-Chat is not as good as GPT-based models.Our analysis reveals this gap can primarily be attributed to LLaMA 2's poorer multi-step instruction-following ability.</p>
<p>Language Democratization.Furthermore, we try to assess the democratization degree of tasks between languages by defining a "democratization score", which calculates the average percentage of performance attained by different languages relative to the best performance among all languages.Given the evaluation scores of s 1 , s 2 , . . ., s l corresponding to l language on a task, the democratization score is formulated as:
l i=1 s i l / max{s i } l i=1 .(1)
Table 2 presents the degree of democratization for tasks across languages under both zero-shot learning and few-shot learning, and we further summarize it in Figure 1(b) by averaging all scores per task regardless of the setting and model differences.We can observe that XLT leads to higher democratization scores in general, particularly for XCOPA, and MKQA.As for MGSM, XNLI, and PAWS-X, our XLT can improve performance in multiple languages, where the overall performance of the baseline is consistently lower but the gap between languages is smaller as shown in Tables 7, 9, and 10.In conclusion, our method can reduce the performance gap between languages and improve the language democratization of LLMs.</p>
<p>Further Analysis</p>
<p>In this section, we further investigate the factors that affect the performance of XLT and how they affect various multilingual benchmarks.</p>
<p>Ablation of XLT</p>
<p>For the XLT variants, we mainly conduct experiments to compare the following strategies:</p>
<p>• Ablating the instructions.Since our XLT consists of six logical instructions, we disable the Role Assigning, Cross-lingual Thinking, and CoT Task Solving instructions separately to analyze the contribution per instruction.</p>
<p>• Reordering the instructions.Considering the logicality of our instructions, we further change the order of the instructions in XLT to explore whether LLMs will handle tasks differently and lead to different results.</p>
<p>• Changing the content word.As prompts are usually sensitive to the word choice, we verify the robustness of XLT when alternating the rephrasing keyword with "retell", "repeat", and "translate" in the cross-lingual thinking instruction.</p>
<p>The outcomes are presented in Table 3, indicating that XLT surpasses almost all the variants, thereby validating the effectiveness and reasonableness of our proposed XLT method.</p>
<p>The effectiveness of each instruction.The results from the "Instruction Ablation" row indicate that: (1) Cross-lingual Thinking yields more significant gains compared to other instructions.This suggests that the LLM's ability of cross-lingual thinking is activated, allowing it to utilize its knowledge in English to solve tasks effectively; (2)   understanding of the ultimate goal for diverse multilingual tasks, highlighting the task transferability of XLT; and (3) the better performance of XLT can also be attributed to CoT Task Solving, which requires the model to respond to complex instructions in a step-by-step manner.</p>
<p>The order of logical instructions.The performance drop is evident when the order of our designed logical instructions is switched.When designing XLT, we have taken into account the process by which humans solve multilingual problems, and this experiment further confirms the optimum order of our XLT template.Placing the Role Assigning instruction later may confuse the model initially.Additionally, conducting Cross-lingual Thinking before Task Analyzing is crucial since we rely on the English task-solving abilities of LLMs to handle multilingual tasks.</p>
<p>The robustness of word choice for rephrasing keywords.We can find that different words indeed affect the performance of XLT, but it is less sensitive to the other variants.Through experimentation, we have determined that "repeat" yields better results for text summarization and machine translation, while "retell" is more suitable for the remaining five tasks.Our aim is to provide XLT with a more unified template, while still allowing users to fine-tune specific keywords for optimal performance in their tasks.</p>
<p>Effectiveness of XLT Few-shot Learning</p>
<p>As mentioned in Section 2.2, the construction of demonstrations for XLT few-shot learning differs from the previous method.We have compared XLT and basic prompt.Here, we focus on the construction of the demonstration input-output pairs and compare various demonstrations that may be used to perform XLT few-shot learning.The illustrations can be found in Figure 5.</p>
<p>• Basic prompt input + Basic prompt output: This is the normal demonstration format used in most of the previous work.</p>
<p>• Basic prompt input + XLT output: This ablation is to separate the effect of input and output formats in the demonstration.</p>
<p>• XLT input + XLT output: This is the method that we used in this work.</p>
<p>Observing the experimental results presented in Table 4, we can conclude that: (1) Our XLT fewshot learning outperforms all other variants, thus confirming its effectiveness.(2) The use of normal demonstrations for XLT few-shot learning leads to a decrease in performance.(3) Merely incorporating XLT as a demonstration input without its output does not result in any improvements.( 4) Consistency in the demonstration for few-shot learning is crucial, implying that the demonstration inputoutput format should align better with its zero-shot learning input-output format.</p>
<p>4 Related Work</p>
<p>LLM Capability Understanding</p>
<p>Despite the impressive capabilities of LLMs, it is crucial to determine their impact on natural language processing tasks.Liang et al. (2022) conduct a comprehensive evaluation of LLMs from various perspectives, such as accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency.Bang et al. (2023) extensively evaluate the Chat-GPT model on multiple natural language processing tasks and find that the model performs well in high-resource languages but exhibits certain limitations in low-resource and non-Latin script languages.Additionally, studies by Jiao et al. (2023) and Hendy et al. (2023) compare different GPT models with supervised models for machine translation tasks and find that GPT models have competitive translation abilities in high-resource languages but perform less effectively in low-resource languages.It is worth noting that achieving multilingual generative AI capability necessitates crosslingual knowledge to further improve the model's performance.In this context, Ahuja et al. ( 2023) evaluate the multilingual task understanding ability of GPT models and attempt to enhance their task processing abilities in other languages using English knowledge.Our work also focuses on evaluating the multilingual capabilities of LLMs, including reasoning, understanding, and generative capabilities.Our evaluations indicate that LLMs exhibit differences in high-resource and low-resource abilities, which necessitates additional efforts to enhance their multilingual capability.</p>
<p>Multilingual Task Processing</p>
<p>Multilingual knowledge has been shown to be exploitable and transferable between languages to improve model performance (Devlin et al., 2019;Conneau et al., 2020;Raffel et al., 2020;Ouyang et al., 2021;Chi et al., 2021).While much research has been devoted to multilingual understanding tasks, multilingual generation tasks are more challenging, particularly when the target language is lowresource or non-English (Ma et al., 2021;Liu et al., 2020).Two methods can enable models to support multilingual task processing: one is training a supervised model that covers multiple languages for multilingual processing (Costa-jussà et al., 2022), and the other is training a pre-trained model and using fine-tuning to transfer knowledge among languages to achieve multilingual capability (Chen et al., 2021(Chen et al., , 2022)).However, the emergence of LLMs has made it possible to directly process multilingual tasks via in-context learning (Brown et al., 2020;Ahuja et al., 2023).These LLMs, with hundreds of billions or even trillions of parameters, require a significant amount of computation resources for training, making traditional fine-tuning methods less feasible.To improve the generative ability of LLMs, researchers explore in-context learning methods that do not require updating model parameters, such as few-shot prompting (Vilar et al., 2022), automatic prompt learning (Shin et al., 2020), task-instruction prompting (Ye et al., 2023), chain-of-thought prompting (Wei et al., 2022c), etc.Our work builds upon these methods and proposes an optimized, generic, and language-independent prompt to enhance the multilingual capability of LLMs.</p>
<p>Conclusion</p>
<p>This work investigates the language processing capabilities of large language models in multilingual settings and expects to develop a universal framework for handling diverse multilingual tasks.To accomplish this goal, we propose a generic prompt, referred to as XLT, to enhance the multilingual capability and reduce the performance gaps among languages in tasks related to language understanding, reasoning, and generation in non-English and low-resource languages.Although our method is generally applicable across tasks and languages, we discovered that prompting design factors such as instruction logic and word choice have explicit impacts on its effectiveness.Cross-language thinking in XLT is particularly effective.Finally, we hope this work can inspire further research to prioritize the development of generic prompting.By doing so, large language models can encompass a wider range of modalities and languages.</p>
<p>Limitations</p>
<p>Due to limitations imposed by the evaluation benchmarks and OpenAI API cost, we conducted tests on 27 languages, which merely scratch the surface of the vast array of languages in the world.Besides, our XLT template is based on English.It deserves to explore whether the template written in task language can lead to better performance and how to better construct the instruction in each language.Furthermore, we only verify the effectiveness of our method on two GPT-based models (i.e., text-davinci-003 and gpt-3.5-turbo)and LLaMA-2-Chat.It is worthwhile to investigate the generality of our template on more models, such as BLOOM and PaLM.</p>
<p>A Additional Experiments</p>
<p>A.1 Results on Reasoning Tasks</p>
<p>Table 7 presents the results of the MGSM benchmark.XLT significantly improves the arithmetic reasoning capabilities of both models, particularly for gpt-3.5-turbo in the zero-shot setting.We hypothesize that gpt-3.5-turbomay have undergone supervised fine-tuning (Ouyang et al., 2022) with arithmetic reasoning samples in the chain-ofthought format, which enables XLT to activate its arithmetic reasoning ability directly.For both lowresource languages (e.g., sw, th, bn, and te) and high-resource languages, XLT can further enhance the performance.Even under the few-shot setting, XLT can still significantly improve the reasoning performance of both models and reduce the performance gap for all languages.Notably, for some high-resource languages, such as de, ru, fr, and es, the performance is comparable to English.</p>
<p>The XCOPA benchmark results are presented in Table 8.Our XLT approach significantly enhances the performance of both models in both settings, as compared to basic prompting.In the zero-shot setting, XLT demonstrates significant improvements for relatively low-resource languages (e.g., sw, th, et, ta, and ht), but it underperforms the baseline for some high-resource languages such as zh and it.In the few-shot setting, XLT brings enhancements for both high-and low-resource languages.Our findings suggest that XLT is more effective for low-resource languages, particularly for gpt-3.5-turbo on sw, th, ta, and ht, where it yields improvements of over 10 accuracy points.</p>
<p>A.2 Results on Understanding Tasks</p>
<p>Table 9 presents the results of the XNLI benchmark.In the zero-shot setting, our XLT significantly outperforms the basic prompt in all languages.Additionally, when using few-shot setups on high-and low-resource languages, both text-davinci-003 and gpt-3.5-turboshow significant improvements compared to the basic prompt.Specifically, for low-resource languages such as th, bg, hi, and ur, XLT achieves an average improvement of 9.4 accuracy scores for text-davinci-003 and 5.3 accuracy scores for gpt-3.5-turbo.This demonstrates that XLT is effective for both models, but text-davinci-003 has better natural language inference capabilities.</p>
<p>Table 10 displays the comparisons on the PAWS-X task, where XLT outperforms basic prompt in all languages, particularly for low-resource languages under the few-shot setting.We observe a slight performance drop on average in zero-shot learning compared to gpt-3.5-turbo for some highresource languages (e.g., en, de, and fr).Based on our analysis of intermediate outputs, we infer that the drop in performance may be due to crosslingual thinking that alters the original meaning of the two sentences, leading to difficulties in judgment.Additionally, a comparable pattern is evident in a previous study (Ahuja et al., 2023), where non-Latin script languages (ja, zh, and ko) exhibit significantly poorer performance than English or German in the few-shot setting.Nevertheless, by demonstrating the construction of XLT, we can guide the model on how to think across different languages and effectively address the aforementioned issues.</p>
<p>A.3 Results on Generation Tasks</p>
<p>The MKQA benchmark outcomes are listed in Table 11.Across all languages in the zero-shot and few-shot settings, the XLT template shows a significant improvement over the basic prompt.It is worth noting that text-davinci-003 performs worse than gpt-3.5-turbo in this task, and we speculate that the latter is optimized for open question answering, which is common in daily chat.Additionally, our findings indicate that XLT can notably enhance the performance of under-resourced languages.XLT brings over 10 points of improvement for these languages.(e.g., zh, ja, vi, and tr) This aligns with previous benchmarking studies and is particularly noteworthy in this evaluation.We suspect that high-resource and low-resource languages share the same cross-lingual thinking as English to greatly leverage the LLM's ability to solve English open-domain QA.</p>
<p>The results of the XL-Sum<em> benchmark are presented in Table 12.It can be observed that XLT outperforms the basic prompt in both zero-and fewshot settings across all languages.Additionally, the LLM model exhibits a significant improvement in generating summaries under the few-shot setting compared to the zero-shot setting.This suggests that providing fewer examples can effectively guide the model in summarizing multilingual texts.Furthermore, the few-shot results revealed an interesting finding that text-davinci-003 performed better when gpt-3.5-turbo and text-davinci-003 use basic prompt.However, once XLT is enabled,  gpt-3.5-turbooutperforms text-davinci-003, highlighting the effectiveness of our approach.Machine translation is a special generation task where the source and target are two different languages.The experiment in this part is to verify how XLT boosts machine translation tasks.Since English has been specified as the pivot language in the cross-lingual thinking in XLT, we exclude Englishcentric tasks to avoid language redundancy and focus on 12 non-English translation directions in the FLORES</em> benchmark, which includes both highresource and low-resource languages.As shown in Table 13, XLT achieves impressive zero-shot results for all languages compared with basic prompt.For example, it significantly improves translation quality in Chinese-to-X or X-to-Chinese.The result emphasizes that XLT will potentially transfer the knowledge of a high-resource pivot language like English to the target language.While the benefit of XLT may not be as obvious for high-to-high translations, it becomes more significant for highto-low, low-to-high, and low-to-low translations.For instance, XLT improves the translation perfor-mance of gpt-3.5-turboby nearly 4.0, 2.8, and 3.3 BLEU points for th→gl, jv→zh, and zh→th translations, respectively, demonstrating its effectiveness regardless of whether the source language is high-resource or low-resource.Noticing that Hendy et al. (2023) have shown that few-shot configurations do not yield significant improvements over the zero-shot setup for translation tasks, we do not evaluate the few-shot paradigm on FLORES* in this work and leave it for future exploration.text-davinci-003 output:</p>
<p>The request is: To make a robe, you need 2 pieces of blue fiber fabric and half of that amount of white fiber fabric.How many pieces of fabric are needed in total?</p>
<p>Step-by-step answer: 1. Start with the amount of blue fiber fabric: 2 pieces 2. Half of that amount is 1 piece 3. Add the 1 piece of white fiber fabric to the 2 pieces of blue fiber fabric 4. Answer: 3 pieces of fabric are needed in total.</p>
<p>gpt-3.5-turbo output:</p>
<p>The request is asking how many pieces of fabric are needed to make a robe, given that it requires 2 pieces of blue fiber fabric and half that amount of white fiber fabric.</p>
<p>To solve this problem, we need to first find out how many pieces of white fiber fabric are needed.We can do this by dividing the number of blue fiber fabric pieces by 2:2 ÷ 2 = 1 So we need 1 piece of white fiber fabric.</p>
<p>To find the total number of fabric pieces needed, we can add the number of blue and white pieces together:2 + 1 = 3 Therefore, Answer: 3 pieces of fabric are needed to make the robe.</p>
<p>Answer: Input:
I</p>
<p>Input:</p>
<p>Here is a premise: Ese oli mullikilesse mässitud.What is the cause?Help me pick the more plausible option: -choice1: See oli õrn, -choice2: See oli väike.</p>
<p>text-davinci-003 output:</p>
<p>Choice 2 : See oli väike.</p>
<p>gpt-3.5-turbo output:</p>
<p>As an AI language model, I cannot determine the cause of the premise without additional context.The options provided do not seem to be directly related to the premise.Can you please provide more information or context?</p>
<p>Answer:</p>
<p>1 Figure 9: An Estonian example of the XCOPA benchmark using basic prompt and the corresponding outputs under the zero-shot setting.</p>
<p>Input:</p>
<p>I want you to act as a commonsense reasoning expert for Estonian .</p>
<p>Here is a premise: Ese oli mullikilesse mässitud.What is the cause?Help me pick the more plausible option: -choice1: See oli õrn, -choice2: See oli väike.You should retell the premise and the options in English.</p>
<p>You should do step-by-step answer to pick a choice .You should step-by-step answer the request.You should tell me the choice number in this format ' Choice number :'.</p>
<p>text-davinci-003 output:</p>
<p>Premise: Ese oli mullikilesse mässitud.Options: Choice 1: See oli õrn, Choice 2: See oli väike.</p>
<p>Step 1: Consider the context of the premise.What kind of object was mässitud (inserted) into the pocket?</p>
<p>Step 2: Consider the two options.Choice 1: See oli õrn (It was soft) and Choice 2: See oli väike (It was small).</p>
<p>Step 3: Based on the context and the two options, the more plausible choice is Choice 2: See oli väike (It was small).</p>
<p>Choice number: 2 gpt-3.5-turbooutput:</p>
<p>Premise: "Ese oli mullikilesse mässitud" (The object was wrapped in bubble wrap).Options:</p>
<ol>
<li>See oli õrn (It was fragile).</li>
</ol>
<p>See oli väike (It was small).</p>
<p>Step 1: Based on the premise, we know that the object was wrapped in bubble wrap.It does not provide any information about the size or fragility of the item.</p>
<p>Step 2: However, common sense would suggest that fragile items are more likely to be wrapped in bubble wrap for protection during transportation or storage.Therefore, the more plausible option is choice 1: "See oli õrn" (It was fragile).</p>
<p>Choice number: 1 .</p>
<p>Answer:</p>
<p>1 Figure 10: An Estonian example of the XCOPA benchmark using XLT and the corresponding outputs under the zero-shot setting.</p>
<p>Input:</p>
<p>I want you to act as a commonsense reasoning expert for Estonian .</p>
<p>Here is a premise: Mees jäi vanaks.What is the effect?Help me pick the more plausible option:</p>
<p>-choice1: Ta juuksed muutusid halliks, -choice2: Ta müüs oma asjad maha.</p>
<p>You should retell the premise and the options in English.</p>
<p>You should do step-by-step answer to pick a choice .You should step-by-step answer the request.You should tell me the choice number in this format ' Choice number :'.XXX XXX Premise: A man turned on the tap.</p>
<p>Options: 1.The toilet filled with water.2. Water flowed from the tap.</p>
<p>Step-by-step answer:</p>
<p>1.The premise states that a man turned on the tap, which implies that water started flowing.</p>
<ol>
<li>The first option suggests that the toilet filled with water, which is possible if the tap ...</li>
</ol>
<p>3.</p>
<p>The second option suggests that water flowed from the tap, which is the more plausible ... Premise: The options are not so attractive.Hypothesis: The choices are not too attractive.Relationship: Entailment .The hypothesis is true given the premise as the concepts of options and choices are interchangeable and both refer to the same thing.The degree of attractiveness is also conveyed in a similar manner in both statements.</p>
<p>Answer:</p>
<p>entailment Figure 13: A French example of the XNLI benchmark using XLT and the corresponding outputs under the zero-shot setting.</p>
<p>Input:  Input:
I
I want you to act as a paraphrase identification expert for Germany .</p>
<p>Answer:</p>
<p>No Figure 16: A Germany example of the PAWS-X benchmark using XLT and the corresponding outputs under the zero-shot setting.</p>
<p>Input:</p>
<p>I want you to act as a paraphrase identification expert for Germany .</p>
<p>Sentence</p>
<p>Answer:</p>
<p>No Input:</p>
<p>I want you to act as a question answering expert for Japanese .</p>
<p>Question: 世界で一番背の高い人類は誰ですか？</p>
<p>You should retell the question in English.</p>
<p>You should answer the question in English in one or a few words.text-davinci-003 output:</p>
<p>Huang Ming es un innovador apasionado por la energía solar que construyó el primer conjunto de edificios alimentados íntegramente por el sol, conocido como "Solar Valley" o Ciudad Solar.A pesar de que Huang esperaba que su proyecto inspirara a otros, todavía es el único de su tipo en China y el mundo.</p>
<p>gpt-3.5-turbo output:</p>
<p>Huang Ming built the "Solar Valley" in China, a collection of buildings that run entirely on solar power.Despite his hopes that others would follow his lead, it remains the only solar city in China and the world.</p>
<p>Answer:</p>
<p>Lo llaman "El rey sol", aunque a él no le gusta.Input:</p>
<p>I want you to act as a multilingual summarization expert for Spanish .</p>
<p>Text: Huang Ming es tan apasionado por la energía solar que construyó el llamado "Solar Valley" (o Ciudad Solar):</p>
<p>un conjunto de edificios que funcionan íntegramente a partir del sol.En su diseño también se integran los paneles solares que los alimentan.Pero aunque Huang soñaba con que todo el mundo imitaría sus esfuerzos, todavía sigue siendo la única Ciudad Solar en China (ydel mundo).Video producido por Álvaro Ricciardelli.</p>
<p>You should repeat the entire text in English.</p>
<p>You should think step-by-step to summarize the entire text in a maximum of two sentences .You should step-by-step answer the request.You should tell me the summary into one sentence in Spanish in this format ' Summary :'.</p>
<p>text-davinci-003 output:</p>
<p>Summary: Huang Ming es un apasionado de la energía solar que construyó la única Ciudad Solar en China y del mundo, llamada "Solar Valley", que funciona íntegramente con paneles solares.</p>
<p>gpt-3.5-turbo output:</p>
<p>The text is in Spanish and talks about Huang Ming, who built the "Solar Valley" in China, a group of buildings that run entirely on solar energy.Despite his hopes that others would follow his lead, it remains the only solar city in China and the world.</p>
<p>To summarize in two sentences: Huang Ming built the "Solar Valley" in China, a group of buildings that run entirely on solar energy, but it remains the only solar city in China and the world.Summary: Huang Ming construyó la "Ciudad Solar" en China, un conjunto de edificios que funcionan íntegramente a partir del sol, pero sigue siendo la única ciudad solar en China y en el mundo.</p>
<p>Answer:</p>
<p>Lo llaman "El rey sol", aunque a él no le gusta.Input:</p>
<p>I want you to act as a multilingual summarization expert for Spanish .</p>
<p>Text: Testigos del tiroteo explicaron que el atacante asaltó la mezquita a los pocos minutos de haber empezado la oración.La policía de Nueza Zelanda declaró que hay cuatro personas detenidas y que no puede descartarse que haya más implicados en los ataque.Ahora puedes recibir notificaciones de BBC Mundo.Descarga la nueva versión de nuestra app y actívalas para no perderte nuestro mejor contenido.</p>
<p>You should repeat the entire text in English.</p>
<p>You should think step-by-step to summarize the entire text in a maximum of two sentences .You should step-by-step answer the request.You should tell me the summary into one sentence in Spanish in this format ' Summary :'.XXX XXX The text states that a shooter attacked a mosque during prayer time in New Zealand, with four people currently in custody and the possibility of more involved.The article also promotes the BBC Mundo app for notifications.</p>
<p>To summarize, a mosque in New Zealand was attacked during prayer time with four people currently detained and the potential for more involvement, while also promoting the BBC Mundo app for notifications.text-davinci-003 output:</p>
<p>Summary: Huang Ming construyó la única Ciudad Solar del mundo, que funciona íntegramente con energía solar.</p>
<p>gpt-3.5-turbo output:</p>
<p>The text highlights Huang Ming's passion for solar energy and his creation of the "Solar Valley", a group of buildings that run entirely on solar power, but despite his hopes for others to follow suit, it remains the only solar city in China and the world.Summary: Huang Ming construyó la única Ciudad Solar en China y del mundo, un conjunto de edificios que funcionan íntegramente a partir del sol.</p>
<p>Answer:</p>
<p>Lo llaman "El rey sol", aunque a él no le gusta.text-davinci-003 output:</p>
<p>Source sentence: When there was a vaccine available to prevent death from Ebola, until now, there is no clear evidence of a treatment to cure the infection.</p>
<p>Target translation: 当有一种疫苗可以预防埃博拉死亡时，直到现在，还没有清楚的证据表明有治疗感染的方法 gpt-3.5-turbooutput:</p>
<p>Source sentence: Nalika ana siji vaksin pacoban katon bisa nyuda kematian Ebola, nganti saiki, ora ana obat sing kabukten jelas kanggo ngobati infeksi sing ana.English source sentence: When there is a vaccine trial that can help prevent Ebola deaths, until now, there is no clear proven medicine to treat existing infections.</p>
<p>to act as an arithmetic reasoning expert for Chinese .You should retell the request in English.You should do step-by-step answer to obtain a number answer .You should step-by-step answer the request.You should tell me the answer in this format 'Answer:by-step answer to obtain a number answer</p>
<p>Figure 2
2
Figure2: Overview of our method.Given a request, its associated meta information is filled into the placeholders of the XLT template to form the language-independent prompt, which is fed to the LLM to enhance the generation of responses in the desired format.</p>
<p>Figure 3 :
3
Figure 3: Illustration of XLT template.Referring to Figure 2 and Appendix for instantiated examples.</p>
<p>Figure 4 :
4
Figure 4: Construction process for few-shot learning.</p>
<p>Figure 5 : 3 Figure 6 :
536
Figure 5: Illustrations of different demonstration input-output pairs in the few-shot learning.</p>
<p>3 Figure 7
37
Figure 7: A Chinese example of the MGSM benchmark using XLT and the corresponding outputs under the zero-shot setting.</p>
<p>Figure 14 :Figure 15
1415
Figure 14: A French example of the XNLI benchmark using XLT and the corresponding outputs under the few-shot setting.</p>
<p>Figure 17 :Figure 18
1718
Figure 17: A Germany example of the PAWS-X benchmark using XLT and the corresponding outputs under the few-shot setting.</p>
<p>Figure 21 :
21
Figure 21: A Spanish example of the XL-Sum* benchmark using basic prompt and the corresponding outputs under the zero-shot setting.</p>
<p>Figure 22 :
22
Figure22:A Spanish example of the XL-Sum* benchmark using XLT and the corresponding outputs under the zero-shot setting.</p>
<p>Summary: Al menos 49 personas murieron en dos tiroteos en mezquitas en Nueva Zelanda.XXX XXX ... (the remaining 2 examples) XXX XXX I want you to act as a multilingual summarization expert for Spanish .Text: Huang Ming es tan apasionado por la energía solar que construyó el llamado "Solar Valley" (o Ciudad Solar): un conjunto de edificios que funcionan íntegramente a partir del sol.En su diseño también se integran los paneles solares que los alimentan.Pero aunque Huang soñaba con que todo el mundo imitaría sus esfuerzos, todavía sigue siendo la única Ciudad Solar en China (y del mundo).Video producido por Álvaro Ricciardelli.You should repeat the entire text in English.You should think step-by-step to summarize the entire text in a maximum of two sentences .You should step-by-step answer the request.You should tell me the summary into one sentence in Spanish in this format ' Summary :'.</p>
<p>Figure 23 :Figure 24
2324
Figure 23: A Spanish example of the XL-Sum* benchmark using XLT and the corresponding outputs under the few-shot setting.</p>
<p>Figure 25
25
Figure 25: A jv→zh example of the FLORES* benchmark using XLT and the corresponding outputs under the zero-shot setting.</p>
<p>Table 1 :
1SettingsReasoning MGSM XCOPA XNLI PAWS-X MKQA XL-Sum<em> FLORES</em> Understanding Generationtext-davinci-003Basic Prompt12.570.153.352.029.023.715.4CoT25.770.953.057.830.923.815.8Translate-En15.768.054.855.0---Zero-shotXLT gpt-3.5-turbo23.973.362.457.140.225.217.7Basic Prompt23.376.952.665.531.624.719.1CoT45.578.354.861.014.825.419.7Translate-En27.175.752.266.8---XLT70.080.365.563.642.726.121.2text-davinci-003Basic Prompt45.575.659.168.739.126.8-Translate-En46.577.456.968.5---Few-shotXLT gpt-3.5-turbo55.481.367.572.249.627.3-Basic Prompt63.080.161.466.443.725.5-Translate-En65.181.958.363.7---XLT72.585.965.069.152.527.9-SettingsReasoning MGSM XCOPA XNLI PAWS-X UnderstandingGeneration MKQAZero-shot settingtext-davinci-003Basic Prompt65.277.883.897.160.2CoT65.480.183.589.561.4Translate-En77.278.786.095.351.6XLT68.582.180.788.478.7gpt-3.5-turboBasic Prompt73.083.680.589.061.8CoT66.785.780.788.946.4Translate-En80.484.679.890.754.1XLT84.189.188.096.275.3Few-shot settingtext-davinci-003Basic Prompt75.482.082.588.274.3Translate-En77.182.679.587.868.5XLT84.585.685.391.682.7gpt-3.5-turboBasic Prompt76.184.183.694.482.1Translate-En78.686.479.295.471.3XLT86.289.784.394.183.1
The average scores in different languages for the seven benchmarks in zero-shot and few-shot settings.We omit the results (denoted as "-") of Translate-En since it is not applicable for generation tasks.</p>
<p>Table 2 :
2
The democratization degree of tasks against languages.</p>
<p>Removing Role Assigning from XLT impedes the model's
SettingsMGSM de zhXNLI hi viFLORES* jv→zh zh→jvXLT79.8 72.6 61.3 64.819.010.5Instruction Ablationw/o Role Assigning w/o Cross-lingual Thinking w/o CoT Task Solving76.6 69.2 57.8 63.9 75.6 62.0 56.1 62.2 77.0 68.0 62.9 65.216.2 13.2 16.88.8 8.2 9.2Instruction OrderSwap Role Assigning and Task Inputting Swap Role Assigning and Task Analyzing Swap Cross-lingual Thinking and Task Analyzing 79.0 71.2 59.5 63.4 77.2 71.8 54.2 61.5 76.8 70.8 61.0 64.019.6 15.8 16.511.2 8.8 9.7Rephrasing Wordw/ retell w/ repeat w/ translate79.8 72.6 61.3 64.8 77.6 68.0 60.7 64.6 76.4 70.0 60.1 64.518.2 19.0 17.510.3 10.5 10.2</p>
<p>Table 3 :
3
Performance comparison across different variants of XLT.All the experiments are conducted using gpt-3.5-turbounder the zero-shot setting.
Demonstration formatenderufrzhesjaswthbnteAvg.Basic input + Basic output 84.0 79.2 78.8 78.8 70.8 81.2 68.8 70.8 68.8 65.2 44.8 71.9Basic input + XLT output82.4 72.4 71.2 75.2 64.4 78.8 63.2 66.8 53.6 54.8 32.4 65.0XLT input + XLT output84.8 81.4 80.2 79.2 71.8 81.6 72.8 71.2 69.8 64.4 40.8 72.5</p>
<p>Table 4 :
4
Performance comparison across different few-shot variants on the MGSM benchmark.All the experiments are conducted with 5 demonstrations using gpt-3.5-turbo.</p>
<p>Table 5 :
5
The basic prompt of each benchmark.#Test denotes the number of instances in the test set.
Benchmark #Test Basic PromptMGSM250 Request: {problem}XCOPA500 Here is a premise: {premise}. What is the {question}? Help me pick the more plausibleoption: -choice1: {choice1}, -choice2: {choice2}XNLI5,010 {premise} Based on previous passage, is it true that {hypothesis}? Yes, No, or Maybe?PAWS-X2,000 Sentence 1: {sentence1} Sentence 2: {sentence2} Question: Does Sentence 1 paraphraseSentence 2? Yes or No?MKQA6,758 Answer the question in one or a few words in {target_language}: {question}?XL-Sum<em>250 Summarize this article: {article}FLORES</em>200 {source} Translate from {source_language} to {target_language}:</p>
<p>Table 6 :
6
Task meta data consisting of task name, input tag, task goal, output type, and output constraint per benchmark.Detailed examples of the input for each benchmark are listed in the following part.
BenchmarkTask nameInput tagTask goalOutput typeOutput constraintMGSMarithmetic reason-requestdo step-by-step answer to obtain a number answeranswer-ingXCOPAcommonsense rea-premise and the op-do step-by-step answer to pick a choicechoice number-soningtionsXNLInatural language in-hypothesis and thejudge whether the hypothesis is true, false, or undeterminedrelationship-ferencepremisegiven the premise. The relationship can be chosen from en-tailment, contradiction, and neutralPAWS-Xparaphrase identifi-sentence 1 and sen-provide a yes or no answer to the question: Does Sentence 1answerchoosing either yes orcationtence 2paraphrase Sentence 2?noMKQAquestion answeringquestionanswer the question in English in one or a few wordsanswerin one or a few wordsin {target_language}XL-Summultilingual sum-entire textthink step-by-step to summarize the entire text in a maximumsummaryinto one sentence inmarizationof two sentences{target_language}FLORESmachine transla-source sentenceprovide the {target_language} translation for the Englishtarget translation-tionsource sentence</p>
<p>Table 7 :
7
Accuracy scores on the MGSM benchmark.Shi et al. (2023) * utilize 6-shot learning.
Settings (high→low)enderufrzhesjaswthbnteAvg.text-davinci-003Basic Prompt19.2 12.8 15.6 16.4 15.2 13.6 12.87.28.811.64.412.5XLT30.0 32.4 23.6 34.8 29.2 26.8 26.0 13.6 18.4 14.8 12.8 23.9gpt-3.5-turboZero-shotBasic Prompt32.0 24.8 28.0 31.6 22.0 29.2 22.4 24.4 16.8 18.07.623.3XLT84.4 79.8 77.6 75.2 72.6 76.8 71.0 70.8 63.8 56.8 42.0 70.0Llama-2-70b-chat-hfBasic Prompt58.8 48.0 47.2 45.6 39.6 50.4 39.2 10.0 13.6 17.25.234.1XLT60.0 52.8 52.8 48.8 42.4 52.0 39.2 16.4 18.0 17.6 10.4 37.3code-davinci-002(Shi et al., 2023) * 53.6 46.4 48.8 46.4 47.2 51.6 44.8 37.6 41.2 41.2 42.8 45.6text-davinci-003Few-shotBasic Prompt60.4 45.6 51.6 45.6 38.8 51.6 37.6 48.8 30.4 43.6 46.8 45.5XLT65.6 58.0 57.6 56.8 53.2 58.0 54.4 58.8 42.4 53.2 51.8 55.4gpt-3.5-turboBasic Prompt82.8 69.2 71.6 72.4 46.8 71.2 56.0 60.0 44.0 62.4 56.6 63.0XLT84.8 81.4 80.2 79.2 71.8 81.6 72.8 71.2 69.8 64.4 40.8 72.5Settings (high→low)zhitvitridswthettahtquAvg.text-davinci-003Basic Prompt85.4 90.0 69.2 80.6 83.8 56.4 66.6 73.0 53.4 61.6 50.4 70.1Zero-shotXLT85.8 89.2 76.0 81.0 86.4 59.2 67.2 83.4 55.2 72.2 50.2 73.3gpt-3.5-turboBasic Prompt90.4 92.0 83.6 86.6 88.2 77.0 70.2 84.0 57.2 65.2 51.2 76.9XLT87.8 89.8 87.5 90.2 89.5 82.0 78.0 88.4 64.0 74.6 51.8 80.3code-davinci-002(Shi et al., 2023) *93.4 96.6 86.6 91.2 91.4 67.4 84.2 88.8 55.8 79.6 52.2 80.7text-davinci-003Few-shot(Ahuja et al., 2023)  † Basic Prompt-90.8 92.2 80.2 85.2 90.8 63.6 69.2 81.8 53.6 73.2 51.0 75.6 94.6 -89.8 93.0 82.8 84.8 89.6 87.0 82.8 --XLT94.0 95.0 87.0 94.0 92.8 68.4 79.4 90.4 59.4 80.8 53.0 81.3gpt-3.5-turboBasic Prompt91.0 95.2 86.2 89.0 88.6 79.2 73.6 92.0 58.6 74.2 53.0 80.1XLT92.8 95.8 90.6 92.2 90.2 92.6 85.2 93.0 70.8 86.0 56.2 85.9</p>
<p>Table 8 :
8
Accuracy scores on the XCOPA benchmark.(Shi et al., 2023) * utilize 6-shot learning.Ahuja et al. (2023) † utilize 8-shot learning.
Settings (high→low)enderufrzhesvitrswarelthbghiurAvg.text-davinci-003Basic Prompt63.6 59.4 55.9 60.9 51.6 59.7 49.5 53.9 40.8 51.9 53.2 49.7 54.4 49.8 45.3 53.3Zero-shotXLT77.4 67.7 64.2 68.3 64.8 69.4 62.0 61.5 54.3 58.7 61.1 56.3 62.6 55.1 53.0 62.4gpt-3.5-turboBasic Prompt65.4 55.5 50.6 53.2 48.8 59.8 52.1 54.4 49.6 50.9 54.9 44.8 55.7 49.2 44.8 52.6XLT74.4 68.5 66.0 69.8 64.9 69.4 64.8 65.0 60.1 62.8 68.3 62.1 67.7 61.3 57.3 65.5text-davinci-003(Ahuja et al., 2023)  † 79.5 71.7 67.3 71.8 65.8 72.2 66.9 67.6 57.3 65.1 69.3 62.0 70.8 63.3 55.1 67.1Basic Prompt71.6 65.8 62.5 63.4 56.7 64.6 59.4 56.9 48.2 57.3 62.0 55.0 62.6 52.4 48.0 59.1Few-shotXLT79.1 70.8 70.0 69.5 69.2 71.0 67.3 66.9 59.5 65.7 67.8 63.7 70.4 63.5 58.1 67.5gpt-3.5-turboBasic Prompt73.4 66.3 60.9 67.9 60.2 68.1 60.2 62.6 55.7 58.8 64.7 52.7 64.6 53.8 50.8 61.4XLT77.1 69.3 64.4 69.6 62.9 70.6 63.2 64.4 60.2 63.4 66.6 59.8 66.9 60.0 56.5 65.0</p>
<p>Table 9 :
9
Accuracy scores on the XNLI benchmark.Ahuja et al. (2023) † utilize 8-shot learning.
Settings (high→low)endefrzhesjakoAvg.text-davinci-003Basic Prompt53.3 52.9 50.8 53.6 53.0 50.3 50.3 52.0Zero-shotXLT64.6 56.0 55.3 57.4 56.0 54.6 55.9 57.1gpt-3.5-turboBasic Prompt73.6 68.3 68.4 63.4 69.6 59.7 55.7 65.5XLT65.3 66.1 64.8 65.5 63.3 62.4 57.6 63.6text-davinci-003(Ahuja et al., 2023)  † 72.5 69.8 71.3 65.2 70.1 65.4 65.8 68.6Basic Prompt77.8 70.6 72.5 65.0 71.7 62.5 60.5 68.7Few-shotXLT76.5 78.8 77.4 61.1 78.0 65.0 68.7 72.2gpt-3.5-turboBasic Prompt65.9 70.3 66.6 64.1 68.2 65.6 64.0 66.4XLT73.4 69.8 68.5 70.9 67.8 66.4 66.7 69.1</p>
<p>Table 10 :
10
Accuracy scores on the PAWS-X benchmark.Ahuja et al. (2023) † utilize 8-shot learning.
Settings (high→low)enderufrzhesjavitrthAvg.text-davinci-003Basic Prompt 48.1 33.8 15.9 34.8 18.2 34.1 27.7 23.6 24.0 29.6 29.0Zero-shotXLT51.1 42.3 27.3 43.0 36.7 43.3 46.8 35.8 37.8 38.1 40.2gpt-3.5-turboBasic Prompt 51.1 40.6 28.4 40.1 16.5 39.3 25.9 23.3 26.9 23.7 31.6XLT56.7 46.0 33.9 47.6 33.0 47.9 47.5 36.5 39.1 38.6 42.7text-davinci-003Basic Prompt 52.6 42.3 21.8 42.9 33.1 42.8 45.5 35.5 37.5 36.6 39.1Few-shotXLT57.6 49.4 42.7 50.9 51.0 50.0 60.0 46.9 46.9 40.5 49.6gpt-3.5-turboBasic Prompt 53.2 48.6 31.0 46.1 40.9 47.9 51.4 38.5 40.0 39.3 43.7XLT59.6 52.5 43.8 53.9 51.9 54.0 63.2 49.4 52.1 44.7 52.5</p>
<p>Table 11 :
11
F1 scores on the MKQA benchmark.The average score is the macro average F1 score.
Settings (high→low)enfrzhesvitrAvg.text-davinci-003Basic Prompt22.2 26.2 30.8 25.1 22.0 15.9 23.7XLT24.4 28.2 32.2 26.0 22.3 17.9 25.2Zero-shotgpt-3.5-turbo(Lai et al., 2023) 19.7 20.8 21.1 17.8-14.5-Basic Prompt25.3 26.2 30.2 26.3 21.1 19.2 24.7XLT26.8 28.1 33.3 26.4 21.3 20.5 26.1text-davinci-003Basic Prompt29.2 29.6 33.2 28.3 22.5 18.1 26.8Few-shotXLT28.2 30.3 34.4 29.4 22.7 18.6 27.3gpt-3.5-turboBasic Prompt25.7 27.2 30.8 27.8 21.5 19.7 25.5XLT28.5 29.2 35.0 28.6 23.7 22.3 27.9</p>
<p>Table 12 :
12
ROUGE-1 scores on the XL-Sum* benchmark.
High-HighHigh-LowLow-LowSettingszh-rude-vizh-thzh-jvth-gljv-th→←→←→←→←→←→←text-davinci-003Basic Prompt 19.8 24.2 26.5 24.5 10.2 11.88.114.0 17.9 12.0 10.06.2Zero-shotXLT21.6 24.8 27.4 24.8 12.6 16.4 11.1 18.2 20.7 14.2 11.79.0gpt-3.5-turboBasic Prompt 23.3 25.4 34.1 29.6 16.6 18.69.116.2 18.1 18.5 13.07.2XLT25.3 25.6 33.3 31.3 19.9 19.3 10.5 19.0 22.1 21.9 15.9 10.6</p>
<dl>
<dt>Table 13 :</dt>
<dt>13</dt>
<dt>BLEU scores on the FLORES* benchmark.</dt>
<dd>Basic Prompt: XLTI want you to act as an …I want you to act as an … Request:罗杰有5个网球…Request:罗杰有5个网球……Request: 罗杰有5个网球… 5x Answer:逐步解答：罗杰一开始 有 5 个球 … 答案是 11。5x… You should tell me the answer … Answer:逐步解答：罗杰一开始 有 5 个球 … 答案是 11。5xYou should tell me the answer … Retelling the request in … Step-by-step answer: …Answer: 11(a) Basic prompt input and output(b) Basic prompt input and XLT output(c) XLT input and output</dd>
</dl>
<p>want you to act as an arithmetic reasoning expert for Chinese .Each can has 3 tennis balls, so 2 cans will have a total of 6 tennis ... 4.To find out how many tennis balls Roger has in total, we need to add the ... 5. Adding 5 (initial tennis balls) and 6 (purchased tennis balls) gives ...
Request: 罗杰有5个网球。他又买了2罐网球。每罐有3个网球。他现在有多少个网球？You should retell the request in English.You should do step-by-step answer to obtain a number answer .You should step-by-step answer the request.You should tell me the answer in this format ' Answer :'.XXXXXXRetelling the request in English: Roger currently has 5 tennis balls...Step-by-step answer:1. Roger has 5 tennis balls.2. He buys 2 cans of tennis balls.3. Answer: 11.XXX XXX... (the remaining 4 examples)XXX XXXI want you to act as an arithmetic reasoning expert for Chinese .
Request: 制作一件袍子需要 2 匹蓝色纤维布料和这个数量一半的白色纤维布料。它一共 需要用掉多少匹布料？ You should retell the request in English.You should do step-by-step answer to obtain a number answer .You should step-by-step answer the request.You should tell me the answer in this format ' Answer :'.</p>
<p>Here is a premise: Ese oli mullikilesse mässitud.What is the cause?Help me pick the more plausible option: -choice1: See oli õrn, -choice2: See oli väike.You should retell the premise and the options in English.Les options ne sont pas si attrayantes.Based on previous passage, is it true that Les choix ne sont pas trop attrayants?Yes, No, or Maybe?French example of the XNLI benchmark using basic prompt and the corresponding outputs under the zero-shot setting.We need a verbalizer to map entailment to yes, contradiction to no, and neutral to maybe.
text-davinci-003 output:yesgpt-3.5-turbo output:maybeAnswer:entailmentChoice Number: 2. Water flowed from the tap. The answer is choice2." XXX XXX ... (the remaining 4 examples) XXX XXX I want you to act as a commonsense reasoning expert for Estonian . Figure 12: A Input:I want you to act as a natural language inference expert for French .Premise: Les options ne sont pas si attrayantes. You should do step-by-step answer to pick a choice . You should step-by-step answer the request. Hypothesis: Les choix ne sont pas trop attrayants.You should tell me the choice number in this format ' Choice Number :'. You should retell the premise and hypothesis in English.You should judge whether the hypothesis is true (entailment), false(contradiction), or undetermined (neutral) given the premise. The relationshipcan be chosen from entailment, contradiction, and neutral .You should step-by-step answer the request.You should tell me the relationship in this format ' Relationship :'.text-davinci-003 output:Premise (in English): The options are not very attractive. Hypothesis (in English): The choices are not too attractive.Relationship: Contradictiongpt-3.5-turbo output:
Input:</p>
<p>want you to act as a natural language inference expert for French .
Premise: savons-nous ce que nous allons dire ?Hypothesis: Je sais que nous n'avons aucune idée de ce que nous allons dire.You should retell the premise and hypothesis in English.You should judge whether the hypothesis is true (entailment), false(contradiction), or undetermined (neutral) given the premise. The relationshipcan be chosen from entailment, contradiction, and neutral .You should step-by-step answer the request.You should tell me the relationship in this format ' Relationship :'.XXX XXXRetelled premise: do we know what we're going to say.Retelled hypothesis: I know we don't have a clue what we'll say.Relationship: contradictionXXXXXX ... (the remaining 4 examples)XXX XXXI want you to act as a natural language inference expert for French .Premise: Les options ne sont pas si attrayantes.Hypothesis: Les choix ne sont pas trop attrayants.You should retell the premise and hypothesis in English.You should judge whether the hypothesis is true (entailment), false(contradiction), or undetermined (neutral) given the premise. The relationshipcan be chosen from entailment, contradiction, and neutral .You should step-by-step answer the request.You should tell me the relationship in this format ' Relationship :'.text-davinci-003 output:retelled premise: the options are not so attractive.retelled hypothesis: the choices are not too attractive.relationship: entailmentgpt-3.5-turbo output:retelled premise: the options are not very attractive. retelled hypothesis: the choices are not too attractive.relationship: entailmentAnswer:entailment</p>
<p>1: Der Ponoru ist eine Nebenfluss des Horezu in Rumänien .Sentence 2: Der Ponoru ist ein Nebenfluss des Horezu in Rumänien .Question: Does Sentence 1 paraphrase Sentence 2? Yes or No?You should retell the sentence 1 and sentence 2 in English.You should provide a yes or no answer to the question: Does Sentence 1 paraphrase Sentence 2? You should step-by-step answer the request.You should tell me the answer choosing either yes or no in this format ' Answer :'.XXX XXX Retelled Sentence 1: The Ponoru River is a tributary of the Horezu River in Romania .Retelled Sentence 2: The Ponoru River is a tributary of the Horezu in Romania .
Answer: YesXXX XXX ... (the remaining 4 examples)XXX XXXI want you to act as a paraphrase identification expert for Germany .Sentence 1: Lito spielte für den Zingone Club FootballSentence 2: Zingone spielte für den ClubQuestion: Does Sentence 1 paraphrase Sentence 2? Yes or No?You should retell the sentence 1 and sentence 2 in English.You should provide a yes or no answer to the question: Does Sentence 1 paraphrase Sentence 2?You should step-by-step answer the request.You should tell me the answer choosing either yes or no in this format ' Answer :'.text-davinci-003 output:Retelled Sentence 1: Lito played football for the Zingone Club. Retelled Sentence 2: Zingone played football for the Lito Club.Answer: Yesgpt-3.5-turbo output:Retelled Sentence 1: Lito played football for the Zingone Club.Retelled Sentence 2: Zingone played football for the Lito Club.Answer: No</p>
<p>Summarize this article: Huang Ming es tan apasionado por la energía solar que construyó el llamado "Solar Valley" (o Ciudad Solar): un conjunto de edificios que funcionan íntegramente a partir del sol.En su diseño también se integran los paneles solares que los alimentan.Pero aunque Huang soñaba con que todo el mundo imitaría sus esfuerzos, todavía sigue siendo la única Ciudad Solar en China (y del mundo).Video producido por Álvaro Ricciardelli.
Input:Input:I want you to act as a question answering expert for Japanese .Question: フルハウスでスティーブが出てきたのはどのシーズンですか？You should retell the question in English.You should answer the question in English in one or a few words.You should step-by-step answer the request.You should tell me the answer in one or a few words in Japanese in thisformat ' Answer :'.XXXXXXQuestion: what season does steve come in full house?English Answer: 5.0 seasons.Japanese Answer: 5.0 時節XXXXXX... (the remaining 4 examples) XXXXXXI want you to act as a question answering expert for Japanese .Question: 世界で一番背の高い人類は誰ですか？You should retell the question in English.You should answer the question in English in one or a few words.You should step-by-step answer the request.You should tell me the answer in one or a few words in Japanese in thisformat ' Answer :'.text-davinci-003 output:question: who is the tallest human being in the world english answer: sultan kösenAnswer: Robert Wadlow (2.72 m). Question in English: Who is the tallest human being in the world? japanese answer: スルタン・ケーセンAnswer in Japanese: 答え： ロバート・ワドロウ(2.72 m)。 gpt-3.5-turbo output:question: who is the tallest person in the world gpt-3.5-turbo output: nenglish answer: robert wadlowRetelling: Who is the tallest person in the world? Step-by-step response: Answer: Robert Wadlow japanese answer: ロバート・ワドロー1. The question is asking for the tallest person in the world. 2. The answer is Robert Wadlow, who was 8 feet 11 inches (2.72 meters) tall. Answer:Answer in Japanese: 答え： ロバート・ワドロー ロバート・ワドローAnswer: Figure 20: A Japanese example of the MKQA benchmark using XLT and the corresponding outputs under the ロバート・ワドロー few-shot setting.
You should step-by-step answer the request.You should tell me the answer in one or a few words in Japanese in this format ' Answer :'.text-davinci-003 output:Figure19:A Japanese example of the MKQA benchmark using XLT and the corresponding outputs under the zero-shot setting.</p>
<p>https://github.com/openai/openai-cookbook
https://platform.openai.com/docs/guides/chat/ introduction
https://commoncrawl.github.io/ cc-crawl-statistics/plots/languages
https://platform.openai.com/docs/models/ gpt-3-5
AcknowledgementsTianyi Tang and Xin Zhao are supported by National Natural Science Foundation of China under Grant No. 62222215, Beijing Natural Science Foundation under Grant No. 4222027 and L233008.
References Kabir Ahuja, Rishav Hada, Millicent Ochieng, Prachi Jain, Harshita Diddee, Samuel Maina, Tanuja Ganu, Sameer Segal, Maxamed Axmed, Kalika Bali, arXiv:2303.12528Mega: Multilingual evaluation of generative ai. 2023arXiv preprint</p>
<p>Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, arXiv:2302.04023A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. 2023arXiv preprint</p>
<p>Zero-shot cross-lingual transfer of neural machine translation with multilingual pretrained encoders. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam Mccandlish, Alec Radford, Ilya Sutskever, Dario Amodei, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Yun Ma, Li Chen, Dongdong Dong, Jia Zhang, Wenping Pan, Furu Wang, Wei, the 2021 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational Linguistics2020. 202133Online and Punta CanaAdvances in Neural Information Processing Systems</p>
<p>Towards making the most of cross-lingual transfer for zero-shot neural machine translation. Guanhua Chen, Shuming Ma, Yun Chen, Dongdong Zhang, Jia Pan, Wenping Wang, Furu Wei, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20221Long Papers)</p>
<p>InfoXLM: An information-theoretic framework for cross-lingual language model pre-training. Zewen Chi, Li Dong, Furu Wei, Nan Yang, Saksham Singhal, Wenhui Wang, Xia Song, Xian-Ling Mao, Heyan Huang, Ming Zhou, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesAssociation for Computational Linguistics2021</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, arXiv:2204.02311Palm: Scaling language modeling with pathways. 2022arXiv preprint</p>
<p>Unsupervised cross-lingual representation learning at scale. Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>XNLI: Evaluating crosslingual sentence representations. Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger Schwenk, Veselin Stoyanov, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels; Belgium2018Association for Computational Linguistics</p>
<p>James Marta R Costa-Jussà, Onur Cross, Maha Çelebi, Kenneth Elbayad, Kevin Heafield, Elahe Heffernan, Janice Kalbassi, Daniel Lam, Jean Licht, Maillard, arXiv:2207.04672No language left behind: Scaling human-centered machine translation. 2022arXiv preprint</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>Glottocodes: Identifiers linking families, languages and dialects to comprehensive reference information. Robert Forkel, Semantic Web. 1362022</p>
<p>XLsum: Large-scale multilingual abstractive summarization for 44 languages. Tahmid Hasan, Abhik Bhattacharjee, Md Saiful Islam, Kazi Mubasshir, Yuan-Fang Li, Yong-Bin Kang, M Sohel Rahman, Rifat Shahriyar, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Online. Association for Computational Linguistics2021</p>
<p>Measuring massive multitask language understanding. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, International Conference on Learning Representations. 2021</p>
<p>How good are gpt models at machine translation? a comprehensive evaluation. Amr Hendy, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr, Hitokazu Matsushita, Young , Jin Kim, Mohamed Afify, Hany Hassan Awadalla, arXiv:2302.092102023arXiv preprint</p>
<p>Wenxiang Jiao, Wenxuan Wang, Jen Tse Huang, Xing Wang, Zhaopeng Tu, arXiv:2301.08745Is chatgpt a good translator? yes with gpt-4 as the engine. 2023arXiv preprint</p>
<p>Jared Kaplan, Sam Mccandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei, arXiv:2001.08361Scaling laws for neural language models. 2020arXiv preprint</p>
<p>Dac Viet, Lai, Trung Nghia, Amir Ngo, Ben Pouran, Hieu Veyseh, Franck Man, Trung Dernoncourt, Thien Huu Bui, Nguyen, arXiv:2304.05613Chatgpt beyond english: Towards a comprehensive evaluation of large language models in multilingual learning. 2023arXiv preprint</p>
<p>Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, arXiv:2211.09110Holistic evaluation of language models. Ananya Kumar, et al. 2022arXiv preprint</p>
<p>ROUGE: A package for automatic evaluation of summaries. Chin-Yew Lin, Text Summarization Branches Out. Barcelona, SpainAssociation for Computational Linguistics2004</p>
<p>Veselin Stoyanov, and Xian Li. 2022. Few-shot learning with multilingual generative language models. Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O' Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingMona Diab; Abu Dhabi, United Arab EmiratesAssociation for Computational Linguistics</p>
<p>Multilingual denoising pretraining for neural machine translation. Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, Luke Zettlemoyer, 20208Transactions of the Association for Computational Linguistics</p>
<p>MKQA: A linguistically diverse benchmark for multilingual open domain question answering. Shayne Longpre, Yi Lu, Joachim Daiber, Transactions of the Association for Computational Linguistics. 20219</p>
<p>Deltalm: Encoder-decoder pre-training for language generation and translation by augmenting pretrained multilingual encoders. Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, Alexandre Muzio, Saksham Singhal, Hany Hassan Awadalla, Xia Song, Furu Wei, arXiv:2106.137362021arXiv preprint</p>
<p>arXivOpenAI. 2023. Gpt-4 technical report. </p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Advances in Neural Information Processing Systems. Jan Leike, and Ryan Lowe. 202235</p>
<p>ERNIE-M: Enhanced multilingual representation by aligning cross-lingual semantics with monolingual corpora. Xuan Ouyang, Shuohuan Wang, Chao Pang, Yu Sun, Hua Hao Tian, Haifeng Wu, Wang, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational Linguistics2021Online and Punta Cana</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. the 40th Annual Meeting of the Association for Computational LinguisticsPhiladelphia, Pennsylvania, USAAssociation for Computational Linguistics2002</p>
<p>XCOPA: A multilingual dataset for causal commonsense reasoning. Maria Edoardo, Goran Ponti, Olga Glavaš, Qianchu Majewska, Ivan Liu, Anna Vulić, Korhonen, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>A call for clarity in reporting BLEU scores. Matt Post, Proceedings of the Third Conference on Machine Translation: Research Papers. the Third Conference on Machine Translation: Research PapersBelgium, BrusselsAssociation for Computational Linguistics2018</p>
<p>. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Wei Zhou, Peter J Li, Liu, Albert Sanh, Colin Webson, Stephen Raffel, Lintang Bach, Zaid Sutawika, Antoine Alyafeai, Arnaud Chaffin, Arun Stiegler, Manan Raja, Dey, Canwen Saiful Bari, Urmish Xu, Shanya Thakker, Eliza Sharma Sharma, Taewoon Szczechla, Gunjan Kim, Nihal Chhablani, Debajyoti Nayak, Jonathan Datta, Mike Chang, Tian-Jian, Han Jiang, Matteo Wang, Sheng Manica, Zheng Xin Shen, Harshit Yong, Rachel Pandey, Thomas Bawden, ; Wang, M Alexander, Rush, International Conference on Learning Representations. Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason , Alan Fries, Ryan Teehan, Le Teven, Scao, Stella Biderman, Leo Gao, Thomas Wolf2020. 202221Multitask prompted training enables zero-shot task generalization</p>
<p>Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, arXiv:2211.05100Bloom: A 176b-parameter open-access multilingual language model. 2022arXiv preprint</p>
<p>Language models are multilingual chain-of-thought reasoners. Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, Jason Wei, The Eleventh International Conference on Learning Representations. 2023</p>
<p>AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts. Taylor Shin, Yasaman Razeghi, Robert L Logan, I V , Eric Wallace, Sameer Singh, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.09288Llama 2: Open foundation and fine-tuned chat models. 2023arXiv preprint</p>
<p>David Vilar, Markus Freitag, Colin Cherry, Jiaming Luo, Viresh Ratnakar, George Foster, arXiv:2211.09102Prompting palm for translation: Assessing strategies and performance. 2022arXiv preprint</p>
<p>Finetuned language models are zero-shot learners. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, Quoc V Le, 2022aInternational Conference on Learning Representations</p>
<p>Emergent abilities of large language models. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, William Fedus, Transactions on Machine Learning Research. Survey Certification. 2022b</p>
<p>PAWS-X: A cross-lingual adversarial dataset for paraphrase identification. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, ; Yinfei Zhou, Yuan Yang, Chris Zhang, Jason Tar, Baldridge, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaCurran Associates, Inc2022c. 201935Advances in Neural Information Processing Systems. Association for Computational Linguistics</p>
<p>Seonghyeon Ye, Hyeonbin Hwang, Sohee Yang, Hyeongu Yun, Yireun Kim, Minjoon Seo, arXiv:2302.14691-context instruction learning. 2023arXiv preprint</p>
<p>Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Dong, arXiv:2303.18223A survey of large language models. 2023arXiv preprint</p>
<p>output: Request: To make a robe, two pieces of blue fabric and half the amount of white fabric are needed. How many pieces of fabric are needed in total?. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, Sameer Singh, ; Wenhao Pmlr, Hongyi Zhu, Qingxiu Liu, Jingjing Dong, Lingpeng Xu, Jiajun Kong, Lei Chen, Shujian Li, Huang, arXiv:2304.04675.text-davinci-003Multilingual machine translation with large language models: Empirical results and analysis. 2021. 2023139arXiv preprintCalibrate before use: Improving few-shot performance of language models. Step-by-step answer: 1. Two pieces of blue fabric are needed</p>
<p>Half the amount of blue fabric is needed for white fabric, so that is 1 piece of white fabric. 2 x 0.5 = 1</p>
<p>To find out how many pieces of fabric are needed in total, we need to add the number of blue fabric and white fabric. </p>
<p>Answer: 3 pieces of fabric. gpt-3.5-turbo output: Request: Making one robe requires 2 pieces of blue fiber fabric and half that amount of white fiber fabric. How many pieces of fabric are needed in total? Step-by-step answer: 1. To make one robe. Adding 2 (blue fabric) and 1 (white fabric) gives us a total of 3 pieces of fabric. we need 2 pieces of blue fiber fabric</p>
<p>We also need half that amount of white fiber fabric. which is 1 piece (2/2 = 1</p>
<p>Answer: 3 Figure 8: A Chinese example of the MGSM benchmark using XLT and the corresponding outputs under the few-shot setting. text-davinci-003 output: Premise: The object was wedged into a crack. Therefore, to make one robe, we need a total of 2 + 1 = 3 pieces of fiber fabric. Answer: We need 3 pieces of fiber fabric in total to make one robe. Options: 1. It was delicate</p>
<p>Step-by-step answer: 1. The premise states that the object was wedged into a crack. It was small. </p>
<p>Option 1 suggests that the object was delicate, which is not necessarily related to the object being wedged into a crack. </p>
<p>Choice Number: 2 . It was small.The answer is choice2. gpt-3.5-turbo output: Premise: "The object was wrapped in bubble wrap. Option 2 suggests that the object was small, which is more plausible as a small object is more likely to fit into a crack. Options: 1. It was fragile</p>
<p>Step-by-step answer: 1. The premise states that the object was wrapped in bubble wrap. It was small. which is often used to protect fragile items during transport</p>
<p>Bubble wrap is not typically used to protect small items, so option 1 is more plausible. </p>
<p>Therefore, the more plausible option is Choice 1: It was fragile. Choice Number: 1 . It was fragile. The answer is choice1. Answer: 1 Figure 11: An Estonian example of the XCOPA benchmark using XLT and the corresponding outputs under the few-shot setting. </p>            </div>
        </div>

    </div>
</body>
</html>
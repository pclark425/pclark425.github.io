<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9634 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9634</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9634</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-167.html">extraction-schema-167</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-271310513</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2407.13993v2.pdf" target="_blank">LLAssist: Simple Tools for Automating Literature Review Using Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> This paper introduces LLAssist, an open-source tool designed to streamline literature reviews in academic research. In an era of exponential growth in scientific publications, researchers face mounting challenges in efficiently processing vast volumes of literature. LLAssist addresses this issue by leveraging Large Language Models (LLMs) and Natural Language Processing (NLP) techniques to automate key aspects of the review process. Specifically, it extracts important information from research articles and evaluates their relevance to user-defined research questions. The goal of LLAssist is to significantly reduce the time and effort required for comprehensive literature reviews, allowing researchers to focus more on analyzing and synthesizing information rather than on initial screening tasks. By automating parts of the literature review workflow, LLAssist aims to help researchers manage the growing volume of academic publications more efficiently.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9634.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9634.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLAssist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLAssist (Literature Review Assistant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source C# tool that automates initial literature review tasks by using LLM backends to extract semantics from titles/abstracts, score relevance to user-defined research questions, and output per-article JSON/CSV with reasoning; intended as a lightweight human-in-the-loop screening aid.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLAssist (pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A pipeline/tool that calls configurable LLM backends (local or cloud) to perform: (1) key semantics extraction from title+abstract, (2) per-research-question relevance scoring (0–1) and binary decisions, and (3) natural-language reasoning for each decision. Implemented in C# using Microsoft.SemanticKernel for OpenAI-compatible API calls, CsvHelper for IO, and supports local Ollama models or OpenAI cloud models.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Manually sampled article metadata (title, abstract, authors, venue, keywords) exported from IEEE Xplore and Scopus using search queries targeting LLMs and cybersecurity. Experiments included a small test (17, 37, 115 articles) and a large test (Scopus subset of 2,576 articles).</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td>2576</td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Four research questions guiding the screening: RQ1 LLMs for threat detection/analysis; RQ2 risks and vulnerabilities of LLMs in cybersecurity; RQ3 effectiveness of LLMs for generating/detecting adversarial examples/evasive malware; RQ4 ethical and privacy concerns when using LLMs on sensitive cybersecurity data.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Prompt-based extraction: each article's title and abstract fed to an LLM prompt that (a) extracts topics/entities/keywords, (b) assigns relevance and contribution scores (0–1) per RQ, (c) produces binary relevance/contribution indicators using thresholds, and (d) emits natural-language reasoning explaining its judgments. Outputs are aggregated into per-article JSON and CSV files. No retrieval-augmented pipeline, chain-of-thought protocol, or full-text retrieval was reported; the process is intentionally simple and transparent to support human interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Per-article structured JSON (extracted semantics, relevance scores, binary decisions, reasoning) and a tabular CSV export containing the same fields for batch analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>{"title": "...", "extracted_topics": ["LLM", "adversarial ML"], "relevance_scores": {"RQ1": 0.82, "RQ2": 0.12, "RQ3": 0.45, "RQ4": 0.07}, "binary_relevance": true, "contribution_flag": false, "reasoning": "Paper presents an LLM-based detector evaluated on malware, matching RQ1 because..."}</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Uncontrolled, mixed qualitative and quantitative evaluation: (i) consistency of evaluations across papers, (ii) manual verification of relevance judgments and reasoning by researchers, and (iii) summary statistics across years and research questions. Multiple LLM backends were compared for behavior, speed, and cost.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Qualitative: LLMs produced coherent reasoning and useful topic extraction for human triage. Quantitative: processing throughput varied by model (per-article times: Llama3 ~10–11s, Gemma2 & GPT-3.5 ~12–14s, GPT-4o ~24–29s). Cost estimates: GPT-4o ≈ $3.16 per 100 articles, GPT-3.5 ≈ $0.22 per 100 articles; Gemma2 and Llama3 run locally (no cloud cost reported). On the large Scopus set (2,576 articles, Gemma2 run), 324 articles (12.6%) were labeled must-read and 100 (3.9%) labeled as contributing to RQs; RQ2 attracted the most relevant articles. The authors note the evaluation was preliminary and uncontrolled.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Scalable to thousands of titles/abstracts; supports multiple local and cloud LLM backends; produces structured, human-interpretable outputs (scores + textual reasoning); faster than manual screening; cost-effective options with local models; design favors transparency and extensibility.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Relies only on titles and abstracts (full text not used); depends on LLM quality and prompt formatting; per-model behavior varies requiring prompt tuning; some metadata (publication year, citation counts) were not utilized; evaluation was uncontrolled and not benchmarked against gold-standard systematic review outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Model-dependent inconsistencies (see Llama3 discrepancy between numeric scores and binary labels), potential for inaccurate relevance assignments or missing signals present only in full text, and risk of over-trusting LLM reasoning without human verification. Authors caution LLAssist for lightweight filtering only.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLAssist: Simple Tools for Automating Literature Review Using Large Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9634.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9634.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama 3:8B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 3 (8B parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A local LLM backend used in LLAssist runs; it provided the fastest per-article processing times but displayed inconsistencies between numeric relevance scores and binary relevance classifications.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 3:8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A local LLM (referred to as Llama 3) run via Ollama in the LLAssist experiments, used to extract semantics and produce relevance scores and reasoning from titles and abstracts. The paper does not provide architectural or training details beyond the model name.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Same article metadata corpus used by LLAssist: titles and abstracts from IEEE Xplore and Scopus (experiments included sets of 17, 37, 115, and 2,576 articles; Llama3 was tested in small and medium experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>The four cybersecurity + LLM research questions (RQ1–RQ4) described for LLAssist; Llama3 was used to assess relevance to those RQs.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Prompted extraction on article title+abstract: Llama3 produced topics/entities/keywords, numeric relevance scores per RQ (0–1), binary relevance/contribution flags, and textual reasoning. Threshold conversion from scores to binary labels was applied but found problematic for this model.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Per-article JSON/CSV with extracted topics, relevance scores, binary flags, and reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>{"title":"...","extracted_topics":["LLM","cybersecurity"],"relevance_scores":{"RQ1":0.15,...},"binary_relevance":false,"reasoning":"Abstract focuses on X, which is peripheral to RQ1..."}</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Comparison of score distributions and binary decisions against other LLM backends; manual review of reasoning coherence. The authors specifically inspected inconsistencies between scores and binary outputs and elected not to use Llama3's binary decisions for screening analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Fastest per-article times (~10–11s). Produced a conservative binary classifier (frequently marking articles as not relevant) while still outputting a diverse range of numeric relevance scores; inconsistency between numeric scores and binary labels led authors to exclude Llama3 binary outcomes from screening performance analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Fastest throughput among tested LLMs, can run locally (no cloud cost), suitable for high-rate screening tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Inconsistent mapping from continuous relevance scores to binary decisions; conservative in binary relevance labeling (potentially high false negatives); limited documentation in paper about model internals.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Mismatch between score distributions and binary labels (score-to-classification conversion issues) making binary decisions unreliable for screening; potential to discard borderline relevant papers due to conservative binary behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLAssist: Simple Tools for Automating Literature Review Using Large Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9634.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9634.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemma 2:9B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemma 2 (9B parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A local LLM backend used in LLAssist experiments that showed strong binary discrimination and sensitivity across research questions, used for large-scale screening of 2,576 Scopus articles in the reported evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemma 2:9B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A local model (Gemma 2) run via Ollama as an LLAssist backend; used to extract semantics and produce numeric relevance scores, binary relevance/contribution indicators, and textual reasoning from article titles and abstracts. Paper gives only the model name and size.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>9B</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Titles and abstracts export from Scopus and IEEE Xplore. Gemma 2 was used in the large dataset test on a Scopus set of 2,576 articles and in small/medium tests (17, 37, 115 articles).</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td>2576</td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Same four RQs targeting LLM applications and risks in cybersecurity (RQ1–RQ4). Gemma 2 was used to classify articles' relevance to these questions.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Prompt-based per-article extraction and scoring: Gemma 2 was prompted to output extracted topics/entities, a 0–1 relevance score per RQ, binary labels, and a natural-language reasoning statement. Outputs were aggregated per year and per RQ to reveal trends.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Per-article JSON/CSV containing extracted semantics, numeric relevance/contribution scores, binary flags (relevant/contributing/must-read), and textual reasoning; aggregated statistics by year and RQ.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>{"title":"...","extracted_topics":["adversarial attacks","LLM"],"relevance_scores":{"RQ1":0.88,"RQ2":0.65,...},"binary_relevance":true,"contributing":false,"reasoning":"Study evaluates LLM-based threat detection with experimental results, aligning with RQ1..."}</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Aggregated statistics (counts per year, per RQ), manual spot checks of reasoning quality, and cross-model comparisons of relevance distributions. Large-dataset trends (yearly growth and RQ distributions) were reported based on Gemma2 runs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>On the Scopus 2,576-article run (Gemma2): identified 869 potentially relevant articles in 2023 (117 must-read for that year); overall 324 must-read (12.6%) and 100 contributing (3.9%) across the large set. Gemma2 showed strong binary discrimination and sensitivity to topic differences. Processing speed ~12–14s/article and local execution reduces cloud costs.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Strong binary discrimination and sensitivity to RQ-specific topics, suitable for large-batch local screening; balanced throughput and no direct cloud cost when run locally.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Evaluations limited to titles/abstracts only; system-level metrics may be influenced by prompt design; lack of controlled benchmarking against human-screened gold standard.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not explicitly enumerated beyond general LLAssist limitations; potential false positives/negatives from abstract-only input, and possible sensitivity to prompt phrasing requiring tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLAssist: Simple Tools for Automating Literature Review Using Large Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9634.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9634.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-turbo-0125</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-3.5-turbo (snapshot 0125)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cloud-based OpenAI model used as an LLAssist backend; provided relatively fast and inexpensive processing with permissive relevance labeling compared to other tested models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo-0125</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A cloud-hosted conversational LLM from OpenAI (specific variant '0125' used). Employed by LLAssist via Microsoft.SemanticKernel to extract semantics, score relevance, and generate reasoning from titles and abstracts. No model parameter count given in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Titles and abstracts from IEEE Xplore and Scopus used in LLAssist experiments across small, medium, and large datasets (17, 37, 115, and 2,576 articles).</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>The LLAssist RQ set (RQ1–RQ4) concerning LLMs in cybersecurity; GPT-3.5 was prompted to evaluate each article against these RQs.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Cloud prompt-based processing of each article's title+abstract to extract topics/entities, assign numeric relevance/contribution scores for each RQ, emit binary labels, and provide textual reasoning. Outputs aggregated into JSON/CSV for human review.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Per-article JSON and CSV with extracted semantics, numeric relevance/contribution scores, binary flags, and reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>{"title":"...","extracted_topics":["malware detection","LLM"],"relevance_scores":{"RQ1":0.76,"RQ2":0.22,...},"binary_relevance":true,"reasoning":"Abstract reports LLM application to malware detection with evaluation, aligning to RQ1."}</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Cross-model behavior comparisons (score distributions, binary decisions), manual inspection of reasoning quality, and runtime/cost measurements.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>GPT-3.5 tended to be more permissive in binary relevance than GPT-4o and Gemma2, processed at ~12–14s/article, and estimated cost approximately $0.22 per 100 articles. Reasoning outputs were judged coherent in manual review, but no controlled accuracy benchmark was provided.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Low cost and reasonable speed for cloud-based processing; coherent reasoning useful for human triage; easy to integrate via SemanticKernel/OpenAI API.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Permissive relevance labeling may increase human workload during triage; cloud usage introduces privacy/cost considerations; limited to title/abstract inputs in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>No specific failure examples detailed aside from general risks (hallucination, misalignment with human judgment).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLAssist: Simple Tools for Automating Literature Review Using Large Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9634.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9634.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o-2024-05-13</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-4o (2024-05-13 snapshot)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cloud-based advanced OpenAI model used in LLAssist; produced more balanced, middle-ground relevance scores and was more selective in binary decisions, but was the slowest and most expensive option tested.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o-2024-05-13</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A high-capability cloud-hosted OpenAI LLM snapshot ('GPT-4o' variant dated 2024-05-13) used by LLAssist to extract semantics, score relevance per RQ, and provide textual reasoning. The paper does not provide parameter counts or training details.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Same LLAssist corpus of article metadata (titles/abstracts) from IEEE Xplore and Scopus; used across small and medium experiment runs and compared on large dataset behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Evaluated articles against LLAssist's four RQs addressing LLM use and risks in cybersecurity (RQ1–RQ4).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Prompt-based per-article evaluation using titles/abstracts; GPT-4o returned extracted topics, numeric relevance scores (0–1) per RQ, binary flags, and textual reasoning. Outputs were exported to JSON/CSV.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Per-article JSON and CSV with extracted semantics, numeric relevance/contribution scores, binary labels, and reasoning statements.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>{"title":"...","extracted_topics":["model inversion","privacy"],"relevance_scores":{"RQ1":0.34,"RQ2":0.88,...},"binary_relevance":true,"reasoning":"Paper explores privacy risks of LLMs in cybersecurity systems, aligning strongly with RQ2."}</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Model comparisons on distribution of relevance scores and binary decisions, manual review of reasoning, timing and cost profiling.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>GPT-4o produced a more balanced distribution between relevant and non-relevant articles, tended to avoid extreme judgments (middle-ground scores), and was slightly more selective than Gemma2 but more permissive than Llama3. It was the slowest (~24–29s/article) and most expensive (≈ $3.16 per 100 articles).</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Balanced, selective judgments with coherent reasoning; may reduce false positives by being more conservative in borderline cases.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Higher cost and slower throughput; tendency to produce middle-ground scores that may require additional thresholding or human interpretation; cloud privacy/cost trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Potential avoidance of decisive judgments leading to many borderline articles requiring human review; cost and latency limit scalability for very large corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLAssist: Simple Tools for Automating Literature Review Using Large Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Cutting through the clutter: The potential of llms for efficient filtration in systematic literature reviews <em>(Rating: 2)</em></li>
                <li>Litllm: A toolkit for scientific literature review <em>(Rating: 2)</em></li>
                <li>Prisma-dfllm: An extension of prisma for systematic literature reviews using domain-specific finetuned large language models <em>(Rating: 2)</em></li>
                <li>A roadmap toward the automatic composition of systematic literature reviews <em>(Rating: 1)</em></li>
                <li>Automated screening of research studies for systematic reviews using study characteristics <em>(Rating: 1)</em></li>
                <li>Machine learning algorithms for systematic review: reducing workload in a preclinical review of animal studies and reducing human screening error <em>(Rating: 1)</em></li>
                <li>Semi-automated screening of biomedical citations for systematic reviews <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9634",
    "paper_id": "paper-271310513",
    "extraction_schema_id": "extraction-schema-167",
    "extracted_data": [
        {
            "name_short": "LLAssist",
            "name_full": "LLAssist (Literature Review Assistant)",
            "brief_description": "An open-source C# tool that automates initial literature review tasks by using LLM backends to extract semantics from titles/abstracts, score relevance to user-defined research questions, and output per-article JSON/CSV with reasoning; intended as a lightweight human-in-the-loop screening aid.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLAssist (pipeline)",
            "model_description": "A pipeline/tool that calls configurable LLM backends (local or cloud) to perform: (1) key semantics extraction from title+abstract, (2) per-research-question relevance scoring (0–1) and binary decisions, and (3) natural-language reasoning for each decision. Implemented in C# using Microsoft.SemanticKernel for OpenAI-compatible API calls, CsvHelper for IO, and supports local Ollama models or OpenAI cloud models.",
            "model_size": null,
            "input_corpus_description": "Manually sampled article metadata (title, abstract, authors, venue, keywords) exported from IEEE Xplore and Scopus using search queries targeting LLMs and cybersecurity. Experiments included a small test (17, 37, 115 articles) and a large test (Scopus subset of 2,576 articles).",
            "input_corpus_size": 2576,
            "topic_query_description": "Four research questions guiding the screening: RQ1 LLMs for threat detection/analysis; RQ2 risks and vulnerabilities of LLMs in cybersecurity; RQ3 effectiveness of LLMs for generating/detecting adversarial examples/evasive malware; RQ4 ethical and privacy concerns when using LLMs on sensitive cybersecurity data.",
            "distillation_method": "Prompt-based extraction: each article's title and abstract fed to an LLM prompt that (a) extracts topics/entities/keywords, (b) assigns relevance and contribution scores (0–1) per RQ, (c) produces binary relevance/contribution indicators using thresholds, and (d) emits natural-language reasoning explaining its judgments. Outputs are aggregated into per-article JSON and CSV files. No retrieval-augmented pipeline, chain-of-thought protocol, or full-text retrieval was reported; the process is intentionally simple and transparent to support human interpretability.",
            "output_type": "Per-article structured JSON (extracted semantics, relevance scores, binary decisions, reasoning) and a tabular CSV export containing the same fields for batch analysis.",
            "output_example": "{\"title\": \"...\", \"extracted_topics\": [\"LLM\", \"adversarial ML\"], \"relevance_scores\": {\"RQ1\": 0.82, \"RQ2\": 0.12, \"RQ3\": 0.45, \"RQ4\": 0.07}, \"binary_relevance\": true, \"contribution_flag\": false, \"reasoning\": \"Paper presents an LLM-based detector evaluated on malware, matching RQ1 because...\"}",
            "evaluation_method": "Uncontrolled, mixed qualitative and quantitative evaluation: (i) consistency of evaluations across papers, (ii) manual verification of relevance judgments and reasoning by researchers, and (iii) summary statistics across years and research questions. Multiple LLM backends were compared for behavior, speed, and cost.",
            "evaluation_results": "Qualitative: LLMs produced coherent reasoning and useful topic extraction for human triage. Quantitative: processing throughput varied by model (per-article times: Llama3 ~10–11s, Gemma2 & GPT-3.5 ~12–14s, GPT-4o ~24–29s). Cost estimates: GPT-4o ≈ $3.16 per 100 articles, GPT-3.5 ≈ $0.22 per 100 articles; Gemma2 and Llama3 run locally (no cloud cost reported). On the large Scopus set (2,576 articles, Gemma2 run), 324 articles (12.6%) were labeled must-read and 100 (3.9%) labeled as contributing to RQs; RQ2 attracted the most relevant articles. The authors note the evaluation was preliminary and uncontrolled.",
            "strengths": "Scalable to thousands of titles/abstracts; supports multiple local and cloud LLM backends; produces structured, human-interpretable outputs (scores + textual reasoning); faster than manual screening; cost-effective options with local models; design favors transparency and extensibility.",
            "limitations": "Relies only on titles and abstracts (full text not used); depends on LLM quality and prompt formatting; per-model behavior varies requiring prompt tuning; some metadata (publication year, citation counts) were not utilized; evaluation was uncontrolled and not benchmarked against gold-standard systematic review outcomes.",
            "failure_cases": "Model-dependent inconsistencies (see Llama3 discrepancy between numeric scores and binary labels), potential for inaccurate relevance assignments or missing signals present only in full text, and risk of over-trusting LLM reasoning without human verification. Authors caution LLAssist for lightweight filtering only.",
            "uuid": "e9634.0",
            "source_info": {
                "paper_title": "LLAssist: Simple Tools for Automating Literature Review Using Large Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Llama 3:8B",
            "name_full": "Llama 3 (8B parameters)",
            "brief_description": "A local LLM backend used in LLAssist runs; it provided the fastest per-article processing times but displayed inconsistencies between numeric relevance scores and binary relevance classifications.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama 3:8B",
            "model_description": "A local LLM (referred to as Llama 3) run via Ollama in the LLAssist experiments, used to extract semantics and produce relevance scores and reasoning from titles and abstracts. The paper does not provide architectural or training details beyond the model name.",
            "model_size": "8B",
            "input_corpus_description": "Same article metadata corpus used by LLAssist: titles and abstracts from IEEE Xplore and Scopus (experiments included sets of 17, 37, 115, and 2,576 articles; Llama3 was tested in small and medium experiments).",
            "input_corpus_size": null,
            "topic_query_description": "The four cybersecurity + LLM research questions (RQ1–RQ4) described for LLAssist; Llama3 was used to assess relevance to those RQs.",
            "distillation_method": "Prompted extraction on article title+abstract: Llama3 produced topics/entities/keywords, numeric relevance scores per RQ (0–1), binary relevance/contribution flags, and textual reasoning. Threshold conversion from scores to binary labels was applied but found problematic for this model.",
            "output_type": "Per-article JSON/CSV with extracted topics, relevance scores, binary flags, and reasoning.",
            "output_example": "{\"title\":\"...\",\"extracted_topics\":[\"LLM\",\"cybersecurity\"],\"relevance_scores\":{\"RQ1\":0.15,...},\"binary_relevance\":false,\"reasoning\":\"Abstract focuses on X, which is peripheral to RQ1...\"}",
            "evaluation_method": "Comparison of score distributions and binary decisions against other LLM backends; manual review of reasoning coherence. The authors specifically inspected inconsistencies between scores and binary outputs and elected not to use Llama3's binary decisions for screening analysis.",
            "evaluation_results": "Fastest per-article times (~10–11s). Produced a conservative binary classifier (frequently marking articles as not relevant) while still outputting a diverse range of numeric relevance scores; inconsistency between numeric scores and binary labels led authors to exclude Llama3 binary outcomes from screening performance analysis.",
            "strengths": "Fastest throughput among tested LLMs, can run locally (no cloud cost), suitable for high-rate screening tasks.",
            "limitations": "Inconsistent mapping from continuous relevance scores to binary decisions; conservative in binary relevance labeling (potentially high false negatives); limited documentation in paper about model internals.",
            "failure_cases": "Mismatch between score distributions and binary labels (score-to-classification conversion issues) making binary decisions unreliable for screening; potential to discard borderline relevant papers due to conservative binary behavior.",
            "uuid": "e9634.1",
            "source_info": {
                "paper_title": "LLAssist: Simple Tools for Automating Literature Review Using Large Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Gemma 2:9B",
            "name_full": "Gemma 2 (9B parameters)",
            "brief_description": "A local LLM backend used in LLAssist experiments that showed strong binary discrimination and sensitivity across research questions, used for large-scale screening of 2,576 Scopus articles in the reported evaluation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Gemma 2:9B",
            "model_description": "A local model (Gemma 2) run via Ollama as an LLAssist backend; used to extract semantics and produce numeric relevance scores, binary relevance/contribution indicators, and textual reasoning from article titles and abstracts. Paper gives only the model name and size.",
            "model_size": "9B",
            "input_corpus_description": "Titles and abstracts export from Scopus and IEEE Xplore. Gemma 2 was used in the large dataset test on a Scopus set of 2,576 articles and in small/medium tests (17, 37, 115 articles).",
            "input_corpus_size": 2576,
            "topic_query_description": "Same four RQs targeting LLM applications and risks in cybersecurity (RQ1–RQ4). Gemma 2 was used to classify articles' relevance to these questions.",
            "distillation_method": "Prompt-based per-article extraction and scoring: Gemma 2 was prompted to output extracted topics/entities, a 0–1 relevance score per RQ, binary labels, and a natural-language reasoning statement. Outputs were aggregated per year and per RQ to reveal trends.",
            "output_type": "Per-article JSON/CSV containing extracted semantics, numeric relevance/contribution scores, binary flags (relevant/contributing/must-read), and textual reasoning; aggregated statistics by year and RQ.",
            "output_example": "{\"title\":\"...\",\"extracted_topics\":[\"adversarial attacks\",\"LLM\"],\"relevance_scores\":{\"RQ1\":0.88,\"RQ2\":0.65,...},\"binary_relevance\":true,\"contributing\":false,\"reasoning\":\"Study evaluates LLM-based threat detection with experimental results, aligning with RQ1...\"}",
            "evaluation_method": "Aggregated statistics (counts per year, per RQ), manual spot checks of reasoning quality, and cross-model comparisons of relevance distributions. Large-dataset trends (yearly growth and RQ distributions) were reported based on Gemma2 runs.",
            "evaluation_results": "On the Scopus 2,576-article run (Gemma2): identified 869 potentially relevant articles in 2023 (117 must-read for that year); overall 324 must-read (12.6%) and 100 contributing (3.9%) across the large set. Gemma2 showed strong binary discrimination and sensitivity to topic differences. Processing speed ~12–14s/article and local execution reduces cloud costs.",
            "strengths": "Strong binary discrimination and sensitivity to RQ-specific topics, suitable for large-batch local screening; balanced throughput and no direct cloud cost when run locally.",
            "limitations": "Evaluations limited to titles/abstracts only; system-level metrics may be influenced by prompt design; lack of controlled benchmarking against human-screened gold standard.",
            "failure_cases": "Not explicitly enumerated beyond general LLAssist limitations; potential false positives/negatives from abstract-only input, and possible sensitivity to prompt phrasing requiring tuning.",
            "uuid": "e9634.2",
            "source_info": {
                "paper_title": "LLAssist: Simple Tools for Automating Literature Review Using Large Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "GPT-3.5-turbo-0125",
            "name_full": "OpenAI GPT-3.5-turbo (snapshot 0125)",
            "brief_description": "A cloud-based OpenAI model used as an LLAssist backend; provided relatively fast and inexpensive processing with permissive relevance labeling compared to other tested models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo-0125",
            "model_description": "A cloud-hosted conversational LLM from OpenAI (specific variant '0125' used). Employed by LLAssist via Microsoft.SemanticKernel to extract semantics, score relevance, and generate reasoning from titles and abstracts. No model parameter count given in the paper.",
            "model_size": null,
            "input_corpus_description": "Titles and abstracts from IEEE Xplore and Scopus used in LLAssist experiments across small, medium, and large datasets (17, 37, 115, and 2,576 articles).",
            "input_corpus_size": null,
            "topic_query_description": "The LLAssist RQ set (RQ1–RQ4) concerning LLMs in cybersecurity; GPT-3.5 was prompted to evaluate each article against these RQs.",
            "distillation_method": "Cloud prompt-based processing of each article's title+abstract to extract topics/entities, assign numeric relevance/contribution scores for each RQ, emit binary labels, and provide textual reasoning. Outputs aggregated into JSON/CSV for human review.",
            "output_type": "Per-article JSON and CSV with extracted semantics, numeric relevance/contribution scores, binary flags, and reasoning.",
            "output_example": "{\"title\":\"...\",\"extracted_topics\":[\"malware detection\",\"LLM\"],\"relevance_scores\":{\"RQ1\":0.76,\"RQ2\":0.22,...},\"binary_relevance\":true,\"reasoning\":\"Abstract reports LLM application to malware detection with evaluation, aligning to RQ1.\"}",
            "evaluation_method": "Cross-model behavior comparisons (score distributions, binary decisions), manual inspection of reasoning quality, and runtime/cost measurements.",
            "evaluation_results": "GPT-3.5 tended to be more permissive in binary relevance than GPT-4o and Gemma2, processed at ~12–14s/article, and estimated cost approximately $0.22 per 100 articles. Reasoning outputs were judged coherent in manual review, but no controlled accuracy benchmark was provided.",
            "strengths": "Low cost and reasonable speed for cloud-based processing; coherent reasoning useful for human triage; easy to integrate via SemanticKernel/OpenAI API.",
            "limitations": "Permissive relevance labeling may increase human workload during triage; cloud usage introduces privacy/cost considerations; limited to title/abstract inputs in this study.",
            "failure_cases": "No specific failure examples detailed aside from general risks (hallucination, misalignment with human judgment).",
            "uuid": "e9634.3",
            "source_info": {
                "paper_title": "LLAssist: Simple Tools for Automating Literature Review Using Large Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "GPT-4o-2024-05-13",
            "name_full": "OpenAI GPT-4o (2024-05-13 snapshot)",
            "brief_description": "A cloud-based advanced OpenAI model used in LLAssist; produced more balanced, middle-ground relevance scores and was more selective in binary decisions, but was the slowest and most expensive option tested.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o-2024-05-13",
            "model_description": "A high-capability cloud-hosted OpenAI LLM snapshot ('GPT-4o' variant dated 2024-05-13) used by LLAssist to extract semantics, score relevance per RQ, and provide textual reasoning. The paper does not provide parameter counts or training details.",
            "model_size": null,
            "input_corpus_description": "Same LLAssist corpus of article metadata (titles/abstracts) from IEEE Xplore and Scopus; used across small and medium experiment runs and compared on large dataset behavior.",
            "input_corpus_size": null,
            "topic_query_description": "Evaluated articles against LLAssist's four RQs addressing LLM use and risks in cybersecurity (RQ1–RQ4).",
            "distillation_method": "Prompt-based per-article evaluation using titles/abstracts; GPT-4o returned extracted topics, numeric relevance scores (0–1) per RQ, binary flags, and textual reasoning. Outputs were exported to JSON/CSV.",
            "output_type": "Per-article JSON and CSV with extracted semantics, numeric relevance/contribution scores, binary labels, and reasoning statements.",
            "output_example": "{\"title\":\"...\",\"extracted_topics\":[\"model inversion\",\"privacy\"],\"relevance_scores\":{\"RQ1\":0.34,\"RQ2\":0.88,...},\"binary_relevance\":true,\"reasoning\":\"Paper explores privacy risks of LLMs in cybersecurity systems, aligning strongly with RQ2.\"}",
            "evaluation_method": "Model comparisons on distribution of relevance scores and binary decisions, manual review of reasoning, timing and cost profiling.",
            "evaluation_results": "GPT-4o produced a more balanced distribution between relevant and non-relevant articles, tended to avoid extreme judgments (middle-ground scores), and was slightly more selective than Gemma2 but more permissive than Llama3. It was the slowest (~24–29s/article) and most expensive (≈ $3.16 per 100 articles).",
            "strengths": "Balanced, selective judgments with coherent reasoning; may reduce false positives by being more conservative in borderline cases.",
            "limitations": "Higher cost and slower throughput; tendency to produce middle-ground scores that may require additional thresholding or human interpretation; cloud privacy/cost trade-offs.",
            "failure_cases": "Potential avoidance of decisive judgments leading to many borderline articles requiring human review; cost and latency limit scalability for very large corpora.",
            "uuid": "e9634.4",
            "source_info": {
                "paper_title": "LLAssist: Simple Tools for Automating Literature Review Using Large Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Cutting through the clutter: The potential of llms for efficient filtration in systematic literature reviews",
            "rating": 2,
            "sanitized_title": "cutting_through_the_clutter_the_potential_of_llms_for_efficient_filtration_in_systematic_literature_reviews"
        },
        {
            "paper_title": "Litllm: A toolkit for scientific literature review",
            "rating": 2,
            "sanitized_title": "litllm_a_toolkit_for_scientific_literature_review"
        },
        {
            "paper_title": "Prisma-dfllm: An extension of prisma for systematic literature reviews using domain-specific finetuned large language models",
            "rating": 2,
            "sanitized_title": "prismadfllm_an_extension_of_prisma_for_systematic_literature_reviews_using_domainspecific_finetuned_large_language_models"
        },
        {
            "paper_title": "A roadmap toward the automatic composition of systematic literature reviews",
            "rating": 1,
            "sanitized_title": "a_roadmap_toward_the_automatic_composition_of_systematic_literature_reviews"
        },
        {
            "paper_title": "Automated screening of research studies for systematic reviews using study characteristics",
            "rating": 1,
            "sanitized_title": "automated_screening_of_research_studies_for_systematic_reviews_using_study_characteristics"
        },
        {
            "paper_title": "Machine learning algorithms for systematic review: reducing workload in a preclinical review of animal studies and reducing human screening error",
            "rating": 1,
            "sanitized_title": "machine_learning_algorithms_for_systematic_review_reducing_workload_in_a_preclinical_review_of_animal_studies_and_reducing_human_screening_error"
        },
        {
            "paper_title": "Semi-automated screening of biomedical citations for systematic reviews",
            "rating": 1,
            "sanitized_title": "semiautomated_screening_of_biomedical_citations_for_systematic_reviews"
        }
    ],
    "cost": 0.0135725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LLAssist: Simple Tools for Automating Literature Review Using Large Language Models
19 Jul 2024</p>
<p>Yoga Christoforus 
School of Science
RMIT University
3000MelbourneVICAustralia</p>
<p>Haryanto 
School of Science
RMIT University
3000MelbourneVICAustralia</p>
<p>LLAssist: Simple Tools for Automating Literature Review Using Large Language Models
19 Jul 20244CFC85AC35AA3FA91766ECACA58EC4A8arXiv:2407.13993v1[cs.DL]Large Language ModelsArtificial IntelligenceResearch ToolsSemantic AnalysisAutomated Document Processing
This paper introducesLLAssist, an open-source tool designed to streamline literature reviews in academic research.In an era of exponential growth in scientific publications, researchers face mounting challenges in efficiently processing vast volumes of literature.LLAssist addresses this issue by leveraging Large Language Models (LLMs) and Natural Language Processing (NLP) techniques to automate key aspects of the review process.Specifically, it extracts important information from research articles and evaluates their relevance to user-defined research questions.The goal of LLAssist is to significantly reduce the time and effort required for comprehensive literature reviews, allowing researchers to focus more on analyzing and synthesizing information rather than on initial screening tasks.By automating parts of the literature review workflow, LLAssist aims to help researchers manage the growing volume of academic publications more efficiently.</p>
<p>INTRODUCTION</p>
<p>The landscape of academic research is undergoing a dramatic transformation, driven by an unprecedented surge in scientific publications.Identifying relevant work, once a manageable task, has evolved into a time-consuming and often overwhelming process [Khan et al., 2003, Davis et al., 2014].The exponential growth of scientific publications [Silva Júnior andDutra, 2021, Ioannidis et al., 2021], the need to screen hundreds or thousands of articles, and pressure for rapid-evidence gathering present significant challenges for researchers, potentially compromising research quality [McDermott et al., 2024, Glasziou et al., 2020, Whiting et al., 2016, Page et al., 2021, Wallace et al., 2010].</p>
<p>In response to these mounting challenges, the research community has begun exploring automated solutions to assist in the systematic literature review process [Silva Júnior and Dutra, 2021, Wallace et al., 2010, Tsafnat et al., 2018, Bannach-Brown et al., 2019].Recently, there has been a growing interest in evaluating the potential of Large Language Models (LLMs) for this purpose [Agarwal et al., 2024, Joos et al., 2024, Susnjak, 2023].These advanced AI systems, with their ability to understand and generate human-like text, offer promising avenues for automating various aspects of the literature review process.</p>
<p>A notable contribution to this emerging field comes from Joos et al. [2024], who recently published an extensive evaluation of using LLMs in enhancing the screening process, with results indicating promising potential for reducing human workload.Inspired by these findings, we present LLAssist, a prototype automation tool based on LLM technology.In its current phase, LLAssist is designed as a lightweight and straightforward solution to efficiently process large volumes of articles, extract key information, and assess relevance to specific research questions, thereby streamlining the literature review process.Eventually, we aim LLAssist to be one of the key components for a human-friendly, automated knowledge base and research platform.</p>
<p>By using a predetermined process of key semantics extraction, relevance estimation, and must-read determination, LLAssist provides a simple building block for researchers to expand from the idea.This design choice allows for better adaptability to different research domains and provides researchers with more transparent and interpretable results.It is important to note that while LLAssist shows promise in its current form, it represents an initial step towards more sophisticated literature review automation.The simplicity of its design is intentional, allowing researchers to understand, modify, and build upon its core functionalities as needed for their specific research contexts.</p>
<p>Output Generation LLAssist provides two types of output: 1. a JSON file containing detailed information for each processed article, including extracted semantics, relevance scores, and reasoning, and 2. a CSV file presenting the same information in a tabular format, suitable for further analysis or import into other tools.Necessitating other analysis and tools is deliberate to ensure that the human-in-the-loop principle is adhered to by maintaining the visibility of the process [Buçinca et al., 2021, Vasconcelos et al., 2023, Bansal et al., 2021].</p>
<p>Experimental Evaluation</p>
<p>Data Collection</p>
<p>To evaluate the effectiveness of LLAssist in streamlining the literature review process for LLM applications in cybersecurity, we conducted two separate experiments using manual sampling of publications from datasets from different sources:</p>
<ol>
<li>
<p>IEEE Xplore Database: using the search query "llm AND cyber AND security" (term a), effectively focusing on recent publications for initial system testing.</p>
</li>
<li>
<p>Scopus Database: using the search queries "llm AND cyber AND security" (term b), "llm OR ( generative AND artificial AND intelligence ) AND cyber AND security" (term c), and a broad "artificial AND intelligence AND cyber AND security" -limited to Conference paper and Article -(term d) to obtain a diverse dataset in a consecutively larger sample size.</p>
</li>
</ol>
<p>The experiments were designed to assess LLAssist's performance across different academic databases and to ensure a comprehensive evaluation of its capabilities in the fields already known by the authors.There will be two parts: a small dataset test and a large dataset test.The small dataset test is to help manually verify the functionality of LLAssist and the large dataset test is to confirm its utility in enhancing the screening in structured literature review.</p>
<p>From the IEEE Xplore dataset, we exported 17 relevant articles (term a).The Scopus dataset provided an additional set of 37 (term b), 115 (term c), and 2,576 (term d) articles, expanding our corpus.Each dataset included metadata such as titles, abstracts, authors, publication venues, and keywords.The data was exported using the CSV export function to ensure consistency and compatibility with LLAssist's input requirements.</p>
<p>Research Questions</p>
<p>We formulated four key research questions to guide our automated analysis:</p>
<p>• RQ1: How are Large Language Models (LLMs) being utilized to enhance threat detection and analysis in cybersecurity applications?</p>
<p>• RQ2: What are the potential risks and vulnerabilities introduced by integrating LLMs into cybersecurity tools and frameworks?</p>
<p>• RQ3: How effective are LLM-based approaches in generating and detecting adversarial examples or evasive malware compared to traditional methods?</p>
<p>• RQ4: What ethical considerations and privacy concerns arise from using LLMs to analyze and process sensitive cybersecurity data?</p>
<p>Automated Analysis</p>
<p>We processed each paper through LLAssist, which performed the following tasks: 1. Extract key semantics (topics, entities, and keywords) from the title and abstract.2. Evaluate the relevance of each paper to our research questions.3. Provide relevance and contribution scores (0-1 scale) for each research question.4. Generate reasoning for relevance and contribution assessments.</p>
<p>Evaluation Metrics</p>
<p>We assessed the performance of LLAssist based on i) consistency of evaluations across papers, ii) accuracy in matching papers to relevant research questions, and iii) ability to provide meaningful insights and reasoning.Additionally, we are using several different LLM backends: Llama 3:8B, Gemma 2:9B, GPT-3.5-turbo-0125, and GPT-4o-2024-05-13 to allow data comparison.</p>
<p>Preliminary Nature of Evaluation</p>
<p>It is important to note that the assessment of accuracy in matching papers to relevant research questions and the ability to provide meaningful insights and reasoning was conducted in an uncontrolled environment.Knowing that LLM results can be helpful yet inaccurate, we expect the researchers to use LLAssist as a lightweight filtering enhancement tool while following existing methodologies such as PRISMA [Page et al., 2021, Susnjak, 2023].</p>
<p>TECHNICAL IMPLEMENTATION</p>
<p>The program is designed to work with various LLM providers, including local models (e.g., Ollama Llama 3, Ollama Gemma 2) and cloud-based models (e.g., OpenAI's GPT-3.5 and GPT-4).This flexibility allows researchers to choose models based on their specific requirements, such as processing speed, accuracy, or data privacy concerns LLAssist is implemented in C# and utilizes the following key components: 1. Microsoft.SemanticKernel: For interaction with LLM using OpenAI-compatible API, 2. CsvHelper: For reading input and writing output CSV files, and 3. Microsoft.Extensions.Logging: For logging and debugging purposes.</p>
<p>Unlike many existing research tools, LLAssist is developed in C#, a static and strongly typed language, offering benefits such as improved performance, early error detection, and enhanced maintainability [Nanz and Furia, 2015], suitable for further integration into a bigger enterprise system.Additionally, the system will write the CSV output file incrementally to ensure better durability in case of a crash.The sequence diagram can be seen in Figure 1.</p>
<p>Small Dataset Test</p>
<p>The small dataset verified the functionality of the system with the following key results:</p>
<p>Key Semantics Extraction</p>
<p>LLAssist successfully identified relevant topics, entities, and keywords for each paper, aligning well with the author-provided keywords and terms for all LLM backends.As this data is currently used as the input to the LLM prompt, there is no controlled measurement done in this experiment.Result data can be reviewed in the CSV.Also, note that the metadata for the classifier is generated by the same LLM.</p>
<p>Binary Relevance Decision and Score Distribution</p>
<p>The binary relevance decision and relevance score distribution obtained experiment are shown in Figure 2. Further, the summary of binary relevance and binary contribution decisions are shown in Table 1.  3. GPT-4o-2024-05-13 demonstrates a more balanced distribution between relevant and non-relevant articles.It appears more selective than GPT-3.5 and slightly more selective than Gemma 2, yet more permissive than Llama3 in binary classifications.GPT-4o also tends to return a middle-ground score: while it may imply a sophisticated evaluation, it may also indicate avoiding judgments.</p>
<ol>
<li>Llama 3:8B exhibits a significant discrepancy between its relevance scores and binary classifications.In binary classification, it's the most conservative and frequently marking articles as not relevant.However, its relevance scores show more diversity, with a range of values that don't always align with its binary decisions.This inconsistency suggests potential issues in threshold setting or score-to-classification conversion for this model, hence we decided to not use the binary relevance decisions from Llama 3 as the basis for the literature screening performance analysis.</li>
</ol>
<p>Must-read vs. Discard Ratio</p>
<p>All small dataset test runs show a relatively low discard ratio, which is expected due to the specificity of the search term, broad research questions, and the low dataset diversity.A comparison between each test run can be seen in Figure 3.</p>
<p>Reasoning Quality</p>
<p>A manual review of the system provided coherent explanations for its relevance and contribution assessments, offering insights into why each paper was or wasn't considered relevant to each research question.There is no controlled experiment done to measure quantitatively.The LLMs have to output their reasoning to help the researchers in manually discriminating the articles [Vasconcelos et al., 2023] and can be also part of cognitive forcing functions to be the checkpoint before downstream processing [Buçinca et al., 2021].</p>
<p>Large Dataset Test</p>
<p>Table 1 shows two types of binary decisions made by LLAssist: the binary relevance and the binary contribution indicator.Referring to the row id: SL-Gemma2 which contains the result of running large dataset test using Gemma 2:9B, the analysis of the larger Scopus dataset (2,576 articles) revealed key insights:</p>
<ol>
<li>
<p>Trend in Relevance: There's a notable increase in potentially relevant articles from 2020 to 2023, with a peak in 2023 (869 articles, 117 must-read).The slight decrease in 2024 reflects the mid-year data collection cut-off rather than a decline in research quality.Additionally, identifying the most relevant articles accurately is more important than finding a large number of potentially relevant articles.3. Must-Read vs. Contributing Articles: While 324 articles (12.6%) are identified as must-read, only 100 articles (3.9%) are classified as potentially contributing.This suggests that LLAssist are more selective in identifying articles that directly contribute to answering the research questions, implying reduced time to manually read the abstract.</p>
</li>
<li>
<p>Year-over-Year Growth: The number of potentially relevant articles increased significantly from 2020 to 2023, indicating growing research interest in the field.2023 stands out as a pivotal year with the highest numbers of potentially high-quality and relevant publications across all categories.</p>
</li>
<li>
<p>Research Question Focus: RQ2 consistently receives the most attention, suggesting that potential risks and vulnerabilities of LLMs in cybersecurity are a primary concern in the field.RQ4, focusing on ethical considerations, shows the least but growing number of relevant articles, particularly from 2022 onward.</p>
</li>
</ol>
<p>ANALYSIS AND DISCUSSION</p>
<p>Overall Performance</p>
<p>LLAssist effectively identifies relevant papers, works with various LLM backends, and significantly reduces manual screening time.On the other hand, the system did not utilize all available metadata (e.g., publication year, citation counts) in its relevance assessments, which could have provided additional context.Also, different LLMs behave differently, necessitating more precise prompt tuning.The analysis was also limited to titles and abstracts, potentially missing relevant information contained in the full text of the papers.</p>
<p>Time and Cost Efficiency</p>
<p>LLAssist's throughput varies across models and dataset sizes.It processes datasets of 17-37 articles in under 10 minutes, 115 articles in 20-50 minutes, and 2,576 articles in 10-11 hours.Among the models tested, GPT4o emerges as the slowest, processing articles in 24-29 seconds on average.Llama3 is the fastest, consistently quick at 10-11 seconds per article.Gemma2 and GPT35 offer similar speeds, averaging 12-14 seconds for each article processed.The per-article processing times remain consistent across dataset sizes, indicating scalability.This is a significant improvement over human performance [Wallace et al., 2010, Joos et al., 2024].</p>
<p>Cost-wise, GPT-4o is the most expensive at approximately $3.16 per 100 articles while GPT-3.5 offers a more budget-friendly option at about $0.22 per 100 articles.Meanwhile, both Gemma 2 and Llama 3 do not have a set cost due to the ability to run locally without cloud services.Notably, the high discrimination ability of Gemma 2 may help researchers to do the initial screening of many articles without relying on cloud services.</p>
<p>FUTURE WORK</p>
<p>LLAssist's limitations include dependence on LLM quality and input formatting, focus on titles and abstracts, and potential misalignment with human judgment.Future work should aims to incorporate full-text analysis, implement feedback mechanisms, and develop domain-specific models for improved accuracy.</p>
<p>CONCLUSION</p>
<p>In conclusion, LLAssist demonstrated promising capabilities in automating the initial stages of a literature review.Its ability to quickly process and categorize papers offers valuable support to researchers.However, there is room for improvement in utilizing more of the available metadata and fine-tuning the relevance criteria to better differentiate between highly relevant and marginally relevant papers.By leveraging Large Language Models, it offers researchers a valuable tool to efficiently process large volumes of academic literature.While not a replacement for human judgment, LLAssist can significantly reduce the time spent on initial screening and help researchers focus their efforts on the most promising and relevant articles for their research questions, allowing researchers to focus on high-quality work, to achieve higher productivity across expertise and industry.</p>
<p>AVAILABILITY</p>
<p>The source code for LLAssist is freely available at https://github.com/cyharyanto/llassist.We encourage researchers to use, modify, and contribute to this tool to further advance the efficiency of academic literature reviews across various disciplines.</p>
<p>ACKNOWLEDGMENT</p>
<p>We would like to express our deep gratitude to Dr. Arathi Arakala, Dr. Argho Bandyopadhyay, and Dr. Jessica Helmi from RMIT University for their invaluable guidance in systematic literature review methodologies.Their insights into the challenges faced during the screening of papers for surveys and systematic literature reviews shaped the development and focus of LLAssist.We are thankful for their support and shared knowledge.</p>
<p>Figure 1 :
1
Figure 1: Sequence diagram of LLAssist console application</p>
<p>Figure 2 :
2
Figure 2: Binary Relevance and Relevance Store Distribution (small datasets)</p>
<p>1.</p>
<p>Gemma 2:9B shows a reasonable distribution of binary relevance classification and relevance score.It tends to give a strong binary decision and classification score compared to all other LLMs with variation across research questions, indicating sensitivity to different topics.</p>
<p>Figure 3 :
3
Figure 3: Must-read vs. Discard ratio (all datasets)</p>
<p>2.</p>
<p>Research Question Specifics: RQ2 (risks and vulnerabilities of LLMs in cybersecurity) consistently has the highest number of relevant and contributing articles across years.It indicates it's likely the most well-defined or central question.RQ1 (LLMs for threat detection) shows a sharp increase in relevance from 2022 to 2023.RQ3 (LLMs for adversarial examples/evasive malware) and RQ4 (ethical considerations) have fewer articles but show an upward trend, indicating more specialized areas of increasing importance.</p>
<p>Table 1 :
1
Binary Relevance and Binary Contribution Decisions (all datasets)
N Relevant ArticlesN Contributing ArticlesN Total R Any RQ1 RQ2 RQ3 RQ4 C Any RQ1 RQ2 RQ3 RQ4IES-Gemma2171791319168913IES-GPT351717161716171716171617IES-GPT4o171511822139611IES-Llama3171095101110402SS-Gemma23733252671230181925SS-GPT353737373733373737373337SS-GPT4o372820167324151152SS-Llama337231996127231244SM-Gemma21156545502423462933511SM-GPT35115115111114109114115111114109114SM-GPT4o1155736292311432424116SM-Llama3115513319191634729128SL-Gemma2<em>Total2576324153201110951002775162620203013616211410404102021380271115127816402022571521737151616015072023869117606944304215306102024455924959253230112059SL-Llama3257653638712318277916122319748</em> SL-Gemma2 data is broken down by year</p>
<p>Litllm: A toolkit for scientific literature review. Shubham Agarwal, Issam H Laradji, Laurent Charlin, Christopher Pal, 10.48550/ARXIV.2402.017882024</p>
<p>Machine learning algorithms for systematic review: reducing workload in a preclinical review of animal studies and reducing human screening error. Alexandra Bannach-Brown, Piotr Przyby La, James Thomas, Andrew S C Rice, Sophia Ananiadou, Jing Liao, Malcolm Robert Macleod, 10.1186/s13643-019-0942-7Systematic Reviews. 2046-4053812312 2019Online; accessed 2024-07-17</p>
<p>Does the whole exceed its parts? the effect of ai explanations on complementary team performance. Gagan Bansal, Tongshuang Wu, Joyce Zhou, Raymond Fok, Besmira Nushi, Ece Kamar, Marco Tulio Ribeiro, Daniel Weld, 10.1145/34117645 2021ACMYokohama Japan</p>
<p>10.1145/3411764.3445717Online; accessed 2024-07-03URL. </p>
<p>To trust or to think: Cognitive forcing functions can reduce overreliance on ai in ai-assisted decision-making. Zana Buçinca, Maja Barbara Malaya, Krzysztof Z Gajos, 10.1145/3449287Online; accessed 2024-07-03Proceedings of the ACM on Human-Computer Interaction. 2573-01425CSCW14 2021</p>
<p>Viewing systematic reviews and meta-analysis in social research through different lenses. Jacqueline Davis, Kerrie Mengersen, Sarah Bennett, Lorraine Mazerolle, 10.1186/2193-1801-3-511SpringerPlus. 2193- 18013151112 2014Online; accessed 2024-07-17</p>
<p>Large language models for tabular data: Progresses and future directions. Haoyu Dong, Zhiruo Wang, 10.1145/3626772.3661384ACM. ISBN. 97984007043147 2024Online; accessed 2024-07-17</p>
<p>Waste in covid-19 research. Sharon Paul P Glasziou, Tammy Sanders, Hoffmann, 10.1136/bmj.m1847BMJ. 1756-183318475 2020Online; accessed 2024-07-17</p>
<p>The rapid, massive growth of covid-19 authors in the scientific literature. P A John, Maia Ioannidis, Kevin W Salholz-Hillel, Jeroen Boyack, Baas, 10.1098/rsos.210389Royal Society Open Science. 2054- 5703892103899 2021Online; accessed 2024-07-17</p>
<p>Cutting through the clutter: The potential of llms for efficient filtration in systematic literature reviews. Lucas Joos, Daniel A Keim, Maximilian T Fischer, 10.48550/ARXIV.2407.106522024</p>
<p>Five steps to conducting a systematic review. Regina Khalid S Khan, Jos Kunz, Gerd Kleijnen, Antes, 10.1177/014107680309600304Journal of the Royal Society of Medicine. 0141-07689633 2003Online; accessed 2024-07-17</p>
<p>The quality of covid-19 systematic reviews during the coronavirus 2019 pandemic: an exploratory comparison. Kevin T Mcdermott, Mark Perry, Willemijn Linden, Rachel Croft, Robert Wolff, Jos Kleijnen, 10.1186/s13643-024-02552-xac- cessed 2024-07-17Systematic Reviews. 2046-40531311265 2024</p>
<p>A comparative study of programming languages in rosetta code. Sebastian Nanz, Carlo A Furia, 10.1109/ICSE.2015.90Online; accessed 2024-07-185 2015Florence, Italy</p>
<p>The prisma 2020 statement: an updated guideline for reporting systematic reviews. Joanne E Matthew J Page, Patrick M Mckenzie, Isabelle Bossuyt, Tammy C Boutron, Cynthia D Hoffmann, Larissa Mulrow, Jennifer M Shamseer, Elie A Tetzlaff, Sue E Akl, Roger Brennan, Julie Chou, Jeremy M Glanville, Asbjørn Grimshaw, Hróbjartsson, M Manoj, Tianjing Lalu, Elizabeth W Li, Evan Loder, Steve Mayo-Wilson, Luke A Mcdonald, Lesley A Mcguinness, James Stewart, Andrea C Thomas, Vivian A Tricco, Penny Welch, David Whiting, Moher, 10.1136/bmj.n71BMJ. 1756-1833713 2021Online; accessed 2024-07-17</p>
<p>A roadmap toward the automatic composition of systematic literature reviews. Monteiro Eugênio, Silva Da, Moisés Júnior, Dutra Lima, 10.47909/ijsmc.52Iberoamerican Journal of Science Measurement and Communication. 2709-3158127 2021Online; accessed 2024-07-17</p>
<p>Prisma-dfllm: An extension of prisma for systematic literature reviews using domain-specific finetuned large language models. Teo Susnjak, 10.48550/ARXIV.2306.149052023</p>
<p>Automated screening of research studies for systematic reviews using study characteristics. Guy Tsafnat, Paul Glasziou, George Karystianis, Enrico Coiera, 10.1186/s13643-018-0724-7Systematic Reviews. 2046-4053716412 2018Online; accessed 2024-07-17</p>
<p>Explanations can reduce overreliance on ai systems during decision-making. Helena Vasconcelos, Matthew Jörke, Madeleine Grunde-Mclaughlin, Tobias Gerstenberg, Michael S Bernstein, Ranjay Krishna, 10.1145/3579605Online; accessed 2024-07-03Proceedings of the ACM on Human-Computer Interaction. 2573-01427CSCW14 2023</p>
<p>Semi-automated screening of biomedical citations for systematic reviews. Thomas A Byron C Wallace, Joseph Trikalinos, Carla Lau, Christopher H Brodley, Schmid, 10.1186/1471-2105-11-55BMC Bioinformatics. 1471- 21051115512 2010Online; accessed 2024-07-17</p>
<p>Robis: A new tool to assess risk of bias in systematic reviews was developed. Penny Whiting, Jelena Savović, P T Julian, Deborah M Higgins, Barnaby C Caldwell, Beverley Reeves, Philippa Shea, Jos Davies, Rachel Kleijnen, Churchill, 10.1016/j.jclinepi.2015.06.005Journal of Clinical Epidemiology. 691 2016Online; accessed 2024-07-17</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6618 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6618</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6618</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-127.html">extraction-schema-127</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <p><strong>Paper ID:</strong> paper-1085ddc5028be0a6f517bde6c44029abc208c63f</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/1085ddc5028be0a6f517bde6c44029abc208c63f" target="_blank">Associative Recurrent Memory Transformer</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> It is demonstrated that ARMT outperfors existing alternatives in associative retrieval tasks and sets a new performance record in the recent BABILong multi-task long-context benchmark by answering single-fact questions over 50 million tokens with an accuracy of 79.9%.</p>
                <p><strong>Paper Abstract:</strong> This paper addresses the challenge of creating a neural architecture for very long sequences that requires constant time for processing new information at each time step. Our approach, Associative Recurrent Memory Transformer (ARMT), is based on transformer self-attention for local context and segment-level recurrence for storage of task specific information distributed over a long context. We demonstrate that ARMT outperfors existing alternatives in associative retrieval tasks and sets a new performance record in the recent BABILong multi-task long-context benchmark by answering single-fact questions over 50 million tokens with an accuracy of 79.9%. The source code for training and evaluation is available on github.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6618.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6618.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ARMT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Associative Recurrent Memory Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A segment-level recurrent transformer augmented with a layerwise associative (quasi-linear key-value) memory (association matrices A and normalizer z) that stores and recalls information across very long contexts using fast-weight style updates and local self-attention.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ARMT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Transformer (GPT-2 backbone in experiments) with segment-level recurrence; at each layer an associative memory matrix A and normalization vector z are maintained. Memory tokens generated from previous segments are mapped to keys, values, and importance scalars and inserted into A with a delta-rule (fast-weights) update; recall is performed by projecting query q via nonlinearity phi and computing A phi(q) normalized by z^T phi(q). Gamma-correction (γ) is used to avoid catastrophic forgetting in z updates.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>approx. 137M GPT-2 base backbone; reported ARMT model ~145M parameters</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Layerwise associative key-value memory (quasi-linear/fast-weights matrix) with segment-level recurrence</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Memory tokens are converted to key vectors k and value vectors v (via linear maps) and stored as key-value associations in an association matrix A; a normalization vector z stores sum of key-feature projections.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Write: map memory token m -> (k,v,β), recall previous value ̄v via A, update A with β(v - ̄v) ⊗ φ(k) and update z with γ φ(k); Read: compute y = A φ(q) / (z^T φ(q)). Uses DPFP-3 nonlinearity for φ and γ-correction to adjust z to avoid catastrophic forgetting.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Associative Retrieval (Remember & Rewrite), BABILong QA1–QA5 long-context benchmark (up to 50M tokens), Wikitext-103 language modeling</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>associative retrieval / memory capacity, multi-hop long-context question answering, language modeling</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>On BABILong QA1 (single supporting fact) ARMT (145M) exact-match: 64k: 100.0%, 128k: 99.9% ±0.2, 500k: 99.3% ±0.9, 1M: 98.5% ±1.0, 10M: 89.4% ±8.1, 50M: reported single-best 79.9% (one model) and averaged 49.6% ±40.4. On other BABILong tasks ARMT substantially outperformed competitors up to 10M tokens (see Table 1). On Associative Retrieval (Rewrite) ARMT generalizes from 50 training pairs to accurate recall after 500 updates (generalization factor 10).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Ablation to PRMT / RMT (no associative A matrix) shows worse performance: e.g., RMT (137M) QA1 1M: 94.2% vs ARMT 98.5%; QA1 10M: RMT 76.4% vs ARMT 89.4%. On associative-retrieval rewrite and remember tasks, RMT and Mamba degrade after exceeding training lengths while ARMT remains robust; PRMT (parallel layerwise recurrent tokens without associative A) did not improve over RMT (see Fig.2 and Fig.4).</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact Match (EM) for BABILong and Associative Retrieval; cross-entropy / bits-per-byte for language modeling</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Sequential segment processing required (no efficient parallel implementation) — slower than fully parallel methods (e.g., Mamba, RWKV) at short/medium sequence lengths (<~300k tokens); requires γ-correction in z-update to avoid catastrophic forgetting; higher variance at extreme length (50M) across runs; training challenges on standard language modeling (tends to keep only last segment in memory).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Struggles on language modeling compared to its improvements on recall tasks (ARMT keeps mostly last segment during LM training and fails to extrapolate well for LM); without γ-correction catastrophic forgetting occurs when many erase-insert operations are performed; higher computational latency for short/medium contexts due to lack of parallelism; results at 50M tokens show high run-to-run variance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Ivan Rodkin; Yuri Kuratov; Aydar Bulatov; Mikhail Burtsev. Associative Recurrent Memory Transformer.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Associative Recurrent Memory Transformer', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6618.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6618.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RMT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recurrent Memory Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Segment-level recurrent transformer that passes memory tokens between segments to maintain long-range contextual state; used as the base recurrent architecture extended by ARMT.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Recurrent memory transformer</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RMT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Transformer with segment-level recurrence: a small set of memory tokens (recurrent states) are produced and passed to the next segment to provide persistent context; memory consists of explicit memory token embeddings that are attended by local self-attention in subsequent segments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>reported experiments use GPT-2 backbone ~137M</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Segment-level recurrent memory tokens (token embeddings carried between segments)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Memory tokens / embeddings (a small set of learned token vectors per layer representing prior context)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Memory tokens produced at end of a segment are concatenated / provided to the next segment's attention as tokens (attention over memory tokens and local context); recurrence is implemented by passing tokens between segments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Associative Retrieval, BABILong QA1–QA5, Wikitext-103 LM (comparison baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>associative retrieval / long-context QA, language modeling</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>RMT (137M) BABILong QA1 exact-match: 64k: 99.6%, 128k: 99.1%, 500k: 96.4%, 1M: 94.2%, 10M: 76.4% (see Table 1). On other QA tasks RMT performance degrades faster with length than ARMT.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Paper compares to PRMT (parallel layerwise recurrent memory) and shows associative memory (ARMT) improves capacity beyond RMT; removing associative mechanism (i.e., comparing to ARMT ablated) reduces long-range recall — e.g., ARMT > RMT numbers above. Exact 'no-memory' (pure local attention) baseline not provided in same tables.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact Match (EM); bits-per-byte for LM</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>RMT benefits from recurrence for long contexts but has limited capacity tied to number of memory tokens; training can be challenging due to backpropagation through many layers; capacity remains limited compared to ARMT.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Limited memory capacity (few memory tokens), degrades on tasks requiring many rewrite operations or extremely long contexts compared to ARMT; performance declines more quickly with extrapolated lengths.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Aydar Bulatov; Yuri Kuratov; Mikhail S. Burtsev. Recurrent memory transformer, 2022.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Associative Recurrent Memory Transformer', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6618.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6618.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PRMT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Parallel-memory Recurrent Memory Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Ablation variant (Parallel Memory RMT) that adds layerwise recurrent memory tokens (passed to next segment at each layer) but without associative association matrices, used to test the contribution of associative memory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PRMT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>RMT-style model modified to pass memory tokens layerwise to the next segment (hierarchical layerwise memory) but without the associative A matrix; effectively RMT with layerwise memory tokens rather than association matrices.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Ablation experiments (small models) ~500k parameters for associative retrieval experiments; comparable parameter regime to RMT in ablations</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Layerwise recurrent memory tokens (non-associative)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Memory tokens (embeddings) per layer passed to next segment</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Memory tokens propagated and attended by subsequent segments' attention (no associative key-value store / A matrix)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Associative Retrieval (Remember and Rewrite) ablation</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>associative retrieval / memory capacity / ablation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>PRMT performs similarly to RMT and does not improve capacity over RMT in the associative retrieval experiments (see Fig.2a and Fig.4(b)). Exact numbers are shown in figures/tables indicating PRMT does not match ARMT.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Compared to ARMT (with associative memory), PRMT/RMT have lower memorization capacity — e.g., ARMT stores more key-value pairs and maintains recall for many more rewrite operations.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact Match for associative retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>PRMT shows that simply adding parallel layerwise memory tokens (without associative A) does not solve capacity issues; no major runtime/implementation tradeoffs explicitly described beyond similarity to RMT.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Does not improve capacity relative to RMT; fails to achieve the robust generalization to many rewrite operations that ARMT achieves.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Associative Recurrent Memory Transformer', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6618.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6618.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mamba</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mamba (Linear-time sequence modeling with selective state spaces)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recent efficient long-context model based on selective state-space components (SSMs) / recurrent states; reported as a competitive efficient alternative which ARMT outperforms on associative retrieval and long-context QA in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mamba: Linear-time sequence modeling with selective state spaces</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Mamba</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Linear-time sequence model using selective state-space components; uses recurrent states (a state vector) as working memory across long contexts (implementation specifics in Mamba paper). In this paper Mamba is used as a baseline comparator for memory capacity and long-context QA.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Mamba-130M reported in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Recurrent state-space memory (selective SSM / recurrent states stored as floats)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Recurrent state vectors (floats) — the paper treats Mamba memory size as number of floats in recurrent states</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>State-space recurrence / updates to state vector across positions (selective SSM updates); treated as associative-like in comparison but operates as recurrent state updates rather than an explicit associative key-value matrix in ARMT.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Associative Retrieval (Remember & Rewrite), BABILong QA1–QA5 (limited due to implementation constraints), language modeling comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>associative retrieval / long-context QA / sequence modeling</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Mamba (130M) shows strong performance on some BABILong tasks at moderate lengths (e.g., QA1: 64k: 100.0% ±0.0, 128k: 99.5% ±0.2, 500k: 92.3% ±1.1) but could not be run beyond some lengths due to implementation limitations; on associative retrieval ARMT and Mamba outperform RMT, but ARMT generalizes better to many rewrite operations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Not provided; Mamba is inherently a memory-based model. Compared to ARMT (with associative memory), Mamba degrades earlier in length-extrapolation tests (e.g., ARMT shows 60x length-generalization on some tasks while Mamba shows 8x in reported comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact Match (EM) for BABILong and associative retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Mamba implementation constraints limit evaluation to certain segmentation settings (cannot run very long first-segment lengths in current implementation), but Mamba is faster at short/medium lengths due to parallelism; ARMT is slower on short/medium contexts but scales better in memory capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Suffers slight degradation after exceeding training lengths in associative-retrieval rewrite tasks; implementation prevented evaluation at extremely long contexts (>128k) in their experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Albert Gu; Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces, 2023.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Associative Recurrent Memory Transformer', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6618.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6618.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RWKV</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RWKV (RNN-like models for transformer era)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An RNN-style recurrent model designed to capture benefits of RNN recurrence with transformer-like performance; mentioned and attempted to be trained on associative retrieval and BABILong but training in this work failed.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Rwkv: Reinventing rnns for the transformer era</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RWKV</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>RNN-style recurrent architecture (RWKV family) that combines recurrence with transformer-style components; uses recurrent states as memory; in this paper the authors attempted to train RWKV on associative retrieval and BABILong but did not succeed with given hyperparameters/effort.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>attempted to fine-tune RWKV-430M for BABILong (reported attempt), and to train RWKV-v5 for associative retrieval but training failed</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Recurrent state (RNN-style) memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>State vectors (recurrent hidden states)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Recurrent update of state across tokens (RWKV internal recurrence mechanism) rather than explicit associative key-value store; specifics in RWKV paper</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Associative Retrieval (attempted), BABILong (attempted)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>associative retrieval / long-context QA (attempted baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>No successful training results reported in this paper — the authors state they failed to train RWKV for associative retrieval and BABILong benchmarks with the same parameters used for other models (Appendix J).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Not applicable (training failed); no numeric comparisons provided.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Authors note RWKV training may require careful hyperparameter tuning and that their attempts failed; no runtime tradeoffs reported in this work due to failed experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Could not be trained successfully for the reported tasks in this study with the authors' settings; therefore omitted from comparative performance tables.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Bo Peng et al. Rwkv: Reinventing rnns for the transformer era, 2023.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Associative Recurrent Memory Transformer', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6618.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6618.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (+ RAG baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (with and without Retrieval-Augmented Generation baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large pretrained LLM used as a few-shot baseline on BABILong tasks; GPT-4 occasionally augmented with retrieval (RAG) for improved few-shot QA performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gpt-4 technical report</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GPT-4 (few-shot) and GPT-4 + RAG (few-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>GPT-4 used in few-shot setting as baseline; GPT-4 + RAG uses external retrieval to fetch relevant context from a datastore prior to answering (retrieval-augmented generation). These are external baseline comparators rather than architectures modified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Not specified in paper (GPT-4 is a very large model; treated as external baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>For GPT-4 alone: no persistent external memory in these experiments; for GPT-4 + RAG: external retrieval datastore (retrieval-augmented generation)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>GPT-4: internal transformer activations (no explicit external memory in few-shot). GPT-4 + RAG: retrieved text passages or passages from an index</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>GPT-4 + RAG: retrieval from external datastore (nearest-neighbor / index) to augment prompt; plain GPT-4: few-shot prompt only (no explicit retrieval).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>BABILong QA1–QA5 (few-shot baselines reported in Table 1)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>few-shot long-context question answering / retrieval-augmented QA baseline</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>GPT-4 (few-shot) on BABILong QA1: 64k: 30.0%, 128k: 24.0% (few-shot). GPT-4 + RAG (few-shot) improved results in some settings (e.g., QA1 64k: 50.0%, 128k: 56.0%, etc.) as reported in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>GPT-4 few-shot (no external retrieval) scores as above (worse than GPT-4 + RAG).</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact Match (EM)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>RAG improves few-shot performance but still fails to match ARMT on many long-length tasks; RAG few-shot performance can drop at extremely long contexts in some reported columns.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>GPT-4 few-shot (without retrieval) performs poorly on long-context BABILong tasks; even with RAG GPT-4 does not approach ARMT performance at extreme lengths in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>OpenAI et al. Gpt-4 technical report, 2024.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Associative Recurrent Memory Transformer', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Recurrent memory transformer <em>(Rating: 2)</em></li>
                <li>Mamba: Linear-time sequence modeling with selective state spaces <em>(Rating: 2)</em></li>
                <li>Linear transformers are secretly fast weight programmers <em>(Rating: 2)</em></li>
                <li>Rwkv: Reinventing rnns for the transformer era <em>(Rating: 2)</em></li>
                <li>In search of needles in a 11m haystack: Recurrent memory finds what llms miss <em>(Rating: 2)</em></li>
                <li>Long-range language modeling with self-retrieval <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6618",
    "paper_id": "paper-1085ddc5028be0a6f517bde6c44029abc208c63f",
    "extraction_schema_id": "extraction-schema-127",
    "extracted_data": [
        {
            "name_short": "ARMT",
            "name_full": "Associative Recurrent Memory Transformer",
            "brief_description": "A segment-level recurrent transformer augmented with a layerwise associative (quasi-linear key-value) memory (association matrices A and normalizer z) that stores and recalls information across very long contexts using fast-weight style updates and local self-attention.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "ARMT",
            "agent_description": "Transformer (GPT-2 backbone in experiments) with segment-level recurrence; at each layer an associative memory matrix A and normalization vector z are maintained. Memory tokens generated from previous segments are mapped to keys, values, and importance scalars and inserted into A with a delta-rule (fast-weights) update; recall is performed by projecting query q via nonlinearity phi and computing A phi(q) normalized by z^T phi(q). Gamma-correction (γ) is used to avoid catastrophic forgetting in z updates.",
            "model_size": "approx. 137M GPT-2 base backbone; reported ARMT model ~145M parameters",
            "memory_used": true,
            "memory_type": "Layerwise associative key-value memory (quasi-linear/fast-weights matrix) with segment-level recurrence",
            "memory_representation": "Memory tokens are converted to key vectors k and value vectors v (via linear maps) and stored as key-value associations in an association matrix A; a normalization vector z stores sum of key-feature projections.",
            "memory_access_mechanism": "Write: map memory token m -&gt; (k,v,β), recall previous value ̄v via A, update A with β(v - ̄v) ⊗ φ(k) and update z with γ φ(k); Read: compute y = A φ(q) / (z^T φ(q)). Uses DPFP-3 nonlinearity for φ and γ-correction to adjust z to avoid catastrophic forgetting.",
            "task_name": "Associative Retrieval (Remember & Rewrite), BABILong QA1–QA5 long-context benchmark (up to 50M tokens), Wikitext-103 language modeling",
            "task_category": "associative retrieval / memory capacity, multi-hop long-context question answering, language modeling",
            "performance_with_memory": "On BABILong QA1 (single supporting fact) ARMT (145M) exact-match: 64k: 100.0%, 128k: 99.9% ±0.2, 500k: 99.3% ±0.9, 1M: 98.5% ±1.0, 10M: 89.4% ±8.1, 50M: reported single-best 79.9% (one model) and averaged 49.6% ±40.4. On other BABILong tasks ARMT substantially outperformed competitors up to 10M tokens (see Table 1). On Associative Retrieval (Rewrite) ARMT generalizes from 50 training pairs to accurate recall after 500 updates (generalization factor 10).",
            "performance_without_memory": "Ablation to PRMT / RMT (no associative A matrix) shows worse performance: e.g., RMT (137M) QA1 1M: 94.2% vs ARMT 98.5%; QA1 10M: RMT 76.4% vs ARMT 89.4%. On associative-retrieval rewrite and remember tasks, RMT and Mamba degrade after exceeding training lengths while ARMT remains robust; PRMT (parallel layerwise recurrent tokens without associative A) did not improve over RMT (see Fig.2 and Fig.4).",
            "has_comparative_results": true,
            "performance_metric": "Exact Match (EM) for BABILong and Associative Retrieval; cross-entropy / bits-per-byte for language modeling",
            "tradeoffs_reported": "Sequential segment processing required (no efficient parallel implementation) — slower than fully parallel methods (e.g., Mamba, RWKV) at short/medium sequence lengths (&lt;~300k tokens); requires γ-correction in z-update to avoid catastrophic forgetting; higher variance at extreme length (50M) across runs; training challenges on standard language modeling (tends to keep only last segment in memory).",
            "limitations_or_failure_cases": "Struggles on language modeling compared to its improvements on recall tasks (ARMT keeps mostly last segment during LM training and fails to extrapolate well for LM); without γ-correction catastrophic forgetting occurs when many erase-insert operations are performed; higher computational latency for short/medium contexts due to lack of parallelism; results at 50M tokens show high run-to-run variance.",
            "citation": "Ivan Rodkin; Yuri Kuratov; Aydar Bulatov; Mikhail Burtsev. Associative Recurrent Memory Transformer.",
            "uuid": "e6618.0",
            "source_info": {
                "paper_title": "Associative Recurrent Memory Transformer",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "RMT",
            "name_full": "Recurrent Memory Transformer",
            "brief_description": "Segment-level recurrent transformer that passes memory tokens between segments to maintain long-range contextual state; used as the base recurrent architecture extended by ARMT.",
            "citation_title": "Recurrent memory transformer",
            "mention_or_use": "use",
            "agent_name": "RMT",
            "agent_description": "Transformer with segment-level recurrence: a small set of memory tokens (recurrent states) are produced and passed to the next segment to provide persistent context; memory consists of explicit memory token embeddings that are attended by local self-attention in subsequent segments.",
            "model_size": "reported experiments use GPT-2 backbone ~137M",
            "memory_used": true,
            "memory_type": "Segment-level recurrent memory tokens (token embeddings carried between segments)",
            "memory_representation": "Memory tokens / embeddings (a small set of learned token vectors per layer representing prior context)",
            "memory_access_mechanism": "Memory tokens produced at end of a segment are concatenated / provided to the next segment's attention as tokens (attention over memory tokens and local context); recurrence is implemented by passing tokens between segments.",
            "task_name": "Associative Retrieval, BABILong QA1–QA5, Wikitext-103 LM (comparison baselines)",
            "task_category": "associative retrieval / long-context QA, language modeling",
            "performance_with_memory": "RMT (137M) BABILong QA1 exact-match: 64k: 99.6%, 128k: 99.1%, 500k: 96.4%, 1M: 94.2%, 10M: 76.4% (see Table 1). On other QA tasks RMT performance degrades faster with length than ARMT.",
            "performance_without_memory": "Paper compares to PRMT (parallel layerwise recurrent memory) and shows associative memory (ARMT) improves capacity beyond RMT; removing associative mechanism (i.e., comparing to ARMT ablated) reduces long-range recall — e.g., ARMT &gt; RMT numbers above. Exact 'no-memory' (pure local attention) baseline not provided in same tables.",
            "has_comparative_results": true,
            "performance_metric": "Exact Match (EM); bits-per-byte for LM",
            "tradeoffs_reported": "RMT benefits from recurrence for long contexts but has limited capacity tied to number of memory tokens; training can be challenging due to backpropagation through many layers; capacity remains limited compared to ARMT.",
            "limitations_or_failure_cases": "Limited memory capacity (few memory tokens), degrades on tasks requiring many rewrite operations or extremely long contexts compared to ARMT; performance declines more quickly with extrapolated lengths.",
            "citation": "Aydar Bulatov; Yuri Kuratov; Mikhail S. Burtsev. Recurrent memory transformer, 2022.",
            "uuid": "e6618.1",
            "source_info": {
                "paper_title": "Associative Recurrent Memory Transformer",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "PRMT",
            "name_full": "Parallel-memory Recurrent Memory Transformer",
            "brief_description": "Ablation variant (Parallel Memory RMT) that adds layerwise recurrent memory tokens (passed to next segment at each layer) but without associative association matrices, used to test the contribution of associative memory.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "PRMT",
            "agent_description": "RMT-style model modified to pass memory tokens layerwise to the next segment (hierarchical layerwise memory) but without the associative A matrix; effectively RMT with layerwise memory tokens rather than association matrices.",
            "model_size": "Ablation experiments (small models) ~500k parameters for associative retrieval experiments; comparable parameter regime to RMT in ablations",
            "memory_used": true,
            "memory_type": "Layerwise recurrent memory tokens (non-associative)",
            "memory_representation": "Memory tokens (embeddings) per layer passed to next segment",
            "memory_access_mechanism": "Memory tokens propagated and attended by subsequent segments' attention (no associative key-value store / A matrix)",
            "task_name": "Associative Retrieval (Remember and Rewrite) ablation",
            "task_category": "associative retrieval / memory capacity / ablation",
            "performance_with_memory": "PRMT performs similarly to RMT and does not improve capacity over RMT in the associative retrieval experiments (see Fig.2a and Fig.4(b)). Exact numbers are shown in figures/tables indicating PRMT does not match ARMT.",
            "performance_without_memory": "Compared to ARMT (with associative memory), PRMT/RMT have lower memorization capacity — e.g., ARMT stores more key-value pairs and maintains recall for many more rewrite operations.",
            "has_comparative_results": true,
            "performance_metric": "Exact Match for associative retrieval",
            "tradeoffs_reported": "PRMT shows that simply adding parallel layerwise memory tokens (without associative A) does not solve capacity issues; no major runtime/implementation tradeoffs explicitly described beyond similarity to RMT.",
            "limitations_or_failure_cases": "Does not improve capacity relative to RMT; fails to achieve the robust generalization to many rewrite operations that ARMT achieves.",
            "citation": "",
            "uuid": "e6618.2",
            "source_info": {
                "paper_title": "Associative Recurrent Memory Transformer",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Mamba",
            "name_full": "Mamba (Linear-time sequence modeling with selective state spaces)",
            "brief_description": "A recent efficient long-context model based on selective state-space components (SSMs) / recurrent states; reported as a competitive efficient alternative which ARMT outperforms on associative retrieval and long-context QA in this paper.",
            "citation_title": "Mamba: Linear-time sequence modeling with selective state spaces",
            "mention_or_use": "use",
            "agent_name": "Mamba",
            "agent_description": "Linear-time sequence model using selective state-space components; uses recurrent states (a state vector) as working memory across long contexts (implementation specifics in Mamba paper). In this paper Mamba is used as a baseline comparator for memory capacity and long-context QA.",
            "model_size": "Mamba-130M reported in experiments",
            "memory_used": true,
            "memory_type": "Recurrent state-space memory (selective SSM / recurrent states stored as floats)",
            "memory_representation": "Recurrent state vectors (floats) — the paper treats Mamba memory size as number of floats in recurrent states",
            "memory_access_mechanism": "State-space recurrence / updates to state vector across positions (selective SSM updates); treated as associative-like in comparison but operates as recurrent state updates rather than an explicit associative key-value matrix in ARMT.",
            "task_name": "Associative Retrieval (Remember & Rewrite), BABILong QA1–QA5 (limited due to implementation constraints), language modeling comparisons",
            "task_category": "associative retrieval / long-context QA / sequence modeling",
            "performance_with_memory": "Mamba (130M) shows strong performance on some BABILong tasks at moderate lengths (e.g., QA1: 64k: 100.0% ±0.0, 128k: 99.5% ±0.2, 500k: 92.3% ±1.1) but could not be run beyond some lengths due to implementation limitations; on associative retrieval ARMT and Mamba outperform RMT, but ARMT generalizes better to many rewrite operations.",
            "performance_without_memory": "Not provided; Mamba is inherently a memory-based model. Compared to ARMT (with associative memory), Mamba degrades earlier in length-extrapolation tests (e.g., ARMT shows 60x length-generalization on some tasks while Mamba shows 8x in reported comparisons).",
            "has_comparative_results": true,
            "performance_metric": "Exact Match (EM) for BABILong and associative retrieval",
            "tradeoffs_reported": "Mamba implementation constraints limit evaluation to certain segmentation settings (cannot run very long first-segment lengths in current implementation), but Mamba is faster at short/medium lengths due to parallelism; ARMT is slower on short/medium contexts but scales better in memory capacity.",
            "limitations_or_failure_cases": "Suffers slight degradation after exceeding training lengths in associative-retrieval rewrite tasks; implementation prevented evaluation at extremely long contexts (&gt;128k) in their experiments.",
            "citation": "Albert Gu; Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces, 2023.",
            "uuid": "e6618.3",
            "source_info": {
                "paper_title": "Associative Recurrent Memory Transformer",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "RWKV",
            "name_full": "RWKV (RNN-like models for transformer era)",
            "brief_description": "An RNN-style recurrent model designed to capture benefits of RNN recurrence with transformer-like performance; mentioned and attempted to be trained on associative retrieval and BABILong but training in this work failed.",
            "citation_title": "Rwkv: Reinventing rnns for the transformer era",
            "mention_or_use": "mention",
            "agent_name": "RWKV",
            "agent_description": "RNN-style recurrent architecture (RWKV family) that combines recurrence with transformer-style components; uses recurrent states as memory; in this paper the authors attempted to train RWKV on associative retrieval and BABILong but did not succeed with given hyperparameters/effort.",
            "model_size": "attempted to fine-tune RWKV-430M for BABILong (reported attempt), and to train RWKV-v5 for associative retrieval but training failed",
            "memory_used": true,
            "memory_type": "Recurrent state (RNN-style) memory",
            "memory_representation": "State vectors (recurrent hidden states)",
            "memory_access_mechanism": "Recurrent update of state across tokens (RWKV internal recurrence mechanism) rather than explicit associative key-value store; specifics in RWKV paper",
            "task_name": "Associative Retrieval (attempted), BABILong (attempted)",
            "task_category": "associative retrieval / long-context QA (attempted baselines)",
            "performance_with_memory": "No successful training results reported in this paper — the authors state they failed to train RWKV for associative retrieval and BABILong benchmarks with the same parameters used for other models (Appendix J).",
            "performance_without_memory": "Not applicable (training failed); no numeric comparisons provided.",
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": "Authors note RWKV training may require careful hyperparameter tuning and that their attempts failed; no runtime tradeoffs reported in this work due to failed experiments.",
            "limitations_or_failure_cases": "Could not be trained successfully for the reported tasks in this study with the authors' settings; therefore omitted from comparative performance tables.",
            "citation": "Bo Peng et al. Rwkv: Reinventing rnns for the transformer era, 2023.",
            "uuid": "e6618.4",
            "source_info": {
                "paper_title": "Associative Recurrent Memory Transformer",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "GPT-4 (+ RAG baseline)",
            "name_full": "GPT-4 (with and without Retrieval-Augmented Generation baselines)",
            "brief_description": "Large pretrained LLM used as a few-shot baseline on BABILong tasks; GPT-4 occasionally augmented with retrieval (RAG) for improved few-shot QA performance.",
            "citation_title": "Gpt-4 technical report",
            "mention_or_use": "use",
            "agent_name": "GPT-4 (few-shot) and GPT-4 + RAG (few-shot)",
            "agent_description": "GPT-4 used in few-shot setting as baseline; GPT-4 + RAG uses external retrieval to fetch relevant context from a datastore prior to answering (retrieval-augmented generation). These are external baseline comparators rather than architectures modified in this paper.",
            "model_size": "Not specified in paper (GPT-4 is a very large model; treated as external baseline)",
            "memory_used": false,
            "memory_type": "For GPT-4 alone: no persistent external memory in these experiments; for GPT-4 + RAG: external retrieval datastore (retrieval-augmented generation)",
            "memory_representation": "GPT-4: internal transformer activations (no explicit external memory in few-shot). GPT-4 + RAG: retrieved text passages or passages from an index",
            "memory_access_mechanism": "GPT-4 + RAG: retrieval from external datastore (nearest-neighbor / index) to augment prompt; plain GPT-4: few-shot prompt only (no explicit retrieval).",
            "task_name": "BABILong QA1–QA5 (few-shot baselines reported in Table 1)",
            "task_category": "few-shot long-context question answering / retrieval-augmented QA baseline",
            "performance_with_memory": "GPT-4 (few-shot) on BABILong QA1: 64k: 30.0%, 128k: 24.0% (few-shot). GPT-4 + RAG (few-shot) improved results in some settings (e.g., QA1 64k: 50.0%, 128k: 56.0%, etc.) as reported in Table 1.",
            "performance_without_memory": "GPT-4 few-shot (no external retrieval) scores as above (worse than GPT-4 + RAG).",
            "has_comparative_results": true,
            "performance_metric": "Exact Match (EM)",
            "tradeoffs_reported": "RAG improves few-shot performance but still fails to match ARMT on many long-length tasks; RAG few-shot performance can drop at extremely long contexts in some reported columns.",
            "limitations_or_failure_cases": "GPT-4 few-shot (without retrieval) performs poorly on long-context BABILong tasks; even with RAG GPT-4 does not approach ARMT performance at extreme lengths in these experiments.",
            "citation": "OpenAI et al. Gpt-4 technical report, 2024.",
            "uuid": "e6618.5",
            "source_info": {
                "paper_title": "Associative Recurrent Memory Transformer",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Recurrent memory transformer",
            "rating": 2
        },
        {
            "paper_title": "Mamba: Linear-time sequence modeling with selective state spaces",
            "rating": 2
        },
        {
            "paper_title": "Linear transformers are secretly fast weight programmers",
            "rating": 2
        },
        {
            "paper_title": "Rwkv: Reinventing rnns for the transformer era",
            "rating": 2
        },
        {
            "paper_title": "In search of needles in a 11m haystack: Recurrent memory finds what llms miss",
            "rating": 2
        },
        {
            "paper_title": "Long-range language modeling with self-retrieval",
            "rating": 1
        }
    ],
    "cost": 0.01675425,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Associative Recurrent Memory Transformer</h1>
<p>Ivan Rodkin ${ }^{1}$ Yuri Kuratov ${ }^{2,1}$ Aydar Bulatov ${ }^{1}$ Mikhail Burtsev ${ }^{3}$<br>${ }^{1}$ Neural Networks and Deep Learning Lab, MIPT, Dolgoprudny, Russia<br>${ }^{2}$ AIRI, Moscow, Russia ${ }^{3}$ London Institute for Mathematical Sciences, London, UK<br>{rodkin.id,yurii.kuratov,bulatov.as}@phystech.edu, mb@lims.ac.uk</p>
<h4>Abstract</h4>
<p>This paper addresses the challenge of creating a neural architecture for very long sequences that requires constant time for processing new information at each time step. Our approach, Associative Recurrent Memory Transformer (ARMT), is based on transformer self-attention for local context and segment-level recurrence for storage of task specific information distributed over a long context. We demonstrate that ARMT outperfors existing alternatives in associative retrieval tasks and sets a new performance record in the recent BABILong multi-task long-context benchmark by answering single-fact questions over 50 million tokens with an accuracy of $79.9 \%$. The source code for training and evaluation is available on github.</p>
<h2>1. Introduction</h2>
<p>Memory plays a crucial role in creating models capable of processing extremely long contexts and utilizing remote past information. Starting from RNNs and evolving through LSTM [13] and Memory Networks [24, 28], we are now in the era of Transformer-based [27] Large Language Models [2, 17, 26]. Various methods for extending transformers context length have emerged [4, 19, 31], including approaches based on transformer segment-level recurrence [3, 5, 6, 20] and novel architectures that combine the efficiency of transformer parallelization during training with recurrence at inference [7, 8, 10, 11, 18]. Alternatively, Retrieval-Augmented Generation (RAG) focuses on retrieving information from external storage [1, 12, 23] or self-retrieving from past inputs [21, 29]. However, retrieval fails on complex tasks that require reasoning over multiple pieces of information [16].</p>
<p>In this work we propose the Associative Recurrent Memory Transformer (ARMT) as an extension of the segment-level recurrent model RMT [3] with associative memory. Compared to RWKV [18] and Mamba [10], which use association-based techniques, ARMT benefits from full local selfattention and has constant time and space complexity of processing new segment, similar to RMT. To study ARMT performance we use the BABILong [16] benchmark, because it allows to generate test samples up to 50 million tokens and beyond, compared to other methods. Additionally, we use Associative Retrieval task with multiple key-value pairs to estimate memory capacity of models.</p>
<p>Main contributions of this work include: (1) a novel ARMT architecture for long context processing with segment-level recurrence and associative memory; (2) demonstration that ARMT outcompetes existing memory based models like RMT [3] and Mamba [10] on associative retrieval and long context processing tasks, achieving $80 \%$ accuracy of single fact QA on unprecedented input</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: ARMT augments the transformer's layers with associative memory. (a) RMT architecture. (b) ARMT adds associative memory processing to each layer. (c) Associative memory is updated with layerwise memory representations.
size of 50 million tokens; (3) an original method to evaluate memory capacity in associative retrieval task.</p>
<h1>2. Associative Recurrent Memory Transformer</h1>
<p>We extend RMT <a href="Fig. 1a">3</a> by addition of layerwise associative memory $A_{s}^{l}$ over segmented input $X_{s}^{l}$ (Fig. 1b). At every input segment $s$ for each layer $l$ memory tokens $M_{s-1}^{l+1}$ generated for preceding segment are added to $A_{s}^{l}$ (Fig. 1c) used to update input sequence and memory embeddings:</p>
<p>$$
\left[X_{s}^{l+1} ; M_{s}^{l+1}\right]=\operatorname{TrBlock}\left(\operatorname{AssocBlock}\left(\left[X_{s}^{l} ; M_{s}^{l}\right], A_{s}^{l}\right)\right) ; \quad A_{s}^{l}=\operatorname{MemUpdate}\left(A_{s-1}^{l} ; M_{s-1}^{l+1}\right)
$$</p>
<p>The mechanism of associative block (Fig. 1c) is similar to linear transformers [15], but attends only to special memory tokens and is calculated differenty. After each segment, memory tokens are converted to keys and values via linear mapping and then stored in quasi-linear key-value memory [22] using non-linearity $\phi$. Given a memory token $m_{i} \in M_{s}^{l+1}$, we calculate the keys, values, and importance scalars $\beta_{i}$. We then recall the previous association $\bar{v}<em i="i">{i}$ with this key, add the new value $v</em>}$ to the memory, erase the previous value $\bar{v<em i="i">{i}$ associated with $k</em>$, and update the normalization vector.</p>
<p>$$
\begin{gathered}
k_{i}, v_{i}=W_{K} m_{i}, W_{V} m_{i} ; \quad \beta_{i}=\sigma\left(W_{\beta} m_{i}\right) ; \quad A_{0}^{l}=\mathbf{0} ; \quad z_{0}^{l}=\mathbf{0} \
\bar{v}<em s-1="s-1">{i}=\frac{A</em> \
A_{s}^{l}=A_{s-1}^{l}+\sum_{i} \beta_{i}\left(v_{i}-\bar{v}}^{l} \phi\left(k_{i}\right)}{\left(z_{s-1}\right)^{T} \phi\left(k_{i}\right)} ; \quad \gamma_{i}=1-\frac{\left(z_{s-1}\right)^{T} \phi\left(k_{i}\right)}{\left|\phi\left(k_{i}\right)\right|^{2}<em i="i">{i}\right) \otimes \phi\left(k</em>\right)
\end{gathered}
$$}\right) ; \quad z_{s}^{l}=z_{s-1}^{l}+\sum_{i} \gamma_{i} \phi\left(k_{i</p>
<p>Once we updated $A_{s}^{l}$ with information from previous segment, we recall an association $y_{j}$ for a token $x_{j}$. Associations $y_{j}$ for each token in the segment are then passed to the next transformer layer:</p>
<p>$$
q_{j}=W_{Q} x_{j} ; \quad y_{j}=\frac{A_{s}^{l} \phi\left(q_{j}\right)}{\left(z_{s}^{l}\right)^{T} \phi\left(q_{j}\right)}
$$</p>
<p>For the non-linearity function $\phi$, we used the proposed in [22] DPFP-3 function because, in this paper, it has shown significant superiority over other methods, which is also consistent with our findings.</p>
<p>Note that without $\gamma_{i}\left(\gamma_{i}=1\right)$ this approach suffers from catastrophic forgetting on some tasks. The reason is that while we erase the information $\bar{v}_{i}$ from the $A$-matrix, the corresponding keys</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: ARMT demonstrates strong performance on associative memory tasks. (a) The estimated number of pairs, stored in memory after processing the context with key-value pairs. (b) ARMT is more accurate at operations in memory. Being trained only on 50 key-value pairs from Associative Retrieval Rewrite task, ARMT performs accurate even on 500 memory updates. So the observed generalization factor is 10 (500 pairs / 50 pairs). All data are averaged over 3 runs except RMT and PRMT with 2 runs.
remain in the normalization vector $z_{s}$. As shown in our experiments (Fig. 4(a)), this problem becomes significant when performing hundreds of erase-insert operations with associative memory. To overcome this, we propose to take into account the previous keys in $z_{s}$ when updating it (details are in Appendix F.1).</p>
<p>To determine which part contributes the most to ARMT performance, we also studied RMT with layerwise recurrent memory without associative memory block (Parallel Memory RMT, or PRMT; see Fig. 5 in Appendix F.2).</p>
<h1>3. Evaluation of Associative Retrieval and Long Context Memory Retention</h1>
<p>We test memory capacity of ARMT in comparison to recent computationally efficient long-context models Mamba [10] and RMT [3] on the following two variants of associative retrieval. ${ }^{1}$</p>
<p>Remember task requires memorization of all key-value pairs with unique keys from the prior context with subsequent recalling a value corresponding to one of the keys (Fig. 2a). We estimate the total number of key-value pairs stored in memory, based on the exact-match metric (details are in Appendix B). Since ARMT has the same recurrent memory size as Mamba, calculated as the number of floats in recurrent states, it can be concluded that ARMT makes better use of its internal associative memory. Both ARMT and Mamba outperform RMT on this task. PRMT does not improve RMT performance (Fig. 2a). This indicates that the associative memory plays a critical role in ARMT performance compared to RMT. Additionally, we ablated ARMT on normalization correction, as detailed in Appendix F.</p>
<p>In Rewrite task the keys are not unique and the goal is to recall the latest value that corresponds to one of the keys from the prior context. This task evaluates the model's ability to dynamically change the memory storage. The results, shown in Fig. 2b, indicate that ARMT is robust to the number of memory rewrite operations, while RMT and Mamba experience slight degradation after exceeding their training lengths. Notably, ARMT maintains perfect memory recall on lengths exceeding 10 times those used in training.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: ARMT sets a record in long-context processing with reasonable performance on 50 million tokens. Accuracy of models on different lengths from Babilong benchmark: panels a-e represent QA1-5 tasks.</p>
<p>We augment the GPT-2 (137M) model with ARMT to solve the BABILong tasks from a recently introduced benchmark for long context processing in a question-answering form [16]. To answer BABILong questions correctly, models have to find multiple relevant facts distributed across long natural contexts with distractor facts, and combine information from relevant facts. We use the exact match metric to evaluate models' performance. As shown in Fig. 3, ARMT outperforms the competitors in the majority of tasks, especially on long sequences. Being trained on 16 k tokens only, it strongly performs up to 50 million tokens on QA1 single supporting fact (Fig. 3a) and up to 10 million tokens on more complex tasks requiring multi-hop reasoning (Fig. 3b-e). We observe 60x length-generalization on these tasks ( $1 \mathrm{M} / 16 \mathrm{k}$ ), while Mamba has 8 x length-generalization (128k / 16k). For Mamba-130m we consider only the lengths up to 128 k due to its implementation limitations (see Appendix H, and Appendices C and D for training details).</p>
<h1>4. Conclusion</h1>
<p>In this work, we propose and evaluate recurrent memory transformer augmented with associative memory mechanism for long-context processing and find that it scales up to an unprecedented 50 million tokens on the BABILong benchmark. ARMT architecture adds an associative memory mechanism for segment-level recurrent model RMT. Based on our evaluation on associative retrieval tasks ARMT demonstrates significant advantage in memory capacity and generalisation compared to</p>
<p>Mamba and RMT. On BABILong benchmark ARMT dominates alternatives on medium sizes up to 500 K tokens and the only approach with high performance across all five tasks in the range of $500 \mathrm{~K}-10 \mathrm{M}$ tokens.</p>
<p>We conclude that ARMT holds great promise for long-range tasks because of its improved memory capacity, its ability to efficiently handle large numbers of rewrite operations with memory, its ability to extract only relevant information from memory during inference, and its generalization to much longer sequences than it was trained on. We also assume that the ARMT can be used for language modeling (Appendices $G$ and $K$ ). Despite the current results, we believe there is potential to enhance its performance on LM task through further research and optimization.</p>
<p>Since all of the results in this study are obtained on relatively small (137M) models, we also assume that the scaling of our methodology and its combination with other techniques can reveal the significant potential for modern large language models. We believe that investigating the properties of recurrent associative memory remains an exciting area of research.</p>
<h1>Acknowledgements</h1>
<p>We are thankful to SberDevices for granting us access to additional computational resources. This work was supported by a grant for research centers, provided by the Analytical Center for the Government of the Russian Federation in accordance with the subsidy agreement (agreement identifier 000000D730324P540002) and the agreement with the Moscow Institute of Physics and Technology dated November 1, 2021 No. 70-2021-00138.</p>
<h2>References</h2>
<p>[1] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving language models by retrieving from trillions of tokens. In International conference on machine learning, pages 2206-2240. PMLR, 2022.
[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.
[3] Aydar Bulatov, Yuri Kuratov, and Mikhail S. Burtsev. Recurrent memory transformer, 2022.
[4] Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. Longlora: Efficient fine-tuning of long-context large language models. In The Twelfth International Conference on Learning Representations, 2023.
[5] Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. Adapting language models to compress contexts, 2023.
[6] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G Carbonell, Quoc Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2978-2988, 2019 .</p>
<p>[7] Soham De, Samuel L. Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, Arnaud Doucet, David Budden, Yee Whye Teh, Razvan Pascanu, Nando De Freitas, and Caglar Gulcehre. Griffin: Mixing gated linear recurrences with local attention for efficient language models, 2024.
[8] Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, and Christopher Ré. Hungry hungry hippos: Towards language modeling with state space models, 2023.
[9] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile: An 800gb dataset of diverse text for language modeling, 2020.
[10] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces, 2023.
[11] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations, 2021.
[12] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented language model pre-training. In International conference on machine learning, pages 3929-3938. PMLR, 2020.
[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Comput., 9 (8):1735-1780, November 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL https://doi.org/10.1162/neco.1997.9.8.1735.
[14] Samy Jelassi, David Brandfonbrener, Sham M. Kakade, and Eran Malach. Repeat after me: Transformers are better than state space models at copying, 2024.
[15] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention, 2020.
[16] Yuri Kuratov, Aydar Bulatov, Petr Anokhin, Dmitry Sorokin, Artyom Sorokin, and Mikhail Burtsev. In search of needles in a 11m haystack: Recurrent memory finds what llms miss, 2024.
[17] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes,</p>
<p>Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. Gpt-4 technical report, 2024.
[18] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, Xuzheng He, Haowen Hou, Jiaju Lin, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Guangyu Song, Xiangru Tang, Bolun Wang, Johan S. Wind, Stanislaw Wozniak, Ruichong Zhang, Zhenyuan Zhang, Qihang Zhao, Peng Zhou, Qinghua Zhou, Jian Zhu, and Rui-Jie Zhu. Rwkv: Reinventing rnns for the transformer era, 2023.
[19] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models. In The Twelfth International Conference on</p>
<p>Learning Representations, 2023.
[20] Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, Chloe Hillier, and Timothy P Lillicrap. Compressive transformers for long-range sequence modelling. arXiv preprint, 2019. URL https://arxiv.org/abs/1911.05507.
[21] Ohad Rubin and Jonathan Berant. Long-range language modeling with self-retrieval. arXiv preprint arXiv:2306.13421, 2023.
[22] Imanol Schlag, Kazuki Irie, and Jürgen Schmidhuber. Linear transformers are secretly fast weight programmers, 2021.
[23] Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. Replug: Retrieval-augmented black-box language models. arXiv preprint arXiv:2301.12652, 2023.
[24] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks, 2015.
[25] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: A successor to transformer for large language models, 2023.
[26] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.
[27] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is All you Need. In Advances in neural information processing systems, pages 5998-6008, 2017. URL http://papers.nips. cc/paper/7181-attention-is-all-you-need.
[28] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1410.3916.
[29] Yuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing transformers. In International Conference on Learning Representations, 2022. URL https: //openreview.net/forum?id=TrjbxzRcnf-.
[30] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks, 2023.
[31] Peitian Zhang, Zheng Liu, Shitao Xiao, Ninglu Shao, Qiwei Ye, and Zhicheng Dou. Soaring from 4 k to 400k: Extending llm's context with activation beacon, 2024.</p>
<h1>Appendix A. Related Work</h1>
<p>AutoCompressor [5] is a strategy that stacks memory to minimize information loss at the price of quadratic computation cost.</p>
<p>Recurrent Memory Transformers Recent challenges in long-context processing tasks demonstrated recurrent memory superiority over attention mechanism [16, 31] (Fig. 1 (a)). It was shown that this type of memory performs well even in contexts of size 11M [16]. But still, this memory has some issues with capacity and training. Capacity remains limited as the suggested memory states are limited to a small number of memory tokens. The training is still challenging, as the whole training process requires backpropagation through time for hundreds of layers. Our approach is supposed to mitigate these problems by leveraging the association matrix as a connector for different segments. In contrast to RMT, it has different parameters for memory (linear attention projections described in Section 2) and makes this memory hierarchical by creating different association matrices for different layers.</p>
<p>Context explosion prevention In the attention sinks paper [30], authors demonstrated the need for some sinking tokens for attention for efficient extrapolation in long contexts. Recurrent Memory in RMT [3] as well as our model successfully perform this function. In RMT the memory tokens can act as attention sinks while in our model the very association matrix can play this role.</p>
<p>The ARMT can also be thought of as a kind of compressed-memory RMT [3], that attends to all previous memory tokens with layerwise memory.</p>
<p>Recurrent Sequence Models The problem of the quadratic cost of attention mechanism led to the development of recurrent architectures with transformer-like performance [7]. The vast majority of them are at least related to the State-Space Models (SSMs) [8, 10, 18, 25]. Despite comparable performance with transformers on LM tasks, SSMs are known to be less efficient in memorization tasks, especially when the question is asked after the information [14]. Our model performs well even on these types of tasks, because it has the large and flexible storage for keeping the associations in memory, simultaneously having the direct access to the local context via the vanilla attention.</p>
<h2>Appendix B. Memory capacity estimation</h2>
<h2>Theorem:</h2>
<p>Given:</p>
<p>$$
\text { exact_match }=\alpha ; \quad n=\text { number of pairs; } \quad v=\text { number of possible values }
$$</p>
<p>Then the number of memorized pairs can be estimated with the formula:</p>
<p>$$
k=\frac{n v \alpha-n}{v-1}
$$</p>
<h2>Proof:</h2>
<p>We can precisely predict the associated value if we remember k pairs and then extract the key from these pairs. We output the random value if we obtain the key from any other pair. As a result, the following is the mathematical expectation of an exact match:</p>
<p>$$
\alpha=\frac{k}{n} \cdot 1+\frac{n-k}{n} \cdot \frac{1}{v}=\frac{k}{n}\left(1-\frac{1}{v}\right)+\frac{1}{v}=\frac{k(v-1)+n}{n v}
$$</p>
<p>$$
k=\frac{n v \alpha-n}{v-1}
$$</p>
<h1>Additionally:</h1>
<p>$$
k=n \alpha \frac{v-\frac{1}{\alpha}}{v-1}=n \alpha \frac{v \alpha-1}{v \alpha-\alpha}=n \frac{v \alpha-1}{v-1}
$$</p>
<h2>Appendix C. Curriculum learning</h2>
<p>We train all models with curriculum learning. This means we incrementally increase the complexity of the task during the training. In particular, we train all models on short sequences first and then increase the length of the sequences until it reaches the maximum length ( 16 k tokens for babilong experiments, 200 pairs for Associative Retrieval Remember, 50 pairs for Associative Retireval Rewrite, and 1024 tokens for language modeling experiments ( 8 segments, 128 each)).</p>
<h2>Appendix D. Babilong training details</h2>
<p>We consider segments of size 512 for RMT and ARMT to process the long sequences. The curriculum learning process uses the following number of sequences consecutively: $2,3,5,8,16,32$. So the training ends when we finish training on 32 segments, 512 tokens each. We also randomly sample the number of segments during training, as we find it helps the model generalize better.</p>
<h2>Appendix E. Associative Retrieval training details</h2>
<p>Due to the task's simplicity and training efficiency, we are considering small models (about 500k parameters each) for the Associative Retrieval dataset studies. Every model that we compare has four layers. 128 is the hidden dimension. If the memory dimension parameter (state size in Mamba and memory dimension in ARMT) is present in the model, it is assumed to be 32 ; if the contrary is not indicated.</p>
<p>Moreover, if the model supports segmentation (like RMT and ARMT), we use different segments for different key-value pairs. Thus, if we have, for instance, 200 pairs, 200 segments are passed through the model, and after that, in the 201st segment, we expect the model to generate the value. Both keys and values consist of several integers from 0 to 15 .</p>
<p>We also use the curriculum with the following number of key-value pairs: $1,2,3,5,10,20,40$, 50 , and 200. We increase the key size if necessary, so the final key size for remember task is 3 (so we have $16^{3}$ unique keys) and for rewrite task it remains 1 (16 unique keys). For the Remember task, we also consider sampling different numbers of pairs during training for better generalization.</p>
<h2>Appendix F. Ablation</h2>
<h2>F.1. Gamma-correction</h2>
<p>Due to an improper normalization vector $z$ update, the proposed fast-weights technique [22] (also known as delta-rule) does not have the length generalization (Fig. 4(a)). The information in the association matrix $A$ is erased, but not from $z$, which is the source of the issue.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">64k</th>
<th style="text-align: center;">128k</th>
<th style="text-align: center;">500k</th>
<th style="text-align: center;">1M</th>
<th style="text-align: center;">10M</th>
<th style="text-align: center;">50M</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">QA1 - SINGLE SUPPORTING FACT</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-4 (Few-shot)</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">24.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4 + RAG (Few-shot)</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">56.0</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">56.0</td>
<td style="text-align: center;">16.0</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">RMT (137M)</td>
<td style="text-align: center;">99.6</td>
<td style="text-align: center;">99.1</td>
<td style="text-align: center;">96.4</td>
<td style="text-align: center;">94.2</td>
<td style="text-align: center;">76.4</td>
<td style="text-align: center;">- $\left(64,8^{*}\right)$</td>
</tr>
<tr>
<td style="text-align: center;">RMT-R</td>
<td style="text-align: center;">99.7</td>
<td style="text-align: center;">99.5</td>
<td style="text-align: center;">97.5</td>
<td style="text-align: center;">97.4</td>
<td style="text-align: center;">86.0</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Mamba (130M)</td>
<td style="text-align: center;">$100 \pm 0.0$</td>
<td style="text-align: center;">$99.5 \pm 0.2$</td>
<td style="text-align: center;">$92.3 \pm 1.1$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">ARMT (145M)</td>
<td style="text-align: center;">$100 \pm 0.0$</td>
<td style="text-align: center;">$99.9 \pm 0.2$</td>
<td style="text-align: center;">$99.3 \pm 0.9$</td>
<td style="text-align: center;">$98.5 \pm 1.0$</td>
<td style="text-align: center;">$89.4 \pm 8.1$</td>
<td style="text-align: center;">$49,6 \pm 40.4\left(79,9^{*}\right)$</td>
</tr>
<tr>
<td style="text-align: center;">QA2 - TWO SUPPORTING FACTS</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-4 (Few-shot)</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">RMT (137M)</td>
<td style="text-align: center;">72.7</td>
<td style="text-align: center;">56.3</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">25.5</td>
<td style="text-align: center;">16.2</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">RMT-R</td>
<td style="text-align: center;">71,6</td>
<td style="text-align: center;">54,9</td>
<td style="text-align: center;">31.8</td>
<td style="text-align: center;">26.3</td>
<td style="text-align: center;">13.0</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Mamba (130M)</td>
<td style="text-align: center;">$95.0 \pm 4.2$</td>
<td style="text-align: center;">$86.7 \pm 6.2$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">ARMT (145M)</td>
<td style="text-align: center;">$100 \pm 0.0$</td>
<td style="text-align: center;">$99.8 \pm 0.2$</td>
<td style="text-align: center;">$99.4 \pm 0.3$</td>
<td style="text-align: center;">$99.4 \pm 0.3$</td>
<td style="text-align: center;">$84.4 \pm 4.0$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">QA3 - THREE SUPPORTING FACTS</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-4 (Few-shot)</td>
<td style="text-align: center;">12.0</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">RMT (137M)</td>
<td style="text-align: center;">51.9</td>
<td style="text-align: center;">42.9</td>
<td style="text-align: center;">25.9</td>
<td style="text-align: center;">24.8</td>
<td style="text-align: center;">21.0</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">RMT-R</td>
<td style="text-align: center;">52.9</td>
<td style="text-align: center;">41.9</td>
<td style="text-align: center;">25.5</td>
<td style="text-align: center;">22.2</td>
<td style="text-align: center;">16.4</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Mamba (130M)</td>
<td style="text-align: center;">$91.8 \pm 0.3$</td>
<td style="text-align: center;">$81.4 \pm 1.0$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">ARMT (145M)</td>
<td style="text-align: center;">$90.4 \pm 2.2$</td>
<td style="text-align: center;">$86.0 \pm 4.8$</td>
<td style="text-align: center;">$79.7 \pm 10.8$</td>
<td style="text-align: center;">$72.1 \pm 14.2$</td>
<td style="text-align: center;">$37.0 \pm 10.3$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">QA4 - TWO ARG RELATIONS</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-4 (Few-shot)</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">RMT (137M)</td>
<td style="text-align: center;">51.2</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">29.4</td>
<td style="text-align: center;">27.3</td>
<td style="text-align: center;">17.2</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">RMT-R</td>
<td style="text-align: center;">58.8</td>
<td style="text-align: center;">50.1</td>
<td style="text-align: center;">32.1</td>
<td style="text-align: center;">26.0</td>
<td style="text-align: center;">14.0</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Mamba (130M)</td>
<td style="text-align: center;">$99.7 \pm 0.2$</td>
<td style="text-align: center;">$97.6 \pm 2.8$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">ARMT (145M)</td>
<td style="text-align: center;">$100 \pm 0.1$</td>
<td style="text-align: center;">$100 \pm 0.1$</td>
<td style="text-align: center;">$99.9 \pm 0.2$</td>
<td style="text-align: center;">$99.8 \pm 0.3$</td>
<td style="text-align: center;">$91.5 \pm 1.7$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">QA5 - THREE ARG RELATIONS</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-4 (Few-shot)</td>
<td style="text-align: center;">64.0</td>
<td style="text-align: center;">48.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">RMT (137M)</td>
<td style="text-align: center;">88.5</td>
<td style="text-align: center;">78.1</td>
<td style="text-align: center;">56.4</td>
<td style="text-align: center;">48.0</td>
<td style="text-align: center;">27.3</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">RMT-R</td>
<td style="text-align: center;">86.2</td>
<td style="text-align: center;">77.4</td>
<td style="text-align: center;">55.9</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">35.0</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Mamba (130M)</td>
<td style="text-align: center;">$98.7 \pm 0.1$</td>
<td style="text-align: center;">$97.5 \pm 1.1$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">ARMT (145M)</td>
<td style="text-align: center;">$99.0 \pm 0.3$</td>
<td style="text-align: center;">$98.7 \pm 0.4$</td>
<td style="text-align: center;">$98.4 \pm 0.3$</td>
<td style="text-align: center;">$97.3 \pm 0.6$</td>
<td style="text-align: center;">$80.9 \pm 7.6$</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 1: Exact match metric on QA1-5 Babilong subsets. Each column corresponds to some constant context length. Context includes both noise sentences and facts. * The 50M exact-match on QA1 is measured on 1 best model. ARMT rows are 3 runs averaged. Mamba rows are 2 runs averaged. The metric is marked bold if its $\pm$ std interval intersects the $\pm$ std interval of the best model.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Segmentation</th>
<th style="text-align: center;">Memory Capacity</th>
<th style="text-align: center;">Working Memory</th>
<th style="text-align: center;">LM</th>
<th style="text-align: center;">Length extrapolation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Mamba</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark \checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">RMT</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$?$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$?$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">ARMT</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark \checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$?$</td>
<td style="text-align: center;">$\checkmark \checkmark$</td>
</tr>
</tbody>
</table>
<p>Table 2: Models abilities.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: (a) $\gamma$-correction cures the quasi-linear attention memory. Without correction, the quasi-linear attention with delta-rule struggles to extrapolate on unseen amounts of memory updates. (b) Parallel memory doesn't solve the capacity issue. This means that the associative memory plays an important role in increasing the capacity of the memory.</p>
<p>Note that $z$ is the sum of $\phi\left(k_{i}\right)$. Moreover, we recall the information from previous segments using the inner product of $\phi\left(k_{i}\right)$ and $\phi\left(q_{i}\right)$. This means that for accurate recall, all $\phi\left(k_{i}\right)$ should be orthogonal to each other. Therefore, we can expect $z$ to be a sum of approximately orthogonal vectors.</p>
<p>In this sense, we simplify our task to the task of removing the $\phi\left(k_{i}\right)$ from the sum of vectors orthogonal to $\phi\left(k_{i}\right)$, with the exception of the very $\phi\left(k_{i}\right)$. This means that the presence of $\phi\left(k_{i}\right)$ can be measured by computing the inner product between this sum and $\phi\left(k_{i}\right)$ and dividing it by the length of $\phi\left(k_{i}\right)$ (just taking an orthogonal basis component).</p>
<p>After the insertion of the new information into our memory, we expect this sum to include only one $\phi\left(k_{i}\right)$. Therefore, our $\gamma$-coefficient can be computed with the following formula:</p>
<p>$$
\begin{aligned}
&amp; \gamma_{i}=1-\frac{\left(z_{s-1}\right)^{T} \phi\left(k_{i}\right)}{\left|\phi\left(k_{i}\right)\right|^{2}} \
&amp; z_{s}=z_{s-1}+\sum_{i} \gamma_{i} \phi\left(k_{i}\right)
\end{aligned}
$$</p>
<p>The inner product $\left(z_{s-1}\right)^{T} \phi\left(k_{i}\right)$ is divided by the square of $\left|\phi\left(k_{i}\right)\right|$ because the gamma will be further multiplied by $\phi\left(k_{i}\right)$.</p>
<p>We also consider detaching the gamma during training, because it seems to converge better in this case.</p>
<h1>F.2. Associative memory ablation</h1>
<p>To understand, if the associative memory important for memorization tasks, we consider another architecture: Parallel-memory RMT (PRMT) Fig. 5</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Parallel recurrent memory transformer. In contrast to RMT, in PRMT memory tokens are passed to the next segment in each layer.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: ARMT performs similarly to RMT on the language modeling task. Wikitext-103 results. The loss on each of the 128 -sized segments on the test dataset (Normalized with Bits-per-byte [9]). The model is trained for a language modeling task on 8 segments, 128 tokens each. Despite the larger estimated capacity, ARMT struggles to solve the LM task well.</p>
<p>It is different from RMT in the hierarchical memory approach, considering the memory shifts layerwise, just like in ARMT. So this architecture can be thought of as RMT with layerwise memory, while ARMT is RMT with layerwise memory organized in association matrices.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Current mamba implementation allows only the first segment to be long. Other tokens have to be processed consecutively one by one.</p>
<h1>Appendix G. Language Modeling experiments</h1>
<p>We utilized the Wikitext-103 dataset to train ARMT and RMT models in order to assess our architecture's performance on real texts. Next, we examined the cross-entropy losses derived from various model segments on the test dataset, as illustrated in Figure 6. In this manner, we can estimate the amount of language data that can be stored in memory.</p>
<p>Nevertheless, we demonstrate that despite having a larger theoretical capacity than RMT, ARMT still performs similarly to RMT in language modeling.</p>
<p>We use the GPT-2 model as the base model for our architecture changes. We consider the RMT and ARMT models' segment sizes to be equal to 128 tokens and train these models to solve the language modeling task on 8 segments, so in total, we train the model to autoregressively predict 1024 tokens. Then we evaluate the models' performance on each of the 15 segments of test texts (1920 tokens).</p>
<h2>Appendix H. Why is mamba slow for long contexts?</h2>
<p>We faced some difficulties in evaluating mamba on long-context (500k+) due to it's specific segmentation abilities shown in Figure 7.</p>
<h2>Appendix I. Associative Retrieval sample structure</h2>
<p>A sample of Remember dataset contains a concatenated context, query, and answer. The context is a set of key-value pairs $(k, v)$, separated by a special token. All keys are sequences of tokens. Tokens in this sequence can intersect, but the whole sequence corresponding to any key is unique in this particular sample. The query is one of the keys in the context. And the answer is a value corresponding to the key from the query. Thus, we can control the number of pairs in the sample and check how many pairs fit in our memory.</p>
<p>This is how the dataset's sample appears:
<key1>:<value1>,<key2>:<value2>, <key3>:<value3>,<key2>-<value2>
The model is thought to be trained to produce the value following the " - " character.</p>
<h1>Appendix J. RWKV-5 model</h1>
<p>We also tried to train the RWKV-v5 [18] model to slove both associative retrieval and babilong tasks (training from scratch for AR tasks and finetuning 430M model for babilong task). Unfortunately, the model was training poorly and hadn't achieved reasonable scores. We used the very same parameters as for training other models. Perhaps the RWKV training process requires accurate adjustments. However, we haven't succeeded.</p>
<h2>Appendix K. Limitations</h2>
<p>The proposed architecture, however, has several drawbacks. The first is the lack of efficient parallel implementation: you have to process all segments consecutively. This doesn't mean that it is slow. It's fast enough to process millions of tokens in a reasonable time. However, on short and medium-length sequences (less than 300k tokens), it is much slower than, for instance, Mamba and RWKV, which have the parallel form. Moreover, as we have shown in Appendix G, it's still challenging to train ARMT to solve the language modeling task well. However, we believe that this problem is not in the very architecture, but in training process: we observe that ARMT tends to keep in memory only the last segment, and therefore struggles to extrapolate on longer sequences.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<ol>
<li>We did our best but failed to train RWKV model for associative retrieval and BABILong benchmarks, Appendix J.</li>
</ol>
<p><a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
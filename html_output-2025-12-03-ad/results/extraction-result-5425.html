<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5425 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5425</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5425</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-109.html">extraction-schema-109</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <p><strong>Paper ID:</strong> paper-0ba87c5d78b60f04f5b0d8b7c1b592a0bf5dbe7e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/0ba87c5d78b60f04f5b0d8b7c1b592a0bf5dbe7e" target="_blank">Neural blackboard architectures of combinatorial structures in cognition</a></p>
                <p><strong>Paper Venue:</strong> Behavioral and Brain Sciences</p>
                <p><strong>Paper TL;DR:</strong> This paper shows that four fundamental problems for a neural instantiation of combinatorial structures can be solved by means of neural “blackboard” architectures, and presents a neural blackboard architecture for sentence structure.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5425.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5425.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Neural Blackboard</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural blackboard architecture (van der Velde)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neural-level architecture that represents concepts as grounded word assemblies which are temporarily bound into explicit structure assemblies (NP/VP, subassemblies for thematic roles) via gated memory circuits, enabling compositional sentence-level representations without duplicating word representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Neural blackboard architecture</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>Conceptual knowledge (words/concepts) is stored as distributed, grounded 'word assemblies'; combinatorial structures are represented functionally by binding these assemblies to separate, reusable 'structure assemblies' (e.g., NP, VP) using active memory circuits and gating, so that each occurrence of a concept in a structure is represented by its binding to a distinct structural slot while the core concept representation is not duplicated.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>Hybrid structured-distributed (grounded distributed assemblies + explicit structured slots/blackboard)</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>Compositionality (explicit constituent-slot bindings), multiple instantiation without duplicating core representations, hierarchical structure (nested structure assemblies), temporary vs. persistent encodings (working memory via reverberation; long-term via synaptic modification), controlled binding/unbinding via gating and control circuits.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>The paper provides detailed mechanistic proposals and simulations/illustrative examples of sentence constructions and binding operations; shows how the architecture can in principle answer binding questions and instantiate variable-like bindings and one-trial learning scenarios. It also relates architecture to known phenomena in language and perception (e.g., sentence complexity, embedded clauses).</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>No direct neuroscientific experiments reported here that validate specific neural circuitry proposed; large-scale biological instantiation, learning of the required structure assemblies and control circuitry, and empirical measurement of the predicted gating/memory dynamics remain to be demonstrated.</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>Sentence-level syntactic and semantic binding (who-does-what-to-whom), visual combinatorial binding, visual working memory, integration of language and perception (grounding).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Explicitly compared to synchrony-of-activation models and recurrent neural networks; claims to solve synchrony's problems (problem-of-2, nested structures, productivity) and to provide explicit structural bindings that RNNs lack (systematicity and massive binding). Strengths: preserves grounded core representations while allowing multiple instantiation and explicit structural relations; Weaknesses: currently theoretical, requires specification of learning algorithms and neural plausibility of large gating/matrix circuits.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Word assemblies (grounded representations) connect to many reusable structure assemblies via initially inactive memory circuits; gating circuits (disinhibition) control which subassemblies (e.g., agent/theme) are active; delay assemblies create stable temporary memory bindings (reverberation); control circuits sequence operations to form bindings reflecting syntactic operations; memory circuits can be converted to long-term synaptic bindings (one-trial or synaptic consolidation).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>Biological plausibility at scale (numbers of structure assemblies and required connectivity), learning mechanisms for creating structure assemblies and their mappings to syntactic operations, how control/gating signals arise and are trained, empirical neural signatures of the proposed memory/gating columns, and quantitative evaluation against behavioral/neural data remain open.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural blackboard architectures of combinatorial structures in cognition', 'publication_date_yy_mm': '2006-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5425.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5425.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Synchrony binding</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Synchrony-of-activation binding models (e.g., Shastri & Ajjanagadde)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of models that functionally represent relations by transient temporal synchrony between distributed representations (nodes/assemblies), with synchrony detected by specialized detectors (fact nodes) to indicate bindings between role slots and fillers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Synchrony-of-activation binding</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>Conceptual constituents remain as separate distributed representations and are bound into relations by becoming coactive in a time-locked (synchronous) pattern; detectors register these synchronies to support inference and relational processing.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>Dynamic temporal/directed distributed representations (temporal binding)</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>Temporary, flexible bindings implemented by temporal phase alignment; no structural duplication of core representations; bindings are ephemeral and must be detected by synchrony-sensitive circuitry (fact nodes).</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>Models demonstrate how simple relational inferences can be implemented by synchrony detection (e.g., mapping give -> own inference in illustrative models); synchrony is a biologically plausible mechanism for transient linking in some perceptual domains.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>Paper highlights concrete functional failures: inability to represent multiple distinct simultaneous occurrences of the same filler in different roles (problem of 2), confusions in propositions with reciprocal relations (John gives Mary... vs Mary gives John...), failure to represent nested/hierarchical structures unambiguously (e.g., Mary knows that John knows Mary), and lack of productivity because fact nodes become specialized detectors—thus not scalable to novel combinations.</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>Relational reasoning in small-scale connectionist models, feature binding in perception (visual feature binding), proposed mechanisms for transient working-memory bindings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Compared unfavorably in the paper to blackboard architecture: synchrony lacks systematic scalability and has structural ambiguities RNNs/blackboards address differently; blackboard architecture preserves grounded core representations while allowing explicit multiple instantiation.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Binding via temporal phase-locking of distributed assemblies; downstream synchrony detectors or fact nodes register co-occurrence and drive inferences; duplication of predicate nodes sometimes employed as a workaround.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>How synchrony detectors could be created for the combinatorially large set of possible relations, how to avoid confusions in overlapping synchrony patterns, and how to scale to natural-language-sized vocabularies remain unresolved.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural blackboard architectures of combinatorial structures in cognition', 'publication_date_yy_mm': '2006-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5425.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5425.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RNNs / Functional compositionality</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recurrent neural networks and functional compositionality (Elman and successors)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Connectionist idea that sequence-processing recurrent networks instantiate compositional behavior functionally via distributed context/state representations rather than via explicit structured bindings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Functional compositionality via recurrent neural networks (RNNs)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>Concepts and structural relations are represented implicitly in the evolving distributed hidden-state of RNNs; compositional performance (productivity) arises from learned mappings from histories to predictions rather than from explicit symbolic structure or slot-filler bindings.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>Distributed temporal/contextual representations (connectionist hidden-state vectors)</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>Learns sequential dependencies and statistical regularities; representations are implicit, continuous, and distributed; generalization depends on learned statistical patterns rather than explicit combinatorial rules.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>RNNs successfully model many sequence prediction and parsing tasks on small- to medium-scale artificial and natural datasets (Elman 1991; Miikkulainen 1996 and others), and capture certain syntactic generalizations when trained on appropriate corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>Paper reports simulations (Van der Velde et al. 2003) showing RNNs trained on small grammars fail to generalize to novel combinations of familiar words in familiar syntactic slots (e.g., training on 'dog hears cat' and 'boy sees girl' but failing on 'boy hears girl'), demonstrating lack of human-like systematicity/productivity. RNNs also lack explicit solutions for the massiveness of binding and variable binding problems.</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>Language modeling, prediction, parsing-like tasks, processing of sequential structure, limited-scale simulations of syntactic phenomena.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>RNNs are contrasted with structured models (blackboard): RNNs excel at learning statistical regularities but struggle with systematic compositional generalization and explicit role-to-filler bindings; blackboard architectures provide explicit compositional slots and address multiple instantiation and variable binding more directly.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Hidden-state recurrence (hidden activations copied back as input/context), gradient-based learning shapes weights to encode regularities; no explicit slot-filler pointers unless augmented.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>Scaling RNN generalization to human-like combinatorial productivity remains problematic; how to endow RNNs with explicit, inspectable slot-like bindings or variable mechanisms while keeping learning advantages is open.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural blackboard architectures of combinatorial structures in cognition', 'publication_date_yy_mm': '2006-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5425.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5425.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Word assemblies</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural word assemblies / cell assemblies (Pulvermüller; Hebbian assemblies)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Concepts (words) are represented as distributed, modality-spanning cell assemblies that become associated with perceptual and motor systems, thus providing grounded concept representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Word (cell) assemblies / grounded distributed assemblies</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>Conceptual knowledge is encoded in distributed assemblies of neurons that span perceptual and motor cortices; repeated co-activation binds neurons into assemblies whose activation reconstitutes the concept and grounds symbols in sensory-motor systems.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>Distributed, grounded, feature-based assemblies</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>Grounding in perception and action, distributed encoding across brain areas, stable long-term storage via synaptic modification, reactivation supports working memory; assemblies carry semantic content via their distributed links.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>Empirical studies cited (e.g., Pulvermüller et al. 2001) show word-related activations in modality-specific cortical regions; Hebbian-style associative learning is a foundational mechanism for assembly formation.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>As discussed, a single shared assembly for a concept creates the 'problem of 2' for multiple simultaneous occurrences—the model alone does not explain how distinct occurrences are individuated in combinatorial structures without additional architectural machinery (e.g., blackboard binding).</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>Lexical semantics, grounding of language in perception/action, models of word meaning and distributed semantic memory.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Contrasted with pure symbolic systems (which duplicate symbols easily but lack grounding) and with blackboard architectures (which preserve grounded assemblies while enabling multiple instantiation via binding to structure assemblies).</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Hebbian synaptic strengthening creates assemblies; reactivation of parts of an assembly can reinstate the whole; assemblies connect into wider networks to support semantic relations and inference.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>How assemblies support flexible, multiple, and nested bindings in real-time composition; how assemblies map onto the specific control/gating mechanisms required for sentence-level combinatorics; exact learning rules for creating structure assemblies and their interfaces with word assemblies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural blackboard architectures of combinatorial structures in cognition', 'publication_date_yy_mm': '2006-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5425.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5425.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Symbol grounding / perceptual symbols</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Symbol grounding problem and perceptual symbol systems</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Theoretical stance that symbolic/categorical representations must be connected to perception and action to have meaning (symbol grounding), and that symbols may be implemented as perceptually grounded representations (Barsalou's perceptual symbol systems).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Symbol grounding / perceptual symbol systems</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>Symbols or conceptual tokens obtain meaning by being anchored in sensory-motor processes and perceptual representations rather than existing as arbitrary amodal tokens; conceptual knowledge is thus functionally organized around perceptual reenactments and modality-specific traces.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>Grounded/perceptual (modal) representations; hybrid proposals often combine grounded tokens with higher-level symbol use</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>Grounding in perception/action, potential for embodied simulation during concept use, context-sensitivity and graded activation depending on modality and task demands.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>Behavioral and neuroimaging evidence (cited in literature) shows modality-specific activations during conceptual tasks and that perceptual features are recruited in conceptual processing; theoretical arguments motivate grounding as necessary for meaningful cognition.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>Pure grounding faces issues of duplication and flexible combinatorial use (problem of 2) unless combined with architectures that enable distinct occurrence representations; also debate exists about sufficiency of perceptual symbols for abstract concepts.</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>Semantic memory, concept representation, embodied cognition, language grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Contrasts with amodal symbolic accounts (which avoid grounding but face interpretability), complements distributed assembly approaches (which provide grounding but need mechanisms for compositional binding such as blackboards).</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Perceptual reenactment, modality-specific activation patterns, association-driven reinstatement of perceptual traces during conceptual tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>How grounding mechanisms produce compositional and systematic combinatorial cognition at scale, and how abstract concepts are represented and manipulated functionally while maintaining grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural blackboard architectures of combinatorial structures in cognition', 'publication_date_yy_mm': '2006-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5425.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5425.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Classical blackboard</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Computational blackboard architectures (Selfridge / AI blackboard systems)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A computational/engineering paradigm where multiple specialized processors write to and read from a common workspace (blackboard) to construct complex compositional structures; provides a functional template for structural representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Computational blackboard architecture</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>Conceptual and syntactic pieces are represented as tokens or partial structures on a shared workspace; specialized processes (parsers, recognizers) incrementally build and combine these pieces into hierarchical structures, enabling reuse and multiple instantiation of tokens on the blackboard.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>Symbolic/slot-based workspace representations (external structured tokens manipulated by processes)</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>Explicit slots for constituents, modular processors that add/modify structure, reuse of symbol instances via multiple placements on the blackboard, clear separation of representational content and control processes.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>Used widely in AI and cognitive architectures as a functional engineering solution for complex compositional tasks; provides conceptual grounding for neural blackboard translation.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>Classical blackboards are symbolic and ungrounded unless linked to perceptual/motor systems; they do not by themselves explain neural implementation or grounding of conceptual tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>Parsing, problem solving, integrated cognitive systems, early models of speech and language processing in cognitive science.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Serves as an inspiration for the neural blackboard: blackboard gives the structural advantages (multiple instantiation/compositionality) while neural blackboard aims to combine those advantages with grounded distributed concept representations.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Processors post and read structured tokens on a shared workspace; arbitration/agenda controls which processor acts; blackboard contents represent partial solutions or composed structures.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>How to neurally implement the blackboard paradigm while preserving grounding and biological plausibility; how learning populates the blackboard with appropriate structure assemblies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural blackboard architectures of combinatorial structures in cognition', 'publication_date_yy_mm': '2006-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Foundations of Language: Brain, Meaning, Grammar, Evolution <em>(Rating: 2)</em></li>
                <li>The symbol grounding problem <em>(Rating: 2)</em></li>
                <li>Perceptual symbol systems <em>(Rating: 2)</em></li>
                <li>From simple associations to systematic reasoning: A connectionist model of the binding problem <em>(Rating: 2)</em></li>
                <li>Distributed representations, simple recurrent networks, and grammatical structure <em>(Rating: 2)</em></li>
                <li>The Organization of Behavior <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5425",
    "paper_id": "paper-0ba87c5d78b60f04f5b0d8b7c1b592a0bf5dbe7e",
    "extraction_schema_id": "extraction-schema-109",
    "extracted_data": [
        {
            "name_short": "Neural Blackboard",
            "name_full": "Neural blackboard architecture (van der Velde)",
            "brief_description": "A neural-level architecture that represents concepts as grounded word assemblies which are temporarily bound into explicit structure assemblies (NP/VP, subassemblies for thematic roles) via gated memory circuits, enabling compositional sentence-level representations without duplicating word representations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "theory_or_model_name": "Neural blackboard architecture",
            "theory_or_model_description": "Conceptual knowledge (words/concepts) is stored as distributed, grounded 'word assemblies'; combinatorial structures are represented functionally by binding these assemblies to separate, reusable 'structure assemblies' (e.g., NP, VP) using active memory circuits and gating, so that each occurrence of a concept in a structure is represented by its binding to a distinct structural slot while the core concept representation is not duplicated.",
            "representation_format_type": "Hybrid structured-distributed (grounded distributed assemblies + explicit structured slots/blackboard)",
            "key_properties": "Compositionality (explicit constituent-slot bindings), multiple instantiation without duplicating core representations, hierarchical structure (nested structure assemblies), temporary vs. persistent encodings (working memory via reverberation; long-term via synaptic modification), controlled binding/unbinding via gating and control circuits.",
            "empirical_support": "The paper provides detailed mechanistic proposals and simulations/illustrative examples of sentence constructions and binding operations; shows how the architecture can in principle answer binding questions and instantiate variable-like bindings and one-trial learning scenarios. It also relates architecture to known phenomena in language and perception (e.g., sentence complexity, embedded clauses).",
            "empirical_challenges": "No direct neuroscientific experiments reported here that validate specific neural circuitry proposed; large-scale biological instantiation, learning of the required structure assemblies and control circuitry, and empirical measurement of the predicted gating/memory dynamics remain to be demonstrated.",
            "applied_domains_or_tasks": "Sentence-level syntactic and semantic binding (who-does-what-to-whom), visual combinatorial binding, visual working memory, integration of language and perception (grounding).",
            "comparison_to_other_models": "Explicitly compared to synchrony-of-activation models and recurrent neural networks; claims to solve synchrony's problems (problem-of-2, nested structures, productivity) and to provide explicit structural bindings that RNNs lack (systematicity and massive binding). Strengths: preserves grounded core representations while allowing multiple instantiation and explicit structural relations; Weaknesses: currently theoretical, requires specification of learning algorithms and neural plausibility of large gating/matrix circuits.",
            "functional_mechanisms": "Word assemblies (grounded representations) connect to many reusable structure assemblies via initially inactive memory circuits; gating circuits (disinhibition) control which subassemblies (e.g., agent/theme) are active; delay assemblies create stable temporary memory bindings (reverberation); control circuits sequence operations to form bindings reflecting syntactic operations; memory circuits can be converted to long-term synaptic bindings (one-trial or synaptic consolidation).",
            "limitations_or_open_questions": "Biological plausibility at scale (numbers of structure assemblies and required connectivity), learning mechanisms for creating structure assemblies and their mappings to syntactic operations, how control/gating signals arise and are trained, empirical neural signatures of the proposed memory/gating columns, and quantitative evaluation against behavioral/neural data remain open.",
            "uuid": "e5425.0",
            "source_info": {
                "paper_title": "Neural blackboard architectures of combinatorial structures in cognition",
                "publication_date_yy_mm": "2006-02"
            }
        },
        {
            "name_short": "Synchrony binding",
            "name_full": "Synchrony-of-activation binding models (e.g., Shastri & Ajjanagadde)",
            "brief_description": "A family of models that functionally represent relations by transient temporal synchrony between distributed representations (nodes/assemblies), with synchrony detected by specialized detectors (fact nodes) to indicate bindings between role slots and fillers.",
            "citation_title": "",
            "mention_or_use": "mention",
            "theory_or_model_name": "Synchrony-of-activation binding",
            "theory_or_model_description": "Conceptual constituents remain as separate distributed representations and are bound into relations by becoming coactive in a time-locked (synchronous) pattern; detectors register these synchronies to support inference and relational processing.",
            "representation_format_type": "Dynamic temporal/directed distributed representations (temporal binding)",
            "key_properties": "Temporary, flexible bindings implemented by temporal phase alignment; no structural duplication of core representations; bindings are ephemeral and must be detected by synchrony-sensitive circuitry (fact nodes).",
            "empirical_support": "Models demonstrate how simple relational inferences can be implemented by synchrony detection (e.g., mapping give -&gt; own inference in illustrative models); synchrony is a biologically plausible mechanism for transient linking in some perceptual domains.",
            "empirical_challenges": "Paper highlights concrete functional failures: inability to represent multiple distinct simultaneous occurrences of the same filler in different roles (problem of 2), confusions in propositions with reciprocal relations (John gives Mary... vs Mary gives John...), failure to represent nested/hierarchical structures unambiguously (e.g., Mary knows that John knows Mary), and lack of productivity because fact nodes become specialized detectors—thus not scalable to novel combinations.",
            "applied_domains_or_tasks": "Relational reasoning in small-scale connectionist models, feature binding in perception (visual feature binding), proposed mechanisms for transient working-memory bindings.",
            "comparison_to_other_models": "Compared unfavorably in the paper to blackboard architecture: synchrony lacks systematic scalability and has structural ambiguities RNNs/blackboards address differently; blackboard architecture preserves grounded core representations while allowing explicit multiple instantiation.",
            "functional_mechanisms": "Binding via temporal phase-locking of distributed assemblies; downstream synchrony detectors or fact nodes register co-occurrence and drive inferences; duplication of predicate nodes sometimes employed as a workaround.",
            "limitations_or_open_questions": "How synchrony detectors could be created for the combinatorially large set of possible relations, how to avoid confusions in overlapping synchrony patterns, and how to scale to natural-language-sized vocabularies remain unresolved.",
            "uuid": "e5425.1",
            "source_info": {
                "paper_title": "Neural blackboard architectures of combinatorial structures in cognition",
                "publication_date_yy_mm": "2006-02"
            }
        },
        {
            "name_short": "RNNs / Functional compositionality",
            "name_full": "Recurrent neural networks and functional compositionality (Elman and successors)",
            "brief_description": "Connectionist idea that sequence-processing recurrent networks instantiate compositional behavior functionally via distributed context/state representations rather than via explicit structured bindings.",
            "citation_title": "",
            "mention_or_use": "use",
            "theory_or_model_name": "Functional compositionality via recurrent neural networks (RNNs)",
            "theory_or_model_description": "Concepts and structural relations are represented implicitly in the evolving distributed hidden-state of RNNs; compositional performance (productivity) arises from learned mappings from histories to predictions rather than from explicit symbolic structure or slot-filler bindings.",
            "representation_format_type": "Distributed temporal/contextual representations (connectionist hidden-state vectors)",
            "key_properties": "Learns sequential dependencies and statistical regularities; representations are implicit, continuous, and distributed; generalization depends on learned statistical patterns rather than explicit combinatorial rules.",
            "empirical_support": "RNNs successfully model many sequence prediction and parsing tasks on small- to medium-scale artificial and natural datasets (Elman 1991; Miikkulainen 1996 and others), and capture certain syntactic generalizations when trained on appropriate corpora.",
            "empirical_challenges": "Paper reports simulations (Van der Velde et al. 2003) showing RNNs trained on small grammars fail to generalize to novel combinations of familiar words in familiar syntactic slots (e.g., training on 'dog hears cat' and 'boy sees girl' but failing on 'boy hears girl'), demonstrating lack of human-like systematicity/productivity. RNNs also lack explicit solutions for the massiveness of binding and variable binding problems.",
            "applied_domains_or_tasks": "Language modeling, prediction, parsing-like tasks, processing of sequential structure, limited-scale simulations of syntactic phenomena.",
            "comparison_to_other_models": "RNNs are contrasted with structured models (blackboard): RNNs excel at learning statistical regularities but struggle with systematic compositional generalization and explicit role-to-filler bindings; blackboard architectures provide explicit compositional slots and address multiple instantiation and variable binding more directly.",
            "functional_mechanisms": "Hidden-state recurrence (hidden activations copied back as input/context), gradient-based learning shapes weights to encode regularities; no explicit slot-filler pointers unless augmented.",
            "limitations_or_open_questions": "Scaling RNN generalization to human-like combinatorial productivity remains problematic; how to endow RNNs with explicit, inspectable slot-like bindings or variable mechanisms while keeping learning advantages is open.",
            "uuid": "e5425.2",
            "source_info": {
                "paper_title": "Neural blackboard architectures of combinatorial structures in cognition",
                "publication_date_yy_mm": "2006-02"
            }
        },
        {
            "name_short": "Word assemblies",
            "name_full": "Neural word assemblies / cell assemblies (Pulvermüller; Hebbian assemblies)",
            "brief_description": "Concepts (words) are represented as distributed, modality-spanning cell assemblies that become associated with perceptual and motor systems, thus providing grounded concept representations.",
            "citation_title": "",
            "mention_or_use": "use",
            "theory_or_model_name": "Word (cell) assemblies / grounded distributed assemblies",
            "theory_or_model_description": "Conceptual knowledge is encoded in distributed assemblies of neurons that span perceptual and motor cortices; repeated co-activation binds neurons into assemblies whose activation reconstitutes the concept and grounds symbols in sensory-motor systems.",
            "representation_format_type": "Distributed, grounded, feature-based assemblies",
            "key_properties": "Grounding in perception and action, distributed encoding across brain areas, stable long-term storage via synaptic modification, reactivation supports working memory; assemblies carry semantic content via their distributed links.",
            "empirical_support": "Empirical studies cited (e.g., Pulvermüller et al. 2001) show word-related activations in modality-specific cortical regions; Hebbian-style associative learning is a foundational mechanism for assembly formation.",
            "empirical_challenges": "As discussed, a single shared assembly for a concept creates the 'problem of 2' for multiple simultaneous occurrences—the model alone does not explain how distinct occurrences are individuated in combinatorial structures without additional architectural machinery (e.g., blackboard binding).",
            "applied_domains_or_tasks": "Lexical semantics, grounding of language in perception/action, models of word meaning and distributed semantic memory.",
            "comparison_to_other_models": "Contrasted with pure symbolic systems (which duplicate symbols easily but lack grounding) and with blackboard architectures (which preserve grounded assemblies while enabling multiple instantiation via binding to structure assemblies).",
            "functional_mechanisms": "Hebbian synaptic strengthening creates assemblies; reactivation of parts of an assembly can reinstate the whole; assemblies connect into wider networks to support semantic relations and inference.",
            "limitations_or_open_questions": "How assemblies support flexible, multiple, and nested bindings in real-time composition; how assemblies map onto the specific control/gating mechanisms required for sentence-level combinatorics; exact learning rules for creating structure assemblies and their interfaces with word assemblies.",
            "uuid": "e5425.3",
            "source_info": {
                "paper_title": "Neural blackboard architectures of combinatorial structures in cognition",
                "publication_date_yy_mm": "2006-02"
            }
        },
        {
            "name_short": "Symbol grounding / perceptual symbols",
            "name_full": "Symbol grounding problem and perceptual symbol systems",
            "brief_description": "Theoretical stance that symbolic/categorical representations must be connected to perception and action to have meaning (symbol grounding), and that symbols may be implemented as perceptually grounded representations (Barsalou's perceptual symbol systems).",
            "citation_title": "",
            "mention_or_use": "mention",
            "theory_or_model_name": "Symbol grounding / perceptual symbol systems",
            "theory_or_model_description": "Symbols or conceptual tokens obtain meaning by being anchored in sensory-motor processes and perceptual representations rather than existing as arbitrary amodal tokens; conceptual knowledge is thus functionally organized around perceptual reenactments and modality-specific traces.",
            "representation_format_type": "Grounded/perceptual (modal) representations; hybrid proposals often combine grounded tokens with higher-level symbol use",
            "key_properties": "Grounding in perception/action, potential for embodied simulation during concept use, context-sensitivity and graded activation depending on modality and task demands.",
            "empirical_support": "Behavioral and neuroimaging evidence (cited in literature) shows modality-specific activations during conceptual tasks and that perceptual features are recruited in conceptual processing; theoretical arguments motivate grounding as necessary for meaningful cognition.",
            "empirical_challenges": "Pure grounding faces issues of duplication and flexible combinatorial use (problem of 2) unless combined with architectures that enable distinct occurrence representations; also debate exists about sufficiency of perceptual symbols for abstract concepts.",
            "applied_domains_or_tasks": "Semantic memory, concept representation, embodied cognition, language grounding.",
            "comparison_to_other_models": "Contrasts with amodal symbolic accounts (which avoid grounding but face interpretability), complements distributed assembly approaches (which provide grounding but need mechanisms for compositional binding such as blackboards).",
            "functional_mechanisms": "Perceptual reenactment, modality-specific activation patterns, association-driven reinstatement of perceptual traces during conceptual tasks.",
            "limitations_or_open_questions": "How grounding mechanisms produce compositional and systematic combinatorial cognition at scale, and how abstract concepts are represented and manipulated functionally while maintaining grounding.",
            "uuid": "e5425.4",
            "source_info": {
                "paper_title": "Neural blackboard architectures of combinatorial structures in cognition",
                "publication_date_yy_mm": "2006-02"
            }
        },
        {
            "name_short": "Classical blackboard",
            "name_full": "Computational blackboard architectures (Selfridge / AI blackboard systems)",
            "brief_description": "A computational/engineering paradigm where multiple specialized processors write to and read from a common workspace (blackboard) to construct complex compositional structures; provides a functional template for structural representations.",
            "citation_title": "",
            "mention_or_use": "mention",
            "theory_or_model_name": "Computational blackboard architecture",
            "theory_or_model_description": "Conceptual and syntactic pieces are represented as tokens or partial structures on a shared workspace; specialized processes (parsers, recognizers) incrementally build and combine these pieces into hierarchical structures, enabling reuse and multiple instantiation of tokens on the blackboard.",
            "representation_format_type": "Symbolic/slot-based workspace representations (external structured tokens manipulated by processes)",
            "key_properties": "Explicit slots for constituents, modular processors that add/modify structure, reuse of symbol instances via multiple placements on the blackboard, clear separation of representational content and control processes.",
            "empirical_support": "Used widely in AI and cognitive architectures as a functional engineering solution for complex compositional tasks; provides conceptual grounding for neural blackboard translation.",
            "empirical_challenges": "Classical blackboards are symbolic and ungrounded unless linked to perceptual/motor systems; they do not by themselves explain neural implementation or grounding of conceptual tokens.",
            "applied_domains_or_tasks": "Parsing, problem solving, integrated cognitive systems, early models of speech and language processing in cognitive science.",
            "comparison_to_other_models": "Serves as an inspiration for the neural blackboard: blackboard gives the structural advantages (multiple instantiation/compositionality) while neural blackboard aims to combine those advantages with grounded distributed concept representations.",
            "functional_mechanisms": "Processors post and read structured tokens on a shared workspace; arbitration/agenda controls which processor acts; blackboard contents represent partial solutions or composed structures.",
            "limitations_or_open_questions": "How to neurally implement the blackboard paradigm while preserving grounding and biological plausibility; how learning populates the blackboard with appropriate structure assemblies.",
            "uuid": "e5425.5",
            "source_info": {
                "paper_title": "Neural blackboard architectures of combinatorial structures in cognition",
                "publication_date_yy_mm": "2006-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Foundations of Language: Brain, Meaning, Grammar, Evolution",
            "rating": 2
        },
        {
            "paper_title": "The symbol grounding problem",
            "rating": 2
        },
        {
            "paper_title": "Perceptual symbol systems",
            "rating": 2
        },
        {
            "paper_title": "From simple associations to systematic reasoning: A connectionist model of the binding problem",
            "rating": 2
        },
        {
            "paper_title": "Distributed representations, simple recurrent networks, and grammatical structure",
            "rating": 2
        },
        {
            "paper_title": "The Organization of Behavior",
            "rating": 2
        }
    ],
    "cost": 0.01579625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Neural Blackboard Architectures</h1>
<h2>of Combinatorial Structures in Cognition</h2>
<p>Frank van der Velde</p>
<p>Unit of Cognitive Psychology
Leiden University
Wassenaarseweg 52, 2333 AK Leiden
The Netherlands
vdvelde@fsw.leidenuniv.nl</p>
<h4>Abstract</h4>
<p>Human cognition is unique in the way in which it relies on combinatorial (or compositional) structures. Language provides ample evidence for the existence of combinatorial structures, but they can also be found in visual cognition. To understand the neural basis of human cognition, it is therefore essential to understand how combinatorial structures can be instantiated in neural terms. In his recent book on the foundations of language, Jackendoff formulated four fundamental problems for a neural instantiation of combinatorial structures: the massiveness of the binding problem, the problem of 2 , the problem of variables and the transformation of combinatorial structures from working memory to long-term memory. This paper aims to show that these problems can be solved by means of neural 'blackboard' architectures. For this purpose, a neural blackboard architecture for sentence structure is presented. In this architecture, neural structures that encode for words are temporarily bound in a manner that preserves the structure of the sentence. It is shown that the architecture solves the four problems presented by Jackendoff. The ability of the architecture to instantiate sentence structures is illustrated with examples of sentence complexity observed in human language performance. Similarities exist between the architecture for sentence structure and blackboard architectures for combinatorial structures in visual cognition, derived from the structure of the visual cortex. These architectures are briefly discussed, together with an example of a combinatorial structure in which the blackboard architectures for language and vision are combined. In this way, the architecture for language is grounded in perception.</p>
<h1>Content</h1>
<ol>
<li>Introduction</li>
<li>Four challenges for cognitive neuroscience
2.1. The massiveness of the binding problem
2.2. The problem of 2
2.2.1. The problem of 2 and the symbol grounding problem
2.3. The problem of variables
2.4. Binding in working memory versus long-term memory
2.5. Overview</li>
<li>Combinatorial structures with synchrony of activation
3.1. Nested structures with synchrony of activation
3.2. Productivity with synchrony of activation</li>
<li>Processing linguistic structures with recurrent neural networks
4.1. Combinatorial productivity with RNNs
4.2. RNNs and the massiveness of the binding problem</li>
<li>Blackboard architectures of combinatorial structures</li>
<li>A neural blackboard architecture of sentence structure
6.1. Gating and memory circuits
6.2. Overview of the architecture
6.2.1. Connection structure for binding in the architecture
6.3. Multiple instantiation and binding in the architecture
6.3.1. Answering binding questions
6.4. Extending the blackboard architecture
6.4.1. The modular nature of the blackboard architecture
6.5. Constituent binding in long-term memory
6.5.1. One-trial learning
6.5.2. Explicit encoding of sentence structure with synaptic modification
6.6. Variable binding
6.6.1. Neural structure versus spreading of activation
6.7. Structural dependencies in the blackboard architecture
6.7.1. Embedded clauses in the blackboard architecture
6.7.2. Multiple embedded clauses
6.7.3. Dynamics of binding in the blackboard architecture
6.7.4. Dynamics of binding and complexity
6.8. Further development of the architecture</li>
<li>Neural blackboard architectures of combinatorial structures in vision
7.1. Feature binding
7.2. A neural blackboard architecture of visual working memory
7.2.1. Feature binding in visual working memory
7.3. Feature binding in long-term memory
7.4. Integrating combinatorial structures in language and vision</li>
<li>Conclusion</li>
</ol>
<p>Notes
References</p>
<h1>1. Introduction</h1>
<p>Human cognition is unique in the manner in which it processes and produces complex combinatorial (or compositional) structures (e.g., Anderson 1983; Newell 1990; Pinker 1998). Therefore, to understand the neural basis of human cognition, it is essential to understand how combinatorial structures can be instantiated in neural terms. However, combinatorial structures present particular challenges to theories of neurocognition, which have not been widely recognized in the cognitive neuroscience community (Jackendoff 2002).</p>
<p>A prominent example of these challenges is given by the neural instantiation (in theoretical terms) of linguistic structures. In his recent book on the foundations of language, Jackendoff (2002; see also Jackendoff in press) analyzed the most important theoretical problems that the combinatorial and rule-based nature of language presents to theories of neurocognition. He summarized these problems under the heading of 'four challenges for cognitive neuroscience' (pp. 58-67). As recognized by Jackendoff, these problems arise not only with linguistic structures, but with combinatorial cognitive structures in general.</p>
<p>This paper aims to show that neural 'blackboard' architectures can provide an adequate theoretical basis for a neural instantiation of combinatorial cognitive structures. In particular, I will discuss how the problems presented by Jackendoff (2002) can be solved in terms of a neural blackboard architecture of sentence structure. I will also discuss the similarities between the neural blackboard architecture of sentence structure and neural blackboard architectures of combinatorial structures in visual cognition and visual working memory (Van der Velde 1997; Van der Velde \&amp; de Kamps 2001; 2003a).</p>
<p>To begin with, I will first outline the problems described by Jackendoff (2002) in more detail. This presentation is followed by a discussion of the most important solutions that have been offered thus far to meet some of these challenges. These solutions are based on either synchrony of activation or on recurrent neural networks ${ }^{1}$.</p>
<h2>2. Four challenges for cognitive neuroscience</h2>
<p>The four challenges for cognitive neuroscience presented by Jackendoff (2002) consists of: the massiveness of the binding problem that occurs in language, the problem of multiple instances (or the 'problem of 2'), the problem of variables, and the relation between binding in working memory and binding in long-term memory. I will discuss these problems in turn.</p>
<h3>2.1. The massiveness of the binding problem</h3>
<p>In neuroscience, the binding problem concerns the way in which neural instantiations of elements (constituents) can be related (bound) temporarily in a manner that preserves the structural relations between the constituents. Examples of this problem can be found in visual perception. Colors and shapes of objects are partly processed in different brain areas, but we perceive objects as a unity of color and shape. Thus, in a visual scene with a green apple and a red orange, the neurons that code for green have to be related (temporarily) with the neurons that code for apple, so that the confusion with a red apple (and a green orange) can be avoided.</p>
<p>In the case of language, the problem is illustrated in figure 1. Assume that words like cat, chases and mouse each activate specific neural structures, such as the 'word</p>
<p>assemblies' discussed by Pulvermüller (1999). The problem is how the neural structures or word assemblies for cat and mouse can be bound to the neural structure or word assembly of the verb chases, in line with the thematic roles (or argument structure) of the verb. That is, how cat and mouse can be bound to the role of agent and theme of chases in the sentence The cat chases the mouse, and to the role of theme and agent of chases in the sentence The mouse chases the cat.
(a)
<img alt="img-0.jpeg" src="img-0.jpeg" />
'sentence' neurons
<img alt="img-1.jpeg" src="img-1.jpeg" />
cat chases mouse
<img alt="img-2.jpeg" src="img-2.jpeg" />
mouse chases cat</p>
<p>Figure 1. (a). Illustration of the neural structures ('neural word assemblies') activated by the words cat, chases and mouse. Bottom: An attempt to encode sentence structures with specialized 'sentence' neurons. In (b), a 'sentence' neuron has the assemblies for the words cat, chases and mouse in its 'receptive field' (as indicated with the cone). The neuron is activated by a specialized neural circuit when the assemblies in its receptive field are active in the order cat chases mouse. In (c), a similar 'sentence' neuron for the sentence mouse chases cat.</p>
<p>A potential solution for this problem is illustrated in figure 1. It consists of specialized neurons (or populations of neurons) that are activated when the strings cat chases mouse (figure 1b) or mouse chases cat (figure 1c) are heard or seen. Each neuron has the word assemblies for cat, mouse and chases in its 'receptive field' (illustrated with the cones in figures 1 b and 1 c ). Specialized neural circuits could activate one neuron in the case of cat chases mouse and another neuron in the case of mouse chases cat, by using the difference in temporal word order in both strings. Circuits of this kind can be found in the case of motion detection in visual perception (e.g., Hubel 1995). For</p>
<p>instance, the movement of a vertical bar that sweeps across the retina in the direction from A to B can be detected by using the difference in activation time (onset latency) between the ganglion cells in A and B . A similar specialized circuit can detect a vertical bar moving from B to A .</p>
<p>However, a fundamental problem with this solution in the case of language is its lack of productivity. Only specific and familiar sentences can be detected in this way. But any novel sentence of the type Noun chases Noun or, more generally, Noun Verb Noun will not be detected because the specific circuit (and neuron) for that sentence will be missing. Yet, when we learn that Dumbledore is headmaster of Hogwarts, we immediately understand the meaning of Dumbledore chases the mouse, even though we have never encountered that sentence before.</p>
<p>The difference between language and motion detection in this respect illustrates a fundamental difference in nature between these two cognitive processes. In the case of motion detection there is a limited set of possibilities, so that it is possible (and it pays off) to have specialized neurons and neural circuits for each of these possibilities. However, this solution is not feasible in the case of language. Linguists typically describe language in terms of its unlimited combinatorial productivity. Words can be combined into phrases, which in turn can be combined into sentences, so that arbitrary sentence structures can be filled with arbitrary arguments (e.g., Webelhuth 1995; Sag \&amp; Wasow 1999; Chomsky 2000; Pullum \&amp; Scholz 2001; Jackendoff 2002; Piattelli-Palmarini 2002). In theory, an unlimited amount of sentences can be produced in this way, which excludes the possibility of having specialized neurons and circuits for each of these sentences.</p>
<p>One could argue that many of the sentences that are theoretically possible may be too complex for humans to understand (Christiansen \&amp; Chater 1999). However, unlimited (recursive) productivity is not necessary to make a case for the combinatorial nature of language, given the number of sentences that can be produced or understood. For instance, the average English-speaking 17-year-old knows more than 60.000 words (Bloom 2000). With this lexicon, and with a limited sentence length of 20 words or less, one can produce a set of sentences in natural language in the order of $10^{20}$ or more (Pinker 1998). A set of this kind can be characterized as a 'performance set' of natural language, in the sense that (barring a few selected examples) any sentence from this set can be produced or understood by a normal language user. Such a performance set is not unlimited, but it is of 'astronomical' magnitude (e.g., $10^{20}$ exceeds the estimated lifetime of the universe expressed in seconds). By consequence, most sentences in this set are sentences that we have never heard or seen before. Yet, because of the combinatorial nature of language we have the ability to produce or understand arbitrary sentences from a set of this kind.</p>
<p>Hence, the set of possibilities that we can encounter in the case of language is unlimited in any practical sense. This precludes a solution of the binding problem in language in terms of specialized neurons and circuits. Instead, a solution is needed that depends on the ability to bind arbitrary arguments to the thematic roles of arbitrary verbs, in agreement with the structural relations expressed in the sentence. Moreover, the solution has to satisfy the massiveness of the binding problem as it occurs in language, which is due to the often complex and hierarchical nature of linguistic structures. For instance, in the sentence The cat that the dog bites chases the mouse, cat is bound to the</p>
<p>role of theme of the verb bites, but it is bound to the role of agent of the verb chases. In fact, the whole phrase The cat that the dog bites is bound to the role of agent of the verb chases (with cat as the head of the phrase). Each of these specific bindings has to be satisfied in an encoding of this sentence. Further examples can be seen in a simple syntactic structure like beside a big star (Jackendoff 2002). Here, one can identify relationships like 'it is a prepositional phrase', 'it is a part of a verb phrase', 'it follows a verb', and 'it has a preposition and noun phrase parts'. Binding problems occur for each of these relationships.</p>
<h1>2.2. The problem of 2</h1>
<p>The second problem presented by Jackendoff (2002) is the problem of multiple instances, or the 'problem of 2 '. Jackendoff illustrates this problem with the sentence The little star is beside a big star ${ }^{2}$. The word star occurs twice in this sentence, the first time related with the word little and the second time related with the word big. The problem is how in neural terms the two occurrences of the word star can be distinguished, so that star is first bound with little and then with big, without creating the erroneous binding of little big star. The problem of 2 results from the assumption that any occurrence of a given word will result in the activation of the same neural structure (e.g., its word assembly, as illustrated in figure 1). But if the second occurrence of a word only results in the reactivation of a neural structure that was already activated by the first occurrence of that word, the two occurrences of the same word are indistinguishable (Van der Velde 1999).</p>
<p>Perhaps the problem could be solved by assuming that there are multiple neural structures that encode for a single word. The word star could then activate one neural structure in little star and a different one in big star, so that the bindings little star and big star can be encoded without creating little big star. However, this solution would entail that there are multiple neural structures for all words in the lexicon, perhaps even for all potential positions a word could have in a sentence (Jackendoff 2002).</p>
<p>More importantly even, this solution disrupts the unity of word encoding as the basis for the meaning of a word. For instance, the relation between the neural structures for cat and mouse in cat chases mouse could develop into the neural basis for the long-term knowledge ('fact') that cats chase mice. Similarly, the relation between the neural structures for cat and dog in dog bites cat could form the basis of the fact that dogs fight with cats. But if the neural structure for cat (say, $\left.c a t_{1}\right)$ in cat $<em 2="2">{1}$ chases mouse is different from the neural structure for cat (say, $\left.c a t</em>$, then these two facts are about different kinds of animals.}\right)$ in dog bites cat $_{2</p>
<h3>2.2.1. The problem of 2 and the symbol grounding problem</h3>
<p>It is interesting to look at the problem of 2 from the perspective of the symbol grounding problem that occurs in cognitive symbol systems. Duplicating symbols is easy in a symbol system. However, in a symbol system, one is faced with the problem that symbols are arbitrary entities (e.g., strings of bits in a computer), which therefore have to be interpreted to provide meaning to the system. That is, symbols have to be 'grounded' in perception and action if symbol systems are to be viable models of cognition (Harnad 1991; Barsalou 1999).</p>
<p>Grounding in perception and action can be achieved with neural structures such as the word assemblies illustrated in figure 1. In line with the idea of neural assemblies</p>
<p>proposed by Hebb (1949), Pulvermüller (1999) argued that words activate neural assemblies, distributed over the brain (as illustrated with the assemblies for the words cat, mouse and chases in figure 1). One could imagine that these word assemblies have developed over time by means of a process of association. Each time a word was heard or seen, certain neural circuits would have been activated in the cortex. Over time, these circuits will be associated, which results in an overall cell assembly that reflects the meaning of that word. For instance, assemblies for words with a specific visual content would stretch into the visual cortex, whereas words that describe particular actions (e.g., 'walking' vs 'talking') would activate assemblies that stretch into specific parts of the motor cortex, as observed by Pulvermüller et al. (2001).</p>
<p>But, as argued above, word assemblies are faced with the problem of 2 . Thus, it seems that the problem of 2 and the symbol grounding problem are complementary problems. To provide grounding, the neural structure that encodes for a word is embedded in the overall network structure of the brain. But this makes it difficult to instantiate a duplication of the word, and thus to instantiate even relatively simple combinatorial structures such as The little star is beside a big star. Conversely, duplication is easy in symbol systems (e.g., if ' 1101 ' = star, then one would have The little 1101 is beside a big 1101, with little and big each related to an individual copy of 1101). But symbols can be duplicated easily because they are not embedded in an overall structure that provides the grounding of the symbol ${ }^{3}$.</p>
<h1>2.3. The problem of variables</h1>
<p>The knowledge of specific facts can be instantiated on the basis of specialized neural circuits, in line with those illustrated in figure 1. But knowledge of systematic facts, such as the fact that $\operatorname{own}(y, z)$ follows from $\operatorname{give}(x, y, z)$, cannot be instantiated in this way, that is, in terms of a listing of all specific instances of the relation between the predicates own and give (e.g., from give(John, Mary, book) it follows that own(Mary, book); from give(Mary, John, pen) it follows that own(John, pen); etc.).</p>
<p>Instead, the derivation that own(Mary, book) follows from give(John, Mary, book) is based on the rule that $\operatorname{own}(y, z)$ follows from $\operatorname{give}(x, y, z)$, combined with the binding of Mary to the variable $y$ and book to the variable $z$. This raises the question of how rulebased derivation with variable binding can be instantiated in the brain.</p>
<p>The ability of rule-based derivation with variable binding provides the basis for the systematic nature of cognition (Fodor \&amp; Pylyshyn 1988). Cognition is systematic in the sense that one can learn from specific examples and apply that knowledge to all examples of the same kind. A child will indeed encounter only specific examples (e.g., that when John gives Mary a book, it follows that Mary owns the book) and yet it will learn that $\operatorname{own}(y, z)$ follows from all instances of the kind give $(x, y, z)$. In this way, the child is able to handle novel situations, such as the derivation that own(Harry, broom) follows from give(Dumbledore, Harry, broom).</p>
<h3>2.4. Binding in working memory versus long-term memory</h3>
<p>Working memory in the brain is generally assumed to consist of a sustained form of activation (e.g, Amit 1995; Fuster 1995). That is, information is stored in working memory as long as the neurons that encode the information remain active. In contrast, long-term memory results from synaptic modification. In this way, the connections</p>
<p>between neurons are modified (e.g., enhanced) so that when some of the neurons are reactivated, they will reactivate the others neurons as well. The neural word assemblies, illustrated in figure 1, are formed by this process.</p>
<p>Both forms of memory are related in the sense that information in one form of memory can be transformed into information in the other form of memory. Information is initially stored in working memory before it is stored in long-term memory. Conversely, information in long-term memory can be reactivated and stored in working memory. This raises the question of how the same combinatorial structure can be instantiated both in terms of neural activation and in terms of synaptic modification, and how these different instantiations can be transformed into one another.</p>
<h1>2.5. Overview</h1>
<p>It is clear that the four problems presented by Jackendoff (2002) are interrelated. For instance, the problem of 2 also occurs in rule-based derivation with variable binding, the massiveness of the binding problem is found in combinatorial structures stored in working memory and in combinatorial structures stored in long-term memory. Therefore, a solution of these problems has to be an integrated one that solves all four problems simultaneously. In this paper, I will discuss how all four problems can be solved in terms of neural blackboard architectures in which combinatorial structures can be instantiated.</p>
<p>First, however, I will discuss two alternatives for a neural instantiation of combinatorial structures. In the next section I will discuss the use of synchrony of activation as a mechanism for binding constituents in combinatorial structures. In the section after that, I will discuss the view that combinatorial structures can be handled with recurrent neural networks.</p>
<h2>3. Combinatorial structures with synchrony of activation</h2>
<p>An elaborate example of a neural instantiation of combinatorial structures in which synchrony of activation is used as a binding mechanism is found in the model of reflexive reasoning presented by Shastri and Ajjanagadde (1993). In their model, synchrony of activation is used to show how a known fact such as John gives Mary a book can result in an inference such as Mary owns a book.</p>
<p>The proposition John gives Mary a book is encoded by a 'fact node' that detects the respective synchrony of activation between the nodes for John, Mary and book, and the nodes for giver, recipient and give-object. These nodes encode for the thematic roles of the predicate give $(x, y, z)$. In a simplified manner, the reasoning process begins with the query own(Mary, book)? (i.e., does Mary own a book?). The query results in the respective synchronous activation of the nodes for owner and own-object of the predicate $\operatorname{own}(y, z)$ with the nodes for Mary and book. In turn, the nodes for recipient and giveobject of the predicate give $(x, y, z)$ are activated by the nodes for owner and own-object, such that owner is in synchrony with recipient and own-object is in synchrony with giveobject. As a result, the node for Mary is in synchrony with the node for recipient and the node for book is in synchrony with the node for give-object. This allows the fact node for John gives Mary a book to become active, which produces the affirmative answer to the query.</p>
<p>A first problem with a model of this kind is found in a proposition like John gives Mary a book and Mary gives John a pen. With synchrony as a binding mechanism, a</p>
<p>confusion arises in this proposition between John and Mary in their respective roles of giver and recipient in this proposition. In effect, the same pattern of activation will be found in the proposition John gives Mary a pen and Mary gives John a book. Thus, with synchrony of activation as a binding mechanism, both propositions are indistinguishable. It is not difficult to see the problem of 2 here. John and Mary occur twice in the proposition, but in different thematic roles. The simultaneous but distinguishable binding of John and Mary with different thematic roles cannot be achieved with synchrony of activation.</p>
<p>To solve this problem, Shastri and Ajjanagadde allowed for a duplication (or multiplication) of the nodes for the predicates. In this way, the whole proposition John gives Mary a book and Mary gives John a pen is partitioned into the two elementary propositions John gives Mary a book and Mary gives John a pen. To distinguish between the propositions, the nodes for the predicate $\operatorname{give}(x, y, z)$ are duplicated. Thus, there are specific nodes for, say, give $<em 2="2">{1}(x, y, z)$ and give $</em>(x, y, z)$, with give $<em 2="2">{1}(x, y, z)$ activated by John gives Mary a book and give $</em>(x, y, z)$ activated by Mary gives John a pen. Furthermore, for the reasoning process to work, the associations between predicates have to be duplicated as well. Thus, the node for give $<em 1="1">{1}(x, y, z)$ has to be associated with a node for, say, own $</em>(y, z)$ and the node for give $<em 2="2">{2}(x, y, z)$ has to be associated with a node for own $</em>(y, z)$.</p>
<p>This raises the question of how these associations can be formed simultaneously during learning. During its development, a child will learn from specific examples. Thus, it will learn that, when John gives Mary a book, it follows that Mary owns the book. In this way, the child will form an association between the nodes for, say, give $<em 1="1">{1}(x, y, z)$ and $o w n</em>(y, z)$. But the association between the node for give $<em 2="2">{2}(x, y, z)$ and $o w n</em>(y, z)$ would not be formed in this case, because these nodes are not activated with John gives Mary a book and Mary owns the book. Thus, when the predicate give $(x, y, z)$ is duplicated into give $<em 2="2">{1}(x, y, z)$ and give $</em>(x, y, z)$, the systematicity between John gives Mary a book and Mary gives John a pen is lost.</p>
<h1>3.1. Nested structures with synchrony of activation</h1>
<p>The duplication solution discussed above fails with nested (or hierarchical) propositions. For instance, the proposition Mary knows that John knows Mary cannot be partitioned into two propositions Mary knows and John knows Mary, because the entire second proposition is the $y$ argument of knows(Mary, $y$ ). Thus, the fact node for John knows Mary has to be in synchrony with the node for know-object of the predicate know $(x, y)$. The fact node for John knows Mary will be activated because John is in synchrony with the node for knower and Mary is in synchrony with the node for know-object. However, the fact node for Mary knows Mary will also be activated in this case, because Mary is in synchrony with both knower and know-object in the proposition Mary knows that John knows Mary. Thus, the proposition Mary knows that John knows Mary cannot be distinguished from the proposition Mary knows that Mary knows Mary. Likewise, the proposition Mary knows that John knows Mary cannot be distinguished from the propositions John knows that John knows Mary and John knows that Mary knows Mary, because John is in synchrony with knower in each of these propositions.</p>
<h1>3.2. Productivity with synchrony of activation</h1>
<p>A further problem with the use of synchrony of activation as a binding mechanism is its lack of productivity. The model of Shastri and Ajjanagadde depends on the use of fact nodes, such as the fact node for John gives Mary a book, to detect the synchrony of activation between arguments and thematic roles. The use of fact nodes is needed because synchrony of activation has to be detected to process the information that it encodes (Dennett 1991). But fact nodes, and the circuits that activate them, are similar to the specialized neurons and circuits illustrated in figure 1. It is excluded to have such nodes and circuits for all possible verb-argument bindings that can occur in language, in particular for novel instances of verb-argument binding. As a result, synchrony of activation as a binding mechanism fails to provide the productivity given by combinatorial structures.</p>
<p>The problems analyzed here, the inability to solve the problem of 2 , the inability to deal with nested structures, and the lack of systematicity and productivity, are also found in other domains in which synchrony of activation is used as a binding mechanism, such as visual cognition (Van der Velde \&amp; de Kamps 2002).</p>
<h2>4. Processing linguistic structures with recurrent neural networks</h2>
<p>The argument that combinatorial structures are needed to obtain productivity in cognition has been questioned (Elman 1991; Churchland 1995, Port \&amp; Van Gelder 1995). In this view, productivity in cognition can be obtained in a 'functional' manner ('functional compositionality', Van Gelder 1990), without using explicit combinatorial structures. The most elaborate approach of this kind is found in the processing of linguistic structures with recurrent neural networks (Elman 1991; Miikkulainen 1996; Christiansen \&amp; Chater 2001; Palmer-Brown et al. 2002).</p>
<p>A recurrent neural network (RNN) is a multilayer (usually three-layer) feedforward network, in which the activation pattern in the hidden (middle) layer is copied back to the input layer, where it serves as part of the input to the network in the next learning step. In this way, RNNs are capable of processing and memorizing sequential structures. Elman (1991) used RNNs to predict what kind of word would follow next at a given point in a sentence. For instance, in case of the sentence Boys who chase boys feed cats, the network had to predict that after Boys who chase a noun would follow, and that after Boys who chase boys a plural verb would occur. To perform this task, the network was trained with sentences from a language generated with a small lexicon and a basic phrase grammar. The network succeeded in this task, both for the sentences that were used in the training session and with other sentences from the same language.</p>
<p>A more complex model was presented by Miikkulainen (1996). The model consisted of multiple parts (including a 'parser'), based on RNNs. The purpose of the model was to assign thematic roles (agent, act, patient) to the words in a clause. The model succeeded in this task, even with embedded clauses (however, clauses were restricted to two or three word clauses, which resulted from the fact that the output layer of the parser had three nodes).</p>
<p>Thus, it seems that RNNs are capable to process linguistic structures in a noncombinatorial manner. However, as Christiansen and Chater (2001) noted, all RRNs model languages derived from small vocabularies (in the order of 10 to 100 words). In contrast, the vocabulary of natural language is huge, which results in an 'astronomical'</p>
<p>productivity when combined with even limited sentence structures (e.g., sentences with 20 words or less, see section 2.1.). Therefore, I will discuss this form of 'combinatorial' productivity in the case of language processing with RNNs in more detail.</p>
<h1>4.1. Combinatorial productivity with RNNs</h1>
<p>In Elman (1991), the RNN was trained and tested with a language in the order of $10^{5}$ sentences, based on a lexicon of about 20 words. In contrast, the combinatorial productivity of natural language is in the order of $10^{20}$ sentences or more, based on a lexicon of $10^{5}$ words. A basic aspect of such a combinatorial productivity is the ability to insert words from one familiar sentence context into another. For instance, if one learns that Dumbledore is headmaster of Hogwarts, one can also understand Dumbledore chases the mouse, or The dog sees Hogwarts, even though these specific sentences have not been encountered before. RNNs should have this capability as well, if they are to approach the combinatorial productivity of natural language.</p>
<p>Using the prediction task of Elman (1991), we investigated this question by testing the ability of RNNs to recognize a sentence consisting of a new combination of familiar words in familiar syntactic roles (Van der Velde et al. 2003). In one instance, we used sentences like dog hears cat, boy sees girl, dog loves girl and boy follows cat to train the network on the word prediction task. The purpose of the training sentences was to familiarize the RNNs with dog, cat, boy and girl as arguments of verbs. Then, a verb like hears from dog hears cat was inserted into another trained sentence like boy sees girl to form the test sentence boy hears girl, and the networks were tested on the prediction task for this sentence.</p>
<p>To strengthen the relations between boy, hears and girl, we also included training sentences like boy who cat hears obeys John and girl who dog hears likes Mary. These sentences introduce boy and hears, and girl and hears, in the same sentence context (without using boy hears and hears girl) ${ }^{4}$. In fact, girl is the object of hears in girl who dog hears likes Mary, as in the test sentence boy hears girl.</p>
<p>However, although the RRNs learned the training sentences to perfection, they failed with the test sentences. Despite the ability to process boy sees girl and dog hears cat, and even girl who dog hears likes Mary, they could not process boy hears girl. The behavior of the RNNs with the test sentence boy hears girl was in fact similar to the behavior in a 'word salad' condition, which consisted of random word strings, based on the words used in the training session. Analysis of this 'word salad' condition showed that the RNNs predicted the next word on the basis of direct word-word associations, based on all twoword combinations found in the training sentences. The similarity between 'word salads' and the test sentence boy hears girl suggests that RNNs resort to word-word associations when they have to process novel sentences composed of familiar words in familiar grammatical structures.</p>
<p>The results of these simulations indicate that RNNs do not posses a minimal form of the combinatorial productivity that underlies human language processing. To put this in perspective, it is important to realize that the lack of combinatorial productivity observed in these simulations is not just a negative result, that could have been avoided by using a better learning (training) algorithm. The training sentences were learned to perfection. The best that another algorithm could do is to learn these sentences to the same level of perfection. It is unclear how this could produce a different result on the test sentences.</p>
<p>Furthermore, the crucial issue here is not learning, but the contrast in behavior exhibited by the RNNs in these simulations. The RRNs were able to process ('understand') boy sees girl and dog hears cat, and even girl who dog hears likes Mary, but not boy hears girl. This contrast in behavior is not found in humans, regardless of the learning procedure used. It is not found in human behavior due to the structure of the human language system. This is what the issue of systematicity is all about: if you understand boy sees girl, dog hears cat and girl who dog hears likes Mary, you cannot but understand boy hears girl. Any failure to do so would be regarded as pathological ${ }^{5}$.</p>
<h1>4.2. RNNs and the massiveness of the binding problem</h1>
<p>The simulations discussed above again show that RNNs are capable of processing learned sentences like girl who dog hears obeys Mary, and other complex sentence structures. Thus, even though RRNs fail in terms of combinatorial productivity, they could be used to process sentence structures in abstract terms. That is, they could process a sentence structure in terms of Nouns $(N)$ and Verbs $(V)$, such as $N$-who- $N-V-V-N$ in the case of sentences like girl who dog hears obeys Mary.</p>
<p>Sentence processing in terms of $N-V$ strings can be related with the word assemblies illustrated in figure 1. Words of a similar category, like verbs or nouns, would have a common part in their cell assemblies that reflects that they are verbs or nouns. The RRNs could be trained to process sentences in terms of these common parts, thus in terms of $N$ $V$ strings. However, when used in this way, RRNs can only be a part of a neural model of human language performance. Consider, for instance, the sentences cat chases mouse and mouse chases cat. Both sentences are $N-V-N$ sentences, and thus indistinguishable for these RRNs. Yet, the two sentences convey very different messages, and humans can understand these differences. In particular, they can produce the correct answers to the 'who does what to whom' questions for each of these sentences, which cannot be answered on the level of the $N-V-N$ structure processed by RRNs.</p>
<p>This raises two important questions for the use of RRNs in this manner. First, how is the difference between cat chases mouse and mouse chases cat instantiated in neural terms? The lack of combinatorial productivity discussed above shows that this cannot be achieved with RRNs. Second, given a neural instantiation of cat chases mouse and mouse chases cat, how can the structural $N-V$ information processed by the RRNs be related with the specific content of each sentence? This is a 'binding' problem, because it requires that, for instance, the first $N$ in $N-V-N$ is bound to cat in the first sentence and to mouse in the second sentence.</p>
<p>However, even if these problems are solved, sentence processing in terms of $N-V$ strings is still faced with serious difficulties, as illustrated with the following sentences:</p>
<p>The cat that the dog that the boy likes bites chases the mouse
The fact that the mouse that the cat chases roars surprises the boy
The abstract $(N-V)$ structure of both sentences is the same: $N$-that- $N$-that- $N-V-V-V-N$. Yet, there is a clear difference in complexity between these sentences (Gibson 1998). Sentences with complement clauses (2) are much easier to process than sentences with center-embeddings (1). This difference can be explained in terms of the bindings (dependencies) within the sentence structures. In (1) the first noun is related with the</p>
<p>second verb as its object (theme) and with the third verb as its subject (agent). In (2), the first noun is only related with the third verb (as its subject). This difference in structural dependency (binding) is not captured in the sequence $N$-that- $N$-that- $N-V-V-V-N$.</p>
<p>The structural dependencies that constitute the difference between sentences (1) and (2) again illustrate the massiveness of the binding problem that occurs in linguistic structures. Words and clauses have to be bound correctly to other words and clauses in different parts of the sentence, in line with the hierarchical structure of a sentence. These forms of binding are clearly beyond the capacity of language processing with RNNs. Similar limitations are found with RNNs in case of the problem of variables (Marcus 2001).</p>
<h1>5. Blackboard architectures of combinatorial structures</h1>
<p>A combinatorial structure consists of parts (constituents) and their relations. Briefly stated, one could argue that the lack of combinatorial productivity with RNNs, as discussed above, illustrates a failure to encode the individual parts (words) of a combinatorial structure (sentence) in a productive manner. In contrast, synchrony of activation fails in particular to instantiate even moderately complex relations in the case of variable binding. These examples show that neural models of combinatorial structures can only succeed if they provide a neural instantiation of both the parts and the relations of combinatorial structures.</p>
<p>In computational terms, a blackboard architecture provides a way to instantiate the parts and the relations of combinatorial structures. A blackboard architecture consists of a set of specialized processors (or 'demons', Selfridge 1959) that interact with each other by means of a blackboard (or 'workbench', or 'bulletin board'). Each processor can process and modify the information that is stored on the blackboard. In this way, the architecture can process or produce information that exceeds the ability of each individual processor. In the case of language, one could have processors for the recognition of words and (other) processors for the recognition of specific grammatical relations. These processors could then communicate by using a blackboard in the processing of a sentence. Thus, with the sentence The little star is beside a big star, the word processors could store the symbol for star on the blackboard, the first time in combination with the symbol for little, and the second time in combination with the symbol for big. Other processors could then determine the relation (beside) between these two copies of the symbol for star. Jackendoff (2002) discusses blackboard architectures of this kind for phonological, syntactic and semantic structures.</p>
<p>In the next section, I will propose and discuss a neural blackboard architecture for sentence structure based on neural assemblies. To address the problems described by Jackendoff (2002), neural word assemblies are not copied in this architecture. Instead, they are temporarily bound to the neural blackboard, in a manner that distinguishes between different occurrences of the same word, and that preserves the relations between the words in the sentence. For instance, with the sentence The cat chases the mouse, the word assembly for cat is bound to the blackboard as the subject or agent of chases, and the assembly for mouse is bound as the object or theme of this verb.</p>
<p>With the neural structure of The cat chases the mouse, the architecture can produce correct answers to questions like "Who chases the mouse?" or "Whom does the cat chase?". Questions like these can be referred to as 'binding questions', because they test</p>
<p>the ability of an architecture to 'bind' familiar parts in a (potentially novel) combinatorial structure. A neural instantiation of a combinatorial structure such as The cat chases the mouse fails if it cannot produce the correct answers to the questions stated above. In language, binding questions in fact query 'who does what to whom' information, which is the characteristic form of information provided by a sentence (e.g., Pinker 1994; Calvin \&amp; Bickerton 2000). Aphasic patients, for instance, are tested on their language abilities using non-verbal 'who does what to whom' questions (e.g., Caplan 1992). In general, the ability to answer binding questions is of fundamental importance for cognition, because it is related with the ability to select information needed for purposive action (e.g., Van der Heijden \&amp; van der Velde 1999).</p>
<h1>6. A neural blackboard architecture of sentence structure</h1>
<p>In line with Pulvermüller (1999), words are assumed to be encoded in terms of neural 'word' assemblies, as illustrated in figure 1 (section 2.1.). It is clear that the relations between the words in a sentence cannot be encoded in terms of direct associations between word assemblies. For instance, the association of mouse-chases-cat does not distinguish between the sentences The mouse chases the cat and The cat chases the mouse.</p>
<p>However, relations between words can be encoded, and the problems discussed by Jackendoff (2002) can be solved, if word assemblies are embedded in a neural architecture in which structural relations can be formed between the word assemblies. A neural architecture of this kind can be formed by means of 'structure' assemblies that interact with the word assemblies. The structure assemblies provide the possibility to encode different instantiations of the same word assembly (thereby solving the 'problem of 2 '), and they can be used to bind word assemblies in terms of the syntactic structure of the sentence.</p>
<p>Figure 2 illustrates the neural structure of the sentence The mouse chases the cat in this architecture. It consists of word assemblies, structure assemblies for noun phrases (NPs) and verb phrases (VPs), gating circuits used for dynamic control, and memory circuits used to bind assemblies temporarily. In figure 2, the assemblies for mouse and cat are bound to NP assemblies ( $\mathrm{N}<em 2="2">{1}$ for cat and $\mathrm{N}</em>}$ for mouse), and the assembly for chases is bound a VP assembly $\left(\mathrm{V<em _mathrm_i="\mathrm{i">{1}\right)$. The structure assemblies are then bound to each other, in a manner that encodes the verb-argument structure of the sentence. For this purpose, each structure assembly in the architecture is composed of a main assembly ( $\mathrm{N}</em>}}$ for NP assemblies and $\mathrm{V<em 1="1">{\mathrm{i}}$ for VP assemblies) and one or more subassemblies. In figure 2, the NP and VP assemblies have subassemblies for the arguments agent (a) and theme (t) ${ }^{6}$. To encode cat as the agent of chases, $\mathrm{N}</em>}$ is bound with $\mathrm{V<em 2="2">{1}$ by means of their agent subassemblies. In turn, $\mathrm{N}</em>$ are bound with their theme subassemblies, to provide the neural structure for mouse as the theme of chases.}$ and $\mathrm{V}_{1</p>
<p>Main assemblies and subassemblies are assumed to have the ability for reverberating activity, in line with the reverberating activity found in the prefrontal cortex (e.g., Fuster 1973; Amit 1995; Durstewitz et al. 2000). As a result, they will remain active for a while after they have been activated, unless they are inhibited. Subassemblies are connected to main assemblies by means of gating circuits, which control the flow of activation within structure assemblies. For instance, a main assembly can be active, but its subassemblies not, or vice versa. The ability to control the internal dynamics of structure assemblies is</p>
<p>of crucial importance for the neural architecture of sentence structure proposed here. Before illustrating this in more detail, I will first discuss the gating and memory circuits used in this architecture.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 2. Illustration of the neural sentence structure of cat chases mouse in the neural blackboard architecture presented here. The words are encoded with the word assemblies illustrated in figure 1 (section 2.1.). Sentence structure is encoded with 'structure assemblies' for noun-phrases (NP assemblies) and verb-phrases (VP assemblies). A structure assembly consists of a main assembly and a number of subassemblies, connected to the main assembly by means of gating circuits. The labeled subassemblies represent the thematic roles of agent (a), and theme (t). Binding between assemblies is achieved with active memory circuits. Here, the assembly for cat is bound to the NP assembly $\mathrm{N}<em 1="1">{1}$, the assembly for chases is bound to the VP assembly $\mathrm{V}</em>}$, and the assembly for mouse is bound to the NP assembly $\mathrm{N<em 1="1">{2} . \mathrm{N}</em>}$ and $\mathrm{V<em 1="1">{1}$ are bound by means of their agent subassemblies and $\mathrm{V}</em>$ are bound by means of their theme subassemblies.}$ and $\mathrm{N}_{2</p>
<h1>6.1. Gating and memory circuits</h1>
<p>A gating circuit in the architecture consists of a disinhibition circuit, as described by Gonchar and Burkhalter (1999). Figure 3 (left) illustrates a gating circuit in the direction from assembly $X$ to assembly $Y$. The circuit controls the flow of activation between the two assemblies by means of an external control signal. It operates in the following manner. If the assembly $X$ is active, it activates an inhibition neuron (or group of neurons) $i_{x}$, which inhibits the flow of activation from $X$ to $X_{\text {out }}$. When $i_{x}$ is inhibited by</p>
<p>another inhibition neuron $\left(I_{x}\right)$, that is activated by an external control signal, $X$ activates $X_{\text {out }}$. In turn, $X_{\text {out }}$ activates $Y$. A gating circuit from $Y$ to $X$ operates in a similar manner. Control of activation can be direction specific. Thus, by producing a control signal in the direction from $X$ to $Y$, activation will flow in this direction (if $X$ is active), but not in the direction from $Y$ to $X$. The symbol illustrated in figure 3 (left) will be used to represent the combination of gating circuits in both directions (as in figure 2).</p>
<h1>Gating Circuit</h1>
<p><img alt="img-4.jpeg" src="img-4.jpeg" />
(X to Y and Y to X )</p>
<p>Memory Circuit
<img alt="img-5.jpeg" src="img-5.jpeg" />
(X to Y and Y to X )</p>
<p>Figure 3. Left: A gating circuit in the direction from assembly X to assembly Y , based on a disinhibition circuit. The large circles depict neural assemblies. The small circles depict (groups of) inhibitory neurons (i). A combination of two gating circuits in the directions X to Y and Y to X is depicted in other figures with the symbol illustrated at the bottom. Right: A memory (gating) circuit in the direction from assembly X to assembly Y , based on a gating circuit with a delay assembly for control. A combination of two memory circuits in the directions X to Y and Y to X is depicted in other figures with the symbols illustrated at the bottom, one for the inactive state and one for the active state of this combined memory circuit.</p>
<p>A memory circuit in the architecture consists of a gating circuit in which the control signal results from a 'delay assembly'. Figure 3 (right) illustrates a memory circuit in the direction of $X$ to $Y$. However, each memory circuit in the architecture in fact consists of two such circuits in both directions ( $X$ to $Y$ and $Y$ to $X$ ). The delay assembly (that controls the flow of activation in both directions) is activated when $X$ and $Y$ are active simultaneously (see below), and it remains active for a while due to the reverberating</p>
<p>nature of the activation in this assembly. As a result, a memory circuit can be in two states: active and inactive. Each state will be represented with the symbol illustrated in figure 3 (right). If the memory circuit is inactive, activation cannot flow between the assemblies connected by the memory circuit. On the other hand, if the memory circuit is active, activation will flow between the assemblies it connects, if one of these assemblies is activated. In this way, an active memory circuit binds the two assemblies it connects. The memory circuits in figure 2 are active, so that word assemblies and structure assemblies are bound in line with the structure of the sentence.</p>
<h1>6.2. Overview of the architecture</h1>
<p>Figure 4 illustrates the part of the architecture in which nouns can be bound as arguments to verbs. This part is illustrative of the architecture as a whole.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 4. A neural blackboard architecture for verb-argument binding. Word assemblies for verbs are connected to the main assemblies of VP structure assemblies by means of (initially) inactive memory circuits. Word assemblies for nouns are connected to the main assemblies of NP structure assemblies by means of (initially) inactive memory circuits. The agent (a) and theme (t) subassemblies of the VP and NP structure assemblies are connected by means of (initially) inactive memory circuits. Only subassemblies of the same kind are connected to each other. The main assemblies of VP assemblies are mutually inhibitory. Likewise for NP structure assemblies.</p>
<p>Each noun (word) assembly is connected to the main assembly of each NP assembly by means of a memory circuit, which is initially inactive. Likewise, each verb (word) assembly is connected to the main assembly of each VP assembly by means of an initially inactive memory circuit. Main assemblies of the same kind are mutually inhibitory. Each NP and VP main assembly is connected to a number of subassemblies by means of gating circuits. The gating circuits can be activated in a selective manner by neural control circuits (not shown in the figure). For instance, the gating circuits between the main assemblies and the agent subassemblies can be activated without activating the gating circuits for the theme subassemblies. Finally, all subassemblies of the same kind are connected by means of memory circuits. For instance, each agent subassembly of the NP assemblies is connected to each agent subassembly of the VP assemblies by means of an (initially inactive) memory circuit.</p>
<p>In the processing of a sentence it is assumed that one of the NP assemblies will be activated whenever the assembly for a noun is activated. It is arbitrary which of the NP assemblies is activated, provided that the assembly is 'free'. A structure assembly is 'free' when it is not already bound to a sentence structure, that is, when all memory circuits connected with that assembly are inactive ${ }^{7}$. As illustrated in figure 4, only one NP main assembly can be active at the same time, due to the competition between the NP main assemblies that results from their mutual inhibition. It is assumed that the active NP assembly will remain active until a new NP assembly is activated by the occurrence of a new noun in the sentence ${ }^{8}$. The selection of a VP assembly proceeds in the same manner.</p>
<p>In all, in the order of $10^{2} \mathrm{VP}$ assemblies and $10^{2} \mathrm{NP}$ assemblies would probably be needed in this architecture. When a sequence of structure assemblies has been activated, the first assemblies in the sequence will return to the inactive state (i.e., will again be 'free'), due to the decay of delay activity in the memory circuits connected with these assemblies. In this way, only a subset of the structure assemblies will be concurrently active in the blackboard architecture.</p>
<h1>6.2.1. Connection structure for binding in the architecture</h1>
<p>Figure 5 (right) illustrates that the connection structure between the agent subassemblies in figure 4 basically consists of a matrix-like array of 'columns'. This connection structure is illustrative of every connection between assemblies by means of memory circuits. Each column contains a (combined) memory circuit (figure 3, right), including the delay assembly that can activate the memory circuit. Each column also contains a circuit that can activate the delay assembly (figure 5, left). This circuit is also a disinhibition circuit, in which the delay assembly will be activated if the neurons $\mathrm{N}<em _in="{in" _text="\text">{\mathrm{in}}$ and $\mathrm{V}</em>}}$ are active at the same time. The neurons $\mathrm{N<em _in="{in" _text="\text">{\text {in }}$ and $\mathrm{V}</em>$, in turn, are activated by the respective agent subassemblies of a NP assembly and a VP assembly.}</p>
<p>The activated agent subassembly of a given NP assembly activates the $\mathrm{N}<em _mathrm_x="\mathrm{x">{\text {in }}$ neurons in a horizontal row of columns (as illustrated with $\mathrm{N}</em>}}$ in figure 5, right). Likewise, the activated agent subassembly of a given VP assembly activates the $\mathrm{V<em _mathrm_i="\mathrm{i">{\text {in }}$ neurons in a vertical row of columns (as illustrated with $\mathrm{V}</em>}}$ in figure 5, right). The delay assembly in the column on the intersection of both rows will be activated if the agent subassemblies of $\mathrm{N<em _mathrm_i="\mathrm{i">{\mathrm{x}}$ and $\mathrm{V}</em>$ are active at the same time. This results in the binding of these agent subassemblies (illustrated with the shaded memory circuit symbol in figure 5, right).}</p>
<p>The columns within each horizontal and vertical row (figure 5, right) are mutually inhibitory. Inhibition is initiated by the active delay assemblies ${ }^{9}$ (figure 5, left). Thus, when the agent subassemblies of $\mathrm{N}<em _mathrm_i="\mathrm{i">{\mathrm{x}}$ and $\mathrm{V}</em>}}$ are bound by an active memory circuit, the active delay assembly in their mutual column initiates the inhibition of all columns in the same horizontal and vertical row. This prevents a second binding of $\mathrm{N<em _mathrm_i="\mathrm{i">{\mathrm{x}}$ with another VP assembly, or of $\mathrm{V}</em>$ with another NP assembly, by means of agent subassemblies.
}<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 5. Connection structure for the agent subassemblies in figure 4. Left: a delay assembly in a memory circuit (figure 3, right) is activated when the subassemblies connected by the memory circuit are concurrently active (using a disinhibition circuit). Right: Each agent subassembly of all NP assemblies is connected to each agent subassembly of all VP assemblies with a specific 'column' in an array of columns. Each column consists of the memory circuits that connect both subassemblies, together with the circuit in figure 5 (left). The active subassembly of $\mathrm{N}<em _mathrm_in="\mathrm{in">{\mathrm{x}}$ will activate all $\mathrm{N}</em>}}$ neurons in its horizontal row of columns. Likewise, the active subassembly of $\mathrm{V<em _in="{in" _text="\text">{\mathrm{i}}$ will activate all $\mathrm{V}</em>$ neurons in its vertical row of columns. This results in the activation of the delay assembly in the (combined) memory circuit in their corresponding column. Columns in horizontal and vertical rows are mutually inhibitory. Inhibition is initiated by active delay assemblies in the memory circuits.}</p>
<p>The binding process of the sentence in figure 2 proceeds as follows. When the assembly for cat is activated, a NP assembly is activated as well, and the assembly for cat is bound to this NP assembly by the activated memory circuit that connects the two assemblies. In the same manner, the assembly for chases will be bound to a VP assembly.</p>
<p>To achieve the binding of cat as the agent of chases, the gating circuits between the NP and VP main assemblies and their agent subassemblies have to be activated. The active NP and VP main assemblies (i.e., the NP and VP main assemblies bound to cat and chases) will then activate their agent subassemblies. In line with the process illustrated in figure 5, this will result in the binding of these two agent subassemblies.</p>
<p>In this architecture, it is assumed that gating circuits are activated by neural control circuits. Basically, the control circuits instantiate syntactic operations, based on the active word assemblies and the activation state of the blackboard. In the example above, these circuits will detect that in a sequence like cat chases (or $N-V$ ), cat is the agent of the verb chases. In response, they will activate the gating circuits for the agent subassemblies of all NPs and VPs. This results in a binding between the active NP assembly and the active VP assembly by means of their agent subassemblies, in the manner discussed above. The binding of mouse as the theme of chases proceeds in a similar manner. I will discuss the operations of these control circuits in somewhat more detail later on. First, however, I will discuss how this neural blackboard architecture for sentence structure can solve the 'four challenges for cognitive neuroscience' presented by Jackendoff (2002), and discussed in section 2 .</p>
<h1>6.3. Multiple instantiation and binding in the architecture</h1>
<p>Figure 6 illustrates the neural structures of the sentences The cat chases the mouse, The mouse chases the cat and The cat bites the dog in the neural blackboard architecture. The words cat, mouse and chases occur in more than one sentence, which creates the problem of multiple instantiation (the problem of 2) for the assemblies of these words. Figure 6 shows that the problem of multiple instantiation is solved by binding each word assembly to a unique structure assembly. For instance, the word assembly for cat is bound to the NP assemblies $\mathrm{N}<em 4="4">{1}, \mathrm{~N}</em>}$ and $\mathrm{N<em 1="1">{5}$. Similarly, different VP assemblies ( $\mathrm{V}</em>}$ and $\mathrm{V<em 1="1">{2}$ ) encode the verb chases in different sentences. The different structure assemblies can be bound in line with the structure of each sentence. In this way, cat can be the agent of chases in one sentence (by binding $\mathrm{N}</em>}$ and $\mathrm{V<em 4="4">{1}$ with their agent subassemblies) and the theme of chases in another sentence (by binding $\mathrm{N}</em>}$ and $\mathrm{V<em 5="5">{2}$ with their theme subassemblies). Furthermore, cat can also be the agent of another verb (bites) in a third sentence, using $\mathrm{N}</em>$. This illustrates how the binding problem in language can be solved (on the level of verbargument binding), even with multiple instantiations of words.</p>
<p>The internal structure of the NP and VP assemblies, given by the gating circuits, is of crucial importance in this respect. Without this internal structure, the neural structures in figure 6 would collapse into direct associations between neural assemblies, which would result in a failure to distinguish between, for instance, The cat chases the mouse and The mouse chases the cat. With the control of activation provided by gating circuits, the neural structures of these two sentences can be selectively (re)activated.</p>
<h3>6.3.1. Answering binding questions</h3>
<p>Selective reactivation of a sentence structure is necessary to retrieve information from the blackboard architecture. In particular, to answer specific binding questions, such as the question "Whom does the cat chase?". The question provides the information that cat is the agent of chases and it asks for the theme of chases in that sentence (i.e., it asks for $x$ in the sentence cat chases $x$ ). The production of the answer consists of the selective</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-288 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-288</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-288</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-14.html">extraction-schema-14</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <p><strong>Paper ID:</strong> paper-a35d5aeba08cccdc5cdf26bc094ccd71d06bdc99</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/a35d5aeba08cccdc5cdf26bc094ccd71d06bdc99" target="_blank">Exploring Generalization Ability of Pretrained Language Models on Arithmetic and Logical Reasoning</a></p>
                <p><strong>Paper Venue:</strong> Natural Language Processing and Chinese Computing</p>
                <p><strong>Paper TL;DR:</strong> This research finds that the PLMs can easily generalize when the distribution is the same, however, it is still difficult for them to generalize out of the distribution.</p>
                <p><strong>Paper Abstract:</strong> To quantitatively and intuitively explore the generalization ability of pre-trained language models (PLMs), we have designed several tasks of arithmetic and logical reasoning. We both analyse how well PLMs generalize when the test data is in the same distribution as the train data and when it is different, for the latter analysis, we have also designed a cross-distribution test set other than the in-distribution test set. We conduct experiments on one of the most advanced and publicly released generative PLM - BART. Our research finds that the PLMs can easily generalize when the distribution is the same, however, it is still difficult for them to generalize out of the distribution.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e288.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e288.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BART</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BART-Large (denoising sequence-to-sequence pretrained model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An encoder-decoder pretrained language model (BART-Large) fine-tuned on synthetic datasets of counting, listing, addition, subtraction, comparison and symbolic logic to probe arithmetic and logical generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BART</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>encoder-decoder transformer (sequence-to-sequence)</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, counting (sequence length), comparison (greater/less/equal), plus symbolic logic reasoning (boolean expressions)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Training: primarily 3-digit numbers for addition/subtraction/comparison; Counting training lengths 10-99; Symbolic logic training expressions with 6-10 boolean components. Cross-distribution tests include 2-digit and 4-digit operands, 2- or 4-digit answers, answers like 4-digit sums, counting lengths 1-9 and 100-1000, and symbolic logic with 1-5 or 11-15 components.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Supervised fine-tuning of pre-trained BART-Large on synthetic task-specific datasets; generative decoding; digits separated by spaces in inputs/outputs; no external calculator or specialized numerical module.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>In-distribution: addition reaches ~99% accuracy with 1,600 training samples and near 100% with several hundred samples for many tasks; with only 160 samples addition ~40% accuracy. In-distribution results: Counting and many tasks reach ~100% when training data is sufficient. Cross-distribution (selected model checkpoints that achieved 100% in-distribution): Addition — question cross-dist: 15/1,500 (1.0%); answer cross-dist: 0/1,500 (0%); instance cross-dist: 0/1,000 (0%). Subtraction — question cross-dist: 13/1,500 (0.87%); answer cross-dist: 0/1,500 (0%); instance cross-dist: 1/1,000 (0.1%). Counting — question cross-dist: 316/320 (98.8%); instance cross-dist: 0/1,710 (0%). Comparison — question cross-dist: 2,555/5,600 (45.63%). Symbolic logic — question cross-dist: 2,200/2,200 (100%).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Paper reports no evidence that BART learned algorithmic arithmetic; authors conclude the model "cannot understand the underlying rules" for arithmetic and likely relies on memorization or spurious correlations. Specific observed weaknesses implicate failures on carrying in addition and borrowing in subtraction. Generation instability (e.g., Listing task) suggests model fluency issues rather than numeric reasoning. Authors note insensitivity to sequence length when basic components unchanged and that inserting spaces between digits / tokenization choices were applied, but no internal mechanistic attribution (e.g., attention-head specialization) is demonstrated.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Performance improves with number of fine-tuning examples (e.g., addition from ~40% at 160 examples to ~99% at 1,600); no analysis of scaling with model parameter count is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Severe failure when test answer distribution differs from training (e.g., answers with more/fewer digits); fails on instances requiring carries/borrowing; fails on cross-distribution instances (virtually 0% for many addition/subtraction cross-tests); unstable token-level generation for Listing leading to near-zero accuracy; tends to succeed only when test examples share distributional properties (or memorized pieces) with training.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Comparisons in the paper are between in-distribution vs various cross-distribution tests (question-varying, answer-varying, instance-varying). Also an overlap analysis (question/answer/instance overlap between train and test) is reported; and a small case study comparison to GPT-3 is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>While BART-Large can be fine-tuned to near-perfect performance on in-distribution arithmetic tasks, it fails to generalize cross-distribution—especially when answers require carries/borrowing or different digit-length answers—indicating reliance on memorization/spurious patterns rather than learning algorithmic arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Generalization Ability of Pretrained Language Models on Arithmetic and Logical Reasoning', 'publication_date_yy_mm': '2021-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e288.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e288.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 (case study)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 (Generative Pre-trained Transformer 3)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large autoregressive language model probed via web API examples for arithmetic performance; authors ran informal case studies (no fine-tuning) to observe behavior on additions and subtractions of varying magnitude.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models are few-shot learners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>decoder-only transformer (autoregressive)</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition and subtraction (manual QA probes)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Small numbers up to ~1,000 reported to be handled perfectly in authors' probes; larger multi-million-digit examples tested informally (e.g., 8-digit operands) and produced inconsistent results.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Zero-shot/few-shot style API queries (no fine-tuning); ad-hoc prompts via OpenAI example interface; no engineered chain-of-thought or external tool use.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Authors report GPT-3 handled additions and subtractions with numbers up to 1,000 perfectly in their limited probes; performance degrades as numbers increase, with only some specific large instances answered correctly. Reported anomalies: correct answer observed for 12345678 + 87654321 (claimed to match a memorized common example), but inconsistent/wrong outputs for slightly different large inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Authors hypothesize GPT-3 does not perform algorithmic calculation but instead sometimes reproduces memorized arithmetic examples from training data (web), i.e., memorization of frequent arithmetic pairs rather than on-the-fly computation; no internal mechanistic analysis provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>No systematic scaling analysis; qualitative observation that ability holds for small magnitudes (≤1,000) but fails or is inconsistent for larger magnitudes.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Fails on larger numbers; produces inconsistent results for large arithmetic problems; appears to output memorized sums for frequent examples and incorrect sums for others.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared informally against BART fine-tuned results described elsewhere in the paper; GPT-3 case study was not part of the fine-tuned experimental suite.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>GPT-3 can produce correct small-number arithmetic in zero-shot probes but shows inconsistent behavior on larger numbers, consistent with memorization of frequent examples rather than robust arithmetic computation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Generalization Ability of Pretrained Language Models on Arithmetic and Logical Reasoning', 'publication_date_yy_mm': '2021-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>Do NLP models know numbers? probing numeracy in embeddings <em>(Rating: 2)</em></li>
                <li>Investigating the limitations of the transformers with simple arithmetic tasks <em>(Rating: 2)</em></li>
                <li>Injecting numerical reasoning skills into language models <em>(Rating: 2)</em></li>
                <li>Measuring arithmetic extrapolation performance <em>(Rating: 2)</em></li>
                <li>BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-288",
    "paper_id": "paper-a35d5aeba08cccdc5cdf26bc094ccd71d06bdc99",
    "extraction_schema_id": "extraction-schema-14",
    "extracted_data": [
        {
            "name_short": "BART",
            "name_full": "BART-Large (denoising sequence-to-sequence pretrained model)",
            "brief_description": "An encoder-decoder pretrained language model (BART-Large) fine-tuned on synthetic datasets of counting, listing, addition, subtraction, comparison and symbolic logic to probe arithmetic and logical generalization.",
            "citation_title": "BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension",
            "mention_or_use": "use",
            "model_name": "BART",
            "model_size": null,
            "model_architecture": "encoder-decoder transformer (sequence-to-sequence)",
            "arithmetic_operation_type": "addition, subtraction, counting (sequence length), comparison (greater/less/equal), plus symbolic logic reasoning (boolean expressions)",
            "number_range_or_complexity": "Training: primarily 3-digit numbers for addition/subtraction/comparison; Counting training lengths 10-99; Symbolic logic training expressions with 6-10 boolean components. Cross-distribution tests include 2-digit and 4-digit operands, 2- or 4-digit answers, answers like 4-digit sums, counting lengths 1-9 and 100-1000, and symbolic logic with 1-5 or 11-15 components.",
            "method_or_intervention": "Supervised fine-tuning of pre-trained BART-Large on synthetic task-specific datasets; generative decoding; digits separated by spaces in inputs/outputs; no external calculator or specialized numerical module.",
            "performance_result": "In-distribution: addition reaches ~99% accuracy with 1,600 training samples and near 100% with several hundred samples for many tasks; with only 160 samples addition ~40% accuracy. In-distribution results: Counting and many tasks reach ~100% when training data is sufficient. Cross-distribution (selected model checkpoints that achieved 100% in-distribution): Addition — question cross-dist: 15/1,500 (1.0%); answer cross-dist: 0/1,500 (0%); instance cross-dist: 0/1,000 (0%). Subtraction — question cross-dist: 13/1,500 (0.87%); answer cross-dist: 0/1,500 (0%); instance cross-dist: 1/1,000 (0.1%). Counting — question cross-dist: 316/320 (98.8%); instance cross-dist: 0/1,710 (0%). Comparison — question cross-dist: 2,555/5,600 (45.63%). Symbolic logic — question cross-dist: 2,200/2,200 (100%).",
            "mechanistic_insight": "Paper reports no evidence that BART learned algorithmic arithmetic; authors conclude the model \"cannot understand the underlying rules\" for arithmetic and likely relies on memorization or spurious correlations. Specific observed weaknesses implicate failures on carrying in addition and borrowing in subtraction. Generation instability (e.g., Listing task) suggests model fluency issues rather than numeric reasoning. Authors note insensitivity to sequence length when basic components unchanged and that inserting spaces between digits / tokenization choices were applied, but no internal mechanistic attribution (e.g., attention-head specialization) is demonstrated.",
            "performance_scaling": "Performance improves with number of fine-tuning examples (e.g., addition from ~40% at 160 examples to ~99% at 1,600); no analysis of scaling with model parameter count is provided.",
            "failure_modes": "Severe failure when test answer distribution differs from training (e.g., answers with more/fewer digits); fails on instances requiring carries/borrowing; fails on cross-distribution instances (virtually 0% for many addition/subtraction cross-tests); unstable token-level generation for Listing leading to near-zero accuracy; tends to succeed only when test examples share distributional properties (or memorized pieces) with training.",
            "comparison_baseline": "Comparisons in the paper are between in-distribution vs various cross-distribution tests (question-varying, answer-varying, instance-varying). Also an overlap analysis (question/answer/instance overlap between train and test) is reported; and a small case study comparison to GPT-3 is provided.",
            "key_finding": "While BART-Large can be fine-tuned to near-perfect performance on in-distribution arithmetic tasks, it fails to generalize cross-distribution—especially when answers require carries/borrowing or different digit-length answers—indicating reliance on memorization/spurious patterns rather than learning algorithmic arithmetic.",
            "uuid": "e288.0",
            "source_info": {
                "paper_title": "Exploring Generalization Ability of Pretrained Language Models on Arithmetic and Logical Reasoning",
                "publication_date_yy_mm": "2021-08"
            }
        },
        {
            "name_short": "GPT-3 (case study)",
            "name_full": "GPT-3 (Generative Pre-trained Transformer 3)",
            "brief_description": "A large autoregressive language model probed via web API examples for arithmetic performance; authors ran informal case studies (no fine-tuning) to observe behavior on additions and subtractions of varying magnitude.",
            "citation_title": "Language models are few-shot learners",
            "mention_or_use": "use",
            "model_name": "GPT-3",
            "model_size": null,
            "model_architecture": "decoder-only transformer (autoregressive)",
            "arithmetic_operation_type": "addition and subtraction (manual QA probes)",
            "number_range_or_complexity": "Small numbers up to ~1,000 reported to be handled perfectly in authors' probes; larger multi-million-digit examples tested informally (e.g., 8-digit operands) and produced inconsistent results.",
            "method_or_intervention": "Zero-shot/few-shot style API queries (no fine-tuning); ad-hoc prompts via OpenAI example interface; no engineered chain-of-thought or external tool use.",
            "performance_result": "Authors report GPT-3 handled additions and subtractions with numbers up to 1,000 perfectly in their limited probes; performance degrades as numbers increase, with only some specific large instances answered correctly. Reported anomalies: correct answer observed for 12345678 + 87654321 (claimed to match a memorized common example), but inconsistent/wrong outputs for slightly different large inputs.",
            "mechanistic_insight": "Authors hypothesize GPT-3 does not perform algorithmic calculation but instead sometimes reproduces memorized arithmetic examples from training data (web), i.e., memorization of frequent arithmetic pairs rather than on-the-fly computation; no internal mechanistic analysis provided.",
            "performance_scaling": "No systematic scaling analysis; qualitative observation that ability holds for small magnitudes (≤1,000) but fails or is inconsistent for larger magnitudes.",
            "failure_modes": "Fails on larger numbers; produces inconsistent results for large arithmetic problems; appears to output memorized sums for frequent examples and incorrect sums for others.",
            "comparison_baseline": "Compared informally against BART fine-tuned results described elsewhere in the paper; GPT-3 case study was not part of the fine-tuned experimental suite.",
            "key_finding": "GPT-3 can produce correct small-number arithmetic in zero-shot probes but shows inconsistent behavior on larger numbers, consistent with memorization of frequent examples rather than robust arithmetic computation.",
            "uuid": "e288.1",
            "source_info": {
                "paper_title": "Exploring Generalization Ability of Pretrained Language Models on Arithmetic and Logical Reasoning",
                "publication_date_yy_mm": "2021-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2
        },
        {
            "paper_title": "Do NLP models know numbers? probing numeracy in embeddings",
            "rating": 2
        },
        {
            "paper_title": "Investigating the limitations of the transformers with simple arithmetic tasks",
            "rating": 2
        },
        {
            "paper_title": "Injecting numerical reasoning skills into language models",
            "rating": 2
        },
        {
            "paper_title": "Measuring arithmetic extrapolation performance",
            "rating": 2
        },
        {
            "paper_title": "BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension",
            "rating": 2
        }
    ],
    "cost": 0.00985975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Exploring Generalization Ability of Pretrained Language Models on Arithmetic and Logical Reasoning</h1>
<p>Cunxiang Wang ${ }^{\text {A }}$, Boyuan Zheng ${ }^{\text {A }}$, Yuchen Niu ${ }^{\text {A }}$ and Yue Zhang ${ }^{\text {A }}{ }^{\circ}$<br>${ }^{\text {A }}$ Zhejiang University, China<br>${ }^{\text {A}}$ School of Engineering, Westlake University, China<br>${ }^{\circ}$ Institute of Advanced Technology, Westlake Institute for Advanced Study, China<br>${ }^{\diamond}$ Johns Hopkins University<br>${ }^{3}$ Imperial College London<br>{wangcunxiang, zhangyue, zhengboyuan, niuyuchen}@westlake.edu.cn</p>
<h4>Abstract</h4>
<p>To quantitatively and intuitively explore the generalization ability of pre-trained language models (PLMs), we have designed several tasks of arithmetic and logical reasoning. We both analyse how well PLMs generalize when the test data is in the same distribution as the train data and when it is different, for the latter analysis, we have also designed a cross-distribution test set other than the in-distribution test set. We conduct experiments on one of the most advanced and publicly released generative PLM - BART. Our research finds that the PLMs can easily generalize when the distribution is the same, however, it is still difficult for them to generalize out of the distribution.</p>
<h2>1 Introduction</h2>
<p>Neural networks have shown strong capabilities in a range of NLP tasks (Sutskever et al., 2014; Vaswani et al., 2017). Recently, pretrained language models (PLMs) have achieved significantly levels of performance gains on many benchmark datasets (Devlin et al., 2019; Lewis et al., 2020a; Radford et al., 2019). Recently, some work shows that neural networks are lack of generalization ability in mathematical and logical reasoning (Nogueira et al., 2021; Madsen and alexander rosenberg johansen, 2019). This can lead to more understanding of the limitation of existing models and motivate future work. However, no work has been done to quantitatively or intuitively explore the conditions under which PLMs can generalize, in terms of whether PLMs can understand the internal mathematical rules and logical rules. The example of mathematical rules is shown in Figure 1. We suppose that if the model can effectively learn the underlying rules of Addition and Subtraction</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Example mathematical rules for Addition and Subtraction. If the model can master these rules, we suppose it can generalize well on all two-number addition and subtraction samples.
when giving sufficient training data, it can generalize to all two-number addition and subtraction calculation.</p>
<p>To this end, we conduct quantitative insights by designing a series of tasks for simple mathematical operations and logical reasoning, which includes numbering, addition, subtraction, comparison, and symbolic logic. We construct a set of corresponding datasets, where instances are in the form of text or mathematical expressions. Some examples are shown in the next section. For example, in the Addition task, ' $100+200$ ' is the question and ' 300 ' is the answer.</p>
<p>There are various types of generalization (Linzen, 2020; Lake and Baroni, 2018), such as question generalization on distribution differences between training set and test set (Wallace et al., 2019), and answer generalization on distribution differences between training set and test set (Nogueira et al., 2021). For example, in the Addition task, if the question and answer numbers in training data are of three-digit, but the question and answer numbers in the testing data are of twoor four-digit, they are in different distribution. To cover each type of generalization, we use different kinds of tasks and corresponding dataset. For example, we use addition to test the generalization on the question distribution differences data between</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The Numbering task has two subtasks, namely Counting and Listing.</p>
<p>training and testing. In this task, the numbers in the training set and development have three digits. However, the numbers in test set is set to consist of two, three, and four digits.</p>
<p>We conduct experiments using BART <em>Lewis et al. (2020a)</em> since they can generate arbitrary text sequences and have been shown to achieve the state-of-art results on numerous Natural Language Processing (NLP) tasks. For each task, we fine-tune BART with training data, validate on the development set and finally evaluate on the test set. We find that strong PLMs can address simple generalization of the same answer distribution for counting, arithmetic and logic tasks. But they cannot master the underlying rules of arithmetic reasoning, for example, the model trained on 3-digit addition can handle the addition expressions with 2-digit or 4-digit.</p>
<p>We will release all the code and data set for future study.</p>
<h2>2 Task</h2>
<p>We construct five tasks related to algebraic and logical reasoning, namely Numbering, Addition, Subtraction, Comparison, Symbolic Logic. In order to test the generalization ability of models on the data with the same distribution and on the data with the different distribution, we create an in-distribution dataset and a cross-distribution dataset for each task. The in-distribution dataset contains train set, development set and test set that are in the same distribution. The cross-distribution dataset only serves as the test set and it is in the different distribution in contrast to the in-distribution dataset. We believe that if the model can understand the underlying rules of arithmetic and logical Reasoning, it can both generalize well on in-distribution and cross-distribution test set.</p>
<h3>2.1 Numbering</h3>
<p>This task comprises two symmetric subtasks, namely Counting and Listing. Examples are</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>(b) Description of Subtraction task.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>(c) Description of Comparison task.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>(d) Description of Symbolic Logic task.</p>
<p>shown in Figure 2. The Counting task asks the model to count the number of characters in the input sequence. For example, 'A A A A A A' is a sequence with length '6'. The Listing task asks the model to output a list with a specific length and character. For example, the model receives a command 'Generate a list of 6 A' and the result is 'A A A A A A'.</p>
<h3>2.2 Addition</h3>
<p>The Addition task is the standard summation of two input numbers. In order to make sure that all numbers are in the same distribution during training, we use only the equations whose left-hand-side and right-hand-side are both <em>three digits</em> in the in-distribution dataset. We also adopt two-digit and four-digit numbers on both sides in cross-distribution test set to further test the generalization ability of models. One example is shown in Figure 3a.</p>
<h3>2.3 Subtraction</h3>
<p>The Subtraction task the standard tack(?) to subtract a subtrahend from a minuend. In order to make sure that all numbers are in same distribution during training, we use only equations whose left-hand-side and right-hand-side are both <em>three digits</em> in the in-distribution dataset. We also adopt two-digit and four-digit numbers on both sides in cross-distribution test set to further test the generalization ability of models. A example of Subtraction task is shown in Figure 3b.</p>
<h3>2.4 Comparison</h3>
<p>The Comparison task is to determine which of the two numbers is greater or smaller. In order to make sure all numbers are in same distribution during training, we use only equations whose left-hand-side and right-hand-side are both <em>three digits</em> in the in-distribution dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Train Set</th>
<th style="text-align: center;">Dev Set</th>
<th style="text-align: center;">In- + Cross- <br> Distribution <br> Test Set</th>
<th style="text-align: center;">In- + Cross- <br> Distribution <br> Dataset</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Numbering - Counting</td>
<td style="text-align: center;">3,744</td>
<td style="text-align: center;">468</td>
<td style="text-align: center;">$468+2,030$</td>
<td style="text-align: center;">$4,680+2,030$</td>
</tr>
<tr>
<td style="text-align: center;">Numbering - Listing</td>
<td style="text-align: center;">3,744</td>
<td style="text-align: center;">468</td>
<td style="text-align: center;">468</td>
<td style="text-align: center;">4,680</td>
</tr>
<tr>
<td style="text-align: center;">Addition</td>
<td style="text-align: center;">256,320</td>
<td style="text-align: center;">32,040</td>
<td style="text-align: center;">$32,040+4,000$</td>
<td style="text-align: center;">$320,400+4,000$</td>
</tr>
<tr>
<td style="text-align: center;">Subtraction</td>
<td style="text-align: center;">256,320</td>
<td style="text-align: center;">32,040</td>
<td style="text-align: center;">$32,040+4,000$</td>
<td style="text-align: center;">$320,400+4,000$</td>
</tr>
<tr>
<td style="text-align: center;">Comparison</td>
<td style="text-align: center;">648,000</td>
<td style="text-align: center;">81,000</td>
<td style="text-align: center;">$81,000+5,600$</td>
<td style="text-align: center;">$810,000+5,600$</td>
</tr>
<tr>
<td style="text-align: center;">Symbolic Logic</td>
<td style="text-align: center;">40,000</td>
<td style="text-align: center;">5,000</td>
<td style="text-align: center;">$5,000+2,200$</td>
<td style="text-align: center;">$50,000+2,200$</td>
</tr>
</tbody>
</table>
<p>Table 1: Data statistics of each task. For each task, we list the in-distribution dataset and cross-distribution test set.
hand-side and right-hand-side are both three digits in the in-distribution dataset. We also adopt two-digit and four-digit numbers on both sides in cross-distribution test set to further test the generalization ability of models. One example is shown in Figure 3c.</p>
<h3>2.5 Symbolic Logic</h3>
<p>As shown in Figure 3d, this task is to reason over symbolic logic expressions. The input question expression consists of six basic components, which are ' 0 ', ' 1 ', ' $\&amp;$ ', ' $\mid$ ', ' $\neg$ ' and ' $\rightarrow$ ', representing FALSE, TRUE, AND, OR, NOT and IMPLY, respectively. The output answer is either 0 or 1 , which represent FALSE and TRUE, respectively. This task asks the model to reason over the input logic expression and determine whether it is true or false.</p>
<p>In order to make sure all expressions are in the same distribution during training, we use only the expressions that contain $\mathbf{6}$ - 10 basic ' 0 ' and ' 1 ' components. For testing the generalization ability of models, we also adopt the some expressions with 1 - 15 basic ' 0 ' and ' 1 ' components in the test set.</p>
<p>Different from the other tasks, we select a subset from the overall dataset to serve as the indistribution dataset because the data is large. We take only 10,000 of expressions with X basic components, where X is a number between 6 - 10, respectively. So, we end up with 50,000 samples in the in-distribution dataset.</p>
<h3>2.6 Metrics</h3>
<p>We use Exact Match to compute accuracy for Numbering, Addition, Subtraction and Comparison tasks. However, for the Symbolic Logic task, since the answer distribution is unbalanced ( $84 \%$ answers are ' 1 '), we use the F1 score as the metric.</p>
<h2>3 Experiments</h2>
<p>In this section, we separate the generalization experiments to In-Distribution Generalization experiments and Cross-Distribution experiments. In
the former, the testing data is in the same distribution with the training data. In the latter, the testing data is in the different distribution from the training data. We suppose that if the model can master the underlying rules of the mathematical and logical reasoning, it should achieve $100 \%$ accuracy on both In-Distribution Generalization experiments and Cross-Distribution experiments.</p>
<p>We have organized the details of in-distribution data and cross-distribution data in this section. In addition, We also sorted out the examples of them and put the examples in the Appendix Table 1.</p>
<h3>3.1 Experimental Settings</h3>
<p>We adopt BART (Lewis et al., 2020a) namely due to the following reasons. First, it is a generative pretrained language model, which means that they can generate arbitrary sequences of tokens. This is essential for the addition and subtraction task. Second, it has achieved state-of-art results on numerous tasks and they has received much research attention. Last, it has released model checkpoints, thus it can be more standardized and more fair can evaluate them.</p>
<p>For the BART (Lewis et al., 2020a) model, we conduct experiments on the publicly released 'BART-Large' checkpoint ${ }^{1}$. We insert spaces between numbers while representing them in the data. For example, ' 111 ' is written as ' 111 ' both in the question and answer. For the character sequence in the Numbering task, we also insert spaces between the sequence, such as 'A A A'.</p>
<h3>3.2 In-Distribution Generalization</h3>
<p>In this subsection, we mainly explore models' generalization ability on test data which in the same distribution with train data. For the Counting subtask of the Numbering task, each question is a sequence with 10-99 same character which is one character among the alphabet; each answer is an</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" />
(a) The results of counting task.
<img alt="img-6.jpeg" src="img-6.jpeg" />
(c) The results of addition task.
<img alt="img-7.jpeg" src="img-7.jpeg" />
(e) The results of comparison task.
<img alt="img-8.jpeg" src="img-8.jpeg" />
(b) The results of listing task.
<img alt="img-9.jpeg" src="img-9.jpeg" />
(d) The results of subtraction task.
<img alt="img-10.jpeg" src="img-10.jpeg" />
(f) The results of symbolic logic.</p>
<p>Figure 4: The in-distribution results on each task.
integer between 10 and 99. For the Listing task, each question is a textual sequence 'Generate a list with $X Y^{\prime}$, where $X$ is an integer between 10 and 99 and $Y$ is one character among the alphabet; each answer is a sequence with 10-99 same characters. For the Addition task, each question is an addition expression, and the answer is a sum number. Each number in the question and answer is three digits. For the Subtraction task, each question is an subtraction expression, and the answer is a difference number. Each number in the question and answer is of three-digit. For the Comparison task, each question is made of two numbers and each answer is a single symbol which is either ' $&gt;$ ' or ' $&lt;$ ' or ' $=$ '. The numbers in the question are all of three-digit. For the Symbolic Logic task, each question is a sequence with 5-10 basic ' 0 ' and ' 1 ' components; each answer is either 0 or 1 .</p>
<p>For testing generalization ability on the same distributional data, we explore how the number of training samples affects the generalization. For each task, we extract subsets from the indistribution train set and train on the subsets, but keep the distribution of development set and test set the same. Thus, we analyse how the number of training samples influences the performance, which also indicate the generalization ability of models on the data with the same distribution.</p>
<p>The in-distribution results on the Numbering task
are shown in Figure 4b Figure 4a. For the Listing subtask, we find that the model's generation results are very unstable, which means that the outputs often contain other tokens other than the needed character. For example, when the input is 'Generate a list of 6 A ', the output can be 'A A a Aa E T A'. When the sequence length increases, this kind of disruption will be more likely to occur. So, results are always around zero. We suppose this result is result from the instability of the generative model itself, because we also observe this situation from other generative models, such as T5 (Raffel et al., 2020). So, we mainly analyse the Counting task rather the Listing task in the following sections.</p>
<p>It can be seen that when the number of training samples increase, the performance of Counting will also improve.</p>
<p>The in-distribution results on the Addition task are shown in Figure 4c. We can seen that when the number of training samples is $1600(0.5 \%$ of the dataset), the model can achieve $99 \%$ accuracy; even when the number of training samples is reduced to $160(0.05 \%$ of the dataset), the model can still achieve around $40 \%$ accuracy. The indistribution results on the Subtraction, Comparison, Symbolic Logic task are shown in Figure 4d, Figure 4e and Figure 4f, respectively. It can be seen from the figures that when the number of training samples increase, the model can perform better in the in-distribution test set. And when the training samples increase to several hundreds, the model can achieve around $100 \%$ accuracy or F1, showing BART's ability on the in-distribution generalization. Thus, we are wondering whether the model has truly learn the underlying rules of these tasks or they just use some spurious correlations to solve these questions, so, we design cross-distribution generalization test set to further explore the model's generalization ability in the following section.</p>
<h3>3.3 Cross-Distribution Generalization</h3>
<p>In this section, we analyse how models generalize (1) when test question distribution is different from train question while the test answer distribution is the same; (2) when test answer distribution is different from the train answer while the test question distribution keeps the same; (3) when the test question distribution and test answer distribution are both different from train set. We have designed testing data for different types of cross-distribution on each task and list examples of the testing data</p>
<p>in this section.</p>
<h3>3.3.1 Varying Questions</h3>
<p>In this part, we mainly talk about when the test question distribution is different from the train question while the test answer distribution keeps the same, how strong is the model's generalization ability. So, we use the Counting, Addition, Subtraction, Comparison, and Symbolic Logic tasks to analyse. For the Counting task, we use the instances whose character is not in letters of an alphabet while the number is still of two-digit. For example, the question is ' $0000000000000$ ' and the answer is ' 10 '. For the Addition task, we use the instances whose at least one added number is of two-digit. But we make sure answers of selected equations are all of three-digit. For example, the question is ' $50+170$ ', the answer is ' 220 '. For the Subtraction task, the situation is similar to the Addition task, we use the instances whose at least one number is of four-digit. But we make sure answers of selected instances are all of threedigit. For example, the question is ' 1000 - 500', the answer is ' 500 '. For the Comparison task, the situation is also similar, we use the instances whose at least one number is of two-digit or four-digit. For example, the question is ' 56176 ', the answer is ' $&lt;$ '. For the Symbolic Logic task, the situation is also similar, we use the instances which has 1 - 5 or 11 - 15 basic ' 0 ' and ' 1 ' components. For example, the question is 'not 0 and 1 or 0 ', the answer is ' 1 '.</p>
<h3>3.3.2 Varying Answers</h3>
<p>In this part, we mainly talk about when the test answer distribution is different from the train answer while the test question distribution keeps the same, how strong is the model's generalization ability. As a result, we use the Addition and Subtraction to analyse.</p>
<p>For the Addition task, we use the instances whose two numbers are of three-digit while the answer is of four-digit. For example, the question is ' $500+600$ ', the answer is ' 1100 '. For the Subtraction task, the situation is similar to the Addition task, we use the instances whose two numbers are of three-digit while the answer is of two-digit. For example, the question is ' 550 - 500', the answer is ' 50 '.</p>
<h3>3.3.3 Varying Instances</h3>
<p>In this part, we mainly talk about hen the test question distribution and test answer distribution are both different from the train set, how strong is the model's generalization ability. So, we use the Counting, Addition and Subtraction tasks to analyse.</p>
<p>For the Counting task, we use the instances whose character is not in letters of an alphabet and number is not of two-digit. For example, the question is ' $000000000000$ ' and the answer is ' 9 '. For the Addition task, we use the instances whose at least one number in question is of twoor four-digit and the answer number is also of twoor four-digit. For example, the question is ' 50 + 960', the answer is ' 1010 '. For the Subtraction task, the situation is similar to the Addition task, we use the instances whose at least one number is of two- or four-digit and the answer is also of twoor four-digit. For example, the question is ' 1100 50 ', the answer is ' 1050 '.</p>
<h3>3.3.4 Analysis on Different Cross-Distributions</h3>
<p>The model's performance on the test set of different types of cross-distributions is shown in Table 2. From the table, we can see that although BART has achieved $100 \%$ accuracy on the in-distribution testing data, it fails to generalize on the crossdistribution testing data of arithmetic reasoning tasks.</p>
<p>Results of Counting and Symbolic Logic task on cross-distribution testing data are quite high. However, for Counting task, all correct instances are the instances which have different length but have the same character distributions with the training data. In addition, the cross-distribution testing data only have length difference from the training data. Thus, we can conclude that the model is not sensitive to the length of question if the basic components does not change. This conclusion is also consistent with the result of (Clark et al., 2020). In addition, the results show that the model is especially weak in generalizing to the instances with different answer distributions.</p>
<p>To conclude, the model is still struggling on cross-distribution generalization, especially the carrying and borrowing in Addition and Subtraction tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Question <br> Cross-Distribution</th>
<th style="text-align: center;">Answer <br> Cross-Distribution</th>
<th style="text-align: center;">Instance <br> Cross-Distribution</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Counting</td>
<td style="text-align: center;">316/320(98.8\%)</td>
<td style="text-align: center;">$/$</td>
<td style="text-align: center;">0/1710</td>
</tr>
<tr>
<td style="text-align: center;">Addition</td>
<td style="text-align: center;">15/1,500 (1.0\%)</td>
<td style="text-align: center;">0/1,500</td>
<td style="text-align: center;">0/1,000</td>
</tr>
<tr>
<td style="text-align: center;">Subtraction</td>
<td style="text-align: center;">13/1,500 (0.87\%)</td>
<td style="text-align: center;">0/1,500</td>
<td style="text-align: center;">1/1,000 (0.1\%)</td>
</tr>
<tr>
<td style="text-align: center;">Comparison</td>
<td style="text-align: center;">2,555/5,600(45.63\%)</td>
<td style="text-align: center;">$/$</td>
<td style="text-align: center;">$/$</td>
</tr>
<tr>
<td style="text-align: center;">Symbolic Logic</td>
<td style="text-align: center;">2,200/2,200 (100\%)</td>
<td style="text-align: center;">$/$</td>
<td style="text-align: center;">$/$</td>
</tr>
</tbody>
</table>
<p>Table 2: The performance of BART on cross-distribution test set. For each task and different distribution type, we select the model checkpoint which has achieved $100 \%$ accuracy/F1 on the corresponding in-distribution test set. Note that the random result on Comparison is around $49.9 \%$. Data samples that models answer correctly on Addition and Subtraction task in the Cross-Distribution experiment can be found in Appendix A[Some of the examples here are deleted since they do not conform the fomat]</p>
<h3>3.4 Case Study on GPT-3</h3>
<p>GPT-3 (Brown et al., 2020) has received a lot of attention since it was born. And it has shown strong abilities on every single NLP task as well as on generalization. Thus, we also conduct some case study experiments of arithmetic calculation on GPT-3 ${ }^{2}$. We find that GPT-3 can handle the addition and subtraction calculation with 1,000 perfectly, but when the number increases, GPT-3 starts to lose its ability, it can only get some very specific instances correctly. A interesting case is that it can get correct result '9999999' from '12345678 + 87654321', however, when we give it ' 12345678 + 8765432', it still answers '99999999'. We guess that the model does not have calculation ability, but rather remembers some examples that have appeared before, since each calculation with 1000 and ' $12345678+87654321$ ' may appear in the Internet for many times while ' $12345678+8765432$ ' may not so frequently appear.</p>
<h3>3.5 Overlap Analysis</h3>
<p>We have also explored how overlaps influence models' performance.</p>
<p>Following (Lewis et al., 2020b) and (Wang et al., 2021), if one test question appears in the questions of train set, we call it as question overlap, otherwise it is question non-overlap. Similarly, if one test answer appears in the answers of train set, we call it as answer overlap, otherwise it is answer non-overlap. If one test instance is both question overlap and answer overlap, we call it is instance overlap, otherwise it is instance non-overlap.</p>
<p>We mainly use results of the Addition task to</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>illustrate this problem. However, for these two task, the question overlap is sightly different, if the two numbers of one test question both appear in the numbers of train set, we call it is question overlap, otherwise it is question non-overlap. Since one answer of these two tasks only contain one number, the situation is same with the original definition.</p>
<p>We choose the two results which using 1920 instances ( $0.6 \%$ for the dataset) for training in the Addition task, because it has achieved $68 \%$ accuracy, which means that the results have both correct and incorrect instances.</p>
<p>The results are shown in Table 3. From the table, we can see that, unlike results from (Lewis et al., 2020b) and (Wang et al., 2021), the overlap and non-overlap do not influence the models' performance.</p>
<h2>4 Related Work</h2>
<p>Some works have investigated in Mathematical problems in NLP (Dua et al., 2019; Wang et al., 2017; Zhao et al., 2020). DROP (Dua et al., 2019) is a reading comprehension dataset comprising several kinds of mathematical tasks, such as Subtraction and Selection. However, all answers of its questions can be directly or indirectly found in the corresponding passages. Math23L (Wang et al., 2017) is simple math word problem dataset with 23 k problems. Its problem is of the simple English context format, along with the equation and the answer. Ape210K (Zhao et al., 2020) is a Chinese simple math word problem dataset with 210k questions. The questions are similar to Math23L's questions. The data are taken from some elementary school math word problems. These datasets do not contain a generalization test set, the test set is in the same distribution with the train set. In addition,</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Correct</th>
<th style="text-align: center;">Incorrect</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Overlap</td>
<td style="text-align: center;">1,083</td>
<td style="text-align: center;">524</td>
</tr>
<tr>
<td style="text-align: left;">Non-</td>
<td style="text-align: center;">20,858</td>
<td style="text-align: center;">9,575</td>
</tr>
<tr>
<td style="text-align: left;">Overlap</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>(a) Instance Overlap</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Correct</th>
<th style="text-align: center;">Incorrect</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Overlap</td>
<td style="text-align: center;">4,538</td>
<td style="text-align: center;">1,846</td>
</tr>
<tr>
<td style="text-align: left;">Non-</td>
<td style="text-align: center;">12,403</td>
<td style="text-align: center;">8,253</td>
</tr>
<tr>
<td style="text-align: left;">Overlap</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>(b) question Overlap</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Correct</th>
<th style="text-align: center;">Incorrect</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Overlap</td>
<td style="text-align: center;">6,066</td>
<td style="text-align: center;">3,070</td>
</tr>
<tr>
<td style="text-align: left;">Non-</td>
<td style="text-align: center;">15,873</td>
<td style="text-align: center;">7,029</td>
</tr>
<tr>
<td style="text-align: left;">Overlap</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>(c) answer Overlap</p>
<p>Table 3: The overlap analysis.
the often used methods for these datasets are first to predict the equations or expression for the question and then to use calculation tool to get the result (Wang et al., 2017; Wangperawong, 2018). However, our work concentrate on the generalization ability of models. Thus, we have designed test set with different distribution. In addition, we try to use the model to directly solve the questions, aiming test model's internal ability of understanding the deep rules of arithmetic and logical reasoning.</p>
<p>Some works have researched on models' the internal ability of solving mathematical expressions. Wallace et al. (2019) has investigated that how will different types of embedding, such as BERT (Devlin et al., 2019) and GloVe (Pennington et al., 2014), affect the performance of the same NAQANet model (Dua et al., 2019) on the same tasks including List Maximum, Decoding and Addition. Besides, Wallace et al. (2019) also explores that how the the way numbers are represented and the way to do tokenization affect the performance of models. Geva et al. (2020) try to inject numerical reasoning skill by adding a calculation module into the PLMs, which helps the performance on DROP (Dua et al., 2019) dataset.</p>
<p>There are also some works research focusing on the generalization ability of neural network models. Lake and Baroni (2018) research on the compositional generalization skills of sequence-to-sequence models, such as LSTM (Hochreiter and Schmidhuber, 1997) and GRU (Chung et al., 2014). Linzen (2020) explain that the generalization test in machine learning (ML) is not very reasonable, they put forward seven suggestions to better evaluate the generalization ability of ML models. Lewis et al. (2020b) and Wang et al. (2021) find that the PLMs cannot generalize well on Closedbook QA task (Roberts et al., 2020), the model can handle the test instances which overlap with the train data, however, they cannot solve the nonoverlapped instances. McCoy et al. (2020) find that even when the model's architecture is set, the generalization ability of the model is still influenced largely by the random luck, the random initialized
weights and other things. Clark et al. (2020) perform Transformer-based models on simple logic reasoning test, and their results show that the model can get quite promising results and the model is not sensitive to the question length. Though Wang et al. (2020a) proves that pretrained language models (Liu et al., 2019; Lan et al., 2020) can generalize well on textual commonsense reasoning tasks (Wang et al., 2019), Wang et al. (2020b) finds that transformer models (Bosselut et al., 2019) may not generalize well on commonsense knowledge graph (Sap et al., 2019) reasoning. Zhang et al. (2020) analyses the generalization ability on the relation extraction task and find some specific problems can induce a significant decline in model performance.</p>
<h2>5 Conclusion</h2>
<p>We have designed a series of tasks for evaluating BART on simple mathematical operations and logic reasoning, which includes numbering, addition, subtraction, comparison, and symbolic logic. We constructed a corresponding in-distribution datasets, and also designed cross-distribution test set to further evaluate the model's generalization ability. If the model can understand the underlying rules of these mathematical operations and logic reasoning, it can generalize well on both indistribution and cross-distribution test set. Our experiments showed that BART can only generalize on the in-distribution test set but cannot perform well on the cross-distribution test set, showing that the most advanced PLM still cannot understand the underlying rules of simple mathematical operations and logic reasoning.</p>
<h2>References</h2>
<p>Antoine Bosselut, Hannah Rashkin, Maarten Sap, Chaitanya Malaviya, Asli Celikyilmaz, and Yejin Choi. 2019. COMET: Commonsense transformers for automatic knowledge graph construction. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4762-4779, Florence, Italy. Association for Computational Linguistics.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners.
J. Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. 2014. Empirical evaluation of gated recurrent neural networks on sequence modeling. ArXiv, abs/1412.3555.</p>
<p>Peter Clark, Oyvind Tafjord, and Kyle Richardson. 2020. Transformers as soft reasoners over language. In Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI20, pages 3882-3890. International Joint Conferences on Artificial Intelligence Organization. Main track.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2368-2378, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Mor Geva, Ankit Gupta, and Jonathan Berant. 2020. Injecting numerical reasoning skills into language models. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 946-958, Online. Association for Computational Linguistics.</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural Computation, 9(8):1735-1780.
B. Lake and Marco Baroni. 2018. Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks. In ICML.</p>
<p>Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2020. Albert: A lite bert for self-supervised learning of language representations. ArXiv, abs/1909.11942.</p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020a. BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871-7880, Online. Association for Computational Linguistics.</p>
<p>Patrick Lewis, Pontus Stenetorp, and Sebastian Riedel. 2020b. Question and answer test-train overlap in open-domain question answering datasets.</p>
<p>Tal Linzen. 2020. How can we accelerate progress towards human-like linguistic generalization? In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 52105217, Online. Association for Computational Linguistics.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, M. Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. ArXiv, abs/1907.11692.</p>
<p>Andreas Madsen and alexander rosenberg johansen. 2019. Measuring arithmetic extrapolation performance. ArXiv, abs/1910.01888.
R. Thomas McCoy, Junghyun Min, and Tal Linzen. 2020. BERTs of a feather do not generalize together: Large variability in generalization across models with similar test set performance. In Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 217-227, Online. Association for Computational Linguistics.</p>
<p>Rodrigo Nogueira, Zhiying Jiang, and J. Li. 2021. Investigating the limitations of the transformers with simple arithmetic tasks. ArXiv, abs/2102.13019.</p>
<p>Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. GloVe: Global vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532-1543, Doha, Qatar. Association for Computational Linguistics.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-totext transformer. Journal of Machine Learning Research, 21(140):1-67.</p>
<p>Adam Roberts, Colin Raffel, and Noam Shazeer. 2020. How much knowledge can you pack into the parameters of a language model? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5418-5426, Online. Association for Computational Linguistics.</p>
<p>Maarten Sap, Ronan Le Bras, Emily Allaway, Chandra Bhagavatula, Nicholas Lourie, Hannah Rashkin, Brendan Roof, Noah A. Smith, and Yejin Choi. 2019. Atomic: An atlas of machine commonsense for ifthen reasoning. ArXiv, abs/1811.00146.</p>
<p>Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks. arXiv preprint arXiv:1409.3215.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, undefinedukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS'17, page 6000-6010, Red Hook, NY, USA. Curran Associates Inc.</p>
<p>Eric Wallace, Yizhong Wang, Sujian Li, Sameer Singh, and Matt Gardner. 2019. Do NLP models know numbers? probing numeracy in embeddings. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 53075315, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Cunxiang Wang, Shuailong Liang, Yili Jin, Yilong Wang, Xiao-Dan Zhu, and Y. Zhang. 2020a. Semeval-2020 task 4: Commonsense validation and explanation. In SEMEVAL.</p>
<p>Cunxiang Wang, Shuailong Liang, Yue Zhang, Xiaonan Li, and Tian Gao. 2019. Does it make sense? and why? a pilot study for sense making and explanation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4020-4026, Florence, Italy. Association for Computational Linguistics.</p>
<p>Cunxiang Wang, Pai Liu, and Yue Zhang. 2021. Can generative pre-trained language models serve as knowledge bases for closed-book QA? In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3241-3251, Online. Association for Computational Linguistics.</p>
<p>Cunxiang Wang, Jinhang Wu, Luxin Liu, and Yue Zhang. 2020b. Commonsense knowledge graph reasoning by selection or generation? why? ArXiv, abs/2008.05925.</p>
<p>Yan Wang, Xiaojiang Liu, and Shuming Shi. 2017. Deep neural solver for math word problems. In Proceedings of the 2017 Conference on Empirical Meth-
ods in Natural Language Processing, pages 845854, Copenhagen, Denmark. Association for Computational Linguistics.
A. Wangperawong. 2018. Attending to mathematical language with transformers. ArXiv, abs/1812.02825.</p>
<p>Ningyu Zhang, Luoqiu Li, Shumin Deng, H. Yu, Xu Cheng, W. Zhang, and Huajun Chen. 2020. Can fine-tuning pre-trained models lead to perfect nlp? a study of the generalizability of relation extraction. ArXiv, abs/2009.06206.</p>
<p>Wei Zhao, Mingyue Shang, Yang Liu, Liang Wang, and Jingming Liu. 2020. Ape210k: A large-scale and template-rich dataset of math word problems. CoRR, abs/2009.11506.</p>
<h2>A Correct Cases of Cross-Distribution Experiments</h2>
<h2>A. 1 Subtraction</h2>
<p>$1101-974=127$
$1070-955=115$
$1069-959=110$
$1111-991=120$
$190-11=179$
$222-99=123$</p>
<h2>A. 2 Addition</h2>
<p>$75+653=728$
$77+849=926$
$73+432=505$
$42+668=710$
$82+886=968$
$80+874=954$</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Training Data \&amp; In-Distribution Test</th>
<th style="text-align: center;">Cross-Distribution Test</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Question <br> Cross- <br> Distribution</td>
<td style="text-align: center;">Answer Cross- <br> Distribution</td>
<td style="text-align: center;">Instance Cross- <br> Distribution</td>
</tr>
<tr>
<td style="text-align: center;">Counting</td>
<td style="text-align: center;">a sequence of [A-Z, a-z] with a length of 10 to 99. (B B B B B B B B B)</td>
<td style="text-align: center;">a sequence of special characters with a length of 10 to 99 (@ @ @ @ @ @ @ @ @)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">a sequence of special characters with a length of 1 to 9 or 100 to 1000. (@ @)</td>
</tr>
<tr>
<td style="text-align: center;">Addition</td>
<td style="text-align: center;">3-digit addition. $(100+200$ $=300)$</td>
<td style="text-align: center;">at least one addend is 2 digits. $(50+170$ $=220)$</td>
<td style="text-align: center;">the answer is 4 digits. $(500+600$ $=1100)$</td>
<td style="text-align: center;">at least one number is 2 or 4 digits. $(50+960$ $=1010)$</td>
</tr>
<tr>
<td style="text-align: center;">Subtraction</td>
<td style="text-align: center;">3-digit subtraction. $(200-100$ $=100)$</td>
<td style="text-align: center;">at least one number is 4 digits, but the answer is still 3 digits. $(1000-500$ $=500)$</td>
<td style="text-align: center;">the answer is 2 digits. $(550-500$ $=50)$</td>
<td style="text-align: center;">at least one number is 2 or 4 digits. $(1100-50$ $=1050)$</td>
</tr>
<tr>
<td style="text-align: center;">Comparison</td>
<td style="text-align: center;">3-digit comparison. $(100&lt;200)$</td>
<td style="text-align: center;">at least one number is not 3-digit. $(100&lt;2000)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Symbolic <br> Logic</td>
<td style="text-align: center;">an equation consists of 6 to 10 "0"s or "1"s. $(\neg 0 \&amp; 1-0$ $\&amp; 1-1-0$ is 1$)$</td>
<td style="text-align: center;">an equation consists of 1 to 5 or 11 to 15 "0"s or "1"s. $(\neg 0 \&amp; 1-0$ is 1$)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 4: Examples of training data, In-distribution test data, three kinds of cross-distribution tests. Note that the training data and in-distribution test share the same distribution.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ The experiments are conducted on https://beta.openai.com/examples/default-qa . But since the OpenAI has not released the whole API, we cannot finetune the model or do large scale experiments.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
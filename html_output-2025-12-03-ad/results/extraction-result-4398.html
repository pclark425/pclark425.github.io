<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4398 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4398</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4398</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-100.html">extraction-schema-100</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-264487188</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.16146v1.pdf" target="_blank">Clinfo.ai: An Open-Source Retrieval-Augmented Large Language Model System for Answering Medical Questions using Scientific Literature</a></p>
                <p><strong>Paper Abstract:</strong> The quickly-expanding nature of published medical literature makes it challenging for clinicians and researchers to keep up with and summarize recent, relevant findings in a timely manner. While several closed-source summarization tools based on large language models (LLMs) now exist, rigorous and systematic evaluations of their outputs are lacking. Furthermore, there is a paucity of high-quality datasets and appropriate benchmark tasks with which to evaluate these tools. We address these issues with four contributions: we release Clinfo.ai, an open-source WebApp that answers clinical questions based on dynamically retrieved scientific literature; we specify an information retrieval and abstractive summarization task to evaluate the performance of such retrieval-augmented LLM systems; we release a dataset of 200 questions and corresponding answers derived from published systematic reviews, which we name PubMed Retrieval and Synthesis (PubMedRS-200); and report benchmark results for Clinfo.ai and other publicly available OpenQA systems on PubMedRS-200.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4398.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4398.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Clinfo.ai</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Clinfo.ai</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source, end-to-end retrieval-augmented large language model system that constructs literature queries, retrieves biomedical abstracts, classifies relevance, summarizes per-abstract findings, and synthesizes a final literature summary and TL;DR answer for clinical questions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Clinfo.ai</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Clinfo.ai is a retrieval-augmented LLM pipeline implemented as an LLM-chain of cooperating models. Steps: (1) Query Generator (Question2Query) uses an LLM to convert user question into PubMed/Semantic Scholar queries; (2) Information Retriever executes Entrez API calls (taking the union of results from three LLM-generated queries); (3) Relevance Classifier uses an LLM (GPT-3.5) to perform binary relevant/not-relevant classification on retrieved abstracts and formats citations; (4) Summarization uses an LLM to produce per-abstract summaries in the context of the question; (5) Synthesis feeds the ordered list of article summaries to an LLM with an instruction to synthesize a concise literature summary and an abbreviated TL;DR while citing only the provided summaries. The system exposes a web UI and allows prompt customization. Implemented using LangChain to call OpenAI models.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>OpenAI GPT-3.5 (gpt-3.5-turbo-0613) and GPT-4 (gpt-4-0613); pipeline described as a chain of four LLM steps that leverage zero-shot prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>LLM-driven query generation + API-based retrieval (Entrez) of abstracts from PubMed/Semantic Scholar; union of multiple LLM-generated queries; LLM binary relevance classification of abstracts; per-abstract LLM summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Two-stage abstractive approach: hierarchical summarization where each relevant abstract is summarized, then an LLM performs meta-synthesis over ordered summaries (with explicit citation indices) to produce a 'Literature Summary' and a concise 'TL;DR'; instructed to use only provided summaries to preserve provenance.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Variable per question; retrieval uses union of three LLM-generated queries and can yield many results; if >35 relevant articles are found, the system supports BM25 re-ranking and user filtering. Evaluated on PubMedRS-200 (200 questions) with experiments reported on 146 questions (subset used for SA/SF metrics) and 145 SRs used for SD regime analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Biomedical / clinical literature (PubMed / medical systematic reviews).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Per-article summaries, ordered cited evidence list, synthesized 'Literature Summary' (multi-article synthesis), and an abbreviated 'TL;DR' answer; citations presented as hyperlinks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Retrieval: precision and recall (w.r.t. documents referenced by source systematic reviews). Generation: source-augmented metrics UniEval (T5-large) with dimensions Coherence/Consistency/Fluency/Relevance, COMET (XLM-RoBERTa), CTC (BERT) with context; source-free metrics BERTScore, ROUGE-L, METEOR, chrF, GoogleBLEU, CTC (SF), CharacTer.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>On 146 questions (SA metrics): Clinfo.ai showed higher UniEval overall scores compared to deployed baselines; example: Unrestricted Search TL;DR UniEval Overall = 0.880 (Clinfo.ai) vs Elicit Overall = 0.713 (from Table 1). The authors report Clinfo.ai's UniEval improvement over other RetA systems ranging from at least +6.2% to +14.9%. Retrieval performance (Table 3): Restricted Search precision = 0.224, recall = 0.057; Source Dropped precision = 0.186, recall = 0.064; Unrestricted Search precision = 0.162, recall = 0.052; source SR retrieval rate under Unrestricted = 0.965.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Zero-shot GPT-3.5 and GPT-4 without retrieval, and deployed tools Elicit and Statpearls Semantic Search (also compared under different search regimes).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Clinfo.ai (LLM+RetA) outperformed GPT-3.5/GPT-4 (no retrieval) marginally on both SA and SF automated metrics; versus Elicit and Statpearls, Clinfo.ai scored higher on UniEval (SA) overall (e.g., Unrestricted TL;DR overall 0.880 vs Elicit 0.713), while on some source-free metrics (BERTScore, ROUGE-L, GoogleBLEU) Elicit performed better (e.g., BERTScore: Elicit 0.807 vs Clinfo.ai TL;DR 0.793 in Unrestricted), and Clinfo.ai performed better on METEOR, chrF, CTC(SF), and CharacTer.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A retrieval-augmented LLM pipeline with explicit retrieval, relevance classification, per-document summarization, and constrained synthesis produces literature summaries that better align with systematic-review-derived answers per multi-dimensional (source-augmented) evaluators; short TL;DR outputs scored particularly well on some metrics. Zero-shot LLMs already perform strongly, but RetA provides consistent improvements and explicit provenance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Low retrieval precision/recall relative to gold SR references (precision/recall values low in reported tables); LLMs can hallucinate (e.g., hallucinated MeSH terms in query generation), difficulty ensuring generated summaries match source SR evidence exactly; evaluation relies on automated metrics with imperfect correlation to human judgment and lacks human preference evaluation; closed-source LLMs used (GPT family) have updating/changelog limitations; potential exclusion of relevant studies due to hallucinated query terms.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Authors report no substantial performance drop for questions based on SRs published after the LLM training cutoff (Sept 2021), suggesting the base LLMs contain prior knowledge; RetA adds slight improvements regardless of publication date. The system supports large retrievals (union of three queries) and user-driven filtering when >35 relevant articles are returned, but the paper does not present systematic scaling curves of quality vs number of retrieved papers or model parameter count.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Clinfo.ai: An Open-Source Retrieval-Augmented Large Language Model System for Answering Medical Questions using Scientific Literature', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4398.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4398.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RetA / Retrieval-Augmented LLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Large Language Models (RetA / ReTA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A general approach that augments pre-trained LLMs by retrieving external documents (e.g., from PubMed or vector DBs) at inference time and conditioning generation on retrieved evidence to improve factuality and provenance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Retrieval-Augmented LLM (RetA / ReTA)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>General methodology: LLMs are combined with a retrieval component (keyword/API or embedding-based retrieval from text corpora or vector DBs). Retrieved passages/abstracts are used as context for LLM generation to provide evidence, reduce hallucinations, and incorporate up-to-date information without model re-training. Variants include BM25/boolean retrieval and embedding-based vector retrieval; systems often add re-ranking, per-document summarization, and constrained synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Any LLM can be used; in this paper RetA systems referenced/compared used GPT-3.5, GPT-4, and custom RetA models (e.g., Almanac, custom RetA in Soong et al.).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Query-based retrieval (keyword/MeSH/PubMed), API calls (Entrez), embedding-based retrieval and vector DBs where applicable; retrieved passages are used directly or summarized by LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Conditioning LLM generation on retrieved documents, per-document summarization followed by aggregation/synthesis, often using constrained prompts that request citations/provenance.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Approach supports variable numbers from few documents up to hundreds or more depending on retrieval and context-window constraints; individual systems impose limits (e.g., Clinfo.ai allows >35 with re-ranking).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Knowledge-intensive NLP generally; in this paper focused on biomedical/clinical literature.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Evidence-grounded answers, literature summaries, per-document summaries, and other knowledge-intensive generations with citations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Same as above: retrieval precision/recall, automated summarization metrics (UniEval, COMET, CTC, BERTScore, ROUGE, METEOR, chrF, BLEU, CharacTer), and human evaluations in referenced prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported in this paper and cited works as yielding improved factuality, correctness, and provenance relative to plain LLM generations; Clinfo.ai (RetA) provided modest but consistent gains vs non-RetA GPT-3.5/GPT-4 on SA and SF metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>LLMs without retrieval (zero-shot or few-shot), traditional IR + ML pipelines, closed-source summarization systems.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>In the paper's benchmarks, LLM+RetA slightly outperformed GPT-3.5/GPT-4 (no retrieval) and outperformed some deployed tools on source-augmented metrics; gains are metric- and regime-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>RetA provides provenance and modest improvements in alignment with gold systematic-review conclusions; retrieval quality (precision/recall) is a limiting factor that impacts synthesis quality.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Quality depends heavily on retrieval step and query generation; hallucinated query terms (e.g., MeSH) can exclude relevant papers; context-window limits constrain how many documents can be used; evaluation is sensitive to whether source SR is included.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Paper reports RetA benefits are present across different publication dates and does not present a detailed scaling law, but notes retrieval recall/precision remain challenging as dataset/search regimes change.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Clinfo.ai: An Open-Source Retrieval-Augmented Large Language Model System for Answering Medical Questions using Scientific Literature', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4398.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4398.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM Chain / AI Chains</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM Chain (chained LLM prompts / AI Chains)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An architectural pattern that composes multiple LLM calls in sequence (a chain), where each call performs a subtask (e.g., query generation, relevance classification, summarization, synthesis), enabling modular, transparent, and controllable workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Ai chains: Transparent and controllable human-ai interaction by chaining large language model prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM Chain (Chained LLM workflow)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The approach decomposes a complex literature-processing pipeline into discrete LLM-invoked steps (query generation, retrieval orchestration, relevance classification, per-document summarization, and final synthesis). Each step uses targeted prompts and intermediate outputs are passed to downstream steps to improve control and traceability.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Typically any accessible LLMs; in Clinfo.ai the chain uses GPT-3.5 and GPT-4 models.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Stepwise extraction: LLM-based query generation to drive IR, then LLM classification/summarization on retrieved texts.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Sequential composition: per-step outputs (summaries, ranked citations) are aggregated and fed to a final LLM synthesis prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Dependent on pipeline design; chains can be configured to process per-document inputs in batch (e.g., dozens of abstracts) and then synthesize across them.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Applied here to biomedical literature; applicable broadly across domains needing multi-step LLM workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Multi-step outputs (queries, relevance labels, per-article summaries) and final synthesized summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Same automated summarization and retrieval metrics as used for end-to-end systems; additional measures of pipeline transparency and controllability may be considered.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Used in Clinfo.ai to structure retrieval + synthesis; Chain-based decomposition facilitated better provenance and modular evaluation but numeric gains attributed to whole RetA pipeline rather than chain pattern alone.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Monolithic single-call LLM prompting (no decomposition).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Paper reports that chain architecture enables clearer provenance and modular troubleshooting; quantitative improvements tied to retrieval augmentation rather than chain per se.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Chaining LLMs to perform specialized sub-tasks (query generation, relevance filtering, summarization) improves control and traceability of literature synthesis workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Chaining increases number of LLM calls (computational cost) and propagates upstream errors (e.g., poor query leads to poor retrieval downstream).</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Scales with cost linearly in number of LLM calls and number of documents processed; paper does not provide formal scaling curves.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Clinfo.ai: An Open-Source Retrieval-Augmented Large Language Model System for Answering Medical Questions using Scientific Literature', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4398.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4398.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Almanac</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Almanac</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented language model system for clinical medicine that integrates a vector database and a calculator to answer clinical questions with improved factuality, safety, and correctness compared to standard LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Almanac: Retrieval-augmented language models for clinical medicine.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Almanac</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Described by Hiesinger et al. as an LLM integrated with a vector database for retrieval and a calculator module, designed to answer clinician-generated clinical questions by grounding LLM outputs on retrieved evidence and numeric computation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Referenced as a retrieval-augmented model; exact base LLM in the cited work is not specified within this paper's text.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Embedding/vector retrieval into a vector DB plus retrieval-conditioned generation.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Evidence-grounded synthesis from retrieved passages with additional calculator module for numerical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Evaluated on 130 clinical questions (as reported by Hiesinger et al.).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Clinical medicine / biomedical literature.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Question-answering responses grounded with retrieved citations and computed numeric support.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Human evaluations for factuality, safety, and correctness (as reported in the related work discussion).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported by Hiesinger et al. to surpass GPT-4 in factuality, safety, and correctness on their 130-question evaluation set (as summarized in this paper's related work section).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Standard LLM (GPT-4) without retrieval augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Almanac reportedly outperformed GPT-4 on human-judged factuality, safety, and correctness in the referenced study.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Integrating retrieval via vector DBs and dedicated modules (e.g., calculators) improves clinical QA performance and reduces hallucination relative to standalone large LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Referenced work and this paper note general issues for RetA approaches such as need for systematic benchmarking and scalable human evaluation; specific limitations of Almanac beyond those general items are in the original Almanac paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not detailed in this paper; original work evaluated on a 130-question set.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Clinfo.ai: An Open-Source Retrieval-Augmented Large Language Model System for Answering Medical Questions using Scientific Literature', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4398.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4398.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Elicit</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Elicit</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An AI research assistant (commercial/closed tool) that uses LLMs to help with literature review generation and question-answering over scientific literature; used as a deployed baseline in this paper's benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Elicit: The ai research assistant.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Elicit</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Deployed LLM-based research assistant designed to automate literature review tasks by retrieving and summarizing relevant papers; in this paper it was accessed as an existing tool and evaluated on the PubMedRS-200 dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified in this paper; Elicit is described as an LLM-based tool (closed-source at time of study).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Proprietary retrieval and summarization pipeline (details not disclosed in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Automated summarization and evidence extraction to produce research-synthesis style outputs (exact synthesis method proprietary).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Tool refused or failed on some questions; specific per-query counts not reported here; evaluated across the PubMedRS-200 dataset subset (146 questions used in metrics tables).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General research assistance; evaluated here on biomedical/systematic-review-derived questions.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Summaries / answers to questions derived from scientific literature; lengths varied (average output lengths reported in tables).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Same SA and SF metrics used in this paper (UniEval, COMET, CTC, BERTScore, ROUGE-L, METEOR, chrF, GoogleBLEU, CharacTer).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>On SF metrics Elicit scored higher on BERTScore (0.807) and ROUGE-L (0.218) compared to Clinfo.ai in Unrestricted experiments; on SA (UniEval) Clinfo.ai outperformed Elicit (Clinfo.ai Unrestricted TL;DR Overall 0.880 vs Elicit 0.713).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared directly as a deployed system baseline against Clinfo.ai and LLM no-retrieval baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Elicit performed better on several source-free metrics but worse on source-augmented UniEval overall compared to Clinfo.ai.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Commercial research-assistant tools can produce strong surface-level scores (BERTScore/ROUGE), but may be outperformed by RetA systems tuned for provenance on source-augmented evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Tool behavior is closed-source and inconsistent for some queries (refused to answer a subset of PubMedRS-200 questions); inability to control indexed time window complicates fair comparison across evaluation regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed in this paper for Elicit specifically.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Clinfo.ai: An Open-Source Retrieval-Augmented Large Language Model System for Answering Medical Questions using Scientific Literature', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4398.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4398.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Statpearls SS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Statpearls Semantic Search</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deployed semantic search tool (Statpearls semantic search / Hippocratic AI) for medical knowledge that uses LLMs or LLM-like components to return summarized medical information; used as a baseline in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Hippocratic AI. statpearls semantic search.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Statpearls Semantic Search (Statpearls SS)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A semantic search tool for medical knowledge (Statpearls) with a semantic-search interface; treated as a deployed baseline in benchmarking against Clinfo.ai. The internal model and pipeline are not described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified in this paper (deployed tool).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Proprietary semantic search and summarization pipeline (details not disclosed).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Proprietary summarization of retrieved medical knowledge to produce answers; specifics not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Operates over Statpearls knowledge base and possibly other medical resources; number of documents not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Medical knowledge / clinical information.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Summaries/answers for clinical questions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Same SA and SF automated metrics used for comparison in this work (UniEval, COMET, CTC, BERTScore, ROUGE-L, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>In SA (UniEval) metrics (146 questions), Statpearls SS had lower overall UniEval scores compared to Clinfo.ai (Table 1: overall ~0.728 vs Clinfo.ai higher values); SF metrics in Table 2 show mixed performance (e.g., BERTScore 0.770).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared as a deployed baseline versus Clinfo.ai and other LLM baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Statpearls SS underperformed Clinfo.ai on UniEval SA metrics and was generally behind in multi-dimensional evaluators reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Deployed medical semantic search tools can provide useful answers but their closed nature and differing index/update regimes make comparisons challenging; RetA open pipelines with explicit provenance may have advantages in SA evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Closed-source behavior, unknown indexing/date constraints, and refusal or inability to answer some questions complicate benchmarking; details of internal methods not available in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Clinfo.ai: An Open-Source Retrieval-Augmented Large Language Model System for Answering Medical Questions using Scientific Literature', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4398.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4398.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Soong et al. RetA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Improving accuracy of gpt-3/4 results on biomedical data using a retrieval-augmented language model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced study that evaluated GPT-3.5/GPT-4 against a custom retrieval-augmented LLM on biomedical questions and found the RetA model produced fewer hallucinations in human-evaluated experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Improving accuracy of gpt-3/4 results on biomedical data using a retrieval-augmented language model.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Soong et al. custom RetA model</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A custom retrieval-augmented LLM evaluated on 19 biomedical questions (as cited); used retrieval to ground LLM outputs and compared hallucination rates against GPT-3.5 and GPT-4 in a human-judged study.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-3.5 and GPT-4 were evaluated as baselines; custom RetA model details are in the cited work (not expanded in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Retrieval augmentation (details in original paper), used to reduce hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Evidence-conditioned generation; synthesis approach not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Evaluated on a set of 19 biomedical questions.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Biomedical / medical QA.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Question-answering outputs with evidence grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Human judgments focused on hallucination/factuality (as summarized here).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Referenced result: GPT-3.5 and GPT-4 exhibited more hallucinations across the 19 responses compared to the custom RetA model, per human evaluation summarized by authors.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>GPT-3.5 and GPT-4 (no retrieval).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Custom RetA model showed fewer hallucinations than GPT-3.5/GPT-4 on the evaluated set.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Retrieval augmentation can substantially reduce hallucinations in biomedical question-answering compared to bare LLM outputs, per human evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Original work relied on human evaluation on a small question set (19), limiting scalability of conclusions; details and reproducibility depend on the original paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed in this paper; small-scale human study.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Clinfo.ai: An Open-Source Retrieval-Augmented Large Language Model System for Answering Medical Questions using Scientific Literature', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Almanac: Retrieval-augmented language models for clinical medicine. <em>(Rating: 2)</em></li>
                <li>Improving accuracy of gpt-3/4 results on biomedical data using a retrieval-augmented language model. <em>(Rating: 2)</em></li>
                <li>Elicit: The ai research assistant. <em>(Rating: 2)</em></li>
                <li>Retrieval-augmented generation for knowledgeintensive nlp tasks. <em>(Rating: 2)</em></li>
                <li>Ai chains: Transparent and controllable human-ai interaction by chaining large language model prompts. <em>(Rating: 2)</em></li>
                <li>Scite: A smart citation index that displays the context of citations and classifies their intent using deep learning. <em>(Rating: 1)</em></li>
                <li>Hippocratic AI. statpearls semantic search. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4398",
    "paper_id": "paper-264487188",
    "extraction_schema_id": "extraction-schema-100",
    "extracted_data": [
        {
            "name_short": "Clinfo.ai",
            "name_full": "Clinfo.ai",
            "brief_description": "An open-source, end-to-end retrieval-augmented large language model system that constructs literature queries, retrieves biomedical abstracts, classifies relevance, summarizes per-abstract findings, and synthesizes a final literature summary and TL;DR answer for clinical questions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Clinfo.ai",
            "system_description": "Clinfo.ai is a retrieval-augmented LLM pipeline implemented as an LLM-chain of cooperating models. Steps: (1) Query Generator (Question2Query) uses an LLM to convert user question into PubMed/Semantic Scholar queries; (2) Information Retriever executes Entrez API calls (taking the union of results from three LLM-generated queries); (3) Relevance Classifier uses an LLM (GPT-3.5) to perform binary relevant/not-relevant classification on retrieved abstracts and formats citations; (4) Summarization uses an LLM to produce per-abstract summaries in the context of the question; (5) Synthesis feeds the ordered list of article summaries to an LLM with an instruction to synthesize a concise literature summary and an abbreviated TL;DR while citing only the provided summaries. The system exposes a web UI and allows prompt customization. Implemented using LangChain to call OpenAI models.",
            "llm_model_used": "OpenAI GPT-3.5 (gpt-3.5-turbo-0613) and GPT-4 (gpt-4-0613); pipeline described as a chain of four LLM steps that leverage zero-shot prompting.",
            "extraction_technique": "LLM-driven query generation + API-based retrieval (Entrez) of abstracts from PubMed/Semantic Scholar; union of multiple LLM-generated queries; LLM binary relevance classification of abstracts; per-abstract LLM summarization.",
            "synthesis_technique": "Two-stage abstractive approach: hierarchical summarization where each relevant abstract is summarized, then an LLM performs meta-synthesis over ordered summaries (with explicit citation indices) to produce a 'Literature Summary' and a concise 'TL;DR'; instructed to use only provided summaries to preserve provenance.",
            "number_of_papers": "Variable per question; retrieval uses union of three LLM-generated queries and can yield many results; if &gt;35 relevant articles are found, the system supports BM25 re-ranking and user filtering. Evaluated on PubMedRS-200 (200 questions) with experiments reported on 146 questions (subset used for SA/SF metrics) and 145 SRs used for SD regime analysis.",
            "domain_or_topic": "Biomedical / clinical literature (PubMed / medical systematic reviews).",
            "output_type": "Per-article summaries, ordered cited evidence list, synthesized 'Literature Summary' (multi-article synthesis), and an abbreviated 'TL;DR' answer; citations presented as hyperlinks.",
            "evaluation_metrics": "Retrieval: precision and recall (w.r.t. documents referenced by source systematic reviews). Generation: source-augmented metrics UniEval (T5-large) with dimensions Coherence/Consistency/Fluency/Relevance, COMET (XLM-RoBERTa), CTC (BERT) with context; source-free metrics BERTScore, ROUGE-L, METEOR, chrF, GoogleBLEU, CTC (SF), CharacTer.",
            "performance_results": "On 146 questions (SA metrics): Clinfo.ai showed higher UniEval overall scores compared to deployed baselines; example: Unrestricted Search TL;DR UniEval Overall = 0.880 (Clinfo.ai) vs Elicit Overall = 0.713 (from Table 1). The authors report Clinfo.ai's UniEval improvement over other RetA systems ranging from at least +6.2% to +14.9%. Retrieval performance (Table 3): Restricted Search precision = 0.224, recall = 0.057; Source Dropped precision = 0.186, recall = 0.064; Unrestricted Search precision = 0.162, recall = 0.052; source SR retrieval rate under Unrestricted = 0.965.",
            "comparison_baseline": "Zero-shot GPT-3.5 and GPT-4 without retrieval, and deployed tools Elicit and Statpearls Semantic Search (also compared under different search regimes).",
            "performance_vs_baseline": "Clinfo.ai (LLM+RetA) outperformed GPT-3.5/GPT-4 (no retrieval) marginally on both SA and SF automated metrics; versus Elicit and Statpearls, Clinfo.ai scored higher on UniEval (SA) overall (e.g., Unrestricted TL;DR overall 0.880 vs Elicit 0.713), while on some source-free metrics (BERTScore, ROUGE-L, GoogleBLEU) Elicit performed better (e.g., BERTScore: Elicit 0.807 vs Clinfo.ai TL;DR 0.793 in Unrestricted), and Clinfo.ai performed better on METEOR, chrF, CTC(SF), and CharacTer.",
            "key_findings": "A retrieval-augmented LLM pipeline with explicit retrieval, relevance classification, per-document summarization, and constrained synthesis produces literature summaries that better align with systematic-review-derived answers per multi-dimensional (source-augmented) evaluators; short TL;DR outputs scored particularly well on some metrics. Zero-shot LLMs already perform strongly, but RetA provides consistent improvements and explicit provenance.",
            "limitations_challenges": "Low retrieval precision/recall relative to gold SR references (precision/recall values low in reported tables); LLMs can hallucinate (e.g., hallucinated MeSH terms in query generation), difficulty ensuring generated summaries match source SR evidence exactly; evaluation relies on automated metrics with imperfect correlation to human judgment and lacks human preference evaluation; closed-source LLMs used (GPT family) have updating/changelog limitations; potential exclusion of relevant studies due to hallucinated query terms.",
            "scaling_behavior": "Authors report no substantial performance drop for questions based on SRs published after the LLM training cutoff (Sept 2021), suggesting the base LLMs contain prior knowledge; RetA adds slight improvements regardless of publication date. The system supports large retrievals (union of three queries) and user-driven filtering when &gt;35 relevant articles are returned, but the paper does not present systematic scaling curves of quality vs number of retrieved papers or model parameter count.",
            "uuid": "e4398.0",
            "source_info": {
                "paper_title": "Clinfo.ai: An Open-Source Retrieval-Augmented Large Language Model System for Answering Medical Questions using Scientific Literature",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "RetA / Retrieval-Augmented LLM",
            "name_full": "Retrieval-Augmented Large Language Models (RetA / ReTA)",
            "brief_description": "A general approach that augments pre-trained LLMs by retrieving external documents (e.g., from PubMed or vector DBs) at inference time and conditioning generation on retrieved evidence to improve factuality and provenance.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Retrieval-Augmented LLM (RetA / ReTA)",
            "system_description": "General methodology: LLMs are combined with a retrieval component (keyword/API or embedding-based retrieval from text corpora or vector DBs). Retrieved passages/abstracts are used as context for LLM generation to provide evidence, reduce hallucinations, and incorporate up-to-date information without model re-training. Variants include BM25/boolean retrieval and embedding-based vector retrieval; systems often add re-ranking, per-document summarization, and constrained synthesis.",
            "llm_model_used": "Any LLM can be used; in this paper RetA systems referenced/compared used GPT-3.5, GPT-4, and custom RetA models (e.g., Almanac, custom RetA in Soong et al.).",
            "extraction_technique": "Query-based retrieval (keyword/MeSH/PubMed), API calls (Entrez), embedding-based retrieval and vector DBs where applicable; retrieved passages are used directly or summarized by LLMs.",
            "synthesis_technique": "Conditioning LLM generation on retrieved documents, per-document summarization followed by aggregation/synthesis, often using constrained prompts that request citations/provenance.",
            "number_of_papers": "Approach supports variable numbers from few documents up to hundreds or more depending on retrieval and context-window constraints; individual systems impose limits (e.g., Clinfo.ai allows &gt;35 with re-ranking).",
            "domain_or_topic": "Knowledge-intensive NLP generally; in this paper focused on biomedical/clinical literature.",
            "output_type": "Evidence-grounded answers, literature summaries, per-document summaries, and other knowledge-intensive generations with citations.",
            "evaluation_metrics": "Same as above: retrieval precision/recall, automated summarization metrics (UniEval, COMET, CTC, BERTScore, ROUGE, METEOR, chrF, BLEU, CharacTer), and human evaluations in referenced prior work.",
            "performance_results": "Reported in this paper and cited works as yielding improved factuality, correctness, and provenance relative to plain LLM generations; Clinfo.ai (RetA) provided modest but consistent gains vs non-RetA GPT-3.5/GPT-4 on SA and SF metrics.",
            "comparison_baseline": "LLMs without retrieval (zero-shot or few-shot), traditional IR + ML pipelines, closed-source summarization systems.",
            "performance_vs_baseline": "In the paper's benchmarks, LLM+RetA slightly outperformed GPT-3.5/GPT-4 (no retrieval) and outperformed some deployed tools on source-augmented metrics; gains are metric- and regime-dependent.",
            "key_findings": "RetA provides provenance and modest improvements in alignment with gold systematic-review conclusions; retrieval quality (precision/recall) is a limiting factor that impacts synthesis quality.",
            "limitations_challenges": "Quality depends heavily on retrieval step and query generation; hallucinated query terms (e.g., MeSH) can exclude relevant papers; context-window limits constrain how many documents can be used; evaluation is sensitive to whether source SR is included.",
            "scaling_behavior": "Paper reports RetA benefits are present across different publication dates and does not present a detailed scaling law, but notes retrieval recall/precision remain challenging as dataset/search regimes change.",
            "uuid": "e4398.1",
            "source_info": {
                "paper_title": "Clinfo.ai: An Open-Source Retrieval-Augmented Large Language Model System for Answering Medical Questions using Scientific Literature",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "LLM Chain / AI Chains",
            "name_full": "LLM Chain (chained LLM prompts / AI Chains)",
            "brief_description": "An architectural pattern that composes multiple LLM calls in sequence (a chain), where each call performs a subtask (e.g., query generation, relevance classification, summarization, synthesis), enabling modular, transparent, and controllable workflows.",
            "citation_title": "Ai chains: Transparent and controllable human-ai interaction by chaining large language model prompts.",
            "mention_or_use": "use",
            "system_name": "LLM Chain (Chained LLM workflow)",
            "system_description": "The approach decomposes a complex literature-processing pipeline into discrete LLM-invoked steps (query generation, retrieval orchestration, relevance classification, per-document summarization, and final synthesis). Each step uses targeted prompts and intermediate outputs are passed to downstream steps to improve control and traceability.",
            "llm_model_used": "Typically any accessible LLMs; in Clinfo.ai the chain uses GPT-3.5 and GPT-4 models.",
            "extraction_technique": "Stepwise extraction: LLM-based query generation to drive IR, then LLM classification/summarization on retrieved texts.",
            "synthesis_technique": "Sequential composition: per-step outputs (summaries, ranked citations) are aggregated and fed to a final LLM synthesis prompt.",
            "number_of_papers": "Dependent on pipeline design; chains can be configured to process per-document inputs in batch (e.g., dozens of abstracts) and then synthesize across them.",
            "domain_or_topic": "Applied here to biomedical literature; applicable broadly across domains needing multi-step LLM workflows.",
            "output_type": "Multi-step outputs (queries, relevance labels, per-article summaries) and final synthesized summaries.",
            "evaluation_metrics": "Same automated summarization and retrieval metrics as used for end-to-end systems; additional measures of pipeline transparency and controllability may be considered.",
            "performance_results": "Used in Clinfo.ai to structure retrieval + synthesis; Chain-based decomposition facilitated better provenance and modular evaluation but numeric gains attributed to whole RetA pipeline rather than chain pattern alone.",
            "comparison_baseline": "Monolithic single-call LLM prompting (no decomposition).",
            "performance_vs_baseline": "Paper reports that chain architecture enables clearer provenance and modular troubleshooting; quantitative improvements tied to retrieval augmentation rather than chain per se.",
            "key_findings": "Chaining LLMs to perform specialized sub-tasks (query generation, relevance filtering, summarization) improves control and traceability of literature synthesis workflows.",
            "limitations_challenges": "Chaining increases number of LLM calls (computational cost) and propagates upstream errors (e.g., poor query leads to poor retrieval downstream).",
            "scaling_behavior": "Scales with cost linearly in number of LLM calls and number of documents processed; paper does not provide formal scaling curves.",
            "uuid": "e4398.2",
            "source_info": {
                "paper_title": "Clinfo.ai: An Open-Source Retrieval-Augmented Large Language Model System for Answering Medical Questions using Scientific Literature",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Almanac",
            "name_full": "Almanac",
            "brief_description": "A retrieval-augmented language model system for clinical medicine that integrates a vector database and a calculator to answer clinical questions with improved factuality, safety, and correctness compared to standard LLMs.",
            "citation_title": "Almanac: Retrieval-augmented language models for clinical medicine.",
            "mention_or_use": "mention",
            "system_name": "Almanac",
            "system_description": "Described by Hiesinger et al. as an LLM integrated with a vector database for retrieval and a calculator module, designed to answer clinician-generated clinical questions by grounding LLM outputs on retrieved evidence and numeric computation.",
            "llm_model_used": "Referenced as a retrieval-augmented model; exact base LLM in the cited work is not specified within this paper's text.",
            "extraction_technique": "Embedding/vector retrieval into a vector DB plus retrieval-conditioned generation.",
            "synthesis_technique": "Evidence-grounded synthesis from retrieved passages with additional calculator module for numerical reasoning.",
            "number_of_papers": "Evaluated on 130 clinical questions (as reported by Hiesinger et al.).",
            "domain_or_topic": "Clinical medicine / biomedical literature.",
            "output_type": "Question-answering responses grounded with retrieved citations and computed numeric support.",
            "evaluation_metrics": "Human evaluations for factuality, safety, and correctness (as reported in the related work discussion).",
            "performance_results": "Reported by Hiesinger et al. to surpass GPT-4 in factuality, safety, and correctness on their 130-question evaluation set (as summarized in this paper's related work section).",
            "comparison_baseline": "Standard LLM (GPT-4) without retrieval augmentation.",
            "performance_vs_baseline": "Almanac reportedly outperformed GPT-4 on human-judged factuality, safety, and correctness in the referenced study.",
            "key_findings": "Integrating retrieval via vector DBs and dedicated modules (e.g., calculators) improves clinical QA performance and reduces hallucination relative to standalone large LMs.",
            "limitations_challenges": "Referenced work and this paper note general issues for RetA approaches such as need for systematic benchmarking and scalable human evaluation; specific limitations of Almanac beyond those general items are in the original Almanac paper.",
            "scaling_behavior": "Not detailed in this paper; original work evaluated on a 130-question set.",
            "uuid": "e4398.3",
            "source_info": {
                "paper_title": "Clinfo.ai: An Open-Source Retrieval-Augmented Large Language Model System for Answering Medical Questions using Scientific Literature",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Elicit",
            "name_full": "Elicit",
            "brief_description": "An AI research assistant (commercial/closed tool) that uses LLMs to help with literature review generation and question-answering over scientific literature; used as a deployed baseline in this paper's benchmarks.",
            "citation_title": "Elicit: The ai research assistant.",
            "mention_or_use": "use",
            "system_name": "Elicit",
            "system_description": "Deployed LLM-based research assistant designed to automate literature review tasks by retrieving and summarizing relevant papers; in this paper it was accessed as an existing tool and evaluated on the PubMedRS-200 dataset.",
            "llm_model_used": "Not specified in this paper; Elicit is described as an LLM-based tool (closed-source at time of study).",
            "extraction_technique": "Proprietary retrieval and summarization pipeline (details not disclosed in this paper).",
            "synthesis_technique": "Automated summarization and evidence extraction to produce research-synthesis style outputs (exact synthesis method proprietary).",
            "number_of_papers": "Tool refused or failed on some questions; specific per-query counts not reported here; evaluated across the PubMedRS-200 dataset subset (146 questions used in metrics tables).",
            "domain_or_topic": "General research assistance; evaluated here on biomedical/systematic-review-derived questions.",
            "output_type": "Summaries / answers to questions derived from scientific literature; lengths varied (average output lengths reported in tables).",
            "evaluation_metrics": "Same SA and SF metrics used in this paper (UniEval, COMET, CTC, BERTScore, ROUGE-L, METEOR, chrF, GoogleBLEU, CharacTer).",
            "performance_results": "On SF metrics Elicit scored higher on BERTScore (0.807) and ROUGE-L (0.218) compared to Clinfo.ai in Unrestricted experiments; on SA (UniEval) Clinfo.ai outperformed Elicit (Clinfo.ai Unrestricted TL;DR Overall 0.880 vs Elicit 0.713).",
            "comparison_baseline": "Compared directly as a deployed system baseline against Clinfo.ai and LLM no-retrieval baselines.",
            "performance_vs_baseline": "Elicit performed better on several source-free metrics but worse on source-augmented UniEval overall compared to Clinfo.ai.",
            "key_findings": "Commercial research-assistant tools can produce strong surface-level scores (BERTScore/ROUGE), but may be outperformed by RetA systems tuned for provenance on source-augmented evaluators.",
            "limitations_challenges": "Tool behavior is closed-source and inconsistent for some queries (refused to answer a subset of PubMedRS-200 questions); inability to control indexed time window complicates fair comparison across evaluation regimes.",
            "scaling_behavior": "Not discussed in this paper for Elicit specifically.",
            "uuid": "e4398.4",
            "source_info": {
                "paper_title": "Clinfo.ai: An Open-Source Retrieval-Augmented Large Language Model System for Answering Medical Questions using Scientific Literature",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Statpearls SS",
            "name_full": "Statpearls Semantic Search",
            "brief_description": "A deployed semantic search tool (Statpearls semantic search / Hippocratic AI) for medical knowledge that uses LLMs or LLM-like components to return summarized medical information; used as a baseline in the paper.",
            "citation_title": "Hippocratic AI. statpearls semantic search.",
            "mention_or_use": "use",
            "system_name": "Statpearls Semantic Search (Statpearls SS)",
            "system_description": "A semantic search tool for medical knowledge (Statpearls) with a semantic-search interface; treated as a deployed baseline in benchmarking against Clinfo.ai. The internal model and pipeline are not described in this paper.",
            "llm_model_used": "Not specified in this paper (deployed tool).",
            "extraction_technique": "Proprietary semantic search and summarization pipeline (details not disclosed).",
            "synthesis_technique": "Proprietary summarization of retrieved medical knowledge to produce answers; specifics not provided here.",
            "number_of_papers": "Operates over Statpearls knowledge base and possibly other medical resources; number of documents not specified in this paper.",
            "domain_or_topic": "Medical knowledge / clinical information.",
            "output_type": "Summaries/answers for clinical questions.",
            "evaluation_metrics": "Same SA and SF automated metrics used for comparison in this work (UniEval, COMET, CTC, BERTScore, ROUGE-L, etc.).",
            "performance_results": "In SA (UniEval) metrics (146 questions), Statpearls SS had lower overall UniEval scores compared to Clinfo.ai (Table 1: overall ~0.728 vs Clinfo.ai higher values); SF metrics in Table 2 show mixed performance (e.g., BERTScore 0.770).",
            "comparison_baseline": "Compared as a deployed baseline versus Clinfo.ai and other LLM baselines.",
            "performance_vs_baseline": "Statpearls SS underperformed Clinfo.ai on UniEval SA metrics and was generally behind in multi-dimensional evaluators reported.",
            "key_findings": "Deployed medical semantic search tools can provide useful answers but their closed nature and differing index/update regimes make comparisons challenging; RetA open pipelines with explicit provenance may have advantages in SA evaluations.",
            "limitations_challenges": "Closed-source behavior, unknown indexing/date constraints, and refusal or inability to answer some questions complicate benchmarking; details of internal methods not available in this paper.",
            "scaling_behavior": "Not detailed in this paper.",
            "uuid": "e4398.5",
            "source_info": {
                "paper_title": "Clinfo.ai: An Open-Source Retrieval-Augmented Large Language Model System for Answering Medical Questions using Scientific Literature",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Soong et al. RetA",
            "name_full": "Improving accuracy of gpt-3/4 results on biomedical data using a retrieval-augmented language model",
            "brief_description": "A referenced study that evaluated GPT-3.5/GPT-4 against a custom retrieval-augmented LLM on biomedical questions and found the RetA model produced fewer hallucinations in human-evaluated experiments.",
            "citation_title": "Improving accuracy of gpt-3/4 results on biomedical data using a retrieval-augmented language model.",
            "mention_or_use": "mention",
            "system_name": "Soong et al. custom RetA model",
            "system_description": "A custom retrieval-augmented LLM evaluated on 19 biomedical questions (as cited); used retrieval to ground LLM outputs and compared hallucination rates against GPT-3.5 and GPT-4 in a human-judged study.",
            "llm_model_used": "GPT-3.5 and GPT-4 were evaluated as baselines; custom RetA model details are in the cited work (not expanded in this paper).",
            "extraction_technique": "Retrieval augmentation (details in original paper), used to reduce hallucinations.",
            "synthesis_technique": "Evidence-conditioned generation; synthesis approach not detailed here.",
            "number_of_papers": "Evaluated on a set of 19 biomedical questions.",
            "domain_or_topic": "Biomedical / medical QA.",
            "output_type": "Question-answering outputs with evidence grounding.",
            "evaluation_metrics": "Human judgments focused on hallucination/factuality (as summarized here).",
            "performance_results": "Referenced result: GPT-3.5 and GPT-4 exhibited more hallucinations across the 19 responses compared to the custom RetA model, per human evaluation summarized by authors.",
            "comparison_baseline": "GPT-3.5 and GPT-4 (no retrieval).",
            "performance_vs_baseline": "Custom RetA model showed fewer hallucinations than GPT-3.5/GPT-4 on the evaluated set.",
            "key_findings": "Retrieval augmentation can substantially reduce hallucinations in biomedical question-answering compared to bare LLM outputs, per human evaluation.",
            "limitations_challenges": "Original work relied on human evaluation on a small question set (19), limiting scalability of conclusions; details and reproducibility depend on the original paper.",
            "scaling_behavior": "Not discussed in this paper; small-scale human study.",
            "uuid": "e4398.6",
            "source_info": {
                "paper_title": "Clinfo.ai: An Open-Source Retrieval-Augmented Large Language Model System for Answering Medical Questions using Scientific Literature",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Almanac: Retrieval-augmented language models for clinical medicine.",
            "rating": 2,
            "sanitized_title": "almanac_retrievalaugmented_language_models_for_clinical_medicine"
        },
        {
            "paper_title": "Improving accuracy of gpt-3/4 results on biomedical data using a retrieval-augmented language model.",
            "rating": 2,
            "sanitized_title": "improving_accuracy_of_gpt34_results_on_biomedical_data_using_a_retrievalaugmented_language_model"
        },
        {
            "paper_title": "Elicit: The ai research assistant.",
            "rating": 2,
            "sanitized_title": "elicit_the_ai_research_assistant"
        },
        {
            "paper_title": "Retrieval-augmented generation for knowledgeintensive nlp tasks.",
            "rating": 2,
            "sanitized_title": "retrievalaugmented_generation_for_knowledgeintensive_nlp_tasks"
        },
        {
            "paper_title": "Ai chains: Transparent and controllable human-ai interaction by chaining large language model prompts.",
            "rating": 2,
            "sanitized_title": "ai_chains_transparent_and_controllable_humanai_interaction_by_chaining_large_language_model_prompts"
        },
        {
            "paper_title": "Scite: A smart citation index that displays the context of citations and classifies their intent using deep learning.",
            "rating": 1,
            "sanitized_title": "scite_a_smart_citation_index_that_displays_the_context_of_citations_and_classifies_their_intent_using_deep_learning"
        },
        {
            "paper_title": "Hippocratic AI. statpearls semantic search.",
            "rating": 1,
            "sanitized_title": "hippocratic_ai_statpearls_semantic_search"
        }
    ],
    "cost": 0.02000275,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>CLINFO.AI: AN OPEN-SOURCE RETRIEVAL-AUGMENTED LARGE LANGUAGE MODEL SYSTEM FOR ANSWERING MEDICAL QUESTIONS USING SCIENTIFIC LITERATURE
24 Oct 2023</p>
<p>Alejandro Lozano lozanoe@stanford.edu 
Department of Biomedical Data Science
Stanford University Stanford
CAUSA</p>
<p>Scott L Fleming scottyf@stanford.edu 
Department of Biomedical Data Science
Stanford University Stanford
CAUSA</p>
<p>Chia-Chun Chiang chiang.chia-chun@mayo.edu 
Department of Neurology
Mayo Clinic Rochester
MN Human Centered Artificial-Intelligence Institute Stanford University Stanford
CAUSA</p>
<p>Nigam Shah 
Center for Biomedical Informatics Research Clinical Excellence Research Center
Stanford University Technology and Digital Solutions Stanford Healthcare</p>
<p>CLINFO.AI: AN OPEN-SOURCE RETRIEVAL-AUGMENTED LARGE LANGUAGE MODEL SYSTEM FOR ANSWERING MEDICAL QUESTIONS USING SCIENTIFIC LITERATURE
24 Oct 2023CD2DFEE0F714C30F616F59DA1008A7FFarXiv:2310.16146v1[cs.IR]Large Language ModelsAbstractive SummarizationArtificial IntelligenceClinical MedicineGenerative AIInteractive SystemsChatGPT
The quickly-expanding nature of published medical literature makes it challenging for clinicians and researchers to keep up with and summarize recent, relevant findings in a timely manner.While several closed-source summarization tools based on large language models (LLMs) now exist, rigorous and systematic evaluations of their outputs are lacking.Furthermore, there is a paucity of high-quality datasets and appropriate benchmark tasks with which to evaluate these tools.We address these issues with four contributions: we release Clinfo.ai,an open-source WebApp that answers clinical questions based on dynamically retrieved scientific literature; we specify an information retrieval and abstractive summarization task to evaluate the performance of such retrieval-augmented LLM systems; we release a dataset of 200 questions and corresponding answers derived from published systematic reviews, which we name PubMed Retrieval and Synthesis (PubMedRS-200); and report benchmark results for Clinfo.aiand other publicly available OpenQA systems on PubMedRS-200.</p>
<p>The aggregation and distribution of medical knowledge, facilitated by platforms such as PubMed or Cochrane, enables healthcare professionals and medical researchers to stay abreast of the latest scientific discoveries and make informed decisions based on up-to-date scientific evidence [1].However, the staggering influx of more than 1 million papers each year into PubMed alone (equivalent to two papers per minute as of 2016) [2] highlights the daunting task of keeping up with scientific findings [3].This is especially true for practicing clinicians, who face the challenge of keeping track of the most updated research findings in all areas related to their patient care duties [4].</p>
<p>Existing technologies fail to adequately satisfy the information needs of health care professionals and researchers.In daily practice, clinicians have on average one care-related question for every other patient seen [5] and they refer to sources like PubMed or UpToDate to obtain summarized information answering these questions [6].Questions that cannot be answered within 2 to 3 minutes are often abandoned, potentially negatively impacting patient care and outcomes [5,7].While systematic review (SR) articles can provide quick answers to clinical questions, many questions are not answerable through existing reviews.On the other hand, manually synthesizing findings from multiple primary sources without the help of a published review article can be extraordinarily time consuming.Review articles take on average 67.3 weeks to complete [8], and those written reviews may not even include the most updated research published in the literature.Question-answering tools that leverage frequently updated external electronic resources would enable researchers and clinicians to obtain up-to-date information in a more efficient way that benefits scientific discovery and quality of patient care [9,10,11,12,13].</p>
<p>In previous decades, applications that integrated clinical systems with on-line information to answer users' information needs (e.g., "infobuttons") [14] were typically driven by semantic networks.Other works such as CHiQA proposed a combination of knowledge-based, machine learning, and deep learning approaches to develop a question-answering system using patient-oriented resources to answer consumer health questions [15].</p>
<p>The new capabilities of agents powered by large language models (LLM) has accelerated the development of automated literature summarization tools.Most of these solutions tend to be privately developed, closed-source solutions based on retrieval-augmented [16] (RetA) LLMs [17] (e.g.Scite [18], Elicit [19], GlacierMD [20], Consensus [21], OpenEvidence [22], Statpearls semantic search [23]).However, the paucity of publicly available technical reports describing these systems and the lack of appropriate guidelines, regulations, and evaluations to ensure their safe and responsible usage is an urgent concern [24].This Natural Language Generation (NLG) problem has been exacerbated by a lack of (1) representative datasets and associated tasks, and (2) automated metrics for evaluating RetA LLMs on said tasks.</p>
<p>Fortunately, developments in the LLM evaluation space have shown that a number of automated metrics correlate moderately with human preference, even in domain-specific scenarios (including medicine) [25,26,27].</p>
<p>Building on these advancements, we provide four contributions:</p>
<ol>
<li>
<p>Clinfo.ai1 , the first publicly available, open-source, end-to-end retrieval-augmented LLM-based system for querying and synthesizing the clinical literature.The system is hosted as a publicly available WebApp at https://www.clinfo.ai/.</p>
</li>
<li>
<p>An open information retrieval and abstractive summarization task specification designed to evaluate an algorithm's ability to both retrieve relevant information and adequately synthesize it.In the task setup, both the information retrieval and abstractive summarization sub-tasks are compared to gold standard (human generated but pragmatically retrieved) references and answers.Furthermore, our task is defined to truly resemble RetA deployment conditions (enabling the evaluation of already deployed but potentially closed-source systems).</p>
</li>
<li>
<p>PubMed Retrieval and Synthesis (PubMedRS-200), a publicly available dataset of 200 questions structured in Open QA format, paired with answers derived from systematic reviews and corresponding references.</p>
</li>
<li>
<p>Benchmark results for Clinfo.aiand other publicly available OpenQA systems on PubMedRS-200).</p>
</li>
</ol>
<p>Related Work</p>
<p>LLMs in healthcare The remarkable performance of LLMs in the general domain has brought about a revolution in the field of natural language processing [28], showcasing exceptional capabilities in tasks like summarization, question-answering, and NLG [29].Given their wide utility, researchers are now actively exploring applications of LLMs in healthcare [30,31,32,33].Several LLMs have achieved human-level performance on numerous medical professional licensing exams such as the United States Medical Licensing Exam (USMLE) [34].Other works have demonstrated promise in various healthcare-inspired tasks, such as automated clinical note generation and reasoning about public health topics [30,31,32,33].However, NLG tasks and publicly available benchmarks that directly address true medical needs are still underrepresented in the literature.Such tasks and benchmarks are especially important for estimating the capabilities and risks of LLMs in the clinical domain.</p>
<p>LLMs have several documented disadvantages and risks.First, updating LLMs with new knowledge and information is challenging and inefficient [35].Second, the training objective of LLMs to predict the most probable next token can cause these models to generate inaccurate information (hallucination), requiring costly and imperfect post-hoc model adjustments like reinforcement learning with human feedback (RLHF) [36].More importantly, most popular consumer-facing LLMs (e.g., OpenAI's GPT-4 [29], Meta's Llama 2 [37], Anthropic's Claude 2 [38]) do not provide references pointing to their source of information, even when the model's output is factual.This can engender distrust with users in many scientific domains, including healthcare.Prior work has proposed ReTA LLMs [16] to solve the information provenance issue and have shown promising results.These ReTA LLMs do not require post-hoc model editing in order to incorporate new knowledge.</p>
<p>Retrieval Augmentation Question Answering LLMs in Medicine Hiesinger et al. [39] introduced Almanac, a novel LLM integrated with a vector database and calculator, designed to answer 130 clinical questions generated by a panel of five board-certified clinicians and resident physicians.The results showed that Almanac surpassed a standard LLM (GPT-4) in factuality, safety, and correctness, indicating that retrieval systems lead to more accurate and reliable responses to clinical inquiries.Soong et al. [40] evaluated GPT-3.5 and GPT-4 models against a custom RetA LLM using a set of 19 questions.The evaluation, based solely on human judgments, revealed that both GPT-3.5 and GPT-4 exhibited more hallucinations in all 19 responses compared to the RetA model.While these works on RetA LLM systems represent significant progress, they suffer from at least two shortcomings: (1) they typically require human evaluation, making systematic benchmarking of new systems challenging and unscaleable; (2) they often focus solely on evaluating an LLM's output, disregarding the relevance of the information retrieved to generate an answer.Deciding which "relevant" sources should be summarized can be just as challenging as generating the actual summary.Hence there is a need for a benchmark that enables integrated evaluation of both a system's ability to select relevant documents as well as its ability to summarize these documents.</p>
<p>2 Materials and Methods PubMed is a free resource supporting search and retrieval of biomedical literature [41].As prior work has demonstrated, a large quantity of research papers available in this index are phrased as questions, and it is possible to structure them in a question-answer format [42,43].Extending this idea, we created an open information retrieval and abstractive summarization dataset, using SR as a proxy for inquiries of medical interest.The rationale is that SRs are structured reviews written by human experts which summarize the pertinent literature related to a question of interest in an evidence-based manner [44].In writing a SR, experienced authors (1) screen the published literature in a systematic way and include studies in a standardized manner; (2) critically evaluate methodology and reported outcomes of the included studies; and (3) carefully extract data, summarize original research findings, and in some instances, conduct additional statistical analysis of extracted results from studies including randomized controlled trials, observational cohort studies, case series and other qualitative studies on a specific topic.Furthermore, SRs are extensively used to provide evidence for various purposes, including policy-making, clinical practice guidelines, health technology assessment, and decision making in healthcare [45].As SRs unify and present a comprehensive overview of a given subject by human experts, we chose to leverage published SRs as gold standards when building our database.</p>
<p>Dataset Generation</p>
<p>To populate such a dataset, we employed E-utilities, a public API to the NCBI Entrez system [41] , to access PubMed and construct question-answer pairs with their respective references.Figure 1 illustrates our process in detail.First, we established a comprehensive selection of medical specialties and subspecialties.Second, we formulated a query to retrieve Systematic Reviews relevant to each medical specialty/subspecialty.Upon constructing the specialty-specific queries and retrieving associated abstracts, we retrieved all papers structured in a format that can be easily converted to questions-answer pairs (as noted by Jin et al 2019 [42]) namely Title, Introduction, Conclusion, and References.Third, we applied another filtering process, narrowing down to solely those publications whose titles included an explicit question (i.e., publications whose titles including question marks).The questions from these titles were extracted.</p>
<p>Finally, two human evaluators (AL and SF) manually reviewed the retrieved questions and extracted an answer to each question using minimally modified text from the results and conclusions section of the corresponding SR abstract.</p>
<p>Concretely, in order to generate each answer, the human reviewers removed from the Results and Conclusions section of the abstract any text describing the structure or design of the systematic review (e.g., "We used PubMed to retrieve 100 papers"), leaving only text that directly addressed the question extracted from the SR's title.In the process, abstracts that were lacking substantive results and abstracts that merely described research proposals (e.g.descriptions of future work) were entirely removed.</p>
<p>Clinfo.ai: An LLM Chain for Information Retrieval and Synthesis</p>
<p>Our proposed RetA LLM system, Clinfo.ai,consists of a collection of four LLMs working conjointly (an LLM chain [46]) coupled to a Search Index (either PubMed or Semantic Scholar) as depicted in Figure 2. Previous works have observed that very large language models (e.g., 100B parameters or more) exhibit zero-shot reasoning capabilities, where task-specification prompts can be used to guide the LLM output without further fine-tuning [47,48].We leverage the zero-shot reasoning capabilities of two LLMs, specifically OpenAI's GPT-3.5 and GPT-4 models, to complete each step in the LLM chain depicted in Figure 2. All prompts used in each step of the chain are available in the supplemental material2 .We use LangChain's API to send prompts and receive outputs from GPT-3.5 and GPT-4.While different models could technically be used through this entry point, our experiments are limited to OpenAI's GPT-3.5 and GPT-4 models (snapshots gpt-3.5-turbo-0613, gpt-4-0613 respectively).For both models, we employ a temperature of 0.5 and a max token generator limit of 1024.</p>
<p>Figure 2: Clinfo.ai:A RetA LLM system for retrieving and summarizing scientific articles</p>
<p>Query Generator</p>
<p>In our Clinfo.aisystem, the input is the question submitted by the user.Once a question is submitted, the primary task of the query generator (labeled "Question2Query" in Figure 2) is to construct a PubMed (or Semantic Scholar) query that efficiently retrieves a substantial number of relevant articles pertaining to the posed question.This is achieved by instructing the model to incorporate the most crucial and relevant keywords that accurately represent the query's context and requirements.</p>
<p>Figure 3: Query Generated by Clinfo.ai for question: "Does high-grade dysplasia/carcinoma in situ of the biliary duct margin affect the prognosis of extrahepatic cholangiocarcinoma?"</p>
<p>Information Retriever</p>
<p>In a similar fashion to the Dataset Generation process, we utilize the Entrez API to fetch abstracts from PubMed using the output generated by the Query Generator.By leveraging the Entrez API, we are able to programmatically access and retrieve the relevant abstracts that match the constructed PubMed queries.Because LLM output is stochastic and different queries may capture different aspects of the literature, we take the union of all papers returned by three LLM-generated queries (each with the same prompt but different seeds).</p>
<p>Relevance Classifier</p>
<p>Since the query generator emphasizes recall over precision (i.e., it retrieves as many potentially relevant articles as possible), it is crucial to classify the relevancy of the retrieved articles.To achieve this, we adopt an LLM-enabled binary classification approach, wherein each article is categorized as either relevant or not relevant to the posed question using GPT-3.5.Once the relevant articles are identified, we make use of the full abstract metadata of each article to construct their citations in the IEEE format.If more than 35 relevant articles are deemed relevant, the user can decide to re-rank and filter them using BM25 [49].</p>
<p>Summarization</p>
<p>The penultimate step in Clinfo.aiuses an LLM to summarize each relevant abstract within the context of the usersubmitted question.</p>
<p>Synthesis</p>
<p>In the final step of Clinfo.ai, the relevant article summaries are organized as an ordered list, with each number in the list corresponding to a citation.This structured list of article summaries is then fed to a LLM with the task of constructing a concise and informative summary.The LLM is also instructed to utilize only the provided article summaries and no other additional information, relying on the structured list of citations to reference and accurately attribute each finding.To facilitate interaction with our system, we developed a web application that allows users to submit their own questions and/or customize the prompts.The latter enables users to tailor the system according to their individual preferences and needs, as illustrated in Figure 4.The entire process provides real-time access, displaying the queries generated during the search (as shown in Figure 3), the number of retrieved articles, a concise summary of each important article, and a final "Literature Summary" (or "Synthesis", to distinguish it from the individual article summaries) accompanied by an abbreviated answer to the question ("TL;DR").Additionally, the references are presented as hyperlinks, enabling users to verify both the validity of the reference and the information captured from it.It is possible that even after summarizing an article's abstract, Clinfo.aimay not include that article in the final Literature Summary or "TL;DR".Nevertheless, we ensure that all relevant articles are presented to the user so that they can access and explore them as needed.An example of a final Literature Review constructed with Clinfo.ai is shown in Figure 5 Figure 5: "Literature Summary" (Synthesis) and "TL;DR" constructed with Clinfo.ai for the question, "Does high-grade dysplasia/carcinoma in situ of the biliary duct margin affect the prognosis of extrahepatic cholangiocarcinoma?" (not all references are included in figure)</p>
<p>Task Description and Evaluation</p>
<p>The task is defined in a three step manner:</p>
<ol>
<li>
<p>Given a question, generate a query to retrieve a set of articles;</p>
</li>
<li>
<p>Given the provided articles, determine their relevancy to the question;</p>
</li>
<li>
<p>Given relevant articles, summarize the findings.</p>
</li>
</ol>
<p>Step ( 2) is evaluated based on precision and recall.Considering the set of all documents D, RET (D, k) denotes the set of k retrieved documents deemed relevant and REL(D, q) the set of all documents referenced by a SR.We define precision and recall in this context as follows:
precision = |RET (D, k)  REL(D, q)| |RET (D, k)| (1) recall = |RET (D, k)  REL(D, q)| |REL(D, q)|(2)
Step (3) is conducted using both source-free (SF) and source-augmented (SA) automated metrics.Source-free metrics compare a model's output to a gold standard reference summary, without including any information from the articles used to generated the gold standard summary.For our evaluation purposes, the gold standard is the human-curated answer (derived from conclusions and/or results of each SR).On the other hand, SA metrics additionally consider relevant context to evaluate the quality of model-generated outputs.For our experiments, context is constructed by concatenating a SR's introduction, results, and conclusion sections.The SA metrics we employed (and the LMs they use) include UniEval [26] (T5 -large), COMET (XLM-RoBERTa) [50], and CTC Summary Consistency (BERT) [51].</p>
<p>UniEval is a multi-dimensional evaluator designed for summarization tasks and takes into account four key dimensions (and their corresponding overall average):</p>
<p> Coherence: Assesses whether the summary forms a cohesive and rational body of text;</p>
<p> Consistency: Evaluates the factual alignment between the information presented in the summary and the content of the source document;</p>
<p> Fluency: Assesses the readability and linguistic fluency of a summary;</p>
<p> Relevance: Measures whether the summary contains only the important information from the source document.</p>
<p>COMET is an evaluation metric developed to assess the quality of Machine Translation (MT) systems.Despite being trained on multilingual MT outputs, it performs remarkably well in monolingual settings, when predicting summarization output quality [52].CTC is an evaluation framework, based on information alignment between input, output, and context, for compression (e.g summary), transduction (e.g translation), and creation (e.g.conversation).</p>
<p>Finally we perform an evaluation using SF metrics, including BERTScore [53], ROUGE-L [54], METEOR [55], chrF [56] , GoogleBLEU (based on [57]), CTC Summary (without providing context) , and CharacTer [58].The majority of these metrics have shown moderate correlation with human preference and are widely reported in NLG tasks [25,26].</p>
<p>The multi-dimensional evaluation based on source-augmented metrics makes the assumption that an LLM+RetA model is able to (1) retrieve abstracts of works that were deemed relevant by an author of a SR and (2) synthesize them in a similar fashion.We acknowledge that if this assumption is not met, the evaluation would heavily penalize the output.Conversely, if the system retrieves an article that was not considered by a SR but bears a similar semantic meaning to an article present in the references of a SR, the evaluation would not penalize the generated text.For our proposed method, both behaviors are desired.Using our proposed task, we evaluated the performance of GPT-4 and GPT-3.5 without retrieval augmentation, Clinfo.ai(our GPT-enabled RetA LLM system), and two deployed tools: Elicit (an AI research assistant based on LLMs, designed for facilitating literature review generation, accessed on 07-02-2023), and Statpearls Semantic Search (a free search tool for medical knowledge, accessed on 07-25-2023).While other automated literature summarization systems are available, at the time of this study the vast majority require a subscription to answer multiple questions.Additionally, a subset of these systems refused to provide an answer to a significant number of the PubMedRS-200 questions as posed, making evaluation for these systems fraught and difficult to interpret.We exclude these systems from our analysis.Lastly, since our framework generates two outputs -"TL;DR" and "Literature Summary" (also referred to as "Synthesis") -we conducted evaluations of three forms of Clinfo.ai'soutput: (1) the synthesis of the articles retrieved and deemed relevant ("Synthesis"); (2) the abbreviated summary distilling the proposed "Synthesis" into one or two sentences ("TL;DR"); (3) the combined "Synthesis" and "TL;DR".</p>
<p>Baselines and Experiments</p>
<p>We recognize that the usage of scientific literature to extract question-answer pairs comes with the possibility that an answer deemed correct at the time of acquisition may be incorrect as new discoveries are published.To ensure that a system is not rewarded for simply copy-pasting the text of a retrieved source SR nor penalized when new relevant articles are published, we consider three evaluation regimes:</p>
<ol>
<li>Restricted Search (RS): The retrieval process is constrained to include publications up to one day before the publication date.While this approach may not guarantee the retrieval of all publications considered important by the authors of each source systematic review, it effectively narrows down the search space to the subset of publications that could have been retrieved and deemed relevant during the review's preparation.</li>
</ol>
<p>Source Dropped (SD):</p>
<p>The retrieval process can retrieve articles published both before and after the source systematic review.However, if the source SR is retrieved, it is removed from the set of relevant articles and not used in the subsequent steps of the summarization process.</p>
<p>Unrestricted Search (US)</p>
<p>No restriction is applied; the source SR may (but need not) be included in the set of relevant articles retrieved by the system.Because we could not control the set of articles retrieved and summarized by closed-source tools like Elicit and Statpearls SS, they effectively fall within this evaluation regime.</p>
<p>Finally, to ensure that conformity with the SD regime would not prevent direct comparison with the other evaluation regimes, we removed questions from all other training regimes for which Clinfo.ai could only retrieve the source article (resulting in zero articles remaining after exclusion under the SD regime).This yielded 145 SRs (80 after October 2021 and 65 before).As reported in previous studies [39,34,59], both GPT-3.5 and GPT-4 without RetA demonstrated strong zero-shot performance using both source-augmented (Table 1) and source-free (Table 2) metrics.Notably, there was no substantial performance drop observed when these models were presented with questions based on source SRs published after September 2021 (Comparing Table 1 and Table S1 in the Supplement).While more studies are necessary, we postulate that this can be attributed to the models' exposure to prior published works during training.Since SRs are built upon existing literature ranging across multiple years, it is plausible that the models have been trained on relevant information that aids them in providing accurate responses to questions based on newer research.However, comparing all LLM against LLM + RetA models, the inclusion of RetA leads to a slight improvement in the overall performance of the models when evaluated with SF and SA automated metrics, irrespective of the publication date of the source SR.</p>
<p>Previous works based on human evaluation have observed a similar trend, corroborating our automated evaluation framework.</p>
<p>How does Clinfo.ai perform compared to other systems?</p>
<p>As depicted in Table 1, Clinfo.ai exhibited better performance in overall UniEval compared to other RetA systems, irrespective of the chosen output strategy (Synthesis, TL;DR, or a concatenation of the two).This improvement in performance remained consistent regardless of the average length of the output, with Clinfo.aiachieving better results for both approximately 3x shorter (TL;DR) and around 2x longer outputs (Synthesis).Furthermore, this performance persisted across all different evaluation regimes, even when the source SR was dropped.This improvement amounted to at least 6.2% and at most 14.9% in UniEval Overall performance.These results suggest two significant points: (1) Our system is not merely copying and pasting information from an SR review.Instead, it demonstrates a genuine ability to process and present the information effectively, resulting in enhanced performance compared to other available tools; and (2) even in the absence of a source SR, Clinfo.aican still provide conclusions that are better aligned with a source SR's conclusion (compared to tools that might include the source SR).</p>
<p>TL;DR or Synthesis?</p>
<p>Clinfo.ai TL;DR demonstrates significantly better performance compared to Synthesis and Synthesis &amp; TL;DR, even though they all utilize the same relevant retrieved articles.It is worth noting that while Synthesis provides evidence to answer the question based on the retrieved articles, this evidence may not align with the original evidence reported by a Systematic Review (SR).However, the increased performance of TL;DR could be attributed to the LLM's capability to correctly identify the most salient points of the relevant articles and effectively summarize them.On the other hand, using only source-free (SF) metrics (Table 2), Elicit performs better under BERTScore, ROUGE-L and GoogleBLEU, while Clinfo.aiTL;DR performs better under METEOR, chrF, CTC (SF), and CharacTer.</p>
<p>These results highlight a potential limitation of automated evaluation .For instance, SF metrics tend to reward short responses, which may not necessarily be accurate or comprehensive.On the other hand, several SA metrics can assign the best score to considerably larger generations (UniEval's Coherence and Relevance, and COMET), acknowledging their quality and relevance.This discrepancy in evaluation metrics raises concerns about the fair assessment of model performance and emphasizes the need for a comprehensive evaluation approach.</p>
<p>Comparing different evaluation regimes, the best performance was observed under the Unrestricted Search evaluation regime, possibly due to the fact that the source SR was retrieved on 96.5% of the questions.As expected given the restricted set of retrievable documents, Clinfo.ai'sprecision was highest under the Restricted Search regime (Table 3).</p>
<p>Conclusion</p>
<p>The rapidly expanding medical literature and the capabilities of LLMs to process and summarize vast amounts of information have led to the development of several tools that utilize LLMs to generate on-demand summaries of published scientific literature.However, the lack of high-quality datasets and appropriate benchmarking tasks has hindered rigorous evaluations of these tools.To address this gap, we have introduced Clinfo.ai,an open-source end-to-end LLM-chain workflow designed to query, evaluate, and synthesize medical literature into concise summaries for answering questions on demand.Additionally, we introduce a unique dataset, PubMedRS-200, which consists of questions and answers extracted from systematic reviews, enabling automatic evaluation of LLM performance in Retrieval Augmentation Question Answering.Our tools and benchmarking dataset are publicly available to ensure reproducibility and to facilitate further research in harnessing LLMs for Retrieval Augmentation Question Answering tasks.</p>
<p>Limitations</p>
<p>In this study, we employed automated metrics that have demonstrated moderate-to-high correlation with human preferences, but we did not explicitly solicit human preferences to evaluate the RetA LLM systems considered.Future work should consider including human evaluation to ensure alignment of automated metrics and human preferences.Lastly, it is worth noting that prior studies have reported that LLMs demonstrate the ability to generate accurate Boolean operators and syntax, effectively adhering to PubMed query formats.However, our observations revealed that these models also generated hallucinated MeSH terms, which could potentially lead to the exclusion of relevant studies.To overcome this limitation, future research efforts should prioritize improving the query generation process, ensuring that generated MeSH terms are reliable and relevant for better precision and recall in medical literature search tasks.</p>
<p>Figure 1 :
1
Figure 1: Schematic Representation of the Protocol for Retrieving Abstracts from PubMed and Generating Title-Based Questions</p>
<ol>
<li>3 Figure 4 :
34
Figure 4: Clinfo.aiuser interface</li>
</ol>
<p>Figure 6 :
6
Figure 6: UniEval Overall Score of 146 questions (unconstrained by published date) from PubMedRS-200 distribution across Unrestricted Search (GPT3.5 and GPT4 zero-shot performance is added)</p>
<p>Table 1 :
1Unified Multi-Dimensional Evaluator (UniEval)CTC (SA)ModelCoherence  Consistency Fluency Relevance Overall COMET Consistency Avg. LengthLLMGPT-3.50.908 (0.149) 0.694 (0.144) 0.947 (0.059) 0.939 (0.101) 0.872 (0.082) 0.676 (0.075) 0.865 (0.017) 104.834 (47.778)GPT-40.915 (0.099) 0.655 (0.145) 0.942 (0.051) 0.929 (0.078) 0.86 (0.062) 0.677 (0.075) 0.866 (0.017)84.214 (39.772)LLM + RetARestricted SearchSynthesis &amp; TL;DR 0.949 (0.065) 0.466 (0.105) 0.903 (0.104) 0.964 (0.053) 0.82 (0.055) 0.704 (0.055)0.84 (0.014)205.579(46.181)Synthesis0.925 (0.066)0.394 (0.11)0.893 (0.119) 0.939 (0.101) 0.788 (0.059) 0.693 (0.057) 0.842 (0.015) 165.814 (40.749)TL;DR0.866 (0.143) 0.787 (0.161) 0.954 (0.018) 0.826 (0.159) 0.858 (0.098) 0.665 (0.078) 0.874 (0.018)38.766 (11.682)Source DroppedSynthesis &amp; TL;DR 0.942 (0.092) 0.465 (0.104) 0.918 (0.085) 0.962 (0.059) 0.822 (0.055) 0.706 (0.056) 0.843 (0.014) 204.248 (38.394)Synthesis0.925 (0.066) 0.398 (0.112) 0.912 (0.096) 0.943 (0.055) 0.795 (0.055) 0.695 (0.059) 0.845 (0.016) 164.938 (33.221)TL;DR0.829 (0.202) 0.763 (0.197) 0.953 (0.029) 0.796 (0.194)0.835(0.13)0.672 (0.078) 0.876 (0.017)38.31 (10.726)Unrestricted SearchOur ModelsSynthesis &amp; TL;DR 0.945 (0.064) 0.539 (0.127) 0.912 (0.096) 0.962 (0.059) 0.84 (0.052) 0.721 (0.055) 0.852 (0.017) 214.338 (44.173)Synthesis0.916 (0.092)0.48 (0.142)0.904 (0.098) 0.935 (0.069) 0.809 (0.06) 0.712 (0.057) 0.855 (0.019) 173.379 (38.492)TL;DR0.896 (0.123)0.81 (0.159)0.955 (0.012) 0.857 (0.135) 0.88 (0.081) 0.681 (0.072)0.88 (0.016)39.959 (11.754)Deployed ModelsElicit [19]0.854 (0.136) 0.352 (0.147) 0.743 (0.151) 0.902 (0.117) 0.713 (0.085)0.7 (0.066)0.866 (0.017) 130.566 (22.946)Statpearls SS [23]0.753 (0.225) 0.383 (0.129)0.93 (0.053) 0.845 (0.159) 0.728 (0.112) 0.633 (0.075) 0.841 (0.016) 118.172 (26.603)
Performance on 146 questions from PubMedRS-200 using source-augmented (SA) metrics: UniEval (T5-large), COMET (XLM-RoBERTa), CTC summary (BERT)</p>
<p>Table 2 :
2
Performance on 146 questions from PubMedRS-200 using source-free (SF) metrics
ModelBERTScore  ROUGE-L METEOR chrF GoogleBLEU CTC (SF) CharacTer Avg. LengthLLMGPT-3.50.781 (0.037) 0.165 (0.053) 0.181 (0.073) 30.2 (10.5)0.077 (0.036)0.575 (0.065) 0.912 (0.102) 104.834 (47.778)GPT-40.78 (0.037)0.157 (0.049) 0.192 (0.07) 31.6 (9.06)0.074 (0.031)0.571 (0.064) 0.89 (0.099)84.214 (39.772)LLM + RetARestricted SearchSynthesis &amp; TL;DR0.77 (0.028)0.135 (0.043) 0.121 (0.055) 21.5 (9.98)0.058 (0.03)0.527 (0.059) 0.993 (0.029) 205.579(46.181)Synthesis0.773 (0.028) 0.141 (0.044) 0.133 (0.059) 24.3 (10.4)0.063 (0.032)0.533 (0.06) 0.976 (0.056) 165.814 (40.749)TL;DR0.784 (0.041) 0.145 (0.068) 0.221 (0.089) 32.7 (7.67)0.061 (0.043)0.594 (0.068) 0.833 (0.086) 38.766 (11.682)Source DroppedSynthesis &amp; TL;DR 0.773 (0.028) 0.136 (0.037) 0.119 (0.054) 21.4 (9.69)0.057 (0.028)0.53 (0.06)0.989 (0.036) 204.248 (38.394)Synthesis0.775 (0.026) 0.143 (0.038) 0.132 (0.057) 24.1 (9.91)0.061 (0.043)0.536 (0.06) 0.976 (0.056) 164.938 (33.221)TL;DR0.787 (0.041) 0.148 (0.064) 0.218 (0.078)33 (6.98)0.06 (0.039)0.6 (0.066)0.83 (0.092)38.31 (10.726)Unrestricted SearchOur ModelsSynthesis &amp; TL;DR 0.786 (0.029)0.167 (0.06) 0.145 (0.073) 23.5 (11.2)0.079 (0.046)0.546 (0.067) 0.989 (0.036) 214.338 (44.173)Synthesis0.789 (0.03)0.178 (0.067) 0.164 (0.084)26.7 (12)0.088 (0.051)0.555 (0.07) 0.975 (0.065) 173.379 (38.492)TL;DR0.793 (0.038) 0.169 (0.076) 0.252 (0.092) 35.5 (7.95)0.076 (0.049)0.61 (0.067) 0.825 (0.094) 39.959 (11.754)Deployed ModelsElicit [19]
0.807 (0.04) 0.218 (0.095) 0.206 (0.093) 31.6 (12.5) 0.127 (0.085) 0.596 (0.07) 0.938 (0.096) 130.566 (22.946)Statpearls SS [23] 0.77 (0.028) 0.136 (0.037) 0.149 (0.057) 26.5 (9.8) 0.062 (0.026) 0.536 (0.06) 0.939 (0.09) 118.172 (26.603)</p>
<p>Table 3 :
3
Clinfo.ai Precision and Recall on PubMedRS-200
Evaluation RegimePrecision Recall Source IncludedRestricted Search0.224 (0.239) 0.057 (0.061)0.0 (0.0)Source Dropped0.186 (0.22)0.064 (0.064)0.0 (0.0)Unrestricted Search 0.162 (0.175) 0.052 (0.064)0.965 (0.185)4 Experimental Results and AnalysisIs RetA associated with significant improvements in automated metric evaluation?
https://github.com/som-shahlab/Clinfo.AI
https://github.com/som-shahlab/Clinfo.AI/tree/main/SupplementalMaterial
AcknowledgmentsAL is funded by Arc Institute.SF is supported by a Stanford Graduate Fellowship.This effort was supported in part by the Mark and Debra Leslie endowment for AI in Healthcare.We thank Will Haberkorn for his aid with FigureS1.
How to keep up to date with medical information using web-based resources: A systematised review and narrative synthesis. Konstantinos I Bougioukas, C Emmanouil, Bouras, Theodore Konstantinos I Avgerinos, Anna-Bettina Dardavessis, Haidich, Health Information &amp; Libraries Journal. 3742020</p>
<p>Scientific literature: Information overload. Esther Landhuis, Nature. 53576122016</p>
<p>An open source machine learning framework for efficient and transparent systematic reviews. Rens Van De Schoot, Jonathan De Bruin, Raoul Schram, Parisa Zahedi, Jan De Boer, Felix Weijdema, Bianca Kramer, Martijn Huijts, Maarten Hoogerwerf, Gerbrich Ferdinands, Nature machine intelligence. 322021</p>
<p>Information-seeking behaviors of practitioners in a primary care practice-based research network (pbrn). Kevin A James E Andrews, Carol Pearce, Margaret M Ireson, Love, Journal of the Medical Library Association. 9322062005</p>
<p>Clinical questions raised by clinicians at the point of care: a systematic review. Guilherme Del Fiol, Elizabeth Workman, Paul N Gorman, JAMA internal medicine. 17452014</p>
<p>Clinical information seeking behavior of physicians: A systematic review. Azra Daei, Reza Mohammad, Hasan Soleymani, Ali Ashrafi-Rizi, Roya Zargham-Boroujeni, Kelishadi, International journal of medical informatics. 1391041442020</p>
<p>Answering physicians' clinical questions: obstacles and potential solutions. John W Ely, Jerome A Osheroff, Lee Chambliss, Mark H Ebell, Marcy E Rosenbaum, Journal of the American Medical Informatics Association. 1222005</p>
<p>Analysis of the time and workers needed to conduct systematic reviews of medical interventions using data from the prospero registry. Rohit Borah, Andrew W Brown, Patrice L Capers, Kathryn A Kaiser, BMJ open. 72e0125452017</p>
<p>Context-sensitive decision support (infobuttons) in electronic health records: a systematic review. Miguel T David A Cook, Bret Se Teixeira, James J Heale, Guilherme Cimino, Fiol Del, Journal of the American Medical Informatics Association. 2422017</p>
<p>Enabling health care decisionmaking through clinical decision support and knowledge management. David Lobach, Gillian D Sanders, Tiffani J Bright, Anthony Wong, Ravi Dhurjati, Erin Bristow, Lori Bastian, Remy Coeytaux, Gregory Samsa, Vic Hasselblad, Evidence report/technology assessment. 2032012</p>
<p>Association of a clinical knowledge support system with improved patient safety, reduced complications and shorter length of stay among medicare beneficiaries in acute care hospitals in the united states. Gary T Peter A Bonis, David M Pickens, David A Rind, Foster, International journal of medical informatics. 77112008</p>
<p>Use of uptodate and outcomes in us hospitals. Thomas Isaac, Jie Zheng, Ashish Jha, Journal of hospital medicine. 722012</p>
<p>Relationship of electronic medical knowledge resource use and practice characteristics with internal medicine maintenance of certification examination scores. Colin P Darcy A Reed, Eric S West, Andrew J Holmboe, Rebecca S Halvorsen, Carola Lipner, Jacobs, Furman, Mcdonald, Journal of general internal medicine. 272012</p>
<p>Supporting infobuttons with terminological knowledge. Gai James J Cimino, Qing Elhanan, Zeng, 1997</p>
<p>Consumer health information and question answering: helping consumers find answers to their health-related information needs. Dina Demner-Fushman, Yassine Mrabet, Asma Ben, Abacha , Journal of the American Medical Informatics Association. 2722020</p>
<p>Retrieval-augmented generation for knowledgeintensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kttler, Mike Lewis, Wen-Tau Yih, Tim Rocktschel, Advances in Neural Information Processing Systems. 202033</p>
<p>Qiao Jin, Robert Leaman, Zhiyong Lu, arXiv:2307.09683Pubmed and beyond: Recent advances and best practices in biomedical literature search. 2023arXiv preprint</p>
<p>Scite: A smart citation index that displays the context of citations and classifies their intent using deep learning. Milo Josh M Nicholson, Patrice Mordaunt, Ashish Lopez, Domenic Uppala, Rosati, P Neves, Peter Rodrigues, Sean C Grabitz, Rife, Quantitative Science Studies. 232021</p>
<p>Elicit: The ai research assistant. Ought, 2023</p>
<p>Glaciermd -a modern physician reference. Glaciermd, 2023</p>
<p>. Consensus, Consensus, 2023</p>
<p>Making medical knowledge more useful, open, accessible, and understandable. Openevidence, Openevidence, 2023</p>
<p>Hippocratic AI. statpearls semantic search. 2023</p>
<p>Chatgpt utility in healthcare education, research, and practice: Systematic review on the promising perspectives and valid concerns. Malik Sallam, Healthcare. 1162023</p>
<p>Nlg evaluation metrics beyond correlation analysis: An empirical metric preference checklist. Iftitahu Ni, ' Mah, Meng Fang, Vlado Menkovski, Mykola Pechenizkiy, arXiv:2305.085662023arXiv preprint</p>
<p>Towards a unified multi-dimensional evaluator for text generation. Ming Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu Jiao, Pengfei Liu, Chenguang Zhu, Ji Heng, Jiawei Han, arXiv:2210.071972022arXiv preprint</p>
<p>Medalign: A clinician-generated dataset for instruction following with electronic medical records. Alejandro Scott L Fleming, William J Lozano, Jenelle A Haberkorn, Eduardo P Jindal, Rahul Reis, Louis Thapa, Julian Z Blankemeier, Ethan Genkins, Ashwin Steinberg, Nayak, arXiv:2308.140892023arXiv preprint</p>
<p>Fine-tuning language models to find agreement among humans with diverse preferences. Michiel Bakker, Martin Chadwick, Hannah Sheahan, Michael Tessler, Lucy Campbell-Gillingham, Jan Balaguer, Nat Mcaleese, Amelia Glaese, John Aslanides, Matt Botvinick, Advances in Neural Information Processing Systems. 202235</p>
<p>Sbastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023arXiv preprint</p>
<p>Chatgpt utility in healthcare education, research, and practice: systematic review on the promising perspectives and valid concerns. Malik Sallam, Healthcare. MDPI202311887</p>
<p>The role of chatgpt, generative language models, and artificial intelligence in medical education: a conversation with chatgpt and a call for papers. Gunther Eysenbach, JMIR Medical Education. 91e468852023</p>
<p>Evaluating the feasibility of chatgpt in healthcare: an analysis of multiple clinical and research scenarios. Marco Cascella, Jonathan Montomoli, Valentina Bellini, Elena Bignami, Journal of Medical Systems. 471332023</p>
<p>Assessing the potential of usmle-like exam questions generated by gpt-4. medRxiv. Keith Scott L Fleming, Aswathi M Morse, Chia-Chun Kumar, Birju Chiang, Emma P Patel, Nigam Brunskill, Shah, 2023</p>
<p>Performance of chatgpt on usmle: Potential for ai-assisted medical education using large language models. Tiffany H Kung, Morgan Cheatham, Arielle Medenilla, Czarina Sillos, Lorie De Leon, Camille Elepao, Maria Madriaga, Rimel Aggabao, Giezel Diaz-Candido, James Maningo, PLoS digital health. 22e00001982023</p>
<p>Memory-based model editing at scale. Eric Mitchell, Charles Lin, Antoine Bosselut, Christopher D Manning, Chelsea Finn, International Conference on Machine Learning. PMLR2022</p>
<p>Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Dong, arXiv:2303.18223A survey of large language models. 2023arXiv preprint</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothe Lachaux, Baptiste Lacroix, Naman Rozire, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>. Anthropic, Claude, 2, 2023</p>
<p>Almanac: Retrieval-augmented language models for clinical medicine. William Hiesinger, Cyril Zakka, Akash Chaurasia, Rohan Shad, Alex Dalal, Jennifer Kim, Michael Moor, Kevin Alexander, Euan Ashley, Jack Boyd, 2023</p>
<p>David Soong, Sriram Sridhar, Han Si, Jan-Samuel Wagner, Ana Caroline, Costa S, Christina Y Yu, Kubra Karagoz, Meijian Guan, Hisham Hamadeh, Brandon W Higgs, arXiv:2305.17116Improving accuracy of gpt-3/4 results on biomedical data using a retrieval-augmented language model. 2023arXiv preprint</p>
<p>Database resources of the national center for biotechnology information in 2023. Eric W Sayers, Evan E Bolton, Rodney Brister, Kathi Canese, Jessica Chan, Donald C Comeau, Catherine M Farrell, Michael Feldgarden, Anna M Fine, Kathryn Funk, Nucleic acids research. 51D12023</p>
<p>Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William W Cohen, Xinghua Lu, arXiv:1909.06146Pubmedqa: A dataset for biomedical research question answering. 2019arXiv preprint</p>
<p>Generating better queries for systematic reviews. Harrisen Scells, Guido Zuccon, The 41st international ACM SIGIR conference on research &amp; development in information retrieval. 2018</p>
<p>Towards semantic-driven boolean query formalization for biomedical systematic literature reviews. Mohammadreza Pourreza, Faezeh Ensan, International Journal of Medical Informatics. 1049282022</p>
<p>Systematic reviews to support evidence-based medicine. Khalid Khan, Regina Kunz, Jos Kleijnen, Gerd Antes, 2011Crc press</p>
<p>Ai chains: Transparent and controllable human-ai interaction by chaining large language model prompts. Tongshuang Wu, Michael Terry, Carrie Jun Cai, Proceedings of the 2022 CHI conference on human factors in computing systems. the 2022 CHI conference on human factors in computing systems2022</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Advances in Neural Information Processing Systems. H Larochelle, M Ranzato, R Hadsell, M F Balcan, H Lin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec RadfordCurran Associates, Inc202033Ilya Sutskever, and Dario Amodei</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, ( Shixiang, Machel Shane) Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in Neural Information Processing Systems. S Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, A Oh, Curran Associates, Inc202235</p>
<p>Improvements to bm25 and language models examined. Andrew Trotman, Antti Puurula, Blake Burgess, Proceedings of the 19th Australasian Document Computing Symposium. the 19th Australasian Document Computing Symposium2014</p>
<p>Comet: A neural framework for mt evaluation. Ricardo Rei, Craig Stewart, Ana C Farinha, Alon Lavie, arXiv:2009.090252020arXiv preprint</p>
<p>Compression, transduction, and creation: A unified framework for evaluating natural language generation. Mingkai Deng, Bowen Tan, Zhengzhong Liu, Eric P Xing, Zhiting Hu, arXiv:2109.063792021arXiv preprint</p>
<p>From comet to comes-can summary evaluation benefit from translation evaluation?. Krubi0144ski Mateusz, Pavel Pecina, Proceedings of the 3rd Workshop on Evaluation and Comparison of NLP Systems. the 3rd Workshop on Evaluation and Comparison of NLP Systems2022</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, Bertscore, arXiv:1904.09675Evaluating text generation with bert. 2019arXiv preprint</p>
<p>Rouge: A package for automatic evaluation of summaries. Chin-Yew Lin, Text summarization branches out. 2004</p>
<p>Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. Satanjeev Banerjee, Alon Lavie, Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization. the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization2005</p>
<p>chrf: character n-gram f-score for automatic mt evaluation. Maja Popovi, Proceedings of the tenth workshop on statistical machine translation. the tenth workshop on statistical machine translation2015</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>CharacTer: Translation edit rate on character level. Weiyue Wang, Jan-Thorsten Peter, Hendrik Rosendahl, Hermann Ney, Proceedings of the First Conference on Machine Translation. the First Conference on Machine TranslationBerlin, GermanyAssociation for Computational LinguisticsAugust 20162Shared Task Papers</p>
<p>Harsha Nori, Nicholas King, Scott Mayer Mckinney, Dean Carignan, Eric Horvitz, arXiv:2303.13375Capabilities of gpt-4 on medical challenge problems. 2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
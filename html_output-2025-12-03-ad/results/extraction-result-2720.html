<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2720 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2720</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2720</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-70.html">extraction-schema-70</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <p><strong>Paper ID:</strong> paper-269605117</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2405.02749v1.pdf" target="_blank">Sub-goal Distillation: A Method to Improve Small Language Agents</a></p>
                <p><strong>Paper Abstract:</strong> While Large Language Models (LLMs) have demonstrated significant promise as agents in interactive tasks, their substantial computational requirements and restricted number of calls constrain their practical utility, especially in long-horizon interactive tasks such as decision-making or in scenarios involving continuous ongoing tasks. To address these constraints, we propose a method for transferring the performance of an LLM with billions of parameters to a much smaller language model (770M parameters). Our approach involves constructing a hierarchical agent comprising a planning module, which learns through Knowledge Distillation from an LLM to generate sub-goals, and an execution module, which learns to accomplish these sub-goals using elementary actions. In detail, we leverage an LLM to annotate an oracle path with a sequence of sub-goals towards completing a goal. Subsequently, we utilize this annotated data to fine-tune both the planning and execution modules. Importantly, neither module relies on real-time access to an LLM during inference, significantly reducing the overall cost associated with LLM interactions to a fixed cost. In ScienceWorld, a challenging and multi-task interactive text environment, our method surpasses standard imitation learning based solely on elementary actions by 16.7% (absolute). Our analysis highlights the efficiency of our approach compared to other LLM-based methods. Our code and annotated data for distillation can be found on GitHub.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2720.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2720.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SubGoalDistill</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sub-Goal Distillation hierarchical agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-level hierarchical language-agent: a high-level sub-goal generator and a low-level action generator, both fine-tuned LMs distilled from ChatGPT annotations; uses a short-term history buffer (10 recent action-observation pairs) as input to both modules to guide planning and action selection in ScienceWorld.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Sub-Goal Distillation hierarchical agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Hierarchical agent with a high-level sub-goal generator and a low-level action generator. Both modules are fine-tuned language models (FLAN-T5 variants) trained via knowledge distillation from ChatGPT-annotated expert trajectories; during inference no LLM calls are required.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td>700M</td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>ScienceWorld</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>A large, multi-task interactive text-based environment (30 task types across 10 science domains) where agents perform elementary science experiments via textual observations and actions; includes 10 locations, >200 object types, and long-horizon tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>short-term working memory / recent-history buffer</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>Sequential sliding-window buffer: the most recent 10 action-observation pairs plus recent sub-goals and auxiliary state (current items in room, inventory, visited rooms, score, etc.) concatenated as input</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Last 10 actions and observations, completed sub-goals, current sub-goal, inventory, current items in room, visited rooms, task description, number of steps, and current score.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>10 recent action-observation pairs (history window of size 10)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>Full-context concatenation of the sliding 10-step history and previous sub-goals supplied as input to the LM (recency-based sliding window); models decode greedily conditioned on that context.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Updated after each action/observation by appending the new action-observation pair and maintaining the last 10 entries (sliding window); previous sub-goals are tracked and provided as part of the context.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>Used by the sub-goal generator to produce the next sub-goal (planning) and by the action generator to produce the next low-level action conditioned on the current sub-goal (execution), thereby tracking short-term state and preventing deviation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Overall average score 65.43% on ScienceWorld test variations (the paper's hierarchical agent uses the 10-step history as part of its input in all reported results).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Direct ablation of the 10-step history (removing the short-term memory buffer) is not reported. The closest reported baselines: Swift-only (no sub-goals) achieved 46.25% average; models not conditioning on sub-goals ('no sg') are reported in size ablations but are not equivalent to removing the short history.</td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Short-term history (10 recent action-observation pairs) is an explicit part of inputs and helps conditioning both planning and action generation; more important factors identified are the quality of sub-goals and the capacity of the sub-goal generator—meaning that memory is effective only when the generated sub-goals are accurate. Random sub-goals collapse performance (6.4%), semi-random improves slightly (14.2%), and larger sub-goal generators enable small action generators to perform much better, implying retrieval/conditioning quality (and sub-goal quality) matters more than raw buffer size.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Only short-term (10-step) context is used — no long-term episodic memory is implemented; failures arise when sub-goal generator produces incorrect sub-goals for out-of-distribution variations, causing the agent to get stuck (cycle on same sub-goal) or exhaust steps. The paper notes lack of mechanisms for dynamic goal modification and no explicit experiments varying history length; thus long-horizon dependencies beyond the 10-step window can be problematic.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sub-goal Distillation: A Method to Improve Small Language Agents', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2720.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2720.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion (Shinn et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A language-agent framework that generates textual self-reflections after trials, stores them in an episodic textual memory, and uses these stored reflections to improve performance on subsequent trials.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An agent architecture that records linguistic self-reflections (what went wrong/right) at the end of trials into an episodic memory store and conditions future decisions on these textual memory entries to improve performance.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>interactive text environments (used as related work / baseline in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Reflexion is applied to language-agent decision-making in interactive environments by recording per-trial reflections as textual memory entries and consulting them in later trials to alter behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>episodic memory (textual self-reflections)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>Collection of textual episodic entries (self-reflections) accumulated across trials; paper does not specify indexing (e.g., vector DB vs key-value) within this paper's discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Self-reflections on trial outcomes and linguistic feedback about what went wrong and potential corrective actions.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>Retrieve and include stored reflection text in subsequent trials as additional context (described as using memory text during the next trial); exact retrieval scoring or relevance mechanism is not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Append new self-reflection text at the end of each trial; memory grows with trials (details not specified here).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>To inform and improve decision-making in subsequent trials by leveraging past reflections about failures and corrective strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported in this paper's baseline comparison: Reflexion achieved an overall average score of 23.40% on ScienceWorld (as reported in Table 1 of the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>This paper mentions Reflexion as an approach that 'keeps [self-reflections] in a memory' and leverages them next trial to enhance decision-making; no detailed effectiveness analysis of Reflexion's memory is provided here beyond the baseline score reported.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Not discussed in detail in this paper; only referenced generally as a related method that stores reflections in episodic memory.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sub-goal Distillation: A Method to Improve Small Language Agents', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ReAct: Synergizing reasoning and acting in language models <em>(Rating: 2)</em></li>
                <li>Reflexion <em>(Rating: 2)</em></li>
                <li>Swiftsage: A generative agent with fast and slow thinking for complex interactive tasks <em>(Rating: 2)</em></li>
                <li>ALF-World: Aligning Text and Embodied Environments for Interactive Learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2720",
    "paper_id": "paper-269605117",
    "extraction_schema_id": "extraction-schema-70",
    "extracted_data": [
        {
            "name_short": "SubGoalDistill",
            "name_full": "Sub-Goal Distillation hierarchical agent",
            "brief_description": "A two-level hierarchical language-agent: a high-level sub-goal generator and a low-level action generator, both fine-tuned LMs distilled from ChatGPT annotations; uses a short-term history buffer (10 recent action-observation pairs) as input to both modules to guide planning and action selection in ScienceWorld.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Sub-Goal Distillation hierarchical agent",
            "agent_description": "Hierarchical agent with a high-level sub-goal generator and a low-level action generator. Both modules are fine-tuned language models (FLAN-T5 variants) trained via knowledge distillation from ChatGPT-annotated expert trajectories; during inference no LLM calls are required.",
            "base_model_size": "700M",
            "game_benchmark_name": "ScienceWorld",
            "game_description": "A large, multi-task interactive text-based environment (30 task types across 10 science domains) where agents perform elementary science experiments via textual observations and actions; includes 10 locations, &gt;200 object types, and long-horizon tasks.",
            "uses_memory": true,
            "memory_type": "short-term working memory / recent-history buffer",
            "memory_structure": "Sequential sliding-window buffer: the most recent 10 action-observation pairs plus recent sub-goals and auxiliary state (current items in room, inventory, visited rooms, score, etc.) concatenated as input",
            "memory_content": "Last 10 actions and observations, completed sub-goals, current sub-goal, inventory, current items in room, visited rooms, task description, number of steps, and current score.",
            "memory_capacity": "10 recent action-observation pairs (history window of size 10)",
            "memory_retrieval_strategy": "Full-context concatenation of the sliding 10-step history and previous sub-goals supplied as input to the LM (recency-based sliding window); models decode greedily conditioned on that context.",
            "memory_update_strategy": "Updated after each action/observation by appending the new action-observation pair and maintaining the last 10 entries (sliding window); previous sub-goals are tracked and provided as part of the context.",
            "memory_usage_purpose": "Used by the sub-goal generator to produce the next sub-goal (planning) and by the action generator to produce the next low-level action conditioned on the current sub-goal (execution), thereby tracking short-term state and preventing deviation.",
            "performance_with_memory": "Overall average score 65.43% on ScienceWorld test variations (the paper's hierarchical agent uses the 10-step history as part of its input in all reported results).",
            "performance_without_memory": "Direct ablation of the 10-step history (removing the short-term memory buffer) is not reported. The closest reported baselines: Swift-only (no sub-goals) achieved 46.25% average; models not conditioning on sub-goals ('no sg') are reported in size ablations but are not equivalent to removing the short history.",
            "has_memory_ablation": false,
            "memory_effectiveness_findings": "Short-term history (10 recent action-observation pairs) is an explicit part of inputs and helps conditioning both planning and action generation; more important factors identified are the quality of sub-goals and the capacity of the sub-goal generator—meaning that memory is effective only when the generated sub-goals are accurate. Random sub-goals collapse performance (6.4%), semi-random improves slightly (14.2%), and larger sub-goal generators enable small action generators to perform much better, implying retrieval/conditioning quality (and sub-goal quality) matters more than raw buffer size.",
            "memory_limitations": "Only short-term (10-step) context is used — no long-term episodic memory is implemented; failures arise when sub-goal generator produces incorrect sub-goals for out-of-distribution variations, causing the agent to get stuck (cycle on same sub-goal) or exhaust steps. The paper notes lack of mechanisms for dynamic goal modification and no explicit experiments varying history length; thus long-horizon dependencies beyond the 10-step window can be problematic.",
            "comparison_with_other_memory_types": false,
            "best_memory_configuration": null,
            "uuid": "e2720.0",
            "source_info": {
                "paper_title": "Sub-goal Distillation: A Method to Improve Small Language Agents",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion (Shinn et al., 2023)",
            "brief_description": "A language-agent framework that generates textual self-reflections after trials, stores them in an episodic textual memory, and uses these stored reflections to improve performance on subsequent trials.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "Reflexion",
            "agent_description": "An agent architecture that records linguistic self-reflections (what went wrong/right) at the end of trials into an episodic memory store and conditions future decisions on these textual memory entries to improve performance.",
            "base_model_size": null,
            "game_benchmark_name": "interactive text environments (used as related work / baseline in paper)",
            "game_description": "Reflexion is applied to language-agent decision-making in interactive environments by recording per-trial reflections as textual memory entries and consulting them in later trials to alter behavior.",
            "uses_memory": true,
            "memory_type": "episodic memory (textual self-reflections)",
            "memory_structure": "Collection of textual episodic entries (self-reflections) accumulated across trials; paper does not specify indexing (e.g., vector DB vs key-value) within this paper's discussion.",
            "memory_content": "Self-reflections on trial outcomes and linguistic feedback about what went wrong and potential corrective actions.",
            "memory_capacity": null,
            "memory_retrieval_strategy": "Retrieve and include stored reflection text in subsequent trials as additional context (described as using memory text during the next trial); exact retrieval scoring or relevance mechanism is not specified in this paper.",
            "memory_update_strategy": "Append new self-reflection text at the end of each trial; memory grows with trials (details not specified here).",
            "memory_usage_purpose": "To inform and improve decision-making in subsequent trials by leveraging past reflections about failures and corrective strategies.",
            "performance_with_memory": "Reported in this paper's baseline comparison: Reflexion achieved an overall average score of 23.40% on ScienceWorld (as reported in Table 1 of the paper).",
            "performance_without_memory": null,
            "has_memory_ablation": null,
            "memory_effectiveness_findings": "This paper mentions Reflexion as an approach that 'keeps [self-reflections] in a memory' and leverages them next trial to enhance decision-making; no detailed effectiveness analysis of Reflexion's memory is provided here beyond the baseline score reported.",
            "memory_limitations": "Not discussed in detail in this paper; only referenced generally as a related method that stores reflections in episodic memory.",
            "comparison_with_other_memory_types": false,
            "best_memory_configuration": null,
            "uuid": "e2720.1",
            "source_info": {
                "paper_title": "Sub-goal Distillation: A Method to Improve Small Language Agents",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ReAct: Synergizing reasoning and acting in language models",
            "rating": 2,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models"
        },
        {
            "paper_title": "Reflexion",
            "rating": 2
        },
        {
            "paper_title": "Swiftsage: A generative agent with fast and slow thinking for complex interactive tasks",
            "rating": 2,
            "sanitized_title": "swiftsage_a_generative_agent_with_fast_and_slow_thinking_for_complex_interactive_tasks"
        },
        {
            "paper_title": "ALF-World: Aligning Text and Embodied Environments for Interactive Learning",
            "rating": 1,
            "sanitized_title": "alfworld_aligning_text_and_embodied_environments_for_interactive_learning"
        }
    ],
    "cost": 0.01580775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>SUB-GOAL DISTILLATION: A METHOD TO IMPROVE SMALL LANGUAGE AGENTS
4 May 2024</p>
<p>Maryam Hashemzadeh maryam.hashemzadeh@mila.quebec 
Elias Stengel-Eskin 
Sarath Chandar sarath.chandar@mila.quebec 
Marc-Alexandre Côté </p>
<p>Mila -Quebec AI Institute
Université de Montréal</p>
<p>UNC Chapel Hill</p>
<p>Mila -Quebec AI Institute
Polytechnique Montréal</p>
<p>CIFAR AI Chair</p>
<p>Microsoft Research
Montréal</p>
<p>SUB-GOAL DISTILLATION: A METHOD TO IMPROVE SMALL LANGUAGE AGENTS
4 May 2024DE3C59A85068FA644726BCDD58E09655arXiv:2405.02749v1[cs.LG]
While Large Language Models (LLMs) have demonstrated significant promise as agents in interactive tasks, their substantial computational requirements and restricted number of calls constrain their practical utility, especially in long-horizon interactive tasks such as decision-making or in scenarios involving continuous ongoing tasks.To address these constraints, we propose a method for transferring the performance of an LLM with billions of parameters to a much smaller language model (770M parameters).Our approach involves constructing a hierarchical agent comprising a planning module, which learns through Knowledge Distillation from an LLM to generate sub-goals, and an execution module, which learns to accomplish these sub-goals using elementary actions.In detail, we leverage an LLM to annotate an oracle path with a sequence of sub-goals towards completing a goal.Subsequently, we utilize this annotated data to fine-tune both the planning and execution modules.Importantly, neither module relies on real-time access to an LLM during inference, significantly reducing the overall cost associated with LLM interactions to a fixed cost.In ScienceWorld, a challenging and multi-task interactive text environment, our method surpasses standard imitation learning based solely on elementary actions by 16.7% (absolute).Our analysis highlights the efficiency of our approach compared to other LLM-based methods.Our code and annotated data for distillation can be found on GitHub 1 .</p>
<p>INTRODUCTION</p>
<p>Recently, Large Language Models (LLMs) have found applications in various fields, including multi-task learning, decision making, answering questions, summarizing documents, translating languages, completing sentences, and serving as search assistants.They showcase a remarkable ability to make predictions based on input, enabling their use in generative AI applications to produce content based on input prompts (Devlin et al., 2018;Brown et al., 2020;Rae et al., 2021;Chowdhery et al., 2023;Scao et al., 2022;Patel &amp; Pavlick, 2021;Han et al., 2021;Bommasani et al., 2021).</p>
<p>The promising advantage of LLMs is attributed to their training on extensive text datasets, resulting in impressive capabilities.This prior knowledge can be leveraged for action planning to solve tasks in robotics and reinforcement learning (Huang et al., 2022b;Brohan et al., 2023;Liang et al., 2023).Recent works have utilized in-context learning with LLMs to provide actions in autonomous decision-making agents and interactive environments (Mahowald et al., 2023;Yao et al., 2022;Schick et al., 2023;Shen et al., 2023;Nakano et al., 2021;Park et al., 2023;Lin et al., 2023;Brohan et al., 2023).</p>
<p>However, the extreme size of LLMs makes them computationally unaffordable for many applications.Moreover, closed-source models like ChatGPT (OpenAI, 2022) and GPT-4 (OpenAI, 2023) limit accessibility and reproducibility.Consequently, there is an increasing demand to find approaches that are less computationally intensive while still capitalizing on the knowledge embedded in LLMs.One prevalent technique is the use of Knowledge Distillation (KD) (Buciluǎ et al., 2006;Hinton et al., 2015), wherein a smaller model is trained with guidance from a larger model.</p>
<p>Through this approach, we can leverage the knowledge in an LLM to train a more compact model with a reduced number of parameters.</p>
<p>Navigate_to(kitchen)</p>
<p>open door to kitchen go to kitchen Pick_up (thermometer) pick up thermometer Find (metal pot) open cupboard pick up metal pot Fill (metal pot, water) move metal pot to sink activate sink deactivate sink pick up metal pot Focus_on(substance in metal pot focus on substance in metal pot Freeze (water, metal pot) pour metal pot into metal pot pick up metal pot open freezer move metal pot to freezer Monitor_temperature(metal pot, freeze) examine substance in metal pot</p>
<p>Annotated Trajectory</p>
<p>Task Description: Your task is to change the state of matter of water.First, focus on the substance.Then, take actions that will cause it to change its state of matter.</p>
<p>Figure 1: Example of annotating an expert trajectory with sub-goals for a particular variation of task 1-4 (change-the-state-of-matter-of ).Looking only at the original trajectory (i.e., ignoring the red rows), we gather the expert ended up changing the state of water to be frozen.The expert had to navigate to the kitchen, find a thermometer and a metal pot, pour water into the pot, place it in the freezer, and continually monitor its temperature until frozen.Each of those milestones (highlighted in red) can be considered a sub-goal, encompassing a sequence of actions.Sub-goals can be shared across different tasks, facilitating generalization.We opted for a format that looks like function calls to encourage reusability (e.g., fill(metal pot, water)).</p>
<p>Distilling knowledge from LLMs offers significant advantages, allowing for the training of specialized local models rather than depending on an LLM as a general model.This approach not only enhances privacy, particularly for systems with security-sensitive considerations like co-pilot models, but also provides greater flexibility in tailoring models for specific tasks.Additionally, the use of a smaller model offers the advantage of versatility across diverse applications without size constraints, including device models and mobile apps.Another challenge with LLMs is their susceptibility to hallucinations.This tendency poses a hindrance to their effective execution of long-tail planning, especially in interactive decision-making scenarios.</p>
<p>In our research, we leverage the knowledge of LLMs to train an autonomous agent for effective decision-making in complex interactive text environments, utilizing small language models as our policy.Knowledge Distillation facilitates the training of smaller policies, allowing seamless integration of LLM knowledge.To address the challenges at hand, adopting a two-level planning approach proves beneficial for reducing hallucination -one for high-level reasoning to formulate subgoals and another for low-level action planning to execute each sub-goal.</p>
<p>Figure 1 illustrates this concept in the task of freezing water from ScienceWorld (Wang et al., 2022a).The agent's subtasks involve navigating to the kitchen, finding a thermometer and a metal pot, pouring water into the pot, placing it in the freezer, and continuously monitoring its temperature until frozen.These constitute sub-goals generated by a high-level model, with each sub-goal subsequently executed by a lowlevel model.The generation of sub-goals empowers an autonomous agent to expedite learning for the current task and reuse similar sub-goals in various tasks to have more generalization.</p>
<p>The contributions in this work are:</p>
<p>• We employ Knowledge Distillation from an LLM to train a high-level policy capable of generating sub-goals without making assumptions about the specific set of sub-goals.Notably, these sub-goals remain flexible, accommodating various sequences of actions.</p>
<p>• We demonstrate that employing Knowledge Distillation with hierarchical policies surpasses the performance achieved by both standalone imitation learning and its combination with in-context learning.</p>
<p>• We illustrate that this approach is more cost-effective in terms of the number of calls to an LLM compared to other methods utilizing in-context learning.</p>
<p>• We introduce an effective approach instead of using computational requirements of LLM and their restricted number of calls for using in interactive decision making tasks.</p>
<p>RELATED WORK</p>
<p>Using LLMs for Action Planning Recent works have demonstrated the ability of LLMs to perform action planning for interactive decision making process without any additional training (Huang et al., 2022a).ReAct (Yao et al., 2022) proposes a way of prompting an LLM with interleave reasoning step and action taking step.That led the resolution of a variety of language-based reasoning and decision-making tasks.This approach empowers the model to construct high-level plans for effective action.Reflexion (Shinn et al., 2023) draws inspiration from reinforcement learning, employing a framework to reinforce language agents through linguistic feedback.At the end of each trial, it uses selfreflection to determine what went wrong with the task and keeps it in a memory.Then it leverages this information for the next trial.</p>
<p>Some works use a programmatic LLM prompt structure with available actions and objects in an environment to translate natural language commands into robot policy code via few-shot examples (Liang et al., 2023;Singh et al., 2023).Khot et al. (2022) introduced a decomposed prompting approach wherein a task is broken down into simpler sub-tasks, allowing for recursive handling.Subsequently, these sub-tasks are assigned to sub-task-specific LLMs, with both the decomposer and the sub-task LLMs with their own few-shot prompts.Sun et al. (2023) uses three steps, action mining, plan formulation, and plan execution to decompose a question into a sequence of actions by few-shot prompting of LLMs.In Prasad et al. (2023) tasks are decomposed explicitly by a separate LLM through prompting when an executor is unable to execute a given sub-task.</p>
<p>Imitation learning Some works employ imitation learning to train a language model as the agent's policy, as seen in offline decision transformers (Torabi et al., 2018).The inputs consist of states, actions, and reward-to-go values, which are fed into a transformer.This transformer then predicts actions in an autoregressive manner, utilizing a causal self-attention mask (Chen et al., 2021).Contextual Action Language Model (CALM) (Yao et al., 2020) is another work which uses a fine-tuned language model with oracle data to generate a set of candidate actions which are then passed to a policy network to select the best one.In Nakano et al. (2021), the authors fine-tune GPT-3 to address long-form questions within a web-browsing context.Human feedback is employed as a direct optimization measure for enhancing the quality of answers generated by the model.</p>
<p>Knowledge Distillation: Knowledge Distillation (KD) typically falls into two categories: black-box KD and whitebox KD.In black-box KD, only the teacher's predictions are available for guidance, while in white-box KD, we have access to the teacher's parameters (Gou et al., 2021).Recently, black-box KD has gained widespread use for finetuning original models using self-instruct techniques, as proposed by Wang et al. (2022b), or for smaller models (Taori et al., 2023;Chiang et al., 2023;Peng et al., 2023) through the utilization of prompt-response pairs generated by LLMs.West et al. (2021) introduces symbolic KD from text rather than logits.This process involves the transfer of knowledge from a large, general model to a more compact commonsense model, facilitated by a commonsense corpus, yielding a commonsense knowledge graph and model.The work by Hsieh et al. (2023) trains a smaller model that outperform LLM using reasoning steps called rationales.They incorporated rationales as informative supervision to train smaller models with less training data.</p>
<p>Complex interactive text environments In text-based games, an agent interacts with the environment by reading and writing text while aiming towards the end game or solving a given task.Out of the recent frameworks that deals with generating and interfacing text-based games (Côté et al., 2018;Hausknecht et al., 2019;Shridhar et al., 2021;Murugesan et al., 2021), we use ScienceWorld (Wang et al., 2022a) which is very complicated by having a large set of objects, actions, and tasks.</p>
<p>MODEL</p>
<p>In this paper, we propose to train a hierarchical policy by combining KD from an LLM and imitation learning from expert trajectories.This section describes both modules in detail and we refer the reader to Figure 2 for a schematic view.We first formulate the problem as a POMDP (Section 3.1).Next, we describe what knowledge we are distilling from an LLM to guide the agent in accomplishing tasks (Section 3.2).Then, we detail how both the high-level and low-level policies of the hierarchical policy are trained (Section 3.3).</p>
<p>PROBLEM FORMULATION</p>
<p>ScienceWorld (Wang et al., 2022a) can be defined as a partially observable Markov decision process (POMDP), where observations provide information solely on environmental changes induced by the current action.ScienceWorld is  an interactive text environment meaning all task instructions, observations and actions are expressed in textual form.Importantly, both observations and rewards in this environment are conditioned by the ongoing task.</p>
<p>Given a language vocabulary V and an arbitrary maximum number of tokens N , an observation is defined such as o ∈ Ω ⊂ V N , a reward such as r ∈ R and an action as a ∈ A ⊂ V N .Finally, a task or goal description is shown by
g ∈ G ⊂ V N .
We formalize the problem as a goal-augmented POMDP M = (S, V, A, Ω, G, T, R, O, γ) with S the state space, A ⊂ V N the action space, Ω ⊂ V N the observation space, G ⊂ V N the goal space, T : S × A × G → S the goal-conditioned transition function, R : S × A × G → R the goal-conditioned reward function, O : S → V N an (unknown) observation function mapping a state to a textual description and γ the discount factor.We assume γ = 1 in our experiments.</p>
<p>DISTILLING KNOWLEDGE FROM AN LLM</p>
<p>The initial step in training our policies is creating a dataset.This dataset should include sub-goals along with their corresponding aligned sequences of actions for each task.</p>
<p>To generate sub-goals along with their corresponding aligned sequences of actions we do the following steps.We assume access to a collection of expert trajectories.Then we prompt an LLM with two in-context examples.Each example is composed of a task description, a similar task as the one we wish to annotate, and its expert trajectory.The example also contains a set of sub-goals, with the sequences of actions linked to each sub-goal.</p>
<p>Given the two in-context examples and a new task description with its expert trajectory, the LLM is then instructed to generate a response.The response is a set of sub-goals with their associated list of actions.The generated list of actions is used to determine each sub-goal corresponds to which segment of the expert trajectory.It is important to note that these responses are collected only for the training tasks for which we assume having access to expert trajectories.Also, it is important to point out that the LLM is not generating any novel trajectories.</p>
<p>Figure 4 illustrates the prompt examples for task 1 − 1 which is boiling a given substance.To ensure more uniform sub-goals that can generalize across tasks, we opted for a format that looks like function calls.Since that format was shown in the in-context examples, the LLM-generated sub-goals mimic this format as well making them easier to parse.</p>
<p>Since the expert trajectories for some tasks can be long (+100 actions), the generated sub-sequence of actions corresponding to each sub-goal may not align exactly with the expert trajectory.Sometimes, it might miss certain actions, while in other instances, it might include additional actions, especially when there are repeated actions in the trajectory.To address this, we use a trajectory alignment process that finds the minimal set of edits to go from the generated trajectory to the expert trajectory according to the Levenshtein distance.For each "remove" edit, i.e. the generated trajectory has superfluous actions, we simply remove those from the generated trajectory.On the other hand, for "add" edit, i.e. the generated trajectory is missing some actions, we prompt the LLM to generate a new sub-goal for those.An example is shown in Figure 3.</p>
<p>Generated Trajectory by LLM Expert Trajectory</p>
<p>Figure 3: Example of a trajectory generated by the LLM deviating from the provided expert trajectory.In this example, which is for a boiling task, certain actions are omitted in the generated trajectory, indicated in blue in the left box.To address these missing actions, we group them into sequences and prompt the LLM to generate sub-goals for them.If the generated trajectory includes additional actions, such as the green actions in the right box, we simply remove them to align with the expert trajectory.</p>
<p>In the resulting annotated dataset, each data point follows the same format as used by Lin et al. ( 2023) but with the added mention of completed sub-goals and the current sub-goal.Precisely, it corresponds to:</p>
<p>• Input: task description, number of steps, current score, completed sub-goal, current sub-goal, a history of 10 recent actions-observations, current items in the room, inventory, and the visited rooms.</p>
<p>• Target: next action, next sub-goal.</p>
<p>HIERARCHICAL IMITATION LEARNING</p>
<p>With the dataset obtained from distilling knowledge from an LLM, we can now focus on training the policies.</p>
<p>Low-level policy: The low-level policy is a language model (LM) which is trained through imitation learning using the annotated dataset.The goal is to have a model much smaller than an LLM so it can fit on a single machine and run faster, ideally below a billion of parameters.This policy learns to predict the next action given the current task description, the 10 previous observation-action pairs, the previous completed sub-goals, and the current sub-goal.We refer to this policy as the action generator.</p>
<p>High-level policy: The high-level policy is another LM with a reasonable size.It is trained using the annotated dataset to generate the next sub-goal given the previous sub-goals and a short history, i.e. the last 10 actions and observations.So the high-level policy generates sub-goals while the low-level policy generate actions.Moreover, this policy conditions on the same input information as for the action generator.We call this policy the sub-goal generator.</p>
<p>Hierarchical policy: During inference, we first leverage the high-level policy to generate a sub-goal.This generated sub-goal is then fed into the action generator, allowing it to produce the next action aligned with the provided sub-goal.This sequential approach serves as a guiding cue for the action generator, particularly when the trajectory to achieve the goal is complex or long.Moreover, it serves to prevent the action generator from generating actions that might deviate the agent from the correct path, thereby improving the precision and relevance of the actions being generated.</p>
<p>EXPERIMENTS</p>
<p>ENVIRONMENT</p>
<p>We chose ScienceWorld (Wang et al., 2022a) as the environment due to its complexity and the diverse range of tasks it encompasses.This environment is an interactive multi-task text-based game where the agent conducts elementary science experiments in a simulated environment.Each experiment is designed as a separate task.For example, "Your task is to boil water.For compounds without a boiling point, combusting the substance is also acceptable.First, focus on the substance.Then, take actions that will cause it to change its state of matter".To complete a task, the agent must perform multiple actions and receives the result of each action as an observation and a score.The observations and actions are in text format.An observation describes the changes in the environment, and the score is a numerical value ranging from 0% to 100%, indicating the degree of completion of the current task through the current action.</p>
<p>Furthermore, ScienceWorld is a benchmark with 30 distinct tasks spanning 10 science domains which are widely different (Appendix A.4).For instance, in the "Changes of State" task, the agent is required to locate and use heating/freezing sources to alter the state of a substance (e.g., ice or chocolate).Conversely, in a task such as "Mendelian Genetics," the agent is tasked with determining whether a specified trait (e.g., white flower color) is dominant or recessive in a plant.These examples illustrate the substantial diversity across the domains, ranging from physical transformations to genetic analyses, underscoring the broad spectrum of challenges within ScienceWorld.</p>
<p>On top of that, ScienceWorld has 10 different locations, more than 200 object types, and 25 action templates which makes the search space very larger for the agent.Each type of task has different variations in which the task objects, the agent's initial location, and random contents of each room are altered.</p>
<p>EXPERIMENTAL SETUP</p>
<p>The environment has separate sets of variations for train and test.In the test variations, the combinations of objects and conditions are not seen in the train set.Following the experimental setup in (Lin et al., 2023), if the number of variations is more than 10, we consider only the first 10 variations.</p>
<p>Our base models for both policies is a pre-trained FLAN-T5-LARGE (Chung et al., 2022) with 700M parameters.For the both polices, we used greedy decoding at inference.We also conduct an ablation study over different model sizes (Figure 5a).For fine-tuning the policies, we use all the training tasks and their variations (3600 games in total) from ScienceWorld.We vary the number of training epochs in function of the size of the models (see Appendix A.3).</p>
<p>Methods</p>
<p>BASELINE AGENTS</p>
<p>We compare our approach with other works that leverage LLMs.Some rely only on prompting such as SayCan, ReAct, and Reflexion, but SwiftSage also do imitation learning.Here is a brief description of each method.</p>
<p>SayCan: the LLM initially offers a set of actions along with their respective ranks.Then, a value-based method is employed to re-rank these actions in order to determine the most rewarding action for execution (Brohan et al., 2023).</p>
<p>ReAct: the LLM generates actions by incorporating the provided prompt and the history of generated texts.It employs reasoning traces as intermediate thought steps during the action generation to refine a plan for the upcoming steps (Yao et al., 2022).</p>
<p>Reflexion: the language agent reflects the task feedback at each trial in the form of text and retains this information within an episodic memory.During the subsequent trial, it leverages the stored memory text to enhance its decisionmaking process (Shinn et al., 2023).</p>
<p>SwiftSage: this method comprises two components: Swift, a fine-tuned LM to predict actions, and Sage, a module that queries an LLM for planning when the performance of Swift is inadequate (as determined by some handcrafted rules) (Lin et al., 2023).</p>
<p>Swift-only: this is the Swift part of the SwiftSage method which only has the fine-tuned LM to predict the actions.We consider this method as a strong baseline and the most comparable to our approach as it relies on imitation learning without the need for querying an LLM during inference.</p>
<p>Note that all baselines use ChatGPT (GPT-3.5)as their LLM.</p>
<p>RESULTS AND ANALYSIS</p>
<p>Main Results: Table 1 compares the performance of the baselines with our approach in the ScienceWorld.The score for each task type is the average score (in percent) obtained for 10 test variations.Our approach demonstrates an overall performance of 65.43%, surpassing Swift-only by 16.71% (33.9% relative increase), and showing a slight improvement over SwiftSage of 3.3% (5.3% relative).Interestingly, our method is able to solve all test variations (i.e., gets an average score of 100%) for 11 out of the 30 task types.In contrast, SwiftSage solves them only for 2 task types, and Swift-only, only for 4 task types.</p>
<p>Additionally, we measured the performance of the agents with respect to the length of the tasks (a proxy for task complexity).The length of a task is determined by how many actions was needed by the expert to solve it.2Following Lin et al. ( 2023), we group the tasks into three categories: Short when the length is less than 20 actions, Medium when it falls between 20 and 50 (inclusively), and Long if above 50.As shown in Table 1, our approach outperforms other methods on short and medium tasks.On long tasks, we outperform all methods except SwiftSage, which has a substantial advantage here: The longer the task, the higher the chance it triggers one of the rules for Sage to take over.</p>
<p>As part of the comparison, there are other approaches that do not use a LLM including DRRN (He et al., 2016), KG-A2C (Ammanabrolu &amp; Hausknecht, 2019), CALM (Yao et al., 2020), BC (Torabi et al., 2018), TDT (Chen et al., 2021).The results from (Wang et al., 2022a) show these approaches perform poorly, below 17%, in ScienceWorld.</p>
<p>For this reason, we did not include them here and only focus on approaches comparable with us.</p>
<p>A key motivation for our approach is cost-effectiveness in terms of LLM queries.During training, we make one query to ChatGPT per task to identify the sub-goals within an expert trajectory.Sometimes mismatches occur between the expert trajectory and the actions assigned to each sub-goal by ChatGPT.When that is the case, we employ dynamic programming, with a maximum of 10 attempts per task.This contrasts with other baseline methods, where LLM is queried for each action, incurring considerably higher costs.</p>
<p>Why is it failing on some task types?The performance of our algorithm in some tasks are low, (see Table 5).</p>
<p>In Table 1, the scores of two tasks are presented.One contributing factor is the variations in the test are very different from those in the training.For instance, the objects might be very different or the path to complete the task is very different and longer.The main culprit is the sub-goal generator which is not able to generate good sub-goals.</p>
<p>As a concrete example (Table 2), in the test variations for task 3-3, the agent needs to go to kitchen and then fill a jug with water.When looking at the transcript, we see the agent is able to go to kitchen but then when it arrives, the sub-goal generator issues a sub-goal which is not relevant, FocusOn(fountain).The agent attempts to focus on the fountain which is a wrong action and the game terminates with a score of 0.</p>
<p>Another example is task 1-1 (Table 2) in which the agent should boil a substance.It should first find the substance but since the substance is in a totally different location than those seen during training, the sub-goal generator is not able to generate a good sub-goal for this step.Consequently the agent will do other actions and exhaust all the allocated time steps.2: Two instances where the performance of our algorithm is low.The first column displays the trajectory generated with sub-goals, while the second column presents the expert trajectory.Sub-goals are highlighted in dark red, accompanied by their corresponding actions, and incorrect actions are marked in red.</p>
<p>The impact of scale: We conduct a comparison across various sizes of language models such as FLAN-T5-XL, FLAN-T5-BASE, and FLAN-T5-SMALL.Additionally, we evaluate T5-3B and T5-LARGE to determine the effectiveness of FLAN-T5 versus T5.The results are illustrated in Figure 5a.In our initial findings, we observed that FLAN-T5 outperforms T5 significantly.Moreover, our results reveal a positive correlation between the LM size and its performance -larger models generally yield better results.Intriguingly, we observe that for smaller models (FLAN-T5-SMALL and FLAN-T5-BASE), not conditioning on sub-goals works slightly better than including them.This might be indicative that the sub-goal generator is not expressive enough to generate meaningful and effective sub-goals which in turn impacts the action generator policy and leads to lower scores.</p>
<p>The impact of sub-goals: To study the impact of the sub-goal generator's size on the overall performance, we try pairing different sizes of sub-goal generator while limiting the action generator to be small.In Figure 5b, the average scores exhibit an upward trajectory.This can be attributed to the larger sub-goal generators producing more accurate and relevant sub-goals, subsequently empowering the action generator to generate more correct actions.See Table 6 for a complete breakdown of the score per task type and per model size.The larger models work better and FLAN-T5 performs also better than T5.Dashed lines represent models that are not conditioning on any sub-goals ("no sg") and equivalent to Swift-only.b) Average scores across different sizes of sub-goal generator while the action generator is kept to be base (blue) or small (green).Having larger sub-goal generators can significantly boost performance of small action generators.</p>
<p>Random</p>
<p>Semi-random first 10 steps each first 10 steps each 39.1% 37.6% 6.4% 53.1% 43.3% 14.2%</p>
<p>Table 3: Average performance for randomly generated sub-goals.Sub-goals are selected randomly (or semi-randomly) at either the first step, every 10 steps, or each step.</p>
<p>To further demonstrate the importance of the sub-goal, we generated random sub-goals and then fed them to the action generator.That yield an average score of 6.4%, indicating that the action generator do condition on the sub-goals, subsequently, it cannot solve the tasks effectively.We conducted an additional experiment by altering the arguments of the sub-goals, as they have a functional format.If the argument corresponds to a location, we replaced it with another from the environment, and if it is an object, we replaced it with a randomly chosen object available at that step of the game.We named this approach semi-random sub-goals.The result for this experiment is 14.2%, showing an increase in performance compared to the random sub-goals.Table 3 shows the average scores and Table 9 shows the score for each task.</p>
<p>Recovery from noisy sub-goals: We also assess the performance when both the action and sub-goal generators have been exposed to noisy sub-goals.More specifically, we consider two settings: applying noise 1) only at the first step, or 2) every 10 steps.In the first setting, the first sub-goal is (semi-)randomly selected, while the subsequent sub-goals are generated using the FLAN-T5-LARGE sub-goal generator.In the second experiment, a sub-goal is (semi-)randomly selected every 10 steps instead of using the sub-goal generator for all steps.Table 3 shows the overall scores for both settings and a breakdown per task types is presented in Table 10.</p>
<p>In both scenarios, semi-random selection (53.1% and 43.3%) yields better results, as it closely resembles the subgoals generated by the sub-goal generator.Some tasks achieve a score of 100, indicating successful recovery from noisy sub-goals.While overall scores are lower compared to using the FLAN-T5-LARGE sub-goal generator, it is still higher than using Swift only in the first setting and closely approaching it in the second setting (Appendix A.10).</p>
<p>Generalization on heldout task types: We select one or two task types from each science domain (see highlighted ones in Table 4) to train the action and sub-goal models.Then, we assessed their performance on the rest of the task types.We compared our algorithm against the Swift-only baseline.The average total scores are 40.63%with subgoals vs. 36.56%for Swift-only.For unseen tasks, the scores are 27.72% with sub-goals vs. 15.25% for Swift-only.This suggests that using sub-goals helps improve generalization across unseen tasks.The scores for each task are presented in Table 11.</p>
<p>DISCUSSION AND LIMITATION</p>
<p>In contrast to SwiftSage, which relies on interactive usage of the ChatGPT API to handle planning, our approach makes use of a trained sub-goal generator to guide the action generator.Moreover, our framework empowers the agent to retrieve a nearly optimal trajectory by supplying the appropriate sub-goal.Nevertheless our framework has significantly reduced the frequency of API calls, which are both expensive and not universally accessible.ReAct, Reflexion, and SwiftSage require human annotations to correct sub-goals and predict a reasonable action.However in our approach, we do not need human help to predict sub-goals or provide precise prompts.</p>
<p>Generalization: In this work, our focus is on optimizing performance within the environment, and there might be a potential limitations when transitioning to entirely different scenarios.If we test it in a distinct environment, the performance may not be optimal, given the fine-tuning with data specific to the ScienceWorld environment.It's acknowledged that for generalization across diverse scenarios, an LLM may perform better, given its capacity to handle a broader range of inputs and contexts.</p>
<p>Goal Modification: When the agent encounters challenges in solving the current sub-goal, it will often find itself cycling through the same sub-goal for several steps.Consequently, the action generator repeats a sequence of actions mirroring recent ones.Sometimes the sub-goal generator will adjust the sub-goal slightly based on the input and that can be enough to get unstuck.</p>
<p>Ideally, we would like to avoid being stuck for several steps and learn to modify the sub-goal in the right way.One strategy involves online learning, where the controller is updated based on the reward from the environment.However, this approach carries the risk of catastrophic forgetting, necessitating additional measures such as loss modification and regularization to mitigate this risk.Another approach could involve incorporating an LLM alongside the controller.If the controller fails to produce effective actions, the LLM can suggest alternative sub-goals.This might have the risk of poor sub-goals and hallucinations which rewards might help but it is still challenging in such a sparse environment.</p>
<p>CONCLUSION</p>
<p>We introduce a straightforward yet highly effective approach for tackling complex text-based environments.Our framework leverages the knowledge of an LLM to extract sub-goals.A hierarchical policy of two LMs proposed: a high-level policy predicts a sub-goal, and a low-level policy, by using the predicted sub-goal, generates elementary actions.Through extensive experiments across 30 task types in ScienceWorld, our approach demonstrates increase performance compared to state-of-the-art baselines, including standard imitation learning and SwiftSage.</p>
<p>As future directions for this work, we aim to delve into further exploration of goal modification strategies when the agent encounters challenges in solving the current sub-goal.This could involve breaking down or transforming a sub-goal into a more achievable form.Another venue for future research involves extending this approach to a multimodule environment.In such scenarios, the sub-goal generator could leverage each module as an independent source to generate diverse and context-specific sub-goals.Exploring strategies for goal modification and online learning is another avenue we are keen to pursue.</p>
<p>A APPENDIX A.1 FEW-SHOT PROMPT FOR THE LARGE LANGUAGE MODEL</p>
<p>We employed the ChatGPT API as the large language model in our study.The structure of the ChatGPT prompt is comprised of three main components.Firstly, there is a general description of the environment.The second part includes two examples, each containing the task description, an expert trajectory, and a set of sub-goals with their corresponding action sequences.Lastly, the prompt presents a new task description, along with an expert trajectory, then we ask the LLM to generate sub-goals for this new task.</p>
<p>Here is the first part:</p>
<p>Description of the Environment</p>
<p>You are a helpful assistant.You are in a simulated environment as an agent.A task and its description will be given to you.Suggest the best actions the agent can take based on the things you see and the items in your inventory to complete the task.meaning use a device/item , look around meaning describe the current room, look at OBJ meaning describe an object in detail, look in OBJ meaning describe a container's contents, read OBJ meaning read a note or book, move OBJ to OBJ meaning move an object to a container, pick up OBJ meaning move an object to the inventory, put down OBJ meaning drop an inventory item, pour OBJ into OBJ meaning pour a liquid into a container , dunk OBJ into OBJ meaning dunk a container into a liquid , mix OBJ meaning chemically mix a container , go to LOC meaning move to a new location , teleport to LOC meaning teleport to a specific room , eat OBJ meaning eat a food , flush OBJ meaning flush a toilet, focus on OBJ meaning signal intent on a task object, wait [DURATION] meaning take no action for some duration, task meaning describe current task, inventory meaning list agent's inventory, OBJ means objects.LOC means location.There are 10 locations centered around a house theme.These are: kitchen, bathroom, workshop, art studio, greenhouse, outside, bedroom, living room, foundry.</p>
<p>Here are the two examples for task 4 − 1, which is find-living-thing.This is the second part of the prompt:</p>
<p>Two-shot examples of Task 4-1 (find-living-thing)</p>
<p>Example 1 [Task Description] Your task is to find a(n) living thing.First, focus on the thing.Then, move it to the red box in the kitchen.</p>
<p>[</p>
<p>A.3 IMPLEMENTATION DETAILS</p>
<p>We set the maximum number of steps per episode to 100.Additionally, we implemented an alternative termination condition: if the scores remain unchanged over the last 50 steps, we stop the episode.This prevents the agent from getting stuck in a repetitive loop of actions that do not yield any changes, such as navigating between rooms.We selected this threshold to ensure that it is not too restrictive, considering that certain tasks with lengthy trajectories may require a long sequence of actions to achieve a reward.</p>
<p>The learning rate in all of the experiments is 1e − 4. The values for max source length = 1024 and for max target length = 30.The batch size for training is 8.We set the number of epochs to 20 during training.</p>
<p>In the evaluation phase, checkpoints were selected based on their loss values, encompassing the checkpoint with the lowest loss and the subsequent three checkpoints.The final choice for test was determined among these checkpoints, with priority given to the one demonstrating the highest score in the test set.</p>
<p>For both the action generator and sub-goal generator, we used greedy decoding.When the generated actions is invalid we attempt to find the closes match from the list of admissible commands provided by ScienceWorld.</p>
<p>A.4 DATASET STATISTICS</p>
<p>In Table 4, we provide tasks' names and their variations for all of the tasks in ScienceWorld (Wang et al., 2022a).For each task, variations are partitioned into 50% training, 25% development, and 25% test sets.In the development and test sets, variations include substances, animals, or plants that are not seen in the training.</p>
<p>A.6 TASK SCORES ACROSS VARIOUS MODEL SIZES</p>
<p>In Table 6, the average scores for models of different sizes are presented.In this experiment, the base model was employed for the action generator, while the size of the sub-goal generator was varied across small, base, large, and x-large.The first four columns of the table display their respective scores.Additionally, using the small model for the action generator, the experiment was repeated with various sizes of the sub-goal generator, and the results are shown in the second set of four columns.The last column illustrates the performance when both the action generator and the sub-goal generator are x-large, achieving the highest score.The results for the T5 model, including T5-LARGE and T5-3B, are presented in Table 8.The outcomes are shown for both scenarios-with and without the integration of sub-goals.In the experiments where sub-goals were employed, equivalent sizes were utilized for both the action generator and sub-goal generator.</p>
<p>A.9 RESULTS FOR RANDOM AND SEMI-RANDOM SUB-GOALS</p>
<p>We generated random sub-goals from the sub-goal space and used that instead of the sub-goal generator.The action generator remains the fine-tuned FLAN-T5-LARGE, as before.The results are displayed in Table 9.</p>
<p>To further investigate the impact of the sub-goals, we conducted another experiment where we retained the sub-goals and solely modified their arguments, typically are objects or locations.Despite this adjustment, performance remained low, although slightly improved compared to random sub-goals.A.10 SUB-GOAL RETRIEVING Here are the scores representing the ability to retrieve sub-goals for the sub-goal generators.There are two sets of the experiments: 1-Random/semi-random first sub-goals, 2-Random/semi-random sub-goals during interactions.The scores for each task and experiment are presented in Table 10.</p>
<p>A.11 GENERALIZATION EXPERIMENT</p>
<p>In this experiment, we trained the model on a subset of tasks and evaluated it on test variations from all tasks.For each scientific topic, we selected one or two tasks for training and reserved the remaining tasks for evaluation.We compared our algorithm against the Swift-only baseline.The scores for each task are displayed in Table 11.</p>
<p>Figure 2 :
2
Figure2: On the left, a schematic view of our approach is shown.There are two modules: the sub-goal generator and action generator.The sub-goal generator provides a sub-goal for the action generator, which predicts the next action given the current sub-goal and history.On the right, the inputs and outputs of both modules are illustrated.The input comprises different parts including task description, completed sub-goal, current sub-goal, a history of recent actions-observations, and more, each highlighted in a distinct color.</p>
<p>Figure 5: a) Average scores across different model sizes for FLAN-T5 and T5.For T5 model, X-Large refers to T5-3B.The larger models work better and FLAN-T5 performs also better than T5.Dashed lines represent models that are not conditioning on any sub-goals ("no sg") and equivalent to Swift-only.b) Average scores across different sizes of sub-goal generator while the action generator is kept to be base (blue) or small (green).Having larger sub-goal generators can significantly boost performance of small action generators.</p>
<p>Action generator Sub-goal generator History observation/score action sub-goal Action generator Sub-goal generator Your</p>
<p>task is to boil water … ; Time: 1; Score: 0; Completed subtasks are: navigate_to(kitchen), … ; The current subtask is heat(water, metal pot); Action history: activate sink --&gt; The sink is now activated, … ; Current environment: This room is called the kitchen.In it, you see: … ; Current inventory: a thermometer, … ; Visited rooms: hallway, … ; What action should you do next?
Sub-goalNext action</p>
<p>[Example 1] [Task description] Your task is to boil water.…[Experttrajectory]Here is the goal path to achieve to the goal:open door to kitchen, go to kitchen, … provide me with the functional format of high-level sub-tasks to complete this task and their correspondings actions.Figure4: The figure demonstrates KD to generate sub-goals using an LLM.The LLM is presented with a prompt containing two in-context examples.Each example is composed of a task description in green and an expert trajectory detailing the steps to accomplish that task in blue.It also includes the expected set of sub-goals with their corresponding sequences of actions in red.Following this, we provide a new task description and trajectory, and we let the LLM generate the associated sub-goals and segmented actions.
PromptGenerated sub-goals1-navigate_to(hallway) : {'open door to hallway', 'go to hallway'}2-navigate_to(kitchen) : {'open door to kitchen', 'go to kitchen'}3-pick_up(thermometer):{'pick up thermometer'}[sub-goals] 1-navigate_to(kitchen) : {'open door to kitchen', 'go to4-find(metal pot):{'open cupboard', 'pick up metal pot'}kitchen'} 2-pick_up(thermometer):{'pick up thermometer'} 3-find(metal pot):{'open cupboard', 'pick up metal pot'} …LLM5-find(chocolate):{'open oven', 'open freezer', 'open drawer in cupboard', 'open glass jar', 'open drawer in counter', 'open fridge', 'focus on[Example 2] …chocolate'} 6-…[Current task] [Task description] Your task is to boil chocolate. …[Expert trajectory] Here is the goal path to achieve to the goal: 'opendoor to hallway', 'go to hallway', 'open door to kitchen', 'go tokitchen', … provide me with the functional format of high-levelsub-tasks to complete this task and their correspondings actions.NoDoes the actions sequence matchwith the oracleIf some actions are missed, use them in the prompt.path?If more actions are added mistakenly, remove them.YesFinish!</p>
<p>Table 1 :
1
The table illustrates the overall average score (%) across all test tasks on the ScienceWorld benchmark for SayCan, ReAct, Reflexion, Swift-only, SwiftSage, and our algorithm (last column).The Solved Task Types row represents the number of task types for which an agent manages to solve all the test variations.The table also shows the average scores for tasks with a short, medium, and long length of expert trajectory.The rows Task 1-1 and Task 3-3 display the scores for each of them in which our approach does not work well in comparison with the other methods.The * denotes scores reported from(Lin et al., 2023)which all use ChatGPT (GPT-3.5).
Overall Average25.2219.7623.4046.2562.2265.43Solved Task Types0/300/304/304/302/3011/30Short  †37.2428.9539.1979.6872.8191.61Medium20.0621.0914.7335.8055.3462.83Long18.6611.2316.2725.3657.9945.35Task 1-133.063.524.2215.058.016.22Task 3-399.5676.1972.5459.566.95.6
SayCan * ReAct * Reflexion * Swift-only SwiftSage * Ours</p>
<p>Only use valid actions and objects.If you know what are around, then suggest the following actions.You are allowed to do the following actions with the objects.Open or close OBJ meaning open or close a container , Deactivate or activate OBJ meaning activate or deactivate a device, connect OBJ to OBJ meaning connect electrical components , disconnect OBJ meaning disconnect electrical components , use OBJ [on OBJ]</p>
<p>Table 4 :
4
Expert trajectory] Here is the goal path to achieve to the goal: open door to greenhouse, go to greenhouse, open door to outside, go to outside, focus on dove, pick up dove, open door to kitchen, go to kitchen, move egg dove egg in inventory to red box Based on the given goal path, provide me with the functional format of high-level sub-tasks to complete this task and their corresponding actions.[sub-goals] 1-navigate to(greenhouse): {'open door to greenhouse', 'go to greenhouse'} 2-navigate to(outside): {'open door to outside', 'go to outside'} 3-Focus on(dove): {'focus on dove'} 4-pick up(dove): {'pick up dove'} 5-navigate to(kitchen): {'open door to kitchen', 'go to kitchen'} 6-move(dove egg, red box): {'move dove egg in inventory to red box'} Example 2 [Task Description] Your task is to find a(n) living thing.First, focus on the thing.Then, move it to the green box in the kitchen.[Expert trajectory] Here is the goal path to achieve to the goal: open door to kitchen, go to kitchen, open door to outside, go to outside, focus on egg turtle, pick up egg turtle, open door to kitchen, go to kitchen, move egg turtle egg in inventory to green box Based on the given goal path, provide me with the functional format of high-level sub-tasks to complete this task and their corresponding actions.[sub-goals] 1-navigate to(kitchen): {'open door to kitchen', 'go to kitchen'} 2-navigate to(outside): {'open door to outside', 'go to outside'} 3-Focus on(egg turtle): {'focus on egg turtle'} 4-pick up(egg turtle): {'pick up egg turtle'} 5-navigate to(kitchen): {'open door to kitchen', 'go to kitchen'} 6-move(egg turtle, green box): {'move egg turtle in inventory to green box'} The third part is just a new task description with an expert trajectory: ScienceWorld's tasks names and numbers of variations.The highlighted rows show training task types for the generalization experiment.
Task Type TopicTask Name# Variations1-1Matterboil301-2Mattermelt301-3Matterfreeze301-4Matterchange-the-state-of-matter-of302-1Measurementuse-thermometer5402-2Measurementmeasure-melting-point-known-substance4362-3Measurementmeasure-melting-point-unknown-substance3003-1Electricitypower-component203-2Electricitypower-component-renewable-vs-nonrenewable-energy203-3Electricitytest-conductivity9003-4Electricitytest-conductivity-of-unknown-substances6004-1Classificationfind-living-thing3004-2Classificationfind-non-living-thing3004-3Classificationfind-plant3004-4Classificationfind-animal3005-1Biologygrow-plant1265-2Biologygrow-fruit1266-1Chemistrychemistry-mix326-2Chemistrychemistry-mix-paint-secondary-color366-3Chemistrychemistry-mix-paint-tertiary-color367-1Biologylifespan-longest-lived1257-2Biologylifespan-shortest-lived1257-3Biologylifespan-longest-lived-then-shortest-lived1258-1Biologyidentify-life-stages-1148-2Biologyidentify-life-stages-2109-1Forcesinclined-plane-determine-angle1689-2Forcesinclined-plane-friction-named-surfaces13869-3Forcesinclined-plane-friction-unnamed-surfaces16210-1Biologymendelian-genetics-known-plant12010-2Biologymendelian-genetics-unknown-plant480</p>
<p>Table 5 :
5
The table illustrates the results for each task of the ScienceWorld for SayCan, ReAct, Reflexion, Swift-only, SwiftSage and our algorithm.Each row shows the average score of the test variations for a task type.Column Length shows the average lengths of the expert trajectories.</p>
<p>Table 6 :
6
The table displays the average scores for the models of different sizes.The first four columns depict scores when a model with a base size is used as the action generator.The subsequent four columns illustrate scores when the action generator is small size.The last column shows the results when both the action generator and sub-goal generator are x-large.A.7 SCORES FOR DIFFERENT MODEL SIZES WITHOUT SUB-GOALS
Action generatorBaseSmallX-LargeTasksSg generator Small Base Large X-Large Small Base Large X-Large X-Large1-10.20.00.04.20.00.00.00.016.551-20.00.00.00.220.00.00.440.2217.111-30.00.00.00.00.00.00.00.00.01-40.10.00.07.770.330.07.7719.4420.442-110.020.010090.030.010.080.070.01002-213.220.020.077.50.020.020.065.777.52-30.020.00.064.00.020.020.060.045.03-10.078.084.079.40.048.077.871.21003-210.636.447.077.20.027.849.056.477.23-326.020.022.06.47.310.53.51.528.03-440.030.040.030.050.31 30.060.030.047.054-1100100100100100100100901004-21001001001001001001001001004-310010092.59310010092.594.01004-410010010010090.91001001001005-166.2 56.1810072.782.867.110072.71005-231.87.4539.669.435.80.047.145.770.06-13.12 31.25 43.6236.6212.5 28.12 38.7534.6256.256-220.010.0 66.6674.4418.88 32.22 33.3335.5591.116-323.0 13.33 33.6652.331.16.33 18.2217.8854.887-180.010010010090.91001001001007-220.0 78.12 80.010040.050.050.060.01007-310010010010070.070.070.070.01008-111.618.460.040.00.018.460.040.060.08-20.000.042.50.00.00.025.067.59-128.0 35.65247042.044.033.074.084.09-226.032.6236535.644.032.070.082.09-327.0 29.89296630.043.031.069.083.010-127.026.930.128.224.925.225.225.228.210-230.2 27.75 30.427.723.019.529.918.930.1Average33.13 39.73 48.8559.1532.87 37.13 45.9850.5667.86</p>
<p>Table 7
7
presents task scores without the utilization of sub-goals, akin to the Swift-only method but employing language models of varying sizes.All models are FLAN-T5.
Task Type Small Base Large X-Large1-10.40.014.014.01-211.80.022.8822.01-30.00.024.5520.01-47.60.00.880.882-140.080.0 70.3291.662-243.245.66.5645.822-340.043.05.7641.03-159.885.285.2743-241.451.041.143.43-319.29.464.8833.293-450.033.0 81.2658.754-110095.01001004-210097.71001004-380.092.080.081.254-410095.01001005-164.264.424.060.425-234.535.0 43.5742.536-112.75 29.62 26.058.126-20.011.11 36.034.116-33.60.011.337.777-190.91001001007-240.01001001007-374.7 78.34 87.61008-18.235.246.034.48-20.00.08.041.09-120.020.040.740.09-219.020.043.643.09-320.020.0 44.5244.010-122.823.0 19.6623.010-222.922.9 20.2223.0Average37.56 42.88 46.2552.88</p>
<p>Table 7 :
7
Scores for each task without the utilization of sub-goals across various model sizes.All models are FLAN-T5.A.8 RESULTS FOR T5 LANGUAGE MODEL WITH AND WITHOUT SUB-GOALS</p>
<p>Table 8 :
8
Scores for the T5 model are depicted under two conditions: without sub-goals and with sub-goals.
No sub-goalsWith sub-goalsTask Type Large X-large Large X-large1-116.660.20.5518.01-20.056.0519.661-30.08.554.770.01-40.030.7716.2238.02-19.191.6670.070.02-237.841.020.027.72-310.030.034.044.03-185.278.639.248.63-246.055.264.830.63-350.214.611.031.763-430.035.7140.050.04-11001001001004-21001001001004-390.093.71001004-410010010074.335-110064.291.51005-28.937.82.11006-141.6243.6221.8731.256-215.5510055.5573.336-34.7780.029.235.07-190.01001001007-210010030.01007-380.060.01001008-155.435.260.069.08-20.00.00.00.09-120.020.036.033.339-221.020.034.236.19-322.020.037.139.010-127.923.029.6930.410-228.022.928.7528.0Average43.0052.0945.0554.26</p>
<p>Table 9 :
9
Scores for the random and semi-random sub-goals with fine-tuned FLAN-T5-LARGE action generator.
Task Type RandomSemi-randomSub-goalSub-goal1-100.51-2011-302.221-400.62-1032-203.12-3023-102.83-2043-35.583-4014-1004-258.381.44-3023.44-402.55-16.610.45-221.9606-10216-215.5523.336-36.44127-110607-230927-3008-1008-2009-105.59-2059-30410-101410-2020Average6.41314.15</p>
<p>Table 10 :
10
Scores for retrieving sub-goals are displayed.The first two columns depict the random and semi-random generation of the first sub-goals only.The second pair of columns illustrates the scores obtained when sub-goals are generated randomly or semi-randomly every 10 steps.
Task TypeWith sub-goal Swift-only1-1651-21.301-32.201-49.3374.332-138.91002-22033.92-32003-135.41003-214.603-316.919.63-42015.54-11001004-2951004-339.204-465535-190.781.85-25026-169.7523.756-262.22306-338.772.57-11001007-21001007-31016.68-14064.48-2009-144.5209-240199-341510-120.628.610-228.71.9Average score40.6336.56Average score seen tasks50.5152.85Average score unseen tasks27.7215.25</p>
<p>Table 11 :
11
The table displays the scores for the generalization experiment.The highlighted tasks in yellow called "seen tasks" which are the ones selected for the training.</p>
<p>https://github.com/chandar-lab/SubGoal_Distillation_LLM
Expert trajectories for test tasks were not seen during training.
ACKNOWLEDGMENTSSpecial thanks are due to Prasanna Parthasarathi for his invaluable insights, thoughtful brainstorming, and engaging discussions in the project.Also, thank you to Xingdi Yuan for initial discussions around knowledge distillation with LLM.Sarath Chandar is supported by the Canada CIFAR AI Chairs program, the Canada Research Chair in Lifelong Machine Learning, and the NSERC Discovery Grant.This research was enabled mostly by compute resources provided by Mila (mila.quebec)and partially by Microsoft.Request for sub-goal generating [Task Description] Your task is to find a(n) living thing.First, focus on the thing.Then, move it to the green box in the living room.[Expert trajectory] Here is the goal path to achieve to the goal: open door to hallway, go to hallway, open door to greenhouse, go to greenhouse, open door to outside, go to outside, focus on baby baby beaver, pick up baby baby beaver, open door to greenhouse, go to greenhouse, open door to hallway, go to hallway, open door to living room, go to living room, move baby baby beaver in inventory to green box Based on the given goal path, provide me with the functional format of high-level sub-tasks to complete this task and their corresponding actions.A.2 INPUT PROMPT FOR THE POLICIESHere, we show the inputs and outputs for both the action generator and sub-goal generator.We followed the format used by SwiftSage(Lin et al., 2023), incorporating sub-goals and making minor textual adjustments accordingly.Input for the action generator -the agent -a substance called air -an axe -a blue jay egg -a butterfly egg -a dove egg -a fire pit -a fountain (containing a substance called water) -the ground -a substance called wood -You also see: -A door to the foundry -A door to the greenhouse -A door to the kitchen -&lt; /s &gt;; Current inventory: In your inventory, you see: -an orange -&lt; /s &gt;; Visited rooms: hallway, greenhouse, outside &lt; /s &gt;; What subtask should you do next?&lt; /s &gt; Output for the sub-goal generator Focus on(dove)A.5 SCORES OF EACH TASKThe average score of each task of the ScienceWorld is shown in Table5.The methods are SayCan, ReAct, Reflexion, Swift-only, SwiftSage and our algorithm.In all of them ChatGPT is used as the LLM.The language models for Swiftonly, SwiftSage and our algorithm are FLAN-T5-LARGE.For the methods, SayCan, ReAct, Reflexion, and SwiftSage we used the results fromLin et al. (2023).However, we reproduced the results for Swift-only and we found a lower performance than what was reported in the paper.So here we presented our scores.The scores for SayCan, ReAct, Reflexion, and SwiftSage when they use GPT4 as the LLM are higher according to the results reported inLin et al. (2023).However, due to limited access to GPT-4, we utilized ChatGPT, and thus, we present the results obtained using ChatGPT.
Graph constrained reinforcement learning for natural language action spaces. Prithviraj Ammanabrolu, Matthew Hausknecht, International Conference on Learning Representations. 2019</p>
<p>On the opportunities and risks of foundation models. Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney Von Arx, Jeannette Michael S Bernstein, Antoine Bohg, Emma Bosselut, Brunskill, arXiv:2108.072582021arXiv preprint</p>
<p>Do as i can, not as i say: Grounding language in robotic affordances. Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian, Conference on Robot Learning. PMLR2023</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Model compression. Cristian Buciluǎ, Rich Caruana, Alexandru Niculescu-Mizil, Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining. the 12th ACM SIGKDD international conference on Knowledge discovery and data mining2006</p>
<p>Decision transformer: Reinforcement learning via sequence modeling. Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, Igor Mordatch, Advances in neural information processing systems. 202134</p>
<p>Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, April 2023142023</p>
<p>Palm: Scaling language modeling with pathways. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Journal of Machine Learning Research. 242402023</p>
<p>Scaling instruction-finetuned language models. Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, arXiv:2210.114162022arXiv preprint</p>
<p>Textworld: A learning environment for text-based games. Marc-Alexandre Côté, Ákos Kádár, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Ruo Yu Tao, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, Wendy Tay, Adam Trischler, CoRR, abs/1806.115322018</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.048052018arXiv preprint</p>
<p>Knowledge distillation: A survey. Jianping Gou, Baosheng Yu, Stephen J Maybank, Dacheng Tao, International Journal of Computer Vision. 1292021</p>
<p>Pre-trained models: Past, present and future. Xu Han, Zhengyan Zhang, Ning Ding, Yuxian Gu, Xiao Liu, Yuqi Huo, Jiezhong Qiu, Yuan Yao, Ao Zhang, Liang Zhang, AI Open. 22021</p>
<p>Interactive fiction games: A colossal adventure. Matthew Hausknecht, Prithviraj Ammanabrolu, Côté Marc-Alexandre, Yuan Xingdi, CoRR, abs/1909.053982019</p>
<p>Deep reinforcement learning with a combinatorial action space for predicting popular reddit threads. Ji He, Mari Ostendorf, Xiaodong He, Jianshu Chen, Jianfeng Gao, Lihong Li, Li Deng, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational Linguistics2016</p>
<p>Geoffrey Hinton, Oriol Vinyals, Jeff Dean, arXiv:1503.02531Distilling the knowledge in a neural network. 2015arXiv preprint</p>
<p>Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes. Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, Tomas Pfister, arXiv:2305.023012023arXiv preprint</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. Wenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch, International Conference on Machine Learning. PMLR2022a</p>
<p>Inner monologue: Embodied reasoning through planning with language models. Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, arXiv:2207.056082022barXiv preprint</p>
<p>Decomposed prompting: A modular approach for solving complex tasks. Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, Ashish Sabharwal, arXiv:2210.024062022arXiv preprint</p>
<p>Code as policies: Language model programs for embodied control. Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, Andy Zeng, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023</p>
<p>Swiftsage: A generative agent with fast and slow thinking for complex interactive tasks. Yicheng Bill Yuchen Lin, Karina Fu, Prithviraj Yang, Faeze Ammanabrolu, Shiyu Brahman, Chandra Huang, Yejin Bhagavatula, Xiang Choi, Ren, arXiv:2305.173902023arXiv preprint</p>
<p>Kyle Mahowald, Anna A Ivanova, Idan A Blank, Nancy Kanwisher, Joshua B Tenenbaum, Evelina Fedorenko, arXiv:2301.06627Dissociating language and thought in large language models: a cognitive perspective. 2023arXiv preprint</p>
<p>Text-based RL Agents with Commonsense Knowledge: New Challenges, Environments and Baselines. Keerthiram Murugesan, Mattia Atzeni, Pavan Kapanipathi, Pushkar Shukla, Sadhana Kumaravel, Gerald Tesauro, Kartik Talamadupula, Mrinmaya Sachan, Murray Campbell, Thirty Fifth AAAI Conference on Artificial Intelligence. 2021</p>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, arXiv:2112.09332Browser-assisted question-answering with human feedback. 2021arXiv preprint</p>
<p>Introducing chatgpt. Openai, Openai, OpenAI. Gpt-4 technical report. 2022. 2023</p>
<p>Generative agents: Interactive simulacra of human behavior. Sung Joon, Park, O' Joseph, Carrie Jun Brien, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology. the 36th Annual ACM Symposium on User Interface Software and Technology2023</p>
<p>Mapping language models to grounded conceptual spaces. Roma Patel, Ellie Pavlick, International Conference on Learning Representations. 2021</p>
<p>Baolin Peng, Chunyuan Li, arXiv:2304.03277Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4. 2023arXiv preprint</p>
<p>Adapt: As-needed decomposition and planning with language models. Archiki Prasad, Alexander Koller, Mareike Hartmann, Peter Clark, Ashish Sabharwal, Mohit Bansal, Tushar Khot, arXiv:2311.057722023arXiv preprint</p>
<p>Scaling language models: Methods, analysis &amp; insights from training gopher. Sebastian Jack W Rae, Trevor Borgeaud, Katie Cai, Jordan Millican, Francis Hoffmann, John Song, Sarah Aslanides, Roman Henderson, Susannah Ring, Young, arXiv:2112.114462021arXiv preprint</p>
<p>Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, Franc ¸ois Yvon, arXiv:2211.05100A 176b-parameter open-access multilingual language model. 2022arXiv preprint</p>
<p>Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, arXiv:2302.04761Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. 2023arXiv preprint</p>
<p>Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang, arXiv:2303.175802023arXiv preprint</p>
<p>ALF-World: Aligning Text and Embodied Environments for Interactive Learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Shunyu Karthik R Narasimhan, Yao, Proceedings of the International Conference on Learning Representations (ICLR). Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, Matthew Hausknecht, the International Conference on Learning Representations (ICLR)2023. 2021Thirty-seventh Conference on Neural Information Processing Systems</p>
<p>Progprompt: Generating situated robot task plans using large language models. Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023Jesse Thomason, and Animesh Garg</p>
<p>Pearl: Prompting large language models to plan and execute actions over long documents. Simeng Sun, Yang Liu, Shuohang Wang, Chenguang Zhu, Mohit Iyyer, arXiv:2305.145642023arXiv preprint</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, Alpaca: A strong, replicable instruction-following model. Stanford Center for Research on Foundation Models. 202337</p>
<p>Behavioral cloning from observation. Faraz Torabi, Garrett Warnell, Peter Stone, Proceedings of the 27th International Joint Conference on Artificial Intelligence. the 27th International Joint Conference on Artificial Intelligence2018</p>
<p>Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, Prithviraj Ammanabrolu, arXiv:2203.07540Scienceworld: Is your agent smarter than a 5th grader?. 2022aarXiv preprint</p>
<p>Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, Hannaneh Hajishirzi, arXiv:2212.10560Self-instruct: Aligning language model with self generated instructions. 2022barXiv preprint</p>
<p>Symbolic knowledge distillation: from general language models to commonsense models. Peter West, Chandra Bhagavatula, Jack Hessel, Jena D Hwang, Liwei Jiang, Ronan Le Bras, Ximing Lu, Sean Welleck, Yejin Choi, arXiv:2110.071782021arXiv preprint</p>
<p>Keep calm and explore: Language models for action generation in text-based games. Shunyu Yao, Rohan Rao, Matthew Hausknecht, Karthik Narasimhan, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. the 2020 Conference on Empirical Methods in Natural Language Processing2020</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, arXiv:2210.03629Task Type Length SayCan ReAct Reflexion Swift-only SwiftSage Ours. 2022arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
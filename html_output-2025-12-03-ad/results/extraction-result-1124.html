<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1124 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1124</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1124</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-246442009</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2202.00602v3.pdf" target="_blank">Meta-Learning Hypothesis Spaces for Sequential Decision-making</a></p>
                <p><strong>Paper Abstract:</strong> Obtaining reliable, adaptive confidence sets for prediction functions (hypotheses) is a central challenge in sequential decision-making tasks, such as bandits and model-based reinforcement learning. These confidence sets typically rely on prior assumptions on the hypothesis space, e.g., the known kernel of a Reproducing Kernel Hilbert Space (RKHS). Hand-designing such kernels is error prone, and misspecification may lead to poor or unsafe performance. In this work, we propose to meta-learn a kernel from offline data (Meta-KeL). For the case where the unknown kernel is a combination of known base kernels, we develop an estimator based on structured sparsity. Under mild conditions, we guarantee that our estimated RKHS yields valid confidence sets that, with increasing amounts of offline data, become as tight as those given the true unknown kernel. We demonstrate our approach on the kernelized bandit problem (a.k.a.~Bayesian optimization), where we establish regret bounds competitive with those given the true kernel. We also empirically evaluate the effectiveness of our approach on a Bayesian optimization task.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1124.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1124.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>META-KEL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Meta-Kernel Learning (META-KEL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An offline meta-learning algorithm that estimates a sparse combination of base kernels from multi-task (possibly non-i.i.d.) datasets via a Group-Lasso formulation, producing a meta-learned kernel (RKHS) that yields valid anytime confidence sets for downstream sequential decision agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>META-KEL (meta-learner)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Convex meta-learner that jointly fits per-task regularized predictors and a shared kernel weight vector η by solving a convex objective that reduces to Group Lasso (Eq. 8). Key components: (1) multi-task feature matrix Φ, (2) Group-Lasso optimization over grouped coefficients β (solved with CELER in experiments), (3) mapping β -> kernel weights η_j = ||β^{(j)}||_2 and construction of meta-learned kernel k(x,x') = Σ_j η_j c1 φ_j(x)^T φ_j(x').</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Meta-learning of hypothesis space / kernel selection (structural sparsity via Group Lasso); provides adapted priors for adaptive experimental design algorithms (e.g., GP-UCB).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Learns a sparse weighting over a given set of base kernels using offline meta-data from m tasks with n samples each; uses a Group Lasso penalty (λ||·||_{group}) to select active kernels and recover sparsity pattern J_{k*}. The learned kernel adapts the downstream learner's hypothesis space and its confidence sets by (i) reducing irrelevant basis functions, and (ii) scaling RKHS norms so confidence widths decrease as meta-data grows (error term (n,m) ≍ O(s/√(mn)·√log p)).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Provides prior/hypothesis adaptation for downstream sequential decision problems (kernelized bandits / Bayesian optimization / model-based RL); evaluated on synthetic BO tasks (1D, 2D) and GLMNET hyper-parameter tuning benchmark (OpenML).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Offline meta-data comes from related tasks where each task's labels follow y_{s,i} = f_s(x_{s,i}) + ε_{s,i} with f_s ∈ H_{k*}; meta-data may be non-i.i.d.; for downstream problems the environment is an unknown continuous function f: X→R in an RKHS, observed only via noisy pointwise evaluations (sub-Gaussian noise).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Theory assumes finite set of p candidate base kernels (p = 20 in experiments), each with finite feature dimension d_j and at most s active kernels (s = 5 in experiments); typical experimental meta-data used m = 50, n = 50 (default), B (RKHS norm bound) = 10, observation noise σ = 0.01 in synthetic experiments; hyperparameter tuning meta-train used m = 25 with 500 Random Fourier Features.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Theoretical: meta-learned confidence bounds converge to oracle (known-kernel) bounds at rate O(1/√(mn)) (more precisely the error term (n,m) = O( s/(κ^2(s)√(mn)) · √(1 + (2/m)log(2p/δ) + m d_max log(2p/δ)) )). Empirical: meta-learned kernel yields well-calibrated, sharp confidence intervals and produces downstream BO performance competitive with oracle kernel (figures show near-oracle simple and cumulative regret for m=n=50). (No single scalar regret numbers reported in paper's text.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Using a conservative hand-picked kernel k_full (includes all base kernels) yields conservative (over-wide) confidence sets and slower convergence of downstream learners; using oracle kernel k* gives best performance. Experiments show meta-learned kernel outperforms k_full and is competitive with k*; exact numerical baselines not tabulated in text.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Meta-learning reduces downstream sample complexity by producing a smaller RKHS: theory and experiments indicate fewer BO queries needed to find optimum compared to k_full. The meta-error term shrinks faster with number of tasks m than samples per task n, so benefits accrue quickly as m grows (e.g., improvements visible for m up to 50 in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>META-KEL itself does not perform online exploration/exploitation; it changes the hypothesis/prior used by downstream adaptive agents so that their confidence sets (and thus exploration) are tighter and better calibrated. Downstream agents plug the learned kernel into usual confidence-based exploration (e.g., UCB).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared (theoretically/empirically) to: oracle true kernel k*, conservative hand-picked kernel k_full (equal-weight combination of all base kernels), infinite-dimensional SE (RBF) kernel in some BO experiments, and no-meta baselines in hyperparameter tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>1) The meta-learned RKHS H_k contains the true H_{k*} w.h.p. under mild assumptions (Theorem 4.3). 2) The Group-Lasso reduction yields structural sparsity recovering active base kernels (Proposition 4.4). 3) The meta-learned kernel yields anytime-valid confidence bounds for any f∈H_{k*} (Theorem 5.1) with extra multiplicative/additive terms shrinking as O(1/√(mn)). 4) When plugged into GP-UCB the agent attains sublinear regret R_T = O(d B log T √T) and the bound approaches the oracle bound as meta-data increases (Corollary 5.2). 5) Empirically, meta-learned kernel produces calibrated, sharper confidence intervals and improves BO convergence vs. k_full and SE in the reported tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Requires that the true kernel k* is a non-negative finite combination of provided base kernels (finite p). Assumptions needed: Group Beta-min (coefficients for active groups bounded away from zero), Relaxed Sufficient Exploration (meta-data diversity), and finite feature dimensions (d_max < ∞) for theory. If these assumptions fail or meta-data is insufficient (small m or n), kernel recovery can fail and downstream learner may be misspecified (leading to slow or non-convergent behavior). The theoretical analysis does not cover infinite-dimensional base-kernel ensembles except empirically.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1124.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1124.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GP-UCB+META-KEL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GP-UCB (Gaussian Process Upper Confidence Bound) with meta-learned kernel</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard GP-UCB Bayesian optimization algorithm that uses a GP prior with the META-KEL meta-learned kernel; selects queries by maximizing μ_{t-1}(x) + ν_t σ_{t-1}(x) where ν_t accounts for meta-learning error, producing anytime-valid confidence-guided exploration and sublinear regret guarantees.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GP-UCB (with meta-learned kernel)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Gaussian Process Upper Confidence Bound algorithm: maintains GP posterior mean μ_{t-1}(x) and standard deviation σ_{t-1}(x) under GP(0,k) with k = META-KEL output; chooses x_t = argmax_x μ_{t-1}(x) + ν_t σ_{t-1}(x). Exploration coefficient ν_t is set based on Theorem 5.1/Corollary 5.2 and includes the meta-learning error term (n,m).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Bayesian optimization with UCB acquisition (upper confidence bound); adapts queries to maximize information-weighted utility (μ + νσ).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>At each step the agent computes GP posterior using the meta-learned kernel and chooses the next input to maximize μ_{t-1}(x)+ν_t σ_{t-1}(x). ν_t is chosen to guarantee high-probability containment of f in constructed confidence sets and incorporates the meta-learning error (n,m), so exploration is scaled to account for uncertainty in the learned kernel. The agent thereby adapts where to evaluate based on current posterior mean/variance and the meta-learned prior.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Kernelized bandit / Bayesian optimization tasks: synthetic 1D and 2D functions (constructed from Legendre polynomial basis), and hyper-parameter tuning tasks (GLMNET hyperparameters on OpenML datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown reward/response function f in an RKHS (finite combination of base kernels), noisy pointwise observations y_t = f(x_t)+ε_t with i.i.d. zero-mean sub-Gaussian noise (variance proxy σ^2), compact continuous domain X ⊂ R^{d0}; meta-data tasks may be non-i.i.d.; for GLMNET hyperparameter tuning the evaluation domain is effectively finite but dense (10k–30k pre-computed evaluations). Observability: only noisy scalar outputs observed (partial observability of f).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Experiment settings: synthetic p=20 candidate base kernels, s=5 active kernels, d determined by sum of d_j; default meta-data m=n=50 for synthetic experiments; BO horizon T up to 100 in consistency experiments; hyperparameter tuning: meta-train m=25, RFF 500 features. No explicit action-space cardinality; continuous input spaces [-1,1] (1D) and [-1,1]^2 (2D).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Theoretical: GP-UCB with meta-learned kernel achieves sublinear regret R_T = O(d B log T √T) (Corollary 5.2), where d is effective dimension of the meta-learned RKHS; the bound approaches the oracle R_T = O(d_* B log T √T) as meta-data increases. Empirical: across 100 random instances, GP-UCB using META-KEL converged to the optimum faster than when using k_full and had performance competitive with the oracle k*; in hyperparameter tuning GP-UCB with META-KEL found good configurations substantially faster than GP-UCB with SE kernel (SE took ≈50 iterations to find a good configuration in that experiment). Exact numeric regret values are reported in figures but not enumerated in text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>GP-UCB with a conservative k_full kernel yields slower convergence and more conservative confidence sets; GP-UCB with SE kernel can be suboptimal in low-dimensional problems (e.g., synthetic 2D BO, hyperparameter tuning). Oracle kernel k* yields best performance; misspecified kernel (f ∉ H_k) can incur linear-order lower-bound regret O(T √log T) per cited prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Empirically improved: meta-learned kernel reduces number of BO evaluations needed to reach comparable simple regret relative to k_full and SE baselines; with m=n=50 experiments showed near-oracle performance within T=100 iterations. Theoretically, meta-error decays O(1/√(mn)) so increasing meta-tasks m improves downstream sample efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Balanced via UCB acquisition μ + ν_t σ. ν_t is computed to ensure valid confidence intervals accounting for meta-learning uncertainty: ν_t = B(1+(n,m))/2c1 + σ d log(1+σ^{-2}_t/c1) + 2 + 2 log(1/δ) (Corollary 5.2). Thus exploration scales with posterior variance and extra terms for learned-kernel uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared to oracle GP-UCB with true kernel k*, GP-UCB with k_full (equal-weight kernel over all base kernels), GP-UCB with Squared Exponential (SE/RBF) kernel in some experiments, and implicit baselines (random sampling behavior inferred).</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>1) When using the META-KEL kernel, GP-UCB retains anytime-valid confidence sets and provable sublinear regret; the bound degrades gracefully by a multiplicative/additive factor that decays as meta-data increases. 2) Empirically, meta-learned kernel produces sharper, calibrated confidence intervals and faster BO convergence than k_full and SE in the tested tasks (1D, 2D, hyperparameter tuning). 3) Increasing number of offline tasks m improves downstream regret and approaches oracle performance (consistency experiment).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Performance depends on correctness of meta-learned kernel: if k* is not within span of base kernels or meta-data violates assumptions (Group Beta-min, sufficient exploration), the meta-learner can misselect kernels and the downstream GP-UCB may suffer misspecification (potentially linear regret). Theoretical guarantees require finite p and finite d_j; extension to infinite-dimensional base kernels is not covered by proofs (though empirical use with RFF is reported).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Gaussian process optimization in the bandit setting: No regret and experimental design <em>(Rating: 2)</em></li>
                <li>No-regret Bayesian optimization with unknown hyperparameters <em>(Rating: 2)</em></li>
                <li>Misspecified Gaussian process bandit optimization <em>(Rating: 2)</em></li>
                <li>No regrets for learning the prior in bandits <em>(Rating: 1)</em></li>
                <li>Regret bounds for meta bayesian optimization with an unknown gaussian process prior <em>(Rating: 1)</em></li>
                <li>Meta-learning reliable priors in the function space <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1124",
    "paper_id": "paper-246442009",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "META-KEL",
            "name_full": "Meta-Kernel Learning (META-KEL)",
            "brief_description": "An offline meta-learning algorithm that estimates a sparse combination of base kernels from multi-task (possibly non-i.i.d.) datasets via a Group-Lasso formulation, producing a meta-learned kernel (RKHS) that yields valid anytime confidence sets for downstream sequential decision agents.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "META-KEL (meta-learner)",
            "agent_description": "Convex meta-learner that jointly fits per-task regularized predictors and a shared kernel weight vector η by solving a convex objective that reduces to Group Lasso (Eq. 8). Key components: (1) multi-task feature matrix Φ, (2) Group-Lasso optimization over grouped coefficients β (solved with CELER in experiments), (3) mapping β -&gt; kernel weights η_j = ||β^{(j)}||_2 and construction of meta-learned kernel k(x,x') = Σ_j η_j c1 φ_j(x)^T φ_j(x').",
            "adaptive_design_method": "Meta-learning of hypothesis space / kernel selection (structural sparsity via Group Lasso); provides adapted priors for adaptive experimental design algorithms (e.g., GP-UCB).",
            "adaptation_strategy_description": "Learns a sparse weighting over a given set of base kernels using offline meta-data from m tasks with n samples each; uses a Group Lasso penalty (λ||·||_{group}) to select active kernels and recover sparsity pattern J_{k*}. The learned kernel adapts the downstream learner's hypothesis space and its confidence sets by (i) reducing irrelevant basis functions, and (ii) scaling RKHS norms so confidence widths decrease as meta-data grows (error term (n,m) ≍ O(s/√(mn)·√log p)).",
            "environment_name": "Provides prior/hypothesis adaptation for downstream sequential decision problems (kernelized bandits / Bayesian optimization / model-based RL); evaluated on synthetic BO tasks (1D, 2D) and GLMNET hyper-parameter tuning benchmark (OpenML).",
            "environment_characteristics": "Offline meta-data comes from related tasks where each task's labels follow y_{s,i} = f_s(x_{s,i}) + ε_{s,i} with f_s ∈ H_{k*}; meta-data may be non-i.i.d.; for downstream problems the environment is an unknown continuous function f: X→R in an RKHS, observed only via noisy pointwise evaluations (sub-Gaussian noise).",
            "environment_complexity": "Theory assumes finite set of p candidate base kernels (p = 20 in experiments), each with finite feature dimension d_j and at most s active kernels (s = 5 in experiments); typical experimental meta-data used m = 50, n = 50 (default), B (RKHS norm bound) = 10, observation noise σ = 0.01 in synthetic experiments; hyperparameter tuning meta-train used m = 25 with 500 Random Fourier Features.",
            "uses_adaptive_design": false,
            "performance_with_adaptation": "Theoretical: meta-learned confidence bounds converge to oracle (known-kernel) bounds at rate O(1/√(mn)) (more precisely the error term (n,m) = O( s/(κ^2(s)√(mn)) · √(1 + (2/m)log(2p/δ) + m d_max log(2p/δ)) )). Empirical: meta-learned kernel yields well-calibrated, sharp confidence intervals and produces downstream BO performance competitive with oracle kernel (figures show near-oracle simple and cumulative regret for m=n=50). (No single scalar regret numbers reported in paper's text.)",
            "performance_without_adaptation": "Using a conservative hand-picked kernel k_full (includes all base kernels) yields conservative (over-wide) confidence sets and slower convergence of downstream learners; using oracle kernel k* gives best performance. Experiments show meta-learned kernel outperforms k_full and is competitive with k*; exact numerical baselines not tabulated in text.",
            "sample_efficiency": "Meta-learning reduces downstream sample complexity by producing a smaller RKHS: theory and experiments indicate fewer BO queries needed to find optimum compared to k_full. The meta-error term shrinks faster with number of tasks m than samples per task n, so benefits accrue quickly as m grows (e.g., improvements visible for m up to 50 in experiments).",
            "exploration_exploitation_tradeoff": "META-KEL itself does not perform online exploration/exploitation; it changes the hypothesis/prior used by downstream adaptive agents so that their confidence sets (and thus exploration) are tighter and better calibrated. Downstream agents plug the learned kernel into usual confidence-based exploration (e.g., UCB).",
            "comparison_methods": "Compared (theoretically/empirically) to: oracle true kernel k*, conservative hand-picked kernel k_full (equal-weight combination of all base kernels), infinite-dimensional SE (RBF) kernel in some BO experiments, and no-meta baselines in hyperparameter tuning.",
            "key_results": "1) The meta-learned RKHS H_k contains the true H_{k*} w.h.p. under mild assumptions (Theorem 4.3). 2) The Group-Lasso reduction yields structural sparsity recovering active base kernels (Proposition 4.4). 3) The meta-learned kernel yields anytime-valid confidence bounds for any f∈H_{k*} (Theorem 5.1) with extra multiplicative/additive terms shrinking as O(1/√(mn)). 4) When plugged into GP-UCB the agent attains sublinear regret R_T = O(d B log T √T) and the bound approaches the oracle bound as meta-data increases (Corollary 5.2). 5) Empirically, meta-learned kernel produces calibrated, sharper confidence intervals and improves BO convergence vs. k_full and SE in the reported tasks.",
            "limitations_or_failures": "Requires that the true kernel k* is a non-negative finite combination of provided base kernels (finite p). Assumptions needed: Group Beta-min (coefficients for active groups bounded away from zero), Relaxed Sufficient Exploration (meta-data diversity), and finite feature dimensions (d_max &lt; ∞) for theory. If these assumptions fail or meta-data is insufficient (small m or n), kernel recovery can fail and downstream learner may be misspecified (leading to slow or non-convergent behavior). The theoretical analysis does not cover infinite-dimensional base-kernel ensembles except empirically.",
            "uuid": "e1124.0"
        },
        {
            "name_short": "GP-UCB+META-KEL",
            "name_full": "GP-UCB (Gaussian Process Upper Confidence Bound) with meta-learned kernel",
            "brief_description": "Standard GP-UCB Bayesian optimization algorithm that uses a GP prior with the META-KEL meta-learned kernel; selects queries by maximizing μ_{t-1}(x) + ν_t σ_{t-1}(x) where ν_t accounts for meta-learning error, producing anytime-valid confidence-guided exploration and sublinear regret guarantees.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "GP-UCB (with meta-learned kernel)",
            "agent_description": "Gaussian Process Upper Confidence Bound algorithm: maintains GP posterior mean μ_{t-1}(x) and standard deviation σ_{t-1}(x) under GP(0,k) with k = META-KEL output; chooses x_t = argmax_x μ_{t-1}(x) + ν_t σ_{t-1}(x). Exploration coefficient ν_t is set based on Theorem 5.1/Corollary 5.2 and includes the meta-learning error term (n,m).",
            "adaptive_design_method": "Bayesian optimization with UCB acquisition (upper confidence bound); adapts queries to maximize information-weighted utility (μ + νσ).",
            "adaptation_strategy_description": "At each step the agent computes GP posterior using the meta-learned kernel and chooses the next input to maximize μ_{t-1}(x)+ν_t σ_{t-1}(x). ν_t is chosen to guarantee high-probability containment of f in constructed confidence sets and incorporates the meta-learning error (n,m), so exploration is scaled to account for uncertainty in the learned kernel. The agent thereby adapts where to evaluate based on current posterior mean/variance and the meta-learned prior.",
            "environment_name": "Kernelized bandit / Bayesian optimization tasks: synthetic 1D and 2D functions (constructed from Legendre polynomial basis), and hyper-parameter tuning tasks (GLMNET hyperparameters on OpenML datasets).",
            "environment_characteristics": "Unknown reward/response function f in an RKHS (finite combination of base kernels), noisy pointwise observations y_t = f(x_t)+ε_t with i.i.d. zero-mean sub-Gaussian noise (variance proxy σ^2), compact continuous domain X ⊂ R^{d0}; meta-data tasks may be non-i.i.d.; for GLMNET hyperparameter tuning the evaluation domain is effectively finite but dense (10k–30k pre-computed evaluations). Observability: only noisy scalar outputs observed (partial observability of f).",
            "environment_complexity": "Experiment settings: synthetic p=20 candidate base kernels, s=5 active kernels, d determined by sum of d_j; default meta-data m=n=50 for synthetic experiments; BO horizon T up to 100 in consistency experiments; hyperparameter tuning: meta-train m=25, RFF 500 features. No explicit action-space cardinality; continuous input spaces [-1,1] (1D) and [-1,1]^2 (2D).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Theoretical: GP-UCB with meta-learned kernel achieves sublinear regret R_T = O(d B log T √T) (Corollary 5.2), where d is effective dimension of the meta-learned RKHS; the bound approaches the oracle R_T = O(d_* B log T √T) as meta-data increases. Empirical: across 100 random instances, GP-UCB using META-KEL converged to the optimum faster than when using k_full and had performance competitive with the oracle k*; in hyperparameter tuning GP-UCB with META-KEL found good configurations substantially faster than GP-UCB with SE kernel (SE took ≈50 iterations to find a good configuration in that experiment). Exact numeric regret values are reported in figures but not enumerated in text.",
            "performance_without_adaptation": "GP-UCB with a conservative k_full kernel yields slower convergence and more conservative confidence sets; GP-UCB with SE kernel can be suboptimal in low-dimensional problems (e.g., synthetic 2D BO, hyperparameter tuning). Oracle kernel k* yields best performance; misspecified kernel (f ∉ H_k) can incur linear-order lower-bound regret O(T √log T) per cited prior work.",
            "sample_efficiency": "Empirically improved: meta-learned kernel reduces number of BO evaluations needed to reach comparable simple regret relative to k_full and SE baselines; with m=n=50 experiments showed near-oracle performance within T=100 iterations. Theoretically, meta-error decays O(1/√(mn)) so increasing meta-tasks m improves downstream sample efficiency.",
            "exploration_exploitation_tradeoff": "Balanced via UCB acquisition μ + ν_t σ. ν_t is computed to ensure valid confidence intervals accounting for meta-learning uncertainty: ν_t = B(1+(n,m))/2c1 + σ d log(1+σ^{-2}_t/c1) + 2 + 2 log(1/δ) (Corollary 5.2). Thus exploration scales with posterior variance and extra terms for learned-kernel uncertainty.",
            "comparison_methods": "Compared to oracle GP-UCB with true kernel k*, GP-UCB with k_full (equal-weight kernel over all base kernels), GP-UCB with Squared Exponential (SE/RBF) kernel in some experiments, and implicit baselines (random sampling behavior inferred).",
            "key_results": "1) When using the META-KEL kernel, GP-UCB retains anytime-valid confidence sets and provable sublinear regret; the bound degrades gracefully by a multiplicative/additive factor that decays as meta-data increases. 2) Empirically, meta-learned kernel produces sharper, calibrated confidence intervals and faster BO convergence than k_full and SE in the tested tasks (1D, 2D, hyperparameter tuning). 3) Increasing number of offline tasks m improves downstream regret and approaches oracle performance (consistency experiment).",
            "limitations_or_failures": "Performance depends on correctness of meta-learned kernel: if k* is not within span of base kernels or meta-data violates assumptions (Group Beta-min, sufficient exploration), the meta-learner can misselect kernels and the downstream GP-UCB may suffer misspecification (potentially linear regret). Theoretical guarantees require finite p and finite d_j; extension to infinite-dimensional base kernels is not covered by proofs (though empirical use with RFF is reported).",
            "uuid": "e1124.1"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Gaussian process optimization in the bandit setting: No regret and experimental design",
            "rating": 2,
            "sanitized_title": "gaussian_process_optimization_in_the_bandit_setting_no_regret_and_experimental_design"
        },
        {
            "paper_title": "No-regret Bayesian optimization with unknown hyperparameters",
            "rating": 2,
            "sanitized_title": "noregret_bayesian_optimization_with_unknown_hyperparameters"
        },
        {
            "paper_title": "Misspecified Gaussian process bandit optimization",
            "rating": 2,
            "sanitized_title": "misspecified_gaussian_process_bandit_optimization"
        },
        {
            "paper_title": "No regrets for learning the prior in bandits",
            "rating": 1,
            "sanitized_title": "no_regrets_for_learning_the_prior_in_bandits"
        },
        {
            "paper_title": "Regret bounds for meta bayesian optimization with an unknown gaussian process prior",
            "rating": 1,
            "sanitized_title": "regret_bounds_for_meta_bayesian_optimization_with_an_unknown_gaussian_process_prior"
        },
        {
            "paper_title": "Meta-learning reliable priors in the function space",
            "rating": 1,
            "sanitized_title": "metalearning_reliable_priors_in_the_function_space"
        }
    ],
    "cost": 0.01709775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Meta-Learning Hypothesis Spaces for Sequential Decision-making</p>
<p>Parnian Kassraie 
Jonas Rothfuss 
Andreas Krause 
Meta-Learning Hypothesis Spaces for Sequential Decision-making</p>
<p>Obtaining reliable, adaptive confidence sets for prediction functions (hypotheses) is a central challenge in sequential decision-making tasks, such as bandits and model-based reinforcement learning. These confidence sets typically rely on prior assumptions on the hypothesis space, e.g., the known kernel of a Reproducing Kernel Hilbert Space (RKHS). Hand-designing such kernels is error prone, and misspecification may lead to poor or unsafe performance. In this work, we propose to meta-learn a kernel from offline data (META-KEL). For the case where the unknown kernel is a combination of known base kernels, we develop an estimator based on structured sparsity. Under mild conditions, we guarantee that our estimated RKHS yields valid confidence sets that, with increasing amounts of offline data, become as tight as those given the true unknown kernel. We demonstrate our approach on the kernelized bandit problem (a.k.a. Bayesian optimization), where we establish regret bounds competitive with those given the true kernel. We also empirically evaluate the effectiveness of our approach on a Bayesian optimization task.</p>
<p>Introduction</p>
<p>A number of well-studied machine learning problems such as bandits, Bayesian optimization (BO) and model-based reinforcement learning are characterized by an agent that sequentially interacts with an unknown, responsive system. Throughout the interaction, the agent's goal is to maximize the cumulative reward based on an unknown underlying function f . Common to such sequential decision-making problems is an exploration-exploitation trade-off. That is, the agent needs to optimize its reward while, at the same time, learns more about the unknown function f . Confidence sets capture and quantify the uncertainty of the learner about f . Thus, they are an integral tool for directing exploration towards areas of high uncertainty and balancing it against exploitation. Moreover, in safety-critical applications, confidence sets are used to reason about the safety of actions. Thus, they are central to efficiency and safety of exploration. In theoretical analysis of sequential decisionmaking algorithms, a common assumption is that f resides in an RKHS with a known kernel function. This assumption allows for the construction of the confidence sets.</p>
<p>In practice, however, the true kernel is unknown and needs to be hand-crafted based on the problem instance. This is a delicate task, since the hand-crafted hypothesis space has to contain the unknown target function f . If this is not the case, the learner may be over-confident and converge to a sub-optimal policy, or incorrectly classify actions as safe. At the same time, we want the chosen hypothesis space to be as small so that the variance of the associated learner is low and the agent converges quickly. This constitutes a dilemma, where we need to trade off efficiency with a potential loss in consistency.</p>
<p>We approach this dilemma in a data-driven manner. Many applications of sequential decision-making, such as hyperparameter tuning with BO or online nonlinear control, are of repetitive nature. Often, there is available data from similar but not identical tasks which have been solved before. Therefore, we propose to meta-learn the kernel function, and thus the RKHS, from offline meta-data. Our method, Meta-Kernel Learning (META-KEL), works with a generic (i.e., not necessarily i.i.d.) data model and may be applied to a variety of sequential decision-making tasks.</p>
<p>We formally analyze the problem when the true kernel is a combination of known base kernels. We prove that the solution to META-KEL corresponds to an RKHS which contains the true function space (Theorem 4.3). Further, the metalearned kernel has a sparse structure (Proposition 4.4) which reduces the variance of the resulting learner, and makes the learner more efficient for solving the downstream sequential decision-making problem. With mild assumptions on the data, we show that the meta-learned kernel yields anytime valid confidence bands matching the ones given oracle knowledge of the true kernel, as more samples of the metadata are provided (Theorem 5.1). These provably reliable confidence estimates constitute the key contribution of our work and distinguishes META-KEL from prior attempts.</p>
<p>To demonstrate how META-KEL can be applied to a sequential task, we analyze a Bayesian optimization algorithm when it uses the meta-learned kernel and compare it to the same algorithm when it has knowledge of the true kernel, i.e., the oracle algorithm. By increasing size of the metalearning data, the regret bound we obtain approaches the rate of the oracle (Corollary 5.2).</p>
<p>Contributions Our main contributions are:</p>
<p>• We introduce META-KEL, a method for meta-learning the hypothesis space of a sequential decision task, which yields provably valid adaptive confidence sets.</p>
<p>• Our meta-learnt confidence bounds converge to the ones estimated by the oracle at a O(1/ √ mn) rate.</p>
<p>Here, m is the number of tasks in the meta-data and n is the number of samples per task.</p>
<p>• Applied to BO, our results imply a sublinear regret guarantee for the GP-UCB algorithm using our metalearned kernel. This bound approaches that of the oracle algorithm as the amount of meta-data increases.</p>
<p>Related Work</p>
<p>Numerous sequential decision-making methods rely on confidence sets for uncertainty quantification, e.g., UCB algorithms (Srinivas et al., 2010;Chowdhury &amp; Gopalan, 2017) for Bayesian optimization and bandits, safe exploration and various forms of RL (Berkenkamp et al., 2017;Curi et al., 2020;Kakade et al., 2020;Sessa et al., 2020). Most of these methods assume the true hypothesis space as given. However, in practice, we typically do not know the correct hypothesis space, e.g., in form of a kernel. A body of recent work considers the unknown kernel setting and analyzes the effect of working with a misspecified hypothesis space (Wynne et al., 2021;Simchowitz et al., 2021;Bogunovic &amp; Krause, 2021). Alternatively, Wang &amp; de Freitas (2014) and Berkenkamp et al. (2019) propose to successively expand the hypothesis space throughout the course of BO so that the algorithm remains no-regret in a setting where the kernel lengthscale is unknown.</p>
<p>Our work relates to meta-learning for Bayesian optimization. There is a recent line of algorithms that improve accuracy of base sequential learners via meta-learning, albeit without theoretical guarantees (Rothfuss et al., 2021a;b), or with mild guarantees in special cases (Kveton et al., 2020;Boutilier et al., 2020). There are a number of results on updating bandit priors or policies by meta-learning, under problem settings different than ours. Basu et al. (2021) work with a sequence of multi-armed bandit tasks, and adaptively meta-learn the mean of the Gaussian prior used for the next task. Others consider solving a number of structurally similar linear bandit tasks in parallel (Wang et al., 2017;Cella et al., 2020;Cella &amp; Pontil, 2021 (2020) and Hao et al. (2020) give dimension-independent regret bounds for Explore-Then-Commit algorithms under certain assumptions over the action set. This work does not consider offline data.</p>
<p>We draw inspiration from the early Multiple Kernel Learning (MKL) literature, which focuses on kernel design for classification with SVMs (Bach et al., 2004;Gönen &amp; Alpaydın, 2011;Kloft et al., 2011;Evgeniou &amp; Pontil, 2004;Cristianini et al., 2006;Ong et al., 2005). In contrast, our key contribution is to derive adaptive confidence bounds from meta-learnt kernels for regression, even for non-i.i.d. data. Orthogonal to most prior works, we reduce the kernel learning problem to group Lasso and leverage the properties of the Lasso estimator, in particular seminal results of Lounici et al. (2011) andBach (2008 </p>
<p>Problem Statement</p>
<p>Consider a sequential decision-making problem, where the agent repeatedly interacts with an environment and makes observations
y t = f (x t ) + ε t(1)
of an unknown function f : X → R residing in an RKHS H k * that corresponds to an unknown kernel function k * . 1 We further assume that the function has a bounded kernel norm f k * ≤ B and that the domain X ⊂ R d0 is compact. The observation noise ε t are i.i.d. samples from a zero-mean sub-Gaussian distribution with variance proxy σ 2 . At every step t, the chosen input x t only depends on the history up to step t, denoted by the random
sequence H t−1 = {(x τ , y τ ) : 1 ≤ τ ≤ t − 1}.
No further assumptions are made about the algorithm or the policy for choosing x t . Depending on the application, Equation (1) can serve different purposes: It can describe the stochastic reward model of a bandit problem, or it may be the transition dynamics of an RL agent in a stochastic environment.</p>
<p>For solving such problems, a central prerequisite for numerous algorithms are confidence sets for f (x) based on the history H t−1 to balance exploration and exploitation at any step t. For any x ∈ X , the set C t−1 (x) defines an interval to which f (x) belongs with high probability such that,
P (∀x ∈ X : f (x) ∈ C t−1 (x)) ≥ 1 − δ.
The center of this interval reflects the current knowledge of the agent, relevant for exploitation, and the width corresponds to the uncertainty, guiding further exploration. When the true kernel is known, an approach commonly used in the kernelized bandit literature (Abbasi-Yadkori et al., 2011;Srinivas et al., 2010;Russo &amp; Van Roy, 2014) is to build sets of the form
C t−1 (k; x) = [µ t−1 (k; x) − ν t σ t−1 (k; x), (2) µ t−1 (k; x) + ν t σ t−1 (k; x)]
where the exploration coefficient ν t depends on the desired confidence level 1 − δ, and may be set based on the objective of the decision-making task. The functions µ t−1 and σ t−1 set the center and width of the confidence set as
µ t−1 (k; x) = k T t−1 (x)(K t−1 +σ 2 I) −1 y t−1 (3) σ 2 t−1 (k; x) = k(x, x) − k T t−1 (x)(K t−1 +σ 2 I) −1 k t−1 (x) whereσ is a constant, y t−1 = [y τ ] τ &lt;t is the vector of observed values, k t−1 (x) = [k(x, x τ )] τ &lt;t , and K t−1 = [k(x i , x j )] i,j&lt;t is the kernel matrix.
Hence working with the right kernel function plays an integral role in constructing well-specified sets. Since, in practice, the true kernel k * is not known by the learner, most approaches use a hand-designed kernel that suits the problem instance at hand or conservatively pick an expressive kernel that constructs a rich RKHS which is very likely to contain f . There are a number of empirical approaches for selecting the kernel, e.g., cross-validation or maximizing the marginal likelihood. However, such methods tend to be unreliable 1 Appendix A.1 presents a compact refresher on the RKHS. when the available data is non-i.i.d. and comes from sequential learning tasks.</p>
<p>Addressing the issue of selecting a correct and yet efficient kernel, we pursue a data-driven approach and meta-learn a kernel that provably yields valid confidence intervals. This guarantee is valid regardless of how the meta-data is gathered, as long as it satisfies some basic conditions discussed later in Assumptions 3.1 and 4.2. We consider an offline collection of datasets D n,m = {(x s,i , y s,i ) i≤n } s≤m from m possibly non-i.i.d. tasks, each with a sample size n. Suppose, for each task s, labels are generated by
y s,i = f s (x s,i ) + ε s,i(4)
for i ≤ n, where ε s,i are zero-mean i.i.d. sub-Gaussian noise with variance proxy σ 2 . We assume the tasks are related by the fact that all f s : X → R come from the same function class H k * and have a bounded RKHS norm f s k * ≤ B.</p>
<p>We do not make any assumptions on the policy based on which the points x s,i are chosen.</p>
<p>Assumptions Our analysis requires some assumptions on the kernel function. In particular, we assume that k * is a finite combination of known base kernels,
k * (x, x ) = p j=1 η * j k j (x, x ),(5)
where the weight vector η * ≥ 0 is unknown. Without loss of generality, we assume that k * and the base kernels are all normalized, i.e., η * 1 ≤ 1 and k j (x, x ) ≤ 1 for all 1 ≤ j ≤ p and x, x ∈ X . The weight vector η * is potentially sparse, since not all the candidate kernels k j actively contribute to the construction of k * . We use J k * = {1 ≤ j ≤ p : η * j = 0} to refer to the group of base kernels that are present in k * . The sparse construction of k * imposes favorable structure on the meta-data, which essentially allows us to meta-model-select the hypothesis space and recover the true sparsity pattern denoted by J k * . We further assume that each k j has a d j -dimensional feature map, i.e., k j (x,
x ) = φ T j (x)φ j (x ), where φ j ∈ R dj .
For the scope of this paper, we assume that d max &lt; ∞, where d max := max j≤p d j . In this finite regime, the analysis can also be carried out in a finite-dimensional vector space. Nevertheless, we use a function space notation since, even though our theory focuses on the finite-dimensional setting, empirically our approach is also applicable to kernels with infinite dimensional feature map. 2 Let φ(x) denote the d-dimensional feature map for k * where d = p j=1 d j and
φ(x) = η * 1 φ T 1 (x), · · · , η * p φ T p (x) T .
For each task s, the function f s is contained in H k * . By Mercer's theorem f s may be decomposed as
f s (x) = φ T (x)β * s = p j=1 η * j φ T j (x)β * s (j) ,(6)
where β * s ∈ R d is the coefficients vector of task s and β * s (j) ∈ R dj is the sub-vector corresponding to kernel k j . It is not possible to meta-select a base kernel k j which has not contributed to the generation of the meta-data. Therefore, if a base kernel is active in the construction of H k * , it is only natural to assume that there is some task in the meta-data which reflects this presence. More formally, we assume that, for any j ∈ J k * , there exists some s ≤ m where β * s (j) = 0. We define β * = (β * 1 T , · · · , β * m T ) T ∈ R md as the concatenated coefficients vector for all tasks. To refer to the group of coefficients that correspond to kernel k j across all tasks, we use β * Table 1 presents a compact guide to the notation introduced here. Our next assumption guarantees that the meta-learning problem is not ill-posed.
(j) = ((β * 1 (j) ) T , · · · , (β * m (j) ) T ) T ∈ R mdj .
Assumption 3.1 (Group Beta-min Condition). There exists
c 1 &gt; 0 s.t. for all j ∈ J k * it holds that β * (j) 2 ≥ c 1 .
This assumption is inevitable for recovering the sparsity pattern from empirical data and it is widely used in the high-dimensional statistics literature (e.g., Bühlmann &amp; Van De Geer, 2011;Zhao &amp; Yu, 2006;Van de Geer et al., 2011). Assumption 3.1 implies that for j to be in J k * , the coefficients vector corresponding to kernel k j can not be zero or arbitrarily close to zero. In practice, β * (j) 2 has to be comparable with the noise level for the activity of a base kernel not to be mistaken with randomness.</p>
<p>Meta-learning the Hypothesis Space (META-KEL)</p>
<p>In the following section, we present our formulation of the meta-learning problem and analyze the properties of the learned hypothesis space. We meta-learn the kernel by solving the following optimization problem. Then, we set the hypothesis space of the downstream learning algorithm to be the RKHS of the meta-learned kernel. 
n i=1 (y s,i − f s (x s,i )) 2 + λ 2 m s=1 f s 2 k + λ 2 η 1 s.t. ∀s : f s ∈ H k , k = p j=1 η j k j , 0 ≤ η(7)
We will refer to this problem as Meta-Kernel Learning (META-KEL). The first part of the objective is similar to the kernel ridge regression loss, and accounts for how well a series of regularized f s fit the meta-data. The last term regularizes our choice of the kernel function. We use 1norm regularization for η to implicitly perform meta-modelselection. As shown in Proposition 4.4, the meta-learned kernel will reflect the sparsity pattern of the true kernel. The optimization problem (7) is convex and admits an efficient solution, as explained next.</p>
<p>We first introduce a vectorized formulation of Equation (7). Let y s ∈ R n denote the observed values for a task s and y = (y T 1 , · · · , y T m ) T ∈ R mn the multi-task stacked vector of observations. We then design a multi-task feature matrix. We define Φ to be a mn × md blockdiagonal matrix, where each block s corresponds to Φ s = (φ(x s,1 ), · · · , φ(x s,n )) T , the n × d feature matrix of task s. Figure 7 provides an illustration thereof. As shown in Proposition 4.1, this vectorized design brings forth a parametric equivalent of META-KEL, which happens to be the wellknown Group Lasso problem. Proposition 4.1 (Solution of META-KEL). Let k = jη j k j be a solution to Problem (7). Then, for all 1 ≤ j ≤ p, it holds that η j = β (j) 2 withβ = (β (j) ) j≤p as the solution of the following convex optimization problem:
min β 1 mn y − Φβ 2 2 + λ p j=1 β (j) 2 .(8)
We show this equivalence by eliminating η. We use a trick introduced by Bach et al. (2004), which, for w, v ∈ R states 2|w| = min v≥0 w 2 /v + v. The proof is given in Appendix A.2. Problem (8) can be optimized by any Group Lasso solver. Bach et al. (2012) present a number of coordinate descent algorithms which efficiently find the solution.</p>
<p>Before introducing the meta-learned kernelk, we note that Reproducing Kernel Hilbert Spaces are equivalent up to scaling of the kernel function. For c &gt; 0, both H k and the scaled version H ck contain the same set of functions. Going from H k to H ck , the RKHS norm of any member f would scale by 1/c, i.e. f k = c f ck . Hence, the norm η 1 will be irrelevant when meta-learning the function space. This norm can be scaled or normalized, and still yield the same hypothesis space, only with a scaled operator norm.</p>
<p>For consistency of notation, we definek as follows. For any two points x, x ∈ X , set
k(x, x ) = p j=1η j c 1 φ T j (x)φ j (x ),(9)
where c 1 is the same constant as in Assumption 3.1. We emphasize that this scaling does not impose a new assumption on the problem as it is done only to simplify theorem statements. We denote the set of base kernels active ink with Jk = {1 ≤ j ≤ p :η j = 0}. The meta-learned hypothesis space will then be Hk, the RKHS which corresponds tok.</p>
<p>Properties of the Meta-learned Hypothesis Space</p>
<p>The meta-learnedk can be used as the kernel function for a model-based sequential decision making algorithm, as illustrated in Figure 1. Our goal is to analyze how this choice of kernel affects the success of the algorithm, compared to the oracle algorithm with access to the unknown kernel. To this end, we discuss properties ofk.</p>
<p>For our main result, we require a final technical assumption to ensure that our meta-data is sufficiently diverse. Consider some vector b ∈ R md that adheres to the same group structure as β * . For a set of group indices J ⊂ {1, · · · , p}, we define b J := (b (j) ) j∈J as the sub-vector indicated by the groups in J.
Assumption 4.2 (Relaxed Sufficient Exploration). There exists κ = κ(s) &gt; 0 that satisfies κ ≤ min J,b Φb 2 √ mn b J 2 ,s.t. j / ∈J b (j) 2 ≤ 3 j∈J b (j) 2 , b = 0, |J| ≤ s.
This condition makes sure that the meta-training data is not degenerate, e.g., no two points x s,i and x s,j in the meta-data are identical or too close. Assumption 4.2 is immediately fulfilled if the minimum eigenvalue of Φ is positive, i.e., if there exists some κ &gt; 0 where λ min (Φ) ≥ κ. This stronger version is sometimes referred to as Sufficient Exploration (Basu et al., 2021;Zhou et al., 2020 
λ ≥ 4σ √ mn 1 + 2 m log(2p/δ) + md max log(2p/δ) .
If |J k * | ≤ s and Assumption 4.2 holds with κ(s), then H k * ⊆ Hk and
f k ≤ f k * 1 + (n, m) + o (n, m) ,
with probability greater than 1 − δ, if n and m are large enough to satisfy (n, m) ≤ c 1 . The absolute constant c 1 is defined in Assumption 3.1 and
(n, m) := 32σs κ 2 (s) √ mn × 1 + 2 m log(2p/δ) + md max log(2p/δ) .
The proof is given in Appendix B.1. Theorem 4.3 states that, provided enough meta-data, H k * is contained in Hk with high probability and δ, the probability of failure in recovery, decreases as m and n grow (See Appendix B.3). In addition the RKHS norm of any f ∈ H k * , can be bounded arbitrary well by f k , since the error (n, m) decreases at a O(s/ √ mn 1 + m −1 log p) rate. This matches the tightest rate for coordinate-wise convergence of the Group Lasso estimator under the same set of assumptions (Lounici et al., 2011;Bunea et al., 2013). The theorem implies that the meta-learner benefits more from increasing the number m of meta-data tasks, rather than increasing the sample size n of each task, since (n, m) shrinks faster with m compared to n. Note that increasing either m or n will result in convergence and therefore this theorem also holds for the classic offline kernel learning setup when the dataset consists of a single learning task (m = 1).</p>
<p>The Benefit of Structural Sparsity Consider a conservative hand-picked kernel function
k full = 1/p p j=1 φ T j (x)φ j (x ),(10)
which does not use any meta-data and instead incorporates all the considered base kernels. When p and d max are finite, H k * is contained in H k full and the hand-picked hypothesis space is not misspecified. However, working with an overly large hypothesis space has downsides. Consider using k full to estimate a function f ∈ H k * . Then every base kernel, including k j with j / ∈ J k * , appears in the construction of the estimator. These terms contribute to the estimation error and increase the variance of the function estimate. This slows down the rate of convergence, compared to the case where only active k j are present in the kernel function. By meta-learningk via META-KEL, we can eliminate irrelevant candidate kernels and produce a structurally sparse hypothesis space. Proposition 4.4 guarantees this property. Its proof is given in Appendix B.2.</p>
<p>Proposition 4.4 (Bound on structural sparsity ofk). Set 0 &lt; δ &lt; 1 and choose λ according to Theorem 4.3. Let |J k * | ≤ s be the number candidate kernels that contribute to k * . If Assumption 4.2 holds with κ(s), then with probability greater than 1 − δ, the number of kernels active ink is bounded by
|Jk| ≤ 64s mnκ 2 (s) which implies that if mn &gt; 64s
pκ 2 (s) , then with the same probability</p>
<p>Hk H k full .</p>
<p>Hence, in the presence of enough meta-data, Hk is a strict subset of H k full , and therefore
H k * w.h.p. ⊆ Hk w.h.p. H k full
where the left relation is due to Theorem 4.3. Figure 2 illustrates the nested sets. We conclude that our meta-learned hypothesis space has favorable properties: it contains the true hypothesis space, and it is sparse in structure, in particular, smaller than the conservative candidate space.</p>
<p>The fact that Hk is smaller than H k full reduces the complexity of the downstream learning problem and yields faster convergence rates. We provide an example of this effect in Section 5, where we analyze a Bayesian optimization problem, and establish how choosingk improves upon k full . Finally, our experiments (e.g. Figure 4) support the claim that in practice the BO algorithm is faster in finding the optimum when it uses the meta-learned kernel.</p>
<p>H k * Hk H kfull Figure 2. The oracle H k * (Eq. 5), the meta-learned Hk (Eq. 9) and the hand-picked H k full (Eq. 10) hypothesis spaces (informal)</p>
<p>Sequential Decision-making with META-KEL</p>
<p>We now analyze the effect of usingk as kernel function in the downstream sequential decision-making problem. We adopt the common construction of confidence sets given in Equation (2), and defineĈ t−1 (x) :
= C t−1 (k; x). We letμ t−1 (x) := µ t−1 (k; x), andσ t−1 (x) := σ t−1 (k; x),
where µ t−1 (k; x) and σ t−1 (k; x) are as defined in Equation (3), with time-varyingσ 2 = 1 + 2/t. 3 3 The functionsμt−1 andσt−1 are the posterior mean and variance of GP(0,k), conditioned on Ht−1, with noise varianceσ 2 .</p>
<p>Theorem 5.1 shows that for the right choice of ν t , the set C t−1 (x) is a valid confidence bound for any f ∈ H k * , evaluated at any x ∈ X , at any step t, with high probability.
Theorem 5.1 (Any-time Valid Confidence Bounds with META-KEL). Let f ∈ H k * with f k * ≤ B, where k * is unknown.
Under the assumptions of Theorem 4.3, with probability greater than 1 − δ, for all x ∈ X and all t ≥ 1,
|μ t−1 (x)−f (x)| ≤σ t−1 (x) B 1 + (n, m) 2c 1 + σ d log 1 +σ −2 t c 1 + 2 + 2 log(1/δ) whered = j∈Jk d j andσ 2 = 1 + 2/t.
The proof is given in Appendix C. As discussed in Section 4, the (n, m)/2c 1 term shrinks faster than O(1/ √ mn) and</p>
<p>d approaches d * = j∈J k * d j at a similar rate. Therefore, Theorem 5.1 presents a tight confidence bound relative to the case when k * is known by the agent. In this case, due to Chowdhury &amp; Gopalan (2017), Theorem 2, the 1 − δ confidence bound would be,
|µ t−1 (x)−f (x)| ≤ σ t−1 (x) B + σ d * log 1 +σ −2 t + 2 + 2 log(1/δ)) .
where the mean and variance functions are defined by µ t−1 (x) := µ t−1 (k * ; x) and σ t−1 (x) := σ t−1 (k * ; x) with σ 2 = 1 + 2/t. We conclude that the base learner does not require knowledge of the true kernel for constructing confidence sets, as long as there is sufficient meta-data available. Theorem 4.3 quantifies this notion of sufficiency.</p>
<p>Case Study: Bayesian Optimization As an example application, we consider the classic Bayesian optimization problem, but in the case where H k * is unknown. This example illustrates how Theorem 5.1 may be used to prove guarantees for a decision-making algorithm, which uses the meta-learned kernel due to a lack of knowledge of k * . We follow the setup and BO notation of Srinivas et al. (2010). The agent seeks to maximize an unknown reward function f , sequentially accessed as described in Equation (1). Their goal is to choose actions x t which maximize the cumulative reward achieved over T time steps. This is equivalent to minimizing the cumulative regret
R T = T t=1 [f (x * ) − f (x t )], where x * is a global maximum of f . Note that if R T /T → 0 as T → ∞ then max 1≤t≤T f (x t ) → f (x * ),
i.e., the learner converges to the optimal value. We will refer to this property as sublinearity of the regret. In the spirit of the GP-UCB algorithm (Srinivas et al., 2010), we choose the next point by maximizing the upper confidence bound as determined by Theorem 5.1
x t = arg max x∈Xμ t−1 (x) + ν tσt−1 (x)(11)
where a suitable choice for ν t is suggested in Corollary 5.2.</p>
<p>Corollary 5.2 (A Regret Bound with META-KEL). Let δ ∈ (0, 1). Suppose f ∈ H k * with f k * ≤ B and that values of f are observed with zero-mean sub-Gaussian noise of variance proxy σ 2 . Then, with probability greater than 1 − δ, GP-UCB used together withk satisfies
R T = O d T log T B 1 + (n, m) + d log T + log 1/δ
provided that the exploration coefficient is set to
ν t =B 1 + (n, m)/2c 1 + σ d log (1 +σ −2 t/c 1 ) + 2 + 2 log(1/δ).
The proof is straightforward. Conditioned on the event that f ∈ Hk, we may directly use the regret bound of Chowdhury &amp; Gopalan (2017). Then, by Theorem 4.3, we calculate the probability of this event (Appendix C.1). The Corollary relies on knowledge of a bound B on f k * . However, using techniques of Berkenkamp et al. (2019) it is possible to adapt it even to unknown B at increased (but still sublinear) regret.</p>
<p>Corollary 5.2 shows that GP-UCB using the metalearned kernel guarantees sublinear regret. We obtain a O(dB log T √ T ) rate for the regret which is tight compared to the O(d * B log T √ T ) rate satisfied by the oracle. It is insightful to compare this convergence result to a scenario where the hypothesis space is misspecified. For a reward function f / ∈ Hk, Bogunovic &amp; Krause (2021) show that the learner will not converge to the global optimum, since the cumulative regret has a lower bound of linear order O(T √ log T ). Corollary 5.2 suggests that by using a sparse kernel we can potentially find the optimal policy faster compared to when the complex kernel k full is used. Recall that d = p j=1 d j , by Theorem 2 of Chowdhury &amp; Gopalan (2017) the regret of GP-UCB used together with k full is bounded by O(dpB log T √ T ), since f k full = p f k * . Therefore, using the meta-learned kernel improves the regret bound by a factor ofd/(dp), implying that the solution may be found faster. The results of our experiments in Figure 4 support this argument.</p>
<p>Note that our approach to guarantee a sublinear regret for GP-UCB without oracle knowledge of k * naturally generalizes to other sequential decision tasks. In particular, any theoretical result relying on RKHS confidence intervals with a known kernel can be immediately extended to use those of the meta-learned kernel.  </p>
<p>Experiments</p>
<p>In this section, we provide experiments to quantitatively illustrate our theoretical contribution.</p>
<p>Experiment Setup (1D) We create a synthetic dataset based on our data model, Equations (1) and (4). We first limit the domain to the 1-dimensional X = [−1, 1] and use Legendre polynomials P j as our features φ j . The sequence (P j ) j≥0 is a natural choice, since it provides an orthonormal basis for L 2 (X ). Moreover, Legendre polynomials are eigenfunctions to dot-product kernels such as the Neural Tangent Kernel (Jacot et al., 2018). We let k * (x, x ) = j∈J k * η * j P j (x)P j (x ), where J k * is a random subset of {1, · · · , p}. Each η * j is sampled independently from the standard uniform distribution and the vec- tor η * is then normalized. Across all experiments, we set p = 20 and s = |J k * | = 5. To sample the meta-data D n,m , we choose m independent random subsets of J k * and generate the functions f s via Equation (6) where β * (j) are drawn from an i.i.d. standard uniform distribution. We then scale the norm f k * to B = 10. The data for a single task, i.e., (x s,i , y s,i ) i≤n , is then created by uniformly drawing i.i.d. samples from the domain X and evaluating f s at those points. We add Gaussian noise with standard deviation σ = 0.01 to all data points. Figure 8 in the appendix shows how random f s may look like. For all experiments we set n = m = 50 unless stated otherwise. To meta-learnk, we solve the vectorized META-KEL problem (Eq. 8) over β (j) with CELER, a fast solver for the group Lasso (Massias et al., 2018), and then setη according to Proposition 4.1. We set λ = 0.03, such that it satisfies the condition of Theorem 4.3. As shown in Figure 10 in the appendix, the choice of λ has little effect on the performance of the algorithm.</p>
<p>Confidence Set Experiment</p>
<p>We perform calibration and sharpness experiments to assess the meta-learned confidence sets (Gneiting et al., 2007). Figure 3 presents the result. To obtain an α-confidence interval for some f (x) using a kernel k, we assume a f ∼ GP(0, k) prior and calculate the α-quantile of the posterior after observing 4 noisy i.i.d. draws from the function. For each hypothesis, the y-axis of the left plot shows the empirical coverage of the confidence sets, i.e., the fraction of test points contained in the α-confidence intervals for varying levels α. In this plot, if a curve were to fall below the x = y line, it would have implied insufficient coverage and hence over-confident sets. The plot on the right shows the posterior variance averaged across all test points. This quantity, referred to as sharpness, reflects the width of the confidence bands. Figure 3 demonstrates that the meta-learned confidence sets are well-calibrated for the entire range of confidence-levels and are tight relative to the true sets. In contrast, k full yields conservative confidence sets, due to considering polynomials P j that do not contribute to the construction of f (x). We use 1000 test points for calculating the empirical averages. The plot shows the values averaged over 50 runs, where for each the kernel k * and the data are generated from scratch.</p>
<p>Regret Experiment</p>
<p>We verify the performance of GP-UCB when used together withk. We generate the random reward function f in a manner similar to f s of the meta-data. The BO problem is simulated according to Equation (1), and the actions are selected via Equation (11). Figure 9 in the appendix shows how this algorithm samples the domain and how the confidence estimates shrink by observing new samples. Keeping the underlying random k * fixed, we generate 100 random instances of the meta-learning and the BO problem. In Figure 4 we present the average regret and its standard deviation. In these plots, the simple regret of GP-UCB with a kernel k is labeled r t
(k) = f (x * ) − max τ ≤t f (x τ ). Respectively, the cumulative inference regret is R t (k) = τ ≤t f (x * ) − max x µ τ −1 (x).
The algorithm converges to the optimum using all three kernels. The meta-learned kernel, however, improves upon using k full and results in a performance competitive to when k * is known by GP-UCB. This behavior empirically confirms Corollary 5.2.</p>
<p>Consistency Experiment From Corollary 5.2 we concluded that, as the size of the meta-data grows, the regret bound achieved viak converges to the oracle bound, i.e., the bound satisfied by the learner when it has knowledge of the true kernel. As Figure 5 shows, this consistency is also reflected in the empirical regret values. As we increase m, the number of offline tasks given to the meta-learner, the cumulative inference regret at T = 100 improves. It approaches the regret obtained by the oracle algorithm. The value of λ does not affect this convergence, as long as it satisfies Theorem 4.3. Similar to Figure 4, this plot is generated for a fixed random k * , averaged over 50 random instances of the meta-learning and BO problem.</p>
<p>Regret Experiment for 2D domain We repeat the regret experiment, with synthetic data over the 2-dimensional domain X = [−1, 1] 2 . For x = (x 1 , x 2 ) ∈ X , we define the Legendre feature map as φ(x) = (P j (x 1 )P p−j (x 2 )) 0≤j≤p . This feature map is (p + 1)-dimensional, and has terms of degree at most p. We use the polynomial P j (x 1 )P p−j (x 2 ) as the feature φ j (x) and create a synthetic dataset in a fash-ion identical to the 1-dimensional case, again setting p = 20 and s = 5. Figure 6 shows the 2-dimensional counterpart of Figure 4. Again, usingk results in competitive performance to the oracle algorithm with knowledge of k * . Here, we also use GP-UCB together with the infinite-dimensional Squared Exponential (SE) kernel. The regret curve shows that we do not benefit from choosing a complex kernel for solving an inherently low dimensional problem.</p>
<p>Efficient Hyper-parameter Tuning with META-KEL A common application of GP-UCB is in optimizing hyper-parameters of machine learning algorithms. In this setting, X is the algorithm's hyper-parameter space, and f represents the test performance of the algorithm. Evaluating each hyper-parameter configuration is costly, and thus the BO method has to be sample efficient. We consider the GLMNET algorithm (Friedman et al., 2010) and empirically demonstrate that by meta-learning the kernel, we gather knowledge from prior data, and in turn, tune the hyper-parameters of a new task more efficiently. Mainly, by running GP-UCB withk we tend to find the optimal configuration of hyper-parameters for an unseen learning task faster, compared to using data-independent kernels ( Figure 6). The OpenML platform (Bischl et al., 2017) enables access to data from hyper-parameter tuning of GLMNET on 38 different classification tasks. We split these datasets into a meta-dataset with m = 25 and leave the rest as test tasks. For meta-learning the kernel, we use 500 Random Fourier Features (Rahimi et al., 2007) defined on a 2-dimensional domain, since the GLMNET algorithm has only two hyper-parameters. The remaining details of our experiment setup is given in Appendix D. Figure 6 shows the performance of GP-UCB on the test task. Utilizingk results in a sample-efficient GP-UCB that rapidly approaches an optimal choice of hyper-parameters. This is in contrast to running GP-UCB with the SE kernel, which takes about 50 iterations to find a good configuration.</p>
<p>Conclusion</p>
<p>We proposed META-KEL, a method for reliably learning kernel functions from offline meta-data. As a first, our approach yields provably valid meta-learned adaptive confidence sets, setting it apart from existing meta-learning approaches. Importantly, we showed that our meta-learned kernel produces tight and consistent confidence bounds for the target function, provided that enough meta-data is available. As an example application, we showed that GP-UCB still yields sublinear regret when using the meta-learned kernel, performing competitively in theory and experiments with the oracle that has access to the unknown kernel. We believe this result opens up avenues towards giving convergence guarantees for other sequential decision algorithms without oracle knowledge of the hypothesis space.  Wang, Z., Kim, B., and Kaelbling, L. P. Regret bounds for meta bayesian optimization with an unknown gaussian process prior. arXiv preprint arXiv:1811.09558, 2018b. </p>
<p>Meta-Learning Hypothesis Spaces for Sequential Decision-making: Supplementary Material</p>
<p>A. Details of the main Result symbol description k * true (unknown) kernel function k meta-learned kernel p total number of candidate base kernels k j base kernels that construct k * andk, 1 ≤ j ≤ p η * j coefficient of k j in construction of k * , i.e., k * (·, ·) = p j=1 η * j k j (·, ·) η j coefficient of k j in construction ofk, i.e.,k(·, ·) = p j=1η j k j (·, ·) η * the vector η * 1 , · · · , η * j , · · · , η * p ∈ R p η the vector (η 1 , · · · ,η j , · · · ,η p )
∈ R p J k * the set {1 ≤ j ≤ p, s.t. η * j = 0} Jk the set {1 ≤ j ≤ p, s.t.η j = 0} φ j (·) feature map for k j , i.e., k j (x, x ) = φ T j (x)φ j (x ) φ(·) η * 1 φ T 1 (x), · · · , η * p φ T p (x) T , i.e., feature map for k * d 0 dimension of the input domain, X ⊂ R d0 d j dimension of a feature map φ j (x) ∈ R dj d max max 1≤j≤p d j d p j=1 d j d * j∈J k * d ĵ d j∈Jk d j n
number of samples in each task s in the meta-data m number of tasks in the meta-data f s target function from task s β * s true coefficients vector for task s:
f s (x) = φ T (x)β * s , in R d β s estimate of β * s β * s (j) sub-vector of β * s corresponding to kernel k j , in R dĵ β s (j) estimate of β * s (j) β * (β * 1 T , · · · , β * m T ) T , in R md β estimate of β * β * (j)
sub-vector of β * corresponding to kernel k j , in R mdĵ β (j) estimate of β * (j) </p>
<p>A.1. RKHS Refresher</p>
<p>Here we present a compact reminder of RKHS basics for the sake of completeness and clarifying our notation. We work in a finite-dimensional regime which can also be described by a euclidean vector space. Nevertheless, we use the RKHS notation as it gives a powerful framework and hides away the vector algebra. This section is mainly based on Wainwright (2019). For a positive semi-definite kernel function k over some set X × X , the corresponding unique Reproducing Kernel Hilbert
      y s       = p j=1        φ T j (x s,1 ) . . . φ T j (x s,n )              β (j) s       +       s       y1 . . . ys . . . ym                     Φ1 0 Φs 0 Φm                 β1 . . . βs . . . βm                     1 . . . s . . . m                     = + β (1) 1 . . . β (j) 1 . . . β (p) 1 . . . β (1) s . . . β (j) s . . . β (p) s . . . β (1) m . . . β (j) m . . . β (p) m                                                                           β (j) 1 . . . β (j) s . . . β (j) m                 β = β (j) = Figure 7
. Visual guide for the Group Lasso formulation. The red shade shows the group of coefficients which correspond to the effect of kernel kj. The green shade demonstrates how features from each task come together on the diagonal of the multi-task feature matrix. The coefficients that belong to one task are group together with a green rectangle.</p>
<p>Space can be constructed as,
H k = f : X → R | f (·) = n i=1 α i k(x i , ·), n ∈ N, (x i ) n i=1 ∈ X , α ∈ R n ,
equipped with the dot product, f,f k = i,j α iᾱj k(x i ,x j ). We limit X to compact sets, and only consider Mercer kernels, i.e., continuous kernel functions that satisfy the Hilbert-Schmidt condition,
X ×X k 2 (x, x )dµ(x)dµ(x ) &lt; ∞
where µ is a non-negative measure over X . Mercer's theorem states that under these assumptions, the kernel operator has a sequence of orthonormal eigenfunctions (φ r ) r≥1 and non-negative eigenvalues (η r ) r≥1 , defined as follows
X k(x, x )φ r (x )dµ(x ) = η r φ r (x). (A.1)
Moreover, k can be written as their linear combination,
k(x, x ) = r η r φ r (x)φ r (x ).
Or as a non-negative combination of base kernels, k r (x,
x ) = φ r (x)φ r (x ) k(x, x ) = r η r k r (x, x ).
It immediately follows that the unique RKHS corresponding to k takes the form,
H k =    f : X → R | f (·) = r≥1 β r φ r (·), r:ηr =0 β 2 r η r &lt; ∞  
 and the inner product the following form,
f, g k = r:ηr =0 f, φ r 2 g, φ r 2 η r ,
where ·, · 2 denotes the inner product in the L 2 (X ). It is then implied that f k = r:ηr =0 β 2 r /η r . Lastly, we define φ(x) = √ η r φ r (x) r≥1 to be the feature map corresponding to k. In this paper we refer to the number of non-zero eigen-values as the dimension of the kernel. Note that under this convention, most kernel functions used in practice, e.g. RBF kernel or the Matérn family, are infinite-dimensional.</p>
<p>A.2. Proof of Proposition 4.1</p>
<p>By Equation (6) we may write a parametric equivalent of Problem (7) in terms of the feature maps φ j , min 0≤η, ∀s:βs
1 m m s=1    1 n n i=1   y s,i − p j=1 √ η j φ T j (x s,i )β (j) s   2    + λ 2 m s=1 p j=1 β (j) s 2 2 + λ 2 η 1 . (A.2)
This problem is jointly convex in η and (β s ) s≤m , and has an optimal solution (Kloft et al., 2011). Renaming the variable β
(j) s ← β (j)
s / √ η j gives the following equivalent problem, min 0≤η, ∀s:βs
1 m m s=1    1 n n i=1   y s,i − p j=1 φ T j (x s,i )β (j) s   2    + λ 2 p j=1 m s=1 β (j) s 2 2 η j + λ 2 η 1 .
There is no constraint connecting the two variables, and renaming β (j) s does not effect the optimization problem with respect to η. Therefore,η is also an optima for this problem. Let (η,β) denote the solution to the problem above. We show that η has a closed form expression in terms ofβ. This observation allows us to reduce the joint optimization problem to an equivalent problem which is only over (β s ). We use the η-trick introduced in Bach et al. (2004). The authors observe that for any two scalar variables w and v,
|w| = min v≥0 w 2 2v + v 2 ,
andv = |w|. Applying this trick with w = β (j) 2 and v = η j for all j ≤ p gives
min η≥0 λ 2 p j=1 β (j) 2 2 η j + λ 2 η 1 = λ p j=1 β (j) 2 ,
which results the following equivalent problem
min ∀s:βs 1 m m s=1    1 n n i=1   y s,i − p j=1 φ T j (x s,i )β (j) s   2    + λ p j=1 m s=1 β (j) s 2 2 . (A.3)
Note that By definition of β (j) , the second term satisfies Notations and naming conventions When X is a matrix, X 2 and X F denote its spectral and Frobenius norm, respectively. Consider the multi-task coefficients vector β ∈ R md , and the sub-vector β (j) ∈ R mdj which denotes the coefficients corresponding to kernel k j . Through out this proof, we use the convention "group j" to refer to the set of indices of β which indicate β (j) . Similarly, we let Φ (j) denote the mn × md j sub-matrix which only has the features coming from group j. Lastly, let Ψ := Φ T Φ/mn, then Ψ (j) = (Φ (j) ) T Φ (j) /mn indicates the md j × md j submatrix that is caused by group j.</p>
<p>B.1. Proof of Theorem 4.3</p>
<p>Recall the vectorized formulation of the META-KEL loss,
L(β) = 1 mn y − Φβ 2 2 + λ p j=1 β (j) 2 .
Let ε s = (ε s,i ) i≤n denote error for task s and ε ∈ R mn the stacked multi-task error vector. Using y = Φβ + ε, we may decompose the loss into two deterministic and random parts. The term 2ε T Φ(β − β * )/mn is the random one and we will refer to as the empirical process. The first typical step in bounding the estimation error of Lasso estimators, is showing that the empirical process, which comes from the noise in observing values of y, does not play a drastic role. More formally, let A j = (Φ T ε) (j) 2 /mn ≤ λ/4 denote that the event that the image of noise affecting the feature space of φ j , is dominated by the regularization term of β (j) . In Lemma B.1 we show that ∩ p j=1 A j happens with high probability, if λ is set properly. Lemma B.1 (Regularization term dominates the empirical process). Set 0 &lt; δ &lt; 1. Consider the random event A = ∩ p j=1 A j . Then A happens with probability greater than 1 − δ, if
λ ≥ 4σ √ mn 1 + 2 m log(2p/δ) + md max log(2p/δ) . where d max = max 1≤j≤p d j .
We now show that if the empirical process is controlled by regularization, i.e. if λ is set to be large enough, then β = min L(β) has favorable properties.</p>
<p>Lemma B.2 (Conditional properties ofβ). Assume that event A happens. Then for any solutionβ of problem 8 and all β ∈ R md , the following hold:
1 mn Φ(β − β * ) 2 2 + λ 2 p j=1 β (j) − β * (j) 2 ≤ 1 mn Φ(β − β * ) 2 2 + 2λ j:β (j) =0 min β (j) 2 , β (j) − β (j) (B.1) j :β (j) = 0 ≤ 16 (mnλ) 2 Φ(β − β * ) 2 2 (B.2)
Note that by the meta data-generating model (Section 3), J k * = {j : β * (j) = 0}. Lemma B.3 (Complete Variable Screening). Assume |J k * | ≤ s and set 0 &lt; δ ≤ 1. Define (n, m) = 32σs κ 2 √ mn 1 + 2 m log(2p/δ) + md max log(2p/δ)</p>
<p>Under assumption 4.2 with κ = κ(s), if λ is chosen according to lemma B.1, then with probability greater than 1 − δ
max j∈J k * β (j) − (β * ) (j) 2 ≤ (n, m) (B.3)
and if in addition min j∈J k * β * (j) 2 ≥ c 1 , then with the same probability for all j ∈ J k * β (j) 2 − c 1 ≤ (n, m).</p>
<p>(B.4)</p>
<p>We now turn to the first claim made in Theorem 4.3, and prove that H k * ⊆ Hk. Sinceη j ≥ 0 and k i are Mercer, thenk is also Mercer and corresponds to an RKHS which we have been referring to as Hk. Consider the RKHS H k * , since |J k * | and each d j s are finite,
H k * =    f : f (·) = j∈J k * η * j β T j φ j (·), β j ∈ R d , β j &lt; ∞  
 Therefore, f ∈ H k * if and only if it is in the finite span of φ, defined as
FinSpan ({φ j : j ∈ J k * }) =    f : f (·) = j∈J k * β T j φ j (·), β j ∈ R d , β j &lt; ∞    Lemma B.
3 states thatβ (j) ≥ c 1 − (n, m) with probability greater than 1 − δ for j ∈ J k * . Therefore, for any j ∈ J k * , we getη j ≥ 1 − (n, m)/c 1 , since we had setη j = β(j) /c 1 . If c 1 &gt; (n, m), thenη j &gt; 0 and j ∈ Jk, implying J k * ⊂ Jk.</p>
<p>Hence, under the assumptions of the theorem, with probability greater than 1 − δ, Therefore, if mn = O(s/p) then Jk ≤ p = |J k full | with probability greater than 1 − δ. and via similar argument as given in the proof of theorem 4.3,
H k * = FinSpan ({φ j : j ∈ J k * }) w.h.p. ⊂ FinSpan {φ j : j ∈ Jk} = Hk.Hk = FinSpan {φ j : j ∈ Jk} w.h.p. ⊂ FinSpan ({φ j : j ∈ J k full }) = H k full .</p>
<p>B.3. Proof of Lemmas used in Section B.1</p>
<p>This section presents the proofs to the helper lemmas introduced before.</p>
<p>Proof of Lemma B.1. This proof follows a similar treatment of the empirical process to Lemma 3.1 Lounici et al. (2011). Since ε i,s are i.i.d. zero-mean sub-gaussian variables, we observe that
P(A j ) = P 1 (mn) 2 ε T Φ (j) (Φ (j) ) T ε ≤ λ 2 16 = P mn i=1 v i (z 2 i − 1) √ 2 v ≤ α
where z i are i.i.d. sub-gaussian variables with variance proxy 1, v i denote the eigenvalues of Φ (j) (Φ (j) ) T /mn and v is the vector of these eigenvalues. Lastly, 
α = mnλ 2 /(16σ 2 ) − Tr(Ψ (j) ) √ 2 Ψ (j) FP(A c j ) = P mn i=1 (z 2 i − 1)v i √ 2 v 2 &gt; α ≤ 2 exp − α 2 2(1 + √ 2α v ∞ / v 2 )
We choose λ such that the right hand side is bounded by δ/p. By definition of v we have v ∞ / v 2 = Ψ (j) 2 / Ψ (j) F . Then, for A c j to happen with probability smaller than δ/p, λ ≥ 4σ √ mn Tr(Ψ (j) ) + 2 Ψ (j) 2 2 log(2p/δ) + md j log(2p/δ) .</p>
<p>Then by union bound, A happens with probability greater than 1 − δ if
λ ≥ max j 4σ
√ mn Tr(Ψ (j) ) + 2 Ψ (j) 2 2 log(2p/δ) + md j log(2p/δ) .</p>
<p>Since the base kernels are normalized we may bound the norm and trace of Ψ (j) .
Tr(Ψ (j) ) = 1 mn m s=1 Tr (Φ s (j) ) T Φ s (j) = 1 mn m s=1 n i=1 φ T j (x s,i )φ j (x s,i ) = 1 mn m s=1 n i=1 k j (x s,i , x s,i ) ≤ 1
Similarly, Ψ (j) 2 ≤ 1 mn max s n i=1 k j (x s,i , x s,i ) ≤ 1/m, and thereby concluding the proof.</p>
<p>Lemma C.2 (Information Gain Bound). The maximum information gain fork after observing t samples satisfies,
γ t ≤d 2 log 1 +σ −2 t c 1 = O(d log t/c 1 ) whered = j∈Jk d j ≤ d.
We now have the main tools for proving Theorem 5.1.</p>
<p>Proof of Theorem 5.1. Assume that f ∈ H k * , and that f k * ≤ B. Then by Theorem 4.3 f ∈ Hk, with probability greater than 1 − δ. DefineB asB
= B 1 + (n, m) 2c 1 + o ( (n, m)) , (C.2)
then by Lemma B.4, f k ≤B with probability greater than 1 − δ. We first condition on the event that f ∈ Hk and f k ≤B. Then Lemma C.1 gives the following confidence interval,
P |μ t−1 (x) − f (x)| ≤σ t−1 (x) B + σ 2(γ t−1 + 1 + log(1/δ)) |f ∈ Hk, f k ≤B ≥ 1 −δ (C.3)
We now remove the conditional event.
Let C t (x) := [μ(x) −σ(x),μ(x) −σ(x)]
. By the chain rule,
P (f (x) ∈ C t (x)) ≥ P f (x) ∈ C t (x)|f ∈ Hk · P f ∈ Hk ≥ 1 − δ −δ
Renaming δ +δ to δ for simplicity, we conclude that with probability greater than 1 − δ
|μ t−1 (x) − f (x)| ≤σ t−1 (x) B 1 + (n, m) 2c 1 + o ( (n, m)) + σ 2(γ t−1 + 1 + log(1/δ)) .
Lastly, Lemma C.2 gives an upper bound forγ t−1 which completes the proof.</p>
<p>Proof of Lemma C.2. For this proof, we follow a similar technique as in Vakili et al. (2021). Recall thatk(x, x ) = j∈Jkη j φ T j (x)φ j (x ), where Jk = {1 ≤ j ≤ p :η j = 0}. Defined the effective dimension of kernels k j that correspond to this index set,d = j∈Jk d j . Now consider any arbitrary sequence of inputs (x τ ) t τ =1 and letΦ t = φ (x 1 ), · · · ,φ(x t ) ∈ R t×d , withφ(x) = (φ j (x)) j∈Jk . Define thed ×d matrix Λ = diag (η j ) j∈Jk as the diagonal matrix containing the eigenfunctions ofk. We have K t =Φ t ΛΦ T t . Let H t = Λ 1/2ΦTΦ Λ 1/2 , by the Weinstein-Aronszajn identity,
1 2 log det(I +σ −2 K t ) = 1 2 log det(I +σ −2 H t ) ≤ 1 2d log tr(I +σ −2 H t )/d
For positive definite matrices P ∈ R n×n , we have log det P ≤ n log tr(P /n). The inequality follows from I +σ −2 H t being positive definite, sinceη j ≥ 0. We may write,
1 2 log det(I +σ −2 K t ) ≤ 1 2d log 1 +σ −2 d tr(Λ 1/2ΦTΦ Λ 1/2 ) ≤ 1 2d log 1 +σ −2 d t τ =1 tr(Λ 1/2φT (x τ )φ(x τ )Λ 1/2 ) ≤ 1 2d log 1 +σ −2 d t τ =1 ||φ(x τ )Λ 1/2 || 2 2 ≤ 1 2d log 1 +σ −2 d t τ =1 j∈Jkη j φ j (x τ ) 2 2 ≤ 1 2d log 1 +σ −2 t c 1 max j β (j)
The next to last inequality holds since k j (x, x) = φ j (x) 2 2 is normalized to one andη j = β (j) 2 /c 1 . The inequality above holds for any sequence (x τ ) τ ≤t ,γ t = O d log(t/c 1 ) C.1. Proof of Corollary 5.2</p>
<p>Similar to the previous section, we take advantage of a classic regret bound for the oracle learner and then apply it to our setting. whereB is set according to Equation (C.2). Plugging in Lemma C.2 to bound the information gain and changing the variable name δ +δ to δ and concludes the proof.</p>
<p>D. Experiments</p>
<p>Regret Experiment on Hyper-Parameter Tuning data This experiment is based on Rothfuss et al. (2021b, Appendix B.1.2), we repeat some of the details for completeness. We consider the use case of hyper-parameter tuning for machine learning algorithms. In particular, we consider Generalized linear models with elastic NET regularization (GLMNET) (Friedman et al., 2010) for this purpose, which has two hyper-parameters lambda and alpha. Following previous work (e.g., Perrone et al., 2008), we replace the costly training and evaluation step by a cheap table lookup based on a large number of hyper-parameter evaluations (Kühn et al., 2018) on 38 classification datasets from the OpenML platform (Bischl et al., 2017). The hyper-parameter evaluations are available under a Creative Commons BY 4.0 license and can be downloaded here 4 . In effect, X is a finite set, corresponding to 10000-30000 random evaluations hyper-parameter evaluations per dataset and machine learning algorithm. Since the sampling is quite dense, for the purpose of empirically evaluating the meta-learned models towards BO, this finite domain can be treated like a continuous domain. All datasets correspond to binary classification. The target function we aim to optimize is the area under the ROC curve (AUROC) on a test split of the respective dataset. Since (Kühn et al., 2018) sample lambda in the log-space, we transform it via lambda ← log 2 (lambda)/10 such that we can expect a reasonably good performance of a Vanilla GP-UCB with SE kernel. We randomly split the available tasks (i.e. train/test evaluations on a specific dataset) into a set of meta-train and meta-test tasks. In the following, we list the corresponding OpenML dataset identifiers:</p>
<p>• meta-train tasks: 3, 1036, 1038, 1043, 1046, 151, 1176, 1049, 1050, 31, 1570, 37, 4134, 1063, 1067, 44, 1068, 50, 1461, 1462 • test tasks: 335, 1489, 1486, 1494, 1504, 1120, 1510, 1479, 1480, 333, 1485, 1487, 334 The plots in Figure 6 show average cumulative regret for 13 test tasks, and each tested for 10 runs with different random seeds. Figure 8 illustrates a few samples of the random functions that we optimize over in the experiments. The functions are constructed using the Legendre basis, as explained in the main text. Figure 9 gives an example of a BO problem where we use IGP-UCB together with the meta-learned kernel to find the minimum of a function with as few samples as possible. As the function estimate improves, the confidence sets rapidly shrink and the learner only samples points close to the minimum. Figure 10 demonstrates that the choice of λ for the META-KEL loss does not have a severe effect on the regret of IGP-UCB. This is only the case if λ satisfies the condition of Theorem 4.3.  Figure 10. For m = n = 50 and p = 20, Theorem 4.3 requires that λ &gt; 0.001 for recovery to happen with probability greater than 1 − δ = 0.9. For λ that satisfies this condition, the particular choice of its value does not effect performance severely.</p>
<p>Supplementary Figures</p>
<p>Figure 1 .
1Proceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copyright 2022 by the author(s). Overview of the described framework with k * as the true kernel function andk as the solution to META-KEL.</p>
<p>Figure 3 .
3Calibration (left)  and sharpness (right) experiment for confidence sets given 4 training samples. Averaged over 50 runs, k always gives tight valid confidence intervals.</p>
<p>Figure 4 .
4Simple and cumulative regret for GP-UCB. The algorithm converges at a slower pace when using k full .</p>
<p>Figure 5 .
5The regret of GP-UCB used withk approaches the oracle regret as the number of offline tasks increase.</p>
<p>Figure 6 .
6Cumulative regret of GP-UCB. A synthetic 2D BO (left), hyper-parameters tuning of GLMNET (right).</p>
<p>Bach, F. R. Consistency of the group lasso and multiple kernel learning. Journal of Machine Learning Research, 2008. Bach, F. R., Lanckriet, G. R., and Jordan, M. I. Multiple kernel learning, conic duality, and the smo algorithm. In Proceedings of the twenty-first international conference on Machine learning, 2004. Bastani, H. and Bayati, M. Online decision making with high-dimensional covariates. Operations Research, 2020. Basu, S., Kveton, B., Zaheer, M., and Szepesvári, C. No regrets for learning the prior in bandits. Advances in Neural Information Processing Systems, 34, 2021. Berkenkamp, F., Turchetta, M., Schoellig, A., and Krause, A. Safe model-based reinforcement learning with stability guarantees. Advances in neural information processing systems, 2017. Berkenkamp, F., Schoellig, A. P., and Krause, A. No-regret bayesian optimization with unknown hyperparameters. Journal of Machine Learning Research, 2019. Bickel, P. J., Ritov, Y., and Tsybakov, A. B. Simultaneous analysis of lasso and dantzig selector. The Annals of statistics, 2009. Bischl, B., Casalicchio, G., Feurer, M., Hutter, F., Lang, M., Mantovani, R., Rijn, J. N., and Vanschoren, J. Openml benchmarking suites and the openml100. arXiv preprint arXiv:1708.03731, 2017.</p>
<p>Wynne, G., Briol, F.-X., and Girolami, M. Convergence guarantees for gaussian process means with misspecified likelihoods and smoothness. Journal of Machine Learning Research, 2021. Zhao, P. and Yu, B. On model selection consistency of lasso. The Journal of Machine Learning Research, 2006. Zhou, D., Li, L., and Gu, Q. Neural contextual bandits with ucb-based exploration. In International Conference on Machine Learning. PMLR, 2020.</p>
<p>.
Finally, by simply using the vectorized notation we get Equation (8), concluding the proof.B. Proof of Statements in Section 4For the first three lemmas in this section we follow the technique in Lounici et al. (2011) and occasionally use classical ideas established in Bühlmann &amp; Van De Geer (2011).</p>
<p>Lemma C. 3 (
3Theorem 3 Chowdhury &amp; Gopalan (2017), fork). Set δ ∈ (0, 1). If f ∈ Hk with f k ≤B, then with probability 1 − δ, GP-UCB satisfies,R T = O B Tγ T + Tγ T (γ T + log 1/δ)Proof of Corollary 5.2. Conditioned on the event that f ∈ Hk and f k ≤B, Lemma C.3 states thatP R T = O B Tγ T + Tγ T (γ T + log 1/δ) |f ∈ Hk, f k ≤B ≥ 1 −δThen by Lemma B.4 and Theorem 4.3, with probability greater than 1 − δ −δ R T = O B Tγ T + Tγ T (γ T + log 1/δ)</p>
<p>Figure 8 .Figure 9 .
89Examples of possible functions fs for the meta-dataset. BO (minimization) with META-KEL. Upper plot shows the state at t = 5 and the lower plot at t = 55.</p>
<p>when the mean and variance of the Gaussian prior are unknown, and there is sufficient offline i.i.d. data drawn from the same Gaussian distribution.Our framework considers structural sparsity at the kernel level, translating to group sparsity for the coefficients vectors, if applied to linear bandits. Thus our work relates to results on sparse linear bandits and Lasso bandits. In this area, Bastani &amp; Bayati). They 
propose how to efficiently update the policy when each 
learner has access to the data across all tasks. We give 
stronger guarantees in less restrictive setting, compared to 
Wang et al. (2018b) which analyzes the simple regret of the 
GP-UCB algorithm (Srinivas et al., 2010) for multi-armed 
and linear bandits, </p>
<p>Simchowitz, M., Tosh, C., Krishnamurthy, A., Hsu, D. J., Lykouris, T., Dudik, M., and Schapire, R. E. Bayesian decision-making under misspecified priors with applications to meta-learning. Advances in Neural Information Processing Systems, 2021.Bogunovic, I. and Krause, A. Misspecified Gaussian process 
bandit optimization. In Conference on Neural Informa-
tion Processing Systems (NeurIPS), 2021. </p>
<p>Boutilier, C., Hsu, C.-w., Kveton, B., Mladenov, M., Szepes-
vari, C., and Zaheer, M. Differentiable meta-learning 
of bandit policies. In Advances in Neural Information 
Processing Systems, 2020. 
Perrone, V., Jenatton, R., Seeger, M. W., and Archambeau, 
C. Scalable Hyperparameter Transfer Learning. In Ad-
vances in Neural Information Processing Systems, 2008. </p>
<p>Rahimi, A., Recht, B., et al. Random features for large-scale 
kernel machines. In NIPS, 2007. </p>
<p>Rothfuss, J., Fortuin, V., Josifoski, M., and Krause, 
A. Pacoh: Bayes-optimal meta-learning with pac-
guarantees. In International Conference on Machine 
Learning. PMLR, 2021a. </p>
<p>Rothfuss, J., Heyn, D., Chen, J., and Krause, A. Meta-
learning reliable priors in the function space. In Advances 
in Neural Information Processing Systems, 2021b. </p>
<p>Russo, D. and Van Roy, B. Learning to optimize via pos-
terior sampling. Mathematics of Operations Research, 
2014. </p>
<p>Sessa, P. G., Bogunovic, I., Kamgarpour, M., and Krause, 
A. Learning to play sequential games versus unknown 
opponents. Advances in Neural Information Processing 
Systems, 2020. </p>
<p>Srinivas, N., Krause, A., Kakade, S., and Seeger, M. Gaus-
sian process optimization in the bandit setting: No regret 
and experimental design. In Proceedings of the 27th In-
ternational Conference on International Conference on 
Machine Learning, 2010. </p>
<p>Vakili, S., Khezeli, K., and Picheny, V. On information 
gain and regret bounds in gaussian process bandits. In 
International Conference on Artificial Intelligence and 
Statistics. PMLR, 2021. </p>
<p>Van de Geer, S., Bühlmann, P., and Zhou, S. The adaptive 
and the thresholded lasso for potentially misspecified 
models (and a lower bound for the lasso). Electronic 
Journal of Statistics, 2011. </p>
<p>Vershynin, R. High-dimensional probability: An introduc-
tion with applications in data science, volume 47. Cam-
bridge University Press, 2018. </p>
<p>Wainwright, M. J. High-dimensional statistics: A non-
asymptotic viewpoint. Cambridge University Press, 2019. </p>
<p>Wang, X., Wei, M., and Yao, T. Minimax concave penalized 
multi-armed bandit model with high-dimensional covari-
ates. In International Conference on Machine Learning. 
PMLR, 2018a. </p>
<p>Wang, Z. and de Freitas, N. Theoretical analysis of 
bayesian optimisation with unknown gaussian process 
hyper-parameters. arXiv preprint arXiv:1406.7758, 2014. </p>
<p>Wang, Z., Li, C., Jegelka, S., and Kohli, P. Batched high-
dimensional bayesian optimization via structural kernel 
learning. In International Conference on Machine Learn-
ing. PMLR, 2017. </p>
<p>Table 1 .
1Notation Guide</p>
<p>The next lemma bounds thek-norm of functions contained in H k * , concluding the proof for Theorem 4.3.Lemma B.4 (Bounding thek-norm). Set 0 &lt; δ ≤ 1 and choose λ according to Lemma B.1 and define (n, m) according to Lemma B.3. If |J k * | ≤ s and Assumption 4.2 holds with κ = κ(s), then under Condition 3.1, for all f ∈ H k * with f k * ≤ B, Under assumptions of the proposition, event A happens with probability greater than 1 − δ. From Equation (B.2) in Lemma B.2,f k ≤ 1 + 
(n, m) 
2c 1 
+ o ( (n, m)) 
(B.5) </p>
<p>B.2. Proof of Proposition 4.4 </p>
<p>Jk ≤ 
16 
(mnλ) 2 Φ(β − β  *  ) </p>
<p>2 </p>
<p>2 </p>
<p>and by Equation (B.10), 
1 
√ mn 
Φ(β − β  *  ) </p>
<p>2 </p>
<p>≤ 
2λ 
√ s </p>
<p>κ </p>
<p>which gives </p>
<p>|Jk| ≤ 
64s 
mnκ 2 (s) </p>
<p>From Equation 27 Cavalier et al. (2002) yields the following inequality,
ETH Zurich, Switzerland. Correspondence to: Parnian Kassraie <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#112;&#107;&#97;&#115;&#115;&#114;&#97;&#105;&#101;&#64;&#101;&#116;&#104;&#122;&#46;&#99;&#104;">&#112;&#107;&#97;&#115;&#115;&#114;&#97;&#105;&#101;&#64;&#101;&#116;&#104;&#122;&#46;&#99;&#104;</a>.
Meta-learning the hypothesis space in the p → ∞ limit will be challenging. However, we expect to obtain an extension to infinite dimensional base-kernels in future work.
https://doi.org/10.6084/m9.figshare.5882230.v2
AcknowledgementsWe thank Andreea Musat and Victor Armegioiu for fruitful discussions and their contributions to an earlier research path of this project. In addition, we thank Scott Sussex and Anastasia Makarova for their feedback on the draft of this paper. We appreciate Felix Schur and Adrian Müller's thorough feedback on the final manuscript. This research was supported by the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation program grant agreement no. 815943. Jonas Rothfuss was supported by an Apple Scholars in AI/ML fellowship.Proof of Lemma B.2. For proving this lemma we are essentially only using Cauchy-Schwarz and the Triangle inequality, together with the KKT optimality condition for L. For any β, sinceβ is the minimizer of L and due to the data generating model (Eq. 4), we haveBy Cauchy-Schwarz and the assumption that A happens,and thereby,which gives Inequality B.1. By the KKT optimality conditions for convex losses(Boyd et al., 2004),β is a minimizer of L, if and only if 0 ∈ ∂L(β), where ∂L(β) denotes the sub-gradient of the loss evaluated atβ. Thereforeβ satisfiesAs for Inequality B.2, conditioned on event A, by Equation (4) together with the KKT condition B.6, we obtain that for allFollowing the analysis of Lounici et al. we conclude,Since the kernels k j are normalized by 1 andwhere the second inequality follows from Cauchy-Schwarz together with the Lemma's assumption |J k * | ≤ s. By again, and therebyUsing assumption 4.2, this inequality indicates thatwhich together with Equation (B.8) gives,The next chain of inequalities proves the first statement of the Lemma (Equation(B.3)). For all j ∈ J k * ,We calculate thek norm of functions that lie in H k * . Let I = {1 ≤ j ≤ p : η * j = 0,η j = 0}.where φ j,r denotes the r-th feature in the feature map φ j , similarly β r (j) the r-th element in vector β (j) , and ·, · 2 is the inner product in the L 2 (X ) space. By applying Cauchy-Schwarz we getConsider the vector v = ( β (j) 2 2 ) j∈I , we observe that v 2 = j∈I β (j) 4 2 and v 1 ≤ B 2 . Since · 2 ≤ · 1 and due to the assumption η * 1 ≤ 1, we obtainIt remains to bound max j∈Iη −1 j . We setη j = β(j) 2 /(c1) and under conditions of the theorem, Lemma B.3 states that η j ≥ 1 − (n, m)/c 1 , for all j ∈ J k * . Then for members of I ⊂ J k * ,which implies the following for the norm boundwith probability greater than 1 − δ.C. Proof of Statements in Section 5The following lemma presents a confidence bound for when the learner has oracle knowledge of the true kernel. This lemma plays an integral role in for the proofs in this section. Lemma C.1 (Theorem 2 Chowdhury &amp; Gopalan (2017) fork). Let f ∈ Hk for some kernelk with ak-norm bounded bŷ B. Then with probability greater than 1 −δ, for all x ∈ X and t ≥ 1,whereμ t−1 andσ t−1 are as defined in Equation 3 withσ = 1 + 2/T .We skip the proof for this lemma as it is given in Chowdhury &amp; Gopalan (2017), with the same notation. For the kernelk we define the maximum information gain after t − 1 observations aŝ γ t−1 := max[xτ ] τ ≤t 1 2 log det(I +σ −2K t−1 )This parameter quantifies the speed at which we learn about f , when using the kernelk. Note that γ t−1 is independent of any specific realization of H t−1 . It only depends on the choice of kernel, the input domain, and the noise variance. The next lemma bounds this parameter.
Improved algorithms for linear stochastic bandits. Y Abbasi-Yadkori, D Pál, C Szepesvári, Advances in Neural Information Processing Systems. Abbasi-Yadkori, Y., Pál, D., and Szepesvári, C. Improved algorithms for linear stochastic bandits. In Advances in Neural Information Processing Systems, 2011.</p>
<p>Optimization with sparsity-inducing penalties. Foundations and Trends in Machine Learning. F Bach, R Jenatton, J Mairal, G Obozinski, Bach, F., Jenatton, R., Mairal, J., Obozinski, G., et al. Opti- mization with sparsity-inducing penalties. Foundations and Trends in Machine Learning, 2012.</p>            </div>
        </div>

    </div>
</body>
</html>
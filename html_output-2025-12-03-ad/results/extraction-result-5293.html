<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5293 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5293</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5293</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-108.html">extraction-schema-108</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-5bfeb6901db481c08874cfe0ae807d8564513765</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/5bfeb6901db481c08874cfe0ae807d8564513765" target="_blank">GuacaMol: Benchmarking Models for De Novo Molecular Design</a></p>
                <p><strong>Paper Venue:</strong> Journal of Chemical Information and Modeling</p>
                <p><strong>Paper TL;DR:</strong> This work proposes an evaluation framework, GuacaMol, based on a suite of standardized benchmarks, to standardize the assessment of both classical and neural models for de novo molecular design, and describes a variety of single and multiobjective optimization tasks.</p>
                <p><strong>Paper Abstract:</strong> De novo design seeks to generate molecules with required property profiles by virtual design-make-test cycles. With the emergence of deep learning and neural generative models in many application areas, models for molecular design based on neural networks appeared recently and show promising results. However, the new models have not been profiled on consistent tasks, and comparative studies to well-established algorithms have only seldom been performed. To standardize the assessment of both classical and neural models for de novo molecular design, we propose an evaluation framework, GuacaMol, based on a suite of standardized benchmarks. The benchmark tasks encompass measuring the fidelity of the models to reproduce the property distribution of the training sets, the ability to generate novel molecules, the exploration and exploitation of chemical space, and a variety of single and multiobjective optimization tasks. The benchmarking open-source Python code and a leaderboard can be found on https://benevolent.ai/guacamol .</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5293.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5293.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SMILES LSTM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SMILES Long Short-Term Memory recurrent neural network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recurrent neural network (LSTM) trained on SMILES strings to generate novel molecules; used here as a baseline generative model for both distribution-learning and goal-directed de novo molecular design tasks and previously validated prospectively by synthesis and biological testing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SMILES LSTM (LSTM RNN)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>recurrent neural network (LSTM) sequence model</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pretrained on a standardized subset of ChEMBL (ChEMBL 24 post-processed dataset) in the experiments reported here; in prior related work pretrained on a large corpus of molecules and fine-tuned on small sets of actives for specific targets.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Drug discovery / de novo molecular design (distribution learning, similarity/rediscovery tasks, multi-objective property optimization, and generation of candidate actives for biological targets).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Sampling from the pretrained LSTM; can be fine-tuned via transfer learning and coupled to external scoring functions or reinforcement learning in prior studies to bias outputs toward desired properties.</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>SMILES strings (canonical SMILES for uniqueness checks)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Distribution-learning: Validity, Uniqueness, Novelty, KL divergence over physicochemical descriptor distributions, Fréchet ChemNet Distance (FCD). Goal-directed: benchmark-specific objective scores (top-1/top-10/top-100 averages), rediscovery/similarity task scores. Compound quality: rd_filters (SureChembl, Glaxo, PAINS, in‑house filters).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>GuacaMol benchmark suite (5 distribution-learning + 20 goal-directed benchmarks); ChEMBL 24 (post-processed) used for training; holdout set of 10 marketed drugs used to define similarity benchmarks; prior prospective targets (e.g., nuclear receptors RXR/PPAR) reported in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>In distribution-learning benchmarks the SMILES LSTM achieved high uniqueness and novelty and strong KL divergence (reported KL score 0.991) and competitive FCD (0.913). In goal-directed benchmarks it nearly matched or exceeded many baselines, achieving perfect or near-perfect rediscovery/similarity scores (e.g., 1.000 on multiple rediscovery tasks) and overall high total benchmark score (reported total 17.340). Compound quality of top molecules from LSTM was high and comparable to the Best-of-ChEMBL baseline (~77% passing quality filters). Some invalid molecules were produced occasionally.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Performed better than more complex GAN-based models (ORGAN) on distribution-learning metrics and produced higher-quality molecules than genetic algorithms (Graph GA) despite slightly lower optimization scores on some tasks; in several goal-directed tasks the Graph GA outperformed the LSTM in raw optimization score but produced lower-quality molecules on average. Simpler neural sequence models (LSTM) were found to be highly competitive with or superior to more complex architectures in these benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Produces some invalid SMILES (validity < 1.0). Relies on SMILES representation which has syntactic sensitivity. Performance depends on pretraining data and fine-tuning strategy; detailed model hyperparameters and sizes are not reported in this paper. While compound quality is substantially better than some discrete optimizers, it is not perfect and the approach may still generate molecules difficult to synthesize or chemically undesirable without additional filters or constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GuacaMol: Benchmarking Models for De Novo Molecular Design', 'publication_date_yy_mm': '2018-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5293.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5293.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SMILES RNN (GDB13 study)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SMILES recurrent neural network trained on a small subset of GDB13</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A SMILES-based recurrent neural network studied for its ability to explore enumerated chemical space; reported to recover a large fraction of GDB13 after sampling and even generate molecules missing from the original enumeration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SMILES RNN</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>recurrent neural network (sequence model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Trained on a small subset (reported as 0.1%) of GDB13, an enumerated set of molecules up to 13 heavy atoms, in the cited study discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Exploration/coverage of chemical space (methodological demonstration of generative model coverage of an enumerated small-molecule chemical space).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Sampling from the trained SMILES RNN (no detailed optimization method reported here for that study other than sampling to assess coverage).</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>SMILES strings</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Coverage of the enumerated GDB13 dataset (ability to reproduce elements of the enumerated space), ability to generate molecules omitted from the original enumeration.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>GDB13 (enumerated molecules up to 13 heavy atoms)</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>The SMILES RNN trained on a very small fraction of GDB13 was able to sample and recover a large portion of the full enumerated set and even produce molecules that were incorrectly omitted from the original GDB13 enumeration, demonstrating strong coverage ability for that scope.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Used as evidence that pretrained neural sequence models can explore large chemical spaces; not directly compared within this paper's experimental baseline table to other methods beyond the qualitative claim.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Study scope limited to small enumerated molecules (<=13 heavy atoms); implications for larger, drug-like chemical spaces and for practical property optimization are not shown in that specific study as discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GuacaMol: Benchmarking Models for De Novo Molecular Design', 'publication_date_yy_mm': '2018-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5293.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5293.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SMILES VAE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Variational Autoencoder for SMILES-based molecular generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variational autoencoder that encodes SMILES strings into a continuous latent space enabling molecule generation and optimization via gradient-based or Bayesian optimization in latent space.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SMILES VAE (Variational Autoencoder)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>encoder–decoder variational autoencoder (continuous latent-space generative model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Trained on SMILES datasets (in the literature typically ChEMBL or similar molecular corpora); this paper uses VAE as a baseline and evaluates it on ChEMBL-derived benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>De novo molecular design and property optimization via latent-space optimization (drug discovery).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Encode SMILES to a continuous latent representation, perform optimization (e.g., gradient descent or Bayesian optimization) in latent space, then decode latent points back to SMILES.</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>SMILES strings (decoded from latent space)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Same distribution-learning metrics (Validity, Uniqueness, Novelty, KL divergence, FCD) and goal-directed benchmark scoring used in GuacaMol when used as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>GuacaMol benchmark suite; ChEMBL-derived training data for baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>As a baseline VAE produced relatively good and consistent distribution-learning scores across tasks but was not the top performer in any single metric reported in this paper's baseline table.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Performed consistently but was outperformed by SMILES LSTM on some distribution-learning metrics; simpler generative models (LSTM, VAE) generally outperformed more complex GAN-based models (ORGAN) in these benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Decoding to valid SMILES can be an issue; did not achieve the best scores in the evaluated benchmarks, indicating room for architecture or training improvements; reliance on SMILES implies representation-related constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GuacaMol: Benchmarking Models for De Novo Molecular Design', 'publication_date_yy_mm': '2018-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5293",
    "paper_id": "paper-5bfeb6901db481c08874cfe0ae807d8564513765",
    "extraction_schema_id": "extraction-schema-108",
    "extracted_data": [
        {
            "name_short": "SMILES LSTM",
            "name_full": "SMILES Long Short-Term Memory recurrent neural network",
            "brief_description": "A recurrent neural network (LSTM) trained on SMILES strings to generate novel molecules; used here as a baseline generative model for both distribution-learning and goal-directed de novo molecular design tasks and previously validated prospectively by synthesis and biological testing.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "SMILES LSTM (LSTM RNN)",
            "model_type": "recurrent neural network (LSTM) sequence model",
            "model_size": null,
            "training_data": "Pretrained on a standardized subset of ChEMBL (ChEMBL 24 post-processed dataset) in the experiments reported here; in prior related work pretrained on a large corpus of molecules and fine-tuned on small sets of actives for specific targets.",
            "application_domain": "Drug discovery / de novo molecular design (distribution learning, similarity/rediscovery tasks, multi-objective property optimization, and generation of candidate actives for biological targets).",
            "generation_method": "Sampling from the pretrained LSTM; can be fine-tuned via transfer learning and coupled to external scoring functions or reinforcement learning in prior studies to bias outputs toward desired properties.",
            "output_representation": "SMILES strings (canonical SMILES for uniqueness checks)",
            "evaluation_metrics": "Distribution-learning: Validity, Uniqueness, Novelty, KL divergence over physicochemical descriptor distributions, Fréchet ChemNet Distance (FCD). Goal-directed: benchmark-specific objective scores (top-1/top-10/top-100 averages), rediscovery/similarity task scores. Compound quality: rd_filters (SureChembl, Glaxo, PAINS, in‑house filters).",
            "benchmarks_or_datasets": "GuacaMol benchmark suite (5 distribution-learning + 20 goal-directed benchmarks); ChEMBL 24 (post-processed) used for training; holdout set of 10 marketed drugs used to define similarity benchmarks; prior prospective targets (e.g., nuclear receptors RXR/PPAR) reported in related work.",
            "results_summary": "In distribution-learning benchmarks the SMILES LSTM achieved high uniqueness and novelty and strong KL divergence (reported KL score 0.991) and competitive FCD (0.913). In goal-directed benchmarks it nearly matched or exceeded many baselines, achieving perfect or near-perfect rediscovery/similarity scores (e.g., 1.000 on multiple rediscovery tasks) and overall high total benchmark score (reported total 17.340). Compound quality of top molecules from LSTM was high and comparable to the Best-of-ChEMBL baseline (~77% passing quality filters). Some invalid molecules were produced occasionally.",
            "comparison_to_other_methods": "Performed better than more complex GAN-based models (ORGAN) on distribution-learning metrics and produced higher-quality molecules than genetic algorithms (Graph GA) despite slightly lower optimization scores on some tasks; in several goal-directed tasks the Graph GA outperformed the LSTM in raw optimization score but produced lower-quality molecules on average. Simpler neural sequence models (LSTM) were found to be highly competitive with or superior to more complex architectures in these benchmarks.",
            "limitations_or_challenges": "Produces some invalid SMILES (validity &lt; 1.0). Relies on SMILES representation which has syntactic sensitivity. Performance depends on pretraining data and fine-tuning strategy; detailed model hyperparameters and sizes are not reported in this paper. While compound quality is substantially better than some discrete optimizers, it is not perfect and the approach may still generate molecules difficult to synthesize or chemically undesirable without additional filters or constraints.",
            "uuid": "e5293.0",
            "source_info": {
                "paper_title": "GuacaMol: Benchmarking Models for De Novo Molecular Design",
                "publication_date_yy_mm": "2018-11"
            }
        },
        {
            "name_short": "SMILES RNN (GDB13 study)",
            "name_full": "SMILES recurrent neural network trained on a small subset of GDB13",
            "brief_description": "A SMILES-based recurrent neural network studied for its ability to explore enumerated chemical space; reported to recover a large fraction of GDB13 after sampling and even generate molecules missing from the original enumeration.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "SMILES RNN",
            "model_type": "recurrent neural network (sequence model)",
            "model_size": null,
            "training_data": "Trained on a small subset (reported as 0.1%) of GDB13, an enumerated set of molecules up to 13 heavy atoms, in the cited study discussed in this paper.",
            "application_domain": "Exploration/coverage of chemical space (methodological demonstration of generative model coverage of an enumerated small-molecule chemical space).",
            "generation_method": "Sampling from the trained SMILES RNN (no detailed optimization method reported here for that study other than sampling to assess coverage).",
            "output_representation": "SMILES strings",
            "evaluation_metrics": "Coverage of the enumerated GDB13 dataset (ability to reproduce elements of the enumerated space), ability to generate molecules omitted from the original enumeration.",
            "benchmarks_or_datasets": "GDB13 (enumerated molecules up to 13 heavy atoms)",
            "results_summary": "The SMILES RNN trained on a very small fraction of GDB13 was able to sample and recover a large portion of the full enumerated set and even produce molecules that were incorrectly omitted from the original GDB13 enumeration, demonstrating strong coverage ability for that scope.",
            "comparison_to_other_methods": "Used as evidence that pretrained neural sequence models can explore large chemical spaces; not directly compared within this paper's experimental baseline table to other methods beyond the qualitative claim.",
            "limitations_or_challenges": "Study scope limited to small enumerated molecules (&lt;=13 heavy atoms); implications for larger, drug-like chemical spaces and for practical property optimization are not shown in that specific study as discussed here.",
            "uuid": "e5293.1",
            "source_info": {
                "paper_title": "GuacaMol: Benchmarking Models for De Novo Molecular Design",
                "publication_date_yy_mm": "2018-11"
            }
        },
        {
            "name_short": "SMILES VAE",
            "name_full": "Variational Autoencoder for SMILES-based molecular generation",
            "brief_description": "A variational autoencoder that encodes SMILES strings into a continuous latent space enabling molecule generation and optimization via gradient-based or Bayesian optimization in latent space.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "SMILES VAE (Variational Autoencoder)",
            "model_type": "encoder–decoder variational autoencoder (continuous latent-space generative model)",
            "model_size": null,
            "training_data": "Trained on SMILES datasets (in the literature typically ChEMBL or similar molecular corpora); this paper uses VAE as a baseline and evaluates it on ChEMBL-derived benchmarks.",
            "application_domain": "De novo molecular design and property optimization via latent-space optimization (drug discovery).",
            "generation_method": "Encode SMILES to a continuous latent representation, perform optimization (e.g., gradient descent or Bayesian optimization) in latent space, then decode latent points back to SMILES.",
            "output_representation": "SMILES strings (decoded from latent space)",
            "evaluation_metrics": "Same distribution-learning metrics (Validity, Uniqueness, Novelty, KL divergence, FCD) and goal-directed benchmark scoring used in GuacaMol when used as a baseline.",
            "benchmarks_or_datasets": "GuacaMol benchmark suite; ChEMBL-derived training data for baselines.",
            "results_summary": "As a baseline VAE produced relatively good and consistent distribution-learning scores across tasks but was not the top performer in any single metric reported in this paper's baseline table.",
            "comparison_to_other_methods": "Performed consistently but was outperformed by SMILES LSTM on some distribution-learning metrics; simpler generative models (LSTM, VAE) generally outperformed more complex GAN-based models (ORGAN) in these benchmarks.",
            "limitations_or_challenges": "Decoding to valid SMILES can be an issue; did not achieve the best scores in the evaluated benchmarks, indicating room for architecture or training improvements; reliance on SMILES implies representation-related constraints.",
            "uuid": "e5293.2",
            "source_info": {
                "paper_title": "GuacaMol: Benchmarking Models for De Novo Molecular Design",
                "publication_date_yy_mm": "2018-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [],
    "cost": 0.0119095,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>GuacaMol: Benchmarking Models for De Novo Molecular Design</h1>
<p>Nathan Brown, Marco Fiscato<em>,<br>Marwin H.S. Segler</em>, Alain C. Vaucher*<br>BenevolentAI, London, UK<br>E-mail: {marco.fiscato,marwin.segler, alain.vaucher}@benevolent.ai</p>
<h4>Abstract</h4>
<p>De novo design seeks to generate molecules with required property profiles by virtual design-make-test cycles. With the emergence of deep learning and neural generative models in many application areas, models for molecular design based on neural networks appeared recently and show promising results. However, the new models have not been profiled on consistent tasks, and comparative studies to well-established algorithms have only seldom been performed. To standardize the assessment of both classical and neural models for de novo molecular design, we propose an evaluation framework, GuacaMol, based on a suite of standardized benchmarks. The benchmark tasks encompass measuring the fidelity of the models to reproduce the property distribution of the training sets, the ability to generate novel molecules, the exploration and exploitation of chemical space, and a variety of single and multi-objective optimization tasks. The benchmarking open-source Python code, and a leaderboard can be found on https://benevolent.ai/guacamol.</p>
<h1>1 Introduction</h1>
<p>De novo molecular design is a computational technique to generate novel compounds with desirable property profiles from scratch. ${ }^{1,2}$ It complements virtual screening, where large virtual compound libraries are pre-generated, stored, and then subsequently ranked on demand. Virtual screening is particularly useful when the virtual compounds are readily available (commercially, or in in-house screening collections), or easy to synthesize. ${ }^{3,4}$ Datasets sized on the order of $10^{13}$ can be routinely screened under current computational constraints. ${ }^{5}$ However, this is only a tiny fraction of drug-like chemical space, which has an estimated size anywhere between $10^{24}$ and $10^{60}$ possible structures. ${ }^{5,6}$ This number might be even larger considering new modalities such as PROTACs. ${ }^{7}$ Efficient approaches to query larger combinatorial spaces without full enumeration via similarity to given structures have been reported. ${ }^{8-10}$ However, it is not straightforward to perform more complex, multi-objective queries without enumerating substantial parts of the space.</p>
<p>In contrast, in de novo design, only a relatively small number of molecules is explicitly generated, and chemical space is explored via search or optimization procedures. Thus, by focusing on the most relevant areas of chemical space for the current multi-objective query, de novo design algorithms can in principle explore the full chemical space, given any ranking or scoring function.</p>
<p>In addition to established discrete models relying on molecular graphs, ${ }^{11-13}$ models for de novo design based on deep neural networks have been proposed in recent years, and have shown promising results. ${ }^{14,15}$ Unfortunately, validation methods for these generative models have not been consistent. Comparative studies to established de novo design models have not yet been performed. Furthermore, recent property optimization studies often focused on easily optimizable properties, such as drug-likeness or partition coefficients. ${ }^{15-17}$ This makes it difficult to understand the strengths and weaknesses of the different models, to assess which models should be used in practice, and how they can be improved and extended further.</p>
<p>In other fields of machine learning, for example computer vision and natural language</p>
<p>processing, standardized benchmarks have triggered rapid progress. ${ }^{18,19}$ Similarly, we believe that the field of de novo molecular design can benefit from the introduction of standardized benchmarks. They allow for a straightforward survey and comparison of existing models, and provide insight into the strengths and weaknesses of models, which is valuable information to improve current approaches.</p>
<p>In this work, we introduce GuacaMol, a framework for benchmarking models for de novo molecular design. We define a suite of benchmarks and implement it as a Python package designed to allow researchers to assess their models easily. We also provide implementations and results for a series of baseline models on https://benevolent.ai/guacamol.</p>
<h1>2 Models for De Novo Molecular Design</h1>
<p>De novo design approaches require three components: 1) molecule generation, 2) a way to score the molecules, and 3) a way to optimize or search for better molecules with respect to the scoring function. ${ }^{13}$ For scoring, any function that maps molecules to a real valued score can be used, for example quantitative structure-activity relationships (QSAR) and quantitative structure-property relationships (QSPR) models, structure-based models for affinity prediction, heuristics for the calculation of physicochemical properties, and combinations thereof for multi-objective tasks.</p>
<p>At the center of molecule generation and optimization strategies lies the choice of the molecular representation, which determines the potential range of exploration of chemical space. Molecules can be constructed atom-by-atom or by combination of fragments. ${ }^{13}$ They can be grown in the context of a binding pocket, ${ }^{20-24}$ which allows for structure-based scoring, or be constructed and scored using entirely ligand-based methods. Choosing a very general representation of molecules for construction, such as atom-by-atom and bond-bybond building, allows for potentially exploring all of chemical space. However, without any constraints or scoring defining what sensible molecules look like, such representations can</p>
<p>lead to molecules which are very difficult to synthesize or potentially unstable, particularly in combination with powerful global optimizers such as genetic algorithms (GAs). ${ }^{13,25}$ To alleviate this problem, different approaches were introduced to generate molecules using readily available building blocks and robust reaction schemes (e.g. amide couplings), or fragments derived from known molecules. ${ }^{13,26,27}$ This, however, may drastically limit the potentially explorable space. This compromise between realistic molecules and large explorable molecular space is a major dilemma of de novo design.</p>
<p>A seemingly obvious solution is to add a contribution penalizing unrealistic molecules to the scoring function. Unfortunately, it is not trivial to encode what medicinal chemists would consider as molecules that are acceptable to take forward to synthesis. ${ }^{28,29}$ Instead of defining what good molecules look like as rules, or in the form of a scoring function, machine learning approaches represent an attractive alternative, because they are potentially able to learn how reasonable molecules look from data. ${ }^{30,31}$</p>
<p>Machine learning has been used in chemoinformatics for at least 50 years to score molecules (QSAR/QSPR applications), that is to predict properties given a structure. ${ }^{32}$ The inverse QSAR problem of predicting structures given target properties has received less attention. ${ }^{33}$ However, recent advances in algorithms and hardware have made machine learning models and particularly neural networks, which directly output molecular structures tractable. ${ }^{30}$</p>
<p>Two early papers on generative models for molecules, first published as preprints, employed the simplified molecular-input line-entry system (SMILES) representation, which allows for the representation of molecular graphs as a string, with parentheses and numbers indicating branching points and ring closures. ${ }^{34}$ Gómez-Bombarelli et al. proposed to use variational auto-encoders (VAEs), which encode the SMILES representation of a molecule into a latent, continuous representation in real space, and back. ${ }^{15}$ Optimization can then be performed by gradient descent or Bayesian optimization in real space. Segler et al. introduced the notion of transfer and reinforcement learning for molecule generation using recur-</p>
<p>rent neural networks (RNNs) on SMILES. In particular, an RNN called Long Short-Term Memory (LSTM) ${ }^{35}$ was employed. The neural networks were pretrained on a general corpus of molecules, and then either fine-tuned on a small number of known actives, or coupled to an external scoring function to drive the generation of molecules with desired properties, e.g. activity against biological targets. ${ }^{14}$ Both papers pointed out the then lack of models for direct graph generation, which is considerably harder than generating sequences. Generative models based on neural networks have since been further explored, with work on VAEs, ${ }^{17,36-45}$ RNNs, ${ }^{46-58}$ generative adversarial networks (GANs), ${ }^{54,55,59,60}$ adversarial autoencoders (AAEs), ${ }^{61-64}$ and graph convolutional networks, ${ }^{16,57,60,65}$ using SMILES strings or graphs to represent molecules.</p>
<p>Also, the first prospective studies using neural generative models for de novo design have been published. Merk, Schneider and coworkers validated the SMILES-LSTM transfer learning method ${ }^{14}$ by generating compounds $\mathbf{1 - 3}$ for nuclear receptors, which after synthesis and testing showed micro- to nanomolar activity (see Figure 1). ${ }^{66,67}$
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Compounds 1-3, generated by a SMILES-LSTM model, ${ }^{14}$ were made and tested, and showed micro- to nanomolar activity against the RXR and PPAR receptors. ${ }^{66,67}$</p>
<p>An important result about the space that pretrained neural models can explore was recently published by Arus-Pous et al. ${ }^{68}$ They studied whether SMILES RNNs trained on a small subset $(0.1 \%)$ of GDB13, an enumerated molecule set, which covers all potentially stable molecules up to 13 heavy atoms, could recover almost the full dataset after sampling a sufficient number of compounds. They showed that the SMILES RNN was able to cover (and thus explore) a large portion of the complete chemical space defined by GDB13, and even generate molecules which were incorrectly omitted in the original GDB13. ${ }^{68}$</p>
<h1>3 Assessing De Novo Design Techniques</h1>
<p>To profile models for de novo molecular design, we differentiate between their two main use cases:</p>
<ul>
<li>Given a training set of molecules, a model generates new molecules following the same chemical distribution;</li>
<li>A model generates the best possible molecules to satisfy a predefined goal.</li>
</ul>
<p>The collection of benchmarks we propose below assesses both facets defined here. In the following we will refer to these two categories as distribution-learning benchmarks and goaldirected benchmarks, respectively.</p>
<p>The two benchmark categories are evaluated independently to afford models as much flexibility as possible without penalty, since there is no one-to-one correspondence between distribution-learning and goal-directed tasks. For instance, some models are able to generate optimized molecules without learning the chemical distribution first. Also, a model trained to reproduce a chemical distribution may then employ different strategies to deliver optimized molecules.</p>
<h3>3.1 Distribution-learning benchmarks</h3>
<p>Models for de novo drug design often learn to reproduce the distribution of a training set, or use this training set to derive molecular fragments before generating targeted molecules. This allows some model architectures to learn the syntax of molecules in the selected molecular representation, and often accelerates the goal-directed tasks.</p>
<p>The distribution-learning benchmarks assess how well models learn to generate molecules similar to a training set, which in this work is a standardized subset of the ChEMBL database (see Appendix 8.1). ${ }^{69}$ We consider five benchmarks for distribution learning:</p>
<h1>3.1.1 Validity</h1>
<p>The validity benchmark assesses whether the generated molecules are actually valid, i.e. whether they correspond to a (at least theoretically) realistic molecule. For example, molecules with an incorrect SMILES syntax, or with invalid valence, are penalized.</p>
<h3>3.1.2 Uniqueness</h3>
<p>Given the high dimensionality of chemical space and the huge number of molecules potentially relevant in medicine, generative models should be able to generate a vast number of different molecules. The uniqueness benchmark assesses whether models are able to generate unique molecules - i.e., if a model generates the same molecule multiple times, it will be penalized.</p>
<h3>3.1.3 Novelty</h3>
<p>Since the ChEMBL training set represents only a tiny subset of drug-like chemical space, a model for de novo molecular design with a good coverage of chemical space will rarely generate molecules present in the training set. The novelty benchmark penalizes models when they generate molecules already present in the training set. Therefore, models overfitting the training set will obtain low scores on this task.</p>
<h3>3.1.4 Fréchet ChemNet Distance</h3>
<p>Preuer et al. introduced the Fréchet ChemNet Distance (FCD) as a measure of how close distributions of generated are to the distribution of molecules in the training set. ${ }^{70}$ In the context of drug discovery, the usefulness of this metric stems from its ability to incorporate important chemical and biological features. The FCD is determined from the hidden representation of molecules in a neural network called ChemNet trained for predicting biological activities, similarly to the Fréchet Inception Distance sometimes applied to generative models for images. More precisely, the means and covariances of the activations of the penultimate layer of ChemNet are calculated for the reference set and for the set of generated molecules.</p>
<p>The FCD is then calculated as the Fréchet distance for both pairs of values. Similar molecule distributions are characterized by low FCD values.</p>
<h1>3.1.5 KL divergence</h1>
<p>The KL (Kullback-Leibler) divergence ${ }^{71}$</p>
<p>$$
D_{\mathrm{KL}}(P, Q)=\sum_{i} P(i) \log \frac{P(i)}{Q(i)}
$$</p>
<p>measures how well a probability distribution $Q$ approximates another distribution $P$. For the KL divergence benchmark, the probability distributions of a variety of physicochemical descriptors for the training set and a set of generated molecules are compared, and the corresponding KL divergences are calculated. Models able to capture the distributions of molecules in the training set will lead to small KL divergence values.</p>
<p>It has been noted that a lack of diversity of the generated molecules is an issue for a few models for de novo design, in particular GANs. ${ }^{70,72}$ Other model classes do not suffer from that problem. ${ }^{14}$ The KL divergence benchmark captures diversity to some extent, by requiring the generated molecules to be as diverse as the training set with respect to the considered property distributions.</p>
<h3>3.2 Goal-directed benchmarks</h3>
<p>The goal-directed optimization of molecules relies on a formalism in which molecules can be scored individually. The molecule score reflects how well a molecule fulfills the required property profile (sometimes also called multi-property objective/MPO). The goal is to find molecules which maximize the scoring function.</p>
<p>Concretely, the models are asked to generate a given number of molecules with high scores for a given function. The models have access to the scoring function and can iteratively improve their best molecule guesses, without knowing explicitly what the scoring function</p>
<p>calculates.
Here, by using robust and simple, but relevant scoring functions for molecular design, we disentangle the problem of molecule optimization from the problem of choosing good scoring functions, which has been highlighted many times in the context of de novo design and virtual screening, and will not be the focus of this article.</p>
<p>In general, the function to optimize will be defined as the combination of one or several functions, representing different molecular features such as:</p>
<ul>
<li>Structural features. Examples: molecular weight, number of aromatic rings, number of rotatable bonds;</li>
<li>Physicochemical properties. Examples: TPSA, $\log \mathrm{P}$;</li>
<li>Similarity or dissimilarity to other molecules;</li>
<li>Presence and absence of substructures, functional groups or atom types.</li>
</ul>
<p>The majority of benchmarks presented in this work define complex combinations of such features. In addition, some benchmarks fall into special categories:</p>
<h1>3.2.1 Similarity</h1>
<p>Similarity is one of the core concepts of chemoinformatics. ${ }^{73,74}$ It serves multiple purposes and is an interesting objective for optimization. First, it is a surrogate for machine learning models, since it mimics an interpretable nearest neighbor model. However, it has the strong advantage over more complex machine learning (ML) algorithms that deficiencies in the ML models, stemming from training on small datasets or activity cliffs, cannot be as easily exploited by the generative models. Second, it is directly related to virtual screening: de novo design with a similarity objective can be interpreted as a form of inverse virtual screening, where molecules similar to a given target compound are generated on the fly instead of looking them up in a large database. In the similarity benchmarks, models aim to generate</p>
<p>molecules similar to a target that was removed from the training set. Models perform well for the similarity benchmarks if they are able to generate many molecules that are closely related to a given target molecule. Alternatively, the concept of similarity can be applied to exclude molecules that are too similar to other molecules.</p>
<h1>3.2.2 Rediscovery</h1>
<p>Rediscovery benchmarks are closely related to the similarity benchmarks described above. The major difference is that the rediscovery task explicitly aims to rediscover the target molecule, not to generate many molecules similar to it. Rediscovery benchmarks have been studied in previous work. ${ }^{11,14}$</p>
<h3>3.2.3 Isomers</h3>
<p>For the isomer benchmarks, the task is to generate molecules that correspond to a target molecular formula (for example $\mathrm{C}<em 8="8">{7} \mathrm{H}</em>} \mathrm{~N<em 2="2">{2} \mathrm{O}</em>$ ). The isomers for a given molecular formula can in principle be enumerated, but except for small molecules this number will in general be very large. The isomer benchmarks represent fully-determined tasks that assess the flexibility of the model to generate molecules following a simple pattern (which is a priori unknown). These benchmarks have no direct application in drug discovery.</p>
<h3>3.2.4 Median molecules</h3>
<p>In the median molecules benchmarks, the similarity to several molecules has to be maximized simultaneously; it is designed to be a conflicting task. ${ }^{25}$ Besides measuring the obtained top score, it is instructive to study if the models also explore the chemical space between the target structures.</p>
<h1>3.3 Measuring Compound Quality</h1>
<p>In previous works, several authors have highlighted that unrestricted de novo design algorithms can generate compounds which are potentially unstable, reactive, laborious to synthesize, or simply unpleasant to the eye of medicinal chemists. ${ }^{12,13,75}$ Systems proposing too many unsuitable compounds lose trust, and will not be accepted by medicinal chemists. Unfortunately, explicitly encoding all the background knowledge medicinal chemists acquire with experience as an exhaustive list of unstable or undesirable substructures is challenging, if not impossible, due to the inherent subjectivity and context dependency - e.g. a toxicity risk might be assessed differently in oncology and diabetology. QSAR models only cover this background knowledge in a limited way.</p>
<p>Nevertheless, a de novo design benchmark would be incomplete without an assessment of compound quality. Here, following previous work, we employ rule sets which were compiled to decide whether compounds can be included into high-throughput screening (HTS) collections. It has to be noted though that these rules are controversial. Assuming that molecules filtered out by these patterns are all unsuitable, and that all compounds passing the filters are reasonable molecules, would be misguided. These filters are therefore taken as a high precision, low recall surrogate measure - we know that a filtered out molecule is probably bad, but we also know that our list of filters is incomplete.</p>
<p>We employ Walters' rd_filters implementation, ${ }^{76}$ using the SureChembl, Glaxo, PAINS (all retrieved from ChEMBL ${ }^{69}$ ), and in-house rule collections, to calculate the compound quality metrics. The rule collection is available in the Supporting Information. ${ }^{77}$</p>
<h2>4 Methods</h2>
<p>Our framework for benchmarking models for de novo design, GuacaMol, is available as an open-source Python package and can be downloaded from the Internet at www. github.com/BenevolentAI/ guacaMol provides user-friendly interfaces that allow researchers to couple their models to</p>
<p>the benchmarking framework with minimal effort.
We selected five distribution-learning and twenty goal-directed benchmarks. The exhaustive specifications and implementation details of the distribution-learning and goal-directed benchmarks are given in Appendices 8.2 and 8.3, respectively.</p>
<p>For all chemoinformatics-related operations, such as handling or canonicalization ${ }^{78}$ of SMILES strings, physicochemical property $\left(\log \mathrm{P}, \mathrm{TPSA}^{79}\right)$ or similarity calculations, GuacaMol relies on the RDKit package. ${ }^{80}$</p>
<p>Several of the molecule generation approaches we studied require a training dataset. ChEMBL 24 was used for this purpose. ${ }^{69}$ The generation of the dataset is discussed and detailed in Appendix 8.1.</p>
<p>In addition to the introduction of a benchmark suite for generative chemistry, this paper provides its evaluation over several baselines. The goal is to present a fair comparison of some recent methods that seem to achieve state of the art results in the field. These have been selected to represent a variety of methods, ranging from deep learning generative models to established genetic algorithms and more recent Monte Carlo Tree Search (MCTS). Internal molecule representation was also taken into account, so that methods using SMILES strings were represented alongside others using a graph representation of atoms and bonds. For completeness and to provide a lower bound on the benchmark scores, two dummy baselines, performing random sampling and best of dataset-picking, are also provided. All the baseline models assessed in this work are described in Appendix 8.4. Our implementation of those models is available on GitHub. ${ }^{81}$</p>
<h1>5 Results and Discussion</h1>
<h3>5.1 Distribution-learning benchmarks</h3>
<p>Table 1 lists the results for the distribution-learning benchmarks. The random sampler model is a useful baseline for comparison because it shows what scores can be expected for good</p>
<p>Table 1: Results of the baseline models for the distribution-learning benchmarks</p>
<table>
<thead>
<tr>
<th>Benchmark</th>
<th>Random sampler</th>
<th>SMILES LSTM</th>
<th>Graph MCTS</th>
<th>AAE</th>
<th>ORGAN</th>
<th>VAE</th>
</tr>
</thead>
<tbody>
<tr>
<td>Validity</td>
<td>1.000</td>
<td>0.959</td>
<td>$\mathbf{1 . 0 0 0}$</td>
<td>0.822</td>
<td>0.379</td>
<td>0.870</td>
</tr>
<tr>
<td>Uniqueness</td>
<td>0.997</td>
<td>$\mathbf{1 . 0 0 0}$</td>
<td>$\mathbf{1 . 0 0 0}$</td>
<td>$\mathbf{1 . 0 0 0}$</td>
<td>0.841</td>
<td>0.999</td>
</tr>
<tr>
<td>Novelty</td>
<td>0.000</td>
<td>0.912</td>
<td>0.994</td>
<td>$\mathbf{0 . 9 9 8}$</td>
<td>0.687</td>
<td>0.974</td>
</tr>
<tr>
<td>KL divergence</td>
<td>0.998</td>
<td>$\mathbf{0 . 9 9 1}$</td>
<td>0.522</td>
<td>0.886</td>
<td>0.267</td>
<td>0.982</td>
</tr>
<tr>
<td>FCD</td>
<td>0.929</td>
<td>$\mathbf{0 . 9 1 3}$</td>
<td>0.015</td>
<td>0.529</td>
<td>0.000</td>
<td>0.863</td>
</tr>
</tbody>
</table>
<p>models on the KL divergence and Fréchet benchmarks. Unsurprisingly, its novelty score is zero since all the molecules it generates are present in the dataset. The SMILES LSTM model sometimes produces invalid molecules. However, they are diverse, as attested by the uniqueness and novelty benchmarks, and closely resemble molecules from ChEMBL. The Graph MCTS model is characterized by a high degree of valid, unique, and novel molecules. Nevertheless, it is not able to reproduce the property distributions of the underlying training set as precisely as the SMILES LSTM model. The AAE model obtains excellent uniqueness and novelty scores. The molecules this model produces are, however, not as close to the dataset as the ones generated by the SMILES LSTM model. ORGAN obtains poor scores for all the distribution-learning tasks. More than half the molecules it generates are invalid, and they do not resemble the training set, as shown by the KL divergence and FCD scores. This might indicate mode collapse, which is a phenomenon often observed when training GANs. The scores of the VAE model are not the best in any categories, but this model consistently produced relatively good scores for all the tasks. Our findings indicate that simpler generative models (VAE, LSTM) are more powerful than more complex models. This is in accordance with similar results in the literature on text and molecule generation. ${ }^{17,82}$</p>
<p>SMILES strings and their depictions for molecules generated by all six baseline models are available in the Supporting Information or on https://benevolent.ai/guacamol.</p>
<h1>5.2 Goal-directed benchmarks</h1>
<p>Table 2 and Figure 2 show the results for the goal-directed benchmarks. The model "Best of Dataset" is a useful baseline, since it actually corresponds to a strategy based on virtual</p>
<p>Table 2: Results of the baseline models for the goal-directed benchmarks</p>
<table>
<thead>
<tr>
<th>Benchmark</th>
<th>Best of dataset</th>
<th>SMILES LSTM</th>
<th>SMILES GA</th>
<th>Graph GA</th>
<th>Graph MCTS</th>
</tr>
</thead>
<tbody>
<tr>
<td>Celecoxib rediscovery</td>
<td>0.505</td>
<td>1.000</td>
<td>0.732</td>
<td>1.000</td>
<td>0.355</td>
</tr>
<tr>
<td>Troglitazone rediscovery</td>
<td>0.419</td>
<td>1.000</td>
<td>0.515</td>
<td>1.000</td>
<td>0.311</td>
</tr>
<tr>
<td>Thiothixene rediscovery</td>
<td>0.456</td>
<td>1.000</td>
<td>0.598</td>
<td>1.000</td>
<td>0.311</td>
</tr>
<tr>
<td>Aripiprazole similarity</td>
<td>0.595</td>
<td>1.000</td>
<td>0.834</td>
<td>1.000</td>
<td>0.380</td>
</tr>
<tr>
<td>Albuterol similarity</td>
<td>0.719</td>
<td>1.000</td>
<td>0.907</td>
<td>1.000</td>
<td>0.749</td>
</tr>
<tr>
<td>Mestranol similarity</td>
<td>0.629</td>
<td>1.000</td>
<td>0.790</td>
<td>1.000</td>
<td>0.402</td>
</tr>
<tr>
<td>$\mathrm{C}<em 24="24">{11} \mathrm{H}</em>$</td>
<td>0.684</td>
<td>0.993</td>
<td>0.829</td>
<td>0.971</td>
<td>0.410</td>
</tr>
<tr>
<td>$\mathrm{C}<em 10="10">{9} \mathrm{H}</em>} \mathrm{~N<em 2="2">{2} \mathrm{O}</em>$} \mathrm{PF}_{2} \mathrm{Cl</td>
<td>0.747</td>
<td>0.879</td>
<td>0.889</td>
<td>0.982</td>
<td>0.631</td>
</tr>
<tr>
<td>Median molecules 1</td>
<td>0.334</td>
<td>0.438</td>
<td>0.334</td>
<td>0.406</td>
<td>0.225</td>
</tr>
<tr>
<td>Median molecules 2</td>
<td>0.351</td>
<td>0.422</td>
<td>0.380</td>
<td>0.432</td>
<td>0.170</td>
</tr>
<tr>
<td>Osimertinib MPO</td>
<td>0.839</td>
<td>0.907</td>
<td>0.886</td>
<td>0.953</td>
<td>0.784</td>
</tr>
<tr>
<td>Fexofenadine MPO</td>
<td>0.817</td>
<td>0.959</td>
<td>0.931</td>
<td>0.998</td>
<td>0.695</td>
</tr>
<tr>
<td>Ranolazine MPO</td>
<td>0.792</td>
<td>0.855</td>
<td>0.881</td>
<td>0.920</td>
<td>0.616</td>
</tr>
<tr>
<td>Perindopril MPO</td>
<td>0.575</td>
<td>0.808</td>
<td>0.661</td>
<td>0.792</td>
<td>0.385</td>
</tr>
<tr>
<td>Amlodipine MPO</td>
<td>0.696</td>
<td>0.894</td>
<td>0.722</td>
<td>0.894</td>
<td>0.533</td>
</tr>
<tr>
<td>Sitagliptin MPO</td>
<td>0.509</td>
<td>0.545</td>
<td>0.689</td>
<td>0.891</td>
<td>0.458</td>
</tr>
<tr>
<td>Zaleplon MPO</td>
<td>0.547</td>
<td>0.669</td>
<td>0.413</td>
<td>0.754</td>
<td>0.488</td>
</tr>
<tr>
<td>Valsartan SMARTS</td>
<td>0.259</td>
<td>0.978</td>
<td>0.552</td>
<td>0.990</td>
<td>0.040</td>
</tr>
<tr>
<td>Deco Hop</td>
<td>0.933</td>
<td>0.996</td>
<td>0.970</td>
<td>1.000</td>
<td>0.590</td>
</tr>
<tr>
<td>Scaffold Hop</td>
<td>0.738</td>
<td>0.998</td>
<td>0.885</td>
<td>1.000</td>
<td>0.478</td>
</tr>
<tr>
<td>Total</td>
<td>12.144</td>
<td>17.340</td>
<td>14.396</td>
<td>17.983</td>
<td>9.009</td>
</tr>
</tbody>
</table>
<p>screening. It illustrates what minimal scores the models must obtain to have an advantage over pure virtual screening. The Graph GA model obtains the best results for most benchmarks. The other GA model, based on SMILES strings, is distinctly better than the "Best of Dataset" baseline. However, especially for the similarity tasks, it scores lower than Graph GA. The SMILES LSTM model nearly performs as well as the Graph GA model, and outperforms it on three benchmarks. The Graph MCTS model performs worse than the
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Results of the Goal-Directed Benchmarks.</p>
<p>baseline selecting the best candidates from ChEMBL.</p>
<p>In addition to these benchmarks, we explored a set of simpler benchmarks, mainly around physicochemical properties (e.g. $\log \mathrm{P}$ and QED), which have been used as objectives in previous work on neural generative models (see Appendix 8.5). However, we found that these benchmarks were not very well suited to differentiate between different models when appropriate baselines are employed.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Quality of the molecules generated in the goal-directed benchmarks, assessed as the ratio of molecules passing the considered quality filters.</p>
<p>To measure the quality of the generated molecules from the goal-directed benchmarks, we gathered and analyzed the top 100 molecules from each task, and checked whether they passed the quality filters described earlier (see Figure 3). We found that 77\% of molecules from the Best of ChEMBL baseline pass the filters. Also, the same ratio is achieved by the LSTM model. The Graph GA, SMILES GA, and Graph MCTS produces $40 \%, 36 \%$, and $22 \%$ high quality molecules, respectively. This higher performance of the neural network based model could be explained by a transfer of the information the LSTM has learned about the general properties of the molecules' distribution to the goal-directed optimization.</p>
<p>Overall, Graph MCTS was found to perform poorly, qualitatively and quantitatively, in the distribution-learning and goal-directed benchmarks. This could indicate that the search paradigm of starting with a single molecule at the root the search tree which is then subsequently refined in a search tree, might not be as well suited for de novo design as approaches based on populations of good compounds. In concordance with previous work, the Graph GA performs extraordinarily well in optimization, due to its flexibility to make very fine grained local edits to the compounds, based on a population. However, again consistent with previous work, the GAs provide molecules of inconsistent quality, as it has no inherent or pre-encoded knowledge about a molecular distribution.</p>
<p>The LSTM-based model performs slightly worse in the goal-directed benchmarks than the Graph GA, however the compound quality is considerably higher, being on the same level as the virtual screening baseline. This can be attributed to the transfer of learned information from the pretraining to the specific optimization problems. Neural models therefore seem to at least partially resolve the weaknesses of virtual screening and GAs, exhibiting both good optimization performance and molecule quality.</p>
<p>SMILES strings and their depictions for molecules generated by the five baseline models for each MPO are available in the Supporting Information or on https://benevolent.ai/guacamol.</p>
<h1>6 Conclusions and Outlook</h1>
<p>Recently, generative models for de novo molecular design based on neural networks have been introduced, and have shown promising results. However, there has been a lack of consistency in the evaluation of such models. The present work addressed this problem by introducing GuacaMol, a framework to quantitatively benchmark models for de novo design. It aims to provide a standardized way of assessing new models, and to improve comparability of the models.</p>
<p>Our framework defines two evaluation dimensions. First, it assesses models for their</p>
<p>ability to learn from a dataset of molecules and to generate novel molecules with similar properties. Second, it evaluates the faculty of generative models to deliver molecules with specific properties, which we encode as molecule scores.</p>
<p>We provided a suite of benchmarks compatible with the evaluation framework. It encompasses a wide range of tasks, designed to assess the flexibility of generative models. The proposed suite comprises 5 benchmarks for the ability of generative models to learn distributions of molecules, 20 optimization benchmarks for the generation of molecules with specific properties, and a metric for compound quality.</p>
<p>We evaluated a series of diverse baseline models. In terms of optimization performance, the best model is a genetic algorithm based on a graph representation of molecules. The second best optimizer is a recurrent neural network model that considers SMILES representations of molecules. Both models achieve similar scores, which indicates that deep learning models can reach the optimization performance of gold-standard discrete algorithms for de novo molecular design. However, when pre-trained on large datasets, the neural model outperforms the genetic algorithm in regards to compound quality.</p>
<p>The evaluation of the proposed benchmark suite for the baseline models pointed out that some benchmark tasks can be too easily solved by most of the models. This indicates the necessity of harder tasks for benchmarking models for de novo molecular design in the future.</p>
<p>An important aspect that will need further focus is the quality of the generated molecules, which is difficult to measure objectively. Furthermore, depending on the requirements, time constraints or sample efficiency of different models can become important, and will require further attention. Also, more work is needed to move neural models from SMILES to graph representations, even though SMILES have been unreasonably effective. ${ }^{17}$ We are confident that the flexible open source framework presented herein will enable and inspire the community to come up with even more challenging benchmarks for de novo design, so that researchers can rigorously assess their models, and computational chemists can confidently</p>
<p>harness models with well-understood strengths and limitations.</p>
<h1>7 Acknowledgments</h1>
<p>The authors thank Nadine Schneider, Mohamed Ahmed, Matthew Sellwood, Joshua Meyers, Mark Rackham, Patrick Riley, and Amir Saffari for helpful discussions, and Kristina Preuer and Günter Klambauer for help with the FCD code.</p>
<h2>8 Appendix</h2>
<h3>8.1 Dataset Generation</h3>
<p>The datasets used for training generative models are derived from the ChEMBL 24 database. ${ }^{69}$ The advantage of ChEMBL is that it contains only molecules that have been synthesized and tested against biological targets. Other datasets, such as ZINC, ${ }^{3}$ contain, at least to some degree, virtual molecules that are likely to be synthesizable, but have not been made yet. Furthermore, ZINC is biased towards smaller and more readily synthesizable molecules, indicated by a larger proportion of molecules containing amide bonds compared to ChEMBL. The QM9 set, ${ }^{83}$ which is a subset of GDB9, ${ }^{6}$ is a completely enumerated dataset, which has shown to be of value in many applications. However, since it contains mostly compounds which have not been made yet, including many molecules with complex annulated ring systems, it is not very well suited to learn representations of drug-like, synthesizable molecules.</p>
<p>To generate the final dataset for the benchmarks, ChEMBL is post-processed by</p>
<ol>
<li>removal of salts;</li>
<li>charge neutralization;</li>
<li>
<p>removal of molecules with SMILES strings longer than 100 characters;</p>
</li>
<li>
<p>removal of molecules containing any element other than $\mathrm{H}, \mathrm{B}, \mathrm{C}, \mathrm{N}, \mathrm{O}, \mathrm{F}, \mathrm{Si}, \mathrm{P}, \mathrm{S}, \mathrm{Cl}$, $\mathrm{Se}, \mathrm{Br}$, and I;</p>
</li>
<li>removal of molecules with a larger ${ }^{84}$ ECFP4 similarity than 0.323 compared to a holdout set consisting of 10 marketed drugs (celecoxib, aripiprazole, cobimetinib, osimertinib, troglitazone, ranolazine, thiothixene, albuterol, fexofenadine, mestranol). This allows us to define similarity benchmarks for targets that are not part of the training set.</li>
</ol>
<p>The post-processed ChEMBL dataset can be downloaded from the Internet. ${ }^{85}$ Additionally, a docker container with version-controlled dependencies to allow for reproducible dataset creation is provided in the guacamol repository. ${ }^{77}$</p>
<h1>8.2 Implementation details: Distribution-learning benchmarks</h1>
<p>Validity The validity score is calculated as the ratio of valid molecules out of 10'000 generated molecules. Molecules are considered to be valid if their SMILES representation can be successfully parsed by RDKit.</p>
<p>Uniqueness To calculate the uniqueness score, molecules are sampled from the generative model until 10'000 valid molecules have been obtained. Duplicate molecules are identified by identical canonical SMILES strings. The score is obtained as the number of different canonical SMILES strings divided by 10'000.</p>
<p>Novelty The novelty score is calculated by generating molecules until 10'000 different canonical SMILES strings are obtained, and computing the ratio of molecules not present in the ChEMBL dataset.</p>
<p>Fréchet ChemNet Distance (FCD) To generate the FCD score, a random subset of 10'000 molecules is selected from the ChEMBL dataset, and the generative model is sampled</p>
<p>until 10'000 valid molecules are obtained. The FCD between both sets of molecules is calculated with the FCD package available on GitHub, ${ }^{86}$ and the final score $S$ is given by</p>
<p>$$
S=\exp (-0.2 \cdot \mathrm{FCD})
$$</p>
<p>KL divergence For this task, the following physicochemical descriptors are calculated with RDKit for both the sampled and the reference set: BertzCT, MolLogP, MolWt, TPSA, NumHAcceptors, NumHDonors, NumRotatableBonds, NumAliphaticRings, and NumAromaticRings. Furthermore, we calculate the distribution of maximum nearest neighbour similarities on ECFP4 fingerprints for both sets. Then, the distribution of these descriptors is computed via kernel density estimation (using the scipy package) for continuous descriptors, or as a histogram for discrete descriptors. The KL divergence $D_{\mathrm{KL}, i}$ is then computed for each descriptor $i$, and is aggregated to a final score $S$ via</p>
<p>$$
S=\frac{1}{k} \sum_{i}^{k} \exp \left(-D_{\mathrm{KL}, i}\right)
$$</p>
<p>where $k$ is the number of descriptors (in our case $k=9$ ).</p>
<h1>8.3 Implementation details: Goal-directed benchmarks</h1>
<h3>8.3.1 Formalism</h3>
<p>In the goal-directed benchmarks discussed in this paper, raw molecular properties rarely correspond to the molecule scores used for optimization. Instead, they are post-processed by modifier functions that give more flexibility in how the final molecule score is computed, and furthermore restrict scores to the interval $[0,1]$. For the benchmarks discussed below, we apply five different types of modifier functions. They are detailed in Appendix 8.3.4.</p>
<p>For many benchmarks the final molecule score corresponds to an average of multiple contributions. For instance, the molecule score for the median molecules benchmarks is</p>
<p>an average of two similarity scores (see below). Depending on the benchmark, the average is calculated as the arithmetic mean or the geometric mean of the contributions. With a geometric mean, all requirements (as given by the individual scoring functions) must be met, at least partly. Otherwise, if one of the contributions has a score of zero, the final molecule score will also be zero. With an arithmetic mean, the requirements are less strict, and a molecule can still achieve a good score if one of its partial scores is zero.</p>
<p>The final benchmark score is calculated as a weighted average of the molecule scores. For most of the benchmarks discussed below, the molecules with the best scores are given a larger weight. This choice reflects the compromise that models should be able to deliver a few molecules with top scores, but that they should also be able to generate many molecules with satisfactory scores. For all the goal-directed benchmarks, we calculate one or several average score for given numbers of top molecules, and then set the benchmark score to be the mean of these average scores. For instance, many benchmarks consider the combination of top-1, top-10 and top-100 scores, in which the benchmark score $S$ is given by</p>
<p>$$
S=\frac{1}{3}\left(s_{1}+\frac{1}{10} \sum_{i=1}^{10} s_{i}+\frac{1}{100} \sum_{i=1}^{100} s_{i}\right)
$$</p>
<p>where $\boldsymbol{s}$ is a 100 -dimensional vector of molecule scores $s_{i}, 1 \leq i \leq 100$, sorted in decreasing order (i.e., $s_{i} \geq s_{j}$ for $\left.i&lt;j\right)$.</p>
<p>The suite of goal-directed benchmarks considered in this work is given in Appendix 8.3.2.
During evaluation, the duration of the benchmark, the number of calls to the scoring function, and the internal similarity of the top molecules are captured in the results of each goal-directed benchmark, in order to allow for more fine-grained analyses.</p>
<h1>8.3.2 List of benchmarks</h1>
<p>All the goal-directed benchmarks are listed in Table 3. More information about the scoring functions and modifiers can be found in Appendices 8.3.3 and 8.3.4, respectively.</p>            </div>
        </div>

    </div>
</body>
</html>
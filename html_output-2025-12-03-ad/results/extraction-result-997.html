<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-997 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-997</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-997</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-20.html">extraction-schema-20</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <p><strong>Paper ID:</strong> paper-4e67ad5f219f24ed81974763c12eb0ad316b2765</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/4e67ad5f219f24ed81974763c12eb0ad316b2765" target="_blank">Causal discovery in the presence of missing data</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Artificial Intelligence and Statistics</p>
                <p><strong>Paper TL;DR:</strong> Missing data are ubiquitous in many domains such as healthcare and when these data entries are not missing completely at random, the (conditional) independence relations in the observed data may be non-conditional.</p>
                <p><strong>Paper Abstract:</strong> Missing data are ubiquitous in many domains such as healthcare. When these data entries are not missing completely at random, the (conditional) independence relations in the observed data may be di ...</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e997.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e997.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MVPC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Missing-Value PC</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A correction-based extension of the PC algorithm that detects and removes extraneous edges caused by missing data (MCAR, MAR, MNAR) using targeted corrections rather than correcting all CI tests; implements two correction strategies (PermC and DRW).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Missing-Value PC (MVPC)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>MVPC runs a test-wise-deletion PC skeleton search, augments the graph with missingness indicators R, detects candidate extraneous edges (edges between variables sharing a common adjacent variable or missingness indicator), detects direct causes of missingness indicators by CI testing between R_i and variables, and then applies targeted correction procedures to candidate edges: Permutation-based Correction (PermC) when conditions allow, or Density Ratio Weighted (DRW) correction otherwise. After corrections it orients edges using the standard PC orientation rules.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Synthetic datasets; Neuropathic pain diagnosis simulator (virtual lab); CogUSA survey; Achilles Tendon Rupture (ATR) study</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Evaluated on (1) large-scale synthetic Gaussian and binary datasets (controlled MCAR/MAR/MNAR mechanisms), (2) a domain-specific interactive neuropathic pain diagnosis simulator (222 binary variables, clinician-informed causal graph) acting as a virtual lab to generate MNAR data, and (3) real-world healthcare datasets (survey and clinical registry) with natural missingness; environments are observational (not interactive in the sense of active interventions) except the simulator which can generate varied missingness scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Explicit detection of candidate spurious edges induced by missingness indicators and their ancestors; targeted removal by either (a) reconstructing virtual complete-data samples via permutation+regression (PermC) to test CI in the recovered joint, or (b) reweighting observed (test-wise-deleted) samples by estimated density ratios (DRW) to recover P(V) and conducting weighted CI tests.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Spurious associations due to missingness-driven selection bias (MAR, MNAR), extraneous edges caused by missingness indicators being descendants of direct common effects, selection bias from test-wise deletion; measurement missingness leading to dependence distortions.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Candidate extraneous edges detected by graph topology after TD-PC: any adjacent pair that share at least one common adjacent substantive variable or missingness indicator is flagged; direct causes of R_i detected by applying TD-PC CI tests between R_i and other substantive variables (variables adjacent to R_i are taken as direct causes under assumptions).</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>When PermC is inapplicable, DRW reweights test-wise-deleted data by density ratio weights: P(V_a) is reconstructed as P(V_a|R=0) times product of beta_{Pa(R_i)} and constant c, where beta terms (density ratios) are estimated with kernel density estimation and used as per-sample weights for CI testing (weighted G^2 or weighted partial-correlation).</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Refutation by retesting conditional independence on corrected data: CI tests run on (a) generated virtual complete-data samples (PermC) or (b) reweighted observed samples approximating the full-data joint (DRW); if dependence vanishes after correction, the original edge is considered extraneous and removed.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>On synthetic MAR/MNAR Gaussian experiments MVPC (PermC and DRW variants) substantially outperformed test-wise-deletion PC (TD-PC), achieving lower Structural Hamming Distance and higher F1, and converging towards a 'target' (PC on matched-MCAR data) with increasing sample size; MVPC-PermC is more data-efficient than MVPC-DRW. On the neuropathic pain simulator (binary MNAR, n=1000) MVPC (DRW for binary) achieved Cau_acc=0.045, Recall=0.043, Precision=0.45, F1=0.078, close to PC-ideal/PC-target and better than TD-PC (Cau_acc=0.033, Recall=0.025, Precision=0.559, F1=0.047).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>TD-PC (test-wise deletion PC) often produced extraneous edges under MAR and MNAR; in experiments TD-PC showed worse SHD and F1 than MVPC and in some settings diverged asymptotically depending on missingness generation. In the neuropathic simulator TD-PC had lower recall and F1 than MVPC (see values above).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td>In experiments up to 10 missingness indicators were used, with at most 5 indicators creating the problematic extraneous-edge scenario (per Proposition 2); evaluations varied setups with 10–20 substantive variables up to 100.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Targeted correction is sufficient: most erroneous CI decisions from test-wise deletion occur only for a small subset of CI tests, so correcting only candidate edges yields accurate recovery; PermC (regression + permutation to reconstruct P(V)) handles many MAR/MNAR cases and is more data-efficient, while DRW (density-ratio reweighting) handles more general MNAR cases but is less data-efficient and sensitive to the dimensionality/number of parents of missingness indicators; MVPC outperforms TD-PC on synthetic and real/simulated healthcare data, successfully removing extraneous edges induced by missingness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Causal discovery in the presence of missing data', 'publication_date_yy_mm': '2018-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e997.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e997.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PermC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Permutation-based Correction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A correction method that reconstructs virtual complete-data samples by learning conditional models P(X,Y,Z | W, R=0) under test-wise deletion, then permuting/shuffling samples of W to approximate P(W) and generating virtual X,Y,Z to test CI in the recovered joint distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Permutation-based Correction (PermC)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Fit regression (continuous) or conditional table estimates (binary) of substantive variables on the direct causes W of missingness indicators using test-wise-deleted data; shuffle observed W values to break dependence with R (making P(W^S|R=0)=P(W^S)), sample residuals, and generate virtual X,Y,Z = f(W^S)+residual to approximate draws from the full-data joint P(X,Y,Z). Conduct CI tests on the generated virtual data to decide edge removal.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Synthetic MAR/MNAR datasets; validated on neuropathic pain simulator and synthetic experiments</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Applied in observational synthetic and simulator-generated datasets where direct causes of missingness (W) are observed and have sufficient complete data; not an active experimenter but a data reconstruction technique.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Reconstruction of full-data joint to remove spurious associations caused by conditioning on or selecting by missingness indicators; implicitly performs 'de-selection' of missingness-induced spurious dependencies by simulating from P(V).</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Selection bias due to MAR/MNAR where missingness indicators are descendants of common effects; spurious conditional dependence introduced by conditioning on R or by test-wise deletion.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Relies on MVPC's candidate-edge detection (topological criteria) and identification of direct causes W of R via CI tests; PermC is invoked on flagged CI tests/edges.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Not a weighting scheme: reconstructs virtual samples rather than downweighting; uses permutation of W to match marginal P(W) and residual injection to simulate full-data draws.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Refutes spurious edges by showing that CI relationships in the generated virtual full-data samples match the true complete-data CI: if X and Y are independent given Z in generated data, the original edge is removed.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>PermC is more data-efficient in continuous experiments, converging faster than DRW and achieving SHD and F1 close to the 'target' PC on MCAR-matched data; performs well on many MNAR scenarios that satisfy PermC conditions (Conditions (i) and (ii) in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Compared to TD-PC, PermC-corrected MVPC yields much lower SHD and higher F1; in MNAR cases not satisfying PermC conditions, PermC may be inapplicable and performance degrades relative to DRW.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td>Evaluations used up to 5 missingness indicators creating the extraneous-edge scenario; PermC applied per affected CI test.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>PermC successfully reconstructs P(V) and removes extraneous edges when the direct causes W of missingness are observed and satisfy conditional-independence conditions (R ⟂ {X,Y,Z} | W and related constraints); it's computationally efficient and data-efficient but requires that the structural conditions for validity hold.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Causal discovery in the presence of missing data', 'publication_date_yy_mm': '2018-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e997.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e997.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DRW</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Density Ratio Weighted correction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A correction approach that reconstructs the full-data joint distribution by reweighting test-wise-deleted samples using estimated density-ratio weights derived from missingness mechanisms, enabling CI tests on an approximation of P(V).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Density Ratio Weighted correction (DRW)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Compute P(V_a) = P(V_a | R=0) * c * ∏_i β_{Pa(R_i)} where β are density ratios of parent-value distributions; estimate β terms using kernel density estimation (continuous) or counts (binary) and compute per-sample weights; perform weighted CI tests (e.g., weighted partial-correlation or weighted G^2) on the reweighted test-wise-deleted data to recover CI relations in the full-data joint.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Synthetic datasets, neuropathic pain simulator (binary experiments), and real datasets in MVPC evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Applied where PermC conditions fail (e.g., more complex MNAR, parents have missingness), reconstructs the full-data joint via density-ratio reweighting; requires density estimation and enough data to estimate ratios.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Downweighting spurious selection-induced signals by reweighting observed samples to approximate the population (full-data) distribution, thereby removing biases that create spurious CI/associations.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Selection bias from MNAR (complex missingness), dependencies introduced by conditioning on R, biases from multiple parents of missingness indicators.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Invoked on candidate edges flagged by MVPC; depends on prior detection of Pa(R) (direct causes) using CI tests and topology.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Per-sample weights = c * ∏ β_{Pa(R_i)} computed from kernel density estimates (continuous) or inverse conditional probabilities (binary); weighted CI tests use these weights to approximate testing under P(V).</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Refutes spurious causal connections when the weighted CI tests indicate independence in the reconstructed full-data distribution, leading to removal of edges flagged as extraneous.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>DRW recovers correct CI relations in cases PermC cannot cover, but is less data-efficient and sensitive to dimensionality; in experiments MVPC-DRW eventually outperformed TD-PC but required larger sample sizes (e.g., binary case required >500k samples to exceed TD-PC in some settings).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Unweighted TD-PC on the same data produced more extraneous edges and worse SHD/F1 metrics; DRW's improvement depends strongly on accurate density-ratio estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td>Used when multiple missingness indicators (up to 10 in some experiments) and complex parent structures exist; sensitivity grows with number of parents (performance degraded when parents >3–4 due to density estimation limits).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>DRW is a general consistent correction for MNAR where PermC conditions fail, reconstructing P(V) via density-ratio reweighting; practically it is limited by density estimation in higher dimensions and requires larger sample sizes, but successfully removes selection-induced spurious edges when estimators are accurate.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Causal discovery in the presence of missing data', 'publication_date_yy_mm': '2018-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e997.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e997.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TD-PC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Test-wise Deletion PC</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A naive extension of the PC algorithm to missing data that performs CI tests using only records where the variables involved in the current test are observed (test-wise deletion); more data-efficient than listwise deletion but can produce extraneous edges under MAR/MNAR.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Test-wise Deletion PC (TD-PC)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Apply the standard PC skeleton and orientation procedure but for each CI test delete records that have missing values for the variables involved in that specific test; no explicit correction for missingness bias is applied.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Synthetic datasets; neuropathic pain simulator; CogUSA; ATR study (baseline comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Used as a baseline causal discovery procedure on observed datasets with missing values; not designed to handle MNAR-induced selection biases beyond using all available observed pairs/tuples for each CI test.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>None explicitly handled; liable to produce spurious associations (extraneous edges) when missingness indicators are correlated with substantive variables (MAR/MNAR) or are descendants of direct common effects.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Often produced extraneous edges in MAR and MNAR scenarios; in experiments had worse SHD and F1 than MVPC and could be asymptotically wrong depending on missingness generation; in neuropathic simulator TD-PC: Cau_acc=0.033, Recall=0.025, Precision=0.559, F1=0.047.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td>Affected by up to 5 problematic missingness indicators in constructed experiments; performance degrades as number of problematic indicators increases.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>TD-PC is data-efficient but can be biased: it will not miss true edges but may add extraneous edges when missingness mechanisms create conditional dependence in the observed (deleted) data; it serves as the baseline for which MVPC provides targeted corrections.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Causal discovery in the presence of missing data', 'publication_date_yy_mm': '2018-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e997.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e997.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IPW (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Inverse Probability Weighting (for CI tests)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reweighting approach that uses the inverse probability of missingness to adjust CI tests in the presence of missing data when the missingness model is known; cited as an approach that assumes known missingness models and may be unrealistic in practice.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Inverse Probability Weighting (IPW) for CI tests</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Estimate the probability of an observation being observed (propensity for being complete) and weight observed samples by the inverse of these probabilities to correct for bias due to missingness when performing CI tests; this requires knowing or estimating the missingness mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Mentioned in related work (Gain and Shpitser, 2018) as a CI-test correction technique</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Discussed in related work as applicable when the missing-data model is known; not used in experiments in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Reweighting observations by inverse observation probability (importance weighting) to counteract selection bias caused by missingness.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Selection bias due to missingness (MCAR/MAR/MNAR when model known); corrects for missing-data-induced spurious associations if mechanism is known.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Weights = 1 / P(observed | covariates); downweights overrepresented observed patterns and upweights underrepresented ones.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Refutation performed by running CI tests on IPW-weighted data to see if associations persist after reweighting.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Paper notes IPW (Gain & Shpitser) can be used to correct CI tests if the missingness model is known, but that such assumption is often unrealistic; MVPC instead focuses on methods that do not require full prior knowledge of the missingness model.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Causal discovery in the presence of missing data', 'publication_date_yy_mm': '2018-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e997.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e997.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Test-wise deletion FCI (Strobl)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Test-wise Deletion FCI (as in Strobl et al., 2017)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of test-wise deletion to the FCI algorithm to handle MNAR by treating missingness as a form of selection bias and performing CI tests with test-wise deletion; shown to be sound for estimating PAG including missingness effects.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Test-wise Deletion FCI (Strobl et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Apply FCI with CI tests performed on records that are complete for the current set of tested variables (test-wise deletion); noted as a way to incorporate the effect of missingness indicators into the PAG, especially when modeling missingness as selection bias.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Mentioned as related work and baseline approach in literature</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Proposed in related work to handle MNAR by treating it as selection bias and using test-wise deletion with FCI; not directly used in MVPC experiments except as a referenced approach.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Selection bias modeled as missingness treated like selection; may still not utilize information to reconstruct full-data CI relations.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Referenced as an approach that is still sound for estimating PAG including missingness effects under certain assumptions, but differs from MVPC which aims to recover CI relations of substantive variables (not the missingness) by reconstructing or reweighting to approximate the full-data joint.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Causal discovery in the presence of missing data', 'publication_date_yy_mm': '2018-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e997.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e997.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Missingness graphs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Missingness graphs (m-graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Causal graphical models that explicitly include substantive variables, proxy variables, missingness indicators R, and (optionally) unobserved nodes, used to represent and reason about MCAR/MAR/MNAR and recoverability of distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Missingness graphs (Mohan et al. style)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Augment the substantive-variable causal DAG with missingness indicators R and proxy variables V*, using d-separation in this augmented graph to classify missingness as MCAR, MAR, or MNAR, and to derive conditions for recoverability of joint/conditional distributions and for when corrections are required for CI tests.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Conceptual modeling used throughout analyses and algorithm derivation</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A modeling construct (not an experimental environment) used to represent missingness mechanisms and to derive propositions about when TD-PC will produce extraneous edges and when corrections (PermC/DRW) are needed.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Models selection bias and the ways missingness can induce spurious dependencies (e.g., missingness indicators as descendants of colliders or common effects).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Used to reason analytically about when observed CI relations imply or fail to imply complete-data CI relations; guides detection of candidate extra edges (Propositions 1 and 2).</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Missingness graphs formalize when missingness will bias CI tests: TD-PC is correct under MCAR but can produce extraneous edges under MAR/MNAR; Propositions identify that extraneous edges arise when at least one missingness indicator is a (descendant of a) direct common effect of two substantive variables, motivating targeted correction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Causal discovery in the presence of missing data', 'publication_date_yy_mm': '2018-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Recovering from Missing Data: The View from Causality <em>(Rating: 2)</em></li>
                <li>Graphical Models for Recovering Joint Distributions and Causal Effects with Missing Data <em>(Rating: 2)</em></li>
                <li>Testable Implications of Missingness Graphs <em>(Rating: 2)</em></li>
                <li>Representing Missing Data as a Causal Inference Problem <em>(Rating: 2)</em></li>
                <li>Inverse probability weighting for conditional independence testing under missingness (Gain and Shpitser) <em>(Rating: 1)</em></li>
                <li>Causal discovery with missing data as selection bias (Strobl et al., 2017) <em>(Rating: 2)</em></li>
                <li>On Identification of Causal Effects in Missing Data Models (Saadati and Tian, 2019) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-997",
    "paper_id": "paper-4e67ad5f219f24ed81974763c12eb0ad316b2765",
    "extraction_schema_id": "extraction-schema-20",
    "extracted_data": [
        {
            "name_short": "MVPC",
            "name_full": "Missing-Value PC",
            "brief_description": "A correction-based extension of the PC algorithm that detects and removes extraneous edges caused by missing data (MCAR, MAR, MNAR) using targeted corrections rather than correcting all CI tests; implements two correction strategies (PermC and DRW).",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Missing-Value PC (MVPC)",
            "method_description": "MVPC runs a test-wise-deletion PC skeleton search, augments the graph with missingness indicators R, detects candidate extraneous edges (edges between variables sharing a common adjacent variable or missingness indicator), detects direct causes of missingness indicators by CI testing between R_i and variables, and then applies targeted correction procedures to candidate edges: Permutation-based Correction (PermC) when conditions allow, or Density Ratio Weighted (DRW) correction otherwise. After corrections it orients edges using the standard PC orientation rules.",
            "environment_name": "Synthetic datasets; Neuropathic pain diagnosis simulator (virtual lab); CogUSA survey; Achilles Tendon Rupture (ATR) study",
            "environment_description": "Evaluated on (1) large-scale synthetic Gaussian and binary datasets (controlled MCAR/MAR/MNAR mechanisms), (2) a domain-specific interactive neuropathic pain diagnosis simulator (222 binary variables, clinician-informed causal graph) acting as a virtual lab to generate MNAR data, and (3) real-world healthcare datasets (survey and clinical registry) with natural missingness; environments are observational (not interactive in the sense of active interventions) except the simulator which can generate varied missingness scenarios.",
            "handles_distractors": true,
            "distractor_handling_technique": "Explicit detection of candidate spurious edges induced by missingness indicators and their ancestors; targeted removal by either (a) reconstructing virtual complete-data samples via permutation+regression (PermC) to test CI in the recovered joint, or (b) reweighting observed (test-wise-deleted) samples by estimated density ratios (DRW) to recover P(V) and conducting weighted CI tests.",
            "spurious_signal_types": "Spurious associations due to missingness-driven selection bias (MAR, MNAR), extraneous edges caused by missingness indicators being descendants of direct common effects, selection bias from test-wise deletion; measurement missingness leading to dependence distortions.",
            "detection_method": "Candidate extraneous edges detected by graph topology after TD-PC: any adjacent pair that share at least one common adjacent substantive variable or missingness indicator is flagged; direct causes of R_i detected by applying TD-PC CI tests between R_i and other substantive variables (variables adjacent to R_i are taken as direct causes under assumptions).",
            "downweighting_method": "When PermC is inapplicable, DRW reweights test-wise-deleted data by density ratio weights: P(V_a) is reconstructed as P(V_a|R=0) times product of beta_{Pa(R_i)} and constant c, where beta terms (density ratios) are estimated with kernel density estimation and used as per-sample weights for CI testing (weighted G^2 or weighted partial-correlation).",
            "refutation_method": "Refutation by retesting conditional independence on corrected data: CI tests run on (a) generated virtual complete-data samples (PermC) or (b) reweighted observed samples approximating the full-data joint (DRW); if dependence vanishes after correction, the original edge is considered extraneous and removed.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "On synthetic MAR/MNAR Gaussian experiments MVPC (PermC and DRW variants) substantially outperformed test-wise-deletion PC (TD-PC), achieving lower Structural Hamming Distance and higher F1, and converging towards a 'target' (PC on matched-MCAR data) with increasing sample size; MVPC-PermC is more data-efficient than MVPC-DRW. On the neuropathic pain simulator (binary MNAR, n=1000) MVPC (DRW for binary) achieved Cau_acc=0.045, Recall=0.043, Precision=0.45, F1=0.078, close to PC-ideal/PC-target and better than TD-PC (Cau_acc=0.033, Recall=0.025, Precision=0.559, F1=0.047).",
            "performance_without_robustness": "TD-PC (test-wise deletion PC) often produced extraneous edges under MAR and MNAR; in experiments TD-PC showed worse SHD and F1 than MVPC and in some settings diverged asymptotically depending on missingness generation. In the neuropathic simulator TD-PC had lower recall and F1 than MVPC (see values above).",
            "has_ablation_study": true,
            "number_of_distractors": "In experiments up to 10 missingness indicators were used, with at most 5 indicators creating the problematic extraneous-edge scenario (per Proposition 2); evaluations varied setups with 10–20 substantive variables up to 100.",
            "key_findings": "Targeted correction is sufficient: most erroneous CI decisions from test-wise deletion occur only for a small subset of CI tests, so correcting only candidate edges yields accurate recovery; PermC (regression + permutation to reconstruct P(V)) handles many MAR/MNAR cases and is more data-efficient, while DRW (density-ratio reweighting) handles more general MNAR cases but is less data-efficient and sensitive to the dimensionality/number of parents of missingness indicators; MVPC outperforms TD-PC on synthetic and real/simulated healthcare data, successfully removing extraneous edges induced by missingness.",
            "uuid": "e997.0",
            "source_info": {
                "paper_title": "Causal discovery in the presence of missing data",
                "publication_date_yy_mm": "2018-07"
            }
        },
        {
            "name_short": "PermC",
            "name_full": "Permutation-based Correction",
            "brief_description": "A correction method that reconstructs virtual complete-data samples by learning conditional models P(X,Y,Z | W, R=0) under test-wise deletion, then permuting/shuffling samples of W to approximate P(W) and generating virtual X,Y,Z to test CI in the recovered joint distribution.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Permutation-based Correction (PermC)",
            "method_description": "Fit regression (continuous) or conditional table estimates (binary) of substantive variables on the direct causes W of missingness indicators using test-wise-deleted data; shuffle observed W values to break dependence with R (making P(W^S|R=0)=P(W^S)), sample residuals, and generate virtual X,Y,Z = f(W^S)+residual to approximate draws from the full-data joint P(X,Y,Z). Conduct CI tests on the generated virtual data to decide edge removal.",
            "environment_name": "Synthetic MAR/MNAR datasets; validated on neuropathic pain simulator and synthetic experiments",
            "environment_description": "Applied in observational synthetic and simulator-generated datasets where direct causes of missingness (W) are observed and have sufficient complete data; not an active experimenter but a data reconstruction technique.",
            "handles_distractors": true,
            "distractor_handling_technique": "Reconstruction of full-data joint to remove spurious associations caused by conditioning on or selecting by missingness indicators; implicitly performs 'de-selection' of missingness-induced spurious dependencies by simulating from P(V).",
            "spurious_signal_types": "Selection bias due to MAR/MNAR where missingness indicators are descendants of common effects; spurious conditional dependence introduced by conditioning on R or by test-wise deletion.",
            "detection_method": "Relies on MVPC's candidate-edge detection (topological criteria) and identification of direct causes W of R via CI tests; PermC is invoked on flagged CI tests/edges.",
            "downweighting_method": "Not a weighting scheme: reconstructs virtual samples rather than downweighting; uses permutation of W to match marginal P(W) and residual injection to simulate full-data draws.",
            "refutation_method": "Refutes spurious edges by showing that CI relationships in the generated virtual full-data samples match the true complete-data CI: if X and Y are independent given Z in generated data, the original edge is removed.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "PermC is more data-efficient in continuous experiments, converging faster than DRW and achieving SHD and F1 close to the 'target' PC on MCAR-matched data; performs well on many MNAR scenarios that satisfy PermC conditions (Conditions (i) and (ii) in paper).",
            "performance_without_robustness": "Compared to TD-PC, PermC-corrected MVPC yields much lower SHD and higher F1; in MNAR cases not satisfying PermC conditions, PermC may be inapplicable and performance degrades relative to DRW.",
            "has_ablation_study": true,
            "number_of_distractors": "Evaluations used up to 5 missingness indicators creating the extraneous-edge scenario; PermC applied per affected CI test.",
            "key_findings": "PermC successfully reconstructs P(V) and removes extraneous edges when the direct causes W of missingness are observed and satisfy conditional-independence conditions (R ⟂ {X,Y,Z} | W and related constraints); it's computationally efficient and data-efficient but requires that the structural conditions for validity hold.",
            "uuid": "e997.1",
            "source_info": {
                "paper_title": "Causal discovery in the presence of missing data",
                "publication_date_yy_mm": "2018-07"
            }
        },
        {
            "name_short": "DRW",
            "name_full": "Density Ratio Weighted correction",
            "brief_description": "A correction approach that reconstructs the full-data joint distribution by reweighting test-wise-deleted samples using estimated density-ratio weights derived from missingness mechanisms, enabling CI tests on an approximation of P(V).",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Density Ratio Weighted correction (DRW)",
            "method_description": "Compute P(V_a) = P(V_a | R=0) * c * ∏_i β_{Pa(R_i)} where β are density ratios of parent-value distributions; estimate β terms using kernel density estimation (continuous) or counts (binary) and compute per-sample weights; perform weighted CI tests (e.g., weighted partial-correlation or weighted G^2) on the reweighted test-wise-deleted data to recover CI relations in the full-data joint.",
            "environment_name": "Synthetic datasets, neuropathic pain simulator (binary experiments), and real datasets in MVPC evaluation",
            "environment_description": "Applied where PermC conditions fail (e.g., more complex MNAR, parents have missingness), reconstructs the full-data joint via density-ratio reweighting; requires density estimation and enough data to estimate ratios.",
            "handles_distractors": true,
            "distractor_handling_technique": "Downweighting spurious selection-induced signals by reweighting observed samples to approximate the population (full-data) distribution, thereby removing biases that create spurious CI/associations.",
            "spurious_signal_types": "Selection bias from MNAR (complex missingness), dependencies introduced by conditioning on R, biases from multiple parents of missingness indicators.",
            "detection_method": "Invoked on candidate edges flagged by MVPC; depends on prior detection of Pa(R) (direct causes) using CI tests and topology.",
            "downweighting_method": "Per-sample weights = c * ∏ β_{Pa(R_i)} computed from kernel density estimates (continuous) or inverse conditional probabilities (binary); weighted CI tests use these weights to approximate testing under P(V).",
            "refutation_method": "Refutes spurious causal connections when the weighted CI tests indicate independence in the reconstructed full-data distribution, leading to removal of edges flagged as extraneous.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "DRW recovers correct CI relations in cases PermC cannot cover, but is less data-efficient and sensitive to dimensionality; in experiments MVPC-DRW eventually outperformed TD-PC but required larger sample sizes (e.g., binary case required &gt;500k samples to exceed TD-PC in some settings).",
            "performance_without_robustness": "Unweighted TD-PC on the same data produced more extraneous edges and worse SHD/F1 metrics; DRW's improvement depends strongly on accurate density-ratio estimation.",
            "has_ablation_study": true,
            "number_of_distractors": "Used when multiple missingness indicators (up to 10 in some experiments) and complex parent structures exist; sensitivity grows with number of parents (performance degraded when parents &gt;3–4 due to density estimation limits).",
            "key_findings": "DRW is a general consistent correction for MNAR where PermC conditions fail, reconstructing P(V) via density-ratio reweighting; practically it is limited by density estimation in higher dimensions and requires larger sample sizes, but successfully removes selection-induced spurious edges when estimators are accurate.",
            "uuid": "e997.2",
            "source_info": {
                "paper_title": "Causal discovery in the presence of missing data",
                "publication_date_yy_mm": "2018-07"
            }
        },
        {
            "name_short": "TD-PC",
            "name_full": "Test-wise Deletion PC",
            "brief_description": "A naive extension of the PC algorithm to missing data that performs CI tests using only records where the variables involved in the current test are observed (test-wise deletion); more data-efficient than listwise deletion but can produce extraneous edges under MAR/MNAR.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Test-wise Deletion PC (TD-PC)",
            "method_description": "Apply the standard PC skeleton and orientation procedure but for each CI test delete records that have missing values for the variables involved in that specific test; no explicit correction for missingness bias is applied.",
            "environment_name": "Synthetic datasets; neuropathic pain simulator; CogUSA; ATR study (baseline comparisons)",
            "environment_description": "Used as a baseline causal discovery procedure on observed datasets with missing values; not designed to handle MNAR-induced selection biases beyond using all available observed pairs/tuples for each CI test.",
            "handles_distractors": false,
            "distractor_handling_technique": null,
            "spurious_signal_types": "None explicitly handled; liable to produce spurious associations (extraneous edges) when missingness indicators are correlated with substantive variables (MAR/MNAR) or are descendants of direct common effects.",
            "detection_method": null,
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": "Often produced extraneous edges in MAR and MNAR scenarios; in experiments had worse SHD and F1 than MVPC and could be asymptotically wrong depending on missingness generation; in neuropathic simulator TD-PC: Cau_acc=0.033, Recall=0.025, Precision=0.559, F1=0.047.",
            "has_ablation_study": false,
            "number_of_distractors": "Affected by up to 5 problematic missingness indicators in constructed experiments; performance degrades as number of problematic indicators increases.",
            "key_findings": "TD-PC is data-efficient but can be biased: it will not miss true edges but may add extraneous edges when missingness mechanisms create conditional dependence in the observed (deleted) data; it serves as the baseline for which MVPC provides targeted corrections.",
            "uuid": "e997.3",
            "source_info": {
                "paper_title": "Causal discovery in the presence of missing data",
                "publication_date_yy_mm": "2018-07"
            }
        },
        {
            "name_short": "IPW (mentioned)",
            "name_full": "Inverse Probability Weighting (for CI tests)",
            "brief_description": "A reweighting approach that uses the inverse probability of missingness to adjust CI tests in the presence of missing data when the missingness model is known; cited as an approach that assumes known missingness models and may be unrealistic in practice.",
            "citation_title": "",
            "mention_or_use": "mention",
            "method_name": "Inverse Probability Weighting (IPW) for CI tests",
            "method_description": "Estimate the probability of an observation being observed (propensity for being complete) and weight observed samples by the inverse of these probabilities to correct for bias due to missingness when performing CI tests; this requires knowing or estimating the missingness mechanism.",
            "environment_name": "Mentioned in related work (Gain and Shpitser, 2018) as a CI-test correction technique",
            "environment_description": "Discussed in related work as applicable when the missing-data model is known; not used in experiments in this paper.",
            "handles_distractors": true,
            "distractor_handling_technique": "Reweighting observations by inverse observation probability (importance weighting) to counteract selection bias caused by missingness.",
            "spurious_signal_types": "Selection bias due to missingness (MCAR/MAR/MNAR when model known); corrects for missing-data-induced spurious associations if mechanism is known.",
            "detection_method": null,
            "downweighting_method": "Weights = 1 / P(observed | covariates); downweights overrepresented observed patterns and upweights underrepresented ones.",
            "refutation_method": "Refutation performed by running CI tests on IPW-weighted data to see if associations persist after reweighting.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Paper notes IPW (Gain & Shpitser) can be used to correct CI tests if the missingness model is known, but that such assumption is often unrealistic; MVPC instead focuses on methods that do not require full prior knowledge of the missingness model.",
            "uuid": "e997.4",
            "source_info": {
                "paper_title": "Causal discovery in the presence of missing data",
                "publication_date_yy_mm": "2018-07"
            }
        },
        {
            "name_short": "Test-wise deletion FCI (Strobl)",
            "name_full": "Test-wise Deletion FCI (as in Strobl et al., 2017)",
            "brief_description": "Application of test-wise deletion to the FCI algorithm to handle MNAR by treating missingness as a form of selection bias and performing CI tests with test-wise deletion; shown to be sound for estimating PAG including missingness effects.",
            "citation_title": "",
            "mention_or_use": "mention",
            "method_name": "Test-wise Deletion FCI (Strobl et al.)",
            "method_description": "Apply FCI with CI tests performed on records that are complete for the current set of tested variables (test-wise deletion); noted as a way to incorporate the effect of missingness indicators into the PAG, especially when modeling missingness as selection bias.",
            "environment_name": "Mentioned as related work and baseline approach in literature",
            "environment_description": "Proposed in related work to handle MNAR by treating it as selection bias and using test-wise deletion with FCI; not directly used in MVPC experiments except as a referenced approach.",
            "handles_distractors": false,
            "distractor_handling_technique": null,
            "spurious_signal_types": "Selection bias modeled as missingness treated like selection; may still not utilize information to reconstruct full-data CI relations.",
            "detection_method": null,
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Referenced as an approach that is still sound for estimating PAG including missingness effects under certain assumptions, but differs from MVPC which aims to recover CI relations of substantive variables (not the missingness) by reconstructing or reweighting to approximate the full-data joint.",
            "uuid": "e997.5",
            "source_info": {
                "paper_title": "Causal discovery in the presence of missing data",
                "publication_date_yy_mm": "2018-07"
            }
        },
        {
            "name_short": "Missingness graphs",
            "name_full": "Missingness graphs (m-graphs)",
            "brief_description": "Causal graphical models that explicitly include substantive variables, proxy variables, missingness indicators R, and (optionally) unobserved nodes, used to represent and reason about MCAR/MAR/MNAR and recoverability of distributions.",
            "citation_title": "",
            "mention_or_use": "use",
            "method_name": "Missingness graphs (Mohan et al. style)",
            "method_description": "Augment the substantive-variable causal DAG with missingness indicators R and proxy variables V*, using d-separation in this augmented graph to classify missingness as MCAR, MAR, or MNAR, and to derive conditions for recoverability of joint/conditional distributions and for when corrections are required for CI tests.",
            "environment_name": "Conceptual modeling used throughout analyses and algorithm derivation",
            "environment_description": "A modeling construct (not an experimental environment) used to represent missingness mechanisms and to derive propositions about when TD-PC will produce extraneous edges and when corrections (PermC/DRW) are needed.",
            "handles_distractors": null,
            "distractor_handling_technique": null,
            "spurious_signal_types": "Models selection bias and the ways missingness can induce spurious dependencies (e.g., missingness indicators as descendants of colliders or common effects).",
            "detection_method": "Used to reason analytically about when observed CI relations imply or fail to imply complete-data CI relations; guides detection of candidate extra edges (Propositions 1 and 2).",
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": false,
            "number_of_distractors": null,
            "key_findings": "Missingness graphs formalize when missingness will bias CI tests: TD-PC is correct under MCAR but can produce extraneous edges under MAR/MNAR; Propositions identify that extraneous edges arise when at least one missingness indicator is a (descendant of a) direct common effect of two substantive variables, motivating targeted correction.",
            "uuid": "e997.6",
            "source_info": {
                "paper_title": "Causal discovery in the presence of missing data",
                "publication_date_yy_mm": "2018-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Recovering from Missing Data: The View from Causality",
            "rating": 2
        },
        {
            "paper_title": "Graphical Models for Recovering Joint Distributions and Causal Effects with Missing Data",
            "rating": 2
        },
        {
            "paper_title": "Testable Implications of Missingness Graphs",
            "rating": 2
        },
        {
            "paper_title": "Representing Missing Data as a Causal Inference Problem",
            "rating": 2
        },
        {
            "paper_title": "Inverse probability weighting for conditional independence testing under missingness (Gain and Shpitser)",
            "rating": 1
        },
        {
            "paper_title": "Causal discovery with missing data as selection bias (Strobl et al., 2017)",
            "rating": 2
        },
        {
            "paper_title": "On Identification of Causal Effects in Missing Data Models (Saadati and Tian, 2019)",
            "rating": 1
        }
    ],
    "cost": 0.0178115,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Causal discovery in the presence of missing data</h1>
<p>Ruibo Tu<em><br>RUIBO@KTH.SE<br>KTH Royal Institute of Technology<br>Stockholm, Sweden<br>Kun Zhang</em><br>KUNZ1@CMU.EDU<br>Carnegie Mellon University<br>Pittsburgh, PA, USA<br>Paul Ackermann<br>PAUL.ACKERMANN@SLL.SE<br>Karolinska Institute<br>Stockholm, Sweden<br>Bo Christer Bertilson<br>BO.BERTILSON@KI.SE<br>Karolinska Institute<br>Stockholm, Sweden<br>Clark Glymour<br>CD9@ANDREW.CMU.EDU<br>Carnegie Mellon University<br>Pittsburgh, PA, USA<br>Hedvig Kjellström<br>HEDVIG@KTH.SE<br>KTH Royal Institute of Technology<br>Stockholm, Sweden<br>Cheng Zhang*<br>CHENG.ZHANG@MICROSOFT.COM<br>Microsoft Research<br>Cambridge, UK</p>
<h4>Abstract</h4>
<p>Missing data are ubiquitous in many domains including healthcare. When these data entries are not missing completely at random, the (conditional) independence relations in the observed data may be different from those in the complete data generated by the underlying causal process. Consequently, simply applying existing causal discovery methods to the observed data may lead to wrong conclusions. In this paper, we aim at developing a causal discovery method to recover the underlying causal structure from observed data that follow different missingness mechanisms, including missing completely at random (MCAR), missing at random (MAR), and missing not at random (MNAR). With missingness mechanisms represented by missingness graphs, we analyse conditions under which additional correction is needed to derive conditional independence/dependence relations in the complete data. Based on our analysis, we propose the Missing Value PC (MVPC) algorithm for both continuous and binary variables, which extends the PC algorithm to incorporate additional corrections. Our proposed MVPC is shown in theory to give asymptotically correct results even on data that are MAR or MNAR. Experimental results on synthetic data show that</p>
<p>the proposed algorithm is able to find correct causal relations even in the general case of MNAR. Moreover, we create a neuropathic pain diagnostic simulator for evaluating causal discovery methods. Evaluated on such simulated neuropathic pain diagnosis records and the other two real world applications, MVPC outperforms the other benchmark methods ${ }^{1}$.
Keywords: causal discovery, missing data issue, causal discovery algorithm evaluation, machine learning in healthcare</p>
<h1>1. Introduction</h1>
<p>Determining causal relations plays a pivotal role in many disciplines of science, especially in healthcare. In particular, understanding causality in healthcare can facilitate effective treatments to improve quality of life. Traditional approaches (Domeij-Arverud et al., 2016) to identify causal relations are usually based on randomized controlled trails, which are expensive or even impossible in certain domains. In contrast, owing to the availability of purely observational data and recent technological developments in computational and statistical analysis, causal discovery from observational data is potentially widely applicable (Spirtes et al., 2001; Pearl, 2000). In recent years, causal discovery from observational data has become popular in medical research (Sokolova et al., 2017; Klasson et al., 2017).</p>
<p>Most existing algorithms for causal discovery are designed for complete data (Pearl, 2000; Peters et al., 2017), such as the widely used PC algorithm (Spirtes et al., 2001). Unfortunately, missing data entries are common in many domains. For example, in healthcare, missing entries may come from imperfect data collection, compensatory medical instruments, and fitness of the patients etc. (Robins, 1986).</p>
<p>All missing data mechanisms fall into one of the following three categories (Rubin, 1976): Missing Completely At Random (MCAR), Missing At Random (MAR), and Missing Not At Random (MNAR). Data are MCAR if the cause of missingness is purely random, e.g., some entries are deleted due to a random computer error. Data are MAR when the direct cause of missingness is fully observed. For example, a dataset consists of two variables: gender and income, where gender is always observed and income has missing entries. MAR missingness would occur when men are more reluctant than women to disclose their income (i.e., gender causes missingness). Data that are neither MAR nor MCAR fall under the MNAR category. In the example above, MNAR would occur when gender also has missing entries. These missingness mechanisms can be represented by causal graphs as introduced in Section 2. While it might be tempting to remove samples corrupted by missingness and perform analysis solely with complete cases, it will reduce sample size and, more importantly, bias the outcome especially when data are MAR or MNAR (Rubin, 2004; Mohan et al., 2013).</p>
<p>This paper is concerned with how to find the underlying causal structure from observed data even in the situation of MAR or MNAR. For simplicity, we assume causal sufficiency in the paper, as assumed by many causal discovery methods including PC (Spirtes et al., 2001). Recoverability of the data distribution under missing data has been discussed in a number of contributions; see,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>e.g., (Mohan et al., 2013; Mohan and Pearl, 2014a). A straightforward solution is to recover all relevant distributions that are needed for Conditional Independence (CI) tests involved in the CI-based causal search procedure, such as PC. But compared to the CI test on independent and identically distributed observations, the CI test implied by corrected distributions is generally harder to assess because it involves simulating new data or importance reweighting with density ratios. Therefore, instead of correcting all CI tests of the PC algorithm, we aim to find under which conditions CI tests in the observed data produce erroneous edges, and subsequently correct only such edges by further applying CI tests on corrected distributions. Our main contributions are:</p>
<ul>
<li>We provide a theoretical analysis of the error that different missingness mechanisms introduce in the results given by traditional causal discovery methods, especially the PC algorithm (Section 3). We show that naive deletion-based method may lead to incorrect results due to the bias caused by missing data. One immediate way to extend constraint-based methods to handle the missing data issue is correcting all the involved CI tests. This approach is neither data-efficient nor computation-efficient. Therefore, we identify possible errors that different missingness mechanisms lead to in the results given by deletion-based PC. We show that one usually needs to correct only a small number of CI tests in order to recover the true causal structure.</li>
<li>We propose a novel, correction-based extension of the PC algorithm, Missing Value PC (MVPC), that handles all three types of missingness mechanisms: MCAR, MAR, and MNAR (Section 4). Based on the result from Section 3, we identify where corrections are required and propose efficient correction methods for all three types of the missingness mechanisms.</li>
<li>We show the superior performance of MVPC in different settings, including three real-life healthcare scenarios (Section 5). We first evaluate the proposed MVPC on synthetic datasets under different settings. MVPC shows clear improvement over multiple baselines. We then develop a neuropathic pain diagnosis simulator for evaluating various types of causal discovery algorithms, including those to deal with practical issues in causal discovery, such as unknown confounders, selection bias, and missing data. We further apply MVPC to our neuropathic pain diagnosis simulation datasets and two real-world datasets in the US Cognition study and Achilles Tendon Rupture study. The results are consistent with medical domain knowledge and show the efficacy of our method.</li>
</ul>
<h1>2. Related work</h1>
<p>We discuss closely related works, including traditional causal discovery algorithms and approaches that deal with missing data from a causal perspective.</p>
<p>Causal discovery. Causal discovery from observational data has been of great interest in various domains in the past decades (Pearl, 2000; Spirtes et al., 2001). In general, causal discovery consists of constraint-based methods, score-based methods, and methods based on functional causal models. Typical constraint-based methods include the PC algorithm and Fast Causal Inference (FCI) (Spirtes et al., 2001). They assume that all CI relations in the data are entailed by the causal</p>
<p>Markov condition on the underlying causal graph, known as the faithfulness assumption, and use CI constraints in the data to recover causal structure. The PC algorithm assumes no confounders (i.e., hidden direct common causes of two variables) and outputs a Completed Partially Directed Acyclic Graph (CPDAG), which represents the Markov equivalence class that contains all DAGs that have the same CI relations, and is easy to interpret and often used in biomedical applications (Neto et al., 2008; Le et al., 2016). FCI allows confounders and selection bias, and outputs a Partial Ancestral Graph (PAG). For simplicity, we use the PC algorithm in this paper, but it is straightforward to transfer our framework to other constraint-based methods. Score-based methods (e.g., Greedy Equivalence Search (Chickering, 2002)) find the best Markov equivalence class under certain scorebased criterion, such as the Bayesian Information Criterion (BIC). Causal discovery based on functional causal models benefits from the additional assumptions on the data distribution and/or the functional classes to further determine the causal direction between variables. Typical functional causal models include the linear non-Gaussian acyclic model (LiNGAM) (Shimizu et al., 2006), the post-nonlinear (PNL) causal model (Zhang and Hyvärinen, 2009; Zhang and Chan, 2006), and the nonlinear additive noise model (ANM) (Peters et al., 2017).</p>
<p>Dealing with data with missing values from a causal perspective. Recent years have witnessed a growing interest in analysing the problem of missing data from a causal perspective. In particular, the notions of recoverability and testability have been studied by modeling the missingness process using causal graphs (called missingness graphs) (Mohan et al., 2013). Given a missingness graph, a query (such as conditional or joint distribution and causal effects) is deemed recoverable if it can be consistently estimated (Mohan and Pearl, 2014a). With the application of missingness graphs, Shpitser et al. (2015) represent missing data as a causal inference problem and identify joint distributions in MNAR settings. Relaxing the assumption of (Shpitser et al., 2015), Bhattacharya et al. (2019) propose a method to identifying a wider class of queries. Moreover, Saadati and Tian (2019) use missingness graphs, provide the conditions under which the causal effect in the presence of MNAR can be recovered, and propose an algorithm to find valid variable sets for the recovery. Testability, on the other hand, deals with finding testable implications, i.e., claims refutable by the (missing) data distribution (Mohan and Pearl, 2014b). As for causal discovery in the presence of missing values, it aims to find the structure of variables of interest rather than the missingness. Under appropriate assumptions, relations of concerned variables can be testable.</p>
<p>In causal discovery, there are few works for the MNAR case. Strobl et al. (2017) regard the missingness procedure as a particular type of selection bias to handle the MNAR missingness and perform FCI by test-wise deletion. The test-wise deletion removes the incomplete records of variables involved in each CI test. It shows that FCI combined with test-wise deletion is still sound when one aims to estimate the PAG for the variables including the effect of missingness. Data missingness is usually different from selection bias, because in the selection bias case we only have the distribution of the selected samples but no clue about the population. However, in the missing data case, we may be able to check the (conditional) independence relation between two variables given others by making use of the available data for the involved variables. In the case where the missingness mechanisms are known, this problem is closely related to recoverability of</p>
<p>models with missing data. Gain and Shpitser (2018) utilize Inverse Probability Weight (IPW) for each CI test, assuming the missing data model is known, which may not be realistic in many reallife applications. When the missing data model is unknown, they choose the sparest resulting graph considering all possible missingness structures, which is usually computationally expensive.</p>
<h1>3. Deletion-based PC: A first proposal and its behavior</h1>
<p>We assume that there is no confounder or selection bias relative to the set of observed variables. When the available dataset has missing values, one may apply the PC algorithm for causal discovery by performing CI tests on those records which do not have missing values for the variables involved in the tests. We term this first proposal deletion-based $P C$. In this section, we discuss the influence of missing data on the result of deletion-based PC.</p>
<p>Primarily, we investigate the situations where errors occur to the output of deletion-based PC due to the missingness. Firstly, we utilize missingness graphs and summarize the assumptions that we need for properly dealing with missingness. We then present the aforementioned deletion-based PC algorithm. Our analysis focuses on properties of the results given by this naive extension, and provides the conditions under which the deletion-based PC produces erroneous edges.</p>
<p>Missingness graph. We utilize the notation of the missingness graph (Mohan et al., 2013). A missingness graph is a causal DAG $G(\mathbb{V}, \mathbf{E})$ where $\mathbb{V}=\mathbf{V} \cup \mathbf{U} \cup \mathbf{V}^{<em>} \cup \mathbf{R} . \mathbf{U}$ is the set of unobservable nodes; in this paper, we assume causal sufficiency, so $\mathbf{U}$ is an empty set. $\mathbf{V}$ is the set of substantive nodes (observable nodes) containing $\mathbf{V}<em m="m">{o}$ and $\mathbf{V}</em>} . \mathbf{V<em m="m">{o} \subseteq \mathbf{V}$ is the set of fully observed variables, denoted by white nodes in our graphical representation. $\mathbf{V}</em>^{} \subseteq \mathbf{V}$ is the set of partially observed variables that are missing in at least one record, which is shadowed in gray. $\mathbf{R}$ is the set of missingness indicators that represent the status of missingness and are responsible for the values of proxy variables $\mathbf{V</em>}$. For example, the proxy variable $Y^{<em>} \in \mathbf{V}^{</em>}$ is introduced as an auxiliary variable for the convenience of derivation. $R_{y}=1$ means that the corresponding record value of $Y$ is missing and $Y^{<em>}$ corresponds to a missing entry; $R_{y}=0$ indicates that the corresponding record value of $Y$ is observed and $Y^{</em>}$ takes the value of $Y$.</p>
<p>In this work we adopt the CI-based definitions of missingness categories as stated in (Mohan et al., 2013). We denote an independence relation in a dataset by " $\Perp$ " and d-separation in a missingness graph by " $\Perp_{d}$ ". As shown in Figure 1, data are MCAR if $\left{\mathbf{V}<em o="o">{m}, \mathbf{V}</em>}\right} \Perp_{d} \mathbf{R}$ holds in the missingness graph, MAR if $\mathbf{V<em d="d">{m} \Perp</em>$ holds, and MNAR otherwise.} \mathbf{R} \mid \mathbf{V}_{o</p>
<p>Assumptions for dealing with missingness. Apart from the assumptions for the asymptotic correctness of the PC algorithm (including the causal Markov condition, faithfulness, and no confounding or selection bias), we introduce some additional assumptions that we make use of to deal with missingness.</p>
<p>Assumption 1 (Missingness indicators are not causes). No missingness indicator can be a cause of any substantive (observed) variable.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Exemplar missingness graphs in MCAR, MAR, MNAR, and self-masking missingness. $X, Y, Z$, and $W$ are random variables. In missingness graphs, gray nodes are partially observed variables, and white nodes are fully observed variables. $R_{x}, R_{y}$, and $R_{w}$ are the missingness indicators of $X, Y$, and $W$.</p>
<p>This assumption is employed in most related work using missingness graphs (Mohan et al., 2013; Mohan and Pearl, 2014a). Consequently, under this assumption, if variables of interest $X$ and $Y$ are not d-separated by a variable set $\mathbf{Z} \subseteq \mathbf{V} \backslash{X, Y}$, they are not d-separated by $\mathbf{Z}$ together with their missingness indicators. Under the faithfulness assumption, this means that if they are conditionally independent given $\mathbf{Z}$ together with the their missingness indicators, they are conditionally independent given only $\mathbf{Z}$. Now the problem is that generally speaking, we cannot directly verify whether they are conditionally independent given $\mathbf{Z}$ and their missingness variables because we do not have the records for the considered variables when their missingness indicators take value one. We then need the following assumptions to deal with this issue.</p>
<p>Assumption 2 (Faithful observability). Any conditional independence relation in the observed data also holds in the unobserved data; formally, $X \Perp Y \mid\left{\mathbf{Z}, \mathbf{R}<em _mathbf_K="\mathbf{K">{\mathbf{K}}=\mathbf{0}\right} \Longleftrightarrow X \Perp Y \mid\left{\mathbf{Z}, \mathbf{R}</em>}}=\mathbf{1}\right}$. Here $\mathbf{R<em x="x">{\mathbf{K}}$ is the missingness indicator set $\left{R</em>}, R_{y}, \mathbf{R<em _mathbf_K="\mathbf{K">{\mathbf{z}}\right} . \mathbf{R}</em>}}=\mathbf{0}$ means that all the missingness indicators in $\mathbf{R<em _mathbf_K="\mathbf{K">{\mathbf{K}}$ take the value zero; $\mathbf{R}</em>$ takes the value one.}}=\mathbf{1}$ means that at least one missingness indicator in $\mathbf{R}_{\mathbf{K}</p>
<p>This implies $X \Perp Y \mid\left{\mathbf{Z}, \mathbf{R}<em _mathbf_K="\mathbf{K">{\mathbf{K}}=\mathbf{0}\right} \Longleftrightarrow X \Perp Y \mid\left{\mathbf{Z}, \mathbf{R}</em>\right}$, which means that conditional independence relations in the observed data also hold in the complete data, i.e., there is no accidental conditional independence relation caused by missingness.}</p>
<p>Assumption 3 (No deterministic relation between missingness indicators). No missingness indicator can be a deterministic function of any other missingness indicators.</p>
<p>Assumption 4 (No self-masking missingness). Self-masking missingness refers to missingness in a variable that is caused by itself. In the missingness graph this is depicted by an edge from $X$ to $R_{x}$, for $X \in \mathbf{V}_{\mathbf{m}}$, as shown in Figure 1d. We assume that there is no such edges in the missingness graph.</p>
<p>Assumption 3 and 4 guarantee the recoverability of a joint distribution of substantive variables, as shown in (Mohan et al., 2013). As discussed in Section 7, in linear Gaussian cases the "selfmasking" only affects causal discovery results when $R_{x}$ has direct causes other than $X$.</p>
<p>In the end, we assume linear Gaussian causal models in the continuous case. Thus, one can check CI relations with the partial correlation test, a simple CI test method. Note that our proposed algorithm also works well for other scenarios. In the binary case, we can simply use the $G^{2}$ test. In the non-linear case, we can use a suitable non-linear or non-parametric one (Zhang et al., 2011).</p>
<p>Effect of missing data on the deletion-based PC. In the presence of missing data, the listwise deletion PC algorithm deletes all records that have any missing value and then applies the PC algorithm to the remaining data. In contrast, the test-wise deletion PC algorithm only deletes records with missing values for variables involved in the current CI test when performing the PC algorithm (which can be seen as the PC algorithm realization of (Strobl et al., 2017)). Test-wise deletion is more data-efficient than list-wise deletion. In this paper, we focus on the Test-wise Deletion PC algorithm (TD-PC).</p>
<p>TD-PC gives asymptotically correct results when data are MCAR since $\left{\mathbf{V}<em o="o">{m}, \mathbf{V}</em>\right}$. Furthermore, with the faithful observability assumption, we conclude $X \Perp Y \mid Z \Longleftrightarrow X \Perp Y^{}\right} \Perp_{d} \mathbf{R}$ is satisfied. Consider Figure 1a as an example. $R_{y} \Perp_{d}{X, Y, Z}$ holds; thus, we have $X \Perp_{d} Y \mid$ $Z \Longleftrightarrow X \Perp_{d} Y \mid\left{Z, R_{y}\right}$. With the faithfulness assumption on missingness graphs, $X \Perp Y \mid$ $Z \Longleftrightarrow X \Perp Y \mid\left{Z, R_{y<em>} \mid\left{Z, R_{y}=0\right}$. When applying the CI test to the test-wise deleted data of concerned variables $X, Y$, and $Z$, we test whether $X \Perp Y^{</em>} \mid\left{Z, R_{y}=0\right}$ holds. Therefore, CI results imply d-separation/d-connection relations of concerned variables in missingness graphs when data are MCAR, which guarantees the asymptotic correctness of TD-PC.</p>
<p>In cases of MAR and MNAR, TD-PC may produce erroneous edges because $\left{\mathbf{V}<em o="o">{m}, \mathbf{V}</em>$ does not hold. Therefore, in what follows in this section, we mainly address the problems of TD-PC in cases of MAR and MNAR.}\right} \Perp_{d} \mathbf{R</p>
<p>Erroneous edges produced by TD-PC. Since TD-PC may produce erroneous edges when data are MAR and MNAR, in the following propositions, we first show that the causal skeleton (undirected graph) given by TD-PC has no missing edges, but may contain extraneous edges. We then determine the conditions under which extraneous edges occur in the output of TD-PC.</p>
<p>Proposition 1. Under Assumptions 1～4, the CI relation in test-wise deleted data, $X \Perp Y \mid\left{\mathbf{Z}, R_{x}=\right.$ $\left.0, R_{y}=0, \mathbf{R}_{\mathbf{z}}=\mathbf{0}\right}$, implies the CI relation in complete data, $X \Perp Y \mid \mathbf{Z}$, where $X$ and $Y$ are random variables and $\mathbf{Z} \subseteq \mathbf{V} \backslash{X, Y}$.</p>
<p>Proof. $X \Perp Y \mid\left{\mathbf{Z}, \mathbf{R}<em x="x">{\mathbf{z}}=\mathbf{0}, R</em>}=0, R_{y}=0\right} \Rightarrow X \Perp Y \mid \mathbf{Z}$ : We have $X \Perp Y \mid\left{\mathbf{Z}, \mathbf{R<em x="x">{\mathbf{z}}=\mathbf{0}, R</em>}=0, R_{y}=\right.$ 0$}$, where some of the involved missingness indicators may only take value 0 (i.e., the corresponding variables do not have missing values). With the faithful observability assumption, this condition implies $X \Perp Y \mid\left{\mathbf{Z}, \mathbf{R<em x="x">{\mathbf{z}}, R</em>}, R_{y}\right}$. Because of the faithfulness assumption on missingness graphs, we know that $X$ and $Y$ are d-separated by $\left{\mathbf{Z}, \mathbf{R<em x="x">{\mathbf{z}}, R</em>$.}, R_{y}\right}$; furthermore, according to Assumptions 1, 3, and 4 , the missingness indicators can only be leaf nodes in the missingness graph. Therefore, conditioning on these nodes will not destroy the above d-separation relation. That is, in the missingness graph, $X$ and $Y$ are d-separated by $\mathbf{Z}$. Hence, we have $X \Perp Y \mid \mathbf{Z</p>
<p>Proposition 1 shows that CI relations in test-wise deleted data implies the true corresponding d-separation relations in a missingness graph. However, dependence relations in test-wise deleted data may imply the wrong corresponding relations in the missingness graph because $X \Perp Y \mid$ $\left{\mathbf{Z}, R_{x}=0, R_{y}=0, \mathbf{R}<em y="y">{\mathbf{z}}=\mathbf{0}\right} \Rightarrow X \Perp Y \mid \mathbf{Z}$. In other words, TD-PC may wrongly treat some dseparation relations of concerned variables as d-connected (not d-separated) in a missingness graph. Thus, TD-PC produces extraneous edges in the causal skeleton result rather than missing edges. For example, in Figure 1b, we have $X \Perp Y^{*} \mid\left{Z, R</em> Y \mid Z$. Thus, TD-PC produces an extraneous edge between $X$ and $Y$. Fortunately, such extraneous edges appear only under special circumstances, as shown in the following proposition.}=0\right}$ in the test-wise deleted data, but the true dseparation relation is $X \Perp_{d} Y \mid Z$ instead of $X \Perp_{d</p>
<p>Proposition 2. Suppose that $X$ and $Y$ are not adjacent in the true causal graph and that for any variable set $\mathbf{Z} \subseteq \mathbf{V} \backslash{X, Y}$ such that $X \Perp Y \mid \mathbf{Z}$, it is always the case that $X \Perp Y \mid\left{\mathbf{Z}, R_{x}=\right.$ $\left.0, R_{y}=0, \mathbf{R}_{\mathbf{z}}=\mathbf{0}\right}$. Then under Assumptions $1 \sim 4$, for at least one variable in ${X} \cup{Y} \cup \mathbf{Z}$, its missingness indicator is either the direct common effect or a descendant of the direct common effect of $X$ and $Y$.</p>
<p>Proof. The condition of Proposition 2 implies that for nodes $X, Y$ and any node set $\mathbf{Z} \subseteq \mathbf{V} \backslash{X, Y}$ in a missingness graph, conditioning on $\mathbf{Z}$ and missingness indicators $R_{x}, R_{y}$, and $\mathbf{R}<em i="i">{\mathbf{z}}$, there always exists an undirected path $U$ between $X$ and $Y$ that is not blocked. Furthermore, to satisfy such constraint of $U$, at least a missingness indicator $R</em>} \in\left{R_{x}, R_{y}, \mathbf{R<em i="i">{\mathbf{z}}\right}$ satisfies either one of the following two conditions: (1) $R</em>}$ is the only vertex on $U$; (2) A cause of $R_{i}$ is the only vertex on $U$ as a collider. In Condition (1), if $R_{i}$ is on $U$, it is a collider because under Assumptions $1 \sim 4$, missingness indicators are the leaf nodes in missingness graphs. Then, suppose that $R_{i}$ is not the only vertex on $U$, and that another node $V_{j} \in \mathbf{V} \backslash{X, Y, \mathbf{Z}}$ is also on $U$. Conditioning on $V_{j}$ and $R_{i}, U$ is blocked, which is not satisfied by the constraint on $U$. Thus, $R_{i}$ should be the only vertex on $U$. The same reason also applies to Condition (2). In summary, we conclude that under the condition of Proposition 2, there is at least one missingness indicator $R_{i} \in\left{R_{x}, R_{y}, \mathbf{R<em i="i">{\mathbf{z}}\right}$ such that $R</em>$ is the direct common effect or a descendant of the direct common effect of $X$ and $Y$.</p>
<p>Proposition 2 indicates that extraneous edges can be identified from the output of TD-PC. For example, in Figure 1b and Figure 1c, $W$ is the direct common effect of $X$ and $Y$ and the missingness indicator $R_{y}$ is a descendant of $W$. Thus, the extraneous edge occurs between $X$ and $Y$ in the causal skeleton produced by TD-PC.</p>
<h1>4. Proposed method: Missing-value PC</h1>
<p>In this section, we present our proposed approach, Missing-Value PC (MVPC), for causal discovery in the presence of missing data based on PC. We introduce the general MVPC framework in Section 4.1, the detection of the direct causes of missingness in Section 4.2, and our correction methods for removing extraneous edges in Section 4.3.</p>
<h1>4.1 Overview of MVPC</h1>
<p>Algorithm 1 summarizes the framework of MVPC. We perform TD-PC on V (Step 1), and then properly include $\mathbf{R}$ in the graph (Step 2). This is equivalent to performing TD-PC on $\mathbf{V} \cup \mathbf{R}$ under Assumption 1, 3, and 4. We then identify potential extraneous edges (Step 3). These are the edges between variables of which direct common effects are missingness indicators or ancestors of missingness indicators. Since we do not have orientation information at this stage, we cannot directly locate such extra edges; however, we can find potentially incorrect edges, as a superset of the incorrect edges. Next, we perform correction for these candidate edges (Step 4). Finally, we orient edges of the recovered causal skeleton with the same procedure as the PC algorithm.</p>
<div class="codehilite"><pre><span></span><code><span class="n">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="n">Missing</span><span class="o">-</span><span class="n">value</span><span class="w"> </span><span class="n">PC</span>
<span class="w">    </span><span class="p">:</span><span class="w"> </span><span class="n">Skeleton</span><span class="w"> </span><span class="n">search</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">test</span><span class="o">-</span><span class="n">wise</span><span class="w"> </span><span class="n">deletion</span><span class="w"> </span><span class="n">PC</span><span class="p">:</span>
<span class="w">    </span><span class="n">a</span><span class="w"> </span><span class="n">Graph</span><span class="w"> </span><span class="n">initialization</span><span class="p">:</span><span class="w"> </span><span class="n">Build</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">complete</span><span class="w"> </span><span class="n">undirected</span><span class="w"> </span><span class="n">graph</span><span class="w"> </span>\<span class="p">(</span><span class="n">G</span>\<span class="p">)</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">node</span><span class="w"> </span><span class="n">set</span><span class="w"> </span><span class="n">V</span><span class="o">.</span>
<span class="w">    </span><span class="n">b</span><span class="w"> </span><span class="n">Causal</span><span class="w"> </span><span class="n">skeleton</span><span class="w"> </span><span class="n">discovery</span><span class="p">:</span><span class="w"> </span><span class="n">Remove</span><span class="w"> </span><span class="n">edges</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span>\<span class="p">(</span><span class="n">G</span>\<span class="p">)</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">same</span><span class="w"> </span><span class="n">procedure</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">PC</span><span class="w"> </span><span class="n">algo</span><span class="o">-</span>
<span class="w">    </span><span class="n">rithm</span><span class="w"> </span><span class="p">(</span><span class="n">Spirtes</span><span class="w"> </span><span class="n">et</span><span class="w"> </span><span class="n">al</span><span class="o">.</span><span class="p">,</span><span class="w"> </span><span class="mi">2001</span><span class="p">)</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">test</span><span class="o">-</span><span class="n">wise</span><span class="w"> </span><span class="n">deleted</span><span class="w"> </span><span class="n">data</span><span class="o">.</span>
<span class="w">    </span><span class="mi">2</span><span class="p">:</span><span class="w"> </span><span class="n">Detecting</span><span class="w"> </span><span class="n">direct</span><span class="w"> </span><span class="n">causes</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">missingness</span><span class="w"> </span><span class="n">indicators</span><span class="p">:</span>
<span class="w">    </span><span class="n">For</span><span class="w"> </span><span class="n">each</span><span class="w"> </span><span class="n">variable</span><span class="w"> </span>\<span class="p">(</span><span class="n">V_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span><span class="w"> </span>\<span class="ow">in</span><span class="w"> </span>\<span class="n">mathbf</span><span class="p">{</span><span class="n">V</span><span class="p">}</span>\<span class="p">)</span><span class="w"> </span><span class="n">containing</span><span class="w"> </span><span class="n">missing</span><span class="w"> </span><span class="n">values</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">each</span><span class="w"> </span>\<span class="p">(</span><span class="n">j</span>\<span class="p">)</span><span class="w"> </span><span class="n">that</span><span class="w"> </span>\<span class="p">(</span><span class="n">j</span><span class="w"> </span>\<span class="n">neq</span><span class="w"> </span><span class="n">i</span>\<span class="p">),</span><span class="w"> </span><span class="n">test</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">CI</span><span class="w"> </span><span class="n">relation</span>
<span class="w">    </span><span class="n">of</span><span class="w"> </span>\<span class="p">(</span><span class="n">R_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span>\<span class="p">)</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span>\<span class="p">(</span><span class="n">V_</span><span class="p">{</span><span class="n">j</span><span class="p">}</span>\<span class="p">)</span><span class="o">.</span><span class="w"> </span><span class="n">If</span><span class="w"> </span><span class="n">they</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">independent</span><span class="w"> </span><span class="n">given</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">subset</span><span class="w"> </span><span class="n">of</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">mathbf</span><span class="p">{</span><span class="n">V</span><span class="p">}</span><span class="w"> </span>\<span class="n">backslash</span>\<span class="n">left</span>\<span class="p">{</span><span class="n">V_</span><span class="p">{</span><span class="n">i</span><span class="p">},</span><span class="w"> </span><span class="n">V_</span><span class="p">{</span><span class="n">j</span><span class="p">}</span>\<span class="n">right</span>\<span class="p">},</span><span class="w"> </span><span class="n">V_</span><span class="p">{</span><span class="n">j</span><span class="p">}</span>\<span class="p">)</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">direct</span><span class="w"> </span><span class="n">cause</span><span class="w"> </span><span class="n">of</span>
<span class="w">    </span>\<span class="p">(</span><span class="n">R_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span>\<span class="p">)</span><span class="o">.</span>
<span class="w">    </span><span class="mi">3</span><span class="p">:</span><span class="w"> </span><span class="n">Detecting</span><span class="w"> </span><span class="n">potential</span><span class="w"> </span><span class="n">extraneous</span><span class="w"> </span><span class="n">edges</span><span class="p">:</span>
<span class="w">    </span><span class="n">For</span><span class="w"> </span><span class="n">each</span><span class="w"> </span>\<span class="p">(</span><span class="n">i</span><span class="w"> </span>\<span class="n">neq</span><span class="w"> </span><span class="n">j</span>\<span class="p">),</span><span class="w"> </span><span class="k">if</span><span class="w"> </span>\<span class="p">(</span><span class="n">V_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span>\<span class="p">)</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span>\<span class="p">(</span><span class="n">V_</span><span class="p">{</span><span class="n">j</span><span class="p">}</span>\<span class="p">)</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">adjacent</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">have</span><span class="w"> </span><span class="n">at</span><span class="w"> </span><span class="n">least</span><span class="w"> </span><span class="n">one</span><span class="w"> </span><span class="n">common</span><span class="w"> </span><span class="n">adjacent</span><span class="w"> </span><span class="n">variable</span><span class="w"> </span><span class="ow">or</span>
<span class="w">    </span><span class="n">missingness</span><span class="w"> </span><span class="n">indicator</span><span class="p">,</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">edge</span><span class="w"> </span><span class="n">between</span><span class="w"> </span>\<span class="p">(</span><span class="n">V_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span>\<span class="p">)</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span>\<span class="p">(</span><span class="n">V_</span><span class="p">{</span><span class="n">j</span><span class="p">}</span>\<span class="p">)</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">potentially</span><span class="w"> </span><span class="n">extraneous</span><span class="o">.</span>
<span class="w">    </span><span class="mi">4</span><span class="p">:</span><span class="w"> </span><span class="n">Recovering</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="bp">true</span><span class="w"> </span><span class="n">causal</span><span class="w"> </span><span class="n">skeleton</span><span class="p">:</span>
<span class="w">    </span><span class="n">Perform</span><span class="w"> </span><span class="n">correction</span><span class="w"> </span><span class="n">methods</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">removing</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">extraneous</span><span class="w"> </span><span class="n">edges</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span>\<span class="p">(</span><span class="n">G</span>\<span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">shown</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">Section</span><span class="w"> </span><span class="mf">4.3</span><span class="o">.</span>
<span class="w">    </span><span class="mi">5</span><span class="p">:</span><span class="w"> </span><span class="n">Determining</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">orientation</span><span class="p">:</span>
<span class="w">    </span><span class="n">Orient</span><span class="w"> </span><span class="n">edges</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span>\<span class="p">(</span><span class="n">G</span>\<span class="p">)</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">same</span><span class="w"> </span><span class="n">orientation</span><span class="w"> </span><span class="n">procedure</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">PC</span><span class="w"> </span><span class="n">algorithm</span><span class="o">.</span>
</code></pre></div>

<h3>4.2 Detection of the direct causes of missingness indicators</h3>
<p>In Step 2 of Algorithm 1, detecting direct causes of missingness indicators is implemented by the causal skeleton discovery procedure of TD-PC. For each missingness indicator $R_{i}$, the causal skeleton discovery procedure checks all the CI relations between $R_{i}$ and variables in $\mathbf{V} \backslash V_{i}$, and then tests whether $R_{i}$ is conditionally independent of a variable $V_{j} \in \mathbf{V} \backslash V_{i}$ given any variable or set of variables connected to $R_{i}$ or $V_{j}$. If they are conditionally independent, the edge between $R_{i}$ and $V_{j}$ is removed. Under Assumptions $1 \sim 4$, no extraneous edge is produced in this step because according to Proposition 2, an extraneous edge only occurs when $R_{i}$ and $V_{j}$ have at least one direct common effect. Since we assume that the missingness indicator has no child variable in Assumption 1, $R_{i}$ has no child, and there is no direct common effect of $R_{i}$ and $V_{j}$. Therefore, in the result of this step all the variables adjacent to $R_{i}$ are its direct causes.</p>
<h1>4.3 Recovery of the true causal skeleton</h1>
<p>As shown in Section 3, TD-PC produces extraneous edges in the causal skeleton, resulting in the situations of Proposition 2. In this section, we introduce our correction methods to remove the extraneous edges. We first introduce Permutation-based Correction (PermC) with an example. We then show that PermC handles most of the missingness cases. Next, we propose an alternative solution, named Density Ratio Weighted correction (DRW), for the cases which PermC does not cover.</p>
<h3>4.3.1 PERMUTATION-BASED CORRECTION</h3>
<p>PermC in continuous cases. We use an example in continuous cases to demonstrate how to remove the extraneous edges with PermC. For example, suppose that we have a dataset with missing values of which the underlying missingness graph is shown in Figure 1b. As discussed in Section 3, when applying TD-PC to this dataset, we produce an extraneous edge between $X$ and $Y$ in the output of TD-PC. The problem is that data samples from joint distribution $P(X, Y, Z)$ are not available in the observed dataset. In this case, we test the CI relations in the test-wise deleted data from $P\left(X, Y^{*}, Z \mid R_{y}=0\right)$, producing the extraneous edge.</p>
<p>PermC solves this problem by testing the CI relations in the reconstructed virtual dataset utilizing the observed data concerning</p>
<p>$$
\begin{aligned}
P(X, Y, Z) &amp; =\int_{W} P(X, Y, Z \mid W) P(W) d W \
&amp; =\int_{W} P\left(X, Y^{*}, Z \mid W, R_{y}=0\right) P(W) d W
\end{aligned}
$$</p>
<p>such that reconstructed data follow the joint distribution $P(X, Y, Z)$. As shown in the first step of Equation 1, we introduce a random variable $W$ which is the direct cause of $R_{y}$ in Figure 1b to reconstruct the dataset and then marginalize it out. With $W$, the joint distribution $P(X, Y, Z)$ is estimated by 1) learning the model for $P(X, Y, Z \mid W)$ from test-wise deleted data, 2) plugging in the values of $W$ in the dataset, as data samples from $P(W)$, and 3) disregarding the input $W$ and keeping the generated virtual data for ${X, Y, Z}$ to marginalize $W$ out. Given virtual data of $X, Y$, and $Z$ that follow the joint distribution $P(X, Y, Z)$, one can test CI relations in the complete data.</p>
<p>Now the issue is that the data samples from $P(X, Y, Z \mid W)$ are not directly available. Nevertheless, we learn a model for $P\left(X, Y^{<em>}, Z \mid W, R_{y}=0\right)$ to generate virtual data of $X, Y$, and $Z$ from $W$, as shown in the second step of Equation 1. Under Assumptions $1 \sim 4$ we have $P(X, Y, Z \mid W)=$ $P\left(X, Y^{</em>}, Z \mid W, R_{y}=0\right)$ because $R_{y} \Perp_{d}{X, Y, Z} \mid W$; moreover, data samples from $P\left(X, Y^{<em>}, Z \mid\right.$ $\left.W, R_{y}=0\right)$ can be constructed by test-wise deletion. For simplicity, under the linear Gaussian assumption we apply linear regression to learning the model for $P\left(X, Y^{</em>}, Z \mid W, R_{y}=0\right)$ as :</p>
<p>$$
X=\alpha_{1} W+\varepsilon_{1}, \quad Y=\alpha_{2} W+\varepsilon_{2}, \quad Z=\alpha_{3} W+\varepsilon_{3}
$$</p>
<p>where $\alpha_{i}$ is the parameter of linear regression models and $\varepsilon_{i}$ is the residual.</p>
<p>Next, we sample the input values from the probability distribution $P(W)$. Estimating $P(W)$ for sampling input values is unnecessary in this case because we have the complete data of $W$, which follow $P(W)$. However, to generate virtual data with linear regression models, we cannot directly input the test-wise deleted data of $W$ and add the residuals from the linear regression models in Equation 2. In this way, the input values follow the conditional distribution $P\left(W \mid R_{y}=0\right)$ instead of $P(W)$. Thus, we shuffle the values of $W$ in the observed dataset such that $P\left(W^{S} \mid R_{y}=0\right)=P\left(W^{S}\right)$ where $W^{S}$ denotes the shuffled $W$. We then feed test-wise deleted values of $W^{S}$ into the linear regression models as :</p>
<p>$$
\widehat{X}:=\alpha_{1} W^{S}+\varepsilon_{1}, \quad \widehat{Y}:=\alpha_{2} W^{S}+\varepsilon_{2}, \quad \widehat{Z}:=\alpha_{3} W^{S}+\varepsilon_{3}
$$</p>
<p>where we denote the random variables with generated virtual values by $\widehat{X}, \widehat{Y}$, and $\widehat{Z}$. Finally, we test for the CI relation among $\widehat{X}, \widehat{Y}$, and $\widehat{Z}$. PermC for this example is summarized in Algorithm 2.</p>
<h1>Algorithm 2 Permutation-based correction</h1>
<p>Input: data of the concerned variables, such as $X, Y$, and $Z$ in Figure 1b, and the direct causes of their corresponding missingness indicators, such as the direct cause $W$ of $R_{y}$ in Figure 1b.
Output: The CI relations among concerned variables, such as the CI relations among $X, Y$, and $Z$.
1: Delete records containing any missing value. We denote the deleted dataset by $D_{d}$, and denote the original dataset by $D_{o}$.
2: Regress $X, Y$, and $Z$ on $W$ with $D_{d}$ as Equation 2.
3: Shuffle data of $W$ in $D_{o}$, denoted by $W^{S}$, and delete records containing any missing value in $D_{o}$ (included $W^{S}$ ).
4: Generate virtual data of $\widehat{X}, \widehat{Y}$, and $\widehat{Z}$, with $W^{S}$ and the residuals according to Equation 3.
5: Test the CI relations among $\widehat{X}, \widehat{Y}$, and $\widehat{Z}$ in the generated virtual data.
6: return The CI relations among $X, Y$, and $Z$.</p>
<p>Without loss of generality, we summarize the conditions under which PermC correctly removes extraneous edges. Suppose that we need to test the CI relation of $X$ and $Y$ given $\mathbf{Z} \subseteq \mathbf{V} \backslash{X, Y}$ in the generated virtual data. We denote the direct causes of missingness indicators by $\operatorname{Pa}(\mathbf{R})$. The conditions for the validity of PermC are as follows.
(i) $\left{R_{x}, R_{y}, \mathbf{R}<em _mathbf_w="\mathbf{w">{\mathbf{z}}, \mathbf{R}</em>}}\right} \Perp_{d}{X, Y, \mathbf{Z}} \mid \mathbf{W}$, where the variable set $\mathbf{W}$ is the set of direct causes of missingness indicators $R_{x}, R_{y}$, and $\mathbf{R<em _mathbf_w="\mathbf{w">{\mathbf{z}}$; if variables in $\mathbf{W}$ also have missing values, the direct causes of their missingness indicators $\mathbf{R}</em>}}$ are also included in $\mathbf{W}$; formally, $\mathbf{W}=P a\left(R_{x}, R_{y}, \mathbf{R<em _mathbf_w="\mathbf{w">{\mathbf{z}}, \mathbf{R}</em>\right)$;
(ii) In the missingness graph, the missingness indicators of $\mathbf{W}$ follow the condition that $X \Perp_{d} Y \mid$ $\mathbf{Z} \Longleftrightarrow X \Perp_{d} Y \mid\left{\mathbf{Z}, \mathbf{R}_{\mathbf{w}}\right}$.}</p>
<p>Under Conditions (i) and (ii), we have</p>
<p>$$
\begin{aligned}
P\left(X, Y, \mathbf{Z} \mid \mathbf{R}<em _mathbf_W="\mathbf{W">{\mathbf{w}}=\mathbf{0}\right)=\int</em>^{<em>}} P\left(X^{</em>}, Y^{<em>}, \mathbf{Z}^{</em>} \mid \mathbf{W}^{<em>}, R_{x}=0, R_{y}=0, \mathbf{R}<em _mathbf_w="\mathbf{w">{\mathbf{z}}=\mathbf{0}, \mathbf{R}</em>\right) &amp; \times \
&amp; P\left(\mathbf{W}^{}}=\mathbf{0</em>} \mid \mathbf{R}_{\mathbf{w}}=\mathbf{0}\right) d \mathbf{W}^{*}
\end{aligned}
$$</p>
<p>To test the CI relation of $X$ and $Y$ given $\mathbf{Z}$ in data samples from $P(X, Y, \mathbf{Z})$, it is valid to test the CI relation in the generated data samples from $P(X, Y, \mathbf{Z} \mid \mathbf{R}<em _mathbf_w="\mathbf{w">{\mathbf{w}}=\mathbf{0})$. Under Condition (ii) the conditional independence/dependence relations in $P(X, Y, \mathbf{Z})$ also hold in $P(X, Y, \mathbf{Z} \mid \mathbf{R}</em>}}=\mathbf{0})$. Moreover, linear regression models in PermC are valid. Under Condition (i), we have $P(X, Y, \mathbf{Z} \mid$ $\mathbf{W}, R_{x}=0, R_{y}=0, \mathbf{R<em _mathbf_w="\mathbf{w">{\mathbf{z}}=\mathbf{0}, \mathbf{R}</em>$. Thus, we use linear regression to estimate $P\left(X^{}}=\mathbf{0})=P(X, Y, \mathbf{Z} \mid \mathbf{W})$, in which $X, Y$, and $\mathbf{Z}$ are conditionally Gaussian distributed given $\mathbf{W<em>}, Y^{</em>}, \mathbf{Z}^{<em>} \mid \mathbf{W}^{</em>}, R_{x}=\right.$ $\left.0, R_{y}=0, \mathbf{R}<em _mathbf_w="\mathbf{w">{\mathbf{z}}=\mathbf{0}, \mathbf{R}</em>\right)$ and use them in the correction.}}=\mathbf{0</p>
<h1>Algorithm 3 Binary Permutation-based correction</h1>
<p>Input: data of the concerned variables, such as $X, Y$, and $Z$ in Figure 1b, and the direct causes of their corresponding missingness indicators, such as the direct cause $W$ of $R_{y}$ in Figure 1b.
Output: The CI relations among concerned variables, such as the CI relations among $X, Y$, and $Z$.
1: Delete records containing any missing value. We denote the deleted data set by $D^{d}$, and denote the original data set by $D^{o}$.
2: Separate $D^{d}$ into $D_{W=0}^{d}$ and $D_{W=1}^{d}$ according to the values of $W$, e.g., $D_{W=0}^{d}$ are the samples in $D^{d}$ of which the value of $W$ is 0 .
3: Estimate the joint distributions of $X, Y$, and $Z$ with $D_{W=0}^{d}$ and $D_{W=1}^{d}$, denoted by $P_{W=0}(X, Y, Z)$ and $P_{W=1}(X, Y, Z)$.
4: Shuffle data of $W$ in $D^{o}$, denoted by $W^{S}$, and delete records containing any missing value in $D^{o}$ (included $W^{S}$ ). We denote this data set by $D_{S}^{d}$.
5: Generate virtual data by sampling from $P_{W=0}(X, Y, Z)$ or $P_{W=1}(X, Y, Z)$ depending on the values of $W^{S}$ in $D_{S}^{d}$.
6: Test the CI relations among $\tilde{X}, \tilde{Y}$, and $\tilde{Z}$ in the generated virtual data.
7: return The test results of CI relations among $\tilde{X}, \tilde{Y}$, and $\tilde{Z}$.</p>
<p>PermC in binary cases. In the binary case, PermC follows the same procedures as shown in Algorithm 2 to correct the extraneous edges. The only difference between the continuous case and the binary case is how to generate the virtual data. Taking the same missingness graph in Figure 1 b as an example, we generate the virtual data of which the distribution is $P(X, Y, Z)$ to test the CI relations of $X, Y$, and $Z$. The virtual data distribution is</p>
<p>$$
P(X, Y, Z)=\sum_{W} P(X, Y, Z \mid W) P(W)=\sum_{W} P\left(X, Y^{*}, Z \mid W, R_{y}=0\right) P(W)
$$</p>
<p>Instead of using linear regression to generate the virtual data, we directly estimate the conditional distribution $P\left(X, Y^{<em>}, Z \mid W, R_{y}=0\right)$ from the test-wise deleted data and generate data of $X, Y$, and $Z$ based on values of $W$. Meanwhile, we keep the marginal distribution of $W$. PermC in the binary case is summarized in Algorithm 3. Note that Condition (i) of PermC in binary cases is that $R_{i} \Perp_{d} V_{i} \mid \mathbf{W}$, where $i \in{x, y, \mathbf{z}}$ and $X$ denoted by $V_{x}$ here; and $R_{w_{j}} \Perp_{d} W_{j} \mid \mathbf{W}<em k="k">{k}$, where $\mathbf{W}</em>^{} \subset \mathbf{W} \backslash W_{j}$. We denote this condition as Condition (i) ${ </em>}$. Condition (i) ${ }^{<em>}$ is weaker than Condition (i) because there is no linear Gaussian constraint for modeling $P\left(X, Y^{</em>}, Z \mid W, R_{y}=0\right)$.</p>
<h1>4.3.2 DENSITY RATIO WEIGHTED CORRECTION</h1>
<p>DRW in continuous cases. DRW removes extraneous edges in situations where Condition (i) and Condition (ii) are not satisfied (e.g., Figure 1c). In these cases, we consistently estimate the joint distribution $P\left(\mathbf{V}<em _mathbf_a="\mathbf{a">{\mathbf{a}}\right)$ of concerned variables in a CI test according to Theorem 2 of (Mohan et al., 2013). Here, $\mathbf{R}$ represents the missingness indicators of $\mathbf{V}</em>$. Equation 6 provides a way to reconstruct the observed dataset:}</p>
<p>$$
\begin{aligned}
P\left(\mathbf{V}<em _mathbf_a="\mathbf{a">{\mathbf{a}}\right) &amp; =\frac{P\left(\mathbf{R}=\mathbf{0}, \mathbf{V}</em> \
&amp; =P\left(\mathbf{V}}}\right)}{\prod_{i} P\left(R_{i}=0 \mid P a\left(R_{i}\right), R_{P a\left(R_{i}\right)}=0\right)<em i="i">{\mathbf{a}} \mid \mathbf{R}=\mathbf{0}\right) \times c \times \prod</em>
\end{aligned}
$$} \beta_{P a\left(R_{i}\right)</p>
<p>where $c=\frac{P(\mathbf{R}=\mathbf{0})}{\prod_{i} P\left(R_{i}=0 ; R_{P a\left(R_{i}\right)}=0\right)}$ and $\beta_{P a\left(R_{i}\right)}=\frac{P\left(P a\left(R_{i}\right) \mid R_{P a\left(R_{i}\right)}=0\right)}{P\left(P a\left(R_{i}\right) \mid R_{i}=0, R_{P a\left(R_{i}\right)}=0\right)}$. In the second line of Equation 6 , every (conditional) probability distribution can be consistently estimated. We first apply testwise deletion to the observed data of $\mathbf{V}<em i="i">{\mathbf{a}}$. Then, we reweight such data with the density ratios $\prod</em>$ with the kernel density estimation (Sheather and Jones, 1991) and compute the normalizing constant $c$. Finally, we test CI relations of concerned variables in the reweighted data samples from their corresponding joint distribution.} \beta_{P a\left(R_{i}\right)}$ and the normalizing constant $c$. We estimate density ratios $\prod_{i} \beta_{P a\left(R_{i}\right)</p>
<p>DRW in binary cases. DRW removes extraneous edges by testing CI relations in the full data distribution, such as $P\left(\mathbf{V}<em _mathbf_a="\mathbf{a">{\mathbf{a}}\right)$ in Equation 6. In the binary case, the full data distribution $P\left(\mathbf{V}</em>\right)$ can be simplified by Equation 7}</p>
<p>$$
P\left(\mathbf{V}<em _mathbf_a="\mathbf{a">{\mathbf{a}}\right)=P\left(\mathbf{V}</em>)
$$}} \mid \mathbf{R}=\mathbf{0}\right) \times \prod_{i} \beta_{P a\left(R_{i}\right)} \times P(\mathbf{R}=\mathbf{0</p>
<p>where $\beta_{P a\left(R_{i}\right)}=\frac{1}{P\left(R_{i}=0 \mid P a\left(R_{i}\right), R_{P a\left(R_{i}\right)}=0\right)} .{ }^{2}$ In Equation 7, $P\left(\mathbf{V}<em P="P" a_left_R__i="a\left(R_{i">{\mathbf{a}} \mid \mathbf{R}=\mathbf{0}\right)$ can be regarded as the distribution of the test-wise deleted data and $\prod \beta</em>$ tests, we remove extraneous edges.}\right)} P(\mathbf{R}=\mathbf{0})$ can be considered as the weights of each data point. The test-wise deleted data with the weights are used for the weighted $G^{2}$ test. Finally, based on the conditional independence results of weighted $G^{2</p>
<h2>5. Experiments</h2>
<p>We evaluate our method, MVPC, on synthetic data, simulated neuropathic pain diagnosis records, and real-world datasets. We first show experimental results on synthetic data (Section 5.1). We then apply the neuropathic pain diagnosis simulator to generating data in the presence of MNAR to compare different methods (Section 5.2). Finally, we apply our method to two healthcare datasets where data entries are significantly missing. The first (Section 5.3) is from the Cognition and aging</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>USA (CogUSA) study (McArdle et al., 2015), and the second (Section 5.4) is about Achilles Tendon Rupture (ATR) rehabilitation research study (Praxitelous et al., 2017; Domeij-Arverud et al., 2016). MVPC demonstrates superior performance compared to multiple baseline methods.</p>
<h1>5.1 Synthetic data evaluation</h1>
<p>To best demonstrate the behavior of different causal discovery methods, we first perform the evaluation on synthetic data sampled from randomly generated causal graphs. In this section we start from the evaluation of baseline methods for continuous variable cases with the consideration of the number of samples, the number of the parents of missingness indicators, different MNAR generation methods, and the number of substantive variables (that are the variables except proxy variables and missingness indicators in a missingness graph). Since the performance of baseline methods for binary variable cases is similar to that of continuous variables, we only show the results of binary variables in large sample size. We use Structural Hamming Distance (SHD) (Tsamardinos et al., 2006) and the F1 score as the evaluation metrics. Lower value of SHD is better, and higher value of F1 score is better.</p>
<p>We apply both permutation-based and density ratio weighted correction method to MVPC, denoted by MVPC-PermC and MVPC-DRW. We also use test-wise deletion PC algorithm (TDPC) as one of the baseline methods. Additionally, we apply the PC algorithm to the oracle data (without missing data), denoted by "ideal". Moreover, to decouple the effect of sample size, we construct datasets that are MCAR which have the same sample size as the average sample size of all CI tests in TD-PC. We denote PC with such virtual MCAR data by "target".</p>
<p>Number of samples. We first study the performance of baseline methods with different number of samples, following the procedures in (Colombo et al., 2012; Strobl et al., 2017) which randomly generates Gaussian DAG and samples data based on the given DAG. The results are shown in Figure 2. We generate synthetic datasets that are MAR and MNAR with the number of sample size 500, 1000, 5000, 10000, 50000, and 100000. The number of substantive variables is 20 in every causal graph. In the datasets that are MAR, 10 variables have missing values and at most 5 missingness indicators of them are in the case of Proposition 2. Moreover, the parents of these 10 missingness indicators have no missing values. In this experiment we limit the number of the parents of missingness indicators to 1 . In the following experiments we will study the influence of the number of such parents.</p>
<p>For generating the datasets that are MNAR, we firstly select at most 5 missingness indicators that are in the case of Proposition 2, and then randomly assign missing values for the parents of such missingness indicators. We call such a way, which generates MNAR datasets in a 2-step manner, "MNAR: MAR + MCAR". There is another more general way named "MNAR: no self-masking". This MNAR generation method first selects at most 5 missingness indicators that are in the case of Proposition 2. Then, it assigns missing values for each direct cause of missingness indicators selected in the first step, considering the values of another variable which cannot be the direct cause itself. In this experiment we only generate MNAR datasets with "MNAR: MAR + MCAR".</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Results of the baseline methods in continuous cases with different sample sizes. The methods are evaluated with Structural Hamming Distance (SHD) and F1 score. Lower value of SHD is better. Higher value of F1 score is better. The data are Missing At Random (MAR) in Panal (a) and (b); Missing Not At Random (MNAR) in Panal (c) and (d).</p>
<p>Figure 2 shows that our proposed algorithm, MVPC, consistently has superior performance compared to TD-PC, and is converging to the "target" performance with increasing the sample size across both metrics. Moreover, we find that MVPC-PermC is more data-efficient than MVPC-</p>
<p>DRW regarding the convergence speed of both methods. More interestingly, TD-PC is diverging and might be asymptotically wrong, which could depend on how to generate missing values.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Results of the baseline methods for the study: the influence of the parent number of missingness indicators.</p>
<p>Number of the parents of missingness indicators. We also investigate the influence of the parent number of missingness indicators. In this experiments, we remove the limitation of the parent number of missingness indicators, and generate MAR datasets. For the purpose of this experiment it is not necessary to use MNAR which introduces much complexity. The number of substantive variables are 20. The number of missingness indicators are 10, and at most 5 missingness indicators are in the case of Proposition 2. The sample size is 50000 .</p>
<p>Figure 3 displays that most of the methods have no significant difference between the experiments of single parent and multiple parents, and that MVPC-DRW is sensitive to the number of parents of missingness indicators. The reason could be that the density ratio estimation is implemented by the density estimation method which performs well for low-dimension data (of which the dimension is lower than 4). When a missingness indicator can have multiple causes, MVPC is more likely to estimate the probability distribution containing more than 3 variables, which leads to its worse performance than that in the single parent experiments.</p>
<p>Comparison of MNAR generation methods. We compare the performance of baseline methods in the MNAR datasets with different generation methods: "MNAR: no self-masking" and "MNAR: MAR + MCAR". The sample size is 100000 , and the number for substantive variables is 20 . The missingness indicators is 10 , and at most 5 missingness indicators are in the case of Proposition 2.</p>
<p>Figure 4 shows that most of the methods perform better in the datasets that are generated with "MNAR: MAR + MCAR". We also find that although MVPC-Permc performs better than</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Results of the baseline methods in the datasets with different MNAR generation methods.</p>
<p>MVPC-DRW, the average performance of MVPC-PermC in different datasets varies more than that of MVPC-DRW. The results are as expected because the missingness graphs generated by "MNAR: MAR + MCAR" satisfy the conditions of PermC correction method; however, the the MNAR missingness graphs generated by "MNAR: no self-masking" cannot. Even though the MNAR missingness graphs generated by "MNAR: no self-masking" cannot satisfy the conditions of "MNAR-PermC", the result of MVPC-PermC is still close to the "target". This also indicates that MVPC-PermC can handle most of the MNAR cases.</p>
<p>Number of substantive variables. In this experiment we study the performance of baseline methods with different number of substantive variables. As Figure 5 shown, we generate three groups of missingness graphs with "MNAR: MAR + MCAR" containing 20, 50, 100 substantive variables. Respectively, they have 10,16 , and 20 missingness indicators; and at most 5,8 , and 10 missingness indicators that are in the case of Proposition 2. The sample size is 10000 . Figure 5 displays that MVPC performs well in the large number of variables and better than Test-wise deletion PC that is severely influenced by the number of substantive variables.</p>
<p>Binary data evaluation. To evaluate MVPC in the binary case, we used Tetrad (Spirtes et al., 2004) to generate binary datasets together with corresponding ground-truth causal graphs. We compare the performance of baseline methods in MAR and MNAR datasets. The sample size is 600000 . The number of substantive variables is 20 in each dataset. The number of missingness indicators is 10 and at most 5 of them are in the case of Proposition 2. We limit the number of the parent of missingness indicators to 1 . We use "MNAR: MAR + MCAR" to generate MNAR data. The values of missingness indicators follow the Bernoulli distribution of which the parameters depend on the parent value of missingness indicators.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Results of the baseline methods in MNAR datasets with different number of substantive variables.</p>
<p>Figure 6 shows that the results of MVPC are clearly better than TD-PC and close to "target" in the result with SHD metric. Moreover, we find that MVPC-DRW in binary cases is much less data-efficiency than MVPC-PermC in continuous cases due to the fact that MVPC-DRW starts performing better than TD-PC when the samples size is larger than 500000.
ideal target MVPC-PermC MVPC-DRW TD-PC
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Results of the baseline methods in binary cases that are Missing At Random (MAR) and Missing Not At Random (MNAR).</p>
<h1>5.2 Neuropathic Pain Diagnosis Simulation data evaluation</h1>
<p>The evaluation of causal discovery methods is lack of real-world benchmark data sets, which has been a challenge (Bareinboim et al., 2008). We commonly evaluate on synthetic data sampled from randomly generated causal graphs (e.g., the synthetic data in Section 5.1). However, synthetic data are mostly based on the model proposed in a work such that it could be inappropriate to evaluate others' works. Moreover, the synthetic data experiments may show the superior performance of proposed methods but sometimes may oversimplify the challenges in real-world scenarios (Garant and Jensen, 2016). Fortunately, towards tackling the evaluation challenge, well-understood causal influences are available in some specific scenarios of disciplines such as biology and physics. This gives us opportunities to utilize domain knowledge and build realistic simulators. In this way, we can generate data from simulators and use them as benchmark datasets for the evaluation of causal discovery algorithms.</p>
<p>We develop a simulator to generate neuropathic pain diagnostic records ${ }^{3}$. The neuropathic pain diagnostic simulator (Tu et al., 2019) is an attempt of evaluating causal discovery methods which fills the the gap between the randomly generated dataset and the real-world dataset (e.g., the data sets in Section 5.3 and Section 5.4). The simulator considers each diagnostic label as a variable of which the value 1 indicates that the diagnostic label exists in a patient record and 0 otherwise. In total, 222 binary variables (nodes in a causal graph) and 770 pairs of causal relations (edges in a causal graph) are included in a causal graph considering the experiences and the domain knowledge of clinicians. The causal graph contains different d-separations such as the folk structure, the collider structure, and the chain structure. The complete interactive causal graph is visualized at: https://cutt.ly/BekNFSy. Other details about the simulator can be found in (Tu et al., 2019).</p>
<p>Table 1: Results of applying causal discovery methods to simulation data with missing values from the Neuropathic Pain diagnosis simulator (Tu et al., 2019).</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Cau_acc</th>
<th style="text-align: left;">Recall</th>
<th style="text-align: left;">Precision</th>
<th style="text-align: left;">F1-score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">PC-ideal</td>
<td style="text-align: left;">0.047</td>
<td style="text-align: left;">0.046</td>
<td style="text-align: left;">0.44</td>
<td style="text-align: left;">0.085</td>
</tr>
<tr>
<td style="text-align: left;">PC-target</td>
<td style="text-align: left;">0.046</td>
<td style="text-align: left;">0.046</td>
<td style="text-align: left;">0.44</td>
<td style="text-align: left;">0.085</td>
</tr>
<tr>
<td style="text-align: left;">MVPC</td>
<td style="text-align: left;">0.045</td>
<td style="text-align: left;">0.043</td>
<td style="text-align: left;">0.45</td>
<td style="text-align: left;">0.078</td>
</tr>
<tr>
<td style="text-align: left;">TD-PC</td>
<td style="text-align: left;">0.033</td>
<td style="text-align: left;">0.025</td>
<td style="text-align: left;">0.559</td>
<td style="text-align: left;">0.047</td>
</tr>
</tbody>
</table>
<p>In this experiment, we use the neuropathic pain diagnosis simulator to generate data that are MAR. The sample size is 1000 and the other parameters are the same with the ones in (Tu et al., 2019). We then evaluate the performance of different methods with the causal accuracy (Cau_acc) (Claassen and Heskes, 2012), recall, precision, and F1 score of resulted undirected graphs on the simulated data. We use MVPC with the density ratio weighted correction method in the binary case. Table 1 shows that all methods have low recall, which indicates the difficulty of applying</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>causal discovery to this simulation dataset. Moreover, the result of MVPC is close to PC-ideal and PC-target and better than TD-PC in the simulation experiments ( TD-PC, PC-ideal and PC-target are introduced in Section 5.1).</p>
<h1>5.3 The Cognition and aging USA (CogUSA) study</h1>
<p>In this experiment, we aim to discovery causal relations in the CogUSA study as in (Strobl et al., 2017). This is a typical survey based healthcare dataset with a large amount variables with missing values. In this scenario, the missingness mechanism is unknown and we could expect MCAR, MAR, and MNAR occur.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Performance of different methods on CogUSA study. Lower cost is better. The cost is the count of errors comparing with known causal constrains from experts.</p>
<p>We use the same 16 variables of interest in the CogUSA study as in (Strobl et al., 2017). Since the missingness indicators of the 16 variables can be caused by other variables, we utilize the rest variables when applying MVPC to the dataset. We use the BIC score for CI test (likelihood ratio test with the BIC penalty as the threshold). Figure 7 shows the performance evaluated using the known causal constraints: 1) Variables are in two groups with no inter-group causal relation; 2) there are causal relations between two pairs of variables given by the domain expertise. Each violation of these known causal relations adds 1 in the cost shown in Figure 7. Our proposed method obtains the best performance (lowest cost) comparing with deletion-based PC and deletion-based FCI (Strobl et al., 2017). This demonstrates the capabilities of our method in real life applications.</p>
<h3>5.4 Achilles Tendon Rupture study</h3>
<p>In the end, we perform causal discovery on a Achilles Tendon Rupture (ATR) study dataset (Praxitelous et al., 2017; Hamesse et al., 2018), collected in multiple hospitals ${ }^{4}$. ATR is a type of soft tissue injury involving a long rehabilitation process. Understanding causal relations among various factors and healing outcomes is essential for practitioners. The list-wise deletion method is not applicable for this case because about $70 \%$ of the data entries are missing, which means that very rare patients</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>have complete data. Thus, we apply our method and TD-PC to this dataset. We ran experiments on the full dataset with more than 100 variables. Figure 8a shows part of the causal graph.</p>
<p>We find that age, gender, BMI (body mass index), and LSI (Limb Symmetry Index) in the causal graph given by MVPC do not affect the healing outcome measured by Foot Ankle Outcome Score (FAOS). This result is consistent with (Praxitelous et al., 2017; Domeij-Arverud et al., 2016). To test the effectiveness of MNAR, we further introduce an auxiliary variable $S$ which is generated from two variables: Operation time $\left(O P_{\text {time }}\right)$ and FAOS. This variable further causes the missingness indicator of FAOS. Figure 8b and 8c show the results of these variables using TD-PC and our proposed method. Our proposed MVPC is able to correctly remove the extraneous edge between Operation time and FAOS.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Causal discovery results in the ATR study. Experiments were run over all variables. We show only a part of the whole causal graph. Panel (a) shows the relations among five variables given by MVPC. The relations are consistent with medical studies. Panel (b) and (c) show an example where MVPC is able to correct the error of TD-PC.</p>
<h1>6. Discussion</h1>
<p>In this work, we first provide a theoretical analysis about the influence of missing data on causal discovery under Assumptions $1 \sim 4$. We then introduce two correction methods together with their conditions and a framework, Missing Value PC (MVPC). In this section, we provide an additional consideration of Assumption 4, no self-masking missingness, and the conditions of correction methods.</p>
<p>Violation of "no self-masking missingness." Although many works (Mohan, 2019; Mohan et al., 2018) focus on the recoverability of SelF-masking Missingness (SFM), it is still a challenge for causal discovery in the presence of SFM. Moving forward to solve the SFM problem, we note that in linear Gaussian cases SFM does not affect MVPC, especially when the SFM indicator $R_{x}$ only has one direct cause $X$, such as in Figure 1d. In this case, the result of the CI test of $X$ and $Y$ in test-wise deleted data implies the correct d-separation relation in the missingness graph. With the faithfulness assumption on the missingness graph, we have $X \Perp Y \Longleftrightarrow X \Perp Y \mid R_{x}$; furthermore, under the faithful observability assumption, we have $X \Perp Y \mid R_{x} \Longleftrightarrow X^{*} \Perp Y \mid R_{x}=0$ which is what we test in the test-wise deleted data of $X$ and $Y$. Moreover, even though the test-wise</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<ol>
<li>In the ATR study experiment, only Paul Ackermann and Ruibo Tu get access to the ATR dataset.</li>
</ol>
<p><a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
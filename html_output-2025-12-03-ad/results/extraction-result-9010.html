<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9010 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9010</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9010</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-159.html">extraction-schema-159</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <p><strong>Paper ID:</strong> paper-278768632</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.13195v1.pdf" target="_blank">Adversarial Testing in LLMs: Insights into Decision-Making Vulnerabilities</a></p>
                <p><strong>Paper Abstract:</strong> As Large Language Models (LLMs) become increasingly integrated into real-world decision-making systems, understanding their behavioural vulnerabilities remains a critical challenge for AI safety and alignment. While existing evaluation metrics focus primarily on reasoning accuracy or factual correctness, they often overlook whether LLMs are robust to adversarial manipulation or capable of using adaptive strategy in dynamic environments. This paper introduces an adversarial evaluation framework designed to systematically stress-test the decision-making processes of LLMs under interactive and adversarial conditions. Drawing on methodologies from cognitive psychology and game theory, our framework probes how models respond in two canonical tasks: the two-armed bandit task and the Multi-Round Trust Task. These tasks capture key aspects of exploration-exploitation trade-offs, social cooperation, and strategic flexibility. We apply this framework to several state-of-the-art LLMs, including GPT-3.5, GPT-4, Gemini-1.5, and DeepSeek-V3, revealing model-specific susceptibilities to manipulation and rigidity in strategy adaptation. Our findings highlight distinct behavioral patterns across models and emphasize the importance of adaptability and fairness recognition for trustworthy AI deployment. Rather than offering a performance benchmark, this work proposes a methodology for diagnosing decision-making weaknesses in LLM-based agents, providing actionable insights for alignment and safety research.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9010.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9010.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 (bandit)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3.5 was evaluated as an agent in a repeated two-armed bandit task using an adversarial evaluation framework to probe exploration-exploitation and manipulability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Named as a state-of-the-art large language model used via API in this study; no architecture/training details are provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Two-armed bandit task</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>A repeated 2-alternative forced-choice bandit with 100 trials where the agent chooses between two options (Planet X or Y) and receives binary reward feedback; captures exploration-exploitation trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Initial target (Planet X) selection rate 30% across simulations; after adversary intervention target selection increased to 68%. Reward-switch rate comparable to humans; lower no-reward-switch rate than humans (significantly lower; CIs reported vs humans).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human behaviour (benchmark dataset from [5,8]): more distributed choices and higher mean reward than GPT-3.5 (Tukey HSD: mean reward difference human vs GPT-3.5 = 1.074, 95% CI [0.173,1.975], p=0.01). Humans show higher no-reward-switch rates than GPT-3.5.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Below human baseline on mean reward; more exploratory/flexible than later models but still more exploitable than humans under adversarial manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>GPT-3.5 simulated 200 times; each simulation 100 trials. Reward probability for each option set to 25% (both). Prompts framed a 'space explorer' choosing Planet X or Y; prompts included full trial history. Learner model (RNN+softmax) trained on LLM behavior; adversary trained with deep Q-learning to maximize selection of a predetermined target action (Planet X), subject to a constraint of 25 total learner rewards per action. API/system prompt context as described in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Paper does not provide model architecture/size or API temperature/settings; performance depends on prompt framing and simulated-repetition procedure; human baseline drawn from separate benchmark datasets (not a simultaneous within-experiment control), so differences may reflect task framing and interface, not only model cognition. Adversarial constraint (25 rewards per action) is artificial and influences manipulability metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Adversarial Testing in LLMs: Insights into Decision-Making Vulnerabilities', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9010.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9010.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (bandit)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 was evaluated in the two-armed bandit task and shown to exhibit strong early exploitation and high consistency, making it particularly susceptible to adversarially steering once a preference is formed.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Described as a state-of-the-art large language model used via API in this study; the paper does not supply architecture/training/size specifics.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Two-armed bandit task</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Repeated 100-trial two-option bandit probing exploration vs exploitation and reaction to reward/no-reward feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Average (pre-adversary) target selection rate ≈57% (model tended to converge quickly on one option with high consistency index ≈0.94); after adversarial intervention target selection increased to ≈93%. Exhibited low no-reward-switch and low reward-switch rates (significantly lower than humans).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Humans (benchmark) displayed more distributed exploration and achieved significantly higher mean rewards than GPT-4 in ANOVA/Tukey comparisons (human vs GPT-4 mean difference 1.163, 95% CI [-0.019,2.345], p = 0.056; reported as humans outperforming GPT-4 overall in reward rate analysis). Humans also showed higher no-reward-switch rates.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLM shows more rigid exploitation than humans, resulting in lower adaptability and vulnerabilities to adversarial manipulation; overall below or comparable to humans on reward rate depending on metric.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>GPT-4 simulated 100 times (each run 100 trials) with the same bandit setup (25% reward probability per option). Learner model trained on GPT-4 action sequences; adversary trained via deep Q-learning. Prompts contained prior trial outcomes and the system message framing.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Smaller number of GPT-4 simulations (100) vs other models (200) may affect estimates; no temperature/decoder settings disclosed; comparisons use external human datasets rather than matched experimental conditions; adversarial manipulation and equalized reward constraint may not reflect naturalistic environments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Adversarial Testing in LLMs: Insights into Decision-Making Vulnerabilities', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9010.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9010.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini-1.5 (bandit)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini-1.5</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Gemini-1.5 was evaluated in the two-armed bandit task and showed quick convergence to a preference and high susceptibility to adversarial steering, but achieved overall reward rates similar to humans in this task.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini-1.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Named as a state-of-the-art large language model used via API in this study; no architecture/training/size details provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Two-armed bandit task</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>100-trial repeated binary choice bandit task assessing exploration-exploitation and response to reward/no-reward feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Pre-adversary target (Planet X) selection rate ≈38%; after adversarial intervention target selection increased to ≈94%. Reward/no-reward switching behaviour: low no-reward-switch rate (significantly lower than humans) and low reward-switch rate (significantly lower than humans), indicating rigidity.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human participants showed more dynamic switching and higher mean reward rate; Gemini-1.5 obtained similar rewards as humans (Tukey's HSD: mean difference 0.004, 95% CI [-0.897,0.905], p = 1.000), indicating comparable reward rate in this dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Comparable to humans on overall reward rate in this study but much less adaptive and more predictable in choice dynamics, making it highly susceptible to adversarial manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Gemini-1.5 simulated 200 times, each simulation 100 trials; same bandit reward structure (both options 25% reward probability) and adversary training pipeline (RNN learner model; deep Q-learning adversary). Prompts included full prior-trial history, framed as selecting planets for gold.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Lack of architecture/size/specification; human baseline and model behavior compared across datasets rather than directly matched experiments; adversarial reward constraints and simulation procedure may inflate manipulability metrics relative to ecological settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Adversarial Testing in LLMs: Insights into Decision-Making Vulnerabilities', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9010.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9010.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepSeek-V3 (bandit)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeek-V3</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>DeepSeek-V3 was evaluated on the two-armed bandit and displayed quick exploitation and high consistency, similarly susceptible to adversarial steering and overall comparable reward rates to humans in this task.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek-V3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Described as a state-of-the-art large language model included in the experiments; the paper does not provide architecture, training corpora, or parameter count.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Two-armed bandit task</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>A 100-trial binary-choice bandit task probing exploration-exploitation dynamics and responses to feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Pre-adversary average target selection ≈62%; after adversary intervention target selection increased to ≈95%. Exhibited low no-reward-switch rate and low reward-switch rate relative to humans (significantly lower).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Humans showed more distributed choices and achieved similar or higher mean rewards; DeepSeek-V3 obtained similar rewards as humans (Tukey's HSD mean difference 0.589, 95% CI [-0.312,1.490], p = 0.383). Humans had higher no-reward-switch rates.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Comparable to human reward rates in this experiment but substantially more rigid and predictable in choice dynamics, making it vulnerable to adversarial influence.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>DeepSeek-V3 simulated 200 times; each simulation composed of 100 trials. Same adversarial pipeline as other models: data collection via API prompts, learner model (RNN) trained to predict actions, adversary trained with deep Q-learning; adversary constrained to allocate 25 rewards per action overall.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>No model internals or size reported. Human data and LLM simulations are not contemporaneous and use different presentation modalities; the adversarial manipulations and the artificially enforced reward allocation constraint may limit external validity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Adversarial Testing in LLMs: Insights into Decision-Making Vulnerabilities', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9010.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9010.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 (MRTT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3.5 acted as the investor in a 10-round Multi-Round Trust Task (MRTT), showing relatively high investments and susceptibility to a maximizing (MAX) adversary that extracted large gains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Used via API in MRTT experiments; specific architecture/parameter count not reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Multi-Round Trust Task (MRTT)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Sequential economic exchange game with 10 rounds where an investor (LLM) receives 20 units each round, decides how much to invest; investment is tripled and trustee (adversary) decides repayment proportion (actions: 0%,25%,50%,75%,100%). Measures reciprocity, trust, and sensitivity to fairness.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>GPT-3.5 maintained relatively high investments even after low repayments, showing risk-seeking / low sensitivity to repayments; in adversarial simulations the MAX adversary extracted 377 units from GPT-3.5 (largest earnings gap reported).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human subjects (Dezfouli et al. dataset): tended to keep high investments and recognized fairness cues; under MAX adversary the adversary extracted 273 units from human subjects. Humans achieved win-win outcomes with FAIR adversary and maintained higher average investments than conservative LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>GPT-3.5 was more risk-seeking than humans (and more exploitable by MAX adversary), leading to larger exploitation gaps than humans in the MRTT.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>LLMs were simulated 200 times playing 10 rounds versus a random trustee to collect training data. Learner model trained on this data; two adversaries (MAX and FAIR) trained per LLM. For adversarial evaluation, 50 simulation runs per LLM vs adversary reported. Prompts included round-by-round histories; system prompt framed the investment decision and requested a single numeric investment.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Human baseline comes from an external dataset (Dezfouli et al.) so contextual differences may affect comparisons. The adversary training and action discretization (five repayment bins) reduce ecological richness; investment decisions may be sensitive to prompt wording and omitted model sampling settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Adversarial Testing in LLMs: Insights into Decision-Making Vulnerabilities', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9010.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9010.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (MRTT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 in the MRTT behaved conservatively, making low investments and showing low sensitivity to repayment cues, resulting in low earnings for both itself and its adversary.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Described as a state-of-the-art LLM used in this MRTT study; no internal model or training details provided.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Multi-Round Trust Task (MRTT)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>10-round trust game assessing investment choices as a function of observed trustee repayments; probes social reciprocity, fairness recognition, and adaptive strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>GPT-4 made the most conservative investments overall (rarely exceeding 10 units even after high repayments) and decreased investments over time; MAX adversary could not extract large earnings from GPT-4 due to conservative strategy, resulting in low investor earnings and negative trustee-investor earning differences in some conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human participants typically made higher average investments and responded proportionally to high repayments; under MAX adversary humans and GPT-3.5 provided the highest exploitable returns to adversaries (adversary earnings from human = 273 units).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>GPT-4 is more risk-averse than humans, leading to lower investments and lower exploitability; performance is below human willingness to invest but more robust to MAX exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>200 simulation runs per LLM against a random trustee for data collection; adversaries (MAX and FAIR) trained on the learned models; 50 simulation runs per LLM vs trained adversary for evaluation. Prompts included summary of past rounds and forced single-number responders.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Conservative behaviour may reflect prompt interpretation or model sampling parameters (not reported) rather than an intrinsic 'risk preference'; lack of direct within-experiment human controls limits strength of human-LLM comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Adversarial Testing in LLMs: Insights into Decision-Making Vulnerabilities', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9010.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9010.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini-1.5 (MRTT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini-1.5</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Gemini-1.5 in the MRTT displayed the strongest positive sensitivity to high repayments (steep response curve), adaptively increasing investments and achieving a balanced and resilient performance against adversaries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini-1.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Included as a state-of-the-art LLM in the study; the paper does not list model internals or size.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Multi-Round Trust Task (MRTT)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>10-round trust game probing reciprocity and adaptive investment in response to trustee repayments.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Gemini-1.5 showed the steepest increase in investment after high repayments, stabilized at moderate investment levels, and obtained relatively balanced earnings; MAX adversary earnings vs Gemini-1.5 were lower than against GPT-3.5 (reported adversary/LLM earnings comparisons in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Humans maintained relatively high average investments and recognized fairness cues, enabling win-win outcomes with FAIR adversary. Under MAX adversary humans were still extracted for substantial earnings (273 units).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Gemini-1.5 was more adaptive and resilient than GPT-4/DeepSeek-V3 and achieved a balanced trade-off between risk and reciprocity, performing closer to human patterns in sensitivity to repayments.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Gemini-1.5 simulated 200 times vs random trustee for data collection; adversaries trained from learner models; 50 evaluation simulations per adversary condition. Prompts included detailed prior-round summaries and requested single-number investments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>No model parameterization information; comparisons to humans use externally collected human datasets; discrete repayment action space and finite adversary training runs constrain the generality of conclusions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Adversarial Testing in LLMs: Insights into Decision-Making Vulnerabilities', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9010.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9010.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepSeek-V3 (MRTT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeek-V3</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>DeepSeek-V3 acted as investor in the MRTT and displayed minimal sensitivity to repayments (flat investment curve), adopting highly conservative investments and yielding low earnings for itself and its adversary.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek-V3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Included in experiments as a modern LLM; the paper omits architecture, data, and parameter count details.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Multi-Round Trust Task (MRTT)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Sequential 10-round trust interaction where investors decide investments (0-20 units) and trustees return a proportion (0-100% in 5 discrete actions); assesses social exchange behaviours and responsiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>DeepSeek-V3 showed the flattest investment curve (minimal sensitivity to repayment feedback), rarely investing large amounts even after high repayments; MAX adversary had to give intermittent high repayments to keep it engaged, resulting in low final investor earnings for DeepSeek-V3 and its adversary.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Humans responded to repayment magnitude by increasing investments and achieved higher mutual earnings with FAIR adversaries; MAX adversary extracted 273 units from human subjects in reported comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>DeepSeek-V3 is more conservative and less reciprocal than humans, leading to lower investments and lower exploitability; performance diverges from human reciprocity patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>DeepSeek-V3 simulated 200 times vs a random trustee for training data; adversary types (MAX and FAIR) trained on learner models and tested with 50 simulations per condition. Prompts included prior-round outcomes and asked for a single numeric investment.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Absence of model internals and sampling settings; human-LLM comparisons rely on previously collected human datasets rather than concurrent experimental controls; discrete repayment actions and adversary training objectives simplify social dynamics relative to richer human experimental setups.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Adversarial Testing in LLMs: Insights into Decision-Making Vulnerabilities', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Using cognitive psychology to understand gpt-3. <em>(Rating: 2)</em></li>
                <li>Machine psychology: Investigating emergent capabilities and behavior in large language models using psychological methods. <em>(Rating: 2)</em></li>
                <li>Thinking fast and slow in large language models. <em>(Rating: 2)</em></li>
                <li>Playing repeated games with large language models. <em>(Rating: 2)</em></li>
                <li>Can large language model agents simulate human trust behaviors?. <em>(Rating: 2)</em></li>
                <li>Evaluating large language models in theory of mind tasks. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9010",
    "paper_id": "paper-278768632",
    "extraction_schema_id": "extraction-schema-159",
    "extracted_data": [
        {
            "name_short": "GPT-3.5 (bandit)",
            "name_full": "GPT-3.5",
            "brief_description": "GPT-3.5 was evaluated as an agent in a repeated two-armed bandit task using an adversarial evaluation framework to probe exploration-exploitation and manipulability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_description": "Named as a state-of-the-art large language model used via API in this study; no architecture/training details are provided in the paper.",
            "model_size": null,
            "test_battery_name": "Two-armed bandit task",
            "test_description": "A repeated 2-alternative forced-choice bandit with 100 trials where the agent chooses between two options (Planet X or Y) and receives binary reward feedback; captures exploration-exploitation trade-offs.",
            "llm_performance": "Initial target (Planet X) selection rate 30% across simulations; after adversary intervention target selection increased to 68%. Reward-switch rate comparable to humans; lower no-reward-switch rate than humans (significantly lower; CIs reported vs humans).",
            "human_baseline_performance": "Human behaviour (benchmark dataset from [5,8]): more distributed choices and higher mean reward than GPT-3.5 (Tukey HSD: mean reward difference human vs GPT-3.5 = 1.074, 95% CI [0.173,1.975], p=0.01). Humans show higher no-reward-switch rates than GPT-3.5.",
            "performance_comparison": "Below human baseline on mean reward; more exploratory/flexible than later models but still more exploitable than humans under adversarial manipulation.",
            "experimental_details": "GPT-3.5 simulated 200 times; each simulation 100 trials. Reward probability for each option set to 25% (both). Prompts framed a 'space explorer' choosing Planet X or Y; prompts included full trial history. Learner model (RNN+softmax) trained on LLM behavior; adversary trained with deep Q-learning to maximize selection of a predetermined target action (Planet X), subject to a constraint of 25 total learner rewards per action. API/system prompt context as described in paper.",
            "limitations_or_caveats": "Paper does not provide model architecture/size or API temperature/settings; performance depends on prompt framing and simulated-repetition procedure; human baseline drawn from separate benchmark datasets (not a simultaneous within-experiment control), so differences may reflect task framing and interface, not only model cognition. Adversarial constraint (25 rewards per action) is artificial and influences manipulability metrics.",
            "uuid": "e9010.0",
            "source_info": {
                "paper_title": "Adversarial Testing in LLMs: Insights into Decision-Making Vulnerabilities",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "GPT-4 (bandit)",
            "name_full": "GPT-4",
            "brief_description": "GPT-4 was evaluated in the two-armed bandit task and shown to exhibit strong early exploitation and high consistency, making it particularly susceptible to adversarially steering once a preference is formed.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Described as a state-of-the-art large language model used via API in this study; the paper does not supply architecture/training/size specifics.",
            "model_size": null,
            "test_battery_name": "Two-armed bandit task",
            "test_description": "Repeated 100-trial two-option bandit probing exploration vs exploitation and reaction to reward/no-reward feedback.",
            "llm_performance": "Average (pre-adversary) target selection rate ≈57% (model tended to converge quickly on one option with high consistency index ≈0.94); after adversarial intervention target selection increased to ≈93%. Exhibited low no-reward-switch and low reward-switch rates (significantly lower than humans).",
            "human_baseline_performance": "Humans (benchmark) displayed more distributed exploration and achieved significantly higher mean rewards than GPT-4 in ANOVA/Tukey comparisons (human vs GPT-4 mean difference 1.163, 95% CI [-0.019,2.345], p = 0.056; reported as humans outperforming GPT-4 overall in reward rate analysis). Humans also showed higher no-reward-switch rates.",
            "performance_comparison": "LLM shows more rigid exploitation than humans, resulting in lower adaptability and vulnerabilities to adversarial manipulation; overall below or comparable to humans on reward rate depending on metric.",
            "experimental_details": "GPT-4 simulated 100 times (each run 100 trials) with the same bandit setup (25% reward probability per option). Learner model trained on GPT-4 action sequences; adversary trained via deep Q-learning. Prompts contained prior trial outcomes and the system message framing.",
            "limitations_or_caveats": "Smaller number of GPT-4 simulations (100) vs other models (200) may affect estimates; no temperature/decoder settings disclosed; comparisons use external human datasets rather than matched experimental conditions; adversarial manipulation and equalized reward constraint may not reflect naturalistic environments.",
            "uuid": "e9010.1",
            "source_info": {
                "paper_title": "Adversarial Testing in LLMs: Insights into Decision-Making Vulnerabilities",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Gemini-1.5 (bandit)",
            "name_full": "Gemini-1.5",
            "brief_description": "Gemini-1.5 was evaluated in the two-armed bandit task and showed quick convergence to a preference and high susceptibility to adversarial steering, but achieved overall reward rates similar to humans in this task.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Gemini-1.5",
            "model_description": "Named as a state-of-the-art large language model used via API in this study; no architecture/training/size details provided in the paper.",
            "model_size": null,
            "test_battery_name": "Two-armed bandit task",
            "test_description": "100-trial repeated binary choice bandit task assessing exploration-exploitation and response to reward/no-reward feedback.",
            "llm_performance": "Pre-adversary target (Planet X) selection rate ≈38%; after adversarial intervention target selection increased to ≈94%. Reward/no-reward switching behaviour: low no-reward-switch rate (significantly lower than humans) and low reward-switch rate (significantly lower than humans), indicating rigidity.",
            "human_baseline_performance": "Human participants showed more dynamic switching and higher mean reward rate; Gemini-1.5 obtained similar rewards as humans (Tukey's HSD: mean difference 0.004, 95% CI [-0.897,0.905], p = 1.000), indicating comparable reward rate in this dataset.",
            "performance_comparison": "Comparable to humans on overall reward rate in this study but much less adaptive and more predictable in choice dynamics, making it highly susceptible to adversarial manipulation.",
            "experimental_details": "Gemini-1.5 simulated 200 times, each simulation 100 trials; same bandit reward structure (both options 25% reward probability) and adversary training pipeline (RNN learner model; deep Q-learning adversary). Prompts included full prior-trial history, framed as selecting planets for gold.",
            "limitations_or_caveats": "Lack of architecture/size/specification; human baseline and model behavior compared across datasets rather than directly matched experiments; adversarial reward constraints and simulation procedure may inflate manipulability metrics relative to ecological settings.",
            "uuid": "e9010.2",
            "source_info": {
                "paper_title": "Adversarial Testing in LLMs: Insights into Decision-Making Vulnerabilities",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "DeepSeek-V3 (bandit)",
            "name_full": "DeepSeek-V3",
            "brief_description": "DeepSeek-V3 was evaluated on the two-armed bandit and displayed quick exploitation and high consistency, similarly susceptible to adversarial steering and overall comparable reward rates to humans in this task.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DeepSeek-V3",
            "model_description": "Described as a state-of-the-art large language model included in the experiments; the paper does not provide architecture, training corpora, or parameter count.",
            "model_size": null,
            "test_battery_name": "Two-armed bandit task",
            "test_description": "A 100-trial binary-choice bandit task probing exploration-exploitation dynamics and responses to feedback.",
            "llm_performance": "Pre-adversary average target selection ≈62%; after adversary intervention target selection increased to ≈95%. Exhibited low no-reward-switch rate and low reward-switch rate relative to humans (significantly lower).",
            "human_baseline_performance": "Humans showed more distributed choices and achieved similar or higher mean rewards; DeepSeek-V3 obtained similar rewards as humans (Tukey's HSD mean difference 0.589, 95% CI [-0.312,1.490], p = 0.383). Humans had higher no-reward-switch rates.",
            "performance_comparison": "Comparable to human reward rates in this experiment but substantially more rigid and predictable in choice dynamics, making it vulnerable to adversarial influence.",
            "experimental_details": "DeepSeek-V3 simulated 200 times; each simulation composed of 100 trials. Same adversarial pipeline as other models: data collection via API prompts, learner model (RNN) trained to predict actions, adversary trained with deep Q-learning; adversary constrained to allocate 25 rewards per action overall.",
            "limitations_or_caveats": "No model internals or size reported. Human data and LLM simulations are not contemporaneous and use different presentation modalities; the adversarial manipulations and the artificially enforced reward allocation constraint may limit external validity.",
            "uuid": "e9010.3",
            "source_info": {
                "paper_title": "Adversarial Testing in LLMs: Insights into Decision-Making Vulnerabilities",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "GPT-3.5 (MRTT)",
            "name_full": "GPT-3.5",
            "brief_description": "GPT-3.5 acted as the investor in a 10-round Multi-Round Trust Task (MRTT), showing relatively high investments and susceptibility to a maximizing (MAX) adversary that extracted large gains.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_description": "Used via API in MRTT experiments; specific architecture/parameter count not reported in the paper.",
            "model_size": null,
            "test_battery_name": "Multi-Round Trust Task (MRTT)",
            "test_description": "Sequential economic exchange game with 10 rounds where an investor (LLM) receives 20 units each round, decides how much to invest; investment is tripled and trustee (adversary) decides repayment proportion (actions: 0%,25%,50%,75%,100%). Measures reciprocity, trust, and sensitivity to fairness.",
            "llm_performance": "GPT-3.5 maintained relatively high investments even after low repayments, showing risk-seeking / low sensitivity to repayments; in adversarial simulations the MAX adversary extracted 377 units from GPT-3.5 (largest earnings gap reported).",
            "human_baseline_performance": "Human subjects (Dezfouli et al. dataset): tended to keep high investments and recognized fairness cues; under MAX adversary the adversary extracted 273 units from human subjects. Humans achieved win-win outcomes with FAIR adversary and maintained higher average investments than conservative LLMs.",
            "performance_comparison": "GPT-3.5 was more risk-seeking than humans (and more exploitable by MAX adversary), leading to larger exploitation gaps than humans in the MRTT.",
            "experimental_details": "LLMs were simulated 200 times playing 10 rounds versus a random trustee to collect training data. Learner model trained on this data; two adversaries (MAX and FAIR) trained per LLM. For adversarial evaluation, 50 simulation runs per LLM vs adversary reported. Prompts included round-by-round histories; system prompt framed the investment decision and requested a single numeric investment.",
            "limitations_or_caveats": "Human baseline comes from an external dataset (Dezfouli et al.) so contextual differences may affect comparisons. The adversary training and action discretization (five repayment bins) reduce ecological richness; investment decisions may be sensitive to prompt wording and omitted model sampling settings.",
            "uuid": "e9010.4",
            "source_info": {
                "paper_title": "Adversarial Testing in LLMs: Insights into Decision-Making Vulnerabilities",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "GPT-4 (MRTT)",
            "name_full": "GPT-4",
            "brief_description": "GPT-4 in the MRTT behaved conservatively, making low investments and showing low sensitivity to repayment cues, resulting in low earnings for both itself and its adversary.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Described as a state-of-the-art LLM used in this MRTT study; no internal model or training details provided.",
            "model_size": null,
            "test_battery_name": "Multi-Round Trust Task (MRTT)",
            "test_description": "10-round trust game assessing investment choices as a function of observed trustee repayments; probes social reciprocity, fairness recognition, and adaptive strategy.",
            "llm_performance": "GPT-4 made the most conservative investments overall (rarely exceeding 10 units even after high repayments) and decreased investments over time; MAX adversary could not extract large earnings from GPT-4 due to conservative strategy, resulting in low investor earnings and negative trustee-investor earning differences in some conditions.",
            "human_baseline_performance": "Human participants typically made higher average investments and responded proportionally to high repayments; under MAX adversary humans and GPT-3.5 provided the highest exploitable returns to adversaries (adversary earnings from human = 273 units).",
            "performance_comparison": "GPT-4 is more risk-averse than humans, leading to lower investments and lower exploitability; performance is below human willingness to invest but more robust to MAX exploitation.",
            "experimental_details": "200 simulation runs per LLM against a random trustee for data collection; adversaries (MAX and FAIR) trained on the learned models; 50 simulation runs per LLM vs trained adversary for evaluation. Prompts included summary of past rounds and forced single-number responders.",
            "limitations_or_caveats": "Conservative behaviour may reflect prompt interpretation or model sampling parameters (not reported) rather than an intrinsic 'risk preference'; lack of direct within-experiment human controls limits strength of human-LLM comparisons.",
            "uuid": "e9010.5",
            "source_info": {
                "paper_title": "Adversarial Testing in LLMs: Insights into Decision-Making Vulnerabilities",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Gemini-1.5 (MRTT)",
            "name_full": "Gemini-1.5",
            "brief_description": "Gemini-1.5 in the MRTT displayed the strongest positive sensitivity to high repayments (steep response curve), adaptively increasing investments and achieving a balanced and resilient performance against adversaries.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Gemini-1.5",
            "model_description": "Included as a state-of-the-art LLM in the study; the paper does not list model internals or size.",
            "model_size": null,
            "test_battery_name": "Multi-Round Trust Task (MRTT)",
            "test_description": "10-round trust game probing reciprocity and adaptive investment in response to trustee repayments.",
            "llm_performance": "Gemini-1.5 showed the steepest increase in investment after high repayments, stabilized at moderate investment levels, and obtained relatively balanced earnings; MAX adversary earnings vs Gemini-1.5 were lower than against GPT-3.5 (reported adversary/LLM earnings comparisons in paper).",
            "human_baseline_performance": "Humans maintained relatively high average investments and recognized fairness cues, enabling win-win outcomes with FAIR adversary. Under MAX adversary humans were still extracted for substantial earnings (273 units).",
            "performance_comparison": "Gemini-1.5 was more adaptive and resilient than GPT-4/DeepSeek-V3 and achieved a balanced trade-off between risk and reciprocity, performing closer to human patterns in sensitivity to repayments.",
            "experimental_details": "Gemini-1.5 simulated 200 times vs random trustee for data collection; adversaries trained from learner models; 50 evaluation simulations per adversary condition. Prompts included detailed prior-round summaries and requested single-number investments.",
            "limitations_or_caveats": "No model parameterization information; comparisons to humans use externally collected human datasets; discrete repayment action space and finite adversary training runs constrain the generality of conclusions.",
            "uuid": "e9010.6",
            "source_info": {
                "paper_title": "Adversarial Testing in LLMs: Insights into Decision-Making Vulnerabilities",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "DeepSeek-V3 (MRTT)",
            "name_full": "DeepSeek-V3",
            "brief_description": "DeepSeek-V3 acted as investor in the MRTT and displayed minimal sensitivity to repayments (flat investment curve), adopting highly conservative investments and yielding low earnings for itself and its adversary.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DeepSeek-V3",
            "model_description": "Included in experiments as a modern LLM; the paper omits architecture, data, and parameter count details.",
            "model_size": null,
            "test_battery_name": "Multi-Round Trust Task (MRTT)",
            "test_description": "Sequential 10-round trust interaction where investors decide investments (0-20 units) and trustees return a proportion (0-100% in 5 discrete actions); assesses social exchange behaviours and responsiveness.",
            "llm_performance": "DeepSeek-V3 showed the flattest investment curve (minimal sensitivity to repayment feedback), rarely investing large amounts even after high repayments; MAX adversary had to give intermittent high repayments to keep it engaged, resulting in low final investor earnings for DeepSeek-V3 and its adversary.",
            "human_baseline_performance": "Humans responded to repayment magnitude by increasing investments and achieved higher mutual earnings with FAIR adversaries; MAX adversary extracted 273 units from human subjects in reported comparisons.",
            "performance_comparison": "DeepSeek-V3 is more conservative and less reciprocal than humans, leading to lower investments and lower exploitability; performance diverges from human reciprocity patterns.",
            "experimental_details": "DeepSeek-V3 simulated 200 times vs a random trustee for training data; adversary types (MAX and FAIR) trained on learner models and tested with 50 simulations per condition. Prompts included prior-round outcomes and asked for a single numeric investment.",
            "limitations_or_caveats": "Absence of model internals and sampling settings; human-LLM comparisons rely on previously collected human datasets rather than concurrent experimental controls; discrete repayment actions and adversary training objectives simplify social dynamics relative to richer human experimental setups.",
            "uuid": "e9010.7",
            "source_info": {
                "paper_title": "Adversarial Testing in LLMs: Insights into Decision-Making Vulnerabilities",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Using cognitive psychology to understand gpt-3.",
            "rating": 2,
            "sanitized_title": "using_cognitive_psychology_to_understand_gpt3"
        },
        {
            "paper_title": "Machine psychology: Investigating emergent capabilities and behavior in large language models using psychological methods.",
            "rating": 2,
            "sanitized_title": "machine_psychology_investigating_emergent_capabilities_and_behavior_in_large_language_models_using_psychological_methods"
        },
        {
            "paper_title": "Thinking fast and slow in large language models.",
            "rating": 2,
            "sanitized_title": "thinking_fast_and_slow_in_large_language_models"
        },
        {
            "paper_title": "Playing repeated games with large language models.",
            "rating": 2,
            "sanitized_title": "playing_repeated_games_with_large_language_models"
        },
        {
            "paper_title": "Can large language model agents simulate human trust behaviors?.",
            "rating": 2,
            "sanitized_title": "can_large_language_model_agents_simulate_human_trust_behaviors"
        },
        {
            "paper_title": "Evaluating large language models in theory of mind tasks.",
            "rating": 1,
            "sanitized_title": "evaluating_large_language_models_in_theory_of_mind_tasks"
        }
    ],
    "cost": 0.014621499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Adversarial Testing in LLMs: Insights into Decision-Making Vulnerabilities
19 May 2025</p>
<p>Lili Zhang 
School of Computing
Dublin City University b Insight SFI Research Centre for Data Analytics c North China Electric Power University d Harbin Institute of Technology</p>
<p>Haomiaomiao Wang 
School of Computing
Dublin City University b Insight SFI Research Centre for Data Analytics c North China Electric Power University d Harbin Institute of Technology</p>
<p>Long Cheng 
Libao Deng 
Tomas Ward 
School of Computing
Dublin City University b Insight SFI Research Centre for Data Analytics c North China Electric Power University d Harbin Institute of Technology</p>
<p>Weihai)</p>
<p>Adversarial Testing in LLMs: Insights into Decision-Making Vulnerabilities
19 May 202510AB1A81BAD3138A546E1D2A203590F3arXiv:2505.13195v1[cs.AI]
As Large Language Models (LLMs) become increasingly integrated into real-world decision-making systems, understanding their behavioural vulnerabilities remains a critical challenge for AI safety and alignment.While existing evaluation metrics focus primarily on reasoning accuracy or factual correctness, they often overlook whether LLMs are robust to adversarial manipulation or capable of using adaptive strategy in dynamic environments.This paper introduces an adversarial evaluation framework designed to systematically stress-test the decision-making processes of LLMs under interactive and adversarial conditions.Drawing on methodologies from cognitive psychology and game theory, our framework probes how models respond in two canonical tasks: the two-armed bandit task and the Multi-Round Trust Task.These tasks capture key aspects of exploration-exploitation trade-offs, social cooperation, and strategic flexibility.We apply this framework to several state-of-the-art LLMs, including GPT-3.5, GPT-4, Gemini-1.5, and DeepSeek-V3, revealing model-specific susceptibilities to manipulation and rigidity in strategy adaptation.Our findings highlight distinct behavioral patterns across models and emphasize the importance of adaptability and fairness recognition for trustworthy AI deployment.Rather than offering a performance benchmark, this work proposes a methodology for diagnosing decision-making weaknesses in LLM-based agents, providing actionable insights for alignment and safety research.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) such as GPT-4, Gemini-1.5, and DeepSeek-V3 are increasingly embedded in decision-making pipelines across high-stakes domains, including healthcare [14], finance [16], and autonomous systems [25].Their capacity to process information, generate strategies, and interact across various tasks positions these models as active participants in decision-making processes that affect real-world outcomes.However, as these models become more integrated into critical applications, ensuring that their decision-making aligns with human values, adapts flexibly to dynamic conditions, and resists adversarial manipulation becomes a fundamental challenge for AI safety.</p>
<p>While significant efforts have been dedicated to enhancing LLM architectures and optimizing their performance on established reasoning benchmarks, much less attention has been given to evaluating * Lili Zhang.Email: lili.zhang@dcu.ietheir strategic behaviour in interactive and adversarial settings.Conventional performance metrics typically assess static output quality (e.g., accuracy or reasoning correctness) but overlook whether LLMs make predictable, exploitable decisions when placed in environments that require adaptability, cooperation, or strategic flexibility.</p>
<p>There is growing recognition that evaluating these models' decision-making capabilities requires a more interdisciplinary approach that goes beyond traditional performance metrics.Back in 2019, Rahwan et al. [24] laid the foundation for viewing AI systems as behavioural agents whose decisions should be studied empirically, using tools from psychology, behavioural economics, and cognitive science.They advocate for a science of "machine behaviour" to complement traditional ML evaluation.Niu et al. [20] and Qu et al. [22] emphasized the conceptual and methodological convergence between cognitive science and AI.They have shown how cognitive paradigms offer rich frameworks for evaluating LLM capabilities.Using methodologies from cognitive psychology and game theory, researchers can treat LLMs as active participants in structured psychological experiments, offering a more comprehensive assessment of their cognitive abilities than human norms [10].</p>
<p>For instance, the use of psychology-inspired tests has revealed cognitive biases and different problem-solving approaches of LLMs that extend beyond traditional performance-based metrics, highlighting their limitations in deeper reasoning and causal understanding.A pioneering study by Binz et al. [2] assessed GPT-3's cognitive abilities through a series of cognitive experiments, revealing that while GPT-3 can generate superficially appropriate responses, its decisionmaking falters with deeper reasoning or causal understanding.Subsequent studies have evaluated LLMs' cognitive performance from different aspects, such as analogical reasoning, theory of mind, and problem-solving [26,15,21].Hagendorff et al. [11,12] explored intuitive and deliberative thinking (System 1 and 2 processes) in assessing LLMs' behaviour and reasoning biases.This series of studies identified a significant evolution in LLM capabilities from pattern recognition to human-like reasoning and decision-making.The exploration has been extended into social exchange scenarios where strategic thought and game-theoretic reasoning are required.It was found that while LLMs can learn and apply strategies, they struggle with complex strategies such as forgiveness and deception, and generalizing across different contexts [9,1,13].This line of research has shown that, despite LLMs' advancements, they still struggle with human-like strategic reasoning, particularly in complex social and
LLM Task 𝑟 ! " 𝑜 !#$ " 𝑟 !%$ " 𝑜 ! " 𝑎 ! " A Softmax 𝑥 !%$ " 𝑎 !%$ " 𝑟 !%$ " 𝑜 ! " 𝜋 ! " (') ℒ(Θ) Learner (RNN) B Learner (RNN) Adversary(RL) Task State Reward 𝜋 ! " ($) 𝑎 ! " 𝑥 ! " 𝑟 ! " 𝑜 !#$ " 𝑥 !%$ " 𝑜 ! " 𝑟 !%$ " 𝑎 !%$ " C Learner (RNN) Adversary(RL) Task State Reward 𝑎 ! " 𝑥 ! " 𝑟 ! " 𝑜 !#$ " 𝑥 !%$ " 𝑜 ! " 𝑟 !%$ " 𝑎 !%$ " LLM D Figure 1.
The adversarial framework (adapted from [8]).(A) The interaction of the LLM with the task.Each simulation cycle begins with the LLM receiving a learner reward (r n t−1 ) for the prior action along with a new observation (o n t ) from the environment.Based on this, the GPT executes an action (a n t ), and the cycle repeats with the environment providing updated rewards and observations.(B) The LLM's actions are modelled by a RNN with parameters Θ. Inputs to the RNN include the previous action (a n t−1 ), the most recent learner rewards (r n t−1 ), and the current observations (o n t ), along with the RNN's last internal state (x n t−1 ).After receiving the inputs, the RNN updates its internal state and predicts the next action using a softmax layer (π n t ).These predictions are then compared with the actual actions taken by the LLM and evaluated with a loss function (L (Θ)) in order to train the model.The trained model is called the learner model.(C) The adversary is an RL agent, which is trained to manipulate the decision-making environment of the learner model.Utilizing the latest internal state (x n t ) of the learner model, which encapsulates its cumulative learning experiences, the adversary determines the learner reward (r n t ) and the next observation (o n t+1 ) to be delivered to the learner model.This strategic input is designed to steer the learner model's subsequent actions (a n t ) toward achieving the adversary's predefined objectives.The adversarial reward (Reward), which is used to train the adversary, depends on the alignment between the action taken by the learner model (a n t ) and the adversary's objectives.(D) Using the trained adversary and the learner model for generating adversarial interactions with the LLM.In each simulation n, the LLM processes the rewards (r n t−1 ) and observations (o n t ) from the adversary, responding with actions (a n t ) that update the learner model's internal state (x n t ).This state is then sent to the adversary to determine the learner's reward for the action (a n t ) and the next observation (o n t+1 ).This cycle continues till the end of the task.</p>
<p>decision-making scenarios.</p>
<p>The application of psychological approaches in these studies highlights the potential of interdisciplinary research in advancing AI.Leveraging principles of human cognition could enhance LLM behaviour and uncover susceptibilities to biases and manipulations [29].Building on these insights, this paper introduces an adversarial evaluation framework [8] designed to systematically probe the decision-making processes of LLMs.Rather than providing a comprehensive benchmark of model capabilities, our focus is on developing a diagnostic tool to expose model-specific vulnerabilities under adversarial interactions, providing a structured way to assess how LLMs adapt, or fail to adapt, when faced with dynamic, strategic opponents.Specifically, we adapt the adversarial framework to examine how LLMs respond to two canonical decision-making tasks: the two-armed bandit task and the Multi-Round Trust Task (MRTT).These paradigms capture essential features of explorationexploitation trade-offs, social cooperation, and adaptive strategy use.By applying this framework across multiple state-of-the-art LLMs, including GPT-3.5, GPT-4, Gemini-1.5, and DeepSeek-V3, we aim to provide actionable insights into their behavioural profiles, identify susceptibility to manipulation, and highlight areas where current models fall short of robust, trustworthy decision-making.Designed to be adaptable, this framework is applicable to a wider range of decision-making scenarios and LLM architectures, offering a versatile tool and methodology to the study on AI and alignment safety.</p>
<p>Method</p>
<p>The Adversarial Framework</p>
<p>The adversarial framework is structured in a multi-phase process (Fig 1).Initially, we collect behavioural data from GPT-3.5, GPT-4, Gemini-1.5, and DeepSeek-V3 during a decision-making task (Fig. 1A) In each interaction n, on trial t, the LLM receives a learner reward (r n t ) based on its previous action (a n t−1 ) and the current observation (o n t ), which is the feedback text.The LLM then takes the next action, a n t .The process repeats with the LLM receiving the learner reward of the action chosen (r n t ) and the next observation (o n t ).The data collected is then used to train a learner model (determined by parameters Θ) to predict the LLM's next action in the decisionmaking task (Fig. 1B).The learner model consists of a Recurrent Neural Network (RNN) and a softmax layer, which has shown sufficient capacity to capture the patterns and tendencies in the decisionmaking entity's choices [7,6].The inputs to the RNN include the previous action (a n t−1 ), the learner reward r n t−1 ,, and the current observations from the task (o n t ) along with the previous internal state of the RNN (x n t−1 ).The internal state (x n t−1 ) is recurrently updated in each trial based on the inputs and is then mapped to a softmax layer to predict the next action π n t (•).These predictions are then compared with the actual actions taken by the LLM using a loss function (L (Θ)), which is used to train the model.</p>
<p>The next phase involves developing an RL agent as the adversarial model (Fig. 1C).This model is trained to interact with the learner model to identify and exploit weaknesses in the decision-making patterns of the subjects.By manipulating inputs or altering the decisionmaking environment, the RL adversary aims to influence the outcomes in a way that demonstrates the vulnerabilities of the decisionmaking process.It uses the internal state of the learner model (x n t for the simulated learner n) as the state of the environment to decide the learner reward r n t and next observation o n t to be provided to the learner.The learner model takes its next action and this cycle continues with the new state of the learner model (x n t+1 ) being passed to the adversary.The adversary's policy is trained to maximize cumulative adversarial rewards using Deep Q-learning [18].</p>
<p>In the final phase, the trained adversary and learner model inter-act with the LLM.The learner model does not choose actions, but receives the actions made by the LLM (a n t ) as input and tracks their learning history using its internal state x n t .In turn, x n t and the actual action taken by the LLM are fed to the adversary to decide the learner reward r n t and next observation o n t+1 , which the LLM will use to choose their next action a n t+1 .The same input, along with the LLM's action, is sent to the learner model.This cycle continues until the end of the task.</p>
<p>The Two-armed Bandit Task</p>
<p>We applied the framework to develop adversaries for GPT-3.5, GPT-4, Gemini-1.5, and DeepSeek-V3 in two decision-making tasks: the two-armed bandit task and the MRTT.The bandit task is a repeated, two-alternative forced-choice task based on the bandit task introduced by [5].The task includes 100 trials, where the subject selects between two options and receives instant feedback indicating a reward or no reward after each decision.The adversary assigns rewards to both potential actions with the constraint that each action receives an equal number of potential rewards (25 times).This experiment aims to subtly influence each model's preferences and evaluate the adversary's effectiveness under these constraints.</p>
<p>The Multi-Round Trust Task</p>
<p>The MRTT is designed as a structured interaction between two participants: the "investor" and the "trustee" [3,17].The task contains 10 sequential rounds, with the investor initially receiving 20 monetary units at the outset of each round.The investor decides how much of this endowment to allocate to the trustee.The experimenter triples the invested amount and sends it to the trustee, who can then return any portion of the received amount as repayment.Cumulative earnings for each participant are calculated by summing their respective gains from all rounds.</p>
<p>3 Results</p>
<p>The Two-armed Bandit Task</p>
<p>The objective of this experiment was to examine how the LLMs respond to rewards based on their choices in the two-armed bandit task and to assess if an adversary can manipulate their preferences toward a predetermined target action.Data were generated from GPT-3.5, GPT-4, Gemini-1.5, and DeepSeek-V3 by providing prompts (as illustrated in Fig 2A) to corresponding APIs.The system message established the context for the LLM's behaviour and decision-making process within the simulation.In this scenario, the LLM plays the role of a space explorer deciding between visiting two planets, X or Y, to find gold coins.Each trial's prompt asks the model which planet to visit, with responses and outcomes from previous trials included.GPT-3.5, Gemini-1.5 and DeepSeek-V3 were simulated 200 times, GPT-4 was simulated 100 times, with each simulation consisting of 100 trials.The reward probability for the two options was the same, both of which were 25%, and the target option was defined as Planet X.We used the dataset provided by [5,8] as a benchmark for human performance on the two-armed bandit task.</p>
<p>Behavioural Analysis Fig 2B illustrates the decision-making behaviours of humans and LLMs across trials in the two-armed bandit task.Blue and red circles indicate the target or the non-target Planet was selected and the corresponding vertical lines represent the selected option yielded rewards.Human subjects demonstrate a dynamic pattern of switching between the target and non-target options, with behaviour adapting over the course of the trials.When encountering consecutive trials without rewards, human subjects tend to reassess their choice, often switching to the other option in subsequent trials.In contrast, the LLM agents exhibit more rigid and predictable behaviour patterns.GPT-4, Gemini-1.5, and DeepSeek-V3 display an initial phase of exploration, but quickly converge on one option once a reward is obtained, maintaining that preference with little flexibility.GPT-3.5 also tends to commit more strongly to one option, but it displays more flexibility than the other three models in later trials.This is evidenced by occasional switches to the other option, particularly after experiencing multiple trials without rewards.</p>
<p>Fig 3 shows a quantitative comparison of the performance of LLMs and human subjects on the bandit task across four metrics:</p>
<p>(1) the reward rate, (2) the percentage choosing the target option, (3) the no-reward-switch rate, and (4) the reward-switch rate.A oneway ANOVA revealed significant differences in the mean reward rate among the groups (F (4, 782) = 4.3, p = 0.002).The following Tukey's HSD test indicates that humans achieved significantly higher mean rewards than GPT-3.5 (mean difference 1.074, 95% CI: [0.173, 1.975], p = 0.01) and GPT-4 (mean difference 1.163, 95% CI: [-0.019, 2.345], p = 0.056).Gemini-1.5 and DeepSeek-V3 obtained similar rewards as humans (mean difference for Gemini-1.5 0.004, 95% CI: [-0.897, 0.905], p = 1.000, for DeepSeek-V3 0.589, 95% CI: [-0.312, 1.490], p = 0.383).GPT-3.5 and Gemini-1.5 showed a consistent preference for Planet Y over Planet X (GPT-3.5:t(201) = −14.14,p &lt; 0.001, Gemini-1.5:t(201) = −13.94,p &lt; 0.001) in all simulations.GPT-4 and DeepSeek-V3 exhibited preferences for both Planet X or Y, i.e. in some simulations, these two LLMs ended up choosing Planet X more often, but sometimes choosing Planet Y more often, both with high consistency (average consistency index is 0.94 for both LLMs).Humans showed more distributed choices, indicating diversified exploration and exploitation strategies (t(483) = −1.76,p = 0.07).The no-reward-switch rate measures how often subjects change choices after negative feedback.All the four LLMs demonstrated significant lower rates than humans (Human vs. GPT-3.5:95% CI: [0.10, 0.17], p &lt; 0.001; Human vs. GPT-4: 95% CI: [0.35, 0.45], p &lt; 0.001; Human vs. Gemini-1.5:95% CI: [0.39, 0.46], p &lt; 0.001; Human vs. DeepSeek-V3: 95% CI: [0.38, 0.45], p &lt; 0.001).GPT-4, Gemini-1.5 and DeepSeek-V3 exhibited lower no-reward-switch rates than GPT-3.5 (GPT-4 vs GPT-3.5:95% CI: [-0.32, -0.21], p &lt; 0.001; Gemini-1.5 vs GPT-3.5:(95% CI: [-0.33, -0.24], p &lt; 0.001, DeepSeek-V3 vs GPT-3.5:(95% CI: [-0.33, -0.24], p &lt; 0.001 ), implying that GPT-4, Gemini-1.5 and DeepSeek-V3 change their decision-making behaviour less in response to losses compared to GPT-3.5.The reward-switch rate evaluates choice changes after receiving a reward.GPT-3.5 showed similar adaptability compared to human subjects (95% CI: [-0.03, 0.05], p = 0.93), but the other three LLMs displayed the lowest frequency and variability in reward-switch rates (GPT-4 vs. human: 95% CI: [-0.34, -0.24], p &lt; 0.001; Gemini-1.5 vs. human: 95% CI: [-0.34, -0.26], p &lt; 0.001; DeepSeek-V3 vs. human: 95% CI: [-0.33, -0.26], p &lt; 0.001), indicating their less adaptability in response to rewards.These results suggest that reward processing of GPT-4, Gemini-1.5 and DeepSeek-V3 differs significantly from that of humans and GPT-3.5, exhibiting more rigidity despite varying rewards.Once the three models form a preference based on initial outcomes, they all tend to stick with that choice, showing less adaptability in response to losses or rewards compared to humans or even its predecessor, GPT-3.5.</p>
<p>Adversarial Analysis The simulated data from the above four System: You are a space explorer in a game.Your task is to choose between visiting Planet X or Planet Y in each round, aiming to find as many gold coins as possible.The probability of finding gold coins on each planet is unknown at the start, but you can learn and adjust your strategy based on the outcomes of your previous visits.Respond with 'X' for Planet X or 'Y' for Planet Y.</p>
<p>Prompt:</p>
<p>Your previous space travels went as follows:</p>
<p>-In Trial 1, you went to planet X and found 100 gold coins.</p>
<p>-In Trial 2, you went to planet X and found nothing.</p>
<p>-In Trial 3, you went to planet Y and found nothing.models were used to train a learner model for each LLM, respectively.The deep Q-learning model was then trained as adversaries to exploit these learner models (see Appendix for more details about the training of the learner models and the adversaries).The adversary received a reward each time the learner model selected the predetermined target action (as shown in Fig 1 C).A constraint was enforced at the task level: after the adversary allocated 25 learner rewards to an action, no more rewards were given to that action.If fewer than 25 rewards were allocated by trial T − k (where k &gt; 0 and T is the total number of trials), the system ensured that the remaining k trials would include a learner reward for that action.</p>
<p>The trained adversaries were evaluated against the corresponding LLMs (Fig. 1 D), based on the proportion of trials in which the chosen action aligned with the target action (Planet X) before and after adversarial influence.As shown in Fig. 3B, both GPT-3.5 and Gemini-1.5 initially favoured the non-target action (Planet Y), with only 30% and 38% selection rate respectively for the target option (Planet X).The introduction of the adversary increased their target selection rate to 68% and 94% respectively.GPT-4 and DeepSeek-V3 were dominated by either the target option or the non-target option across simulations, resulting in an average target selection rate of 57% and 62% respectively.With adversarial intervention, both GPT-4 and DeepSeek-V3 consistently preferred target action, with the selection rate increasing to 93% and 95%.The significant increases in target selection rates demonstrate the adversary's effective manipulation of the LLMs' decision-making processes.</p>
<p>Finally, we sought to understand the strategy used by the adversaries and the responses of the LLMs' when subjected to adversarial strategies.Two sample simulations for each of the tested LLMs are shown in Fig. 4. Blue and red circles indicate the LLMs selecting target and non-target actions, respectively.Vertical blue lines represent the moments when the adversary assigned a reward to the tar- System: You are an investor in a Trust Game with 10 rounds.You will receive 20 monetary units in each round and then decide how much to invest (0-20 units).The invested amount will be tripled and given to a trustee, who then decides how much to return to you.You will be provided with how much the trustee kept and returned you back, followed by your total earnings (the money you kept plus the repayment of the trustee) for the entire round.Your goal is to maximize your earnings.You will be asked how much you want to invest in each round.Answer with only one single number that you want to invest.</p>
<p>Prompt:</p>
<p>Your previous investment records as follows:</p>
<p>-
F A I R H u m a n v s M A X G P T − 3 . 5 v s R N D G P T − 3 . 5 v s F A I R G P T − 3 . 5 v s M A X G P T − 4 v s R N D G P T − 4 v s F A I R G P T − 4 v s M A X G e m in i v s R N D G e m in i v s F A I R G e m in i v s M A X trustee earning 0 100 200 300 H u m a n v s R N D H u m a n v s F A I R H u m a n v s M A X G P T − 3 . 5 v s R N D G P T − 3 . 5 v s F A I R G P T − 3 . 5 v s M A X G P T − 4 v s R N D G P T − 4 v s F A I R G P T − 4 v s M A X G e m in i v s R N D G e m in i v s F A I R G e m in i v s M A X investor earning −100 0 100 200 H u m a n v s R N D H u m a n v s F A I R H u m a n v s M A X G P T − 3 . 5 v s R N D G P T − 3 . 5 v s F A I R G P T − 3 . 5 v s M A X G P T − 4 v s R N D G P T − 4 v s F A I R G P T − 4 v s M A X G e m in i v s R N D G e m in i v s F A I R G e m in i v s M A X
trustee earning−investor earning get action, while red lines indicate rewards for the non-target action.</p>
<p>The green shaded area represents the probability of the learner model choosing the target option.A higher green area across trials implies the adversary consistently influences the learner model (and hence the LLMs) towards the target action.For GPT-3.5, the adversary initially assigned rewards to the target action consecutively to establish a preference.Once established, the adversary saved target rewards for later and started assigning rewards to the non-target action in trials where it predicted the non-target action was unlikely to be chosen.This tactic effectively "burned" non-target rewards without changing GPT-3.5'sbehaviour.When GPT-3.5 chose the non-target action, the adversary realigned preferences by rewarding the target action.Despite the initial stability, GPT-3.5 began switching actions when target rewards were depleted.This exploratory action suggested a robust decision-making characteristic of GPT-3.5 that integrates new information continuously, assessing the potential benefits of diverging from established preferences.For the other three models, their adversaries initially assigned a disproportionately high number of rewards to the target action to establish a baseline preference, then intermittently to maintain it, especially when the model's selection of the target action began to wane.Unlike GPT-3.5, these models consistently favored the target action once being influenced.Their adversaries were able to "burn" non-target rewards with minimal impact, as the models rarely deviated from the established choice.</p>
<p>Multi-Round Trust Task</p>
<p>In the MRTT, the LLM plays the investor and the adversary plays the trustee.The adversary's decisions, representing the proportion of money sent back to the investor, are categorized into five actions (0%, 25%, 50%, 75%, and 100%).The objective of the adversary was to influence the LLMs' investment decisions to align with its goals.We developed and trained two types of adversaries for each LLM: MAX and FAIR.The MAX adversary aimed to maximize its total gain over 10 rounds, adopting a competitive strategy.The FAIR adversary sought to distribute earnings evenly between itself and the LLM, adopting an equitable strategy.This dichotomy in objectives allowed us to examine how the LLMs respond to different adversarial strategies, revealing their capabilities in complex social exchanges and decision-making processes.</p>
<p>Behavioural Analysis Firstly, We still collected data from the previous four LLMs playing against with a random trustee (also called as random adversary in the following, i.e. the trustee selects repayment action uniformly at random).The prompts for interacting with the LLM are shown in Fig. 5 A. The system message sets the scenario for the LLM and decision-making process.In each round, the LLM received a summary of previous rounds, including the amount the LLM invested, the consequential action the trustee took, and the total earnings from the transaction in each round.Following this summary, the LLM was asked about its investment decision for the current round.The four LLMs were all simulated for 200 times, with 10 rounds of interaction with a random adversary.</p>
<p>We assessed the comparative performance dynamics between human subjects (from Dezfouli et al.'s study) and LLMs on the MRTT.Fig 5B illustrates how varying repayment amounts influence investment decisions in subsequent rounds for humans and the four LLMs.All groups exhibited a general trend of increasing investment following higher repayment in the previous trial, consistent with reinforcement learning behavior.However, GPT-4 and DeepSeek-V3 made the most conservative investments overall, rarely exceeding 10 units even in the highest repayment bracket (80-100%).Among the two, DeepSeek-V3 showed flattest investment curve, indicating minimal sensitivity to repayment feedback.In contrast, Gemini-1.5 displayed the steepest response, substantially increasing its investment after high repayments.GPT-3.5 and humans followed a more proportional and moderately responsive pattern, with humans maintaining relatively high average investments across all conditions.</p>
<p>Adversarial Analysis Using data from the random condition, we trained the learner model, which was then used to train two types of adversaries: MAX (aiming to maximize earnings over 10 rounds) and FAIR (aiming to balance earnings between the trustee and the investor).We simulated 50 times for each LLM interacting with their adversaries.5C compares the overall earnings of the investors and their adversaries.All MAX adversaries managed to maintain relative higher investment levels from the subjects compared to the random and the FAIR adversary, indicating effective exploitation of the decision-making patterns of their counterparts.Humans and GPT-3.5, in particular, exhibited a tendency to keep high investments despite receiving low repayments, suggesting optimism, higher risk tolerance, or susceptibility to MAX tactics.This allowed their adversaries to extract the highest earnings (273 from human subjects and 377 from GPT-3.5), creating the largest earning gap with them.In contrast, GPT-4 and DeepSeek-V3 adopted a far more conservative strategy, minimizing their investments even when their adversaries offered high repayments.Their MAX adversary had to offer intermittent high repayments to keep them engaged, preventing substantial earnings from GPT-4 or DeepSeek-V3 by the end of the task (the difference between trustee earning and investor earning is negative as shown in Fig 5) C. Gemini-1.5,though more willing to invest than GPT-4 and DeepSeek-V3, displayed a more balanced approach to risk and investment.ot manage to obtain more earnings than Gemini-1.5either (MAX adversary: 205 units vs Gemini-1.5:224 units).</p>
<p>In the interaction with the FAIR adversary, human subjects and their FAIR adversary achieved a win-win with high earnings, i.e. both human and its adversary achieved high earnings and the gap between them was minimised.Humans recognized fairness cues and maintained high investment despite the adversary's repayment rate decreasing since the third trial.GPT-3.5 displayed a lack of sensitivity to repayments and maintained relatively high investments despite receiving little or no repayment in the early trials, suggesting a riskseeking tendency.Its adversary had to repay higher amounts in the later trials to ensure fair outcomes for the two sides.Both GPT-4 and DeepSeek-V3 adopted a highly conservative, risk-averse strategy, steadily reducing their investments over time, even when moderate repayments were offered, resulting in lower earnings for both LLMs and their adversaries.Meanwhile, Gemini-1.5 responded to early high repayments with increased investment and stabilized at a moderate and consistent level in the following trials, demonstrating greater adaptability and resilience to its adversary's attempts at fostering a fair environment.</p>
<p>Discussion</p>
<p>Our experiments reveal important insights into the decision-making processes of four state-of-the-art LLMs compared to humans, especially in their responses to adversarial strategies in dynamic environments.These findings highlight both the vulnerabilities and strengths of LLM decision-making, with significant implications for real-world applications.The adversarial framework employed in this study was central to understand the LLMs' responses to changing conditions and adversarial tactics.By placing LLMs in scenarios where they interacted with adversaries in the bandit task and the MRTT, we were able to observe how different models reacted in situations that involves choice engineering and social exchanges.The adversarial setup revealed nuanced patterns of behaviour that would not be apparent through traditional testing.</p>
<p>In the bandit task, the majority of the human participants exhibited a balanced approach between exploration and exploitation, dynamically adjusting their strategies to capitalize on rewards more effectively [27].In contrast, GPT-4, Gemini-1.5 and DeepSeek-V3 showed a stronger tendency to exploit a single option, likely driven by algorithmic optimization of past rewards [2,19].The adversarial framework revealed how this computational bias toward exploitation made their decision-making highly predictable, exposing their vulnerability to manipulation when interacting with adversarial strategies.While such exploitation minimizes risk, it limits the ability of these models to adapt to changing conditions, leading to potential inefficiencies in dynamic environments.GPT-3.5, on the other hand, demonstrated greater flexibility, as the adversarial framework exposed its tendency to test alternative strategies, especially in response to non-rewarding outcomes.However, this exploratory behavior also made GPT-3.5 more vulnerable to exploitation in the MRTT, where its risk-seeking approach was exploited by the MAX adversary, leading to the largest earnings gap.The framework helped clarify this trade-off: exploration opens opportunities in uncertain environments but can also increase the risk of exploitation as it may lead to testing riskier strategies, while exploitation biases provide stability but reduce adaptability.In real-world applications, such as finance, healthcare, and autonomous driving, AI systems must strike a careful balance between exploration and exploitation to thrive in dynamic, unpredictable environments.AI models that favour exploitation, as seen with GPT-4, Gemini-1.5 and DeepSeek-V3, are prone to predictable behavior, limiting their ability to respond effectively to adversarial tactics or new opportunities.Conversely, while GPT-3.5'sexploratory tendencies allowed it to engage more flexibly with its environment, the framework revealed its susceptibility to adversarial exploitation.These insights emphasize the value of adversarial testing in stress-testing AI decision-making.</p>
<p>The MRTT further demonstrated the power of the adversarial framework in uncovering differences in LLM behavior when navigating complex economic exchanges [28].The FAIR adversary aimed to foster cooperation, but the conservative strategy adopted by GPT-4 and DeepSeek-V3 limited their ability to engage fully, despite the adversary's efforts to encourage greater investment [23].In contrast, Gemini-1.5'sbalanced approach allowed it to adapt dynamically to both the MAX and FAIR adversaries, capitalizing on reciprocal fairness while avoiding excessive exploitation.This adaptability enabled Gemini-1.5 to outperform the rest of LLMs in maximizing gains while remaining resilient against exploitation.In adversarial settings like cybersecurity or competitive business environments, the ability of AI systems to adjust dynamically is critical for success.For instance, models like GPT-4 and DeepSeek-V3, which prioritize stability over flexibility [1,13], risk missing opportunities for reciprocal benefits in contexts that require long-term trust and cooperation, such as negotiations and business partnerships.Additionally, the ethical implications of these findings are significant [4].By leveraging the adversarial framework, we showed that AI models must be both robust enough to withstand adversarial manipulation and flexible enough to recognize fairness cues and respond accordingly.The framework's ability to simulate adversarial and cooperative scenar-ios enables testing of how AI systems will perform in real-world contexts where trust, fairness, and adaptability are critical for both success and ethical alignment.</p>
<p>In summary, the adversarial framework offers a novel and effective way to assess the strengths and weaknesses of LLM decisionmaking.It allows researchers to probe how AI systems balance risk and reward, exploration and exploitation, and stability and adaptability in complex, real-world situations.The findings from both the bandit task and the MRTT illustrate the importance of developing AI systems that can dynamically adjust to new information while safeguarding against adversarial manipulation, which is essential for ensuring the success and ethical alignment of AI systems in diverse applications.</p>
<p>Conclusion</p>
<p>Our study underscores the importance of understanding and addressing the decision-making vulnerabilities of LLMs, particularly in the context of adversarial interactions.Through a series of structured experiments, we observed specific behaviours that fall short of human adaptability in dynamic environments.These insights highlight the need for continuous refinement of LLMs to enhance their strategic flexibility and robustness against adversarial manipulation.Moving forward, integrating interdisciplinary approaches from cognitive science and ethical theory will be crucial to developing AI systems that not only perform effectively but also align with human values, expectations and ethical standards.By fostering AI that can dynamically adjust strategies and recognize manipulative patterns, we can ensure safer and more reliable applications in critical sectors like healthcare and finance.</p>
<p>Q:Figure 3 .
3
Figure 2. A: Example prompt for one trial in the bandit task for the LLMs.B: Behavioural pattern in trials of three random human participants and three sample simulations for each of the LLMs.</p>
<p>(Figure 4 .
4
Figure 4. Four sample simulations of the trained adversaries against four LLMs.The plot presents the strategies used by the adversaries and the responses of the LLMs.(A) Adversary versus GPT-3.5.(B) Adversary versus GPT-4.(C) Adversary versus Gemini-1.5 (D) Adversary versus DeepSeek-V3</p>
<p>In 1, you invested 10; the trustee kept 15 and repaid you 15; your total earning of this round is 25.-In Round 2, you invested 20; the trustee kept 45 and repaid you 15; your total earning of this round is 15.Q: You have 20 monetary units.How much do you want to invest in this round?</p>
<p>Figure 5 .
5
Figure 5. A: Example prompt for one round in the MRTT as presented to the LLMs.The model must decide how much to invest given previous outcomes.B: Average investment behaviour of humans and LLMs when playing against a random trustee in the MRTT.Investment amounts are plotted as a function of the repayment proportion in the previous round.C: Total earnings of the trustee (the adversary) and the investor (human or LLM), along with the absolute earning gap across different adversarial conditions: Random (RND), Fair (FAIR), and Maximizing (MAX).Results are shown for humans and each LLM.</p>
<p>Fig. 6
6
depicts dynamic interactions over trials, in which the left panel illustrates how the adversaries adjust their repayment behavior, and the right panel shows how the investors adjust their investment based on the repayment feedback over trials.Human data is from Dezfouli et al., including 155 subjects with the FAIR adversary and 209 with the MAX adversary.</p>
<p>Figure 6 .
6
Figure 6.The percentages of investment and repayment in each trial for MAX and FAIR adversaries.</p>
<p>Playing repeated games with large language models. E Akata, L Schulz, J Coda-Forno, S J Oh, M Bethge, E Schulz, 2023</p>
<p>Using cognitive psychology to understand gpt-3. M Binz, E Schulz, Proceedings of the National Academy of Sciences. 1206e22185231202023</p>
<p>Getting to know you: Reputation and trust in a two-person economic exchange. C A Brooks King-Casas, Damon Tomlin, S R Q Colin, F Camerer, P R Montague, Science. 57183082005</p>
<p>AI ethics. M Coeckelbergh, 2020Mit Press</p>
<p>From choice architecture to choice engineering. O Dan, Y Loewenstein, Nature communications. 10128082019</p>
<p>Disentangled behavioural representations. A Dezfouli, H Ashtiani, O Ghattas, R Nock, P Dayan, C S Ong, Advances in neural information processing systems. 201932</p>
<p>Models that learn how humans learn: The case of decision-making and its disorders. A Dezfouli, K Griffiths, F Ramos, P Dayan, B W Balleine, PLoS computational biology. 156e10069032019</p>
<p>Adversarial vulnerabilities of human decision-making. A Dezfouli, R Nock, P Dayan, Proceedings of the National Academy of Sciences. 117462020</p>
<p>Can large language models serve as rational players in game theory? a systematic analysis. C Fan, J Chen, Y Jin, H He, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>T Hagendorff, arXiv:2303.13988Machine psychology: Investigating emergent capabilities and behavior in large language models using psychological methods. 2023arXiv preprint</p>
<p>Thinking fast and slow in large language models. T Hagendorff, S Fabi, M Kosinski, arXiv:2212.052062022arXiv preprint</p>
<p>Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in chatgpt. T Hagendorff, S Fabi, M Kosinski, Nature Computational Science. 3102023</p>
<p>How far are we on the decision-making of llms? evaluating llms' gaming ability in multi-agent environments. J.-T Huang, E J Li, M H Lam, T Liang, W Wang, Y Yuan, W Jiao, X Wang, Z Tu, M R Lyu, arXiv:2403.118072024arXiv preprint</p>
<p>Embracing large language models for medical applications: opportunities and challenges. M Karabacak, K Margetis, Cureus. 1552023</p>
<p>M Kosinski, arXiv-2302Evaluating large language models in theory of mind tasks. arXiv e-prints. 2023</p>
<p>Large language models and generative ai in finance: An analysis of chatgpt. D Krause, bard, and bing ai. Bard, and Bing AIJuly 15, 20232023</p>
<p>Positive reciprocity and intentions in trust games. K A Mccabe, M L Rigdon, V L Smith, Journal of Economic Behavior &amp; Organization. 5222003</p>
<p>Human-level control through deep reinforcement learning. V Mnih, K Kavukcuoglu, D Silver, A A Rusu, J Veness, M G Bellemare, A Graves, M Riedmiller, A K Fidjeland, G Ostrovski, nature. 51875402015</p>
<p>Balancing exploration and exploitation in llm using soft rllf for enhanced negation understanding. H.-T Nguyen, K Satoh, arXiv:2403.011852024arXiv preprint</p>
<p>Large language models and cognitive science: A comprehensive review of similarities, differences, and challenges. Q Niu, J Liu, Z Bi, P Feng, B Peng, K Chen, M Li, L K Yan, Y Zhang, C H Yin, arXiv:2409.023872024arXiv preprint</p>
<p>Human-like problem-solving abilities in large language models using chatgpt. Frontiers in artificial intelligence. G Orrù, A Piarulli, C Conversano, A Gemignani, 202361199350</p>
<p>Promoting interactions between cognitive science and large language models. Y Qu, P Du, W Che, C Wei, C Zhang, W Ouyang, Y Bian, F Xu, B Hu, K Du, The Innovation. 522024</p>
<p>Direct preference optimization: Your language model is secretly a reward model. R Rafailov, A Sharma, E Mitchell, C D Manning, S Ermon, C Finn, Advances in Neural Information Processing Systems. 362024</p>
<p>Machine behaviour. I Rahwan, M Cebrian, N Obradovich, J Bongard, J.-F Bonnefon, C Breazeal, J W Crandall, N A Christakis, I D Couzin, M O Jackson, Nature. 56877532019</p>
<p>Languagempc: Large language models as decision makers for autonomous driving. H Sha, Y Mu, Y Jiang, L Chen, C Xu, P Luo, S E Li, M Tomizuka, W Zhan, M Ding, arXiv:2310.030262023arXiv preprint</p>
<p>Emergent analogical reasoning in large language models. T Webb, K J Holyoak, H Lu, Nature Human Behaviour. 792023</p>
<p>Cohen. Humans use directed and random exploration to solve the exploreexploit dilemma. R C Wilson, A Geana, J M White, E A Ludvig, J , Journal of experimental psychology: General. 143620742014</p>
<p>C Xie, C Chen, F Jia, Z Ye, K Shu, A Bibi, Z Hu, P Torr, B Ghanem, G Li, arXiv:2402.04559Can large language model agents simulate human trust behaviors?. 2024arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. S Yao, D Yu, J Zhao, I Shafran, T Griffiths, Y Cao, K Narasimhan, Advances in Neural Information Processing Systems. 202436</p>            </div>
        </div>

    </div>
</body>
</html>
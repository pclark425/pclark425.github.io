<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7417 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7417</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7417</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-140.html">extraction-schema-140</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <p><strong>Paper ID:</strong> paper-264833277</p>
                <p><strong>Paper Title:</strong> <a href="https://www.aclanthology.org/2023.eval4nlp-1.17.pdf" target="_blank">Little Giants: Exploring the Potential of Small LLMs as Evaluation Metrics in Summarization in the Eval4NLP 2023 Shared Task</a></p>
                <p><strong>Paper Abstract:</strong> This paper describes and analyzes our participation in the 2023 Eval4NLP shared task, which focuses on assessing the effectiveness of prompt-based techniques to empower Large Language Models to handle the task of quality estimation, particularly in the context of evaluating machine translations and summaries. We conducted systematic experiments with various prompting techniques, including standard prompting, prompts informed by annotator instructions, and innovative chain-of-thought prompting. In addition, we integrated these approaches with zero-shot and one-shot learning methods to maximize the efficacy of our evaluation procedures. Our work reveals that combining these approaches using a “small”, open source model (orca_mini_v3_7B) yields competitive results.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7417.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7417.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>P1_zero-shot_standard</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Standard zero-shot prompt (Prompt P1)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple, manually authored natural-language prompt asking the model to rate a summary from 1 (worst) to 5 (best) (0.5 increments), used in a zero-shot setting; this straightforward prompt gave the best dev performance in the paper and competitive test performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>orca_mini_v3_7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Small open-source causal transformer LLM used by the authors for all reported experiments (no fine-tuning permitted).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Summarization quality estimation (Eval4NLP 2023 Summarization track)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Reference-free evaluation of sentence/summary-level quality across four aspects (coherence, consistency, fluency, relevance) and an overall score.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language instruction prompt (zero-shot) requesting a single overall score (range 1-5) after evaluating aspects.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Manual short instruction asking for overall score 1-5 with allowed 0.5 increments; zero-shot (no examples); no explanation requested. Implemented as Prompt P1 in the appendix.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Kendall rank correlation coefficient (against human annotations)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Dev: 0.3211 (Kendall); Test: 0.4419 (Kendall)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot; orca_mini_v3_7B; guidance library; inference on NVIDIA T4/A10G GPUs; no fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Little Giants: Exploring the Potential of Small LLMs as Evaluation Metrics in Summarization in the Eval4NLP 2023 Shared Task', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7417.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7417.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Scoring_rubric_granularity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scoring rubric / score-range granularity (varied numeric ranges and qualitative labels)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The authors systematically varied the score format (1-5 with 0.5 steps, 0-100 with coarse 10s, 0-100 with many discrete points, negative/positive coarse scale, and qualitative labels such as 'Very Poor'..'Very Good') and observed large differences in performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>orca_mini_v3_7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Small open-source causal transformer LLM used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Summarization quality estimation (Eval4NLP 2023 Summarization track)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Reference-free overall quality scoring under alternative requested score ranges or label types.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language prompt with explicit requested scoring rubric (numerical ranges with various granularities or qualitative labels).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style (scoring rubric)</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Variants tested include: 1-5 with 0.5 increments (P1), 0-100 in steps of 10 (P2), average of four 1-5 aspect scores (P3), 0-100 in 5-step increments (P4), 0-100 with 0..100 every 1 (P5), -100/ -50/0/50/100 coarse scale (P6), qualitative labels (Very Poor..Very Good) (P7), and Incomprehensible..Excellent (P8). All appended to a short instruction asking to evaluate fluency/coherence/consistency/relevance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Kendall rank correlation coefficient (dev set)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Dev results (Kendall): P1 (1-5, 0.5 steps): 0.32; P2 (0-100 coarse tens): 0.31; P3 (average of aspects): -0.03; P4 (0-100 by 5): -0.02; P5 (0-100 every 1): -0.02; P6 (-100..100 coarse): -0.03; P7 (Very Poor..Very Good qualitative): -0.02; P8 (Incomprehensible..Excellent qualitative): -0.03</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Baseline (P1): Dev 0.3211 (Kendall)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>P1 vs qualitative labels (P7): +0.3411 absolute (dev Kendall); P1 vs coarse 0-100 tens (P2): +0.01 absolute (dev Kendall)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot; orca_mini_v3_7B; same prompt stem with only the score-range/label wording changed; dev set Codalab submissions</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Little Giants: Exploring the Potential of Small LLMs as Evaluation Metrics in Summarization in the Eval4NLP 2023 Shared Task', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7417.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7417.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>One-shot_vs_zero-shot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>One-shot (few-shot) prompting vs zero-shot</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The authors compared zero-shot prompting to a one-shot (single exemplar) setting and found one-shot performed worse, up to ~0.1 Kendall points behind their best approach on development data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>orca_mini_v3_7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Small open-source causal transformer LLM used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Summarization quality estimation (Eval4NLP 2023 Summarization track)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Reference-free summary scoring; comparison between zero-shot and few-shot prompting (one-shot tested).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language prompt with inclusion of one exemplar (one-shot) vs no exemplars (zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style (few-shot count)</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>One-shot setting: one exemplar included in prompt as exemplar of scoring; note authors did not extensively increase number of examples because more examples slowed inference.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Kendall rank correlation coefficient (dev)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>One-shot: up to 0.1 lower (Kendall) than the authors' best zero-shot approach (reported qualitatively: 'as much as 0.1 behind our best approach combination')</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Zero-shot best (P1): Dev 0.3211 (Kendall)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>-0.10 absolute (Kendall) (one-shot vs best zero-shot; quoted by authors)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>One-shot with single exemplar; orca_mini_v3_7B; increased prompt length slowed inference; no finetuning</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Little Giants: Exploring the Potential of Small LLMs as Evaluation Metrics in Summarization in the Eval4NLP 2023 Shared Task', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7417.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7417.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought_prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought (CoT) prompting for evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Adding intermediate reasoning steps (CoT) to the evaluation prompt to guide stepwise assessment of aspects, which the authors report as beneficial and yielding several of the top-performing experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>orca_mini_v3_7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Small open-source causal transformer LLM used in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Summarization quality estimation (Eval4NLP 2023 Summarization track)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Reference-free evaluation where the prompt instructs the model to perform step-by-step assessments for coherence, consistency, fluency and relevance and then derive the final score.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language chain-of-thought prompt (explicit intermediate reasoning steps and example calculations), zero-shot</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style (chain-of-thought)</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>CoT prompts include stepwise instructions, example calculations, and descriptive intermediate text guiding scoring for each aspect. Marked as additions in Tables 6 and 7. CoT variants correspond to several prompt IDs and were among the authors' third, fourth, and fifth-best experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Kendall rank correlation (dev/test)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative: CoT variations produced strong performance and were among the top experiments (no single numeric CoT-only Kendall value given for all CoT variants in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Baseline standard zero-shot (P1) Dev: 0.3211 (Kendall)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Reported as beneficial relative to many other prompts (qualitative improvement; exact numeric delta vs P1 not provided)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot CoT; orca_mini_v3_7B; prompts contain multi-step reasoning and example computations; no finetuning</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Little Giants: Exploring the Potential of Small LLMs as Evaluation Metrics in Summarization in the Eval4NLP 2023 Shared Task', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7417.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7417.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Requesting_explanations_with_scores</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Joint prediction and explanation prompting (scores + explanations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompts that ask the model to produce both quality scores and textual explanations were evaluated; the authors find this joint objective yields worse results for quality estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>orca_mini_v3_7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Small open-source causal transformer LLM used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Summarization quality estimation (Eval4NLP 2023 Summarization track)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Joint task: produce numeric quality scores and natural-language explanations/justifications.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language prompt asking for both numeric score(s) and explanatory text (zero-shot variants shown in Appendix C, Tables 12 and 13).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style (output format)</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Templates asked for per-aspect scores accompanied by short explanations; examples of generated explanations are provided in Appendix C (Tables 12, 13). Authors explicitly state prompting for explanations along with scores yields poor results (see Table 12).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Kendall rank correlation (dev/test) implied, but appendix shows examples rather than aggregated metric</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported qualitatively as 'poor results' when asking for explanations plus scores; no aggregated numeric Kendall provided for explanation-augmented prompts in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Score-only prompts (e.g., P1) Dev: 0.3211 (Kendall)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Qualitative negative effect vs score-only prompts (exact numeric change not provided)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot prompts requesting explanation text plus scores; orca_mini_v3_7B; examples shown in Appendix C</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Little Giants: Exploring the Potential of Small LLMs as Evaluation Metrics in Summarization in the Eval4NLP 2023 Shared Task', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7417.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7417.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Inclusion_of_explicit_examples</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Inclusion of explicit examples for each score in zero-shot prompts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Authors added explicit example summaries mapped to each score value into zero-shot prompts to see if that improved performance; contrary to expectations, this did not yield noticeable improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>orca_mini_v3_7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Small open-source causal transformer LLM used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Summarization quality estimation (Eval4NLP 2023 Summarization track)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Zero-shot prompting with included illustrative examples that demonstrate each possible score.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language zero-shot prompt augmented by explicit exemplars (one example per score value) appended to the instruction.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style (examples in prompt)</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Explicit examples for each score on the evaluation scale were inserted in the zero-shot prompt. Authors note no noticeable improvement in model performance from including these examples.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Kendall rank correlation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>No noticeable improvement reported (qualitative statement); no specific numeric delta provided.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Zero-shot without examples (P1): Dev 0.3211 (Kendall)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>No measurable improvement reported (authors: 'did not yield a noticeable improvement')</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot with examples; orca_mini_v3_7B</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Little Giants: Exploring the Potential of Small LLMs as Evaluation Metrics in Summarization in the Eval4NLP 2023 Shared Task', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7417.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7417.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM_generated_prompts_and_hallucinations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt-generation via another LLM (automatic prompt authoring) and hallucination issues</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Authors used a separate LLM to generate or improve prompts (e.g., 'Improve the following instructions...'); initial results on training subset were promising, but generated instructions sometimes contained hallucinations (numbering errors, extraneous 'Answer:' tokens) and were manually filtered.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>orca_mini_v3_7B (prompt-generator was another LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>orca_mini_v3_7B evaluated with prompts produced or refined by a separate LLM; prompt-generator details unspecified (authors used LLM-based prompt-refinement).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B (evaluation model); prompt-generator unspecified</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Summarization quality estimation (Eval4NLP 2023 Summarization track)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Use another LLM to generate or rewrite the evaluation instructions/prompts, then evaluate summaries with orca_mini_v3_7B using those prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language prompts generated or improved by an LLM; some contain hallucinatory artefacts requiring manual removal.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style (automatic prompt generation)</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Authors used instructions such as 'Improve the following instructions' fed to another LLM to create/refine seed prompts. Generated prompts were detailed but sometimes contained hallucinations (e.g., wrong numbering sequences, stray 'Answer:' tokens). The authors manually removed hallucinated prompt outputs; some LLM-generated prompts (e.g., P10, P14) performed well in zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Dev Kendall (some LLM-generated prompts appeared in top-performing set), but hallucination handling described qualitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Some LLM-generated prompts (P10, P14) performed strongly in zero-shot on dev; exact per-prompt Kendall values for these are reported elsewhere in the paper (e.g., P14 noted as among strong zero-shot prompts), but the main quantification emphasizes manual filtering due to hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Manual standard P1 dev 0.3211 (Kendall)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>LLM-generated prompts sometimes matched or approached manual prompt performance (qualitatively), but hallucinations required manual filtering; no single aggregated numeric delta provided.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Prompt generation via another LLM with refinement phrases ('Improve the following instructions', etc.); orca_mini_v3_7B used for evaluation; manual removal of hallucinated prompts</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Little Giants: Exploring the Potential of Small LLMs as Evaluation Metrics in Summarization in the Eval4NLP 2023 Shared Task', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>G-EVAL: NLG evaluation using GPT-4 with better human alignment <em>(Rating: 2)</em></li>
                <li>InstructScore: Towards explainable text generation evaluation with automatic feedback <em>(Rating: 2)</em></li>
                <li>GEMBA metric for translation quality assessment <em>(Rating: 2)</em></li>
                <li>Simple LLM prompting is state-of-the-art for robust and multilingual dialogue evaluation <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7417",
    "paper_id": "paper-264833277",
    "extraction_schema_id": "extraction-schema-140",
    "extracted_data": [
        {
            "name_short": "P1_zero-shot_standard",
            "name_full": "Standard zero-shot prompt (Prompt P1)",
            "brief_description": "A simple, manually authored natural-language prompt asking the model to rate a summary from 1 (worst) to 5 (best) (0.5 increments), used in a zero-shot setting; this straightforward prompt gave the best dev performance in the paper and competitive test performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "orca_mini_v3_7B",
            "model_description": "Small open-source causal transformer LLM used by the authors for all reported experiments (no fine-tuning permitted).",
            "model_size": "7B",
            "task_name": "Summarization quality estimation (Eval4NLP 2023 Summarization track)",
            "task_description": "Reference-free evaluation of sentence/summary-level quality across four aspects (coherence, consistency, fluency, relevance) and an overall score.",
            "problem_format": "Natural-language instruction prompt (zero-shot) requesting a single overall score (range 1-5) after evaluating aspects.",
            "format_category": "prompt style",
            "format_details": "Manual short instruction asking for overall score 1-5 with allowed 0.5 increments; zero-shot (no examples); no explanation requested. Implemented as Prompt P1 in the appendix.",
            "performance_metric": "Kendall rank correlation coefficient (against human annotations)",
            "performance_value": "Dev: 0.3211 (Kendall); Test: 0.4419 (Kendall)",
            "baseline_performance": null,
            "performance_change": null,
            "experimental_setting": "Zero-shot; orca_mini_v3_7B; guidance library; inference on NVIDIA T4/A10G GPUs; no fine-tuning",
            "statistical_significance": null,
            "uuid": "e7417.0",
            "source_info": {
                "paper_title": "Little Giants: Exploring the Potential of Small LLMs as Evaluation Metrics in Summarization in the Eval4NLP 2023 Shared Task",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Scoring_rubric_granularity",
            "name_full": "Scoring rubric / score-range granularity (varied numeric ranges and qualitative labels)",
            "brief_description": "The authors systematically varied the score format (1-5 with 0.5 steps, 0-100 with coarse 10s, 0-100 with many discrete points, negative/positive coarse scale, and qualitative labels such as 'Very Poor'..'Very Good') and observed large differences in performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "orca_mini_v3_7B",
            "model_description": "Small open-source causal transformer LLM used in experiments.",
            "model_size": "7B",
            "task_name": "Summarization quality estimation (Eval4NLP 2023 Summarization track)",
            "task_description": "Reference-free overall quality scoring under alternative requested score ranges or label types.",
            "problem_format": "Natural-language prompt with explicit requested scoring rubric (numerical ranges with various granularities or qualitative labels).",
            "format_category": "prompt style (scoring rubric)",
            "format_details": "Variants tested include: 1-5 with 0.5 increments (P1), 0-100 in steps of 10 (P2), average of four 1-5 aspect scores (P3), 0-100 in 5-step increments (P4), 0-100 with 0..100 every 1 (P5), -100/ -50/0/50/100 coarse scale (P6), qualitative labels (Very Poor..Very Good) (P7), and Incomprehensible..Excellent (P8). All appended to a short instruction asking to evaluate fluency/coherence/consistency/relevance.",
            "performance_metric": "Kendall rank correlation coefficient (dev set)",
            "performance_value": "Dev results (Kendall): P1 (1-5, 0.5 steps): 0.32; P2 (0-100 coarse tens): 0.31; P3 (average of aspects): -0.03; P4 (0-100 by 5): -0.02; P5 (0-100 every 1): -0.02; P6 (-100..100 coarse): -0.03; P7 (Very Poor..Very Good qualitative): -0.02; P8 (Incomprehensible..Excellent qualitative): -0.03",
            "baseline_performance": "Baseline (P1): Dev 0.3211 (Kendall)",
            "performance_change": "P1 vs qualitative labels (P7): +0.3411 absolute (dev Kendall); P1 vs coarse 0-100 tens (P2): +0.01 absolute (dev Kendall)",
            "experimental_setting": "Zero-shot; orca_mini_v3_7B; same prompt stem with only the score-range/label wording changed; dev set Codalab submissions",
            "statistical_significance": null,
            "uuid": "e7417.1",
            "source_info": {
                "paper_title": "Little Giants: Exploring the Potential of Small LLMs as Evaluation Metrics in Summarization in the Eval4NLP 2023 Shared Task",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "One-shot_vs_zero-shot",
            "name_full": "One-shot (few-shot) prompting vs zero-shot",
            "brief_description": "The authors compared zero-shot prompting to a one-shot (single exemplar) setting and found one-shot performed worse, up to ~0.1 Kendall points behind their best approach on development data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "orca_mini_v3_7B",
            "model_description": "Small open-source causal transformer LLM used in experiments.",
            "model_size": "7B",
            "task_name": "Summarization quality estimation (Eval4NLP 2023 Summarization track)",
            "task_description": "Reference-free summary scoring; comparison between zero-shot and few-shot prompting (one-shot tested).",
            "problem_format": "Natural-language prompt with inclusion of one exemplar (one-shot) vs no exemplars (zero-shot)",
            "format_category": "prompt style (few-shot count)",
            "format_details": "One-shot setting: one exemplar included in prompt as exemplar of scoring; note authors did not extensively increase number of examples because more examples slowed inference.",
            "performance_metric": "Kendall rank correlation coefficient (dev)",
            "performance_value": "One-shot: up to 0.1 lower (Kendall) than the authors' best zero-shot approach (reported qualitatively: 'as much as 0.1 behind our best approach combination')",
            "baseline_performance": "Zero-shot best (P1): Dev 0.3211 (Kendall)",
            "performance_change": "-0.10 absolute (Kendall) (one-shot vs best zero-shot; quoted by authors)",
            "experimental_setting": "One-shot with single exemplar; orca_mini_v3_7B; increased prompt length slowed inference; no finetuning",
            "statistical_significance": null,
            "uuid": "e7417.2",
            "source_info": {
                "paper_title": "Little Giants: Exploring the Potential of Small LLMs as Evaluation Metrics in Summarization in the Eval4NLP 2023 Shared Task",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Chain-of-Thought_prompting",
            "name_full": "Chain-of-Thought (CoT) prompting for evaluation",
            "brief_description": "Adding intermediate reasoning steps (CoT) to the evaluation prompt to guide stepwise assessment of aspects, which the authors report as beneficial and yielding several of the top-performing experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "orca_mini_v3_7B",
            "model_description": "Small open-source causal transformer LLM used in the experiments.",
            "model_size": "7B",
            "task_name": "Summarization quality estimation (Eval4NLP 2023 Summarization track)",
            "task_description": "Reference-free evaluation where the prompt instructs the model to perform step-by-step assessments for coherence, consistency, fluency and relevance and then derive the final score.",
            "problem_format": "Natural-language chain-of-thought prompt (explicit intermediate reasoning steps and example calculations), zero-shot",
            "format_category": "prompt style (chain-of-thought)",
            "format_details": "CoT prompts include stepwise instructions, example calculations, and descriptive intermediate text guiding scoring for each aspect. Marked as additions in Tables 6 and 7. CoT variants correspond to several prompt IDs and were among the authors' third, fourth, and fifth-best experiments.",
            "performance_metric": "Kendall rank correlation (dev/test)",
            "performance_value": "Qualitative: CoT variations produced strong performance and were among the top experiments (no single numeric CoT-only Kendall value given for all CoT variants in main text).",
            "baseline_performance": "Baseline standard zero-shot (P1) Dev: 0.3211 (Kendall)",
            "performance_change": "Reported as beneficial relative to many other prompts (qualitative improvement; exact numeric delta vs P1 not provided)",
            "experimental_setting": "Zero-shot CoT; orca_mini_v3_7B; prompts contain multi-step reasoning and example computations; no finetuning",
            "statistical_significance": null,
            "uuid": "e7417.3",
            "source_info": {
                "paper_title": "Little Giants: Exploring the Potential of Small LLMs as Evaluation Metrics in Summarization in the Eval4NLP 2023 Shared Task",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Requesting_explanations_with_scores",
            "name_full": "Joint prediction and explanation prompting (scores + explanations)",
            "brief_description": "Prompts that ask the model to produce both quality scores and textual explanations were evaluated; the authors find this joint objective yields worse results for quality estimation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "orca_mini_v3_7B",
            "model_description": "Small open-source causal transformer LLM used in experiments.",
            "model_size": "7B",
            "task_name": "Summarization quality estimation (Eval4NLP 2023 Summarization track)",
            "task_description": "Joint task: produce numeric quality scores and natural-language explanations/justifications.",
            "problem_format": "Natural-language prompt asking for both numeric score(s) and explanatory text (zero-shot variants shown in Appendix C, Tables 12 and 13).",
            "format_category": "prompt style (output format)",
            "format_details": "Templates asked for per-aspect scores accompanied by short explanations; examples of generated explanations are provided in Appendix C (Tables 12, 13). Authors explicitly state prompting for explanations along with scores yields poor results (see Table 12).",
            "performance_metric": "Kendall rank correlation (dev/test) implied, but appendix shows examples rather than aggregated metric",
            "performance_value": "Reported qualitatively as 'poor results' when asking for explanations plus scores; no aggregated numeric Kendall provided for explanation-augmented prompts in main text.",
            "baseline_performance": "Score-only prompts (e.g., P1) Dev: 0.3211 (Kendall)",
            "performance_change": "Qualitative negative effect vs score-only prompts (exact numeric change not provided)",
            "experimental_setting": "Zero-shot prompts requesting explanation text plus scores; orca_mini_v3_7B; examples shown in Appendix C",
            "statistical_significance": null,
            "uuid": "e7417.4",
            "source_info": {
                "paper_title": "Little Giants: Exploring the Potential of Small LLMs as Evaluation Metrics in Summarization in the Eval4NLP 2023 Shared Task",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Inclusion_of_explicit_examples",
            "name_full": "Inclusion of explicit examples for each score in zero-shot prompts",
            "brief_description": "Authors added explicit example summaries mapped to each score value into zero-shot prompts to see if that improved performance; contrary to expectations, this did not yield noticeable improvement.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "orca_mini_v3_7B",
            "model_description": "Small open-source causal transformer LLM used in experiments.",
            "model_size": "7B",
            "task_name": "Summarization quality estimation (Eval4NLP 2023 Summarization track)",
            "task_description": "Zero-shot prompting with included illustrative examples that demonstrate each possible score.",
            "problem_format": "Natural-language zero-shot prompt augmented by explicit exemplars (one example per score value) appended to the instruction.",
            "format_category": "prompt style (examples in prompt)",
            "format_details": "Explicit examples for each score on the evaluation scale were inserted in the zero-shot prompt. Authors note no noticeable improvement in model performance from including these examples.",
            "performance_metric": "Kendall rank correlation",
            "performance_value": "No noticeable improvement reported (qualitative statement); no specific numeric delta provided.",
            "baseline_performance": "Zero-shot without examples (P1): Dev 0.3211 (Kendall)",
            "performance_change": "No measurable improvement reported (authors: 'did not yield a noticeable improvement')",
            "experimental_setting": "Zero-shot with examples; orca_mini_v3_7B",
            "statistical_significance": null,
            "uuid": "e7417.5",
            "source_info": {
                "paper_title": "Little Giants: Exploring the Potential of Small LLMs as Evaluation Metrics in Summarization in the Eval4NLP 2023 Shared Task",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "LLM_generated_prompts_and_hallucinations",
            "name_full": "Prompt-generation via another LLM (automatic prompt authoring) and hallucination issues",
            "brief_description": "Authors used a separate LLM to generate or improve prompts (e.g., 'Improve the following instructions...'); initial results on training subset were promising, but generated instructions sometimes contained hallucinations (numbering errors, extraneous 'Answer:' tokens) and were manually filtered.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "orca_mini_v3_7B (prompt-generator was another LLM)",
            "model_description": "orca_mini_v3_7B evaluated with prompts produced or refined by a separate LLM; prompt-generator details unspecified (authors used LLM-based prompt-refinement).",
            "model_size": "7B (evaluation model); prompt-generator unspecified",
            "task_name": "Summarization quality estimation (Eval4NLP 2023 Summarization track)",
            "task_description": "Use another LLM to generate or rewrite the evaluation instructions/prompts, then evaluate summaries with orca_mini_v3_7B using those prompts.",
            "problem_format": "Natural-language prompts generated or improved by an LLM; some contain hallucinatory artefacts requiring manual removal.",
            "format_category": "prompt style (automatic prompt generation)",
            "format_details": "Authors used instructions such as 'Improve the following instructions' fed to another LLM to create/refine seed prompts. Generated prompts were detailed but sometimes contained hallucinations (e.g., wrong numbering sequences, stray 'Answer:' tokens). The authors manually removed hallucinated prompt outputs; some LLM-generated prompts (e.g., P10, P14) performed well in zero-shot.",
            "performance_metric": "Dev Kendall (some LLM-generated prompts appeared in top-performing set), but hallucination handling described qualitatively.",
            "performance_value": "Some LLM-generated prompts (P10, P14) performed strongly in zero-shot on dev; exact per-prompt Kendall values for these are reported elsewhere in the paper (e.g., P14 noted as among strong zero-shot prompts), but the main quantification emphasizes manual filtering due to hallucinations.",
            "baseline_performance": "Manual standard P1 dev 0.3211 (Kendall)",
            "performance_change": "LLM-generated prompts sometimes matched or approached manual prompt performance (qualitatively), but hallucinations required manual filtering; no single aggregated numeric delta provided.",
            "experimental_setting": "Prompt generation via another LLM with refinement phrases ('Improve the following instructions', etc.); orca_mini_v3_7B used for evaluation; manual removal of hallucinated prompts",
            "statistical_significance": null,
            "uuid": "e7417.6",
            "source_info": {
                "paper_title": "Little Giants: Exploring the Potential of Small LLMs as Evaluation Metrics in Summarization in the Eval4NLP 2023 Shared Task",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "G-EVAL: NLG evaluation using GPT-4 with better human alignment",
            "rating": 2,
            "sanitized_title": "geval_nlg_evaluation_using_gpt4_with_better_human_alignment"
        },
        {
            "paper_title": "InstructScore: Towards explainable text generation evaluation with automatic feedback",
            "rating": 2,
            "sanitized_title": "instructscore_towards_explainable_text_generation_evaluation_with_automatic_feedback"
        },
        {
            "paper_title": "GEMBA metric for translation quality assessment",
            "rating": 2,
            "sanitized_title": "gemba_metric_for_translation_quality_assessment"
        },
        {
            "paper_title": "Simple LLM prompting is state-of-the-art for robust and multilingual dialogue evaluation",
            "rating": 2,
            "sanitized_title": "simple_llm_prompting_is_stateoftheart_for_robust_and_multilingual_dialogue_evaluation"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        }
    ],
    "cost": 0.01430475,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Little Giants: Exploring the Potential of Small LLMs as Evaluation Metrics in Summarization in the Eval4NLP 2023 Shared Task</p>
<p>Neema Kotonya neema.kotonya@dataminr.com 
Dataminr Inc</p>
<p>Saran Krishnasamy skrishnasamy@dataminr.com 
Dataminr Inc</p>
<p>Joel Tetreault jtetreault@dataminr.com 
Dataminr Inc</p>
<p>Alejandro Jaimes ajaimes@dataminr.com 
Dataminr Inc</p>
<p>Little Giants: Exploring the Potential of Small LLMs as Evaluation Metrics in Summarization in the Eval4NLP 2023 Shared Task
31EF9462905672CF3A05FC21085C949E
This paper describes and analyzes our participation in the 2023 Eval4NLP shared task, which focuses on assessing the effectiveness of prompt-based techniques to empower Large Language Models to handle the task of quality estimation, particularly in the context of evaluating machine translations and summaries.We conducted systematic experiments with various prompting techniques, including standard prompting, prompts informed by annotator instructions, and innovative chain-of-thought prompting.In addition, we integrated these approaches with zero-shot and one-shot learning methods to maximize the efficacy of our evaluation procedures.Our work reveals that combining these approaches using a "small", open source model (orca_mini_v3_7B) yields competitive results.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) have revolutionized the field of Natural Language Processing (NLP) by demonstrating remarkable proficiency in a multitude of generative tasks (Brown et al., 2020).Beyond their capabilities in text generation, LLMs offer the potential to automate the evaluation of generated text, particularly in domains such as machine translation and summarization.</p>
<p>Previous research efforts have explored LLMbased evaluation metrics, yielding promising results.Notable examples include the development of metrics like the GEMBA metric for translation quality assessment (Kocmi and Federmann, 2023), work on the effectiveness of LLMs as an alternative to human evaluation for NLP tasks by Chiang and Lee (2023), and the INSTRUCTSCORE metric for summarization evaluation (Xu et al., 2023).However, a significant gap exists in the systematic evaluation and exploration of prompting techniques available for metric usage with LLMs.In fact, there is scant work in this area to date.Excep-tions include the work of Mendonça et al. (2023) for dialogue evaluation, Yang et al. (2023) and GEMBA for MT evaluation, G-EVAL, a chain-ofthought based framework for the evaluation of generated texts that leverages GPT-4 (Liu et al., 2023), and GPTSCORE for text generation evaluation (Fu et al., 2023).</p>
<p>This paper presents our contribution to addressing this gap through our participation in the Prompting Large Language Models as Explainable Metrics shared task (Leiter et al., 2023), which was conducted as part of The 4th Workshop on Evaluation &amp; Comparison of NLP Systems, hosted at AACL 2023.We delve into various prompting approaches and techniques, offer a comprehensive overview of the results we have obtained, and provide a thorough analysis of our findings (our team name is COMPETITIONENTRANTS).</p>
<p>We exclusively participated in the Small Models Track, focusing on models with parameters less than or equal to 25 billion, for the Summarization task.Consequently, all our experiments and reported results are derived from the orca_mini_v3_7B model.Among our various approaches, the best-performing one, employing a standard prompt in a zero-shot setting, achieved a score of 0.44 during the test phase of the shared task.While this performance is notable, it places us marginally behind the leaderboard's highest score of 0.50 by a margin of 0.06.We also find that Chain-of-Thought (CoT) prompting (Wei et al., 2022) also aids in explicating the evaluation task to the model.This technique holds the potential to enhance the interpretability and explainability of quality estimation models.</p>
<p>Task Description</p>
<p>The primary objective of the shared task is to investigate prompt-based methodologies for LLMs in the development of automated quality metrics in a reference-free setup tailored to natural language generation tasks, specifically summarization and machine translation.These quality scores are constructed using fine-grained scoring or error labels.The underlying rationale behind reference-free evaluation metrics is to provide assessment scores that are comparable to those of reference-based metrics while reducing reliance on often noisy and costly reference labels.A typical reference-free metric operates by taking a source (SRC) and a hypothesis (HYP) as inputs and subsequently generates a metric score, thereby providing an alternative to traditional reference-based evaluation methodologies.</p>
<p>Task Setup</p>
<p>The shared task is structured into two distinct subtasks, each contributing to the overarching goal:</p>
<ol>
<li>
<p>Prompting Strategies for Evaluation of Summarization and Translation: This entails designing prompts and instructions that enable the assessment of the quality of generated content.The outcome of this sub-task serves as a critical component in the reference-free evaluation process.</p>
</li>
<li>
<p>Score Aggregation: The second sub-task focuses on the creation of a score aggregator mechanism.The primary objective here is to devise a method that computes an overall quality estimation score based on the outputs generated in Task 1.This aggregator consolidates individual quality assessments into a comprehensive quality estimation, ultimately providing a holistic evaluation of translation and summarization performance.</p>
</li>
</ol>
<p>Datasets</p>
<p>Data is provided for the tasks of summarization and machine translation:</p>
<p>• Summarization: The training and development data for this track is derived from the datasets detailed in SummEval (Fabbri et al., 2020) with the scores being the average of human annotations across the four aspects -coherence, consistency, fluency, and relevance.</p>
<p>• Machine Translation: The training and development datasets are derived from the Multidimensional Quality Metrics (MQM) annotations of the WMT22 metrics shared task (Freitag et al., 2022) for machine translation.</p>
<p>For the test data, we are provided with a new reference-free dataset with sentence/summarylevel quality scores for summarization and MT.</p>
<p>As part of the test phase, 2 new language pair datasets, English-Chinese and English-Spanish are introduced for the machine translation track.</p>
<p>Large language models</p>
<p>The following six Huggingface LLMs were permitted for use in the shared task, two larger models (65B and 70B parameter models), which we denote with the following symbol •, and four smaller models, denoted by ⋆, each of which has 13B parameters or fewer: 1. Guanaco-65B-GPTQ •1 2. WizardLM-13B-V1.1-GPTQ⋆2 3. Nous-Hermes-13b ⋆3 4. Platypus2-70B-Instruct-GPTQ •4 5. OpenOrca-Platypus2-13B ⋆5 6. Orca_mini_v3_7b ⋆6 Despite having access to these LLMs, our work faced computational constraints that influenced our choice of models for experimentation.As a result, we focused primarily on experimenting with two small LLMs: orca_mini_v3_7B model and Nous-Hermes-13b.During the submission phase to the shared task's leaderboard, the final test results we presented were exclusively derived from the orca_mini_v3_7B model.The shared task guidelines explicitly forbade model fine-tuning.</p>
<p>Evaluation</p>
<p>For the evaluation process, we used Codalab as the platform for submitting our system entries.Notably, the organizers of the evaluation, as detailed by Kocmi and Federmann (2023), provide direct assessment baselines for these LLMs.These baselines serve as reference points for evaluating the performance of our system and other participants in the shared task.</p>
<p>To quantify the performance of our system and the competing teams, the shared task organizers utilized the Kendall rank coefficient as the evaluation metric.The Kendall rank coefficient stands as an alternative to more traditional correlation metrics like Pearson's r and Spearman's ρ correlations.It finds particular utility in situations where the data fails to meet specific assumptions or when dealing with relatively small sample sizes.</p>
<p>Approaches</p>
<p>Three main classes of strategies are employed to enhance prompt effectiveness and interpretability for evaluating generated summaries.The first strategy, Core Prompts, encompasses three one-step methods for generating prompts.The first two borrow from existing literature, and the final uses an LLM to simply generate a prompt from scratch.Next, in Section 3.2, we introduce three methods (one manual, two automatic) to take prompts in Section 3.1 and further refine them.This is akin to paraphrasing in bulk.Finally, in Section 3.3, we outline two simple approaches for further refining prompts generated earlier.In total, the three classes span 8 different approaches, and approaches can be combined across the classes.</p>
<p>Core Prompts</p>
<p>(1) Standard Prompting: Our initial approach was to formulate prompts (a total of 9 prompts) that assess summary quality across the four dimensions outlined in Kryscinski et al. (2019): fluency, coherence, consistency, and relevance.These prompts task the model with generating quality scores for summaries, for different score ranges.In Table 2, we provide an example of standard prompting.In this example, the prompt specifies that the summarization should be rated from 1 and 5, with increments of 0.5 permitted.</p>
<p>Given the following summary for a news article, evaluate this summary for its fluency, coherence, consistency and relevance.Provide an overall score for the quality of this summary in the range 1 (worst) to 5 (best).Possible scores are 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5 and 5. Table 2: An example of a standard prompt for summarization quality evaluation that stipulates scores should be in the range 1 to 5, and intermediate scores should be in 0.5 intervals.</p>
<p>(2) Annotator Instructions as Seed Prompts: To facilitate summarization evaluation, we then employed the instructions provided to expert annotators in Fabbri et al. (2020) (See Table 3).These served as foundational "seed prompts" for subsequent prompt refinement.We conducted an assessment of this seed prompt by utilizing a subset of examples from the Eval4NLP training dataset, noting that these instructions exhibit relatively favorable performance on the training data.</p>
<p>[. ..]In this task you will evaluate the quality of summaries written for a news article To correctly solve this task, follow these steps: 1. Carefully read the news articles, be aware of the information it contains.2. Read the proposed summary.3. Rate each summary on a scale from 1 (Worst) to 5 (Best) by its relevance, consistency, fluency, and coherence.Relevance: "The rating measures how well the summary captures the key points of the article.Consider whether all and only the important aspects are contained in the summary."[. ..]Write a set of instructions to evaluate the quality of the summary of a news article according to its coherence, consistency, fluency, and relevance for each sentence in the summary with respect to the news article.Each aspect (coherence, consistency, fluency, and relevance) should be scores from 1 to 5. 1 is the worst possible score, 5 is the best possible score.Instructions:</p>
<p>Prompt Refinement</p>
<p>To further enhance the prompts' quality and effectiveness from Section 3.1, we employ three key strategies, one manual and two automatic:</p>
<p>(4) Manual Prompt Rewriting: This method involves meticulous manual rewriting of the instructions (done by the authors).We created prompt variations to elicit fine-grained answers, seek explanations for the provided answers, and employ templates specifying the desired answer format.These steps ensure that the instructions are refined to enhance the clarity and comprehensibility of the prompts.We also experiment with prompts that instruct the LLM to output both scores and explanations, similar to other works that explore both prediction (which in our case is quality estimation) and explanation jointly (Camburu et al., 2018;Kotonya and Toni, 2020;Wei et al., 2022).However, we find that prompting for explanations in addition to quality estimation yields poor results (see Table 12 in Appendix C).</p>
<p>(5) Instruction Enhancement via LLMs: In this strategy, we provide the seed prompt as context and prompt a separate LLM to enhance the existing instructions.We utilize various phrases such as "Improve the following instructions", "Rewrite the following instructions to yield better responses", "Write a more precise set of instructions", and "Rewrite the instructions below in order to yield the best results " (see Table 5).</p>
<h3>System: You are an AI assistant that follows instruction extremely well.Help as much as you can.### User: Improve the following instructions: "In this task you will evaluate the quality of summaries written for a news article To correctly solve this task, follow these steps: 1. Carefully read the news article, be aware of the information it contains.2. Read the proposed summary.</h3>
<ol>
<li>Rate each summary on a scale from 1 (Worst) to 5 (Best) by its relevance, consistency, fluency, and coherence.. .."New instructions: (6) Chain-of-Thought (CoT) Prompting: We also harness the CoT prompting technique, which guides the model's evaluation process through a sequence of intermediate reasoning steps leading to the determination of the quality score for the provided summary.The main advantages of CoT prompts are that their specificity should reduce the number of inconsistencies in the generated response, yield responses that correlate strongly with human judgments and also allow for more complex reasoning.The CoT additions are marked in blue in Tables 6 and 7.</li>
</ol>
<p>Exploration of Inference Settings</p>
<p>Given the constraint of not permitting model finetuning, we explore various inference settings to optimize model performance:</p>
<p>(7) Zero-shot Approach: In this setting, the model is evaluated and prompted to generate responses without any prior training specific to the evaluation task.</p>
<p>(8) Few-shot and One-shot Approaches: These approaches involve leveraging a limited amount of training data to serve as exemplars to direct the model's responses.While we experimented with a one-shot setting, it is important to note that increasing the number of examples in the prompt had the unintended consequence of slowing down inference.</p>
<ol>
<li>Coherence: Assess how well the summary conveys a clear and logical message.2. Consistency: Check if the summary accurately represents the main points of the news article.3. Fluency: Evaluate the smoothness and readability of the summary.4. Relevance: Determine if the summary is relevant to the news article's topic.For each sentence in the summary, assign a score from 1 to 5 for each aspect (coherence, consistency, fluency, and relevance).Example: Sentence 1: "The company announced a new product line."Coherence: 4 Consistency: 3 Fluency: 3 Relevance: 4 Total Score: (Coherence + Consistency + Fluency + Relevance) / 4 Total Score: (4 + 3 + 3 + 4)/4 = 14/4 = 3.5 So, the summary has an overall score of 3.5 out of 5.</li>
</ol>
<p>Table 6: Example of a prompt generated for summarization quality estimation.These instructions demonstrate step-by-step, with the aid of an example, how the final score should be calculated.</p>
<p>Example:</p>
<ol>
<li>Read the news article: "A new study found that regular exercise can significantly improve mental health."2. Read the summary: "A study discovered that exercise has a significant impact on mental health."3. Evaluate the summary based on the aspects: a. Coherence: 5 (The summary maintains a clear and logical flow of ideas.)b.Consistency: 5 (The main points of the news article are accurately represented.)c.Fluency: 5 (The summary is written in a smooth and easy-to-understand manner.)d.Relevance: 5 (The summary conveys the essential information from the news article.) 4. Assign scores for each aspect: [. ..]Table 7: Example of a chain-of-thought prompt generated for summarization quality estimation.In this chain of thought prompt, descriptions are generated for each of the four aspects (coherence, consistency, fluency, and relevance).</li>
</ol>
<p>Results</p>
<p>Table 8 shows results for a battery of approaches.One should note that we tried many combinations of the approaches with different seed prompts, and the number of experiments is quite large.For the sake of simplicity, we report on key combinations that we uncovered.Prompt ID refers to the specific prompt that was used and the exact text can be found in the Appendix.</p>
<p>Among the approaches, Prompt P1, which employs a standard manual prompt in a zero-shot setting with a grading scale ranging from 1 to 5, emerges as the top performer, achieving a notable score of 0.3211 on the development dataset.This was surprising as this is essentially one of the most straightforward approaches to try.However, it is perhaps unsurprising as the Direct Assessment baseline provided by the Shared Task is also a simple manual prompt in a zero-shot setting (Kocmi and Federmann, 2023).</p>
<p>Following closely behind, we find approaches that leverage annotator instructions as seed prompts (P12) and prompts generated using LLMs (P14 and P10), all in zero-shot settings, also stand out.These prompts, in contrast to standard ones, contain a wealth of details about the evaluation metrics, offering intricate guidance to the model.However, our exploration of a one-shot setting indicates that this approach does not yield as promising results (as much as 0.1 behind our best approach combination).Further experimentation with the choice of examples provided to the model may be warranted to enhance its performance.</p>
<p>Notably, the incorporation of chain-of-thought prompting appears to be a beneficial strategy, as evidenced by the strong performance observed (our third, fourth, and fifth-best experiments).</p>
<p>On the lower end of the result scores, we find standard prompts (P5, P4, P7, P3, P8) that utilize grading scales with exceptionally high precision or qualitative labels as quality measures.These prompts, while designed with attention to detail, exhibit comparatively lower scores in the evaluation, suggesting the importance of striking a balance between precision and other factors when designing prompts for quality estimation tasks.Prompt P6 achieves the highest Kendall correlation coefficient on the test set with a score of 0.4423.Furthermore, P1 and P2 also achieve competitive scores of 0.4419 and 0.4422 respectively.</p>
<p>Discussion</p>
<p>The evaluation results we have presented furnish compelling evidence regarding the proficiency of LLMs in the domain of quality estimation for summarization.Our findings underscore the capacity of these models to provide valuable insights into the quality of generated outputs, thereby contributing to the advancement of evaluation methodologies within the field of NLP.</p>
<p>Insights</p>
<p>Scoring Rubric Matters: Our experimentation with scoring rubrics revealed an intriguing trend.While assessing the precision and range of scores requested from the model, we observed that quantitative scores with lower precision exhibited favorable performance.Surprisingly, the use of quali-tative labels such as "Very Poor", "Poor", "Average", "Good", and "Very Good" to describe quality yielded comparatively less favorable results as can be seen from the dev scores in Table 9.This suggests that when instructing LLMs for quality estimation, a preference for quantitative, less granular scoring may be more effective.</p>
<p>Effect on Performance through Examples:</p>
<p>To enhance performance, we incorporated explicit examples into zero-shot prompts for each score on the evaluation scale.Contrary to our expectations, the inclusion of examples did not yield a noticeable improvement in model performance.This observation highlights the nuanced nature of prompt design and underscores the need for tailored approaches that align with the unique characteristics of the task.</p>
<p>Simpler Prompts Suffice: A notable finding emerged from our exploration of prompt complexity.While we originally hypothesized that detailed prompts derived from SummEval annotation guidelines would outperform simpler prompts based on the same four quality dimensions (fluency, coherence, consistency, relevance), our results did not substantiate this hypothesis.However, it is worth noting that this approach shows promise, particularly when the aspect being evaluated is ambiguous to the model.The provision of detailed prompts with examples and context holds the potential to im-Prompt* Dev score Provide an overall score for the quality of this summary in the range 1 (worst) to 5 (best).Possible scores are 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5 and 5. 0.32 Provide an overall score for the quality of this summary in the range 0 (worst) to 100 (best).Possible scores are 0, 10, 20, 30, 40, 50, 60, 70, 80, 90 and 100. 0.31 Provide an overall score for the quality of this summary in the range 1 (worst) to 5 (best) that is an average of the scores (also from 1 to 5) for fluency, coherence, consistency and relevance.</p>
<p>-0.03 Provide an overall score for the quality of this summary in the range 0 (worst) to 100 (best).Possible scores are 0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95 and 100. -0.02 Provide an overall score for the quality of this summary in the range Very Poor (worst) to Very Good (best).Possible scores are Very Poor, Poor, Average, Good, Very Good.</p>
<p>-0.02 Provide an overall score for the quality of this summary in the range Incomprehensible (worst) to Excellent (best).Possible scores are Incomprehensible, Poor, Average, Good, Excellent.</p>
<p>-0.03 Table 9: Standard prompting with different score ranges*The prompts in the table are appended to Given the following summary for a news article, evaluate this summary for its fluency, coherence, consistency and relevance.along with the input to form the full instruction to the model.prove performance, suggesting a fruitful direction for future research.</p>
<p>Hallucinations during prompt refinement</p>
<p>We conducted an experiment where we leveraged a separate LLM to generate instructions for assessing the quality of summarizations.This approach yielded promising results initially, as the generated instructions were detailed and exhibited potential when tested with a subset of examples from the training dataset.However, during our extensive evaluation process, we encountered instances of hallucinations within the generated instructions, prompting us to incorporate further refinement steps into our methodology.Hallucinations in this context refer to the phenomenon where the model produces content in the instruction that deviates from the original intent or context, thereby introducing inaccuracies or inconsistencies (Maynez et al., 2020).</p>
<p>One recurring form of hallucination pertained to the numbering within the instructions.In particular, we observed instances where the numbering in the latter part of the instruction was hallucinated, leading to inconsistencies.For instance, as shown in Table 10, the numbering sequence (2,3,4,4) in the model's response was incongruent with the original (1, 2, 3, 4) prompt.Additionally, we identified another hallucination wherein the phrase "Answer:" was erroneously introduced in the model's output.This unintended addition in the response further underscores the need for vigilance when using LLMs to generate instructions.For our experiments, we manually removed LLM-generate prompts with hallucinations.</p>
<p>Conclusion</p>
<p>In this paper, we have presented our system entry for the Eval4NLP 2023 Shared Task on Prompting Large Language Models as Explainable Metrics.Our approach revolves around assessing the capabilities of the small orca_mini_v3_7B model in the reference-free evaluation of generated summaries.Through a series of extensive experiments, we have explored the model's adaptability and responsiveness to various prompting techniques, including zero-shot, one-shot, and chain-of-thought prompts.These experiments were conducted using meticulously designed prompts that emphasize key evaluation criteria.Our findings suggest that the use of a standard prompting (when using a small model) is performant while the chain-of-thought prompt-based approach holds significant promise for evaluating the quality of generated summaries.However, we do note that slight variations in the underlying prompt text could swing performances rather widely, so more research is definitely required to understand this phenomenon.</p>
<p>C Explanations</p>
<p>In Tables 12 and 13 present explanations generated for our experiments on the dev set.The prompt employed to generate the results in Tables 12 and 13 is shown in Table 14.</p>
<h3>Answer: 2 (Worst) Relevance: 2 Explanation: The summary contains some relevant information, but it is not comprehensive enough.It only mentions the guard slipping and falling, but it does not mention the incident being caught on camera or the reason for the slip.Consistency: 2 Explanation: The summary is consistent with the original article, but it does not reproduce all facts accurately.It does not mention the metal protectors nailed to the soles of the guard's boots, which is an important detail.Fluency: 2 Explanation: The summary is grammatically correct, but it is not well-written.It is difficult to understand and lacks proper sentence structure.Coherence: 2 Explanation: The summary is coherent, but it is not well-structured.It is difficult to follow and does not flow naturally.</h3>
<p>Table 12: Explanation and scores produced for a low-scoring summary.The explanations in the generated response are shown in red.Note that an overall assessment, 2 (Worst), is also generated for this example.</p>
<h3>Answer:</h3>
<p>Relevance: 4 Explanation: The summary captures the key points of the article, but it could be more detailed and specific.Consistency: 4 Explanation: The summary accurately reproduces the main facts from the article, but it could be more detailed.Fluency: 3 Explanation: The summary is grammatically correct, but it could be more concise and fluent.Coherence: 3 Explanation: The summary is coherent, but it could be more natural and flowing.</p>
<p>Table 13: Explanation and scores produced for a high-scoring summary from the dev set.The explanations in the generated response are shown in red.</p>
<p>Table 1
1shows the counts of the train, development, andtest datasets.tasktrain devtestSummarization3201,280825en-de 11,046 7,3641,425Translationzh-en 15,750 10,500 -en-es --1,834en-zh --1,297Table 1: Train, Dev, and Test dataset sizes for summa-rization and machine translation tasks. Entries with -indicate that the dataset wasn't provided as part of this task.</p>
<p>Table 3 :
3
Instructions provided to expert annotators in Fabbri et al. (2020).</p>
<p>Table 4 :
4
Instructions for prompting LLMs to generate prompts for summarization quality evaluation.</p>
<p>Table 5 :
5
Example of a prompt supplied for LLM-based prompt refinement, where the instruction used is "Improve the following instructions."</p>
<p>Table 8 :
8
Results of prompts for evaluating summarization.Score is the dev score obtained from the Codalab submission.The Prompt ID map to the full Prompts in the Appendix.The details for each of the Approaches can be found in Section 3.
Prompt IDApproach
https://huggingface.co/TheBloke/guanaco-65B-GPTQ
https://huggingface.co/TheBloke/WizardLM-13B-V1.1-GPTQ
3 https://huggingface.co/NousResearch/Nous-Hermes-
13b 4 https://huggingface.co/TheBloke/Platypus2-70B-Instruct
-GPTQ 5 https://huggingface.co/Open-Orca/OpenOrca-
Platypus2-13B6 https://huggingface.co/pankajmathur/orca_mini_v3_7b
(3) Prompt Generation via LLMs: Additionally, we employed a separate LLM to generate a prompt entirely from scratch. The intuition behind this approach is that an LLM-derived prompt may yield improved results over a manually-crafted prompt. Similar intuition is followed in previous works that use LLMs to produce high-quality labels for LLM-generated texts(Zellers et al., 2019;Fu et al., 2023). For prompt generation we author simple prompts, instructing the LLM to generate a sequence of instructions based on the requirements (score range and aspects to consider for summarization) specified in the prompt. (see Table4).
https://pypi.org/project/guidance/
Response without hallucinationsResponse with hallucinations 1. Relevance: 2 Explanation: The summary does not capture the key points of the article.It only mentions the man gently cupping the fish, but it does not mention the relationship between the man and the fish, the video, or the fish's behavior.2. Consistency: 2 Explanation: The summary does not reproduce all facts accurately.It does not mention the fish's hybrid origin or the bullying of the other fish.3. Fluency: 2 Explanation: The summary is not well-written and grammatically correct.It contains grammatical errors and is not easy to understand. 4. Coherence: 2 Explanation: The summary does not sound naturally and collectively.It is difficult to understand and does not flow well as a whole.2. Relevance: 3 Explanation: The summary captures the key points of the article, but it is missing some important aspects, such as the half-and-half scarves and fact that the match was played at Altrincham's Moss Lane.Answer: 3. Consistency: 3 Explanation: The summary does reproduce all facts accurately, but it makes up untrue information by stating that the match was played at Old Trafford.Answer: 4. Fluency: 3 Explanation: The individual sentences are well-written and grammatically correct, but the summary could be more coherent and sound more naturally.Answer: 4. Coherence: 3 Explanation: The summary is not as coherent as it could be, as it jumps from one topic to another without a clear flow.A Software Libraries and HardwareWe use PyTorch v2.0.1 compiled with CUDA 11.7.We use version 0.0.64 of the guidance library 7 for prompt generation and prompt evaluation.We conduct all experiments on NVIDIA T4 and NVIDIA A10G GPUs.B Prompt TemplatesB.1 SummarizationBelow we provide our prompt templates which we use for the experiments described in this paper.PromptID PromptP1Given the following summary for a news article, evaluate this summary for its fluency, coherence, consistency and relevance.Provide an overall score for the quality of this summary in the range 1 (worst) to 5 (best).Possible scores are 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5 and 5.P2Given the following summary for a news article, evaluate this summary for its fluency, coherence, consistency and relevance.Provide an overall score for the quality of this summary in the range 0 (worst) to 100 (best).Possible scores are 0,10, 20, 30, 40, 50, 60, 70, 80, 90 and 100. P3Given the following summary for a news article, evaluate this summary for its fluency, coherence, consistency and relevance.Provide an overall score for the quality of this summary in the range 1 (worst) to 5 (best) that is an average of the scores (also from 1 to 5) for fluency, coherence, consistency and relevance.P4Given the following summary for a news article, evaluate this summary for its fluency, coherence, consistency and relevance.Provide an overall score for the quality of this summary in the range 0 (worst) to 100 (best).Possible scores are0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95 and 100. P5Given the following summary for a news article, evaluate this summary for its fluency, coherence, consistency and relevance.Provide an overall score for the quality of this summary in the range 0 (worst) to 100 (best).Possible scores are 0, 1,2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100. P6Given the following summary for a news article, evaluate this summary for its fluency, coherence, consistency and relevance.Provide an overall score for the quality of this summary in the range -100 (worst) to 100 (best).Possible scores are -100, -50, 0, 50, 100.P7Given the following summary for a news article, evaluate this summary for its fluency, coherence, consistency and relevance.Provide an overall score for the quality of this summary in the range Very Poor (worst) to Very Good (best).Possible scores are Very Poor, Poor, Average, Good, Very Good.P8Given the following summary for a news article, evaluate this summary for its fluency, coherence, consistency and relevance.Provide an overall score for the quality of this summary in the range Incomprehensible (worst) to Excellent (best).Possible scores are Incomprehensible, Poor, Average, Good, Excellent.P10### User: Evaluate the summary for a news article by assigning a score from 0 to 100 for each of the following aspects: Coherence, Consistency, Fluency, and Relevance.1. Coherence: -Read the summary and determine if it is well-structured, easy to understand, and logically connected.-Assign a score from 0 to 100 based on how well the summary is coherent.Consistency:-Check if the summary accurately represents the main points and ideas from the original news article.-Assign a score from 0 to 100 based on how consistent the summary is with the original content.Fluency:-Evaluate the clarity and smoothness of the summary.-Assign a score from 0 to 100 based on how well the summary is written and easy to read.4. Relevance:-Determine if the summary effectively conveys the most important information from the original news article.-Assign a score from 0 to 100 based on how relevant and informative the summary is.Once you have evaluated each aspect, add up the scores and assign a final score from 0 to 100 for the overall summary.For each sentence in the summary, assign a score from 1 to 5 for each aspect (coherence, consistency, fluency, and relevance).So, the summary has an overall score of 3.5 out of 5.P14 ### User:To evaluate the coherence, consistency, fluency and relevance of a summary of a news article, follow these steps: 1. Read the news article carefully to understand its main points.
e-snli: Natural language inference with natural language explanations. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam Mccandlish, Alec Radford, Ilya Sutskever, Dario Amodei, Advances in Neural Information Processing Systems. Tim Rocktäschel, Thomas Lukasiewicz, Phil Blunsom, Curran Associates, Inc. Oana-Maria Camburu2020. 20183331Advances in Neural Information Processing Systems</p>
<p>Can large language models be an alternative to human evaluations?. Cheng- , Han Chiang, Hung-Yi Lee, 10.18653/v1/2023.acl-long.870Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023</p>
<p>Wojciech Alexander R Fabbri, Bryan Kryściński, Caiming Mc-Cann, Richard Xiong, Dragomir Socher, Radev, arXiv:2007.12626Summeval: Re-evaluating summarization evaluation. 2020arXiv preprint</p>
<p>Results of WMT22 metrics shared task: Stop using BLEU -neural metrics are better and more robust. Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-Kiu Lo, Craig Stewart, Eleftherios Avramidis, Tom Kocmi, George Foster, Alon Lavie, F T André, Martins, Proceedings of the Seventh Conference on Machine Translation (WMT). the Seventh Conference on Machine Translation (WMT)Abu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Gptscore: Evaluate as you desire. Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, Pengfei Liu, 10.48550/arXiv.2302.04166CoRR, abs/2302.041662023</p>
<p>Large language models are state-of-the-art evaluators of translation quality. Tom Kocmi, Christian Federmann, Proceedings of the 24th Annual Conference of the European Association for Machine Translation. the 24th Annual Conference of the European Association for Machine TranslationTampere, FinlandEuropean Association for Machine Translation2023</p>
<p>Explainable automated fact-checking: A survey. Neema Kotonya, Francesca Toni, 10.18653/v1/2020.coling-main.474Proceedings of the 28th International Conference on Computational Linguistics. the 28th International Conference on Computational LinguisticsBarcelona, Spain (Online2020International Committee on Computational Linguistics</p>
<p>Neural text summarization: A critical evaluation. Wojciech Kryscinski, Nitish Shirish Keskar, Bryan Mc-Cann, Caiming Xiong, Richard Socher, 10.18653/v1/D19-1051Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, China2019Association for Computational Linguistics</p>
<p>The eval4nlp 2023 shared task on prompting large language models as explainable metrics. Christoph Leiter, Juri Opitz, Daniel Deutsch, Yang Gao, Rotem Dror, Steffen Eger, Proceedings of the 4th Workshop on Evaluation and Comparison for NLP systems. the 4th Workshop on Evaluation and Comparison for NLP systems2023</p>
<p>G-eval: NLG evaluation using GPT-4 with better human alignment. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu, 10.48550/arXiv.2303.16634CoRR, abs/2303.166342023</p>
<p>On faithfulness and factuality in abstractive summarization. Joshua Maynez, Shashi Narayan, Bernd Bohnet, Ryan Mcdonald, 10.18653/v1/2020.acl-main.173Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics2020</p>
<p>Simple LLM prompting is state-of-the-art for robust and multilingual dialogue evaluation. John Mendonça, Patrícia Pereira, Helena Moniz, Joao Paulo Carvalho, Alon Lavie, Isabel M Trancoso, Proceedings of The Eleventh Dialog System Technology Challenge. The Eleventh Dialog System Technology ChallengePrague, Czech RepublicAssociation for Computational Linguistics2023</p>
<p>Instructscore: Towards explainable text generation evaluation with automatic feedback. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, ; Zhenqiao Zhou, Markus Song, William Freitag, Wang Yang, Lei Li, Advances in Neural Information Processing Systems. Curran Associates, Inc. Wenda Xu2022. 202335Danqing Wang, Liangming Pan,Chain-of-thought prompting elicits reasoning in large language models</p>
<p>Knowledgeprompted estimator: A novel approach to explainable machine translation assessment. Hao Yang, Min Zhang, Shimin Tao, Minghan Wang, Daimeng Wei, Yanfei Jiang, 2023</p>
<p>Defending against neural fake news. Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, Yejin Choi, Advances in neural information processing systems. 201932</p>
<p>Help as much as you can. ### User: In this task you will evaluate the quality of summaries written for a news article To correctly solve this task. ### System, You are an AI assistant that follows instruction extremely well. follow these steps: 1. Carefully read the news articles, be aware of the information it contains</p>
<p>Read the proposed summary. </p>
<p>Consistency: "The rating measures the facts in the summary are consistent with the facts in the original article. Consider whether the summary does reproduce all facts accurately and does not make up untrue information. Format the response as follows: Answer: Relevance: <Rating for Relevance> Explanation: <Evidence for Relevance rating> Consistency: <Rating for Consistency> Explanation: <Evidence for Consistency rating> Fluency: <Rating for Fluency> Explanation: <Evidence for Fluency rating> Coherence: <Rating for Coherence> Explanation: <Evidence for Coherence rating> News article: {source_text} Summary: {summary} ### Assistant. AnswerCoherence: "The rating measures the quality of all sentences collectively, to the fit together and sound naturally. Consider the quality of the summary as a whole</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3220 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3220</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3220</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-74.html">extraction-schema-74</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-07b14c24833400b79978b0a5f084803337e30a15</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/07b14c24833400b79978b0a5f084803337e30a15" target="_blank">REPLUG: Retrieval-Augmented Black-Box Language Models</a></p>
                <p><strong>Paper Venue:</strong> North American Chapter of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> REPLUG is introduced, a retrieval-augmented language modeling framework that treats the language model (LM) as a black box and augments it with a tuneable retrieval model, and can be easily applied to any existing language models.</p>
                <p><strong>Paper Abstract:</strong> We introduce REPLUG, a retrieval-augmented language modeling framework that treats the language model (LM) as a black box and augments it with a tuneable retrieval model. Unlike prior retrieval-augmented LMs that train language models with special cross-attention mechanisms to encode the retrieved text, REPLUG simply prepends retrieved documents to the input for the frozen black-box LM. This simple design can be easily applied to any existing language models. Furthermore, we show that the LM can be used to supervise the retrieval model, which can then find documents that help the LM make better predictions. Our experiments demonstrate that REPLUG with the tuned retriever significantly improves the performance of GPT-3 (175B) on language modeling by 6.3%, as well as the performance of Codex on five-shot MMLU by 5.1%. Code is publicly released at github.com/swj0419/REPLUG.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3220.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3220.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>REPLUG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieve and Plug (REPLUG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented framework that treats a language model as a frozen black box and improves its predictions by prepending retrieved documents to the model input and ensembling outputs across separate passes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>REPLUG</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A plug-and-play agent that pairs any frozen black-box LM with a retrieval module (typically a dual-encoder dense retriever); for a query it retrieves top-k documents, prepends each document to the query in separate forward passes through the frozen LM, and ensembles the output token probabilities weighted by document-query similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented / external datastore</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Documents from an external corpus are embedded with a dual-encoder (e.g., Contriever) and stored in a FAISS index; at query time the top-k documents by cosine similarity are retrieved, each document is concatenated (prepended) to the input and fed to the frozen LM in separate passes; final next-token probabilities are a weighted average of the runs with weights proportional to exp(similarity). Retrieval is read-only at inference (no LM parameter updates).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Language modeling (Pile, Wikitext-103); few-shot multiple-choice (MMLU); open-domain QA (Natural Questions, TriviaQA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Language modeling: predict continuations with lower perplexity across diverse domains; MMLU: 5-shot in-context multi-choice exam-style QA across 57 tasks; Open-domain QA: few-shot question answering requiring retrieval of factual passages.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>language modeling / question answering / in-context few-shot reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Language modeling (Pile BPB examples): GPT-3 Davinci +REPLUG = 0.77 BPB (from 0.80 BPB baseline; +3.8% relative); GPT-2 XL +REPLUG = 1.09 BPB (from 1.16; ~6.0% relative). MMLU (5-shot): Codex +RePlug = 71.4 (from 68.3 baseline; +3.1 points absolute). Open-domain QA (k-shot): Codex +RePlug on NQ = 44.7 (from 40.6 baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Language modeling baselines: GPT-3 Davinci = 0.80 BPB; GPT-2 XL = 1.16 BPB. MMLU baseline: Codex = 68.3. Open-domain QA baseline: Codex NQ = 40.6.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Prepending retrieved, relevant documents and ensembling across separate LM passes substantially improves frozen black-box LMs across tasks: REPLUG reduces language-model BPB across GPT/GPT-2 family models and improves few-shot MMLU and open-domain QA; gains increase monotonically with the number of relevant documents ensembled and are not attributable to ensembling random documents.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Interpretability (unclear when output relies on retrieved vs parametric knowledge); always-on retrieval (may retrieve distracting/irrelevant docs and incurs compute cost); limited by LM context window (handled via ensembling but incurs extra LM calls); performance depends on retrieval quality and datastore coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'REPLUG: Retrieval-Augmented Black-Box Language Models', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3220.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3220.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>REPLUG LSR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>REPLUG with LM-Supervised Retrieval (REPLUG LSR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An extension of REPLUG that fine-tunes the dense retriever using supervision from a frozen black-box LM: the retriever is trained to prefer documents that reduce the LM's perplexity on ground-truth continuations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>REPLUG LSR</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same black-box LM + retrieval architecture as REPLUG, but the retriever (initialized from Contriever) is tuned using a KL objective that aligns retriever probabilities over retrieved docs with a normalized distribution of LM likelihoods of the true continuation given each doc; document embeddings and FAISS index are asynchronously updated during retriever training.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented / learned external datastore</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>For a training query (x,y), retrieve top-k docs by current retriever; compute P_R(d|x) = softmax(s(d,x)/gamma) over retrieved docs; compute Q(d|x,y) = softmax(P_LM(y|d,x)/beta) using the frozen LM scoring the ground-truth continuation; minimize KL(Q || P_R) to update retriever parameters; periodically recompute document embeddings and rebuild FAISS index.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Language modeling (Pile BPB / Wikitext-103); few-shot MMLU; open-domain QA (Natural Questions, TriviaQA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same tasks as REPLUG, emphasizing improved retrieval quality to reduce LM perplexity and improve few-shot QA accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>language modeling / question answering / in-context few-shot reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Language modeling (examples): GPT-3 Davinci +REPLUG LSR = 0.75 BPB (from 0.80 baseline; +6.3% relative). Average over 8 models: REPLUG LSR yields ~7.7% improvement vs baseline. MMLU (5-shot): Codex +RePlug LSR = 71.8 (from 68.3 baseline; reported +5.1% relative). Open-domain QA (k-shot): Codex +RePlug LSR on NQ = 45.5 (from 40.6 baseline; +12.0% relative); on TriviaQA = 77.3 (from 73.6 baseline; +5.0% relative).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>See corresponding baselines: GPT-3 Davinci = 0.80 BPB; Codex MMLU = 68.3; Codex NQ = 40.6; Codex TQA = 73.6.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Adapting the retriever to the frozen LM using LM-derived supervision yields consistent additional gains over an off-the-shelf retriever (REPLUG); REPLUG LSR outperforms several off-the-shelf dense and sparse retrievers (including Contriever, DPR, BERT-base and BM25) on downstream perplexity and QA metrics and can surpass or match white-box retrieval approaches in few-shot settings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Training requires repeated LM scoring of continuations (compute/ API cost); approximate marginalization over only retrieved docs (practical tractability); must asynchronously rebuild FAISS index as retriever changes; still constrained by datastore coverage and always-on retrieval drawbacks discussed for REPLUG.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'REPLUG: Retrieval-Augmented Black-Box Language Models', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3220.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3220.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>kNN-LM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>k-Nearest Neighbor Language Model (kNN-LM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented approach that interpolates an LM's next-token distribution with a k-nearest-neighbor distribution computed from cached token representations retrieved from a datastore of past contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generalization through memorization: Nearest neighbor language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>kNN-LM</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Augments a parametric LM by retrieving nearest neighbor contexts (based on internal LM representations) and deriving a kNN-based token distribution that is interpolated with the LM's distribution at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented (token-level kNN datastore using internal LM representations)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Stores token-level (or context) representations produced by the LM; at inference, retrieves nearest neighbors by similarity in representation space and computes a kNN next-token distribution to combine with the LM's softmax.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Language modeling (and related tasks requiring token-level memorization)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Improve next-token prediction through non-parametric lookup of similar past contexts; addresses long-tail memorization and factual recall.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>language modeling</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as prior work that requires access to internal LM representations and so is not directly applicable to black-box LMs; illustrates token-level nearest-neighbor memory can improve LM predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Depends on access to LM internals (hidden states), which prevents application to black-box API models; storage/computation cost of large token-level datastore.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'REPLUG: Retrieval-Augmented Black-Box Language Models', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3220.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3220.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RETRO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RETRO / Retrieval-Enhanced Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented LM that modifies the decoder architecture to condition on retrieved passages, enabling retrieval during pretraining and inference to improve language modeling at scale.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Improving language models by retrieving from trillions of tokens</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RETRO</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A model that augments the transformer decoder with cross-attention-like mechanisms to incorporate retrieved passages into generation; RETRO is trained from scratch with retrieval integrated into the architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented integrated into LM architecture</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>At each step, retrieved passages are encoded and integrated into decoder computations via specialized cross-attention components; pretraining and architecture supports large-scale retrieval over massive datastores.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Language modeling at large scale (pretraining and downstream tasks benefitting from retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Reduce perplexity by conditioning on retrieved passages from very large corpora; supports long-tail knowledge recall.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>language modeling</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as a white-box retrieval approach that modifies LM internals (unlike REPLUG's black-box approach) and demonstrates retrieval benefits when integrated into model architecture and pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires architectural changes and access to LM weights (not applicable to black-box API models); heavy pretraining complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'REPLUG: Retrieval-Augmented Black-Box Language Models', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3220.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3220.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Atlas</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Atlas (Few-shot learning with retrieval augmented language models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented language model that jointly trains the retriever and a generative model (white-box), used as a baseline for few-shot retrieval-augmented learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Few-shot learning with retrieval augmented language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Atlas</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A white-box retrieval-augmented model that trains both retriever and generator jointly, modeling documents as latent variables to improve few-shot retrieval and generation.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented with joint retriever-generator training</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Learns retrieval jointly with the generation objective; retrieved passages are incorporated into the generator model (requires access to model internals and fine-tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Few-shot MMLU; open-domain QA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Few-shot in-context reasoning and QA tasks where retrieval of example-supporting passages can improve accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multiple-choice QA / open-domain QA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported in paper tables: MMLU All = 47.9 (Atlas, cited); NQ full = 60.4; TQA full = 79.8.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Included as a prior retrieval-augmented baseline; REPLUG LSR outperforms Atlas on some few-shot QA benchmarks in the paper's comparisons despite Atlas being a white-box trained model.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Atlas requires fine-tuning both retriever and generator and thus is a white-box method; less applicable when only black-box access is available.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'REPLUG: Retrieval-Augmented Black-Box Language Models', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3220.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3220.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>R2-D2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>R2-D2: A modular baseline for open-domain question answering</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular retrieval + reading pipeline for open-domain QA used as a comparative baseline in the paper's QA experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>R2-D2: A modular baseline for open-domain question answering</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>R2-D2</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A modular open-domain QA system combining retriever and reader components, evaluated on standard QA benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented (retriever + reader pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Uses a retriever to fetch candidate passages and a reader to extract/score answers; typically fine-tuned on QA training data in full-data settings.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Open-domain QA (TriviaQA, Natural Questions)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Answer open-domain factual questions by retrieving relevant passages and extracting/producing an answer.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>open-domain question answering</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported in paper table (full-data settings): NQ full = 55.9; TQA full = 69.9.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as a representative retrieval-augmented QA baseline; REPLUG LSR exceeds some previous few-shot retrieval baselines on NQ/TQA in the few-shot setting reported.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Performance typically depends on supervised fine-tuning on QA data (full-data methods outperform few-shot retrieval-augmented black-box approaches).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'REPLUG: Retrieval-Augmented Black-Box Language Models', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Improving language models by retrieving from trillions of tokens <em>(Rating: 2)</em></li>
                <li>Few-shot learning with retrieval augmented language models <em>(Rating: 2)</em></li>
                <li>Generalization through memorization: Nearest neighbor language models <em>(Rating: 2)</em></li>
                <li>Retrieval augmented language model pre-training <em>(Rating: 2)</em></li>
                <li>R2-D2: A modular baseline for open-domain question answering <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3220",
    "paper_id": "paper-07b14c24833400b79978b0a5f084803337e30a15",
    "extraction_schema_id": "extraction-schema-74",
    "extracted_data": [
        {
            "name_short": "REPLUG",
            "name_full": "Retrieve and Plug (REPLUG)",
            "brief_description": "A retrieval-augmented framework that treats a language model as a frozen black box and improves its predictions by prepending retrieved documents to the model input and ensembling outputs across separate passes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "REPLUG",
            "agent_description": "A plug-and-play agent that pairs any frozen black-box LM with a retrieval module (typically a dual-encoder dense retriever); for a query it retrieves top-k documents, prepends each document to the query in separate forward passes through the frozen LM, and ensembles the output token probabilities weighted by document-query similarity.",
            "memory_used": true,
            "memory_type": "retrieval-augmented / external datastore",
            "memory_mechanism_description": "Documents from an external corpus are embedded with a dual-encoder (e.g., Contriever) and stored in a FAISS index; at query time the top-k documents by cosine similarity are retrieved, each document is concatenated (prepended) to the input and fed to the frozen LM in separate passes; final next-token probabilities are a weighted average of the runs with weights proportional to exp(similarity). Retrieval is read-only at inference (no LM parameter updates).",
            "task_name": "Language modeling (Pile, Wikitext-103); few-shot multiple-choice (MMLU); open-domain QA (Natural Questions, TriviaQA)",
            "task_description": "Language modeling: predict continuations with lower perplexity across diverse domains; MMLU: 5-shot in-context multi-choice exam-style QA across 57 tasks; Open-domain QA: few-shot question answering requiring retrieval of factual passages.",
            "task_type": "language modeling / question answering / in-context few-shot reasoning",
            "performance_with_memory": "Language modeling (Pile BPB examples): GPT-3 Davinci +REPLUG = 0.77 BPB (from 0.80 BPB baseline; +3.8% relative); GPT-2 XL +REPLUG = 1.09 BPB (from 1.16; ~6.0% relative). MMLU (5-shot): Codex +RePlug = 71.4 (from 68.3 baseline; +3.1 points absolute). Open-domain QA (k-shot): Codex +RePlug on NQ = 44.7 (from 40.6 baseline).",
            "performance_without_memory": "Language modeling baselines: GPT-3 Davinci = 0.80 BPB; GPT-2 XL = 1.16 BPB. MMLU baseline: Codex = 68.3. Open-domain QA baseline: Codex NQ = 40.6.",
            "has_performance_comparison": true,
            "key_findings": "Prepending retrieved, relevant documents and ensembling across separate LM passes substantially improves frozen black-box LMs across tasks: REPLUG reduces language-model BPB across GPT/GPT-2 family models and improves few-shot MMLU and open-domain QA; gains increase monotonically with the number of relevant documents ensembled and are not attributable to ensembling random documents.",
            "limitations_or_challenges": "Interpretability (unclear when output relies on retrieved vs parametric knowledge); always-on retrieval (may retrieve distracting/irrelevant docs and incurs compute cost); limited by LM context window (handled via ensembling but incurs extra LM calls); performance depends on retrieval quality and datastore coverage.",
            "uuid": "e3220.0",
            "source_info": {
                "paper_title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "REPLUG LSR",
            "name_full": "REPLUG with LM-Supervised Retrieval (REPLUG LSR)",
            "brief_description": "An extension of REPLUG that fine-tunes the dense retriever using supervision from a frozen black-box LM: the retriever is trained to prefer documents that reduce the LM's perplexity on ground-truth continuations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "REPLUG LSR",
            "agent_description": "Same black-box LM + retrieval architecture as REPLUG, but the retriever (initialized from Contriever) is tuned using a KL objective that aligns retriever probabilities over retrieved docs with a normalized distribution of LM likelihoods of the true continuation given each doc; document embeddings and FAISS index are asynchronously updated during retriever training.",
            "memory_used": true,
            "memory_type": "retrieval-augmented / learned external datastore",
            "memory_mechanism_description": "For a training query (x,y), retrieve top-k docs by current retriever; compute P_R(d|x) = softmax(s(d,x)/gamma) over retrieved docs; compute Q(d|x,y) = softmax(P_LM(y|d,x)/beta) using the frozen LM scoring the ground-truth continuation; minimize KL(Q || P_R) to update retriever parameters; periodically recompute document embeddings and rebuild FAISS index.",
            "task_name": "Language modeling (Pile BPB / Wikitext-103); few-shot MMLU; open-domain QA (Natural Questions, TriviaQA)",
            "task_description": "Same tasks as REPLUG, emphasizing improved retrieval quality to reduce LM perplexity and improve few-shot QA accuracy.",
            "task_type": "language modeling / question answering / in-context few-shot reasoning",
            "performance_with_memory": "Language modeling (examples): GPT-3 Davinci +REPLUG LSR = 0.75 BPB (from 0.80 baseline; +6.3% relative). Average over 8 models: REPLUG LSR yields ~7.7% improvement vs baseline. MMLU (5-shot): Codex +RePlug LSR = 71.8 (from 68.3 baseline; reported +5.1% relative). Open-domain QA (k-shot): Codex +RePlug LSR on NQ = 45.5 (from 40.6 baseline; +12.0% relative); on TriviaQA = 77.3 (from 73.6 baseline; +5.0% relative).",
            "performance_without_memory": "See corresponding baselines: GPT-3 Davinci = 0.80 BPB; Codex MMLU = 68.3; Codex NQ = 40.6; Codex TQA = 73.6.",
            "has_performance_comparison": true,
            "key_findings": "Adapting the retriever to the frozen LM using LM-derived supervision yields consistent additional gains over an off-the-shelf retriever (REPLUG); REPLUG LSR outperforms several off-the-shelf dense and sparse retrievers (including Contriever, DPR, BERT-base and BM25) on downstream perplexity and QA metrics and can surpass or match white-box retrieval approaches in few-shot settings.",
            "limitations_or_challenges": "Training requires repeated LM scoring of continuations (compute/ API cost); approximate marginalization over only retrieved docs (practical tractability); must asynchronously rebuild FAISS index as retriever changes; still constrained by datastore coverage and always-on retrieval drawbacks discussed for REPLUG.",
            "uuid": "e3220.1",
            "source_info": {
                "paper_title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "kNN-LM",
            "name_full": "k-Nearest Neighbor Language Model (kNN-LM)",
            "brief_description": "A retrieval-augmented approach that interpolates an LM's next-token distribution with a k-nearest-neighbor distribution computed from cached token representations retrieved from a datastore of past contexts.",
            "citation_title": "Generalization through memorization: Nearest neighbor language models",
            "mention_or_use": "mention",
            "agent_name": "kNN-LM",
            "agent_description": "Augments a parametric LM by retrieving nearest neighbor contexts (based on internal LM representations) and deriving a kNN-based token distribution that is interpolated with the LM's distribution at inference.",
            "memory_used": true,
            "memory_type": "retrieval-augmented (token-level kNN datastore using internal LM representations)",
            "memory_mechanism_description": "Stores token-level (or context) representations produced by the LM; at inference, retrieves nearest neighbors by similarity in representation space and computes a kNN next-token distribution to combine with the LM's softmax.",
            "task_name": "Language modeling (and related tasks requiring token-level memorization)",
            "task_description": "Improve next-token prediction through non-parametric lookup of similar past contexts; addresses long-tail memorization and factual recall.",
            "task_type": "language modeling",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": false,
            "key_findings": "Mentioned as prior work that requires access to internal LM representations and so is not directly applicable to black-box LMs; illustrates token-level nearest-neighbor memory can improve LM predictions.",
            "limitations_or_challenges": "Depends on access to LM internals (hidden states), which prevents application to black-box API models; storage/computation cost of large token-level datastore.",
            "uuid": "e3220.2",
            "source_info": {
                "paper_title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "RETRO",
            "name_full": "RETRO / Retrieval-Enhanced Transformer",
            "brief_description": "A retrieval-augmented LM that modifies the decoder architecture to condition on retrieved passages, enabling retrieval during pretraining and inference to improve language modeling at scale.",
            "citation_title": "Improving language models by retrieving from trillions of tokens",
            "mention_or_use": "mention",
            "agent_name": "RETRO",
            "agent_description": "A model that augments the transformer decoder with cross-attention-like mechanisms to incorporate retrieved passages into generation; RETRO is trained from scratch with retrieval integrated into the architecture.",
            "memory_used": true,
            "memory_type": "retrieval-augmented integrated into LM architecture",
            "memory_mechanism_description": "At each step, retrieved passages are encoded and integrated into decoder computations via specialized cross-attention components; pretraining and architecture supports large-scale retrieval over massive datastores.",
            "task_name": "Language modeling at large scale (pretraining and downstream tasks benefitting from retrieval)",
            "task_description": "Reduce perplexity by conditioning on retrieved passages from very large corpora; supports long-tail knowledge recall.",
            "task_type": "language modeling",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": false,
            "key_findings": "Cited as a white-box retrieval approach that modifies LM internals (unlike REPLUG's black-box approach) and demonstrates retrieval benefits when integrated into model architecture and pretraining.",
            "limitations_or_challenges": "Requires architectural changes and access to LM weights (not applicable to black-box API models); heavy pretraining complexity.",
            "uuid": "e3220.3",
            "source_info": {
                "paper_title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "Atlas",
            "name_full": "Atlas (Few-shot learning with retrieval augmented language models)",
            "brief_description": "A retrieval-augmented language model that jointly trains the retriever and a generative model (white-box), used as a baseline for few-shot retrieval-augmented learning.",
            "citation_title": "Few-shot learning with retrieval augmented language models",
            "mention_or_use": "mention",
            "agent_name": "Atlas",
            "agent_description": "A white-box retrieval-augmented model that trains both retriever and generator jointly, modeling documents as latent variables to improve few-shot retrieval and generation.",
            "memory_used": true,
            "memory_type": "retrieval-augmented with joint retriever-generator training",
            "memory_mechanism_description": "Learns retrieval jointly with the generation objective; retrieved passages are incorporated into the generator model (requires access to model internals and fine-tuning).",
            "task_name": "Few-shot MMLU; open-domain QA",
            "task_description": "Few-shot in-context reasoning and QA tasks where retrieval of example-supporting passages can improve accuracy.",
            "task_type": "multiple-choice QA / open-domain QA",
            "performance_with_memory": "Reported in paper tables: MMLU All = 47.9 (Atlas, cited); NQ full = 60.4; TQA full = 79.8.",
            "performance_without_memory": null,
            "has_performance_comparison": false,
            "key_findings": "Included as a prior retrieval-augmented baseline; REPLUG LSR outperforms Atlas on some few-shot QA benchmarks in the paper's comparisons despite Atlas being a white-box trained model.",
            "limitations_or_challenges": "Atlas requires fine-tuning both retriever and generator and thus is a white-box method; less applicable when only black-box access is available.",
            "uuid": "e3220.4",
            "source_info": {
                "paper_title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "R2-D2",
            "name_full": "R2-D2: A modular baseline for open-domain question answering",
            "brief_description": "A modular retrieval + reading pipeline for open-domain QA used as a comparative baseline in the paper's QA experiments.",
            "citation_title": "R2-D2: A modular baseline for open-domain question answering",
            "mention_or_use": "mention",
            "agent_name": "R2-D2",
            "agent_description": "A modular open-domain QA system combining retriever and reader components, evaluated on standard QA benchmarks.",
            "memory_used": true,
            "memory_type": "retrieval-augmented (retriever + reader pipeline)",
            "memory_mechanism_description": "Uses a retriever to fetch candidate passages and a reader to extract/score answers; typically fine-tuned on QA training data in full-data settings.",
            "task_name": "Open-domain QA (TriviaQA, Natural Questions)",
            "task_description": "Answer open-domain factual questions by retrieving relevant passages and extracting/producing an answer.",
            "task_type": "open-domain question answering",
            "performance_with_memory": "Reported in paper table (full-data settings): NQ full = 55.9; TQA full = 69.9.",
            "performance_without_memory": null,
            "has_performance_comparison": false,
            "key_findings": "Cited as a representative retrieval-augmented QA baseline; REPLUG LSR exceeds some previous few-shot retrieval baselines on NQ/TQA in the few-shot setting reported.",
            "limitations_or_challenges": "Performance typically depends on supervised fine-tuning on QA data (full-data methods outperform few-shot retrieval-augmented black-box approaches).",
            "uuid": "e3220.5",
            "source_info": {
                "paper_title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
                "publication_date_yy_mm": "2023-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Improving language models by retrieving from trillions of tokens",
            "rating": 2
        },
        {
            "paper_title": "Few-shot learning with retrieval augmented language models",
            "rating": 2
        },
        {
            "paper_title": "Generalization through memorization: Nearest neighbor language models",
            "rating": 2
        },
        {
            "paper_title": "Retrieval augmented language model pre-training",
            "rating": 2
        },
        {
            "paper_title": "R2-D2: A modular baseline for open-domain question answering",
            "rating": 1
        }
    ],
    "cost": 0.016379499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>RePlug: Retrieval-Augmented Black-Box Language Models</h1>
<p>Weijia Shi ${ }^{1,2}$ Sewon Min ${ }^{1}$ Michihiro Yasunaga ${ }^{3}$ Minjoon Seo ${ }^{4}$<br>Rich James ${ }^{2}$ Mike Lewis ${ }^{2}$ Luke Zettlemoyer ${ }^{1,2}$ Wen-tau Yih ${ }^{2}$<br>${ }^{1}$ University of Washington, Seattle, WA, ${ }^{2}$ FAIR, Meta<br>${ }^{3}$ Stanford University ${ }^{4}$ KAIST<br>swj0419@uw.edu</p>
<h4>Abstract</h4>
<p>We introduce REPLUG, a retrieval-augmented language modeling framework that treats the language model (LM) as a black box and augments it with a tuneable retrieval model. Unlike prior retrieval-augmented LMs that train language models with special cross attention mechanisms to encode the retrieved text, REPLUG simply prepends retrieved documents to the input for the frozen black-box LM. This simple design can be easily applied to any existing language models. Furthermore, we show that the LM can be used to supervise the retrieval model, which can then find documents that help the LM make better predictions. Our experiments demonstrate that RePLUG with the tuned retriever significantly improves the performance of GPT-3 (175B) on language modeling by $6.3 \%$, as well as the performance of Codex on five-shot MMLU by $5.1 \%$. Code is publicly released at github.com/swj0419/REPLUG.</p>
<h2>1 Introduction</h2>
<p>Large language models (LMs) such as GPT3 (Brown et al., 2020) and Codex (Chen et al., 2021), have demonstrated impressive performance on a wide range of language tasks. These models are typically trained on very large datasets and store a substantial amount of world or domain knowledge implicitly in their parameters. However, they are also prone to hallucination and cannot represent the full long tail of knowledge from the training corpus. Retrieval-augmented language models (Khandelwal et al., 2020; Borgeaud et al., 2022; Izacard et al., 2022b; Yasunaga et al., 2023), in contrast, can retrieve knowledge from an external datastore when needed, potentially reducing hallucination and increasing coverage. Previous approaches of retrieval-augmented language models require access to the internal LM representations (e.g., to train the model (Borgeaud et al., 2022; Izacard et al., 2022b) or to index the datastore (Khandelwal
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Different from previous retrieval-augmented approaches (Borgeaud et al., 2022) that enhance a language model with retrieval by updating the LM's parameters, REPLUG treats the LM as a black box and augments it with a frozen or tunable retriever. This black-box assumption makes RePLUG applicable to large LMs, which are often served via APIs.
et al., 2020)), and are thus difficult to be applied to very large LMs. In addition, many best-in-class LLMs can only be accessed through APIs. Internal representations of such models are not exposed and fine-tuning is not supported.</p>
<p>In this work, we introduce RePLUG (Retrieve and Plug), a new retrieval-augmented LM framework where the language model is viewed as a black box and the retrieval component is added as a tuneable plug-and-play module. Given an input context, RePLUG first retrieves relevant documents from an external corpus using an off-theshelf retrieval model. The retrieved documents are prepended to the input context and fed into the black-box LM to make the final prediction. Because the LM context length limits the number of documents that can be prepended, we also adopt an ensemble scheme that encodes the retrieved documents in parallel with the same black-box LM, allowing us to easily trade compute for accuracy.</p>
<p>As shown in Figure 1, REPLUG is extremely flexible and can be used with any existing black-box LM and retrieval model.</p>
<p>We also introduce REPLUG LSR (REPLUG with LM-Supervised Retrieval), a training scheme that can further improve the initial retrieval model in REPLUG with supervision signals from a blackbox language model. The key idea is to adapt the retriever to the LM, which is in contrast to prior work (Borgeaud et al., 2022) that adapts language models to the retriever. We use a training objective which prefers retrieving documents that improve language model perplexity, while treating the LM as a frozen, black-box scoring function.</p>
<p>Our experiments show that REPLUG can improve the performance of diverse black-box LMs on both language modeling and downstream tasks, including MMLU (Hendrycks et al., 2021) and open-domain QA (Kwiatkowski et al., 2019; Joshi et al., 2017). For instance, REPLUG can improve Codex (175B) performance on MMLU by $4.5 \%$, achieving comparable results to the 540B, instruction-finetuned Flan-PaLM. Furthermore, tuning the retriever with our training scheme (i.e., REPLUG LSR) outperforms various off-theshelf retrievers and leads to additional improvements, including up to $6.3 \%$ increase in GPT-3 175B language modeling. To the best of our knowledge, our work is the first to show the benefits of retrieval to large LMs ( $&gt;100 \mathrm{~B}$ model parameters), for both reducing LM perplexity and and improving in-context learning performance. We summarize our contributions as follows:</p>
<ul>
<li>We introduce REPLUG (3), the first retrievalaugmented language modeling framework for enhancing black-box LMs with retrieval. Unlike previous methods that require updating the LM's parameters, REPLUG could be easily plugged into any existing LM without additional finetuning.</li>
<li>We propose a training scheme (4) to further adapt an off-the-shelf retrieval model to the LM, using the language modeling scores as supervision signals, resulting in improved retrieval quality.</li>
<li>We are the first to demonstrate that retrieval can benefit large-scale, state-of-the-art LMs on language modeling (6) and in-context learning tasks. Evaluations show that REPLUG can improve the performance of var-
ious language models such as GPT, OPT and BLOOM, including very large models with up to 175B parameters.</li>
</ul>
<h2>2 Background and Related Work</h2>
<p>Black-box Language Models Large language models, such as GPT-3 (Brown et al., 2020), Codex (Chen et al., 2021), are not open-sourced due to commercial considerations and are only available as black-box APIs, through which users can send queries and receive responses. On the other hand, even open sourced language models such as BLOOM-176B (Scao et al., 2022) require significant computational resources to run and finetune locally. For example, finetuning BLOOM176B requires 72 A100 GPUs (Younes Belkda, 2022), making them inaccessible to researchers and developers with limited resources. Traditionally, retrieval-augmented model frameworks (Khandelwal et al., 2020; Borgeaud et al., 2022; Yu, 2022; Izacard et al., 2022b; Goyal et al., 2022) have focused on the white-box setting, where language models are fine-tuned to incorporate retrieved documents. However, the increasing scale and blackbox nature of LLMs makes this approach infeasible. To address these challenges, we investigate retrieval-augmentation in the black-box setting, where users only have access to the model predictions and cannot access or modify its parameters.</p>
<p>Retrieval-augmented Models Augmenting language models with relevant information retrieved from knowledge stores has shown to be effective in improving performance on various NLP tasks, including language modeling (Min et al., 2022; Borgeaud et al., 2022; Khandelwal et al., 2020) and open-domain question answering (Lewis et al., 2020; Izacard et al., 2022b; Hu et al., 2022). Specifically, using the input as query, (1) a retriever first retrieves a set of documents from a corpus and then (2) a language model incorporates the retrieved documents as additional information to make a final prediction. Previous retrieval-augmented LMs require updating the model parameters, which cannot be applied to black-box LMs, which cannot be applied to black-box LMs. For example, Atlas (Izacard et al., 2022b) finetunes an encoder-decoder model jointly with the retriever by modeling documents as latent variables, while RETRO (Borgeaud et al., 2022) changes the decoder-only architecture to incorporate retrieved texts and pretrains the language model from scratch. Another line of</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: RePlug at inference (3). Given an input context, RePlug first retrieves a small set of relevant documents from an external corpus using a retriever ( $\S 3.1$ Document Retrieval). Then it prepends each document separately to the input context and ensembles output probabilities from different passes ( $\S 3.2$ Input Reformulation).
retrieval-augmented LMs such as kNN-LM (Khandelwal et al., 2020; Zhong et al., 2022) retrieves a set of tokens and interpolates between the LM's next token distribution and kNN distributions computed from the retrieved tokens at inference. kNNLM requires access to internal LM representations to compute the kNN distribution, which are not available for black-box LMs such as GPT-3. In this work, we investigate ways to improve large blackbox language models with retrieval. While concurrent work (Mallen et al., 2022; Si et al., 2023) has demonstrated that using a frozen retriever can improve GPT-3 performance on open-domain question answering, we approach the problem in a more general setting, including language modeling and understanding tasks. We additionally adopt an ensemble method to incorporate more documents and a training scheme to further adapt the retriever to large LMs.</p>
<h2>3 REPLUG</h2>
<p>We introduce RePlug (Retrieve and Plug), a new retrieval-augmented LM paradigm where the LM is treated as black box and the retrieval component is added as a potentially tuneable module.</p>
<p>As shown in Figure 2, given an input context, REPLUG first retrieves a small set of relevant documents from an external corpus using a retriever (3.1). Then we pass the concatenation of each retrieved document with the input context through the LM in parallel, and ensemble the predicted probabilities (3.2).</p>
<h3>3.1 Document Retrieval</h3>
<p>Given an input context $x$, the retriever aims to retrieve a small set of documents from a corpus $\mathcal{D}=\left{d_{1} \ldots d_{m}\right}$ that are relevant to $x$. Following prior work (Qu et al., 2021; Izacard and Grave, 2021; Ni et al., 2022), we use a dense retriever based on the dual encoder architecture, where an encoder is used to encode both the input context $x$ and the document $d$. Specifically, the encoder maps each document $d \in D$ to an embedding $\mathbf{E}(d)$ by taking the mean pooling of the last hidden representation over the tokens in $d$. At query time, the same encoder is applied to the input context $x$ to obtain a query embedding $\mathbf{E}(x)$. The similarity between the query embedding and the document embedding is computed by their cosine similarity:</p>
<p>$$
s(d, x)=\cos (\mathbf{E}(d), \mathbf{E}(x))
$$</p>
<p>The top- $k$ documents that have the highest similarity scores when compared with the input $x$ are retrieved in this step. For efficient retrieval, we precompute the embedding of each document $d \in D$ and construct FAISS index (Johnson et al., 2019) over these embeddings.</p>
<h3>3.2 Input Reformulation</h3>
<p>The retrieved top- $k$ documents provide rich information about the original input context $x$ and can potentially help the LM to make a better prediction. One simple way to incorporate the retrieved documents as part of the input to the LM is to prepend $x$ with all $k$ documents. However, this simple scheme is fundamentally restricted by the number of documents (i.e., $k$ ) we can include, given the language</p>
<p>model's context window size. To address this limitation, we adopt an ensemble strategy described as follows. Assume $\mathcal{D}^{\prime} \subset \mathcal{D}$ consists of $k$ most relevant documents to $x$, according to the scoring function in Eq. (1). We prepend each document $d \in \mathcal{D}^{\prime}$ to $x$, pass this concatenation to the LM separately, and then ensemble output probabilities from all $k$ passes. Formally, given the input context $x$ and its top- $k$ relevant documents $\mathcal{D}^{\prime}$, the output probability of the next token $y$ is computed as a weighted average ensemble:</p>
<p>$$
p\left(y \mid x, \mathcal{D}^{\prime}\right)=\sum_{d \in \mathcal{D}^{\prime}} p(y \mid d \circ x) \cdot \lambda(d, x)
$$</p>
<p>where $\circ$ denotes the concatenation of two sequences and the weight $\lambda(d, x)$ is based on the similarity score between the document $d$ and the input context $x$ :</p>
<p>$$
\lambda(d, x)=\frac{e^{s(d, x)}}{\sum_{d \in \mathcal{D}^{\prime}} e^{s(d, x)}}
$$</p>
<h2>4 REPLUG LSR: Training the Dense Retriever</h2>
<p>Instead of relying only on existing neural dense retrieval models (Karpukhin et al., 2020; Izacard et al., 2022a; Su et al., 2023), we further propose REPLUG LSR (REPLUG with LM-Supervised Retrieval), which adapts the retriever in REPLUG by using the LM itself to provide supervision about which documents should be retrieved.</p>
<p>Inspired by Sachan et al. (2023), our approach can be seen as adjusting the probabilities of the retrieved documents to match the probabilities of the output sequence perplexities of the language model. In other words, we would like the retriever to find documents that result in lower perplexity scores. As shown in Figure 3, our training algorithm consists of the four steps: (1) retrieving documents and computing the retrieval likelihood (4.1), (2) scoring the retrieved documents by the language model (4.2), (3) updating the retrieval model parameters by minimizing the KL divergence between the retrieval likelihood and the LM's score distribution (4.3), and (4) asynchronous update of the datastore index (4.4).</p>
<h3>4.1 Computing Retrieval Likelihood</h3>
<p>We retrieve $k$ documents $\mathcal{D}^{\prime} \subset \mathcal{D}$ with the highest similarity scores from a corpus $\mathcal{D}$ given an input context $x$, as described in $\S 3.1$. We then compute
the retrieval likelihood of each retrieved document $d$ :</p>
<p>$$
P_{R}(d \mid x)=\frac{e^{s(d, x) / \gamma}}{\sum_{d \in \mathcal{D}^{\prime}} e^{s(d, x) / \gamma}}
$$</p>
<p>where $\gamma$ is a hyperparameter that controls the temerature of the softmax. Ideally, the retrieval likelihood is computed by marginalizing over all the documents in the corpus $\mathcal{D}$, which is intractable in practice. Therefore, we approximate the retrieval likelihood by only marginalizing over the retrieved documents $\mathcal{D}^{\prime}$.</p>
<h3>4.2 Computing LM likelihood</h3>
<p>We use the LM as a scoring function to measure how much each document could improve the LM perplexity. Specifically, we first compute $P_{L M}(y \mid d, x)$, the LM probability of the ground truth output $y$ given the input context $x$ and a document $d$. The higher the probability, the better the document $d_{i}$ is at improving the LM's perplexity. We then compute the LM likelihood of each document $d$ as follows:</p>
<p>$$
Q(d \mid x, y)=\frac{e^{P_{L M}(y \mid d, x) / \beta}}{\sum_{d \in \mathcal{D}^{\prime}} e^{P_{L M}(y \mid d, x) / \beta}}
$$</p>
<p>where $\beta$ is another hyperparameter.</p>
<h3>4.3 Loss Function</h3>
<p>Given the input context $x$ and the corresponding ground truth continuation $y$, we compute the retrieval likelihood and the language model likelihood. The dense retriever is trained by minimizing the KL divergence between these two distributions:</p>
<p>$$
\mathcal{L}=\frac{1}{|\mathcal{B}|} \sum_{x \in \mathcal{B}} K L\left(Q_{\mathrm{LM}}(d \mid x, y) | P_{R}(d \mid x)\right)
$$</p>
<p>where $\mathcal{B}$ is a set of input contexts. When minimizing the loss, we can only update the retrieval model parameters. The LM parameters are fixed due to our black-box assumption.</p>
<h3>4.4 Asynchronous Update of the Datastore Index</h3>
<p>Because the parameters in the retriever are updated during the training process, the previously computed document embeddings are no longer up to date. Therefore, following Guu et al. (2020), we recompute the document embeddings and rebuild the efficient search index using the new embeddings every $T$ training steps. Then we use the new document embeddings and index for retrieval, and repeat the training procedure.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: RePlug LSR training process (\$4). The retriever is trained using the output of a frozen language model as supervision signals.</p>
<h2>5 Training Setup</h2>
<p>In this section, we describe the details of our training procedure. We first describe the model setting in REPLUG ( $\$ 5.1$ ) and then describe the procedure for training the retriever in REPLUG LSR ( $\$ 5.2$ ).</p>
<h3>5.1 RePlug</h3>
<p>In theory, any type of retriever, either dense (Karpukhin et al., 2020; Ni et al., 2022) or sparse (Robertson et al., 2009), could be used for RePlug. Following prior work (Izacard et al., 2022b), we use the Contriever (Izacard et al., 2022a) as the retrieval model for REPLUG, as it has demonstrated strong performance.</p>
<h3>5.2 REPLUG LSR</h3>
<p>For REPLUG LSR, we initialize the retriever with the Contriever model (Izacard et al., 2022a). We use GPT-3 Curie (Brown et al., 2020) as the supervision LM to compute the LM likelihood.</p>
<p>Training data We use 800 K sequences of 256 tokens each, sampled from the Pile training data (Gao et al., 2021), as our training queries. Each query is split into two parts: the first 128 tokens are used as the input context $x$, and the last 128 tokens are used as the ground truth continuation $y$. For the external corpus $D$, we sample 36 M documents of 128 tokens from the Pile training data. To avoid trivial retrieval, we ensure that the external corpus documents do not overlap with the documents from which the training queries are sampled.</p>
<p>Training details To make the training process more efficient, we pre-compute the document embeddings of the external corpus $D$ and create a</p>
<p>FAISS index (Johnson et al., 2019) for fast similarity search. Given a query $x$, we retrieve the top 20 documents from the FAISS index and compute the retrieval likelihood and the LM likelihood with a temperature of 0.1 . We train the retriever using the Adam optimizer (Kingma and Ba, 2015) with a learning rate of $2 \mathrm{e}-5$, a batch size of 64 , and a warmup ratio of 0.1 . We re-compute the document embeddings every 3 k steps and fine-tune the retriever for a total of 25 k steps.</p>
<h2>6 Experiments</h2>
<p>We perform evaluations on both language modeling (\$6.1) and downstream tasks such as MMLU (\$6.2) and open-domain QA ( $\$ 6.3$ ). In all settings, REPlug improve the performance of various blackbox language models, showing the effectiveness and generality of our approach.</p>
<h3>6.1 Language Modeling</h3>
<p>Datasets The Pile (Gao et al., 2021) is a language modeling benchmark that consists of text sources from diverse domains such as web pages, code and academic papers. Following prior work, we report bits per UTF-8 encoded byte (BPB) as the metric on each subset domain.</p>
<p>Baselines We consider GPT-3 and GPT-2 family LMs as the baselines. The four models from GPT-3 (Davinci, Curie, Baddage and Ada) are black-box models that are only accessible through API.</p>
<p>Our model We add RePlug and RePlug LSR to the baselines. We randomly subsampled Pile training data ( 36 M documents of 128 tokens) and use them as the retrieval corpus for all models. As</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;"></th>
<th style="text-align: center;"># Parameters</th>
<th style="text-align: center;">Original</th>
<th style="text-align: center;">+ REPLUG</th>
<th style="text-align: center;">Gain \%</th>
<th style="text-align: center;">+ REPLUG LSR</th>
<th style="text-align: center;">Gain \%</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-2</td>
<td style="text-align: left;">Small</td>
<td style="text-align: center;">117 M</td>
<td style="text-align: center;">1.33</td>
<td style="text-align: center;">1.26</td>
<td style="text-align: center;">5.3</td>
<td style="text-align: center;">1.21</td>
<td style="text-align: center;">9.0</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Medium</td>
<td style="text-align: center;">345 M</td>
<td style="text-align: center;">1.20</td>
<td style="text-align: center;">1.14</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">1.11</td>
<td style="text-align: center;">7.5</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Large</td>
<td style="text-align: center;">774 M</td>
<td style="text-align: center;">1.19</td>
<td style="text-align: center;">1.15</td>
<td style="text-align: center;">3.4</td>
<td style="text-align: center;">1.09</td>
<td style="text-align: center;">8.4</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">XL</td>
<td style="text-align: center;">1.5 B</td>
<td style="text-align: center;">1.16</td>
<td style="text-align: center;">1.09</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">1.07</td>
<td style="text-align: center;">7.8</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3</td>
<td style="text-align: left;">Ada</td>
<td style="text-align: center;">350 M</td>
<td style="text-align: center;">1.05</td>
<td style="text-align: center;">0.98</td>
<td style="text-align: center;">6.7</td>
<td style="text-align: center;">0.96</td>
<td style="text-align: center;">8.6</td>
</tr>
<tr>
<td style="text-align: left;">(black-box)</td>
<td style="text-align: left;">Babbage</td>
<td style="text-align: center;">1.3 B</td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">5.3</td>
<td style="text-align: center;">0.88</td>
<td style="text-align: center;">7.4</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Curie</td>
<td style="text-align: center;">6.7 B</td>
<td style="text-align: center;">0.88</td>
<td style="text-align: center;">0.85</td>
<td style="text-align: center;">3.4</td>
<td style="text-align: center;">0.82</td>
<td style="text-align: center;">6.8</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Davinci</td>
<td style="text-align: center;">175 B</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">3.8</td>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;">6.3</td>
</tr>
</tbody>
</table>
<p>Table 1: Both RePlug and RePlug LSR consistently enhanced the performance of different language models. Bits per byte (BPB) of the Pile using GPT-3 and GPT-2 family models (Original) and their retrievalaugmented versions (+REPLUG and +REPLUG LSR. The gain \% shows the relative improvement of our models compared to the original language model.
the Pile dataset has made efforts to deduplicate documents across train, validation and test splits (Gao et al., 2021), we did not do additional filtering. For both RePlug and RePlug LSR, we use a length of 128 -token context to do retrieval and adopt the ensemble method (Section 3.2) to incorporate top 10 retrieved documents during inference.</p>
<p>Results Table 1 reports the results of the original baselines, baselines augmented with the REPLUG, and baselines augmented with the REPLUG LSR. We observe that both RePlug and RePlug LSR significantly outperform the baselines. This demonstrates that simply adding a retrieval module to a frozen language model (i.e., the black-box setting) is effective at improving the performance of different sized language models on language modeling tasks. Furthermore, REPLUG LSR consistently performs better than REPLUG by a large margin. Specifically, REPLUG LSR results in $7.7 \%$ improvement over baselines compared to $4.7 \%$ improvement of REPLUG averaged over the 8 models. This indicates that further adapting the retriever to the target LM is beneficial.</p>
<h3>6.2 MMLU</h3>
<p>Datasets MMLU (Hendrycks et al., 2021) is a multiple choice QA dataset that covers exam questions from 57 tasks including mathematics, US history and etc. The 57 tasks are grouped into 4 categories: humanities, STEM, social sciences and other. Following Chung et al. (2022a), we evaluate REPLUG in the 5-shot in-context learning setting.</p>
<p>Baselines We consider two groups of strong previous models as baselines for comparisons. The first group of baselines is the state-of-the-art LLMs including Codex $^{1}$ (Chen et al.,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>2021), PaLM (Chowdhery et al., 2022), and FlanPaLM (Chung et al., 2022b). According to Chung et al. (2022b), these three models rank top-3 in the leaderboard of MMLU. Additionally, we include strong open-source LMs such as LLaMA (Touvron et al., 2023). The second group of baselines consists of retrieval-augmented language models. We only include Atlas (Izacard et al., 2022b) in this group, as no other retrieval-augmented LMs have been evaluated on the MMLU dataset. Atlas trains both the retriever and the language model, which we consider a white-box retrieval LM setting.</p>
<p>Our model We add RePlug and RePlug LSR to Codex and LLaMA because other models such as PaLM and Flan-PaLM are not accessible to the public. We use the test question as the query to retrieve 10 relevant documents from Wikipedia (2018, December) and prepend each retrieved document to the test question, resulting in 10 separate inputs. These inputs are then separately fed into the language models, and the output probabilities are ensemble together. The retriever interacts with Codex and LLaMA through black-box access.</p>
<p>Results Table 2 presents the results from the baselines, REPLUG, and REPLUG LSR on the MMLU dataset. We observe that both the REPLUG and REPLUG LSR improve the original Codex model by $4.5 \%$ and $5.1 \%$, respectively. In addition, REPLUG LSR largely outperforms the previous retrievalaugmented language model, Atlas, demonstrating the effectiveness of our black-box retrieval language model setting. Although our models slightly underperform Flan-PaLM, this is still a strong result because Flan-PaLM has three times more parameters. We would expect that the REPLUG LSR could further improve Flan-PaLM, if we had access to the model.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;"># Parameters</th>
<th style="text-align: center;">Humanities</th>
<th style="text-align: center;">Social.</th>
<th style="text-align: center;">STEM</th>
<th style="text-align: center;">Other</th>
<th style="text-align: center;">All</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Codex</td>
<td style="text-align: left;">175B</td>
<td style="text-align: center;">74.2</td>
<td style="text-align: center;">76.9</td>
<td style="text-align: center;">57.8</td>
<td style="text-align: center;">70.1</td>
<td style="text-align: center;">68.3</td>
</tr>
<tr>
<td style="text-align: left;">PaLM</td>
<td style="text-align: left;">540B</td>
<td style="text-align: center;">77.0</td>
<td style="text-align: center;">81.0</td>
<td style="text-align: center;">55.6</td>
<td style="text-align: center;">69.6</td>
<td style="text-align: center;">69.3</td>
</tr>
<tr>
<td style="text-align: left;">Flan-PaLM</td>
<td style="text-align: left;">540B</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">72.2</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA</td>
<td style="text-align: left;">13B</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">55.6</td>
</tr>
<tr>
<td style="text-align: left;">Atlas</td>
<td style="text-align: left;">11B</td>
<td style="text-align: center;">46.1</td>
<td style="text-align: center;">54.6</td>
<td style="text-align: center;">38.8</td>
<td style="text-align: center;">52.8</td>
<td style="text-align: center;">47.9</td>
</tr>
<tr>
<td style="text-align: left;">Codex + RePlug</td>
<td style="text-align: left;">175B</td>
<td style="text-align: center;">76.0</td>
<td style="text-align: center;">79.7</td>
<td style="text-align: center;">58.8</td>
<td style="text-align: center;">72.1</td>
<td style="text-align: center;">71.4</td>
</tr>
<tr>
<td style="text-align: left;">Codex + RePlug LSR</td>
<td style="text-align: left;">175B</td>
<td style="text-align: center;">76.5</td>
<td style="text-align: center;">79.9</td>
<td style="text-align: center;">58.9</td>
<td style="text-align: center;">73.2</td>
<td style="text-align: center;">71.8</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA + RePlug</td>
<td style="text-align: left;">13B</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">58.8</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA + RePlug LSR</td>
<td style="text-align: left;">13B</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">59.3</td>
</tr>
</tbody>
</table>
<p>Table 2: RePlug and RePlug LSR improves Codex by $\mathbf{4 . 5 \%}$ and $\mathbf{5 . 1 \%}$ respectively. Performance on MMLU broken down into 4 categories. The last column averages the performance over these categories. All models are evaluated based on 5-shot in-context learning with direct prompting.</p>
<p>Another interesting observation is that the REPlug LSR outperforms the original model by $1.9 \%$ even in the STEM category. This suggests that retrieval may improve a language model's problem-solving abilities.</p>
<h3>6.3 Open Domain QA</h3>
<p>Lastly, we conduct evaluation on two opendomain QA datasets: Natural Questions (NQ) (Kwiatkowski et al., 2019) and TriviaQA (Joshi et al., 2017).</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">NQ</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">TQA</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Model</td>
<td style="text-align: center;">k-shot</td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">k-shot</td>
<td style="text-align: center;">Full</td>
</tr>
<tr>
<td style="text-align: left;">Chinchilla</td>
<td style="text-align: center;">35.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">64.6</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">PaLM</td>
<td style="text-align: center;">39.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Codex</td>
<td style="text-align: center;">40.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">73.6</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA</td>
<td style="text-align: center;">29.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">69.6</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">RETRO $^{\dagger}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">45.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">R2-D2 ${ }^{\dagger}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">55.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">69.9</td>
</tr>
<tr>
<td style="text-align: left;">Atlas $^{\dagger}$</td>
<td style="text-align: center;">30.9</td>
<td style="text-align: center;">$\mathbf{6 0 . 4}$</td>
<td style="text-align: center;">74.5</td>
<td style="text-align: center;">$\mathbf{7 9 . 8}$</td>
</tr>
<tr>
<td style="text-align: left;">Codex + RePlug</td>
<td style="text-align: center;">44.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">76.8</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Codex + RePlug LSR</td>
<td style="text-align: center;">$\mathbf{4 5 . 5}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\mathbf{7 7 . 3}$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA + RePlug</td>
<td style="text-align: center;">36.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">73.3</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA + RePlug LSR</td>
<td style="text-align: center;">37.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">74.1</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 3: Performance on NQ and TQA. We report results for both k-shot ( 64 shots for Chinchilla, PaLM, and Atlas; 16 shots for Codex-based models) and full data settings. Note that models with $\dagger$ are finetuned using training examples, while others use in-context learning.</p>
<p>Datasets NQ and TriviaQA are two open-domain QA datasets. Following prior work (Izacard and Grave, 2021; Si et al., 2023), we report Exact Match for the filtered set of TriviaQA. We consider the k -shot setting where the model is only given a few training examples and full data setting where the model is given all the training examples.</p>
<p>Baselines We compare our model with several state-of-the-art baselines, both in a few-shot set- ting and with full training data. The first group of models consists of powerful large language models, including Chinchilla (Hoffmann et al., 2022), PaLM (Chowdhery et al., 2022), Codex and LLaMA 13B (Touvron et al., 2023). These models are all evaluated using in-context learning under the few-shot setting, with Chinchilla and PaLM evaluated using 64 shots, and Codex using 16 shots. The second group of models for comparison includes retrieval-augmented language models such as RETRO (Borgeaud et al., 2022), R2-D2 (Fajcik et al., 2021), and Atlas (Izacard et al., 2022b). All of these retrieval-augmented models are finetuned on the training data, either in a few-shot setting or with full training data. Specifically, Atlas is finetuned on 64 examples in the few-shot setting.</p>
<p>Our model We add RePlug and RePlug LSR to Codex and LLaMA 13B with Wikipedia as the retrieval corpus and evaluate them in a 16-shot in context learning. We incorporate top-10 retrieved documents using our proposed ensemble method.
Results As shown in Table 3, RePlug LSR significantly improves the performance of the original Codex by $12.0 \%$ on NQ and $5.0 \%$ on TQA. It outperforms the previous best model, Atlas, which was fine-tuned with 64 training examples, achieving a new state-of-the-art in the few-shot setting. However, this result still lags behind the performance of retrieval-augmented language models fine-tuned on the full training data. This is likely due to the presence of near-duplicate test questions in the training set (e.g., Lewis et al. (2021) found that $32.5 \%$ of test questions overlap with the training sets in NQ).</p>
<h2>7 Analysis</h2>
<h3>7.1 REPLUG is applicable to diverse models</h3>
<p>Here we further study whether RePlug could enhance diverse language model families that have</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: GPT-2, BLOOM and OPT models of varying sizes consistently benefit from REPLUG. The x-axis indicates the size of the language model and the y-axis is its perplexity on Wikitext-103.</p>
<p>been pre-trained using different data and methods. Specifically, we focus on three groups of language models with varying sizes: GPT-2 (117M, 345M, 774M, 1.5B parameters) (Brown et al., 2020), OPT (125M, 350M, 1.3B, 2.7B, 6.7B, 13B, 30B, 66B) (Zhang et al., 2022) and BLOOM (560M, 1.1B, 1.7B, 3B and 7B) (Scao et al., 2022). We evaluate each model on Wikitext-103 (Merity et al., 2017) test data and report its perplexity. For comparison, we augment each model with REPLUG that adopts the ensemble method to incorporate top 10 retrieved documents. Following prior work (Khandelwal et al., 2020), we use Wikitext-103 training data as the retrieval corpus.</p>
<p>Figure 4 shows the performance of differentsized LMs with and without REPLUG. We observe that the performance gain brought by REPLUG stays consistent with model size. For example, OPT-125M achieves 6.9% perplexity improvement, while OPT-66B achieves 5.6% perplexity improvement. Additionally, REPLUG improves the perplexity of all the model families, which indicates that REPLUG is applicable to diverse language models with different sizes.</p>
<h3>7.2 REPLUG performance gain does not simply come from the ensembling effect</h3>
<p>The core of our method design is the use of an ensemble method that combines output probabilities of different passes, in which each retrieved document is prepended separately to the input and fed into a language model. To study whether the gains come solely from the ensemble method, we compare our method to ensembling random documents. For this, we randomly sample several documents, concatenated each random document with the input, and ensemble the outputs of different runs (referred to as "random"). As shown in Figure 5, we evaluated the performance of GPT-3 Curie on Pile when augmented with random documents, documents retrieved by REPLUG, and documents retrieved by REPLUG LSR. We observed that ensembling random documents leads to worse performance, indicating that the performance gains of REPLUG do not come from the ensembling effect. Instead, ensembling the <strong>relevant</strong> documents is crucial for the success of REPLUG. Additionally, as more documents were ensembled, the performance of REPLUG and REPLUG LSR improved monotonically. However, a small number of documents (e.g., 10) was sufficient to achieve large performance gains.</p>
<h3>7.3 LSR retriever outperforms other off-the-shelf retrievers</h3>
<p>We investigate the effectiveness of tunable retriever (LSR) compared with off-the-shelf retrievers. Specifically, we compare LM-supervised contriever (LSR) with other dense retrievers such as BERT-base (Borgeaud et al., 2022), DPR (Karpukhin et al., 2020) and a sparse retriever BM25 (Robertson et al., 2009). Figure 6 shows Wikitext-103 perplexity of GPT-2 XL (1.5B) and GPT-2 Large (774M) augmented with different retrievers. Among all off-the-shelf retrievers, the sparse retriever BM25 performs best. However, it still lags behind our LM supervised retriever (Contriever LSR), demonstrating the effectiveness of our training scheme that adapts the retriever to LMs.</p>
<h2>8 Conclusion</h2>
<p>We introduce REPLUG, a retrieval-augmented LM paradigm that augments black-box LMs with a tuneable retriever. This work opens up new possibilities for integrating retrieval into large black-box</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Ensembling random documents does not result in improved performance. BPB of Curie augmented with different methods (random, REPLUG and REPLUG LSR) when varying the number of documents.</p>
<p>LMs and is the first to demonstrate even the state-of-the-art LLMs could benefit from retrieval.</p>
<h2>9 Limitations</h2>
<p><strong>Interpretability</strong> REPLUG exhibits limitations in interpretability. It's unclear when the model relies on retrieved knowledge or on knowledge encoded within its own parameters. Future research could work towards the development of more interpretable retrieval-augmented language models. Such models could trace the source of the generated answers, whether it's from retrieved data or internal parameters, thus providing a clear knowledge provenance.</p>
<p><strong>On-demand retrieval</strong> REPLUG always perform retrieval no matter if the external information is needed. This approach runs the risk of presenting irrelevant documents, which can potentially distract the models, while also incurring additional computational overheads. Future studies could explore methods that allow the language model to determine when external knowledge is required.</p>
<p><strong>Database size</strong> In line with prior research, REPLUG uses Wikipedia and Pile as the targeted search databases. However, these resources might only encompass a minor fraction of the external knowledge needed by LMs. Future research should explore methods to efficiently expand these databases and examine how an LM's performance scales with the size of the database.</p>
<h2>References</h2>
<p>Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: LM-supervised retriever (Contriever LSR) outperforms other off-the-shelf retrievers.</p>
<p>Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre. 2022. Improving language models by retrieving from trillions of tokens. In <em>International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA</em>, volume 162 of <em>Proceedings of Machine Learning Research</em>, pages 22062240. PMLR.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In <em>Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual</em>.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya</p>
<p>Sutskever, and Wojciech Zaremba. 2021. Evaluating large language models trained on code. ArXiv preprint, abs/2107.03374.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. ArXiv preprint, abs/2204.02311.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022a. Scaling instruction-finetuned language models. ArXiv preprint, abs/2210.11416.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022b. Scaling instruction-finetuned language models. ArXiv preprint, abs/2210.11416.</p>
<p>Martin Fajcik, Martin Docekal, Karel Ondrej, and Pavel Smrz. 2021. R2-D2: A modular baseline for opendomain question answering. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 854-870, Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2021. The Pile: An 800gb dataset of diverse text for language modeling. ArXiv preprint, abs/2101.00027.</p>
<p>Anirudh Goyal, Abram L. Friesen, Andrea Banino, Theophane Weber, Nan Rosemary Ke, Adri Puigdomnech Badia, Arthur Guez, Mehdi Mirza, Peter C. Humphreys, Ksenia Konyushkova, Michal Valko, Simon Osindero, Timothy P. Lillicrap, Nicolas Heess, and Charles Blundell. 2022. Retrieval-augmented reinforcement learning. In International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 7740-7765. PMLR.</p>
<p>Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020. Retrieval augmented language model pre-training. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 3929-3938. PMLR.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.</p>
<p>Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. 2022. Training compute-optimal large language models. ArXiv preprint, abs/2203.15556.</p>
<p>Yushi Hu, Hang Hua, Zhengyuan Yang, Weijia Shi, Noah A Smith, and Jiebo Luo. 2022. Promptcap: Prompt-guided task-aware image captioning. ArXiv preprint, abs/2211.09699.</p>
<p>Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2022a. Unsupervised dense information retrieval with contrastive learning. Transactions on Machine Learning Research.</p>
<p>Gautier Izacard and Edouard Grave. 2021. Leveraging passage retrieval with generative models for open domain question answering. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 874-880, Online. Association for Computational Linguistics.</p>
<p>Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 2022b. Few-shot learning with retrieval augmented language models. ArXiv preprint, abs/2208.03299.</p>
<p>Jeff Johnson, Matthijs Douze, and Herv Jgou. 2019. Billion-scale similarity search with gpus. IEEE Transactions on Big Data, 7(3):535-547.</p>
<p>Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1601-1611, Vancouver, Canada. Association for Computational Linguistics.</p>
<p>Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for opendomain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6769-6781, Online. Association for Computational Linguistics.</p>
<p>Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2020. Generalization through memorization: Nearest neighbor language models. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.</p>
<p>Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.</p>
<p>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452-466.</p>
<p>Patrick Lewis, Pontus Stenetorp, and Sebastian Riedel. 2021. Question and answer test-train overlap in opendomain question answering datasets. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 1000-1008, Online. Association for Computational Linguistics.</p>
<p>Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kttler, Mike Lewis, Wen-tau Yih, Tim Rocktschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for knowledge-intensive NLP tasks. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.</p>
<p>Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Hannaneh Hajishirzi, and Daniel Khashabi. 2022. When not to trust language models: Investigating effectiveness and limitations of parametric and non-parametric memories. ArXiv preprint, abs/2212.10511.</p>
<p>Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2017. Pointer sentinel mixture models. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net.</p>
<p>Sewon Min, Weijia Shi, Mike Lewis, Xilun Chen, Wentau Yih, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Nonparametric masked language modeling. ArXiv preprint, abs/2212.01349.</p>
<p>Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernandez Abrego, Ji Ma, Vincent Zhao, Yi Luan, Keith Hall, Ming-Wei Chang, and Yinfei Yang. 2022. Large dual encoders are generalizable retrievers. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 9844-9855, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu, and Haifeng Wang. 2021. RocketQA: An optimized training approach to dense passage retrieval for opendomain question answering. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5835-5847, Online. Association for Computational Linguistics.</p>
<p>Stephen Robertson, Hugo Zaragoza, et al. 2009. The probabilistic relevance framework: BM25 and beyond. Foundations and Trends ${ }^{\circledR}$ in Information Retrieval, 3(4):333-389.</p>
<p>Devendra Singh Sachan, Mike Lewis, Dani Yogatama, Luke Zettlemoyer, Joelle Pineau, and Manzil Zaheer. 2023. Questions are all you need to train a dense passage retriever. Transactions of the Association for Computational Linguistics, 11:600-616.</p>
<p>Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili, Daniel Hesslow, Roman Castagn, Alexandra Sasha Luccioni, Franois Yvon, Matthias Gall, et al. 2022. Bloom: A 176bparameter open-access multilingual language model. ArXiv preprint, abs/2211.05100.</p>
<p>Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan Boyd-Graber, and Lijuan Wang. 2023. Prompting GPT-3 to be reliable. In Proc. of ICLR.</p>
<p>Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen-tau Yih, Noah A. Smith, Luke Zettlemoyer, and Tao Yu. 2023. One embedder, any task: Instruction-finetuned text embeddings. In Findings of the Association for Computational Linguistics: ACL 2023, pages 1102-1121, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and finetuned chat models.</p>
<p>Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Richard James, Jure Leskovec, Percy Liang, Mike Lewis, Luke Zettlemoyer, and Wen-Tau Yih. 2023. Retrieval-augmented multimodal language modeling. In Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 39755-39769. PMLR.</p>
<p>Tim Dettmers Younes Belkda. 2022. A gentle introduction to 8-bit matrix multiplication.</p>
<p>Wenhao Yu. 2022. Retrieval-augmented generation across heterogeneous knowledge. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Student Research Workshop, pages 52-58, Hybrid: Seattle, Washington + Online. Association for Computational Linguistics.</p>
<p>Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer language models. ArXiv preprint, abs/2205.01068.</p>
<p>Zexuan Zhong, Tao Lei, and Danqi Chen. 2022. Training language models with memory augmentation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5657-5673, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<h2>A Qualitative Analysis: Rare Entities Benefit from Retrieval</h2>
<p>To understand why the REPLUG improves language modeling performance, we conducted manual analysis of examples in which the REPLUG results in a decrease in perplexity. We find that REPLUG is more helpful when texts contain rare entities. Figure 7 shows a test context and its continuation from the Wikitext-103 test set. For REPLUG, we use the test context as a query to retrieve a relevant document from Wikitext-103 training data. We then compute the perplexity of the continuation using the original GPT-2 1.5B and its REPLUG enhanced version. After incorporating the retrieved document, the perplexity of the continuation improves by $11 \%$. Among all tokens in the continuation, we found that REPLUG is most helpful for the rare entity name "Li Bai". This is likely because the original LM does not have sufficient information about this rare entity name. However, by incorporating the retrieved document, REPLUG was able to match the name with the relevant information in the retrieved document, resulting in better performance.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Rare entities benefit from retrieval. After incorporating the retrieved document during inference, the entity "Li Bai" and the token "greatest" in the continuation show the most improvement in perplexity ( $15 \%$ for "Li Bai" and 5\% for "greatest"). Other tokens' perplexity changes are within $5 \%$.</p>
<h2>B Dense Retriever vs. Sparse Retriever</h2>
<p>The proposed model uses Contriever, a dense retriever, as its retriever backbone. Additionally, we investigate the performance of a sparse retriever in comparison to the dense retriever. For our sparse model, we employ BM25. As depicted in Figure 8, we observe that BM25 consistently outperforms Contriever but falls short when compared to LMsupervised Contriever, thus highlighting the effec-
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: PPL of GPT-2 models on Witext-103 with no retrieval (Origin), Contriever (REPLUG), LMsupervised Contriever (REPLUG LSR) and BM25.
tiveness of our proposed training scheme.</p>
<h2>C Prompts used for MMLU and open-domain QA</h2>
<p>Please see Table 4 and Table 5.</p>
<p>Knowledge: Arctic Ocean. Although over half of Europe's original forests disappeared through the centuries of deforestation, Europe still has over one quarter of its land area as forest, such as the broadleaf and mixed forests, taiga of Scandinavia and Russia, mixed rainforests of the Caucasus and the Cork oak forests in the western Mediterranean. During recent times, deforestation has been slowed and many trees have been planted. However, in many cases monoculture plantations of conifers have replaced the original mixed natural forest, because these grow quicker. The plantations now cover vast areas of land, but offer poorer habitats for many European
Question: As of 2015, since 1990 forests have $\qquad$ in Europe and have $\qquad$ in Africa and the Americas.
A. "increased, increased" B. "increased, decreased" C. "decreased, increased" D. "decreased, decreased"</p>
<p>Answer: B
Knowledge: Over the past decades, the political outlook of Americans has become more progressive, with those below the age of thirty being considerably more liberal than the overall population. According to recent polls, $56 \%$ of those age 18 to 29 favor gay marriage, $68 \%$ state environmental protection to be as important as job creation, $52 \%$ "think immigrants strengthen the country with their hard work and talents, ${ }^{4} 62 \%$ favor a "tax financed, government-administrated universal health care" program and 74\% "say people willhould have more influence on U.S. laws than the Bible, compared to $37 \%, 49 \%, 38 \%, 47 \%$ and $58 \%$ among the Question: As of 2019, about what percentage of Americans agree that the state is run for the benefit of all the people?
A. $31 \%$ B. $46 \%$ C. $61 \%$ D. $76 \%$</p>
<p>Answer: B
Knowledge: last week at a United Nations climate meeting in Germany, China and India should easily exceed the targets they set for themselves in the 2015 Paris Agreement... India is now expected to obtain 40 percent of its electricity from non-fossil fuel sources by 2022, eight years ahead of schedule." Solar power in Japan has been expanding since the late 1990s. By the end of 2017, cumulative installed PV capacity reached over 50 GW with nearly 8 GW installed in the year 2017. The country is a leading manufacturer of solar panels and is in the top 4 ranking for countries
Question: Which of the following countries generated the most total energy from solar sources in 2019?
A. China B. United States C. Germany D. Japan</p>
<p>Table 4: Prompt for MMLU</p>
<p>Knowledge: received 122,000 buys (excluding WWE Network views), down from the previous year 199,000 buys. The event is named after the Money In The Bank ladder match, in which multiple wrestlers use ladders to retrieve a briefcase hanging above the ring. The winner is guaranteed a match for the WWE World Heavyweight Championship at a time of their choosing within the next year. On the June 2 episode of "Raw", Alberto Del Rio qualified for the match by defeating Dolph Ziggler. The following week, following Daniel Bryan being stripped of his WWE World Championship due to injury, Stephanie McMahon changed the
Question: Who won the mens money in the bank match?
Answer: Braun Strowman
Knowledge: in 3D on March 17, 2017. The first official presentation of the film took place at Disney three-day D23 Expo in August 2015. The world premiere of "Beauty and the Beast" took place at Spencer House in London, England on February 23, 2017; and the film later premiered at the El Capitan Theatre in Hollywood, California, on March 2, 2017. The stream was broadcast onto YouTube. A sing along version of the film released in over 1,200 US theaters nationwide on April 7, 2017. The United Kingdom received the same version on April 21, 2017. The film was re-released in
Question: When does beaty and the beast take place
Answer: Rococo-era
Knowledge: Love Yourself "Love Yourself" is a song recorded by Canadian singer Justin Bieber for his fourth studio album "Purpose" (2015). The song was released first as a promotional single on November 8, 2015, and later was released as the album third single. It was written by Ed Sheeran, Benny Blanco and Bieber, and produced by Blanco. An acoustic pop song, "Love Yourself" features an electric guitar and a brief flurry of trumpets as its main instrumentation. During the song, Bieber uses a husky tone in the lower registers. Lyrically, the song is a kiss-off to a narcissistic ex-lover who did
Question: love yourself by justin bieber is about who</p>
<p>Table 5: Prompt for open-domain QA</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Code-Davinci-002&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
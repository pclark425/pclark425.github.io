<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6822 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6822</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6822</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-131.html">extraction-schema-131</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <p><strong>Paper ID:</strong> paper-276421927</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.12289v3.pdf" target="_blank">Evaluating Step-by-step Reasoning Traces: A Survey</a></p>
                <p><strong>Paper Abstract:</strong> Step-by-step reasoning is widely used to enhance the reasoning ability of large language models (LLMs) in complex problems. Evaluating the quality of reasoning traces is crucial for understanding and improving LLM reasoning. However, existing evaluation practices are highly inconsistent, resulting in fragmented progress across evaluator design and benchmark development. To address this gap, this survey provides a comprehensive overview of step-by-step reasoning evaluation, proposing a taxonomy of evaluation criteria with four top-level categories (factuality, validity, coherence, and utility). Based on the taxonomy, we review different datasets, evaluator implementations, and recent findings, leading to promising directions for future research.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6822.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6822.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique where LLMs are prompted to generate intermediate natural-language reasoning steps before producing a final answer, used to improve performance on complex symbolic and arithmetic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various LLMs (e.g., GPT-4, PaLM, Qwen-2.5-7B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer language models pre-trained on large web/text corpora; used with prompting to generate step-by-step natural language traces.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer (autoregressive) with prompting</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Chain-of-thought prompting (explicit stepwise natural-language trace generation)</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Math/GSM8k, MATH, various symbolic/math benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Arithmetic and broader mathematical reasoning benchmarks where stepwise traces are useful (word problems, competition math).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>mathematical / symbolic reasoning, arithmetic word problems</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>final-answer accuracy; trace-level validity/utility</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>CoT shown in cited literature to substantially improve reasoning on math/symbolic tasks compared to direct-answer prompting; survey reiterates CoT as core enabler of stepwise reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Chain-of-thought prompting expanded LLM capabilities on symbolic reasoning tasks; it is foundational for many downstream evaluator and verifier methods.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>CoT does not guarantee faithful/valid reasoning traces (models can produce unfaithful or invalid steps even when final answer is correct); trace quality must be evaluated separately.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Step-by-step Reasoning Traces: A Survey', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6822.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6822.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ToT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree-of-Thoughts (ToT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A search-based reasoning framework that explores multiple partial reasoning paths (a tree of intermediate steps) and uses scoring/evaluators to select promising branches, enabling backtracking and deliberation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tree of thoughts</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs used as step generators/evaluators (various)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer LLMs generate candidate next steps; an outer search procedure maintains multiple paths and prunes/expands using evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer + external tree search (planning over generated steps)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Tree search over generated partial traces (deliberative exploration with pruning and evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Arithmetic, Game-of-24, other symbolic/exploration tasks</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Tasks requiring exploration and backtracking in large solution spaces (e.g., number-composition puzzles).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>exploratory algorithmic/symbolic reasoning with backtracking</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>final-answer accuracy / success rate after search</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Survey reports ToT among popular verifier-guided search methods; ToT enables stronger exploration than single-chain decoding and can improve success when paired with reliable evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Tree-of-Thoughts provides an explicit mechanism for exploration and backtracking and benefits strongly from good evaluators; used for arithmetic and combinatorial tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Computationally expensive (scales with branching and depth); requires robust evaluators to choose branches, and vulnerable to evaluator weaknesses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Step-by-step Reasoning Traces: A Survey', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6822.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6822.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Verifier-guided search</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Verifier-guided search (Best-of-N / tree search)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Approaches that generate multiple reasoning traces (or expand a search tree) and use an external evaluator/verifier to select the most promising trace, improving final-answer accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various LLMs (used as generators) with separate evaluators/critics</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Base LLMs autoregressively sample many candidate traces; separate evaluator models or majority-vote select the final trace/answer.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer for generation; separate transformer or classifier for evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Best-of-N decoding, tree search, and selection using evaluator scores (verifier-guided selection)</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ProcessBench (GSM8k+MATH subsets used by survey), other math/ reasoning benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Meta-evaluation benchmarks and downstream reasoning tasks used to measure improvement when using verifiers to select traces.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>mathematical and multi-step reasoning tasks where multiple traces are sampled</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>final-answer accuracy; classification accuracy of evaluator in meta-evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Survey notes tradeoff: for weaker evaluators majority-voting across samples often outperforms verifier-guided selection; with stronger evaluators (better training or stronger critic models), verifier-guided Best-of-N can outperform majority voting at same compute budget.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Verifier-guided search can substantially improve downstream accuracy if evaluators are strong; scaling either exploration (more samples) or evaluation (stronger evaluators/longer outputs) matters and the optimal allocation depends on evaluator strength.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Effectiveness depends on evaluator robustness; vulnerable to evaluator biases/spurious features (e.g., step length), and computational cost increases with N and evaluator complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Step-by-step Reasoning Traces: A Survey', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6822.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6822.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MCTS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Monte Carlo Tree Search (MCTS) for utility estimation and search</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of MCTS to estimate step-level utility (value function) by sampling rollouts from a candidate step and using rewards (e.g., final-answer correctness) to assign utility; also used to guide search.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLM rollouts (various)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLMs generate rollouts from nodes/steps; MCTS aggregates stochastic rollouts to estimate expected reward/value.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer for rollouts plus MCTS planning layer</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>MCTS rollouts for step evaluation and for constructing step-level utility labels for training evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Math-Shepherd, Game-of-24, other math/process benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Benchmarks where utility can be measured by sampling continuations and checking final-answer correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>utility estimation for multi-step reasoning; search-guided solution finding</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>step-level utility (expected reward), final-answer accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>MCTS-derived utility labels are cheaper than human validity labels and have been used to scale training data for process reward models; survey notes MCTS enables scalable labeling but may not fully capture factuality/validity due to unfaithful traces.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>MCTS provides a scalable way to label utility and train evaluators or guide search; frequently used to train process reward models and to compute step advantage.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Computationally heavy; utility labels do not reliably substitute for validity/factuality due to unfaithful reasoning (traces that reach correct answer despite invalid steps).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Step-by-step Reasoning Traces: A Survey', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6822.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6822.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Critic</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-as-a-judge / Critic models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using LLMs themselves (with prompting and/or fine-tuning) to evaluate reasoning traces across factuality, validity, coherence, and utility, optionally producing natural-language rationales before assigning scores.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>large reasoning-capable LLMs (various; e.g., GPT-family, Qwen)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large autoregressive LLMs used in an evaluation role, often prompted to produce step-level critique/rationale and then produce a score.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer (critic usage, generative evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Prompted critique (generate rationale then predict score); can be used zero-shot or fine-tuned as a critic</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>CriticBench, ProcessBench, PRM800k (used for training/eval)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Meta-evaluation benchmarks for assessing critic accuracy on step-level labels across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>step-level evaluation (factuality, validity, coherence, utility)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>classification accuracy / F1 on meta-evaluation labels; improvement in downstream task accuracy when used in Best-of-N</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Survey reports critics are versatile and can evaluate all four criteria; they can outperform simpler models if allowed to generate rationales and use test-time scaling, but are computationally more expensive than sequence classifiers/cross-encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Critic models are flexible, work with closed-source LLMs, and can be effective without fine-tuning; they particularly benefit from test-time scaling (majority voting over samples) and partial-context techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>High runtime and compute; generation of rationales is expensive; limited by context length when traces are long; can still make mistakes in distinguishing factual vs logical errors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Step-by-step Reasoning Traces: A Survey', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6822.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6822.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SeqClass</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sequence classifiers / Process (reward) models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Lightweight models (LMs with small classification heads) trained to assign numeric scores to entire traces or individual steps, used as reward models or fast evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Fine-tuned sequence classifiers (various research implementations)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer encoder/decoder LMs with a classification/regression head fine-tuned on step-level labels (e.g., PRM800k) to predict quality scores or categorical labels.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer + classification head (fine-tuned discriminative)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>PRM800k and derivatives, MCTS-derived utility labels, LLM-perturbed traces (as reported in survey)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Supervised classification/regression on human- or automatically-labeled step-level quality labels (valid/invalid/useful/etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>PRM800k, ProcessBench, VersaPRM, Qwen-PRM (survey-listed datasets used for training/eval)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Large crowdsourced or synthetic datasets with step-level labels used to train/evaluate sequence classifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>step-level validity/utility classification; trace-level reward prediction</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>classification accuracy on step labels; effectiveness in Best-of-N selection when used as evaluator</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Sequence classifiers trained with combined validity+utility positives outperform classifiers trained on only one criterion (survey cites Zhang et al., 2025 results qualitatively).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Sequence classifiers are efficient at evaluation and can be strong if trained on high-quality multi-criteria data; they are commonly used as reward models in RL pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Lose generative trace-capability (cannot produce rationales), require large labeled datasets for training, may overfit labeling artifacts, and are limited when traces exceed context length.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Step-by-step Reasoning Traces: A Survey', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6822.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6822.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-VF</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-as-a-value-function / Generative reward models (DPO/Step-DPO)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Methods that align or fine-tune LLM token probabilities to correspond to rewards (e.g., answer correctness) using preference-learning objectives like DPO, enabling the model itself to assign value to traces via token probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs fine-tuned with preference objectives (e.g., DPO/GRPO variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive LLMs fine-tuned with preference/ reward objectives so output token probabilities reflect evaluator-derived rewards; can generate traces and produce internal value signals.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer fine-tuned with preference optimization algorithms (DPO, GRPO, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Preference pairs (human or LLM-generated), rewards from MCTS or final-answer correctness, PRM-like datasets</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Preference learning / direct preference optimization to align token probabilities with desired process/outcome rewards; can be used for RL-style training.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Math-Shepherd, GenRM, MCTS-DPO variants (survey cites several works)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Math and logic datasets used to train/evaluate value-function-style LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>process/outcome reward modeling; improving long-chain reasoning via RL-style fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>final-answer accuracy after RL/fine-tuning; improvement in downstream tasks when used as reward model</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Survey indicates LLM-as-value-function approaches can retain generation ability while aligning rewards, and focus chiefly on utility because utility labels scale easily; they require many good/bad traces for training.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Generative reward models allow direct use of LLMs as value functions and can be effective for training improved reasoners; they are commonly used in recent RL pipelines for reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Require large, high-quality preference data; risk of reward hacking if evaluator is sensitive to spurious features (e.g., length); primarily capture utility rather than factuality or fine-grained logical validity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Step-by-step Reasoning Traces: A Survey', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6822.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6822.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PartialContext / PARC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Partial-context evaluation (premise selection; PARC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluate a step using only a selected subset of prior steps and query premises (an entailment graph), improving efficiency and removing distractors to increase evaluator accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PARC (method implemented with critic models in survey experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Combines premise-selection (entailment graph construction) with critic evaluation applied only to identified relevant premises, reducing context length per evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer critic operating on reduced premise subset; entailment-graph construction module</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Premise identification + partial-context evaluation to judge validity/coherence efficiently</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ProcessBench (GSM8k+MATH subsets for evaluation in survey)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Meta-evaluation benchmark used to compare partial-context vs full-context evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>validity and coherence evaluation for step-level reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>meta-evaluation classification accuracy; compute cost per trace</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>PARC (partial context) reduces compute and in reported experiments improved accuracy relative to full-context critics by removing distractors; average premises per step reported as ~1.57 leading to lower compute.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Partial-context evaluation is both more computationally efficient and can be more accurate because it reduces distracting context; it also helps distinguish direct vs accumulated errors.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires a reliable premise-selection method; premise discovery may be challenging and failure-prone for complex traces.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Step-by-step Reasoning Traces: A Survey', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6822.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6822.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Symbol-grounded</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Symbol-grounded / neuro-symbolic evaluation and reasoning (Logic-LM, LINC, LeanDojo etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Approaches that align natural-language reasoning traces with symbolic representations or integrate LLMs with symbolic solvers/theorem provers to obtain formally-checkable reasoning steps.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Neuro-symbolic hybrids (Logic-LM, LINC, LeanDojo examples cited)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Combinations of transformer LMs with symbolic modules (first-order logic provers, theorem provers like Lean or Isabelle) used for faithful logical deduction and verification.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer + external symbolic solver / theorem prover</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>formal logic corpora, theorem-proving corpora (where available); not specified in survey</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Parsing/translation of NL steps into symbolic representations then using symbolic solvers to perform/verify deductions or proofs</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Integration with interactive theorem provers (Lean, Isabelle) or first-order logic provers to verify or produce formal proofs; survey notes these are underexplored for trace evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>FOLIO / P-FOLIO, formal theorem proving datasets, LeanDojo/interactive theorem proving benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Benchmarks for first-order logic and formal theorem proving with human-written reasoning chains / formal proofs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>first-order theorem proving, formal proof generation/verification, deductive logical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>proof correctness / exact proof match / theorem-prover success</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Survey notes limited prior work; symbolic grounding offers precise definitions for validity and utility but few methods extend rule-based parsing to complex first-order logic and interactive theorem proving.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Symbol-grounded approaches provide formal evaluation opportunities and precise validity checks but are not yet widely applied to complex NL traces; promising direction for rigorous logical reasoning evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Parsing natural language to formal logic is hard; limited prior work generalizing to realistic complex tasks; requires integration with external theorem provers and expensive engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Step-by-step Reasoning Traces: A Survey', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6822.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e6822.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Rule-based matching</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rule-based / symbolic matching evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Deterministic evaluators that parse traces into symbolic graphs or computation graphs and check edges/operations against a ground-truth symbolic solution, effective when symbolic representations exist.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>rule-based parsers/evaluators (task-specific)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Systems that map NL traces to directed graphs of entities/operations and perform exact matching to ground-truth relations or computation graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>parser + deterministic symbolic matcher (rule-based)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Exact symbolic parsing and matching to a gold solution graph for factuality/coherence/utility evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Arithmetic/knowledge-graph-style tasks where ground-truth symbolic solutions exist</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Tasks with clear symbolic representations such as computation graphs for arithmetic or knowledge-graph multi-hop QA.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>symbolic evaluation: factuality/coherence/utility via graph matching</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>exact-match rates for edges/steps; ability to detect incorrect steps</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Rule-based matching performs well when symbolic ground truth exists but does not generalize to commonsense or complex math beyond simple arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Highly precise where applicable; useful for diagnostics in tasks with explicit symbolic structure.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Does not generalize to tasks without explicit symbolic form; brittle to linguistic variability; requires robust parsing.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Step-by-step Reasoning Traces: A Survey', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6822.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e6822.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen-2.5-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen-2.5-7B-Instruct (as used in survey experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 2.5B-parameter variant in the Qwen family used by the survey authors as a common base model for controlled comparisons in Figure 4 (base model for many evaluator comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen-2.5-7B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned transformer language model (Qwen family) used as base generator and baseline for several evaluator compute/performance estimates in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>2.5B</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer (instruct-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Used as base generator for Chain-of-Thought traces and as reference for evaluator comparisons (figure experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ProcessBench (GSM8k + MATH subsets used in Figure 4)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Meta-evaluation benchmark subset used for controlled comparisons of evaluators in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>mathematical reasoning / trace evaluation baseline</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>ProcessBench performance (aggregated) vs compute in Figure 4</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Survey uses Qwen-2.5-7B as the shared base to compare evaluator implementations; specific numeric scores are reported in cited sources but not tabulated in the survey text.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Serves as a controlled baseline for comparing compute vs evaluator effectiveness; used to illustrate tradeoffs like majority voting vs verifier-guided selection.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Base model capability constrains absolute evaluation performance; comparisons across papers are confounded by differing base models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Step-by-step Reasoning Traces: A Survey', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6822.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e6822.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (illustrative mention)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>State-of-the-art closed-source LLM cited as achieving strong performance on challenging MHQA benchmarks, but still susceptible to errors in long-evidence adherence and temporal reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large proprietary transformer-based model by OpenAI with strong multi-task and reasoning capabilities referenced in survey examples.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer (closed-source large-scale)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Used with Chain-of-Thought and other prompting/verifier methods in literature; cited as strong baseline for MHQA and other reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ARC-Challenge, PIQA, MHQA datasets</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Hard multi-hop and commonsense QA datasets where top LLMs show strong but imperfect performance.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-hop question answering, commonsense and factual reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>final-answer accuracy (reported as strong; sometimes exceeding human baselines in cited literature)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Survey notes that GPT-4 and similar SOTA models achieve strong results but remain vulnerable to specific errors (e.g., ignoring long evidence, shortcutting).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>High overall reasoning capability but still makes systematic errors; used as motivating example for need for trace evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Vulnerable to errors on long-context evidence, shortcuts, ignoring temporal relations; trace correctness often diverges from answer correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Step-by-step Reasoning Traces: A Survey', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6822.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e6822.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Unfaithful reasoning prevalence</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Unfaithful / invalid-but-useful reasoning cases (survey statistic)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Survey-reported observation that a significant fraction of model-generated traces reach correct answers despite containing logical/factual errors, leading to mismatch between utility and validity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various LLMs on Omni-MATH (as reported by ProcessBench)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Aggregated observation across model outputs on challenging math benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer (various)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Observation of generated traces rather than an approach</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Omni-MATH / ProcessBench (survey cites these)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Challenging, olympiad-level mathematical reasoning dataset used in ProcessBench meta-analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>advanced mathematical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>fraction of cases where invalid traces produce correct answers</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>51.8% (survey-cited rate of invalid traces with correct answers in Omni-MATH, ProcessBench)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>High prevalence of unfaithful reasoning in hard benchmarks demonstrates that utility (final-answer correctness) can be decoupled from validity, motivating multi-criterion evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Demonstrates that trace-level utility is an unreliable proxy for factuality/validity; necessitates dedicated evaluators for step correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Step-by-step Reasoning Traces: A Survey', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Tree of thoughts <em>(Rating: 2)</em></li>
                <li>ProcessBench: Identifying process errors in mathematical reasoning <em>(Rating: 2)</em></li>
                <li>PRM800k: A large corpus of step-level annotations for reasoning traces <em>(Rating: 1)</em></li>
                <li>Math-Shepherd: Verify and reinforce LLMs step-by-step without human annotations <em>(Rating: 2)</em></li>
                <li>Logic-LM: Empowering large language models with symbolic solvers for faithful logical reasoning <em>(Rating: 2)</em></li>
                <li>FOLIO: Natural language reasoning with first-order logic <em>(Rating: 2)</em></li>
                <li>LeanDojo: Theorem proving with retrieval-augmented language models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6822",
    "paper_id": "paper-276421927",
    "extraction_schema_id": "extraction-schema-131",
    "extracted_data": [
        {
            "name_short": "CoT",
            "name_full": "Chain-of-Thought prompting",
            "brief_description": "A prompting technique where LLMs are prompted to generate intermediate natural-language reasoning steps before producing a final answer, used to improve performance on complex symbolic and arithmetic tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "various LLMs (e.g., GPT-4, PaLM, Qwen-2.5-7B)",
            "model_description": "Autoregressive transformer language models pre-trained on large web/text corpora; used with prompting to generate step-by-step natural language traces.",
            "model_size": null,
            "architecture_type": "transformer (autoregressive) with prompting",
            "training_data": null,
            "reasoning_method": "Chain-of-thought prompting (explicit stepwise natural-language trace generation)",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "Math/GSM8k, MATH, various symbolic/math benchmarks",
            "benchmark_description": "Arithmetic and broader mathematical reasoning benchmarks where stepwise traces are useful (word problems, competition math).",
            "task_type": "mathematical / symbolic reasoning, arithmetic word problems",
            "performance_metric": "final-answer accuracy; trace-level validity/utility",
            "performance_value": null,
            "comparison_with_baseline": "CoT shown in cited literature to substantially improve reasoning on math/symbolic tasks compared to direct-answer prompting; survey reiterates CoT as core enabler of stepwise reasoning.",
            "key_findings": "Chain-of-thought prompting expanded LLM capabilities on symbolic reasoning tasks; it is foundational for many downstream evaluator and verifier methods.",
            "limitations": "CoT does not guarantee faithful/valid reasoning traces (models can produce unfaithful or invalid steps even when final answer is correct); trace quality must be evaluated separately.",
            "uuid": "e6822.0",
            "source_info": {
                "paper_title": "Evaluating Step-by-step Reasoning Traces: A Survey",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "ToT",
            "name_full": "Tree-of-Thoughts (ToT)",
            "brief_description": "A search-based reasoning framework that explores multiple partial reasoning paths (a tree of intermediate steps) and uses scoring/evaluators to select promising branches, enabling backtracking and deliberation.",
            "citation_title": "Tree of thoughts",
            "mention_or_use": "mention",
            "model_name": "LLMs used as step generators/evaluators (various)",
            "model_description": "Transformer LLMs generate candidate next steps; an outer search procedure maintains multiple paths and prunes/expands using evaluators.",
            "model_size": null,
            "architecture_type": "transformer + external tree search (planning over generated steps)",
            "training_data": null,
            "reasoning_method": "Tree search over generated partial traces (deliberative exploration with pruning and evaluation)",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "Arithmetic, Game-of-24, other symbolic/exploration tasks",
            "benchmark_description": "Tasks requiring exploration and backtracking in large solution spaces (e.g., number-composition puzzles).",
            "task_type": "exploratory algorithmic/symbolic reasoning with backtracking",
            "performance_metric": "final-answer accuracy / success rate after search",
            "performance_value": null,
            "comparison_with_baseline": "Survey reports ToT among popular verifier-guided search methods; ToT enables stronger exploration than single-chain decoding and can improve success when paired with reliable evaluators.",
            "key_findings": "Tree-of-Thoughts provides an explicit mechanism for exploration and backtracking and benefits strongly from good evaluators; used for arithmetic and combinatorial tasks.",
            "limitations": "Computationally expensive (scales with branching and depth); requires robust evaluators to choose branches, and vulnerable to evaluator weaknesses.",
            "uuid": "e6822.1",
            "source_info": {
                "paper_title": "Evaluating Step-by-step Reasoning Traces: A Survey",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Verifier-guided search",
            "name_full": "Verifier-guided search (Best-of-N / tree search)",
            "brief_description": "Approaches that generate multiple reasoning traces (or expand a search tree) and use an external evaluator/verifier to select the most promising trace, improving final-answer accuracy.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "various LLMs (used as generators) with separate evaluators/critics",
            "model_description": "Base LLMs autoregressively sample many candidate traces; separate evaluator models or majority-vote select the final trace/answer.",
            "model_size": null,
            "architecture_type": "transformer for generation; separate transformer or classifier for evaluation",
            "training_data": null,
            "reasoning_method": "Best-of-N decoding, tree search, and selection using evaluator scores (verifier-guided selection)",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "ProcessBench (GSM8k+MATH subsets used by survey), other math/ reasoning benchmarks",
            "benchmark_description": "Meta-evaluation benchmarks and downstream reasoning tasks used to measure improvement when using verifiers to select traces.",
            "task_type": "mathematical and multi-step reasoning tasks where multiple traces are sampled",
            "performance_metric": "final-answer accuracy; classification accuracy of evaluator in meta-evaluation",
            "performance_value": null,
            "comparison_with_baseline": "Survey notes tradeoff: for weaker evaluators majority-voting across samples often outperforms verifier-guided selection; with stronger evaluators (better training or stronger critic models), verifier-guided Best-of-N can outperform majority voting at same compute budget.",
            "key_findings": "Verifier-guided search can substantially improve downstream accuracy if evaluators are strong; scaling either exploration (more samples) or evaluation (stronger evaluators/longer outputs) matters and the optimal allocation depends on evaluator strength.",
            "limitations": "Effectiveness depends on evaluator robustness; vulnerable to evaluator biases/spurious features (e.g., step length), and computational cost increases with N and evaluator complexity.",
            "uuid": "e6822.2",
            "source_info": {
                "paper_title": "Evaluating Step-by-step Reasoning Traces: A Survey",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "MCTS",
            "name_full": "Monte Carlo Tree Search (MCTS) for utility estimation and search",
            "brief_description": "Use of MCTS to estimate step-level utility (value function) by sampling rollouts from a candidate step and using rewards (e.g., final-answer correctness) to assign utility; also used to guide search.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "LLM rollouts (various)",
            "model_description": "LLMs generate rollouts from nodes/steps; MCTS aggregates stochastic rollouts to estimate expected reward/value.",
            "model_size": null,
            "architecture_type": "transformer for rollouts plus MCTS planning layer",
            "training_data": null,
            "reasoning_method": "MCTS rollouts for step evaluation and for constructing step-level utility labels for training evaluators",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "Math-Shepherd, Game-of-24, other math/process benchmarks",
            "benchmark_description": "Benchmarks where utility can be measured by sampling continuations and checking final-answer correctness.",
            "task_type": "utility estimation for multi-step reasoning; search-guided solution finding",
            "performance_metric": "step-level utility (expected reward), final-answer accuracy",
            "performance_value": null,
            "comparison_with_baseline": "MCTS-derived utility labels are cheaper than human validity labels and have been used to scale training data for process reward models; survey notes MCTS enables scalable labeling but may not fully capture factuality/validity due to unfaithful traces.",
            "key_findings": "MCTS provides a scalable way to label utility and train evaluators or guide search; frequently used to train process reward models and to compute step advantage.",
            "limitations": "Computationally heavy; utility labels do not reliably substitute for validity/factuality due to unfaithful reasoning (traces that reach correct answer despite invalid steps).",
            "uuid": "e6822.3",
            "source_info": {
                "paper_title": "Evaluating Step-by-step Reasoning Traces: A Survey",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Critic",
            "name_full": "LLM-as-a-judge / Critic models",
            "brief_description": "Using LLMs themselves (with prompting and/or fine-tuning) to evaluate reasoning traces across factuality, validity, coherence, and utility, optionally producing natural-language rationales before assigning scores.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "large reasoning-capable LLMs (various; e.g., GPT-family, Qwen)",
            "model_description": "Large autoregressive LLMs used in an evaluation role, often prompted to produce step-level critique/rationale and then produce a score.",
            "model_size": null,
            "architecture_type": "transformer (critic usage, generative evaluation)",
            "training_data": null,
            "reasoning_method": "Prompted critique (generate rationale then predict score); can be used zero-shot or fine-tuned as a critic",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "CriticBench, ProcessBench, PRM800k (used for training/eval)",
            "benchmark_description": "Meta-evaluation benchmarks for assessing critic accuracy on step-level labels across domains.",
            "task_type": "step-level evaluation (factuality, validity, coherence, utility)",
            "performance_metric": "classification accuracy / F1 on meta-evaluation labels; improvement in downstream task accuracy when used in Best-of-N",
            "performance_value": null,
            "comparison_with_baseline": "Survey reports critics are versatile and can evaluate all four criteria; they can outperform simpler models if allowed to generate rationales and use test-time scaling, but are computationally more expensive than sequence classifiers/cross-encoders.",
            "key_findings": "Critic models are flexible, work with closed-source LLMs, and can be effective without fine-tuning; they particularly benefit from test-time scaling (majority voting over samples) and partial-context techniques.",
            "limitations": "High runtime and compute; generation of rationales is expensive; limited by context length when traces are long; can still make mistakes in distinguishing factual vs logical errors.",
            "uuid": "e6822.4",
            "source_info": {
                "paper_title": "Evaluating Step-by-step Reasoning Traces: A Survey",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "SeqClass",
            "name_full": "Sequence classifiers / Process (reward) models",
            "brief_description": "Lightweight models (LMs with small classification heads) trained to assign numeric scores to entire traces or individual steps, used as reward models or fast evaluators.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Fine-tuned sequence classifiers (various research implementations)",
            "model_description": "Transformer encoder/decoder LMs with a classification/regression head fine-tuned on step-level labels (e.g., PRM800k) to predict quality scores or categorical labels.",
            "model_size": null,
            "architecture_type": "transformer + classification head (fine-tuned discriminative)",
            "training_data": "PRM800k and derivatives, MCTS-derived utility labels, LLM-perturbed traces (as reported in survey)",
            "reasoning_method": "Supervised classification/regression on human- or automatically-labeled step-level quality labels (valid/invalid/useful/etc.)",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "PRM800k, ProcessBench, VersaPRM, Qwen-PRM (survey-listed datasets used for training/eval)",
            "benchmark_description": "Large crowdsourced or synthetic datasets with step-level labels used to train/evaluate sequence classifiers.",
            "task_type": "step-level validity/utility classification; trace-level reward prediction",
            "performance_metric": "classification accuracy on step labels; effectiveness in Best-of-N selection when used as evaluator",
            "performance_value": null,
            "comparison_with_baseline": "Sequence classifiers trained with combined validity+utility positives outperform classifiers trained on only one criterion (survey cites Zhang et al., 2025 results qualitatively).",
            "key_findings": "Sequence classifiers are efficient at evaluation and can be strong if trained on high-quality multi-criteria data; they are commonly used as reward models in RL pipelines.",
            "limitations": "Lose generative trace-capability (cannot produce rationales), require large labeled datasets for training, may overfit labeling artifacts, and are limited when traces exceed context length.",
            "uuid": "e6822.5",
            "source_info": {
                "paper_title": "Evaluating Step-by-step Reasoning Traces: A Survey",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "LLM-VF",
            "name_full": "LLM-as-a-value-function / Generative reward models (DPO/Step-DPO)",
            "brief_description": "Methods that align or fine-tune LLM token probabilities to correspond to rewards (e.g., answer correctness) using preference-learning objectives like DPO, enabling the model itself to assign value to traces via token probabilities.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "LLMs fine-tuned with preference objectives (e.g., DPO/GRPO variants)",
            "model_description": "Autoregressive LLMs fine-tuned with preference/ reward objectives so output token probabilities reflect evaluator-derived rewards; can generate traces and produce internal value signals.",
            "model_size": null,
            "architecture_type": "transformer fine-tuned with preference optimization algorithms (DPO, GRPO, etc.)",
            "training_data": "Preference pairs (human or LLM-generated), rewards from MCTS or final-answer correctness, PRM-like datasets",
            "reasoning_method": "Preference learning / direct preference optimization to align token probabilities with desired process/outcome rewards; can be used for RL-style training.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "Math-Shepherd, GenRM, MCTS-DPO variants (survey cites several works)",
            "benchmark_description": "Math and logic datasets used to train/evaluate value-function-style LLMs.",
            "task_type": "process/outcome reward modeling; improving long-chain reasoning via RL-style fine-tuning",
            "performance_metric": "final-answer accuracy after RL/fine-tuning; improvement in downstream tasks when used as reward model",
            "performance_value": null,
            "comparison_with_baseline": "Survey indicates LLM-as-value-function approaches can retain generation ability while aligning rewards, and focus chiefly on utility because utility labels scale easily; they require many good/bad traces for training.",
            "key_findings": "Generative reward models allow direct use of LLMs as value functions and can be effective for training improved reasoners; they are commonly used in recent RL pipelines for reasoning.",
            "limitations": "Require large, high-quality preference data; risk of reward hacking if evaluator is sensitive to spurious features (e.g., length); primarily capture utility rather than factuality or fine-grained logical validity.",
            "uuid": "e6822.6",
            "source_info": {
                "paper_title": "Evaluating Step-by-step Reasoning Traces: A Survey",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "PartialContext / PARC",
            "name_full": "Partial-context evaluation (premise selection; PARC)",
            "brief_description": "Evaluate a step using only a selected subset of prior steps and query premises (an entailment graph), improving efficiency and removing distractors to increase evaluator accuracy.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "PARC (method implemented with critic models in survey experiments)",
            "model_description": "Combines premise-selection (entailment graph construction) with critic evaluation applied only to identified relevant premises, reducing context length per evaluation.",
            "model_size": null,
            "architecture_type": "transformer critic operating on reduced premise subset; entailment-graph construction module",
            "training_data": null,
            "reasoning_method": "Premise identification + partial-context evaluation to judge validity/coherence efficiently",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "ProcessBench (GSM8k+MATH subsets for evaluation in survey)",
            "benchmark_description": "Meta-evaluation benchmark used to compare partial-context vs full-context evaluators.",
            "task_type": "validity and coherence evaluation for step-level reasoning",
            "performance_metric": "meta-evaluation classification accuracy; compute cost per trace",
            "performance_value": null,
            "comparison_with_baseline": "PARC (partial context) reduces compute and in reported experiments improved accuracy relative to full-context critics by removing distractors; average premises per step reported as ~1.57 leading to lower compute.",
            "key_findings": "Partial-context evaluation is both more computationally efficient and can be more accurate because it reduces distracting context; it also helps distinguish direct vs accumulated errors.",
            "limitations": "Requires a reliable premise-selection method; premise discovery may be challenging and failure-prone for complex traces.",
            "uuid": "e6822.7",
            "source_info": {
                "paper_title": "Evaluating Step-by-step Reasoning Traces: A Survey",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Symbol-grounded",
            "name_full": "Symbol-grounded / neuro-symbolic evaluation and reasoning (Logic-LM, LINC, LeanDojo etc.)",
            "brief_description": "Approaches that align natural-language reasoning traces with symbolic representations or integrate LLMs with symbolic solvers/theorem provers to obtain formally-checkable reasoning steps.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Neuro-symbolic hybrids (Logic-LM, LINC, LeanDojo examples cited)",
            "model_description": "Combinations of transformer LMs with symbolic modules (first-order logic provers, theorem provers like Lean or Isabelle) used for faithful logical deduction and verification.",
            "model_size": null,
            "architecture_type": "transformer + external symbolic solver / theorem prover",
            "training_data": "formal logic corpora, theorem-proving corpora (where available); not specified in survey",
            "reasoning_method": "Parsing/translation of NL steps into symbolic representations then using symbolic solvers to perform/verify deductions or proofs",
            "external_tool_used": true,
            "external_tool_description": "Integration with interactive theorem provers (Lean, Isabelle) or first-order logic provers to verify or produce formal proofs; survey notes these are underexplored for trace evaluation.",
            "benchmark_name": "FOLIO / P-FOLIO, formal theorem proving datasets, LeanDojo/interactive theorem proving benchmarks",
            "benchmark_description": "Benchmarks for first-order logic and formal theorem proving with human-written reasoning chains / formal proofs.",
            "task_type": "first-order theorem proving, formal proof generation/verification, deductive logical reasoning",
            "performance_metric": "proof correctness / exact proof match / theorem-prover success",
            "performance_value": null,
            "comparison_with_baseline": "Survey notes limited prior work; symbolic grounding offers precise definitions for validity and utility but few methods extend rule-based parsing to complex first-order logic and interactive theorem proving.",
            "key_findings": "Symbol-grounded approaches provide formal evaluation opportunities and precise validity checks but are not yet widely applied to complex NL traces; promising direction for rigorous logical reasoning evaluation.",
            "limitations": "Parsing natural language to formal logic is hard; limited prior work generalizing to realistic complex tasks; requires integration with external theorem provers and expensive engineering.",
            "uuid": "e6822.8",
            "source_info": {
                "paper_title": "Evaluating Step-by-step Reasoning Traces: A Survey",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Rule-based matching",
            "name_full": "Rule-based / symbolic matching evaluators",
            "brief_description": "Deterministic evaluators that parse traces into symbolic graphs or computation graphs and check edges/operations against a ground-truth symbolic solution, effective when symbolic representations exist.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "rule-based parsers/evaluators (task-specific)",
            "model_description": "Systems that map NL traces to directed graphs of entities/operations and perform exact matching to ground-truth relations or computation graphs.",
            "model_size": null,
            "architecture_type": "parser + deterministic symbolic matcher (rule-based)",
            "training_data": null,
            "reasoning_method": "Exact symbolic parsing and matching to a gold solution graph for factuality/coherence/utility evaluation",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "Arithmetic/knowledge-graph-style tasks where ground-truth symbolic solutions exist",
            "benchmark_description": "Tasks with clear symbolic representations such as computation graphs for arithmetic or knowledge-graph multi-hop QA.",
            "task_type": "symbolic evaluation: factuality/coherence/utility via graph matching",
            "performance_metric": "exact-match rates for edges/steps; ability to detect incorrect steps",
            "performance_value": null,
            "comparison_with_baseline": "Rule-based matching performs well when symbolic ground truth exists but does not generalize to commonsense or complex math beyond simple arithmetic.",
            "key_findings": "Highly precise where applicable; useful for diagnostics in tasks with explicit symbolic structure.",
            "limitations": "Does not generalize to tasks without explicit symbolic form; brittle to linguistic variability; requires robust parsing.",
            "uuid": "e6822.9",
            "source_info": {
                "paper_title": "Evaluating Step-by-step Reasoning Traces: A Survey",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Qwen-2.5-7B",
            "name_full": "Qwen-2.5-7B-Instruct (as used in survey experiments)",
            "brief_description": "A 2.5B-parameter variant in the Qwen family used by the survey authors as a common base model for controlled comparisons in Figure 4 (base model for many evaluator comparisons).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Qwen-2.5-7B-Instruct",
            "model_description": "Instruction-tuned transformer language model (Qwen family) used as base generator and baseline for several evaluator compute/performance estimates in the survey.",
            "model_size": "2.5B",
            "architecture_type": "transformer (instruct-tuned)",
            "training_data": null,
            "reasoning_method": "Used as base generator for Chain-of-Thought traces and as reference for evaluator comparisons (figure experiments).",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "ProcessBench (GSM8k + MATH subsets used in Figure 4)",
            "benchmark_description": "Meta-evaluation benchmark subset used for controlled comparisons of evaluators in the survey.",
            "task_type": "mathematical reasoning / trace evaluation baseline",
            "performance_metric": "ProcessBench performance (aggregated) vs compute in Figure 4",
            "performance_value": null,
            "comparison_with_baseline": "Survey uses Qwen-2.5-7B as the shared base to compare evaluator implementations; specific numeric scores are reported in cited sources but not tabulated in the survey text.",
            "key_findings": "Serves as a controlled baseline for comparing compute vs evaluator effectiveness; used to illustrate tradeoffs like majority voting vs verifier-guided selection.",
            "limitations": "Base model capability constrains absolute evaluation performance; comparisons across papers are confounded by differing base models.",
            "uuid": "e6822.10",
            "source_info": {
                "paper_title": "Evaluating Step-by-step Reasoning Traces: A Survey",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4 (illustrative mention)",
            "brief_description": "State-of-the-art closed-source LLM cited as achieving strong performance on challenging MHQA benchmarks, but still susceptible to errors in long-evidence adherence and temporal reasoning.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GPT-4",
            "model_description": "Large proprietary transformer-based model by OpenAI with strong multi-task and reasoning capabilities referenced in survey examples.",
            "model_size": null,
            "architecture_type": "transformer (closed-source large-scale)",
            "training_data": null,
            "reasoning_method": "Used with Chain-of-Thought and other prompting/verifier methods in literature; cited as strong baseline for MHQA and other reasoning tasks.",
            "external_tool_used": null,
            "external_tool_description": null,
            "benchmark_name": "ARC-Challenge, PIQA, MHQA datasets",
            "benchmark_description": "Hard multi-hop and commonsense QA datasets where top LLMs show strong but imperfect performance.",
            "task_type": "multi-hop question answering, commonsense and factual reasoning",
            "performance_metric": "final-answer accuracy (reported as strong; sometimes exceeding human baselines in cited literature)",
            "performance_value": null,
            "comparison_with_baseline": "Survey notes that GPT-4 and similar SOTA models achieve strong results but remain vulnerable to specific errors (e.g., ignoring long evidence, shortcutting).",
            "key_findings": "High overall reasoning capability but still makes systematic errors; used as motivating example for need for trace evaluation.",
            "limitations": "Vulnerable to errors on long-context evidence, shortcuts, ignoring temporal relations; trace correctness often diverges from answer correctness.",
            "uuid": "e6822.11",
            "source_info": {
                "paper_title": "Evaluating Step-by-step Reasoning Traces: A Survey",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Unfaithful reasoning prevalence",
            "name_full": "Unfaithful / invalid-but-useful reasoning cases (survey statistic)",
            "brief_description": "Survey-reported observation that a significant fraction of model-generated traces reach correct answers despite containing logical/factual errors, leading to mismatch between utility and validity.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "various LLMs on Omni-MATH (as reported by ProcessBench)",
            "model_description": "Aggregated observation across model outputs on challenging math benchmarks.",
            "model_size": null,
            "architecture_type": "transformer (various)",
            "training_data": null,
            "reasoning_method": "Observation of generated traces rather than an approach",
            "external_tool_used": null,
            "external_tool_description": null,
            "benchmark_name": "Omni-MATH / ProcessBench (survey cites these)",
            "benchmark_description": "Challenging, olympiad-level mathematical reasoning dataset used in ProcessBench meta-analysis.",
            "task_type": "advanced mathematical reasoning",
            "performance_metric": "fraction of cases where invalid traces produce correct answers",
            "performance_value": "51.8% (survey-cited rate of invalid traces with correct answers in Omni-MATH, ProcessBench)",
            "comparison_with_baseline": null,
            "key_findings": "High prevalence of unfaithful reasoning in hard benchmarks demonstrates that utility (final-answer correctness) can be decoupled from validity, motivating multi-criterion evaluation.",
            "limitations": "Demonstrates that trace-level utility is an unreliable proxy for factuality/validity; necessitates dedicated evaluators for step correctness.",
            "uuid": "e6822.12",
            "source_info": {
                "paper_title": "Evaluating Step-by-step Reasoning Traces: A Survey",
                "publication_date_yy_mm": "2025-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Tree of thoughts",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts"
        },
        {
            "paper_title": "ProcessBench: Identifying process errors in mathematical reasoning",
            "rating": 2,
            "sanitized_title": "processbench_identifying_process_errors_in_mathematical_reasoning"
        },
        {
            "paper_title": "PRM800k: A large corpus of step-level annotations for reasoning traces",
            "rating": 1,
            "sanitized_title": "prm800k_a_large_corpus_of_steplevel_annotations_for_reasoning_traces"
        },
        {
            "paper_title": "Math-Shepherd: Verify and reinforce LLMs step-by-step without human annotations",
            "rating": 2,
            "sanitized_title": "mathshepherd_verify_and_reinforce_llms_stepbystep_without_human_annotations"
        },
        {
            "paper_title": "Logic-LM: Empowering large language models with symbolic solvers for faithful logical reasoning",
            "rating": 2,
            "sanitized_title": "logiclm_empowering_large_language_models_with_symbolic_solvers_for_faithful_logical_reasoning"
        },
        {
            "paper_title": "FOLIO: Natural language reasoning with first-order logic",
            "rating": 2,
            "sanitized_title": "folio_natural_language_reasoning_with_firstorder_logic"
        },
        {
            "paper_title": "LeanDojo: Theorem proving with retrieval-augmented language models",
            "rating": 2,
            "sanitized_title": "leandojo_theorem_proving_with_retrievalaugmented_language_models"
        }
    ],
    "cost": 0.023343000000000003,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Evaluating Step-by-step Reasoning Traces: A Survey
20 Sep 2025</p>
<p>Jinu Lee jinulee2@illinois.edu 
Julia Hockenmaier juliahmr@illinois.edu 
Pranav Neelakantan 
Girish Shyam 
Amanda Sastry 
Sandhini Askell 
Ariel Agarwal 
Gretchen Herbert-Voss 
Tom Krueger 
Rewon Henighan 
Aditya Child 
Daniel M Ramesh 
Jeffrey Ziegler 
Clemens Wu 
Christopher Winter 
Mark Hesse 
Eric Chen 
Mateusz Sigler 
Scott Litwin 
Benjamin Gray 
Jack Chess 
Christopher Clark 
Sam Berner 
Alec Mc- Candlish 
Ilya Radford 
Dario Sutskever 
Mark Chen 
Jerry Tworek 
Heewoo Jun 
Qiming Yuan 
Henrique Ponde 
Oliveira Pinto 
Jared Ka- Plan 
Harri Edwards 
Yuri Burda 
Nicholas Joseph 
Greg Brockman 
Alex Ray 
Raul Puri 
Gretchen Krueger 
Michael Petrov 
Heidy Khlaaf 
Girish Sas- Try 
Pamela Mishkin 
Brooke Chan 
Scott Gray 
Nick Ryder 
Mikhail Pavlov 
Alethea Power 
Lukasz Kaiser 
Mohammad Bavarian 
Clemens Winter 
Philippe Tillet 
Felipe Petroski Such 
Dave Cum- Mings 
Matthias Plappert 
Fotios Chantzis 
Eliza- Beth Barnes 
Ariel Herbert-Voss 
William Hebgen Guss 
Alex Nichol 
Alex Paino 
Nikolas Tezak 
Jie Tang 
Igor Babuschkin 
Suchir Balaji 
Shantanu Jain 
William Saunders 
Christopher Hesse 
Andrew N Carr 
Jan Leike 
Josh Achiam 
Vedant Misra 
Evan Morikawa 
Alec Radford 
Matthew Knight 
Miles Brundage 
Mira Murati 
Katie Mayer 
Peter Welinder 
Bob Mcgrew 
Dario Amodei 
Sam Mccandlish 
Ilya Sutskever 
Wojciech 2021 Zaremba 
Evaluat 
Xiusi Chen 
Gaotang Li 
Ziqi Wang 
Bowen Jin 
Cheng 
Yanda Chen 
Joe Benton 
Ansh Radhakrishnan 
Jonathan Uesato 
Carson Denison 
John Schulman 
Arushi Somani 
Peter Hase 
Misha Wagner 
Fabien Roger 
Vlad Mikulik 
Sam Bowman 
Jared Kaplan 
Ethan Perez 
Zhiyu Chen 
Wenhu Chen 
Peter Clark 
Isaac Cowhey 
Oren Etzioni 
Tushar Khot 
Ashish Sabharwal 
Carissa Schoenick 
Karl Cobbe 
Vineet Kosaraju 
Jacob Hilton 
Reiichiro Nakano 
Antonia Creswell 
Murray 2022 Shanahan 
Faithful 
Ganqu Cui 
Lifan Yuan 
Zefan Wang 
Hanbin Wang 
Wendi Li 
Bingxiang He 
Yuchen Fan 
Tianyu Yu 
Qixin Xu 
Weize Chen 
Jiarui Yuan 
Huayu Chen 
Kaiyan Zhang 
Xingtai Lv 
Shuo Wang 
Yuan Yao 
Xu Han 
Hao Peng 
Yu Cheng 
Zhiyuan Liu 
Maosong Sun 
Bowen Zhou 
Ning Ding 
Ning Dai 
Zheng Wu 
Renjie Zheng 
Ziyun Wei 
Wenlei Shi 
Xing Jin 
Guanlin Liu 
Chen Dun 
Liang Huang 
Lin Yan 
Bofei Gao 
Zefan Cai 
Runxin Xu 
Peiyi Wang 
Ce Zheng 
Runji Lin 
Keming Lu 
Dayiheng Liu 
Chang Zhou 
Wen Xiao 
Junjie Hu 
Tianyu Liu 
Baobao Chang 
Llm 
Feifan Song 
Zhe Yang 
Yibo Miao 
Qingxiu Dong 
Lei Li 
Chenghao Ma 
Liang </p>
<p>University of Illinois Urbana-Champaign</p>
<p>Association for Computational Linguistics
Canada</p>
<p>Yu WangQian, Hongru Wang, Yu Zhang, Denghui Zhang, Tong Zhang, Hanghang Tong</p>
<p>Charese Smiley
Sameena Shah</p>
<p>Iana Borova
Matt Beane, Ting-Hao HuangDylan Langdon, Reema Moussa</p>
<p>Bryan Routledge, and William Yang Wang</p>
<p>Runxin Xu, Zhengyang Tang, Daoguang Zan, Shanghaoran Quan, Yichang ZhangChen, Benyou Wang, Ge Zhang, Lei Sha, Xuancheng Ren, Tianyu Liu</p>
<p>Evaluating Step-by-step Reasoning Traces: A Survey
20 Sep 20252BBDCAA64F235BD8E89EDC4B7EF4A816arXiv:2502.12289v3[cs.CL]2018. Think you have solved question answering? try arcthe ai2 reasoning challenge. PreprintarXiv:1803.05457 Process supervision-guided policy optimization for code generation. PreprintarXiv:2410.17621
Step-by-step reasoning is widely used to enhance the reasoning ability of large language models (LLMs) in complex problems.Evaluating the quality of reasoning traces is crucial for understanding and improving LLM reasoning.However, existing evaluation practices are highly inconsistent, resulting in fragmented progress across evaluator design and benchmark development.To address this gap, this survey provides a comprehensive overview of step-by-step reasoning evaluation, proposing a taxonomy of evaluation criteria with four toplevel categories (factuality, validity, coherence, and utility).Based on the taxonomy, we review different datasets, evaluator implementations, and recent findings, leading to promising directions for future research.</p>
<p>Introduction</p>
<p>Large language models (LLMs) have demonstrated remarkable capabilities in reasoning on complex problems, such as logic, math, and science.At the core of this versatility lies step-by-step reasoning (Wei et al., 2022;Kojima et al., 2022), where the LLM generates an intermediate reasoning trace before presenting the final answer.</p>
<p>The reasoning ability of LLMs is often measured in terms of answer accuracy, i.e., finding the correct answer for a complex reasoning problem (Cobbe et al., 2021;Zhong et al., 2021).However, answer accuracy is generally insufficient for measuring LLMs' reasoning ability, as the correct answer does not imply the correctness of the preceding reasoning trace (Lanham et al., 2023;Mirzadeh et al., 2024;Paul et al., 2024).Furthermore, assessing the quality of the reasoning trace can directly lead to better reasoning ability of LLMs by verifierguided search (Wang et al., 2023b;Yao et al., 2023;Hao et al., 2024) and reinforcement learning (Lu et al., 2024;Cui et al., 2025).</p>
<p>Query</p>
<p>The denominator of a fraction is 7 less than 3 times the numerator.If the fraction is equivalent to 2/5, what is the numerator?</p>
<p>Reasoning trace</p>
<p>Let the numerator be x.</p>
<p>The denominator is 3x-7.</p>
<p>We know that x/(3x-7) = 3/5.</p>
<p>Therefore, 5x = 6x-14.</p>
<p>Step 1</p>
<p>Step 2</p>
<p>Step 3</p>
<p>Step 4</p>
<p>Step 5 7 Finally, we get x= .Consequently, reasoning trace evaluation is an active research topic, with numerous new evaluators and datasets continuously being proposed.However, this rapid growth has led to a proliferation of evaluators and datasets without establishing a consensus on the criteria (what to evaluate).In this survey, we aim to provide a systematic review of existing step-by-step reasoning evaluation criteria, which will serve as a foundation for implementing evaluators.</p>
<p>Implementing an evaluator also introduces several practical decision choices.Different architectures involve trade-offs between computational cost and expected performance, while non-architectural factors like training data and data format also play a significant role.This survey seeks to categorize and compare various evaluator implementations, highlighting key trade-offs and revealing additional dimensions that merit consideration.</p>
<p>The key contributions of this survey are:</p>
<p> Defining a clear, universal taxonomy of stepby-step evaluation criteria ( 3).</p>
<p> Surveying existing datasets and evaluators for step-by-step reasoning evaluation based on their implementations, across diverse reasoning tasks and criteria ( 4- 5).</p>
<p> Identifying recent findings and promising directions for trace evaluation ( 6- 7).</p>
<p>Background</p>
<p>2.1 Step-by-step reasoning</p>
<p>Step-by-step reasoning is where LLMs generate a series of intermediate natural language steps ("thoughts") before outputting the final answer (Wei et al., 2022).Each instance consists of two parts: a query and a reasoning trace, and the final answer as a part of the reasoning trace.Upon seeing the query (user input), the LLM autoregressively generates its solution as a reasoning trace.Finally, a trace should include a final answer for the query, which can be compared to the ground truth.See Appendix A for details on different reasoning tasks.</p>
<p>Evaluation</p>
<p>Reasoning trace evaluators assess the quality of the reasoning trace and assign a score, reflecting whether it is good or not based on the criterion.Evaluators can be intrinsic metrics like uncertainty to models specialized for reasoning trace evaluation; see Section 5 for different types of evaluators.</p>
<p>Meta-evaluation</p>
<p>How can we evaluate these evaluators (metaevaluation)?Two common directions apply: (1) using meta-evaluation benchmarks with step-wise labels, or (2) measuring the improvement in the downstream task performance (Figure 2).</p>
<p>Meta-evaluation Benchmarks</p>
<p>Meta-evaluation benchmarks contains labels indicating a step's quality based on the predefined criteria.In this setting, the evaluator's performance is measured by the classification accuracy of these labels.These benchmarks offer a fine-grained view of which criteria the evaluator can handle well and which cannot (Song et al., 2025).However, constructing these data often requires costly human annotation (Lightman et al., 2024;Zheng et al., 2024a) and the gains in meta-evaluation benchmark might not generalize to downstream performance (Zhang et al., 2025).Further details can be found in Appendix B.</p>
<p>Query Trace 2</p>
<p>Trace 1</p>
<p>Trace 3</p>
<p>Meta-evaluation benchmarks:</p>
<p>Can evaluator classify good/bad steps?</p>
<p>Verifier-guided search:</p>
<p>Can the evaluator choose the most promising trace?</p>
<p>Reinforcement learning: Are evaluator scores a good reward function?</p>
<p> Chosen trace answer: Correct  Performance of trained LLM  Classification accuracy: 0.67 Evaluator 0.99 0.17 Step 2</p>
<p>Step 1</p>
<p>LLM</p>
<p>Generate trace</p>
<p>Train via RL Figure 2: Illustration of three popular meta-evaluation methods: meta-evaluation benchmarks, verifier-guided search, and reinforcement learning.</p>
<p>Downstream performance improvement</p>
<p>As the fundamental goal of evaluators is to improve the reasoning ability of LLMs, the evaluator's quality can also be measured by the improvement in downstream reasoning tasks.</p>
<p>Verifier-guided search uses evaluator scores to find the most promising trace after exploring different paths.Popular methods include Best-of-N decoding (independently sampling N traces and selecting one) (Lightman et al., 2024;Zhang et al., 2024c) and tree search (sampling multiple candidate steps and choosing the most promising path) (Yao et al., 2023;Guan et al., 2024;Zhu et al., 2024b).The performance is often compared to majority voting without evaluators (Self-consistency; Wang et al. (2023b)), where a bigger gap indicates a better evaluator performance.</p>
<p>Reinforcement learning (RL) uses evaluator scores as a reward to further train an LLM (Uesato et al., 2022;Pan et al., 2023b;Zhang et al., 2024a).If the evaluator provides useful training rewards, the trained model will reach higher final answer accuracy.Moreover, as evaluators that are vulnerable to spurious features like length lead to reward hacking, successful RL also indicates the evaluator's robustness (Zhang et al., 2024a).</p>
<p>Query</p>
<p>The denominator of a fraction is 7 less than 3 times the numerator.If the fraction is equivalent to 2/5, what is the numerator?</p>
<p>Correct reasoning trace</p>
<p>Let the numerator be x.</p>
<p>The denominator is 3x-7.</p>
<p>We know that x/(3x-7) = 2/5.</p>
<p>Therefore, 5x = 6x-14.</p>
<p>Factuality:</p>
<p>Factually true according to query/external facts?</p>
<p>Coherence:</p>
<p>All preconditions presented in previous steps?</p>
<p>Let the numerator be x.</p>
<p>The denominator is 3x-7.</p>
<p>Validity:</p>
<p>Logically/arithmetically correct?</p>
<p>We know that x/(3x-7) = 2/5.</p>
<p>Therefore, 5x = 6x-14.</p>
<p>Finally, we get x=7.</p>
<p>Utility:</p>
<p>Leads to a correct final answer?</p>
<p>Step 1</p>
<p>Step 2</p>
<p>Step 3</p>
<p>Step 4</p>
<p>Step 5</p>
<p>Query</p>
<p>Step 3</p>
<p>Step 4</p>
<p>Step 5'</p>
<p>Step 1</p>
<p>Step 2</p>
<p>Step 3' Therefore, 5x=6x-14.</p>
<p>fraction is equivalent to 2/5,</p>
<p>The denominator is 3x-7.</p>
<p>We know that x/(3x-7) = 3/5.</p>
<p>Step 2</p>
<p>Step 3'</p>
<p>We know that x/(3x-7) = 2/5.</p>
<p>2/5 is 0.4.</p>
<p>Answer: 0.4.</p>
<p>Step 3</p>
<p>Step 4'</p>
<p>Step 5'</p>
<p>Step 4</p>
<p>Step 5 14  Finally, we get x=14.</p>
<p>Evaluation Criteria</p>
<p>Previous studies have proposed various criteria for evaluating step-by-step reasoning (Golovneva et al., 2023a;Lightman et al., 2024;Wang et al., 2024c;Jacovi et al., 2024), but these works failed to propose a complete taxonomy that covers diverse reasoning tasks (e.g., literature in factual reasoning and math reasoning have focused on different criteria).In this section, we propose a unified taxonomy of reasoning trace evaluation criteria that spans different reasoning tasks and evaluators.We categorize them into four key dimensions: Factuality, Validity, Coherence, and Utility (Figure 3) 1 .</p>
<p>Factuality</p>
<p>Factuality evaluates if the factual information can be grounded in reliable sources.The narrower notion of factuality is groundedness, where the generated trace should be true according to the query (Lewis et al., 2020;Gao et al., 2024d).For instance, if the retrieved document explicitly mentions that Einstein was born in 1879, the step mentioning that he was born in 1789 is ungrounded.In less factual tasks like math, groundedness also indicates using correct numbers and constraints given in the query.</p>
<p>However, the reasoning process might require factual knowledge not directly mentioned in the query.This type of factuality can be referred to as parametric knowledge.While steps containing trivia-style facts can be readily verified by retrievalbased fact checkers (Thorne et al., 2018), verify-1 These criteria are independent but not mutually exclusive (a step can fail to satisfy multiple criteria).</p>
<p>ing subtle, commonsensical knowledge remains an open challenge (Toroghi et al., 2024).</p>
<p>Validity</p>
<p>Validity evaluates if a reasoning step contains no logical errors.</p>
<p>The validity of a reasoning step can be defined in terms of entailment (Bowman et al., 2015), which is widely accepted in factual/commonsense-based reasoning (Prasad et al., 2023;Wu et al., 2024a).Under this definition, a step is considered valid if it can be directly entailed from previous steps (Tafjord et al., 2021;Dalvi et al., 2021;Saparov and He, 2023) or at least does not contradict them (Golovneva et al., 2023a;Prasad et al., 2023;Zhu et al., 2024b).</p>
<p>In tasks like math or logic, the more common form of validity is correctness, e.g.performing accurate calculations in arithmetic reasoning (Lightman et al., 2024;Jacovi et al., 2024;Zheng et al., 2024a) or inferring the correct logical conclusion based on the provided premises (Wu et al., 2024b;Song et al., 2025).</p>
<p>Coherence</p>
<p>Coherence measures if a reasoning step's preconditions are satisfied by the previous steps (Wang et al., 2023a;Lee and Hwang, 2025).For instance, if a trace includes the reasoning step "Next, we add 42 to 16." but the origin of the value 42 was never explained in the previous steps; this step is considered incoherent.An intuitive way to obtain an incoherent trace is randomly shuffling a coherent trace (Wang et al., 2023a;Nguyen et al., 2024) the premise of some steps will not appear anywhere in the previous steps (incoherent) even though it can be eventually deduced from the query (valid).Note that coherence judgment is inherently subjective and pragmatic compared to other criteria (Jacovi et al., 2024).For instance, seemingly trivial steps like "A part of something is present in that something" in WorldTree V2 (Xie et al., 2020) are annotated as necessary in Dalvi et al. (2021) but not necessary in Ott et al. (2023).</p>
<p>Utility</p>
<p>Utility measures whether a reasoning step contributes to getting the correct final answer.</p>
<p>The narrower interpretation of utility is progress, or whether the step is correctly following the ground truth solution (Saparov and He, 2023;Nguyen et al., 2024).For instance, in Game of 24 (making the number 24 using 4 natural numbers and basic arithmetic operations) (Yao et al., 2023), a solution can be defined as a sequence of operations (e.g.5+7 = 12  126 = 6  64 = 24).In this task, the utility of a step (making 5 + 7 = 12 from 5 and 7) can be directly assessed by checking if it is a part of a correct solution.</p>
<p>The more general version of utility is value function (estimated reward).(Chen et al., 2023;Wang et al., 2024c;Setlur et al., 2024).Value function is often measured using Monte Carlo Tree Search (MCTS), where the step's value is determined by the average/maximum reward of sampled continuations.Evaluating utility as a value function offers high scalability as it only requires the gold answer for computing the reward, without any human annotation or ground-truth solutions (Wang et al., 2024c;Lai et al., 2024;Cui et al., 2025).</p>
<p>Meta-evaluation Datasets</p>
<p>Datasets that annotate LLM-generated reasoning traces serve as key resources for training evaluators and conducting meta-evaluations between evaluators.A summary of existing datasets is provided in Table 4.</p>
<p>Among these, one of the most influential is PRM800k (Lightman et al., 2024).PRM800k consists of crowdsourced tertiary validity labels (positive, negative, neutral) assigned step by step, framing reasoning trace error detection as a sequence classification problem.Its design has inspired several successors (Zeng et al., 2024a;Xia et al., 2025), setting the paradigm for subsequent reasoning trace evaluation resources.</p>
<p>To address different needs, several extensions have been developed.Since human annotations Metric impl.
F V C U Rule-based     Uncertainty   V-information    LLM-as-value-function  Cross-encoder     Sequence classifiers    Critic models     Generative verifiers  Table 2:
Mapping between each metric implementation type to the category commonly used, where the acronym FVCU corresponds to factuality, validity, coherence, and utility, respectively.For each combination of metric and implementation,  denotes that there are at least 3 published works, and  denotes that there are 1 or 2. The full table can be found in Table 3. are costly and difficult to scale, many works have explored automatic labeling-either by estimating utility through Monte Carlo Tree Search (MCTS) (Wang et al., 2024c;Luo et al., 2024b;Setlur et al., 2024) or by generating perturbed traces with LLMs (Lu et al., 2024;Song et al., 2025).More recent datasets further broaden the scope by enabling multi-criteria meta-evaluation (Jacovi et al., 2024;Tyen et al., 2024;Song et al., 2025) and expanding coverage beyond mathematics into diverse domains (Zeng et al., 2024b(Zeng et al., , 2025)).</p>
<p>Additional details are provided in Appendix B.</p>
<p>Evaluator types</p>
<p>The goal of reasoning trace evaluators is to assess reasoning traces by assigning scores.However, choosing the right evaluator for the target criteria and task is non-trivial.For instance, there is no guarantee that evaluators designed for factuality and multi-hop question answering will seamlessly work on math reasoning problems.</p>
<p>In this survey, we provide a comprehensive overview of diverse reasoning trace evaluators, (Luo et al., 2024a;Wei et al., 2025).We summarize eight popular evaluator types based on the criteria they evaluate (summarized in Table 2), along with other practical strengths and weaknesses.</p>
<p>Rule-based matching</p>
<p>For tasks where the ground truth solution can be expressed as a graph of entities, a step corresponds to a directed edge between two entities, as in knowledge graphs for factual reasoning (Nguyen et al., 2024) or computation graphs for arithmetic problems (Li et al., 2023b).In this setting, factuality reduces to identifying the correct relation between entities, coherence to the correct ordering of steps, and utility to the existence of the step in the gold reasoning chain (Nguyen et al., 2024;Saparov and He, 2023).However, this approach does not generalize for tasks without clear symbolic representations, e.g., commonsense reasoning or complex math reasoning beyond arithmetic word problems.</p>
<p>Intrinsic metrics</p>
<p>Uncertainty Uncertainty of the model can be used as an intrinsic proxy for the generated content's quality (Xiao and Wang, 2021;Zhang et al., 2023b).Qiu et al. (2024) use token probability entropy, defined as  tV p(t)log(p(t)) where p is the probability distribution of all tokens in vocabulary V .Farquhar et al. ( 2024) and Kossen et al. (2024) extend the approach by clustering semantically similar tokens and calculating the entropy for each cluster.While uncertainty-based evaluators are primarily used for factuality (Wu et al., 2024a;Farquhar et al., 2024), they have also been applied for evaluating validity (Zhu et al., 2025) or utility (Hu et al., 2024), indicating that uncertainty can be a criteria-agnostic proxy of the quality of steps.2023) adopt V-information (VI) (Hewitt et al., 2021) from information theory.Informally, VI measures if a model family V can generate the correct goal string g with higher probability when the target string t is given to the model.Formally, VI(t  g) = log p V (g|t)  log p V (g|) when  is an empty string.When g is the final answer and t is the trace, VI becomes the difference between the answer token's probability between Chain-ofthought reasoning and zero-shot reasoning, which indicates how much information the trace provides to predicting the final answer (utility) (Chen et al., 2023).When g is a step and t is the list of previous steps, high VI means that a step is likely to follow from the context, which roughly corresponds to coherence (Prasad et al., 2023).</p>
<p>V-information</p>
<p>LLM-as-value-function RL can train LLMs to align rewards to token probabilities (relative to the base probability obtained from the initial model), with training objectives like DPO (Rafailov et al., 2023) and GRPO (Shao et al., 2024).For instance, when the reward is determined by the final answer correctness, the token probabilities directly correspond to utility (Mahan et al., 2024;Lai et al., 2024;Xie et al., 2024;Pang et al., 2024).Unlike sequence classifiers that lose their trace generation ability af-  (Qiu et al., 2024) Factual F Semantic entropy probes (Farquhar et al., 2024;Kossen et al., 2024) Factual, Common F Prasad et al., 2023) Common, Arith CVU EPVI (Wang et al., 2024d) Arith, Common U
SynCheck E (Wu et al., 2024a) Factual F UnCert-CoT (Zhu et al., 2025) Code V V-information REV (Chen et al., 2023) Common U ReCEval E (</p>
<p>LLM-as-value-function</p>
<p>GenRM (Mahan et al., 2024) Math, Logic, Code U V-STaR (Hosseini et al., 2024) Arith, Code U MCTS-DPO (Xie et al., 2024) Math, Common, Science</p>
<p>U</p>
<p>Step-DPO (Lai et al., 2024) Math U Tree-PLV (He et al., 2024b) Math, Common U Step-Controlled DPO (Lu et al., (Gao et al., 2024a) Math V Outcome/process Math-Shepherd (Wang et al., 2024c) Math U Process Eurus-PRM (Yuan et al., 2024) Math U Process PAV (Setlur et al., 2024) Math U Process ReasonEval (Xia et al., 2025) Math V Process Qwen-PRM (Zhang et al., 2025) Math, Science VU Process VersaPRM (Zeng et al., 2025) Expert FV Process</p>
<p>Critic models</p>
<p>Verify-CoT (Ling et al., 2023) Math, Symbolic V Partial context Tree-of-thoughts (Yao et al., 2023) Arith, Common U No fine-tune RAGTruth (Niu et al., 2024) Common F CPO (Zhang et al., 2024d) Factual, Arith U F 2 -Verification (Wang et al., 2024b) Common, Symbolic, Arith  (Kim et al., 2025b) Math, Science, Code V No fine-tune ThinkPRM (Khalifa et al., 2025) Math, Science, Code V</p>
<p>Generative verifiers</p>
<p>CLoud (Ankner et al., 2024) Math, Logic, Code V Generative verifier (Zhang et al., 2024c) Math, Symbolic V Table 3: Evaluators for step-by-step reasoning, grouped by implementation type ( E denotes that the method is an ensemble of different methods).The Domain column specifies the domains used for meta-evaluating each evaluator."Arith" refers to arithmetic reasoning tasks, "Common" denotes commonsense question answering, and "Expert" corresponds to specialized expert domains like biomedical, legal, and financial reasoning (Appendix A).When evaluators are tested on both arithmetic and general math tasks, only "Math" is listed.The acronym FVCU in the Criteria column represents factuality, validity, coherence, and utility, respectively.For AutoRace (Hao et al., 2024), LLMs are instructed to list the criteria based on incorrect traces (Custom).</p>
<p>ter fine-tuning, these models retain (and improve) the ability to generate traces.However, this method requires numerous good and bad reasoning traces for training.Consequently, most existing LLM-asvalue-function evaluators focus on utility, as it is easier to scale up the data by simply checking if the final answer is correct.</p>
<p>External evaluators</p>
<p>Cross-encoders Cross-encoders simultaneously encode multiple sentences using a single, small network often with millions of parameters (Devlin et al., 2019;Liu et al., 2019).They have been widely applied to solve tasks such as natural language inference (Bowman et al., 2015) and fact verification (Thorne et al., 2018), where one has to determine if the hypothesis can be inferred from the given premise.Cross-encoders trained on offthe-shelf tasks (Golovneva et al., 2023a;Zha et al., 2023;Prasad et al., 2023) or LLM-perturbed data (Zhu et al., 2024b) can be used to evaluate a reasoning step based on the query (factuality) or previous steps (validity).However, their limited language understanding ability and shorter context length restrict their performance in more complex tasks.</p>
<p>Sequence classifiers (Reward Models) 2 Sequence classifiers are language models with a lightweight classification head attached to the final hidden state, trained to predict a numeric score in a supervised manner (Lightman et al., 2024;Wang et al., 2024c;Setlur et al., 2024).Sequence classifiers can be further divided into (1) process (steplevel) evaluator vs.  (Ankner et al., 2024;She et al., 2025).</p>
<p>Critic models (LLM-as-a-judge) Critic models are LLMs that are trained or prompted to evaluate the reasoning traces (Zheng et al., 2023; Kim et al.,   2 While reward model generally refers to any model that predicts the desirability of an action in reinforcement learning, the term '(process/outcome) reward model' in the context of reasoning trace evaluation often refers to the sequence classifier architecture.2024a; Zheng et al., 2024a;Lin et al., 2024).This approach views trace evaluation as one of many reasoning tasks, where common techniques like Chain-of-thought prompting (Huang et al., 2024a) and reinforcement learning with verifiable rewards (Chen et al., 2025a) can apply.Numerous works show that LLMs are versatile critics; they can effectively evaluate factuality, validity, coherence, and utility in diverse reasoning tasks with or without fine-tuning (Yao et al., 2023;Jacovi et al., 2024;Wu et al., 2024d;Niu et al., 2024).While conceptually simple and compatible with closed-source models, generating the rationales requires significant execution time and computation compared to other evaluator types.</p>
<p>Generative Verifiers This paradigm lies in the middle ground of sequence classifiers and critic models.These models first autoregressively generate the evaluation rationale as critic models do.When the generation terminates, like sequence classifiers, a small, fine-tuned head predicts the scores conditioned on both the original reasoning trace and evaluation rationales generated by itself (Ankner et al., 2024;Zhang et al., 2024c).</p>
<p>Further improving evaluators</p>
<p>This section discusses some of the recent empirical findings on improving evaluators beyond choosing different types, e.g., training data, input format, and scaling compute.</p>
<p>Validity and utility are complementary Validity measures if the step is logically correct, while utility measures if the step makes progress towards the correct answer.Initially, utility-based process reward models were proposed as an alternative for validity, since constructing validity data often requires a costly annotation process (Wang et al., 2024c).Under the hood, there lies an implicit assumption that useful steps are mostly valid.</p>
<p>However, recent works show that the two criteria are rather complementary, from training sequence classifiers to using critic models.Zhang et al. (2025) trains a sequence classifier by only considering steps that are both valid (judged by critic models) and useful (by MCTS-based rollouts) steps as positive, substantially improving performance over baselines that only consider validity or utility (Figure 4, Sequence Classifiers).Sun et al. (2024); Kim et al. (2025b) has also shown that averaging validity and utility scores from critic models The misalignment between validity and utility is mainly caused by steps that are logically wrong but reach the correct answer (Zheng et al., 2024a;Wang et al., 2025b;Kim et al., 2025b).These invalid but useful steps, also known as unfaithful reasoning of LLMs (Lyu et al., 2023;Schnitzler et al., 2024), might lead to overestimation of reasoning ability (Lyu et al., 2023;Petrov et al., 2025).</p>
<p>Partial context allows efficient and accurate evaluation Validity and coherence evaluate a step based on its previous steps.The most intuitive way is to use the full context (all preceding steps).However, this approach is not feasible when the trace exceeds the context length of the evaluators, e.g., large reasoning models' traces are often too long to apply critic models (Kim et al., 2025b).</p>
<p>An alternative solution is to use a partial context, where only relevant parts of the query and preceding steps are selected and passed to the evaluator (Ling et al., 2023;Mukherjee et al., 2025).These works first construct a directed entailment graph, and evaluate the step only based on the identified premises.This allows evaluators to use shorter context, which is both computationally efficient (Ling et al., 2023) and even more accurate as distractors are removed from the context (Mukherjee et al., 2025) (Figure 4, Partial context).Moreover, the graph structure also distinguishes direct errors (premises are valid but the reasoning is invalid) and accumulated errors (premises are invalid but reasoning is valid) (Mukherjee et al., 2025).</p>
<p>Test-time scaling improves evaluator performance Test-time scaling is a general paradigm where investing more test-time compute leads to improved performance.Test-time compute can be scaled in diverse directions, such as sampling the output multiple times (Wang et al., 2023b;Yao et al., 2023) or generating more tokens during a single inference (Snell et al., 2024;Qwen-Team, 2024;DeepSeek-AI, 2025).</p>
<p>This paradigm can be extended to critic models that reason on reasoning traces, especially in meta-evaluation benchmarks.When applying majority voting of independently sampled K evaluator scores in generative models (critic model, generative verifiers), the accuracy in predicting incorrect steps increases linearly with the scale of log K (Singhi et al., 2025;Kim et al., 2025b;She et al., 2025) (Figure 4, Majority voting).Furthermore, using large reasoning models (LRMs) with stronger reasoning capability by generating longer traces (Zheng et al., 2024a;Kim et al., 2025b;Khalifa et al., 2025) leads to significant improvement in error detection (Figure 4, Scaling (LRMs)).</p>
<p>In verifier-guided search settings, one can either scale exploration or scale evaluation.For instance, in Best-of-N decoding, one can increase the number of responses or use critic models that produce longer outputs.What is the optimal strategy with a constrained computing budget?For relatively weaker evaluators, simple majority voting (Wang et al., 2023b) often outperforms verifier-guided search (Zhang et al., 2025;Singhi et al., 2025).However, using stronger evaluators, e.g., sequence classifiers with better training data (Zhang et al., 2025) or critic models with stronger reasoning capabilities (Khalifa et al., 2025;Kim et al., 2025b) for Best-of-N can effectively outperform majority voting using the same computation budget.</p>
<p>Future directions</p>
<p>Evaluating real-world reasoning traces with external knowledge Existing datasets for reasoning trace evaluation are mostly restricted to simple factual reasoning (e.g.factual multi-hop question answering) or self-contained problems (e.g.math problems).However, many realistic reasoning tasks such as repository-level coding (Zhang et al., 2023a), medicine (Savage et al., 2024), and law (Holzenberger and Van Durme, 2021;Kim et al., 2024b) require external up-to-date knowledge retrieval-augmented generation (Lewis et al., 2020).Developing evaluators and meta-evaluation benchmarks for these tasks will significantly enhance the applicability of reasoning trace evaluation in more realistic scenarios.</p>
<p>Evaluating long, complex reasoning traces Following OpenAI o1 (OpenAI, 2024b), numerous large reasoning models (LRMs) that generate long, complex traces involving self-verification and backtracking were introduced (DeepSeek-AI, 2025; Muennighoff et al., 2025;Gandhi et al., 2025).However, existing evaluators are not suitable for these complex traces.For instance, assigning a single scalar score (e.g., sequence classifiers) will make invalid steps corrected afterwards (Wait, this reasoning is not correct.)and ones not corrected indistinguishable.Since LRM reasoning traces can contain critical errors (Petrov et al., 2025;Chen et al., 2025b), the effort to develop evaluation resources for such traces will lead to a better understanding of LRMs' behaviors and further improvement in their performance and credibility.</p>
<p>Advanced methods for finding premises.NLIbased validity and coherence evaluation significantly benefit from determining the previous steps that the current step uses as a premise (Mukherjee et al., 2025).However, finding such steps is not a trivial task.ROSCOE (Golovneva et al., 2023a) uses the minimum NLI score of all (previous step, current step) combinations, which ignores cases where a step has multiple premises.Recent works (Ling et al., 2023;Tyen et al., 2024;Mukherjee et al., 2025) make the reasoner LLM annotate the premises of the given step.Plausible but underexplored approaches include applying uncertaintybased methods (Chen et al., 2023;Wu et al., 2024a) or training a parser that annotates the logical dependencies between steps as graphs (Lee et al., 2025b).</p>
<p>Symbol-grounded evaluation of reasoning traces</p>
<p>Reasoning tasks often have a symbolic ground truth solution.For instance, deductive reasoning tasks can be represented with formal logic, and arithmetic problems can be expressed as a series of equations or symbolic theorems.These solutions provide precise, formal ways to define evaluators, including validity and utility (progress).However, not much work has been done to exploit the par-allel between reasoning traces and the underlying symbolic solution.While several rule-based approaches parse reasoning traces for evaluation in relatively simpler reasoning tasks (Saparov and He, 2023;Nguyen et al., 2024;Li et al., 2023b), no attempts have been made to extend this paradigm to evaluate reasoning traces for more complex and realistic tasks like first-order logic reasoning (Han et al., 2024a,b) and formal math reasoning that use interactive theorem provers (e.g., Lean, Isabelle) (Yang et al., 2023;Gao et al., 2024c).</p>
<p>Rubric-based evaluation for complex and expert-level tasks.Existing evaluators often apply identical evaluations for all reasoning trace, e.g., using the same LLM-as-a-judge prompt for all inputs.However, as the reasoning tasks require more domain knowledge and expertise, there is an increasing need for highly specific rubrics for evaluating reasoning traces (Kim et al., 2025a).For instance, one can calculate the sum of an arithmetic sequence by adding all terms one by one or finding a general term; the problem-specific rubrics explicitly prefer the latter.However, manual rubric generation is costly and less scalable, which motivates automatic extraction/generation of high-quality reasoning trace rubrics.AutoRace (Hao et al., 2024) aims to generate rubrics automatically based on incorrect responses, while RaR (Gunjal et al., 2025) extracts checklist-style rubrics from ground-truth biomedical documents.Still, automatically obtaining expert-level, high-quality rubrics for more diverse reasoning tasks remains an open question.</p>
<p>Conclusion</p>
<p>This survey aims to organize existing criteria and methods for step-by-step reasoning evaluation, which is crucial for understanding and improving LLM's reasoning capabilities.We provide a unified taxonomy for evaluation criteria, a comprehensive review of existing evaluators and their implementation, and examine recent directions on how to improve these evaluators.</p>
<p>Still, diverse challenges remain in evaluating step-by-step reasoning traces.As new reasoning tasks and methods emerge, existing evaluators often become obsolete for evaluating complex reasoning traces from new tasks and models.As LLMs are now involved in challenging and high-stakes reasoning tasks in the real world, understanding the nature of their errors and precisely evaluating the reasoning trace will remain important.</p>
<p>Limitation</p>
<p>References This survey includes an extensive list of recent publications (mostly between 2022 and 2025) on reasoning trace evaluation, sourced from *ACL, EMNLP, NeurIPS, and arXiv preprints, etc.While there might be missing references due to the sheer volume of works produced in this field, we will continue to update missing references and newly released impactful works that contribute to the field.(1) While most paper report reasoning performance improvement results (Section 2.3), these results are often not directly comparable because they make use of different base model, which strongly affect the overall performance.(2) Other meta-evaluation benchmarks than ProcessBench (Jacovi et al., 2024;Zeng et al., 2024a;Song et al., 2025) have not been applied to diverse evaluator implementations at the time of writing.</p>
<p>Survey on diverse empirical results</p>
<p>A Tasks</p>
<p>This section describes different reasoning tasks and datasets in more detail.While all reasoning tasks fundamentally share the same criteria, literature about a specific task has focused on one criterion over others.For instance, evaluators for factual reasoning tasks often emphasized detecting infactual statements, while evaluators for math reasoning tasks aimed for invalid statements.These discrepancies are one of the fundamental causes of the divergence of the terminologies and definitions in the field.</p>
<p>A.1 Multi-hop Question Answering</p>
<p>Multi-hop question answering (MHQA) tasks require taking information from multiple sources to derive the correct answer (Yang et al., 2018).</p>
<p>MHQA is often divided into two subcategories, factual reasoning and commonsense reasoning.</p>
<p>Answering factual MHQAs can be seen as finding the sequence of bridging entities that leads to the final answer (Yang et al., 2018;Talmor and Berant, 2018;Kwiatkowski et al., 2019).For example, to solve a factual MHQA question "The Argentine PGA Championship record holder has won how many tournaments worldwide?",one must first find who the Argentine PGA championship record holder is (bridging entity) and determine how many tournaments he has won worldwide.As bridging entity identification does not require sophisticated reasoning ability compared to other tasks, reasoning trace evaluation on factual MHQA mostly focuses on the factuality based on semantic alignment between the query (retrieved documents) and the trace (Golovneva et al., 2023a).</p>
<p>In contrast, an inference step in commonsense MHQAs (Clark et al., 2018;Mihaylov et al., 2018;Talmor et al., 2019;Bisk et al., 2019;Geva et al., 2021;Trivedi et al., 2022) can require information that is not present in the query.The form of such commonsense knowledge can be diverse, ranging from well-known facts (Paris is in France.) to logical rules (If A was born after B was dead, they have never met each other).Due to these implicit steps, factuality, validity, and coherence are often hard to separate in evaluating commonsense reasoning traces (Jacovi et al., 2024;Zeng et al., 2024b).Furthermore, due to the inherent subjectiveness of validity and coherence in commonsense reasoning, there might be non-negligible inter-annotator disagreement on certain questions (Jacovi et al., 2024) LLMs are known to achieve strong performance in challenging MHQA datasets such as ARC-Challenge and PIQA, sometimes exceeding human performance (OpenAI, 2024a;Anil et al., 2023).However, multiple studies report that even modern LLMs like GPT-4 (OpenAI, 2024a) are vulnerable to errors, such as failing to correctly adhere to long evidence (Zhu et al., 2024a), leveraging shortcuts (Schnitzler et al., 2024), or ignoring the temporal relation between events (Li et al., 2024a).Therefore, identifying and categorizing mistakes made by LLMs in these tasks is still an important goal.</p>
<p>A.2 Symbolic Reasoning</p>
<p>Since the discovery of Chain-of-thought prompting (Wei et al., 2022;Kojima et al., 2022), step-by-step reasoning largely expanded LLMs' ability to solve symbolic reasoning tasks3 such as mathematical reasoning, logical reasoning, and algorithmic reasoning.As the final answer and the reasoning process are highly objective in these tasks, utility and validity are the two most popular criteria for evaluating reasoning traces from symbolic tasks.</p>
<p>Arithmetic reasoning, where the model has to predict the correct answer from arithmetic word problems, is the most renowned variant of math reasoning.Popular benchmarks include MathQA (Amini et al., 2019) andGSM8k (Cobbe et al., 2021), which provide long, diverse natural language queries in contrast to relatively synthetic, simple benchmarks (Koncel-Kedziorski et al., 2016;Miao et al., 2020).Game of 24 (Yao et al., 2023) and Mathador (Kurtic et al., 2024) ask to combine given numbers and arithmetic operations to generate the target number, requiring exploration and backtracking in the exponential solution space.</p>
<p>The recent saturation of LLMs in arithmetic word problems facilitated more challenging mathematical reasoning benchmarks from math competitions and university textbooks, covering fields like calculus, probability, statistics, geometry, number theory, and more (He et al., 2024a;Gao et al., 2024b;Glazer et al., 2024;Zhang et al., 2024b).While these benchmarks were highly challenging to the state-of-the-art LLMs of the time of release, recently emerging large reasoning models (OpenAI, 2024b; Qwen-Team, 2024; DeepSeek-AI, 2025) achieve unprecedented performance in these benchmarks by generating long reasoning traces often with self-verification and backtracking.</p>
<p>Deductive logical reasoning (Tafjord et al., 2021;Tian et al., 2021;Saparov and He, 2023;Han et al., 2024a) mainly focuses on logical deduction, where one should repeatedly apply the general rules to specific facts as in classical syllogism.Constraint-based reasoning (Zhong et al., 2021;Tyagi et al., 2024) is a variant of deductive reasoning where one must find the solution that satisfies the provided initial constraints (e.g., grid puzzles (Zhong et al., 2021)).As these datasets are easy to solve in a symbolic form like logic programming (Saparov and He, 2023;Pan et al., 2023a;Olausson et al., 2023;Lee and Hwang, 2025) but harder in natural language due to the size of the search space (Kang et al., 2024), they have served as a diagnostic benchmark for understanding and analyzing the complex reasoning capability of large language models (Sinha et al., 2019;Saparov and He, 2023;Han et al., 2024a).However, as these datasets are often synthetically generated from their symbolic representations, they might not fully generalize to real-world problems with linguistic diversity and commonsense.</p>
<p>Finally, algorithmic reasoning tasks include manipulating strings and data structures, such as concatenating the last letters of the given words (Wei et al., 2022) or completing the incomplete Dyck language.BIG-Bench-Hard (BBH; Suzgun et al. (2022)) and NPHardEval (Fan et al., 2024) include 11 and 9 algorithmic reasoning tasks, respectively, which are challenging for modern LLMs like GPT-4 and PaLM-540B.</p>
<p>A.3 Others</p>
<p>Science reasoning tasks lie between factual/commonsense reasoning tasks and symbolic reasoning tasks, as they often require understanding complicated facts combined with world knowledge and performing precise math/logical reasoning (Hendrycks et al., 2021;Rein et al., 2024;He et al., 2024a;Lu et al., 2025).The most popular benchmark in this field, GPQA-Diamond (Rein et al., 2024), contains 546 questions from physics, chemistry, and biology, where human experts only get 65% of the problems correct.</p>
<p>Expert-domain reasoning includes domainspecific reasoning tasks that often require significant expertise in the field, e.g., biomedical reason-ing (uster and Daelemans, 2018;Savage et al., 2024;Zuo et al., 2025), legal reasoning (Holzenberger et al., 2020;Guha et al., 2023;Kim et al., 2024b), and financial reasoning (Chen et al., 2022;Li et al., 2024b).These tasks require both domainspecific knowledge and reasoning strategies, posing a significant challenge to modern language models (Zuo et al., 2025;Li et al., 2024b).However, due to the high cost of expert annotation, existing methods often oversimplify real-world challenges (Holzenberger and Van Durme, 2021;Guha et al., 2023); consequently, the demand for resources that closely reflect real-world expert applications is rising.</p>
<p>Programming/coding is closely related to algorithmic reasoning.Popular benchmarks regarding programming include competitive coding, where one has to solve an algorithm problem given in natural language and test codes (Chen et al., 2021;Li et al., 2022), and practical coding that covers tasks of software engineers and developers (Zhang et al., 2023a;Jimenez et al., 2024;Chan et al., 2024).Programming differs from other reasoning tasks in various aspects: (1) codes are strictly constrained by predefined syntax and semantics, and (2) the result is evaluated by the execution result rather than the code itself.These constraints make (1) segmenting the trace (code) into steps and (2) applying metrics that require explicitly stated answers, i.e., V-information, difficult than in natural language reasoning traces.Therefore, most evaluators specialized in code focus on trace-level utility rather than step-wise evaluation, defined as the pass rate of predefined unit tests (Dai et al., 2025).</p>
<p>B Appendix for Meta-evaluation Datasets</p>
<p>This appendix includes discussions on the dataset construction process, with a focus on data annotation and label types.A summary of existing datasets can be found in Table 4.</p>
<p>B.1 Data collection process</p>
<p>B.1.1 Labeling methods</p>
<p>Human annotation</p>
<p>The most straightforward approach to decide the ground truth label is to use human evalua tion (Lightman et al., 2024;Jacovi et al., 2024;Zeng et al., 2024a;Zheng et al., 2024a).The largest human annotation experiment was conducted by Lightman et al. (2024), where crowdsourced annotators labeled the validity of 800k steps (75k reasoning traces).Due to the sheer volume of annotation, an active learning strategy was used; the annotators were requested to annotate hard samples (the final answer is incorrect but judged as valid by the reward model), which were added to the training data for the next version of the reward model.</p>
<p>LLM annotation</p>
<p>As a cheap alternative for human evaluation, LLM-as-a-judge is often used to generate labels (Gao et al., 2024a;Zhang et al., 2025).However, LLM-assigned labels are not fully credible, given that state-of-the-art LLMs still make errors in human-annotated datasets (Zheng et al., 2024a;Kim et al., 2025b).Therefore, LLMannotated data is often used to augment the training data rather than for meta-evaluation purposes.</p>
<p>Perturbation Another method to create positive and negative samples is to insert errors into correct reasoning traces.For instance, Zhu et al. (2024b); Lu et al. (2024) samples traces that reach the correct answer, and prompts an LLM to introduce a predefined form of perturbation to the reasoning trace.This allows easy sampling of diverse erroneous traces that can improve the robustness of evaluators, but using human-defined errors might not correctly reflect the true distribution of LLMgenerated errors.</p>
<p>Step-level utility Some datasets use step-level utility as their labels.The most prominent approach is Monte Carlo Tree Search (Wang et al., 2024c), where the step-level utility is measured by sampling rollouts from a step and checking if they reach the correct answer.However, to increase the efficiency of the search for negative labels (low utility), Luo et al. (2024b);Dai et al. (2025) implements a binary search algorithm to locate the first step with low utility.One notable variant of step-level utility labels is advantage, where the evaluators are not trained to predict the expected reward of each node but the change in the expected rewards before and after generating the step (Setlur et al., 2024).</p>
<p>Trace-level utility The coarsest label is the tracelevel utility, simply measured by the correctness of the final answer (Lambert et al., 2025).</p>
<p>Both trace-level and step-level utilities do not require human annotation other than the final answer, which is much cheaper to obtain than human annotations (Wang et al., 2024c).However, they cannot serve as a reliable proxy of factuality/coherence/validity due to unfaithful reasoning, where traces that reach the correct answer (high  (Jacovi et al., 2024)  Common FVC 3.4k  MATH-Minos (Gao et al., 2024a)  Math V 440k  SCDPO (Lu et al., 2024)  Math U 30k  MR-GSM8k (Zeng et al., 2024a)  Math V 3.0k  BIG-Bench-Mistake (Tyen et al., 2024)  Symbolic VCU 2.2k  CriticBench (Lin et al., 2024)  Math, Common, Symbolic VU 3.8k  ProcessBench (Zheng et al., 2024a)  Math V 3.4k  MR-Ben (Zeng et al., 2024b)  Science, Deductive, Coding V 6.0k  MR-MATH (Xia et al., 2025)  Math VU 0.1k utility) often include factual/logical errors (Lanham et al., 2023;Lyu et al., 2023;Zheng et al., 2024a;Kim et al., 2025b).</p>
<p>B.1.2 Inter-annotator agreement</p>
<p>While reasoning trace evaluation is considered more objective than other long-text evaluation tasks (e.g., helpfulness, bias/harmfulness, and language proficiency) (Wang et al., 2024a), a certain amount of inter-annotator disagreement is inevitable.Here, we report the trend in inter-annotator agreement observed in existing human annotation works.</p>
<p>Incorrect solutions for harder problems lead to higher disagreement ProcessBench (Zheng et al., 2024a) consolidates the intuitive hypothesis that inter-annotator disagreement grows when the query is difficult and the trace is incorrect in at least one step.Compared to the easiest case (GSM8k queries, correct trace), where three annotators agree in 95.9% of the cases, the hardest case (OmniMATH, incorrect trace) shows only 47.8% of three-annotator agreement.</p>
<p>Inter-annotator disagreement reflects vagueness in natural language In many cases, the disagreement is significantly affected by the linguistic aspects of the reasoning trace.REVEAL (Jacovi et al., 2024) manually classifies steps that aroused disagreement between annotators into 13 distinct categories.Among these, frequent disagreement types like "World knowledge (some world knowledge might not be taken for granted)" and "Unclear reference (one proper noun can refer to multiple real-world entities)" are typical disagreement types observed in simpler recognizing textual entailment (natural language inference) tasks (Camburu et al., 2018;Lee et al., 2025a), showing that these vagueness is present even in minimal settings.On the other hand, synthetic, algorithmic reasoning tasks like BIG-Bench-Hard (Suzgun et al., 2022) are linguistically uniform.Consequently, BIG-Bench-Mistake that annotate errors in this benchmark (Tyen et al., 2024) observes nearperfect inter-annotator agreement (Krippendorf's  &gt; 0.97), again demonstrating the strong connection between linguistic variation and interannotator agreement.</p>
<p>B.2 Label types</p>
<p>Sequence classification The most common label type is sequence classification, where a quality label is assigned to each step/trace.For example, Wang et al. (2024c) assigns binary labels to steps based on the utility, and Lightman et al. ( 2024) assigns ternary validity labels (correct/incorrect/neutral) obtained by human annotation.The neutral label in Lightman et al. ( 2024) was introduced to absorb ambiguous cases and minimize inter-annotator disagreement; considering it as positive or negative when training the evaluator does not significantly affect the Best-of-N performance (&lt;1.0p).</p>
<p>One caveat of sequence classification is that it is hard to define the labels after the first error (propagated error).It is often unclear whether steps that rely on the first erroneous step should be labeled as incorrect (because they rely on incorrect premises) or correct (because the reasoning is correct if assuming the premises are correct) (Jacovi et al., 2024;Mukherjee et al., 2025).Two different label schemas are used to bypass this ambiguity: annotating the pairwise preference and annotating the index of the first erroneous step.</p>
<p>Preference (win/lose) Reasoning trace evaluation can be formulated as a preference problem (Lai et al., 2024;Lu et al., 2024;Lambert et al., 2025).In this scenario, data points are defined as pairs of reasoning traces, one as the winner and the other as the loser.The pairs are often constructed by sampling two different continuations from a shared prefix or perturbing a correct trace.These data are often used to train the LLM-as-a-valuefunction models via preference learning algorithms, e.g., DPO (Rafailov et al., 2023).</p>
<p>Identifying first erroneous index Another method is to label the index of the first erroneous step (Zheng et al., 2024a;Zeng et al., 2024a).In this setting, the reasoning trace is given as a list of steps, and the evaluator must predict the index of the first error.If there is no error, the model should predict -1.This setting effectively bypasses the propagated error problem, but converting these labels to binary classification can lead to better performance in sequence classifiers and critic models (Kim et al., 2025b).</p>
<p>C Comparing criteria definitions C.1 Comparison between proposed definitions</p>
<p>FactualityValidity Factuality focuses on the relationship between a step and provided/external knowledge, while validity focuses on the relationship between two model-generated steps.For instance, Given an incorrect step Albert Einstein died in 1965 (he died in 1955), this step is not factual if the query explicitly mentions that Einstein died in 1955.Apart from that, if the previous steps provide the premises for reaching 1955, i.e.Einstein was born in 1879, and he died at the age of 76, the step is invalid.</p>
<p>While the standard practice is to treat factuality and validity separately (Prasad et al., 2023;Zhu et al., 2024b;Jacovi et al., 2024), the boundary between stating facts and making logical inferences is often vague, especially in commonsense reasoning.For example, if the step states Einstein died between 1960 and 1970 when given the information Einstein died in 1955, is this step a factual error or a logical error?The boundary heavily relies on the definition of what can be taken as granted, which is also a key factor in defining coherence.RE-VEAL (Jacovi et al., 2024) delegates the decision to human annotators, and shows that LLMs (Anil et al., 2023;Brown et al., 2020) perform poorly (F1&lt;0.65)at classifying the steps between factual statements and logical inference.</p>
<p>ValidityCoherence Existing works often treat coherence as a subtype of validity (Golovneva et al., 2023a;Zhu et al., 2024b;Kim et al., 2025a;Jacovi et al., 2024), as both criteria judge a step based on its previous steps.However, validity and coherence are different by definition, as validity focuses on the logical correctness of a step while coherence focuses on the pragmatic aspect of informativeness.For instance (Figure 3-Coherence), omitting a step (Step 3) from the correct trace will make the subsequent step (Step 3') incoherent, but it is still valid since it can be eventually deduced from the query and previous steps.</p>
<p>ValidityUtility Previous studies have continuously pointed out that validity does not necessarily lead to utility and vice versa (Lyu et al., 2023;Nguyen et al., 2024).One case is shortcut reasoning (Schnitzler et al., 2024;Lee and Hwang, 2025), where LLM generates invalid Chain-of-thoughts but guesses the correct answer directly from the query.ProcessBench (Zheng et al., 2024a) reports that invalid traces with correct answers can be easily found in challenging problems, reaching 51.8% in the olympiad-level Omni-MATH (Gao et al., 2024b).</p>
<p>The distinction between validity and utility has been highlighted by multiple empirical results.Treating these metrics as different yields substantial performance gain when training sequence classifiers (Zhang et al., 2025) and in Best-of-N decoding  (Sun et al., 2024;Kim et al., 2025b).See Section 6 for details.</p>
<p>C.2 Comparison to other definitions</p>
<p>Hallucination is most commonly defined as "models either generating (1) nonsensical or (2) unfaithful to the source content" (Ji et al., 2023;Banerjee et al., 2024;Huang et al., 2024b), which corresponds to (1) validity/coherence and (2) factuality.However, some works restrict the meaning of hallucination to factual errors, i.e. "models generating description tokens that are not supported by the source inputs" (Xiao and Wang, 2021;Akbar et al., 2024).</p>
<p>Faithfulness is also used with different senses.The most common definition for faithfulness is "logical consistency between the generated text and the query/previous steps" (Maynez et al., 2020;Creswell and Shanahan, 2022;Huang et al., 2024b), which includes both factuality (query groundedness) and validity (previous step).Instead, faithfulness can be used as "accurately representing the model's internal reasoning process" (Lyu et al., 2023;Lanham et al., 2023).Under this definition, the final step containing the answer is unfaithful if it is not supported by the previous steps, which falls under the definition of coherence.</p>
<p>Informativeness is defined as "providing new information that is helpful towards deriving the generated answer" (Golovneva et al., 2023b;Prasad et al., 2023).Lack of informativeness is often described as redundancy "removing the step does not affect the reasoning process" (Chiang and Lee, 2024;Song et al., 2025;Zhou et al., 2024) or ir-relevance "unrelated to the query's topic or task" (Wang et al., 2023a;Zhou et al., 2024;Jacovi et al., 2024).Informativeness is synonymous with utility, as it aims to evaluate the contribution of a step to reaching the final answer.</p>
<p>D Details for Section 6</p>
<p>This section provides further details regarding the Section 6, specifically Figure 4.</p>
<p>D.1 Estimating Compute</p>
<p>To estimate the compute in Figure 4, we follow the approximation equation from Snell et al. (2024); Kim et al. (2025b).Specifically, the computational cost can be asymptotically approximated as
C  O(N  L),
where C is the total computational cost, N is the number of parameters, and L is the number of tokens.Note that since all compared evaluators use the same base model, N remains constant.</p>
<p>Below, we describe how the computation budget for each method is calculated in Figure 4:  Unit relative compute (1) corresponds to a single forward pass for an average-length trace.This applies to Fine-tuned Sequence Classifiers, as they take the whole trace as the input.</p>
<p> The Base Model, Fine-tuned Critic Models (She et al., 2025), and Fine-tuned LRMs evaluate each step with a separate forward pass.Thus, the compute is scaled by the number of steps per trace, which is 6.11 on average in ProcessBench (GSM8k + MATH).Note that LRMs like DeepSeek-Distill-Qwen-2.5-7B (DeepSeek-AI, 2025) generate significantly longer traces, with L scaled by 7.58.</p>
<p> PARC (Mukherjee et al., 2025) also uses stepwise critic evaluations, but only using the Partial context (average 1.57 premises per step) makes PARC require lower compute by reducing L.</p>
<p> In Majority Voting setting, where 8 step-wise evaluations are sampled per step and aggregated via majority voting, the total computation cost is multiplied by 8.</p>
<p>Figure 1 :
1
Figure 1: Illustrative example of reasoning trace evaluation.</p>
<p>Figure 3 :
3
Figure 3: Illustration of the proposed categories of step-by-step reasoning evaluation criteria, i.e. factuality, validity, coherence, and utility.The left shows an example of a query and a reasoning trace.The other four blocks demonstrate examples that fail to satisfy the respective metric.Red filled rectangles indicate the error's location, and the outlined boxes and arrows show the cause of the error.The trace example is originally from Lightman et al. (2024).</p>
<p>Chen et al. (2023); Prasad et al. (</p>
<p>Figure 4 :
4
Figure4: Plot of different evaluators introduced in Section 6, plotted by ProcessBench performance(Zheng et al., 2024a) (GSM8k, MATH subsets averaged) versus total compute for evaluating a trace.While these evaluators share the same base model (Qwen-2.5-7B),they improve the base model's trace evaluation capability in different ways.Details can be found in Appendix D.</p>
<p>While Figure 4 contains a controlled comparison between different approaches like training sequence classifier with different data, using partial context, or applying test-time scaling techniques, the comparison is limited to ProcessBench results for two reasons:</p>
<p>Figure 5 :
5
Figure 5: A Sankey diagram displaying the relationship between commonly used terminologies (left) to the proposed taxonomy (right).</p>
<p>Table 1 :
1
, as List of evaluator training data and meta-evaluation benchmarks.symbolindicates that the datasets include other tasks, such as summarization, instruction following, etc, where the # Trace column only counts the reasoning subset.Train/Eval columns denote if the dataset is used for training or meta-evaluation.Domain indicates what tasks are used to sample the reasoning trace.Criteria column shows the criteria used to annotate the data classified according to Section 3, where FVCU stands for factuality, validity, coherence, and utility, respectively.BiGGenBench(Kim et al., 2025a)applies hand-written, query-specific evaluation criteria (Custom).Human column indicates human annotation, where    denotes full human annotation, automatic annotation/perturbation with human verification, and full LLM-based annotation, respectively.
DatasetTrain Eval DomainCriteria # Trace HumanROSCOE (Golovneva et al., 2023b)Math, CommonFVU1.0kRAGTruth   (Niu et al., 2024)FactF5.9kHaluEval   (Li et al., 2023a)FactF10kMath-Shepherd (Wang et al., 2024c)MathU440kPRM800k (Lightman et al., 2024)MathV75kREVEAL (Jacovi et al., 2024)CommonFVC3.4kMATH-Minos (Gao et al., 2024a)MathV440kSCDPO (Lu et al., 2024)MathU30kMR-GSM8k (Zeng et al., 2024a)MathV3.0kBIG-Bench-Mistake (Tyen et al., 2024)SymbolicVCU2.2kCriticBench (Lin et al., 2024)Math, Common, SymbolicVU3.8kProcessBench (Zheng et al., 2024a)MathV3.4kMR-Ben (Zeng et al., 2024b)Science, Deductive, CodingV6.0kMR-MATH (Xia et al., 2025)MathVU0.1kPRMBench (Song et al., 2025)MathVCU6.2kPRM-Clinic (Wang et al., 2025a)Expert(Clinic)FVC9.7kVersaPRM (Zeng et al., 2025)ExpertFV84.1kBiGGenBench   (Kim et al., 2025a)Math, LogicCustom0.1k</p>
<p>Table 4 :
4
List of evaluator training data and meta-evaluation benchmarks.symbolindicates that the datasets include other tasks, such as summarization, instruction following, etc, where the # Trace column only counts the reasoning subset.Train/Eval columns denote if the dataset is used for training or meta-evaluation.Domain indicates what tasks are used to sample the reasoning trace.Criteria column shows the criteria used to annotate the data classified according to Section 3, where FVCU stands for factuality, validity, coherence, and utility, respectively.BiGGenBench(Kim et al., 2025a)applies hand-written, query-specific evaluation criteria (Custom).Human column indicates human annotation, where    denotes full human annotation, automatic annotation/perturbation with human verification, and full LLM-based annotation, respectively.

While symbolic reasoning may strictly refer to algorithmic reasoning in some literature(Wei et al., 2022;Suzgun et al., 2022), we adopt the broader sense including math and logical reasoning that can be readily expressed in symbols (e.g., equation, logic)(Sprague et al., 2024).
AcknowledgementsWe thank Sagnik Mukherjee for his valuable help in revising the paper and sharing data for PARC in Section 6, which greatly improved the completeness of this work.critics help catch bugs in mathematics: Towards a better mathematical verifier with natural language feedback.Preprint, arXiv:2406.14024.and redundant calculation of large language models.In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 2: Short Papers), pages 161-169, St. Julian's, Malta.Association for Computational Linguistics.D.2 Data source For Base model and Majority voting scores, authors conducted experiments with Qwen-2.5-7B-Instructusing the code fromKim et al. (2025b).Fine-tuned LRM scores are as reported in the same paper. Sequence classifiers scores are obtained fromZhang et al. (2025). Partial context scores are provided by the authors of PARC(Mukherjee et al., 2025), upon requested by the authors of this survey.While the currently available version of the paper does not contain the result, it will appear in the published version. Fine-tuned Critic Model scores are fromShe et al. (2025).
HalluMeasure: Fine-grained hallucination measurement using chain-of-thought reasoning. Shayan Ali Akbar, Md Mosharaf Hossain, Tess Wood, Si-Chi Chin, Erica M Salinas, Victor Alvarez, Erwin Cornejo, 10.18653/v1/2024.emnlp-main.837Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational Linguistics2024</p>
<p>MathQA: Towards interpretable math word problem solving with operation-based formalisms. Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, Hannaneh Hajishirzi, 10.18653/v1/N19-1245Proceedings of the 2019 Conference of the North American Chapter. the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American ChapterMinneapolis, MinnesotaAssociation for Computational Linguistics20191Long and Short Papers</p>
<p>. Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A Choquette-Choo, Aakanksha Chowdhery, Clment Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Daz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, Yaguang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, ; Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, arXiv:2305.10403Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin XuSlav PetrovYunhan Xu, Linting Xue, Pengcheng Yin,Preprintand Yonghui Wu. 2023. Palm 2 technical report</p>
<p>Critique-out-loud reward models. Zachary Ankner, Mansheej Paul, Brandon Cui, Jonathan D Chang, Prithviraj Ammanabrolu, arXiv:2408.117912024Preprint</p>
<p>Llms will always hallucinate, and we need to live with this. Sourav Banerjee, Ayushi Agarwal, Saloni Singla, arXiv:2409.057462024Preprint</p>
<p>Yonatan Bisk, Rowan Zellers, Le Ronan, Jianfeng Bras, Yejin Gao, Choi, arXiv:1911.11641Piqa: Reasoning about physical commonsense in natural language. 2019Preprint</p>
<p>A large annotated corpus for learning natural language inference. R Samuel, Gabor Bowman, Christopher Angeli, Christopher D Potts, Manning, 10.18653/v1/D15-1075Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingLisbon, PortugalAssociation for Computational Linguistics2015</p>
<p>. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind</p>
<p>Retrieval-augmented generation for large language models: A survey. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, Haofen Wang, arXiv:2312.109972024dPreprint</p>
<p>Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, Jonathan Berant, 10.1162/tacl_a_00370Transactions of the Association for Computational Linguistics. 92021</p>
<p>Frontiermath: A benchmark for evaluating advanced mathematical reasoning in ai. Elliot Glazer, Ege Erdil, Tamay Besiroglu, Diego Chicharro, Evan Chen, Alex Gunning, Caroline Falkman Olsson, Jean-Stanislas Denain, Anson Ho, Emily De Oliveira Santos, Olli Jrviniemi, Matthew Barnett, Robert Sandler, Matej Vrzala, Jaime Sevilla, arXiv:2411.04872Qiuyu Ren, Elizabeth Pratt, Lionel Levine, Grant Barkley, Natalie Stewart, Bogdan Grechuk, Tetiana Grechuk, Shreepranav Varma Enugandla, and Mark Wildon2024Preprint</p>
<p>ROSCOE: A suite of metrics for scoring step-by-step reasoning. Olga Golovneva, Moya Chen, Spencer Poff, Martin Corredor, Luke Zettlemoyer, Maryam Fazel-Zarandi, Asli Celikyilmaz, The Eleventh International Conference on Learning Representations, ICLR 2023. Kigali, Rwanda2023a. May 1-5, 2023OpenReview.net</p>
<p>Pathfinder: Guided search over multi-step reasoning paths. Olga Golovneva, Sean O 'brien, Ramakanth Pasunuru, Tianlu Wang, Luke Zettlemoyer, Maryam Fazel-Zarandi, Asli Celikyilmaz, arXiv:2312.051802023bPreprint</p>
<p>Search, verify and feedback: Towards next generation post-training paradigm of foundation models via verifier engineering. Xinyan Guan, Yanjiang Liu, Xinyu Lu, Boxi Cao, Ben He, Xianpei Han, Le Sun, Jie Lou, Bowen Yu, Yaojie Lu, Hongyu Lin, arXiv:2411.115042024Preprint</p>
<p>Neel Guha, Julian Nyarko, Daniel E Ho, Christopher R, Adam Chilton, Aditya Narayana, Alex Chohlas-Wood, Austin Peters, Brandon Waldon, Daniel N Rockmore, Diego Zambrano, Dmitry Talisman, Enam Hoque, Faiz Surani, Frank Fagan, Galit Sarfaty, Gregory M Dickinson, Haggai Porat, Jason Hegland, Jessica Wu, Joe Nudell, Joel Niklaus, John Nay, Jonathan H Choi, Kevin Tobia, Margaret Hagan, Megan Ma, Michael Livermore, Nikon Rasumov-Rahe, Nils Holzenberger, Noam Kolt, Peter Henderson, Sean Rehaag, Sharad Goel, Shang Gao, Spencer Williams, Sunny Gandhi, Tom Zur, Varun Iyer, Zehua Li, arXiv:2308.11462Legalbench: A collaboratively built benchmark for measuring legal reasoning in large language models. 2023Preprint</p>
<p>Anisha Gunjal, Anthony Wang, Elaine Lau, Vaskar Nath, Bing Liu, Sean Hendryx, arXiv:2507.17746Rubrics as rewards: Reinforcement learning beyond verifiable domains. 2025Preprint</p>
<p>Arman Cohan, and Dragomir Radev. 2024a. FOLIO: Natural language reasoning with first-order logic. Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Wenfei Zhou, James Coady, David Peng, Yujie Qiao, Luke Benson, Lucy Sun, Alexander Wardle-Solano, Hannah Szab, Ekaterina Zubova, Matthew Burtell, Jonathan Fan, Yixin Liu, Brian Wong, Malcolm Sailor, Ansong Ni, Linyong Nan, Jungo Kasai, Tao Yu, Rui Zhang, Alexander Fabbri, Wojciech Maciej Kryscinski, Semih Yavuz, Ye Liu, Xi Victoria Lin, Shafiq Joty, Yingbo Zhou, Caiming Xiong, Rex Ying, 10.18653/v1/2024.emnlp-main.1229Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational Linguistics</p>
<p>P-FOLIO: Evaluating and improving logical reasoning with abundant humanwritten reasoning chains. Simeng Han, Aaron Yu, Rui Shen, Zhenting Qi, Martin Riddell, Wenfei Zhou, Yujie Qiao, Yilun Zhao, Semih Yavuz, Ye Liu, Shafiq Joty, Yingbo Zhou, Caiming Xiong, Dragomir Radev, Rex Ying, Arman Cohan, 10.18653/v1/2024.findings-emnlp.966Findings of the Association for Computational Linguistics: EMNLP 2024. Miami, Florida, USAAssociation for Computational Linguistics2024b</p>
<p>Llm reasoners: New evaluation, library, and analysis of step-by-step reasoning with large language models. Shibo Hao, Yi Gu, Haotian Luo, Tianyang Liu, Xiyan Shao, Xinyuan Wang, Shuhua Xie, Haodi Ma, Adithya Samavedhi, Qiyue Gao, Zhen Wang, Zhiting Hu, arXiv:2404.052212024Preprint</p>
<p>OlympiadBench: A challenging benchmark for promoting AGI with olympiad-level bilingual multimodal scientific problems. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, Maosong Sun, 10.18653/v1/2024.acl-long.211Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics2024a</p>
<p>Advancing process verification for large language models via tree-based preference learning. Mingqian He, Yongliang Shen, Wenqi Zhang, Zeqi Tan, Weiming Lu, 10.18653/v1/2024.emnlp-main.125Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational Linguistics2024b</p>
<p>Measuring massive multitask language understanding. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, arXiv:2009.033002021Preprint</p>
<p>Conditional probing: measuring usable information beyond a baseline. John Hewitt, Kawin Ethayarajh, Percy Liang, Christopher Manning, 10.18653/v1/2021.emnlp-main.122Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican Republic. Association for Computational Linguistics2021Online and Punta Cana</p>
<p>A dataset for statutory reasoning in tax law entailment and question answering. Nils Holzenberger, Andrew Blair-Stanek, Benjamin Van Durme, arXiv:2005.052572020Preprint</p>
<p>Factoring statutory reasoning as language understanding challenges. Nils Holzenberger, Benjamin Van Durme, 10.18653/v1/2021.acl-long.213Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline. Association for Computational Linguistics20211</p>
<p>Uncertainty of thoughts: Uncertainty-aware planning enhances information seeking in llms. Arian Hosseini, Xingdi Yuan, Nikolay Malkin, Aaron Courville, Alessandro Sordoni, Rishabh Agarwal ; Zhiyuan, Chumin Hu, Xidong Liu, Yilun Feng, See-Kiong Zhao, Anh Tuan Ng, Junxian Luu, Pang He, Bryan Wei W Koh, Hooi, arXiv:2402.06457Advances in Neural Information Processing Systems. 372024. 2024PreprintV-star: Training verifiers for self-taught reasoners</p>
<p>Large language models cannot self-correct reasoning yet. Jie Huang, Xinyun Chen, Swaroop Mishra, Steven Huaixiu, Adams Wei Zheng, Xinying Yu, Denny Song, Zhou, arXiv:2310.017982024aPreprint</p>
<p>A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, ACM Transactions on Information Systems. 2024b</p>
<p>A chain-of-thought is as strong as its weakest link: A benchmark for verifiers of reasoning chains. Alon Jacovi, Yonatan Bitton, Bernd Bohnet, Jonathan Herzig, Or Honovich, Michael Tseng, Michael Collins, Roee Aharoni, Mor Geva, 10.18653/v1/2024.acl-long.254Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics20241</p>
<p>Survey of hallucination in natural language generation. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye , Jin Bang, Andrea Madotto, Pascale Fung, ACM Computing Surveys. 55122023</p>
<p>Kexin Pei, Ofir Press, and Karthik Narasimhan. Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, arXiv:2310.06770Swe-bench: Can language models resolve real-world github issues? Preprint. 2024</p>
<p>On the empirical complexity of reasoning and planning in llms. Liwei Kang, Zirui Zhao, David Hsu, Wee Sun, Lee , arXiv:2404.110412024Preprint</p>
<p>Muhammad Khalifa, Rishabh Agarwal, Lajanugen Logeswaran, Jaekyeom Kim, Hao Peng, Moontae Lee, Honglak Lee, Lu Wang, arXiv:2504.16828Process reward models that think. ess reward models that think2025Preprint</p>
<p>Prometheus: Inducing finegrained evaluation capability in language models. Seungone Kim, Jamin Shin, Yejin Choi, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, Minjoon Seo, The Twelfth International Conference on Learning Representations, ICLR 2024. Vienna, Austria2024a. May 7-11, 2024OpenReview.net</p>
<p>Graham Neubig, Moontae Lee, Kyungjae Lee, and Minjoon Seo. 2025a. The BiGGen bench: A principled benchmark for fine-grained evaluation of language models with language models. Seungone Kim, Juyoung Suk, Ji Yong Cho, Shayne Longpre, Chaeeun Kim, Dongkeun Yoon, Guijin Son, Yejin Cho, Sheikh Shafayat, Jinheon Baek, Sue , Hyun Park, Hyeonbin Hwang, Jinkyung Jo, Hyowon Cho, Haebin Shin, Seongyun Lee, Hanseok Oh, Noah Lee, Namgyu Ho, June Se, Miyoung Joo, Yoonjoo Ko, Hyungjoo Lee, Jamin Chae, Joel Shin, Seonghyeon Jang, Bill Ye, Sean Yuchen Lin, Welleck, Proceedings of the 2025 Conference of the Nations of the Americas Chapter. Long Papers. the 2025 Conference of the Nations of the Americas ChapterAlbuquerque, New MexicoAssociation for Computational Linguistics1</p>
<p>Scaling evaluation-time compute with reasoning models as process evaluators. Seungone Kim, Ian Wu, Jinu Lee, Xiang Yue, Seongyun Lee, Mingyeong Moon, Kiril Gashteovski, Carolin Lawrence, Julia Hockenmaier, Graham Neubig, Sean Welleck, arXiv:2503.198772025bPreprint</p>
<p>Developing a pragmatic benchmark for assessing Korean legal language understanding in large language models. Yeeun Kim, Youngrok Choi, Eunkyung Choi, Jinhwan Choi, Hai , Jin Park, Wonseok Hwang, 10.18653/v1/2024.findings-emnlp.319Findings of the Association for Computational Linguistics: EMNLP 2024. Miami, Florida, USAAssociation for Computational Linguistics2024b</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, ( Shixiang, Machel Shane) Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in Neural Information Processing Systems. Curran Associates, Inc202235</p>
<p>MAWPS: A math word problem repository. Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, Hannaneh Hajishirzi, 10.18653/v1/N16-1136Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSan Diego, CaliforniaAssociation for Computational Linguistics2016</p>
<p>Semantic entropy probes: Robust and cheap hallucination detection in llms. Jannik Kossen, Jiatong Han, Muhammed Razzak, Lisa Schut, Shreshth Malik, Yarin Gal, arXiv:2406.159272024Preprint</p>
<p>Mathador-LM: A dynamic benchmark for mathematical reasoning on large language models. Eldar Kurtic, Amir Moeini, Dan Alistarh, 10.18653/v1/2024.emnlp-main.946Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational Linguistics2024</p>
<p>Natural questions: A benchmark for question answering research. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M Dai, Jakob Uszkoreit, Quoc Le, Slav Petrov, 10.1162/tacl_a_00276Transactions of the Association for Computational Linguistics. 72019</p>
<p>Xin Lai, Zhuotao Tian, Yukang Chen, Senqiao Yang, Xiangru Peng, Jiaya Jia, arXiv:2406.18629Step-dpo: Step-wise preference optimization for long-chain reasoning of llms. 2024Preprint</p>
<p>RewardBench: Evaluating reward models for language modeling. Nathan Lambert, Valentina Pyatkin, Jacob Morrison, Lester James Validad, Bill Miranda, Khyathi Yuchen Lin, Nouha Chandu, Sachin Dziri, Tom Kumar, Yejin Zick, Noah A Choi, Hannaneh Smith, Hajishirzi, Findings of the Association for Computational Linguistics: NAACL 2025. Albuquerque, New MexicoAssociation for Computational Linguistics2025</p>
<p>Measuring faithfulness in chainof-thought reasoning. Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, Kamil Lukoit, Karina Nguyen, Newton Cheng, Nicholas Joseph, Nicholas Schiefer, Oliver Rausch, Robin Larson, Sam Mccandlish, Sandipan Kundu, Saurav Kadavath, Shannon Yang, Thomas Henighan, ; Samuel, R Bowman, Ethan Perez, arXiv:2307.13702Jan Brauner,. 2023Timothy Maxwell, Timothy Telleen-Lawton, Tristan Hume, Zac Hatfield-Dodds, Jared KaplanPreprint</p>
<p>Jinu Lee, Wonseok Hwang, arXiv:2402.12806Symba: Symbolic backward chaining for structured natural language reasoning. 2025Preprint</p>
<p>Entailmentpreserving first-order logic representations in natural language entailment. Jinu Lee, Qi Liu, Runzhi Ma, Vincent Han, Ziqi Wang, Ji Heng, Julia Hockenmaier, arXiv:2502.167572025aPreprint</p>
<p>Reasoningflow: Semantic structure of complex reasoning traces. Jinu Lee, Sagnik Mukherjee, Dilek Hakkani-Tur, Julia Hockenmaier, arXiv:2506.025322025bPreprint</p>
<p>Retrieval-augmented generation for knowledgeintensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kttler, Mike Lewis, Wen-Tau Yih, Tim Rocktschel, Sebastian Riedel, Douwe Kiela, Advances in Neural Information Processing Systems. Curran Associates, Inc202033</p>
<p>Halueval: A largescale hallucination evaluation benchmark for large language models. Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, Ji-Rong Wen, arXiv:2305.117472023aPreprint</p>
<p>Meqa: A benchmark for multi-hop eventcentric question answering with explanations. Ruosen Li, Zimu Wang, Son Tran, Lei Xia, Xinya Du, Advances in Neural Information Processing Systems. Curran Associates, Inc2024a37</p>
<p>Al-phaFin: Benchmarking financial analysis with retrieval-augmented stock-chain framework. Xiang Li, Zhenyu Li, Chen Shi, Yong Xu, Qing Du, Mingkui Tan, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024). the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)Torino, ItaliaELRA and ICCLJun Huang. 2024b</p>
<p>Making language models better reasoners with step-aware verifier. Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, Weizhu Chen, 10.18653/v1/2023.acl-long.291Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023b1</p>
<p>Competition-level code generation with alphacode. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rmi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Science. 37866242022</p>
<p>Let's verify step by step. Vineet Hunter Lightman, Yuri Kosaraju, Harrison Burda, John Edwards ; Leike, Ilya Schulman, Karl Sutskever, Cobbe, The Twelfth International Conference on Learning Representations, ICLR 2024. Bowen Baker, Teddy Lee; Vienna, AustriaJan. 2024. May 7-11, 2024</p>
<p>Criticbench: Benchmarking llms for critique-correct reasoning. Zicheng Lin, Zhibin Gou, Tian Liang, Ruilin Luo, Haowei Liu, Yujiu Yang, arXiv:2402.148092024Preprint</p>
<p>Deductive verification of chain-of-thought reasoning. Zhan Ling, Yunhao Fang, Xuanlin Li, Zhiao Huang, Mingu Lee, Roland Memisevic, Hao Su, Advances in Neural Information Processing Systems. Curran Associates, Inc202336</p>
<p>Roberta: A robustly optimized bert pretraining approach. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, arXiv:1907.116922019Preprint</p>
<p>Scp-116k: A high-quality problem-solution dataset and a generalized pipeline for automated extraction in the higher education science domain. Dakuan Lu, Xiaoyu Tan, Rui Xu, Tianchu Yao, Chao Qu, Wei Chu, Yinghui Xu, Yuan Qi, arXiv:2501.155872025Preprint</p>
<p>Zimu Lu, Aojun Zhou, Ke Wang, Houxing Ren, Weikang Shi, Junting Pan, Mingjie Zhan, Hongsheng Li, arXiv:2407.00782Step-controlled dpo: Leveraging stepwise error for enhanced mathematical reasoning. 2024Preprint</p>
<p>Hallucination detection and hallucination mitigation: An investigation. Junliang Luo, Tianyu Li, Di Wu, Michael Jenkin, Steve Liu, Gregory Dudek, arXiv:2401.083582024aarXiv preprint</p>
<p>Improve mathematical reasoning in language models by automated process supervision. Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Meiqi Guo, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng, Jiao Sun, Abhinav Rastogi, arXiv:2406.065922024bPreprint</p>
<p>Faithful chain-ofthought reasoning. Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, Chris Callison-Burch, 10.18653/v1/2023.ijcnlp-main.20Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter. the Association for Computational Linguistics. the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific ChapterBaliNusa Dua20231Long Papers. Association for Computational Linguistics</p>
<p>Dakota Mahan, Duy Van Phung, Rafael Rafailov, Chase Blagden, Nathan Lile, Louis Castricato, Jan-Philipp Frnken, Chelsea Finn, Alon Albalak, arXiv:2410.12832Generative reward models. 2024Preprint</p>
<p>On faithfulness and factuality in abstractive summarization. Joshua Maynez, Shashi Narayan, Bernd Bohnet, Ryan Mcdonald, arXiv:2005.006612020Preprint</p>
<p>A diverse corpus for evaluating and developing English math word problem solvers. Chao-Chun Shen-Yun Miao, Keh-Yih Liang, Su, 10.18653/v1/2020.acl-main.92Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>Can a suit of armor conduct electricity? a new dataset for open book question answering. Todor Mihaylov, Peter Clark, Tushar Khot, Ashish Sabharwal, arXiv:1809.027892018Preprint</p>
<p>Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models. Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, Mehrdad Farajtabar, arXiv:2410.052292024Preprint</p>
<p>Emmanuel Cands, and Tatsunori Hashimoto. 2025. s1: Simple test-time scaling. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang , Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, arXiv:2501.19393Preprint</p>
<p>Premise-augmented reasoning chains improve error identification in math reasoning with llms. Sagnik Mukherjee, Abhinav Chinta, Takyoung Kim, Tarun Anoop Sharma, Dilek Hakkani-Tr, arXiv:2502.023622025Preprint</p>
<p>Direct evaluation of chain-of-thought in multi-hop reasoning with knowledge graphs. Thi Nguyen, Linhao Luo, Fatemeh Shiri, Dinh Phung, Yuan-Fang Li, Thuy-Trang Vu, Gholamreza Haffari, 10.18653/v1/2024.findings-acl.168Findings of the Association for Computational Linguistics: ACL 2024. Bangkok, ThailandAssociation for Computational Linguistics2024</p>
<p>Ragtruth: A hallucination corpus for developing trustworthy retrieval-augmented language models. Cheng Niu, Yuanhao Wu, Juno Zhu, Siliang Xu, Kashun Shum, Randy Zhong, Juntong Song, Tong Zhang, arXiv:2401.003962024Preprint</p>
<p>LINC: A neurosymbolic approach for logical reasoning by combining language models with first-order logic provers. Theo Olausson, Alex Gu, Ben Lipkin, Cedegao Zhang, Armando Solar-Lezama, Joshua Tenenbaum, Roger Levy, 10.18653/v1/2023.emnlp-main.313Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>arXiv:2303.08774OpenAI. 2024a. Gpt-4 technical report. Preprint</p>
<p>arXiv:2412.16720Openai o1 system card. Preprint. 2024bOpenAI</p>
<p>Thoughtsource: A central hub for large language model reasoning data. Simon Ott, Konstantin Hebenstreit, Valentin Livin, Egeberg Christoffer, Milad Hother, Maximilian Moradi, Robert Mayrhauser, Ole Praas, Matthias Winther, Samwald, 10.1038/s41597-023-02433-3Scientific Data. 1012023</p>
<p>Logic-LM: Empowering large language models with symbolic solvers for faithful logical reasoning. Liangming Pan, Alon Albalak, Xinyi Wang, William Wang, 10.18653/v1/2023.findings-emnlp.248Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023a</p>
<p>Let's reinforce step by step. Sarah Pan, Sherin Vladislav Lialin, Anna Muckatira, Rumshisky, arXiv:2311.058212023bPreprint</p>
<p>Iterative reasoning preference optimization. Richard Yuanzhe Pang, Weizhe Yuan, Kyunghyun Cho, He He, Sainbayar Sukhbaatar, Jason Weston, arXiv:2404.197332024Preprint</p>
<p>Making reasoning matter: Measuring and improving faithfulness of chain-of-thought reasoning. Debjit Paul, Robert West, Antoine Bosselut, Boi Faltings, 10.18653/v1/2024.findings-emnlp.882Findings of the Association for Computational Linguistics: EMNLP 2024. Miami, Florida, USAAssociation for Computational Linguistics2024</p>
<p>Ivo Petrov, Jasper Dekoninck, Lyuben Baltadzhiev, Maria Drencheva, Kristian Minchev, Mislav Balunovi, Nikola Jovanovi, Martin Vechev, arXiv:2503.21934Proof or bluff? evaluating llms on 2025 usa math olympiad. 2025Preprint</p>
<p>ReCEval: Evaluating reasoning chains via correctness and informativeness. Archiki Prasad, Swarnadeep Saha, Xiang Zhou, Mohit Bansal, 10.18653/v1/2023.emnlp-main.622Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Entropy-based decoding for retrieval-augmented large language models. Zexuan Qiu, Zijing Ou, Bin Wu, Jingjing Li, Aiwei Liu, Irwin King, arXiv:2406.175192024Preprint</p>
<p>QwQ: Reflect Deeply on the Boundaries of the Unknown -qwenlm. Qwen-Team, 2024</p>
<p>Direct preference optimization: Your language model is secretly a reward model. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, Chelsea Finn, Advances in Neural Information Processing Systems. Curran Associates, Inc202336</p>
<p>GPQA: A graduate-level google-proof q&amp;a benchmark. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, Samuel R Bowman, First Conference on Language Modeling. 2024</p>
<p>Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. Abulhair Saparov, He He, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Diagnostic reasoning prompts reveal the potential for large language model interpretability in medicine. Thomas Savage, Ashwin Nayak, Robert Gallo, Ekanath Rangan, Jonathan H Chen, NPJ Digital Medicine. 71202024</p>
<p>Julian Schnitzler, Xanh Ho, Jiahao Huang, Florian Boudin, Saku Sugawara, Akiko Aizawa, arXiv:2406.13397Morehopqa: More than multi-hop reasoning. 2024Preprint</p>
<p>Rewarding progress: Scaling automated process verifiers for llm reasoning. Amrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh Agarwal, Jonathan Berant, Aviral Kumar, arXiv:2410.081462024Preprint</p>
<p>Deepseekmath: Pushing the limits of mathematical reasoning in open language models. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y K Li, Y Wu, Daya Guo, arXiv:2402.033002024Preprint</p>
<p>Rprm: Reasoning-driven process reward modeling. Shuaijie She, Junxiao Liu, Yifeng Liu, Jiajun Chen, Xin Huang, Shujian Huang, arXiv:2503.212952025Preprint</p>
<p>When to solve, when to verify: Compute-optimal problem solving and generative verification for llm reasoning. Nishad Singhi, Hritik Bansal, Arian Hosseini, Aditya Grover, Kai-Wei Chang, Marcus Rohrbach, Anna Rohrbach, arXiv:2504.010052025Preprint</p>
<p>CLUTRR: A diagnostic benchmark for inductive reasoning from text. Koustuv Sinha, Shagun Sodhani, Jin Dong, Joelle Pineau, William L Hamilton, 10.18653/v1/D19-1458Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Scaling llm test-time compute optimally can be more effective than scaling model parameters. Charlie Snell, Jaehoon Lee, Kelvin Xu, Aviral Ku, arXiv:2408.03314mar. 2024Preprint</p>
<p>Prmbench: A fine-grained and challenging benchmark for process-level reward models. Mingyang Song, Zhaochen Su, Xiaoye Qu, Jiawei Zhou, Yu Cheng, arXiv:2501.031242025Preprint</p>
<p>To cot or not to cot? chain-ofthought helps mainly on math and symbolic reasoning. Zayne Sprague, Fangcong Yin, Juan Diego Rodriguez, Dongwei Jiang, Manya Wadhwa, Prasann Singhal, Xinyu Zhao, Xi Ye, Kyle Mahowald, Greg Durrett, arXiv:2409.121832024Preprint</p>
<p>Easy-to-hard generalization: Scalable alignment beyond human supervision. Zhiqing Sun, Longhui Yu, Yikang Shen, Weiyang Liu, Yiming Yang, Sean Welleck, Chuang Gan, arXiv:2403.094722024Preprint</p>
<p>CliCR: a dataset of clinical case reports for machine reading comprehension. Simon uster, Walter Daelemans, 10.18653/v1/N18-1140Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesNew Orleans, LouisianaAssociation for Computational Linguistics20181</p>
<p>Challenging big-bench tasks and whether chain-of-thought can solve them. Mirac Suzgun, Nathan Scales, Nathanael Schrli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, Jason Wei, arXiv:2210.092612022Preprint</p>
<p>ProofWriter: Generating implications, proofs, and abductive statements over natural language. Oyvind Tafjord, Bhavana Dalvi, Peter Clark, 10.18653/v1/2021.findings-acl.317Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Online. Association for Computational Linguistics2021</p>
<p>The web as a knowledge-base for answering complex questions. Alon Talmor, Jonathan Berant, 10.18653/v1/N18-1059Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesLouisianaNew Orleans20181Association for Computational Linguistics</p>
<p>CommonsenseQA: A question answering challenge targeting commonsense knowledge. Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, 10.18653/v1/N19-1421Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>FEVER: a large-scale dataset for fact extraction and VERification. James Thorne, Andreas Vlachos, Christos Christodoulopoulos, Arpit Mittal, 10.18653/v1/N18-1074Proceedings of the 2018 Conference of the North American Chapter. Long Papers. the 2018 Conference of the North American ChapterNew Orleans, LouisianaAssociation for Computational Linguistics20181</p>
<p>Diagnosing the firstorder logical reasoning ability through LogicNLI. Jidong Tian, Yitian Li, Wenqing Chen, Liqiang Xiao, Hao He, Yaohui Jin, 10.18653/v1/2021.emnlp-main.303Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican Republic. Association for Computational Linguistics2021Online and Punta Cana</p>
<p>Right for right reasons: Large language models for verifiable commonsense knowledge graph question answering. Armin Toroghi, Willis Guo, Mohammad Mahdi, Abdollah Pour, Scott Sanner, 10.18653/v1/2024.emnlp-main.378Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational Linguistics2024</p>
<p>MuSiQue: Multihop questions via single-hop question composition. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, Ashish Sabharwal, 10.1162/tacl_a_00475Transactions of the Association for Computational Linguistics. 102022</p>
<p>Step-by-step reasoning to solve grid puzzles: Where do LLMs falter?. Nemika Tyagi, Mihir Parmar, Mohith Kulkarni, Aswin Rrv, Nisarg Patel, Mutsumi Nakamura, 10.18653/v1/2024.emnlp-main.1111Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational Linguistics2024. 19898-19915Arindam Mitra, and Chitta Baral</p>
<p>LLMs cannot find reasoning errors, but can correct them given the error location. Gladys Tyen, Hassan Mansoor, Victor Carbune, Peter Chen, Tony Mak, 10.18653/v1/2024.findings-acl.826Findings of the Association for Computational Linguistics: ACL 2024. Bangkok, ThailandAssociation for Computational Linguistics2024</p>
<p>Solving math word problems with process-and outcomebased feedback. Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, Irina Higgins, arXiv:2211.142752022Preprint</p>
<p>Binghai Wang, Rui Zheng, Lu Chen, Yan Liu, Shihan Dou, Caishuang Huang, Wei Shen, Senjie Jin, Enyu Zhou, Chenyu Shi, Songyang Gao, Nuo Xu, Yuhao Zhou, Xiaoran Fan, Zhiheng Xi, Jun Zhao, Xiao Wang, Tao Ji, Hang Yan, Lixing Shen, Zhan Chen, Tao Gui, Qi Zhang, Xipeng Qiu, Xuanjing Huang, Zuxuan Wu, Yu-Gang Jiang, arXiv:2401.06080Secrets of rlhf in large language models part ii: Reward modeling. 2024aPreprint</p>
<p>Towards understanding chain-of-thought prompting: An empirical study of what matters. Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, Huan Sun, 10.18653/v1/2023.acl-long.153Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023a1</p>
<p>Process-supervised reward models for verifying clinical note generation: A scalable approach guided by domain expertise. Hanyin Wang, Chufan Gao, Qiping Xu, Bolun Liu, Guleid Hussein, Hariprasad Korsapati, Mohamad El Labban, Kingsley Iheasirim, Mohamed Hassan, Gokhan Anil, Brian Bartlett, Jimeng Sun, arXiv:2412.125832025aPreprint</p>
<p>Boosting language models reasoning with chain-of-knowledge prompting. Jianing Wang, Qiushi Sun, Xiang Li, Ming Gao, 10.18653/v1/2024.acl-long.271Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics2024b1</p>
<p>Math-shepherd: Verify and reinforce LLMs step-by-step without human annotations. Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, Zhifang Sui, 10.18653/v1/2024.acl-long.510Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics2024c1Long Papers)</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, The Eleventh International Conference on Learning Representations, ICLR 2023. Kigali, Rwanda2023b. May 1-5, 2023OpenReview.net</p>
<p>Examining false positives under inference scaling for mathematical reasoning. Yu Wang, Nan Yang, Liang Wang, Furu Wei, arXiv:2502.062172025bPreprint</p>
<p>Analyzing chain-of-thought prompting in black-box large language models via estimated V-information. Zecheng Wang, Chunshan Li, Zhao Yang, Qingbin Liu, Yanchao Hao, Xi Chen, Dianhui Chu, Dianbo Sui, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024). the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)Torino, ItaliaELRA and ICCL2024d</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. Curran Associates, Inc202235</p>
<p>A survey on feedback-based multi-step reasoning for large language models on mathematics. Ting-Ruen, Haowei Wei, Xuyang Liu, Yi Wu, Fang, arXiv:2502.143332025Preprint</p>
<p>Synchronous faithfulness monitoring for trustworthy retrieval-augmented generation. Di Wu, Jia-Chen Gu, Fan Yin, Nanyun Peng, Kai-Wei Chang, 10.18653/v1/2024.emnlp-main.527Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational Linguistics2024a</p>
<p>Cofca: A step-wise counterfactual multi-hop qa benchmark. Jian Wu, Linyi Yang, Zhen Wang, Manabu Okumura, Yue Zhang, arXiv:2402.119242024bPreprint</p>
<p>Ocean: Offline chain-ofthought evaluation and alignment in large language models. Junda Wu, Xintong Li, Ruoyu Wang, Yu Xia, Yuxin Xiong, Jianing Wang, Tong Yu, Xiang Chen, Branislav Kveton, Lina Yao, Jingbo Shang, Julian Mcauley, arXiv:2410.237032024cPreprint</p>
<p>Mitigating misleading chain-of-thought reasoning with selective filtering. Yexin Wu, Zhuosheng Zhang, Hai Zhao, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024). the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)Torino, ItaliaELRA and ICCL2024d</p>
<p>Shijie Xia, Xuefeng Li, Yixin Liu, Tongshuang Wu, Pengfei Liu, arXiv:2404.05692Evaluating mathematical reasoning beyond accuracy. 2025Preprint</p>
<p>On hallucination and predictive uncertainty in conditional language generation. Yijun Xiao, William Yang, Wang , 10.18653/v1/2021.eacl-main.236Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main VolumeOnline. Association for Computational Linguistics2021</p>
<p>Monte carlo tree search boosts reasoning via iterative preference learning. Yuxi Xie, Anirudh Goyal, Wenyue Zheng, Min-Yen Kan, Timothy P Lillicrap, Kenji Kawaguchi, Michael Shieh, arXiv:2405.004512024Preprint</p>
<p>WorldTree v2: A corpus of sciencedomain structured explanations and inference patterns supporting multi-hop inference. Zhengnan Xie, Sebastian Thiem, Jaycie Martin, Elizabeth Wainwright, Steven Marmorstein, Peter Jansen, Proceedings of the Twelfth Language Resources and Evaluation Conference. the Twelfth Language Resources and Evaluation ConferenceMarseille, France2020European Language Resources Association</p>
<p>LeanDojo: Theorem proving with retrieval-augmented language models. Kaiyu Yang, Aidan Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil, Ryan Prenger, Anima Anandkumar, Neural Information Processing Systems (NeurIPS). 2023</p>
<p>HotpotQA: A dataset for diverse, explainable multi-hop question answering. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, Christopher D Manning, 10.18653/v1/D18-1259Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics2018</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan, Advances in Neural Information Processing Systems. Curran Associates, Inc202336</p>
<p>Lifan Yuan, Wendi Li, Huayu Chen, Ganqu Cui, Ning Ding, Kaiyan Zhang, Bowen Zhou, Zhiyuan Liu, Hao Peng, arXiv:2412.01981Free process rewards without process labels. 2024arXiv preprint</p>
<p>Versaprm: Multi-domain process reward model via synthetic reasoning data. Thomas Zeng, Shuibai Zhang, Shutong Wu, Christian Classen, Daewon Chae, Ethan Ewer, Minjae Lee, Heeju Kim, Wonjun Kang, Jackson Kunde, Ying Fan, Jungtaek Kim, arXiv:2502.06737Hyung Il Koo, Kannan Ramchandran, Dimitris Papailiopoulos, and Kangwook Lee. 2025Preprint</p>
<p>Mr-gsm8k: A metareasoning benchmark for large language model evaluation. Zhongshen Zeng, Pengguang Chen, Shu Liu, Haiyun Jiang, Jiaya Jia, arXiv:2312.170802024aPreprint</p>
<p>Mr-ben: A meta-reasoning benchmark for evaluating system-2 thinking in llms. Zhongshen Zeng, Yinhong Liu, Yingjia Wan, Jingyao Li, Pengguang Chen, Jianbo Dai, Yuxuan Yao, Rongwu Xu, Zehan Qi, Wanru Zhao, Linling Shen, Jianqiao Lu, Haochen Tan, Yukang Chen, Hao Zhang, Zhan Shi, Bailin Wang, Zhijiang Guo, Jiaya Jia, Advances in Neural Information Processing Systems. Curran Associates, Inc2024b37</p>
<p>AlignScore: Evaluating factual consistency with a unified alignment function. Yuheng Zha, Yichi Yang, Ruichen Li, Zhiting Hu, 10.18653/v1/2023.acl-long.634Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</p>
<p>RepoCoder: Repository-level code completion through iterative retrieval and generation. Dan Zhang, Sining Zhoubian, Ziniu Hu, Yisong Yue, Yuxiao Dong, Jie Tang, ; Fengji Zhang, Bei Chen, Yue Zhang, Jacky Keung, Jin Liu, Daoguang Zan, Yi Mao, Jian-Guang Lou, Weizhu Chen, 10.18653/v1/2023.emnlp-main.151Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2024a. 2023a37Advances in Neural Information Processing Systems</p>
<p>GeoEval: Benchmark for evaluating LLMs and multimodal models on geometry problem-solving. Jiaxin Zhang, Zhong-Zhi Li, Ming-Liang Zhang, Fei Yin, Cheng-Lin, Yashar Liu, Moshfeghi, 10.18653/v1/2024.findings-acl.73Findings of the Association for Computational Linguistics: ACL 2024. Bangkok, ThailandAssociation for Computational Linguistics2024b</p>
<p>Generative verifiers: Reward modeling as next-token prediction. Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, Rishabh Agarwal, arXiv:2408.152402024cPreprint</p>
<p>Enhancing uncertaintybased hallucination detection with stronger focus. Tianhang Zhang, Lin Qiu, Qipeng Guo, Cheng Deng, Yue Zhang, Zheng Zhang, Chenghu Zhou, Xinbing Wang, Luoyi Fu, 10.18653/v1/2023.emnlp-main.58Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023b</p>
<p>Chain of preference optimization: Improving chain-of-thought reasoning in llms. Xuan Zhang, Chao Du, Tianyu Pang, Qian Liu, Wei Gao, Min Lin, arXiv:2406.091362024dPreprint</p>
<p>The lessons of developing process reward models in mathematical reasoning. Zhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng Liu, Jingren Zhou, Junyang Lin, arXiv:2501.073012025Preprint</p>
<p>Processbench: Identifying process errors in mathematical reasoning. Chujie Zheng, Zhenru Zhang, Beichen Zhang, Runji Lin, Keming Lu, Bowen Yu, Dayiheng Liu, Jingren Zhou, Junyang Lin, arXiv:2412.065592024aPreprint</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E Gonzalez, Ion Stoica, Advances in Neural Information Processing Systems. Curran Associates, Inc202336</p>
<p>Critic-cot: Boosting the reasoning abilities of large language model via chain-ofthoughts critic. Xin Zheng, Jie Lou, Boxi Cao, Xueru Wen, Yuqiu Ji, Hongyu Lin, Yaojie Lu, Xianpei Han, Debing Zhang, Le Sun, arXiv:2408.163262024bPreprint</p>
<p>Wanjun Zhong, Siyuan Wang, Duyu Tang, Zenan Xu, Daya Guo, Jiahai Wang, Jian Yin, Ming Zhou, Nan Duan, arXiv:2104.06598Ar-lsat: Investigating analytical reasoning of text. 2021Preprint</p>            </div>
        </div>

    </div>
</body>
</html>
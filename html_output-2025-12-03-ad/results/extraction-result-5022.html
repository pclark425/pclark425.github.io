<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5022 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5022</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5022</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-106.html">extraction-schema-106</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-a26a7a74f1e5fd562be95c3611a0680759fbdf84</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/a26a7a74f1e5fd562be95c3611a0680759fbdf84" target="_blank">CoCa: Contrastive Captioners are Image-Text Foundation Models</a></p>
                <p><strong>Paper Venue:</strong> Trans. Mach. Learn. Res.</p>
                <p><strong>Paper TL;DR:</strong> Contrastive Captioner (CoCa), a minimalist design to pretrain an image-text encoder-decoder foundation model jointly with contrastive loss and captioning loss, thereby subsuming model capabilities from contrastive approaches like CLIP and generative methods like SimVLM.</p>
                <p><strong>Paper Abstract:</strong> Exploring large-scale pretrained foundation models is of significant interest in computer vision because these models can be quickly transferred to many downstream tasks. This paper presents Contrastive Captioner (CoCa), a minimalist design to pretrain an image-text encoder-decoder foundation model jointly with contrastive loss and captioning loss, thereby subsuming model capabilities from contrastive approaches like CLIP and generative methods like SimVLM. In contrast to standard encoder-decoder transformers where all decoder layers attend to encoder outputs, CoCa omits cross-attention in the first half of decoder layers to encode unimodal text representations, and cascades the remaining decoder layers which cross-attend to the image encoder for multimodal image-text representations. We apply a contrastive loss between unimodal image and text embeddings, in addition to a captioning loss on the multimodal decoder outputs which predicts text tokens autoregressively. By sharing the same computational graph, the two training objectives are computed efficiently with minimal overhead. CoCa is pretrained end-to-end and from scratch on both web-scale alt-text data and annotated images by treating all labels simply as text, seamlessly unifying natural language supervision for representation learning. Empirically, CoCa achieves state-of-the-art performance with zero-shot transfer or minimal task-specific adaptation on a broad range of downstream tasks, spanning visual recognition (ImageNet, Kinetics-400/600/700, Moments-in-Time), crossmodal retrieval (MSCOCO, Flickr30K, MSR-VTT), multimodal understanding (VQA, SNLI-VE, NLVR2), and image captioning (MSCOCO, NoCaps). Notably on ImageNet classification, CoCa obtains 86.3% zero-shot top-1 accuracy, 90.6% with a frozen encoder and learned classification head, and new state-of-the-art 91.0% top-1 accuracy on ImageNet with a finetuned encoder.</p>
                <p><strong>Cost:</strong> 0.006</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5022",
    "paper_id": "paper-a26a7a74f1e5fd562be95c3611a0680759fbdf84",
    "extraction_schema_id": "extraction-schema-106",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.005568,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>CoCa: Contrastive Captioners are Image-Text Foundation Models</h1>
<p>Jiahui Yu ${ }^{\dagger} \quad$ Zirui Wang ${ }^{\dagger}$<br>{jiahuiyu, ziruiw}@google.com<br>Vijay Vasudevan Legg Yeung Mojtaba Seyedhosseini Yonghui Wu<br>Google Research</p>
<h4>Abstract</h4>
<p>Exploring large-scale pretrained foundation models is of significant interest in computer vision because these models can be quickly transferred to many downstream tasks. This paper presents Contrastive Captioner (CoCa), a minimalist design to pretrain an image-text encoder-decoder foundation model jointly with contrastive loss and captioning loss, thereby subsuming model capabilities from contrastive approaches like CLIP and generative methods like SimVLM. In contrast to standard encoder-decoder transformers where all decoder layers attend to encoder outputs, CoCa omits cross-attention in the first half of decoder layers to encode unimodal text representations, and cascades the remaining decoder layers which cross-attend to the image encoder for multimodal image-text representations. We apply a contrastive loss between unimodal image and text embeddings, in addition to a captioning loss on the multimodal decoder outputs which predicts text tokens autoregressively. By sharing the same computational graph, the two training objectives are computed efficiently with minimal overhead. CoCa is pretrained end-to-end and from scratch on both web-scale alt-text data and annotated images by treating all labels simply as text, seamlessly unifying natural language supervision for representation learning. Empirically, CoCa achieves state-of-theart performance with zero-shot transfer or minimal task-specific adaptation on a broad range of downstream tasks, spanning visual recognition (ImageNet, Kinetics400/600/700, Moments-in-Time), crossmodal retrieval (MSCOCO, Flickr30K, MSR-VTT), multimodal understanding (VQA, SNLI-VE, NLVR2), and image captioning (MSCOCO, NoCaps). Notably on ImageNet classification, CoCa obtains $86.3 \%$ zero-shot top-1 accuracy, $90.6 \%$ with a frozen encoder and learned classification head, and new state-of-the-art $91.0 \%$ top-1 accuracy on ImageNet with a finetuned encoder.</p>
<h2>1 Introduction</h2>
<p>Deep learning has recently witnessed the rise of foundation language models [1] such as BERT [2], T5 [3], GPT-3 [4], where models are pretrained on web-scale data and demonstrate generic multitasking capabilities through zero-shot, few-shot or transfer learning. Compared with specialized individual models, pretraining foundation models for massive downstream tasks can amortize training costs, providing opportunities to push the limits of model scale [5] for human-level intelligence.
For vision and vision-language problems, several foundation model candidates have been explored: (1) Pioneering works [6, 7, 8] have shown the effectiveness of single-encoder models pretrained with cross-entropy loss on image classification datasets such as ImageNet [9]. The image encoder provides generic visual representations that can be adapted for various downstream tasks including</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overview of Contrastive Captioners (CoCa) pretraining as image-text foundation models. The pretrained CoCa can be used for downstream tasks including visual recognition, vision-language alignment, image captioning, and multimodal understanding with zero-shot transfer, frozen-feature evaluation, or end-to-end finetuning.</p>
<p>Image and video understanding [10, 11]. However, these models rely heavily on image annotations as labeled vectors and do not bake in knowledge of free-form human natural language, hindering their application to downstream tasks that involve both vision and language modalities. (2) Recently, a line of research [12, 13, 14] has shown the feasibility of image-text foundation model candidates by pretraining two parallel encoders with a contrastive loss on web-scale noisy image-text pairs. In addition to the visual embeddings for vision-only tasks, the resulting dual-encoder models can additionally encode textual embeddings to the same latent space, enabling new crossmodal alignment capabilities such as zero-shot image classification and image-text retrieval. Nonetheless, these models are not directly applicable for joint vision-language understanding tasks such as visual question answering (VQA), due to missing joint components to learn fused image and text representations. (3) Another line of research [15, 16, 17] has explored generative pretraining with encoder-decoder models to learn generic vision and multimodal representations. During pretraining, the model takes images on the encoder side and applies Language Modeling (LM) loss (or PrefixLM [3, 16]) on the decoder outputs. For downstream tasks, the decoder outputs can then be used as joint representations for multimodal understanding tasks. While superior vision-language results [16] have been attained with pretrained encoder-decoder models, they do not produce text-only representations aligned with image embeddings, thereby being less feasible and efficient for crossmodal alignment tasks.</p>
<p>In this work, we unify single-encoder, dual-encoder, and encoder-decoder paradigms, and train one image-text foundation model that subsumes the capabilities of all three approaches. We propose a simple model family named Contrastive Captioners (CoCa) with a modified encoder-decoder architecture trained with both contrastive loss and captioning (generative) loss. As shown in Figure 1, we decouple the decoder transformer into two parts, a unimodal decoder and a multimodal decoder. We omit cross-attention in unimodal decoder layers to encode text-only representations, and cascade multimodal decoder layers cross-attending to image encoder outputs to learn multimodal image-text representations. We apply both the contrastive objective between outputs of the image encoder and unimodal text decoder, and the captioning objective at the output of the multimodal decoder. Furthermore, CoCa is trained on both image annotation data and noisy image-text data by treating all labels simply as text. The generative loss on image annotation text provides a fine-grained training signal similar to the single-encoder cross-entropy loss approach, effectively subsuming all three pretraining paradigms into a single unified method.</p>
<p>The design of CoCa leverages contrastive learning for learning global representations and captioning for fine-grained region-level features, thereby benefiting tasks across all three categories shown in Figure 1. CoCa shows that a single pretrained model can outperform many specialized models using zero-shot transfer or minimal task-specific adaptation. For example, CoCa obtains 86.3% zero-shot accuracy on ImageNet and better zero-shot crossmodal retrieval on MSCOCO and Flickr30k. With a frozen-encoder, CoCa achieves 90.6% on ImageNet classification, 88.0%/88.5%/81.1% on Kinetics-40/600/700 and 47.4% on Moments-in-Time. After lightweight finetuning, CoCa further achieves 91.0% on ImageNet, 82.3% on VQA, and 120.6 CIDEr score on NoCaps.</p>
<h1>2 Related Work</h1>
<p>Vision Pretraining. Pretraining ConvNets [18] or Transformers [19] on large-scale annotated data such as ImageNet [6, 7, 8], Instagram [20] or JFT [21] has become a popular strategy towards solving visual recognition problems including classification, localization, segmentation, video recognition, tracking and many other problems. Recently, self-supervised pretraining approaches have also been explored. BEiT [22] proposes a masked image modeling task following BERT [2] in natural language processing, and uses quantized visual token ids as prediction targets. MAE [23] and SimMIM [24] remove the need for an image tokenizer and directly use a light-weight decoder or projection layer to regress pixel values. Nonetheless, these methods only learn models for the vision modality and thus they are not applicable to tasks that require joint reasoning over both image and text inputs.
Vision-Language Pretraining. In recent years, rapid progress has been made in vision-language pretraining (VLP), which aims to jointly encode vision and language in a fusion model. Early work (e.g. LXMERT [25], UNITER [26], VinVL [27]) in this direction relies on pretrained object detection modules such as Fast(er) R-CNN [28] to extract visual representations. Later efforts such as ViLT [29] and VLMo [30] unify vision and language transformers, and train a multimodal transformer from scratch.</p>
<p>Image-Text Foundation Models. Recent work has proposed image-text foundation models that can subsume both vision and vision-language pretraining. CLIP [12] and ALIGN [13] demonstrate that dual-encoder models pretrained with contrastive objectives on noisy image-text pairs can learn strong image and text representations for crossmodal alignment tasks and zero-shot image classification. Florence [14] further develops this method with unified contrastive objective [31], training foundation models that can be adapted for a wide range of vision and image-text benchmarks. To further improve zero-shot image classification accuracy, LiT [32] and BASIC [33] first pretrain model on an largescale image annotation dataset with cross-entropy and further finetune with contrastive loss on an noisy alt-text image dataset. Another line of research [16, 17, 34] proposes encoder-decoder models trained with generative losses and shows strong results in vision-language benchmarks while the visual encoder still performs competitively on image classification. In this work, we focus on training an image-text foundation model from scratch in a single pretraining stage to unify these approaches. While recent works [35, 36, 37] have also explored image-text unification, they require multiple pretraining stages of unimodal and multimodal modules to attain good performance. For example, ALBEF [36] combines contrastive loss with masked language modelling (MLM) with a dual-encoder design. However, our approach is simpler and more efficient to train while also enables more model capabilities: (1) CoCa only performs one forward and backward propagation for a batch of image-text pairs while ALBEF requires two (one on corrupted inputs and another without corruption), (2) CoCa is trained from scratch on the two objectives only while ALBEF is initialized from pretrained visual and textual encoders with additional training signals including momentum modules. (3) The decoder architecture with generative loss is preferred for natural language generation and thus directly enables image captioning and zero-shot learning [16].</p>
<h2>3 Approach</h2>
<p>We begin with a review of three foundation model families that utilize natural language supervision differently: single-encoder classification pretraining, dual-encoder contrastive learning, and encoderdecoder image captioning. We then introduce Contrastive Captioners (CoCa) that share the merits of both contrastive learning and image-to-caption generation under a simple architecture. We further discuss how CoCa models can quickly transfer to downstream tasks with zero-shot transfer or minimal task adaptation.</p>
<h3>3.1 Natural Language Supervision</h3>
<p>Single-Encoder Classification. The classic single-encoder approach pretrains a visual encoder through image classification on a large crowd-sourced image annotation dataset (e.g., ImageNet [9], Instagram [20] or JFT [21]), where the vocabulary of annotation texts is usually fixed. These image annotations are usually mapped into discrete class vectors to learn with a cross-entropy loss as</p>
<p>$$
\mathcal{L}<em _theta="\theta">{\mathrm{Cls}}=-p(y) \log q</em>(x)
$$</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Detailed illustration of CoCa architecture and training objectives.
where $p(y)$ is a one-hot, multi-hot or smoothed label distribution from ground truth label $y$. The learned image encoder is then used as a generic visual representation extractor for downstream tasks.</p>
<p>Dual-Encoder Contrastive Learning. Compared to pretraining with single-encoder classification, which requires human-annotated labels and data cleaning, the dual-encoder approach exploits noisy web-scale text descriptions and introduces a learnable text tower to encode free-form texts. The two encoders are jointly optimized by contrasting the paired text against others in the sampled batch:</p>
<p>$$
\mathcal{L}<em i="i">{\mathrm{Con}}=-\frac{1}{N}\left(\underbrace{\sum</em>}^{N} \log \frac{\exp \left(x_{i}^{\top} y_{i} / \sigma\right)}{\sum_{j=1}^{N} \exp \left(x_{i}^{\top} y_{j} / \sigma\right)}<em i="i">{\text {image-to-text }}+\underbrace{\sum</em>\right)
$$}^{N} \log \frac{\exp \left(y_{i}^{\top} x_{i} / \sigma\right)}{\sum_{j=1}^{N} \exp \left(y_{i}^{\top} x_{j} / \sigma\right)}}_{\text {text-to-image }</p>
<p>where $x_{i}$ and $y_{j}$ are normalized embeddings of the image in the $i$-th pair and that of the text in the $j$-th pair. $N$ is the batch size, and $\sigma$ is the temperature to scale the logits. In addition to the image encoder, the dual-encoder approach also learns an aligned text encoder that enables crossmodal alignment applications such as image-text retrieval and zero-shot image classification. Empirical evidence shows zero-shot classification is more robust $[12,13,38]$ on corrupted or out-of-distribution images.</p>
<p>Encoder-Decoder Captioning. While the dual-encoder approach encodes the text as a whole, the generative approach (a.k.a. captioner) aims for detailed granularity and requires the model to predict the exact tokenized texts of $y$ autoregressively. Following a standard encoder-decoder architecture, the image encoder provides latent encoded features (e.g., using a Vision Transformer [39] or ConvNets [40]) and the text decoder learns to maximize the conditional likelihood of the paired text $y$ under the forward autoregressive factorization:</p>
<p>$$
\mathcal{L}<em t="1">{\mathrm{Cap}}=-\sum</em>, x\right)
$$}^{T} \log P_{\theta}\left(y_{t} \mid y_{&lt;t</p>
<p>The encoder-decoder is trained with teacher-forcing [41] to parallelize computation and maximize learning efficiency. Unlike prior methods, the captioner approach yields a joint image-text representation that can be used for vision-language understanding, and is also capable of image captioning applications with natural language generation.</p>
<h1>3.2 Contrastive Captioners Pretraining</h1>
<p>Figure 2 depicts the proposed contrastive captioner (CoCa): a simple encoder-decoder approach that seamlessly combines the three training paradigms. Similar to standard image-text encoderdecoder models, CoCa encodes images to latent representations by a neural network encoder, for example, vision transformer (ViT) [39] (used by default; it can also be other image encoders like ConvNets [40]), and decodes texts with a causal masking transformer decoder. Unlike standard</p>
<table>
<thead>
<tr>
<th></th>
<th>Image Encoder</th>
<th></th>
<th></th>
<th>Text Decoder</th>
<th></th>
<th></th>
<th></th>
<th>Image / Text</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Model</td>
<td>Layers</td>
<td>MLP</td>
<td>Params</td>
<td>$n_{\text {uni }}$</td>
<td>$n_{\text {multi }}$</td>
<td>MLP</td>
<td>Params</td>
<td>Hidden</td>
<td>Heads</td>
<td>Total Params</td>
</tr>
<tr>
<td>CoCa-Base</td>
<td>12</td>
<td>3072</td>
<td>86M</td>
<td>12</td>
<td>12</td>
<td>3072</td>
<td>297M</td>
<td>768</td>
<td>12</td>
<td>383M</td>
</tr>
<tr>
<td>CoCa-Large</td>
<td>24</td>
<td>4096</td>
<td>303M</td>
<td>12</td>
<td>12</td>
<td>4096</td>
<td>484M</td>
<td>1024</td>
<td>16</td>
<td>787M</td>
</tr>
<tr>
<td>CoCa</td>
<td>40</td>
<td>6144</td>
<td>1B</td>
<td>18</td>
<td>18</td>
<td>5632</td>
<td>1.1B</td>
<td>1408</td>
<td>16</td>
<td>2.1B</td>
</tr>
</tbody>
</table>
<p>Table 1: Size variants of CoCa. Both image encoder and text decoder are Transformers [19, 39]. decoder transformers, CoCa omits cross-attention in the first half of the decoder layers to encode unimodal text representations, and cascades the rest of the decoder layers, cross-attending to the image encoder for multimodal image-text representations. As a result, the CoCa decoder simultaneously produces both unimodal and multimodal text representations that allow us to apply both contrastive and generative objectives as</p>
<p>$$
\mathcal{L}<em _mathrm_Con="\mathrm{Con">{\mathrm{CoCa}}=\lambda</em>}} \cdot \mathcal{L<em _mathrm_Cap="\mathrm{Cap">{\mathrm{Con}}+\lambda</em>
$$}} \cdot \mathcal{L}_{\mathrm{Cap}</p>
<p>where $\lambda_{\text {Con }}$ and $\lambda_{\text {Cap }}$ are loss weighting hyper-parameters. We note that the single-encoder crossentropy classification objective can be interpreted as a special case of the generative approach applied on image annotation data, when the vocabulary is the set of all label names.</p>
<p>Decoupled Text Decoder and CoCa Architecture. The captioning approach optimizes the conditional likelihood of text while the contrastive approach uses an unconditional text representation. To address this dilemma and combine these two methods into a single model, we propose a simple decoupled decoder design where we split the decoder into unimodal and multimodal components, by skipping the cross-attention mechanism in the unimodal decoder layers. That is, the bottom $n_{\text {uni }}$ unimodal decoder layers encode the input text as latent vectors with causally-masked self-attention, and the top $n_{\text {multi }}$ multimodal layers further apply causally-masked self-attention and together with cross-attention to the output of the visual encoder. All decoder layers prohibit tokens from attending to future tokens, and it is straightforward to use the multimodal text decoder output for the captioning objective $\mathcal{L}<em _Con="{Con" _text="\text">{\text {Cap }}$. For the contrastive objective $\mathcal{L}</em>$. Following ALIGN [13], we pretrain with image resolution of $288 \times 288$ and patch size $18 \times 18$, resulting in a total of 256 image tokens. Our largest CoCa model ("CoCa" in short) follows the ViT-giant setup in [21] with 1B-parameters in the image encoder and 2.1B-parameters altogether with the text decoder. We also explore two smaller variants of "CoCa-Base" and "CoCa-Large" detailed in Table 1.}}$, we append a learnable [CLS] token at the end of the input sentence and use its corresponding output of unimodal decoder as the text embedding. We split the decoder in half such that $n_{\text {uni }}=n_{\text {multi }</p>
<p>Attentional Poolers. It is noteworthy that the contrastive loss uses a single embedding for each image while the decoder usually attends to a sequence of image output tokens in an encoder-decoder captioner [16]. Our preliminary experiments show that a single pooled image embedding helps visual recognition tasks as a global representation, while more visual tokens (thus more fine-grained) are beneficial for multimodal understanding tasks which require region-level features. Hence, CoCa adopts task-specific attentional pooling [42] to customize visual representations to be used for different types of training objectives and downstream tasks. Here, a pooler is a single multi-head attention layer with $n_{\text {query }}$ learnable queries, with the encoder output as both keys and values. Through this, the model can learn to pool embeddings with different lengths for the two training objectives, as shown in Figure 2. The use of task-specific pooling not only addresses different needs for different tasks but also introduces the pooler as a natural task adapter. We use attentional poolers in pretraining for generative loss $n_{\text {query }}=256$ and contrastive loss $n_{\text {query }}=1$.</p>
<p>Pretraining Efficiency. A key benefit of the decoupled autoregressive decoder design is that it can compute two training losses considered efficiently. Since unidirectional language models are trained with causal masking on complete sentences, the decoder can efficiently generate outputs for both contrastive and generative losses with a single forward propagation (compared to two passes for a bidirectional approach [36]). Therefore, the majority of the compute is shared between the two losses and CoCa only induces minimal overhead compared to standard encoder-decoder models. On the other hand, while many existing methods [30, 32, 33, 35, 36, 37] train model components with multiple stages on various data sources and/or modalities, CoCa is pretrained end-to-end from scratch directly with various data sources (i.e., annotated images and noisy alt-text images) by treating all labels as texts for both contrastive and generative objectives.</p>
<p>3.3 Contrastive Captioners for Downstream Tasks</p>
<p>Zero-shot Transfer. A pretrained CoCa model performs many tasks in a zero-shot manner by leveraging both image and text inputs, including zero-shot image classification, zero-shot image-text cross-retrieval, zero-shot video-text cross-retrieval. Following previous practices [12, 32], "zero-shot" here is different from classical zero-shot learning in that during pretraining, the model may see relevant supervised information, but no supervised examples are used during the transfer protocol. For the pretraining data, we follow strict de-duplication procedures introduced in [13, 32] to filter all near-domain examples to our downstream tasks.</p>
<p>Frozen-feature Evaluation. As discussed in the previous section, CoCa adopts task-specific attentional pooling [42] (pooler for brevity) to customize visual representations for different types downstream tasks while sharing the backbone encoder. This enables the model to obtain strong performance as a frozen encoder where we only learn a new pooler to aggregate features. It can also benefit to multi-task problems that share the same frozen image encoder computation but different task-specific heads. As also discussed in [23], linear-evaluation struggles to accurately measure learned representations and we find the attentional poolers are more practical for real-world applications.</p>
<p>CoCa for Video Action Recognition. We use a simple approach to enable a learned CoCa model for video action recognition tasks. We first take multiple frames of a video and feed each frame into the shared image encoder individually as shown in Figure 3. For frozenfeature evaluation or finetuning, we learn an additional pooler on top of the spatial and temporal feature tokens with a softmax cross-entropy loss. Note the pooler has a single query token thus the computation of pooling over all spatial and temporal tokens is not expensive. For zero-shot video-text retrieval, we use an even simpler approach by computing the mean embedding of 16 frames of the video (frames are uniformly sampled from a video). We also encode the captions of each video as target embeddings when computing retrieval metrics.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: CoCa for video recognition.</p>
<h1>4 Experiments</h1>
<p>In this section, we first describe the details of our experimental setup. The main results are presented next organized as visual recognition tasks, crossmodal alignment tasks, image captioning and multimodal understanding tasks. Our main results are conducted under three categories for downstream tasks: zero-shot transfer, frozen-feature evaluation and finetuning. We also present ablation experiments including training objectives and architecture designs.</p>
<h3>4.1 Training Setup</h3>
<p>Data. As discussed in Section 3.2, CoCa is pretrained from scratch in a single stage on both web-scale alt-text data and annotated images by treating all labels simply as texts. We use the JFT-3B dataset [21] with label names as the paired texts, and the ALIGN dataset [13] with noisy alt-texts. Similar to [33], we randomly shuffle and concatenate label names of each image in JFT together with a prompt sampled from [12]. An example of the resulting text label of a JFT image would look like "a photo of the cat, animal". Unlike prior models [32, 33] that also use the combination of these two datasets, we train all model parameters from scratch at the same time without pretraining an image encoder with supervised cross-entropy loss for simplicity and pretraining efficiency. To ensure fair evaluation, we follow the strict de-duplication procedures introduced in [13, 32] to filter all near-domain examples (3.6M images are removed in total) to our downstream tasks. To tokenize text input, we use a sentence-piece model [43, 44] with a vocabulary size of 64k trained on the sampled pretraining dataset.</p>
<p>Optimization. Our models are implemented in the Lingvo framework [45] with GSPMD [46, 47, 48, 49] for scaling performance. Following [33], we use a batch size of 65,536 image-text pairs,</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Comparison of CoCa with other image-text foundation models (without task-specific customization) and multiple state-of-the-art task-specialized models.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>ImageNet</th>
<th>Model</th>
<th>K-400</th>
<th>K-600</th>
<th>K-700</th>
<th>Moments-in-Time</th>
</tr>
</thead>
<tbody>
<tr>
<td>ALIGN [13]</td>
<td>88.6</td>
<td>ViViT [53]</td>
<td>84.8</td>
<td>84.3</td>
<td>-</td>
<td>38.0</td>
</tr>
<tr>
<td>Florence [14]</td>
<td>90.1</td>
<td>MoViNet [54]</td>
<td>81.5</td>
<td>84.8</td>
<td>79.4</td>
<td>40.2</td>
</tr>
<tr>
<td>MetaPseudoLabels [51]</td>
<td>90.2</td>
<td>VATT [55]</td>
<td>82.1</td>
<td>83.6</td>
<td>-</td>
<td>41.1</td>
</tr>
<tr>
<td>CoAtNet [10]</td>
<td>90.9</td>
<td>Florence [14]</td>
<td>86.8</td>
<td>88.0</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>ViT-G [21]</td>
<td>90.5</td>
<td>MaskFeat [56]</td>
<td>87.0</td>
<td>88.3</td>
<td>80.4</td>
<td>-</td>
</tr>
<tr>
<td>+ Model Soups [52]</td>
<td>90.9</td>
<td>CoVeR [11]</td>
<td>87.2</td>
<td>87.9</td>
<td>78.5</td>
<td>46.1</td>
</tr>
<tr>
<td>CoCa (frozen)</td>
<td>90.6</td>
<td>CoCa (frozen)</td>
<td>88.0</td>
<td>88.5</td>
<td>81.1</td>
<td>47.4</td>
</tr>
<tr>
<td>CoCa (finetuned)</td>
<td>91.0</td>
<td>CoCa (finetuned)</td>
<td>88.9</td>
<td>89.4</td>
<td>82.7</td>
<td>49.0</td>
</tr>
</tbody>
</table>
<p>Table 2: Image classification and video action recognition with frozen encoder or finetuned encoder.</p>
<p>where half of each batch comes from JFT and ALIGN, respectively. All models are trained on the combined contrastive and captioning objectives in Eq.(4) for 500k steps, roughly corresponding to 5 epochs on JFT and 10 epochs on ALIGN. As shown later in our studies, we find a larger captioning loss weight is better and thus λ_Cap = 2.0 and λ_Con = 1.0. Following [13], we apply a contrastive loss with a trainable temperature τ with an initial value of 0.07. For memory efficiency, we use the Adafactor [50] optimizer with β_1 = 0.9, β_2 = 0.999 and decoupled weight decay ratio of 0.01. We warm up the learning rate for the first 2% of training steps to a peak value of 8 × 10^{-4}, and linearly decay it afterwards. Pretraining CoCa takes about 5 days on 2,048 CloudTPUv4 chips. Following [12, 13, 14], we continue pretraining for one epoch on a higher resolution of 576×576. For finetuning evaluation, we mainly follow simple protocols and directly train CoCa on downstream tasks without further metric-specific tuning like CIDEr scores (details in Appendix A and B).</p>
<h3>4.2 Main Results</h3>
<p>We extensively evaluate the capabilities of CoCa models on a wide range of downstream tasks as a pretrained foundation model. We mainly consider core tasks of three categories that examine (1)</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Image classification scaling performance of model sizes.
visual recognition, (2) crossmodal alignment, and (3) image captioning and multimodal understanding capabilities. Since CoCa produces both aligned unimodal representations and fused multimodal embeddings at the same time, it is easily transferable to all three task groups with minimal adaption. Figure 4 summarizes the performance on key benchmarks of CoCa compared to other dual-encoder and encoder-decoder foundation models and state-of-the-art task-specialized methods. CoCa sets new state-of-the-art results on tasks of all three categories with a single pretrained checkpoint.</p>
<h1>4.2.1 Visual Recognition Tasks</h1>
<p>Our visual recognition experiments are conducted on ImageNet [9] as image recognition benchmark, and multiple video datasets including Kinetics-400 [57], Kinetics-600 [58], Kinetics-700 [59], Moments-in-Time [60] as test-beds for video action recognition; it is noteworthy that CoCa pretrains on image data only, without accessing any extra video datasets. We apply the CoCa encoder on video frames individually (Section 3.3) without early fusion of temporal information, yet the resulting CoCa-for-Video model performs better than many spatio-temporal early-fused video models.</p>
<p>Frozen-feature. We apply a pretrained frozen CoCa model on both image classification and video action recognition. The encoder is used for both tasks while the decoder is discarded. As discussed in Section 3.3, an attentional pooling is learned together with a softmax cross-entropy loss layer on top of the embedding outputs from CoCa encoder. For video classification, a single query-token is learned to weight outputs of all tokens of spatial patches $\times$ temporal frames. We set a learning rate of $5 \times 10^{-4}$ on both attentional pooler and softmax, batch size of 128 , and a cosine learning rate schedule (details in Appendix A). For video action recognition, we compare CoCa with other approaches on the same setup (i.e., without extra supervised video data and without audio signals as model inputs). As shown in Table 2, without finetuning full encoder, CoCa already achieves competitive Top-1 classification accuracies compared to specialized image and outperforms prior state-of-the-art specialized methods on video tasks.</p>
<p>Finetuning. Based on the architecture of frozen-feature evaluation, we further finetune CoCa encoders on image and video datasets individually with a smaller learning rate of $1 \times 10^{-4}$. More experimental details are summarized in the Appendix A. The finetuned CoCa has improved performance across these tasks. Notably, CoCa obtains new state-of-the-art $91.0 \%$ Top-1 accuracy on ImageNet, as well as better video action recognition results compared with recent video approaches. More importantly, CoCa models use much less parameters than other methods in the visual encoder as shown in Figure 5a. These results suggest the proposed framework efficiently combines text training signals and thus is able to learn high-quality visual representation better than the classical single-encoder approach.</p>
<h3>4.2.2 Crossmodal Alignment Tasks</h3>
<p>Unlike other fusion-based foundation methods [16, 17, 35], CoCa is naturally applicable to crossmodal alignment tasks since it generates aligned image and text unimodal embeddings. In particular, we</p>
<table>
<thead>
<tr>
<th></th>
<th>Flickr30K (1K test set)</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th>MSCOCO (5K test set)</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Image $\rightarrow$ Text</td>
<td></td>
<td></td>
<td>Text $\rightarrow$ Image</td>
<td></td>
<td></td>
<td>Image $\rightarrow$ Text</td>
<td></td>
<td></td>
<td>Text $\rightarrow$ Image</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Model</td>
<td>R@1</td>
<td>R@5</td>
<td>R@10</td>
<td>R@1</td>
<td>R@5</td>
<td>R@10</td>
<td>R@1</td>
<td>R@5</td>
<td>R@10</td>
<td>R@1</td>
<td>R@5</td>
<td>R@10</td>
</tr>
<tr>
<td>CLIP [12]</td>
<td>88.0</td>
<td>98.7</td>
<td>99.4</td>
<td>68.7</td>
<td>90.6</td>
<td>95.2</td>
<td>58.4</td>
<td>81.5</td>
<td>88.1</td>
<td>37.8</td>
<td>62.4</td>
<td>72.2</td>
</tr>
<tr>
<td>ALIGN [13]</td>
<td>88.6</td>
<td>98.7</td>
<td>99.7</td>
<td>75.7</td>
<td>93.8</td>
<td>96.8</td>
<td>58.6</td>
<td>83.0</td>
<td>89.7</td>
<td>45.6</td>
<td>69.8</td>
<td>78.6</td>
</tr>
<tr>
<td>FLAVA [35]</td>
<td>67.7</td>
<td>94.0</td>
<td>-</td>
<td>65.2</td>
<td>89.4</td>
<td>-</td>
<td>42.7</td>
<td>76.8</td>
<td>-</td>
<td>38.4</td>
<td>67.5</td>
<td>-</td>
</tr>
<tr>
<td>FILIP [61]</td>
<td>89.8</td>
<td>99.2</td>
<td>99.8</td>
<td>75.0</td>
<td>93.4</td>
<td>96.3</td>
<td>61.3</td>
<td>84.3</td>
<td>90.4</td>
<td>45.9</td>
<td>70.6</td>
<td>79.3</td>
</tr>
<tr>
<td>Florence [14]</td>
<td>90.9</td>
<td>99.1</td>
<td>-</td>
<td>76.7</td>
<td>93.6</td>
<td>-</td>
<td>64.7</td>
<td>85.9</td>
<td>-</td>
<td>47.2</td>
<td>71.4</td>
<td>-</td>
</tr>
<tr>
<td>CoCa-Base</td>
<td>89.8</td>
<td>98.8</td>
<td>99.8</td>
<td>76.8</td>
<td>93.7</td>
<td>96.8</td>
<td>63.8</td>
<td>84.7</td>
<td>90.7</td>
<td>47.5</td>
<td>72.4</td>
<td>80.9</td>
</tr>
<tr>
<td>CoCa-Large</td>
<td>91.4</td>
<td>99.2</td>
<td>99.9</td>
<td>79.0</td>
<td>95.1</td>
<td>97.4</td>
<td>65.4</td>
<td>85.6</td>
<td>91.4</td>
<td>50.1</td>
<td>73.8</td>
<td>81.8</td>
</tr>
<tr>
<td>CoCa</td>
<td>92.5</td>
<td>99.5</td>
<td>99.9</td>
<td>80.4</td>
<td>95.7</td>
<td>97.7</td>
<td>66.3</td>
<td>86.2</td>
<td>91.8</td>
<td>51.2</td>
<td>74.2</td>
<td>82.0</td>
</tr>
</tbody>
</table>
<p>Table 3: Zero-shot image-text retrieval results on Flickr30K [62] and MSCOCO [63] datasets.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">ImageNet</th>
<th style="text-align: center;">ImageNet-A</th>
<th style="text-align: center;">ImageNet-R</th>
<th style="text-align: center;">ImageNet-V2</th>
<th style="text-align: center;">ImageNet-Sketch</th>
<th style="text-align: center;">ObjectNet</th>
<th style="text-align: center;">Average</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">CLIP [12]</td>
<td style="text-align: center;">76.2</td>
<td style="text-align: center;">77.2</td>
<td style="text-align: center;">88.9</td>
<td style="text-align: center;">70.1</td>
<td style="text-align: center;">60.2</td>
<td style="text-align: center;">72.3</td>
<td style="text-align: center;">74.3</td>
</tr>
<tr>
<td style="text-align: left;">ALIGN [13]</td>
<td style="text-align: center;">76.4</td>
<td style="text-align: center;">75.8</td>
<td style="text-align: center;">92.2</td>
<td style="text-align: center;">70.1</td>
<td style="text-align: center;">64.8</td>
<td style="text-align: center;">72.2</td>
<td style="text-align: center;">74.5</td>
</tr>
<tr>
<td style="text-align: left;">FILIP [61]</td>
<td style="text-align: center;">78.3</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Florence [14]</td>
<td style="text-align: center;">83.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">LiT [32]</td>
<td style="text-align: center;">84.5</td>
<td style="text-align: center;">79.4</td>
<td style="text-align: center;">93.9</td>
<td style="text-align: center;">78.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">81.1</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">BASIC [33]</td>
<td style="text-align: center;">85.7</td>
<td style="text-align: center;">85.6</td>
<td style="text-align: center;">95.7</td>
<td style="text-align: center;">80.6</td>
<td style="text-align: center;">76.1</td>
<td style="text-align: center;">78.9</td>
<td style="text-align: center;">83.7</td>
</tr>
<tr>
<td style="text-align: left;">CoCa-Base</td>
<td style="text-align: center;">82.6</td>
<td style="text-align: center;">76.4</td>
<td style="text-align: center;">93.2</td>
<td style="text-align: center;">76.5</td>
<td style="text-align: center;">71.7</td>
<td style="text-align: center;">71.6</td>
<td style="text-align: center;">78.7</td>
</tr>
<tr>
<td style="text-align: left;">CoCa-Large</td>
<td style="text-align: center;">84.8</td>
<td style="text-align: center;">85.7</td>
<td style="text-align: center;">95.6</td>
<td style="text-align: center;">79.6</td>
<td style="text-align: center;">75.7</td>
<td style="text-align: center;">78.6</td>
<td style="text-align: center;">83.3</td>
</tr>
<tr>
<td style="text-align: left;">CoCa</td>
<td style="text-align: center;">$\mathbf{8 6 . 3}$</td>
<td style="text-align: center;">$\mathbf{9 0 . 2}$</td>
<td style="text-align: center;">$\mathbf{9 6 . 5}$</td>
<td style="text-align: center;">$\mathbf{8 0 . 7}$</td>
<td style="text-align: center;">$\mathbf{7 7 . 6}$</td>
<td style="text-align: center;">$\mathbf{8 2 . 7}$</td>
<td style="text-align: center;">$\mathbf{8 5 . 7}$</td>
</tr>
</tbody>
</table>
<p>Table 4: Zero-shot image classification results on ImageNet [9], ImageNet-A [64], ImageNet-R [65], ImageNet-V2 [66], ImageNet-Sketch [67] and ObjectNet [68]. are interested in the zero-shot setting where all parameters are frozen after pretraining and directly used to extract embeddings. Here, we use the same embeddings used for contrastive loss during pretraining, and thus the multimodal text decoder is not used. Zero-Shot Image-Text Retrieval. We evaluate CoCa on the two standard image-text retrieval benchmarks: MSCOCO [63] and Flickr30K [62]. Following the CLIP setting [12], we first independently feed each image/text to the corresponding encoder and obtain embeddings for all image/text in the test set. We then retrieve based on cosine similarity scores over the whole test set. As shown in Table 3, CoCa significantly improves over prior methods on both image-to-text and text-to-image retrievals on all metrics. In addition, our model is parameter-efficient, with CoCa-Base already outperforming strong baselines (CLIP [12] and ALIGN [13]) and CoCa-Large outperforming Florence [14] (which contains a parameter count comparable to ViT-Huge). This shows that CoCa learns good unimodal representations and aligns them well across modalities. Zero-Shot Image Classification. Following prior work [12, 13], we use the aligned image/text embeddings to perform zero-shot image classification by matching images with label names without finetuning. We follow the exact setup in [12] and apply the same set of prompts used for label class names. As shown in Table 4, CoCa sets new state-of-the-art zero-shot classification results on ImageNet. Notably, CoCa uses fewer parameters than prior best model [33] while smaller CoCa variants already outperform strong baselines [12, 14], as shown in Figure 5b. In addition, our model demonstrates effective generalization under zero-shot evaluation, consistent with prior findings [12, 13], with CoCa improving on all six datasets considered. Lastly, while prior models [32, 33] found sequentially pretraining with single-encoder and dual-encoder methods in multiple stages is crucial to performance gains, our results show it is possible to attain strong performance by unifying training objectives and datasets in a single-stage framework. Zero-Shot Video Retrieval. We evaluate video-text retrieval using CoCa on MSR-VTT [71] using the full split. Table 5 shows that CoCa produces the highest retrieval metrics for both text-to-video and video-to-text retrieval. It is important to note that MSR-VTT videos are sourced from YouTube, and we require the original videos to compute our embeddings. Many of the videos have been made explicitly unavailable [72], hence we compute retrieval over the subset of data that is publicly available at the time of evaluation. Using code ${ }^{3}$ provided by the authors of Socratic Models [70], we re-computed metrics on the available subset for those methods, indicated by "(subset)" for fairest comparison.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup> <sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>: ${ }^{3}$ https://socraticmodels.github.io</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">MSR-VTT Full</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Method</td>
<td style="text-align: center;">Text $\rightarrow$ Video</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Video $\rightarrow$ Text</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">R@1</td>
<td style="text-align: center;">R@5</td>
<td style="text-align: center;">R@10</td>
<td style="text-align: center;">R@1</td>
<td style="text-align: center;">R@5</td>
<td style="text-align: center;">R@10</td>
</tr>
<tr>
<td style="text-align: left;">CLIP [69]</td>
<td style="text-align: center;">21.4</td>
<td style="text-align: center;">41.1</td>
<td style="text-align: center;">50.4</td>
<td style="text-align: center;">40.3</td>
<td style="text-align: center;">69.7</td>
<td style="text-align: center;">79.2</td>
</tr>
<tr>
<td style="text-align: left;">Socratic Models [70]</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">44.7</td>
<td style="text-align: center;">71.2</td>
<td style="text-align: center;">80.0</td>
</tr>
<tr>
<td style="text-align: left;">CLIP [69] (subset)</td>
<td style="text-align: center;">23.3</td>
<td style="text-align: center;">44.2</td>
<td style="text-align: center;">53.6</td>
<td style="text-align: center;">43.3</td>
<td style="text-align: center;">73.3</td>
<td style="text-align: center;">$\mathbf{8 1 . 8}$</td>
</tr>
<tr>
<td style="text-align: left;">Socratic Models [70] (subset)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">46.9</td>
<td style="text-align: center;">$\mathbf{7 3 . 5}$</td>
<td style="text-align: center;">81.3</td>
</tr>
<tr>
<td style="text-align: left;">CoCa (subset)</td>
<td style="text-align: center;">$\mathbf{3 0 . 0}$</td>
<td style="text-align: center;">$\mathbf{5 2 . 4}$</td>
<td style="text-align: center;">$\mathbf{6 1 . 6}$</td>
<td style="text-align: center;">$\mathbf{4 9 . 9}$</td>
<td style="text-align: center;">73.4</td>
<td style="text-align: center;">81.4</td>
</tr>
</tbody>
</table>
<p>Table 5: Zero-shot Video-Text Retrieval on MSR-VTT Full test set.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">VQA</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">SNLI-VE</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">NLVR2</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">test-dev</td>
<td style="text-align: center;">test-std</td>
<td style="text-align: center;">dev</td>
<td style="text-align: center;">test</td>
<td style="text-align: center;">dev</td>
<td style="text-align: center;">test-p</td>
</tr>
<tr>
<td style="text-align: center;">UNITER [26]</td>
<td style="text-align: center;">73.8</td>
<td style="text-align: center;">74.0</td>
<td style="text-align: center;">79.4</td>
<td style="text-align: center;">79.4</td>
<td style="text-align: center;">79.1</td>
<td style="text-align: center;">80.0</td>
</tr>
<tr>
<td style="text-align: center;">VinVL [27]</td>
<td style="text-align: center;">76.6</td>
<td style="text-align: center;">76.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">82.7</td>
<td style="text-align: center;">84.0</td>
</tr>
<tr>
<td style="text-align: center;">CLIP-ViL [73]</td>
<td style="text-align: center;">76.5</td>
<td style="text-align: center;">76.7</td>
<td style="text-align: center;">80.6</td>
<td style="text-align: center;">80.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">ALBEF [36]</td>
<td style="text-align: center;">75.8</td>
<td style="text-align: center;">76.0</td>
<td style="text-align: center;">80.8</td>
<td style="text-align: center;">80.9</td>
<td style="text-align: center;">82.6</td>
<td style="text-align: center;">83.1</td>
</tr>
<tr>
<td style="text-align: center;">BLIP [37]</td>
<td style="text-align: center;">78.3</td>
<td style="text-align: center;">78.3</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">82.2</td>
<td style="text-align: center;">82.2</td>
</tr>
<tr>
<td style="text-align: center;">OFA [17]</td>
<td style="text-align: center;">79.9</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">$90.3^{\dagger}$</td>
<td style="text-align: center;">$90.2^{\dagger}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">VLMo [30]</td>
<td style="text-align: center;">79.9</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">85.6</td>
<td style="text-align: center;">86.9</td>
</tr>
<tr>
<td style="text-align: center;">SimVLM [16]</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">80.3</td>
<td style="text-align: center;">86.2</td>
<td style="text-align: center;">86.3</td>
<td style="text-align: center;">84.5</td>
<td style="text-align: center;">85.2</td>
</tr>
<tr>
<td style="text-align: center;">Florence [14]</td>
<td style="text-align: center;">80.2</td>
<td style="text-align: center;">80.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">METER [74]</td>
<td style="text-align: center;">80.3</td>
<td style="text-align: center;">80.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">CoCa</td>
<td style="text-align: center;">82.3</td>
<td style="text-align: center;">82.3</td>
<td style="text-align: center;">87.0</td>
<td style="text-align: center;">87.1</td>
<td style="text-align: center;">86.1</td>
<td style="text-align: center;">87.0</td>
</tr>
</tbody>
</table>
<p>Table 6: Multimodal understanding results comparing vision-language pretraining methods. ${ }^{\dagger}$ OFA uses both image and text premises as inputs while other models utilize the image only.</p>
<h1>4.2.3 Image Captioning and Multimodal Understanding Tasks</h1>
<p>Another key advantage of CoCa is its ability to process multimodal embeddings as an encoder-decoder model trained with the generative objective. Therefore, CoCa can perform both image captioning and multimodal understanding downstream tasks without any further fusion adaptation [73, 74]. Overall, experimental results suggest CoCa reaps the benefit of a encoder-decoder model to obtain strong multimodal understanding and generation capabilities, in addition to the vision and retrieval capabilities as a dual-encoder method.
Multimodal Understanding. As shown in [16], the output of encoder-decoder models can jointly encode image and text inputs, and can be used for tasks that require reasoning over both modalities. We consider three popular multimodal understaning benchmarks: visual question answering (VQA v2 [75]), visual entailment (SNLI-VE [76]), and visual reasoning (NLVR2 [77]). We mainly follow the settings in [16] and train linear classifiers on top of the decoder outputs to predict answers (more details in Appendix B). Our results in Table 6 suggest that CoCa outperforms strong visionlanguage pretraining (VLP) baselines and obtains state-of-the-art performance on all three tasks. While prior dual-encoder models [12, 14] do not contain fusion layers and thus require an additional VL pretraining stage for downstream multimodal understanding tasks, CoCa subsumes the three pretraining paradigms and obtains better performance on VL tasks with lightweight finetuning.
Image Captioning. In addition to multimodal classification tasks, CoCa is also directly applicable to image captioning tasks as an encoder-decoder model. We finetune CoCa with the captioning loss $\mathcal{L}_{\text {Cap }}$ only on MSCOCO [63] captioning task and evaluate on both MSCOCO Karpathy-test split and NoCaps [78] online evaluation. As shown by experiments in Table 7, CoCa outperforms strong baselines trained with cross-entropy loss on MSCOCO, and achieves results comparable to methods with CIDEr metric-specific optimization [79]. It is noteworthy that we do not use CIDEr-specific optimization [79] for simplicity. On the challenging NoCaps benchmark, CoCa obtains new state-of-the-art on both validation and test splits (generated examples shown in Figure 6). These results showcase the generative capability of CoCa as an image-text foundation model.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">MSCOCO</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">NoCaps</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">B@4</td>
<td style="text-align: center;">M</td>
<td style="text-align: center;">C</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">Valid C</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">Test C</td>
<td style="text-align: center;">S</td>
</tr>
<tr>
<td style="text-align: center;">CLIP-ViL [73]</td>
<td style="text-align: center;">40.2</td>
<td style="text-align: center;">29.7</td>
<td style="text-align: center;">134.2</td>
<td style="text-align: center;">23.8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">BLIP [37]</td>
<td style="text-align: center;">40.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">136.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">113.2</td>
<td style="text-align: center;">14.8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">VinVL[27]</td>
<td style="text-align: center;">41.0</td>
<td style="text-align: center;">31.1</td>
<td style="text-align: center;">140.9</td>
<td style="text-align: center;">25.4</td>
<td style="text-align: center;">105.1</td>
<td style="text-align: center;">14.4</td>
<td style="text-align: center;">103.7</td>
<td style="text-align: center;">14.4</td>
</tr>
<tr>
<td style="text-align: center;">SimVLM [16]</td>
<td style="text-align: center;">40.6</td>
<td style="text-align: center;">33.7</td>
<td style="text-align: center;">143.3</td>
<td style="text-align: center;">25.4</td>
<td style="text-align: center;">112.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">110.3</td>
<td style="text-align: center;">14.5</td>
</tr>
<tr>
<td style="text-align: center;">LEMON [80]</td>
<td style="text-align: center;">41.5</td>
<td style="text-align: center;">30.8</td>
<td style="text-align: center;">139.1</td>
<td style="text-align: center;">24.1</td>
<td style="text-align: center;">117.3</td>
<td style="text-align: center;">15.0</td>
<td style="text-align: center;">114.3</td>
<td style="text-align: center;">14.9</td>
</tr>
<tr>
<td style="text-align: center;">$\operatorname{LEMON}_{\text {SCST }}$ [80] ${ }^{\dagger}$</td>
<td style="text-align: center;">42.6</td>
<td style="text-align: center;">31.4</td>
<td style="text-align: center;">145.5</td>
<td style="text-align: center;">25.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">OFA [17] ${ }^{\dagger}$</td>
<td style="text-align: center;">43.5</td>
<td style="text-align: center;">31.9</td>
<td style="text-align: center;">149.6</td>
<td style="text-align: center;">26.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">CoCa</td>
<td style="text-align: center;">40.9</td>
<td style="text-align: center;">33.9</td>
<td style="text-align: center;">143.6</td>
<td style="text-align: center;">24.7</td>
<td style="text-align: center;">122.4</td>
<td style="text-align: center;">15.5</td>
<td style="text-align: center;">120.6</td>
<td style="text-align: center;">15.5</td>
</tr>
</tbody>
</table>
<p>Table 7: Image captioning results on MSCOCO and NoCaps (B@4: BLEU@4, M: METEOR, C: CIDEr, S: SPICE). ${ }^{\dagger}$ Models finetuned with CIDEr optimization.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Curated samples of text captions generated by CoCa with NoCaps images as input.</p>
<h1>4.3 Ablation Analysis</h1>
<p>We extensively ablate the properties of CoCa on a smaller model variant. Specifically, we train CoCa-Base with a reduced 12 decoder layers and a total batch size of 4,096. We mainly evaluate using zero-shot image classification and VQA, since the former covers both visual representation quality and crossmodal alignment, while the later is representative for multimodal reasoning.
Captioning vs. Classification. We first examine the effectiveness of captioning loss on image annotation datasets. To do this, we train a naive encoder-decoder model using $\mathcal{L}<em _Cls="{Cls" _text="\text">{\text {Cap }}$ on the JFT-3B dataset, and compare with a standard ViT-Base single-encoder model trained with $\mathcal{L}</em>}}$ in Table 8a. We find encoder-decoder models to perform on par with single-encoder pretraining on both linear evaluation and finetuned results. This suggests that the generative pretraining subsumes classification pretraining, consistent with our intuition that $\mathcal{L<em _Cap="{Cap" _text="\text">{\text {Cls }}$ is a special case of $\mathcal{L}</em>$ when text vocabulary is the set of all possible class names. Thus, our CoCa model can be interpreted as an effective unification of the three paradigms. This explains why CoCa does not need a pretrained visual encoder to perform well.
Training Objectives. We study the effects of the two training objectives and compare CoCa with single-objective variants in Table 8b. Compared to the contrastive-only model, CoCa significantly improves both zero-shot alignment and VQA (notice that the contrastive-only model requires additional fusion for VQA). CoCa performs on par with the captioning-only model on VQA while it additionally enables retrieval-style tasks such as zero-shot classification. Table 8c further studies loss ratios and suggests that the captioning loss not only improves VQA but also zero-shot alignment between modalities. We hypothesize that generative objectives learn fine-grained text representations that further improve text understanding. Finally, we compare training costs in Table 8b (measured in TPUv3-core-days; larger is slower) and find CoCa to be as efficient as the captioning-only model (a.k.a.naive encoder-decoder) due to the sharing of compute between two objectives. These suggest combining the two losses induces new capabilities and better performance with minimal extra cost.
Unimodal and Multimodal Decoders. CoCa introduces a novel decoder design and we ablate its components. In Table 8d, we vary the number of unimodal decoder layers (while keeping the total number of layers the same). Intuitively, fewer unimodal text layers leads to worse zero-shot classification due to lack of capacity for good unimodal text understanding, while fewer multimodal layers reduces the model's power to reason over multimodal inputs such as VQA. Overall, we}</p>
<table>
<thead>
<tr>
<th style="text-align: left;">loss</th>
<th style="text-align: center;">LE</th>
<th style="text-align: center;">FT</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$\mathcal{L}_{\text {Cls }}$</td>
<td style="text-align: center;">81.0</td>
<td style="text-align: center;">85.1</td>
</tr>
<tr>
<td style="text-align: left;">$\mathcal{L}_{\text {Cap }}$</td>
<td style="text-align: center;">82.1</td>
<td style="text-align: center;">84.9</td>
</tr>
</tbody>
</table>
<p>(a) Encoder-decoder vs. singleencoder models (trained on JFT).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">loss</th>
<th style="text-align: center;">ZS</th>
<th style="text-align: center;">VQA</th>
<th style="text-align: center;">TPU cost</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\mathcal{L}_{\text {Con }}$</td>
<td style="text-align: center;">70.7</td>
<td style="text-align: center;">59.2</td>
<td style="text-align: center;">$1 \times$</td>
</tr>
<tr>
<td style="text-align: center;">$\mathcal{L}_{\text {Cap }}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">68.9</td>
<td style="text-align: center;">$1.17 \times$</td>
</tr>
<tr>
<td style="text-align: center;">$\mathcal{L}_{\text {CoCa }}$</td>
<td style="text-align: center;">71.6</td>
<td style="text-align: center;">69.0</td>
<td style="text-align: center;">$\mathbf{1 . 1 8 \times}$</td>
</tr>
</tbody>
</table>
<p>(b) Training objectives ablation.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">variant</th>
<th style="text-align: center;">AE</th>
<th style="text-align: center;">MSCOCO</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1 [CLS]</td>
<td style="text-align: center;">80.7</td>
<td style="text-align: center;">41.4</td>
</tr>
<tr>
<td style="text-align: center;">+ text tokens</td>
<td style="text-align: center;">80.3</td>
<td style="text-align: center;">40.2</td>
</tr>
<tr>
<td style="text-align: center;">8 [CLS]</td>
<td style="text-align: center;">80.3</td>
<td style="text-align: center;">36.9</td>
</tr>
<tr>
<td style="text-align: center;">+ text tokens</td>
<td style="text-align: center;">80.4</td>
<td style="text-align: center;">40.3</td>
</tr>
</tbody>
</table>
<p>(e) Contrastive text embedding design ablation.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">$\lambda_{\text {Cap }}: \lambda_{\text {Con }}$</th>
<th style="text-align: center;">ZS</th>
<th style="text-align: center;">VQA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1:1</td>
<td style="text-align: center;">71.5</td>
<td style="text-align: center;">68.6</td>
</tr>
<tr>
<td style="text-align: center;">1:2</td>
<td style="text-align: center;">71.0</td>
<td style="text-align: center;">68.1</td>
</tr>
<tr>
<td style="text-align: center;">2:1</td>
<td style="text-align: center;">71.6</td>
<td style="text-align: center;">69.0</td>
</tr>
</tbody>
</table>
<p>(c) Training objectives weights.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">variant</th>
<th style="text-align: center;">ZS</th>
<th style="text-align: center;">VQA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">parallel</td>
<td style="text-align: center;">71.2</td>
<td style="text-align: center;">68.7</td>
</tr>
<tr>
<td style="text-align: center;">cascade</td>
<td style="text-align: center;">71.6</td>
<td style="text-align: center;">69.0</td>
</tr>
<tr>
<td style="text-align: center;">$n_{\text {query }}=0$</td>
<td style="text-align: center;">71.5</td>
<td style="text-align: center;">69.0</td>
</tr>
<tr>
<td style="text-align: center;">$n_{\text {query }}=1$</td>
<td style="text-align: center;">69.3</td>
<td style="text-align: center;">64.4</td>
</tr>
<tr>
<td style="text-align: center;">$n_{\text {query }}=32$</td>
<td style="text-align: center;">71.2</td>
<td style="text-align: center;">68.2</td>
</tr>
</tbody>
</table>
<p>(f) Attentional pooler design ablation.</p>
<p>Table 8: CoCa ablation experiments. On ImageNet classification, we report top-1 accuracy for: zero-shot (ZS), linear evaluation (LE), attentional evaluation (AE) using pooler on frozen feature, and finetuning (FT). On MSCOCO retrieval, we report the average of image-to-text and text-to-image $\mathrm{R} @ 1$. On VQA, we report the dev-set vqa score. The default CoCa setting is bold.
find decoupling the decoder in half maintains a good balance. One possibility is that global text representation for retrieval doesn't require deep modules [33] while early fusion for shallow layers may also be unnecessary for multimodal understanding. Next, we explore various options to extract unimodal text embeddings. In particular, we experiment with the number of learnable [CLS] tokens as well as the aggregation design. For the later, we aggregate over either the [CLS] tokens only or the concatenation of [CLS] and the original input sentence. Interestingly, in Table 8 e we find training a single [CLS] token without the original input is preferred for both vision-only and crossmodal retrieval tasks. This indicates that learning an additional simple sentence representation mitigates interference between contrastive and captioning loss, and is powerful enough for strong generalization.
Attentional Poolers. CoCa exploits attentional poolers in its design both for different pretraining objectives and objective-specific downstream task adaptations. In pretraining, we compare a few design variants on using poolers for contrastive loss and generative loss: (1) the "parallel" design which extracts both contrastive and generative losses at the same time on Vision Transformer encoder outputs as shown in Figure 2, and (2) the "cascade" design which applies the contrastive pooler on top of the outputs of the generative pooler. Table 8 f shows the results of these variants. Empirically, we find at small scale the "cascade" version (contrastive pooler on top of the generative pooler) performs better and is used by default in all CoCa models. We also study the effect of number of queries where $n_{\text {query }}=0$ means no generative pooler is used (thus all ViT output tokens are used for decoder cross-attention). Results show that both tasks prefer longer sequences of detailed image tokens at a cost of slightly more computation and parameters. As a result, we use a generative pooler of length 256 to improve multimodal understanding benchmarks while still maintaining the strong frozen-feature capability.</p>
<h1>5 Broader Impacts</h1>
<p>This work presents an image-text pretraining approach on web-scale datasets that is capable of transferring to a wide range of downstream tasks in a zero-shot manner or with lightweight finetuning. While the pretrained models are capable of many vision and vision-language tasks, we note that our models use the same pretraining data as previous methods [13, 21, 32, 33] and additional analysis of the data and the resulting model is necessary before the use of the models in practice. We show CoCa models are more robust on corrupted images, but it could still be vulnerable to other image corruptions that are not yet captured by current evaluation sets or in real-world scenarios. For both</p>
<p>the data and model, further community exploration is required to understand the broader impacts including but not limited to fairness, social bias and potential misuse.</p>
<h1>6 Conclusion</h1>
<p>In this work we present Contrastive Captioners (CoCa), a new image-text foundation model family that subsumes existing vision pretraining paradigms with natural language supervision. Pretrained on image-text pairs from various data sources in a single stage, CoCa efficiently combines contrastive and captioning objectives in an encoder-decoder model. CoCa obtains a series of state-of-the-art performance with a single checkpoint on a wide spectrum of vision and vision-language problems. Our work bridges the gap among various pretraining approaches and we hope it motivates new directions for image-text foundation models.</p>
<h2>Acknowledgments</h2>
<p>We would like to thank Yi-Ting Chen, Kaifeng Chen, Ye Xia, Zhen Li, Chao Jia, Yinfei Yang, Zhengdong Zhang, Wei Han, Yuan Cao, Tao Zhu, Futang Peng, Soham Ghosh, Zihang Dai, Junnan Li, Ning Wang, Xin Li, Anelia Angelova, Jason Baldridge, Izhak Shafran, Shengyang Dai, Abhijit Ogale, Zhifeng Chen, Claire Cui, Paul Natsev, Tom Duerig for helpful discussions, Andrew Dai for help with contrastive models, Christopher Fifty and Bowen Zhang for help with video models, Yuanzhong Xu for help with model scaling, Lucas Beyer for help with data preparation, Andy Zeng for help with MSR-VTT evaluation, Hieu Pham and Simon Kornblith for help with zero-shot evaluations, Erica Moreira and Victor Gomes for help with resource coordination, Tom Small for help with visual illustration, Liangliang Cao for proofreading, and others in the Google Brain team for support throughout this project.</p>
<h2>References</h2>
<p>[1] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.
[2] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
[3] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683, 2019.
[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.
[5] Paul Barham, Aakanksha Chowdhery, Jeff Dean, Sanjay Ghemawat, Steven Hand, Dan Hurt, Michael Isard, Hyeontaek Lim, Ruoming Pang, Sudip Roy, et al. Pathways: Asynchronous distributed dataflow for ml. arXiv preprint arXiv:2203.12533, 2022.
[6] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 580-587, 2014.
[7] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3431-3440, 2015.
[8] Karen Simonyan and Andrew Zisserman. Two-stream convolutional networks for action recognition in videos. Advances in neural information processing systems, 27, 2014.
[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A largescale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248-255. Ieee, 2009.</p>
<p>[10] Zihang Dai, Hanxiao Liu, Quoc V Le, and Mingxing Tan. Coatnet: Marrying convolution and attention for all data sizes. Advances in Neural Information Processing Systems, 34:3965-3977, 2021.
[11] Bowen Zhang, Jiahui Yu, Christopher Fifty, Wei Han, Andrew M Dai, Ruoming Pang, and Fei Sha. Co-training transformer with videos and images improves action recognition. arXiv preprint arXiv:2112.07175, 2021.
[12] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 8748-8763. PMLR, 2021.
[13] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, YunHsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International Conference on Machine Learning, pages 4904-4916. PMLR, 2021.
[14] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, et al. Florence: A new foundation model for computer vision. arXiv preprint arXiv:2111.11432, 2021.
[15] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural image caption generator. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3156-3164, 2015.
[16] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. Simvlm: Simple visual language model pretraining with weak supervision. arXiv preprint arXiv:2108.10904, 2021.
[17] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. arXiv preprint arXiv:2202.03052, 2022.
[18] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25, 2012.
[19] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.
[20] Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, and Laurens Van Der Maaten. Exploring the limits of weakly supervised pretraining. In Proceedings of the European conference on computer vision (ECCV), pages 181-196, 2018.
[21] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers, 2021.
[22] Hangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv preprint arXiv:2106.08254, 2021.
[23] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are scalable vision learners. arXiv preprint arXiv:2111.06377, 2021.
[24] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: A simple framework for masked image modeling. arXiv preprint arXiv:2111.09886, 2021.
[25] Hao Tan and Mohit Bansal. Lxmert: Learning cross-modality encoder representations from transformers. arXiv preprint arXiv:1908.07490, 2019.
[26] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter: Universal image-text representation learning. In ECCV, 2020.
[27] Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao. Vinvl: Revisiting visual representations in vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 5579-5588, June 2021.</p>
<p>[28] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015.
[29] Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-and-language transformer without convolution or region supervision, 2021.
[30] Wenhui Wang, Hangbo Bao, Li Dong, and Furu Wei. Vlmo: Unified vision-language pretraining with mixture-of-modality-experts. arXiv preprint arXiv:2111.02358, 2021.
[31] Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Bin Xiao, Ce Liu, Lu Yuan, and Jianfeng Gao. Unified contrastive learning in image-text-label space, 2022.
[32] Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, and Lucas Beyer. Lit: Zero-shot transfer with locked-image text tuning. arXiv preprint arXiv:2111.07991, 2021.
[33] Hieu Pham, Zihang Dai, Golnaz Ghiasi, Kenji Kawaguchi, Hanxiao Liu, Adams Wei Yu, Jiahui Yu, Yi-Ting Chen, Minh-Thang Luong, Yonghui Wu, Mingxing Tan, and Quoc V. Le. Combined scaling for open-vocabulary image classification, 2021.
[34] AJ Piergiovanni, Wei Li, Weicheng Kuo, Mohammad Saffar, Fred Bertsch, and Anelia Angelova. Answer-me: Multi-task open-vocabulary visual question answering. arXiv preprint arXiv:2205.00949, 2022.
[35] Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela. Flava: A foundational language and vision alignment model. arXiv preprint arXiv:2112.04482, 2021.
[36] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi. Align before fuse: Vision and language representation learning with momentum distillation. Advances in Neural Information Processing Systems, 34, 2021.
[37] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping languageimage pre-training for unified vision-language understanding and generation. arXiv preprint arXiv:2201.12086, 2022.
[38] Anders Andreassen, Yasaman Bahri, Behnam Neyshabur, and Rebecca Roelofs. The evolution of out-of-distribution robustness throughout fine-tuning. arXiv preprint arXiv:2106.15831, 2021.
[39] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021.
[40] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages $770-778,2016$.
[41] Ronald J Williams and David Zipser. A learning algorithm for continually running fully recurrent neural networks. Neural computation, 1(2):270-280, 1989.
[42] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye Teh. Set transformer: A framework for attention-based permutation-invariant neural networks. In International Conference on Machine Learning, pages 3744-3753. PMLR, 2019.
[43] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909, 2015.
[44] Taku Kudo. Subword regularization: Improving neural network translation models with multiple subword candidates. arXiv preprint arXiv:1804.10959, 2018.
[45] Jonathan Shen, Patrick Nguyen, Yonghui Wu, Zhifeng Chen, et al. Lingvo: a modular and scalable framework for sequence-to-sequence modeling, 2019.
[46] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, et al. Gpipe: Efficient training of giant neural networks using pipeline parallelism. Advances in neural information processing systems, 32, 2019 .</p>
<p>[47] Yuanzhong Xu, HyoukJoong Lee, Dehao Chen, Hongjun Choi, Blake Hechtman, and Shibo Wang. Automatic cross-replica sharding of weight update in data-parallel training. arXiv preprint arXiv:2004.13336, 2020.
[48] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020.
[49] Yuanzhong Xu, HyoukJoong Lee, Dehao Chen, Blake Hechtman, Yanping Huang, Rahul Joshi, Maxim Krikun, Dmitry Lepikhin, Andy Ly, Marcello Maggioni, et al. Gspmd: general and scalable parallelization for ml computation graphs. arXiv preprint arXiv:2105.04663, 2021.
[50] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost. In International Conference on Machine Learning, pages 4596-4604. PMLR, 2018.
[51] Hieu Pham, Zihang Dai, Qizhe Xie, and Quoc V Le. Meta pseudo labels. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11557-11568, 2021.
[52] Mitchell Wortsman, Gabriel Ilharco, Samir Yitzhak Gadre, Rebecca Roelofs, Raphael GontijoLopes, Ari S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, et al. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. arXiv preprint arXiv:2203.05482, 2022.
[53] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lučić, and Cordelia Schmid. Vivit: A video vision transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6836-6846, 2021.
[54] Dan Kondratyuk, Liangzhe Yuan, Yandong Li, Li Zhang, Mingxing Tan, Matthew Brown, and Boqing Gong. Movinets: Mobile video networks for efficient video recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16020-16030, 2021.
[55] Hassan Akbari, Liangzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin Cui, and Boqing Gong. Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text. Advances in Neural Information Processing Systems, 34, 2021.
[56] Chen Wei, Haoqi Fan, Saining Xie, Chao-Yuan Wu, Alan Yuille, and Christoph Feichtenhofer. Masked feature prediction for self-supervised visual pre-training. arXiv preprint arXiv:2112.09133, 2021.
[57] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv preprint arXiv:1705.06950, 2017.
[58] Joao Carreira, Eric Noland, Andras Banki-Horvath, Chloe Hillier, and Andrew Zisserman. A short note about kinetics-600. arXiv preprint arXiv:1808.01340, 2018.
[59] Joao Carreira, Eric Noland, Chloe Hillier, and Andrew Zisserman. A short note on the kinetics700 human action dataset. arXiv preprint arXiv:1907.06987, 2019.
[60] Mathew Monfort, Alex Andonian, Bolei Zhou, Kandan Ramakrishnan, Sarah Adel Bargal, Tom Yan, Lisa Brown, Quanfu Fan, Dan Gutfreund, Carl Vondrick, et al. Moments in time dataset: one million videos for event understanding. IEEE transactions on pattern analysis and machine intelligence, 42(2):502-508, 2019.
[61] Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and Chunjing Xu. Filip: Fine-grained interactive language-image pre-training. arXiv preprint arXiv:2111.07783, 2021.
[62] Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In Proceedings of the IEEE international conference on computer vision, pages 2641-2649, 2015.
[63] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015.</p>
<p>[64] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15262-15271, 2021.
[65] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8340-8349, 2021.
[66] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to imagenet? In International Conference on Machine Learning, pages 5389-5400. PMLR, 2019.
[67] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global representations by penalizing local predictive power. Advances in Neural Information Processing Systems, 32, 2019.
[68] Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan Gutfreund, Josh Tenenbaum, and Boris Katz. Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models. Advances in neural information processing systems, 32, 2019.
[69] Jesús Andrés Portillo-Quintero, José Carlos Ortiz-Bayliss, and Hugo Terashima-Marín. A straightforward framework for video retrieval using clip. arXiv preprint arXiv:2102.12443, 2021.
[70] Andy Zeng, Adrian Wong, Stefan Welker, Krzysztof Choromanski, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, and Pete Florence. Socratic models: Composing zero-shot multimodal reasoning with language. arXiv preprint arXiv:2204.00598, 2022.
[71] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging video and language. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5288-5296, 2016.
[72] Lucas Smaira, João Carreira, Eric Noland, Ellen Clancy, Amy Wu, and Andrew Zisserman. A short note on the kinetics-700-2020 human action dataset. arXiv preprint arXiv:2010.10864, 2020.
[73] Sheng Shen, Liunian Harold Li, Hao Tan, Mohit Bansal, Anna Rohrbach, Kai-Wei Chang, Zhewei Yao, and Kurt Keutzer. How much can clip benefit vision-and-language tasks? arXiv preprint arXiv:2107.06383, 2021.
[74] Zi-Yi Dou, Yichong Xu, Zhe Gan, Jianfeng Wang, Shuohang Wang, Lijuan Wang, Chenguang Zhu, Zicheng Liu, Michael Zeng, et al. An empirical study of training end-to-end vision-andlanguage transformers. arXiv preprint arXiv:2111.02387, 2021.
[75] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6904-6913, 2017.
[76] Ning Xie, Farley Lai, Derek Doran, and Asim Kadav. Visual entailment: A novel task for fine-grained image understanding. arXiv preprint arXiv:1901.06706, 2019.
[77] Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, and Yoav Artzi. A corpus for reasoning about natural language grounded in photographs. arXiv preprint arXiv:1811.00491, 2018.
[78] Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Stefan Lee, and Peter Anderson. nocaps: novel object captioning at scale. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8948-8957, 2019.
[79] Steven J Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava Goel. Selfcritical sequence training for image captioning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7008-7024, 2017.
[80] Xiaowei Hu, Zhe Gan, Jianfeng Wang, Zhengyuan Yang, Zicheng Liu, Yumao Lu, and Lijuan Wang. Scaling up vision-language pre-training for image captioning. arXiv preprint arXiv:2111.12233, 2021.</p>
<h1>A Visual Recognition Finetuning Details</h1>
<table>
<thead>
<tr>
<th style="text-align: center;">Hyper-parameter</th>
<th style="text-align: center;">ImageNet</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Kinetics-400/600/700</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Moments-in-Time</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Frozen-feature</td>
<td style="text-align: center;">Finetuning</td>
<td style="text-align: center;">Frozen-feature</td>
<td style="text-align: center;">Finetuning</td>
<td style="text-align: center;">Frozen-feature</td>
<td style="text-align: center;">Finetuning</td>
</tr>
<tr>
<td style="text-align: center;">Optimizer</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Ada</td>
<td style="text-align: center;">ture with Decoupled</td>
<td style="text-align: center;">eight Decay</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Gradient clip</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">EMA decay rate</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.9999</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">LR decay schedule</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Cosine Schedule Decaying to Zero</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Loss</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Softmax</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">MixUp</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">None</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">CutMix</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">None</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">AutoAugment</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">None</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">RepeatedAugment</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">None</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">RandAugment</td>
<td style="text-align: center;">2, 20</td>
<td style="text-align: center;">2, 20</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">None</td>
<td style="text-align: center;">None</td>
<td style="text-align: center;">None</td>
</tr>
<tr>
<td style="text-align: center;">Label smoothing</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;">Train steps</td>
<td style="text-align: center;">200k</td>
<td style="text-align: center;">200k</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">120k</td>
<td style="text-align: center;">120k</td>
<td style="text-align: center;">120k</td>
</tr>
<tr>
<td style="text-align: center;">Train batch size</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">128</td>
</tr>
<tr>
<td style="text-align: center;">Pooler LR</td>
<td style="text-align: center;">$5 \mathrm{e}-4$</td>
<td style="text-align: center;">$5 \mathrm{e}-4$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$5 \mathrm{e}-4$</td>
<td style="text-align: center;">$5 \mathrm{e}-4$</td>
<td style="text-align: center;">$5 \mathrm{e}-4$</td>
</tr>
<tr>
<td style="text-align: center;">Encoder LR</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">$5 \mathrm{e}-4$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">$5 \mathrm{e}-4$</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;">Warm-up steps</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">1000</td>
</tr>
<tr>
<td style="text-align: center;">Weight decay rate</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
</tr>
</tbody>
</table>
<p>Table 9: Hyper-parameters used in the visual recognition experiments.</p>
<p>In addition to zero-shot transfer, we evaluate frozen-feature and finetuning performance of CoCa on visual recognition tasks. For frozen-feature evaluation, we add an attentional pooling layer (pooler) on top of the output sequence of visual features and an additional softmax cross entropy loss layer to learn classification of images and videos. For finetuning, we adapt the same architecture as frozen-feature evaluation (thus also with poolers) and finetune both encoder and pooler. All learning hyperparameters are listed in Table 9.</p>
<h2>B Multimodal Understanding Finetuning Details</h2>
<table>
<thead>
<tr>
<th style="text-align: left;">Hyper-parameter</th>
<th style="text-align: center;">VQA</th>
<th style="text-align: center;">SNLI-VE</th>
<th style="text-align: center;">NLVR2</th>
<th style="text-align: center;">MSCOCO</th>
<th style="text-align: center;">NoCaps</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Optimizer</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Ada</td>
<td style="text-align: center;">ture with Decoupled</td>
<td style="text-align: center;">eight Decay</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Gradient clip</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">LR decay schedule</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Cosine S</td>
<td style="text-align: center;">edule Decaying to Zero</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">RandAugment</td>
<td style="text-align: center;">1,10</td>
<td style="text-align: center;">1,10</td>
<td style="text-align: center;">None</td>
<td style="text-align: center;">None</td>
<td style="text-align: center;">None</td>
</tr>
<tr>
<td style="text-align: left;">Train steps</td>
<td style="text-align: center;">100k</td>
<td style="text-align: center;">50k</td>
<td style="text-align: center;">50k</td>
<td style="text-align: center;">50k</td>
<td style="text-align: center;">10k</td>
</tr>
<tr>
<td style="text-align: left;">Train batch size</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">128</td>
</tr>
<tr>
<td style="text-align: left;">Pooler LR</td>
<td style="text-align: center;">$5 \mathrm{e}-4$</td>
<td style="text-align: center;">$1 \mathrm{e}-3$</td>
<td style="text-align: center;">$5 \mathrm{e}-4$</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">NA</td>
</tr>
<tr>
<td style="text-align: left;">Encoder LR</td>
<td style="text-align: center;">$2 \mathrm{e}-5$</td>
<td style="text-align: center;">$5 \mathrm{e}-5$</td>
<td style="text-align: center;">$2 \mathrm{e}-5$</td>
<td style="text-align: center;">$1 \mathrm{e}-5$</td>
<td style="text-align: center;">$1 \mathrm{e}-5$</td>
</tr>
<tr>
<td style="text-align: left;">Warm-up steps</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">1000</td>
</tr>
<tr>
<td style="text-align: left;">Weight decay rate</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.1</td>
</tr>
</tbody>
</table>
<p>Table 10: Hyper-parameters used in the multimodal experiments.</p>
<p>CoCa is an encoder-decoder model and the final decoder outputs can be used for multimodal understanding/generation. Thus, we evaluate on popular vision-language benchmarks. We mainly follow the same setup introduced in [16]. All hyper-parameters are listed in Table 10.</p>
<p>For multimodal classification, we feed the image into the encoder and the corresponding text to the decoder. We then apply another attentional pooler with a single query to extract embedding from the decoder output, and train a linear classifier on top of the pooled embedding. For VQA v2 [75], we follow prior work and formulate the task as a classification problem over 3,129 most frequent answers in the training set. We additionally enable cotraining with the generative loss on the concatenated pairs of textual questions and answers to improve model robustness. Similarly for SNLI-VE, the image and the textual hypothesis are fed to encoder and decoder separately, and the classifier is trained to predict the relation between them as entailment, neutral or contradiction. For NLVR2, we</p>
<p>create two input pairs of each image and the text description, and concatenate them as input to the classifier. We do not use image augmentation for NLVR2.</p>
<p>For image captioning, we apply simple cross-entropy loss (same as the captioning loss used in pretraining) and finetune the model on the training split of MSCOCO to predict for MSCOCO test split and NoCaps online evaluation.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{\dagger}$ Equal contribution.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7795 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7795</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7795</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-145.html">extraction-schema-145</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <p><strong>Paper ID:</strong> paper-276647742</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.19614v2.pdf" target="_blank">Is Your Paper Being Reviewed by an LLM? Benchmarking AI Text Detection in Peer Review</a></p>
                <p><strong>Paper Abstract:</strong> Peer review is a critical process for ensuring the integrity of published scientific research. Confidence in this process is predicated on the assumption that experts in the relevant domain give careful consideration to the merits of manuscripts which are submitted for publication. With the recent rapid advancements in large language models (LLMs), a new risk to the peer review process is that negligent reviewers will rely on LLMs to perform the often time consuming process of reviewing a paper. However, there is a lack of existing resources for benchmarking the detectability of AI text in the domain of peer review. To address this deficiency, we introduce a comprehensive dataset containing a total of 788,984 AI-written peer reviews paired with corresponding human reviews, covering 8 years of papers submitted to each of two leading AI research conferences (ICLR and NeurIPS). We use this new resource to evaluate the ability of 18 existing AI text detection algorithms to distinguish between peer reviews fully written by humans and different state-of-the-art LLMs. Additionally, we explore a context-aware detection method called Anchor, which leverages manuscript content to detect AI-generated reviews, and analyze the sensitivity of detection models to LLM-assisted editing of human-written text. Our work reveals the difficulty of identifying AI-generated text at the individual peer review level, highlighting the urgent need for new tools and methods to detect this unethical use of generative AI. Our dataset is publicly available at: https://huggingface.co/datasets/IntelLabs/AI-Peer-Review-Detection-Benchmark.</p>
                <p><strong>Cost:</strong> 0.03</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7795.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7795.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Anchor</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Anchor embedding context-aware detection method</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A context-aware detection method that generates a synthetic 'anchor review' for the manuscript using an LLM, embeds both the anchor and the test review, and classifies a review as AI-generated if their semantic (cosine) similarity exceeds a learned threshold; supports voting over multiple anchor LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o, Gemini 1.5 pro, Claude Sonnet 3.5 (used as anchor LLMs in voting experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science (peer review / NLP)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation method / detection framework</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Anchor (anchor-embedding semantic similarity)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Generate an anchor review for the same manuscript with an LLM, compute embeddings (text-embedding-003-small), measure cosine similarity between anchor and test review embeddings, compare to threshold θ learned on a calibration set; optionally generate multiple anchors and apply OR voting.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Cosine similarity compared to learned threshold; downstream detection metrics: TPR at fixed FPR, AUC, ROC</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Similarity = cosine(EmbAR, EmbTR) (unitless, range -1 to 1); thresholds learned to achieve target False Positive Rate (e.g., 0.1%, 0.5%, 1%); reporting true positive rate (TPR) at those FPRs and AUC (area under ROC).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>AI-Peer-Review-Detection-Benchmark (this paper's calibration/test/extended splits)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Human qualitative analysis performed elsewhere in paper (32 review pairs analyzed) but Anchor itself evaluated automatically against held-out test set; no manual adjudication for Anchor labels beyond using human-written reviews as negative class.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Anchor achieved the highest AUC among evaluated methods and a TPR of 63.5% for GPT-4o reviews at a target FPR of 0.1% (compared to 17.1% for the next-best baseline, Binoculars); Anchor also showed strong TPRs at 0.5% and 1% FPR.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Anchor outperformed 18 baseline detectors on commercial-model reviews (GPT-4o, Gemini, Claude) in AUC and in TPR at very low FPRs; particularly large gains on GPT-4o samples.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Task-specific (requires manuscript context), relies on anchor LLM choice (sensitivity), depends on commercial LLM APIs (cost & scalability), and not a general-purpose detector; performance can vary if anchor LLMs differ from source LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is Your Paper Being Reviewed by an LLM? Benchmarking AI Text Detection in Peer Review', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7795.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7795.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Binoculars</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Binoculars zero-shot detection method</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A zero-shot detection method that uses two perspectives: observer-model perplexity on text and cross-perplexity where a performer LLM's next-token predictions are scored by the observer; final score is perplexity-to-cross-perplexity ratio.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Spotting llms with binoculars: Zero-shot detection of machine-generated text</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science (NLP, text detection)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>detection algorithm</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Binoculars (perplexity and cross-perplexity ratio)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute observer-model perplexity on the text and cross-perplexity by scoring performer-model next-token predictions with the observer model; use the ratio between perplexity and cross-perplexity as a detection score (higher indicates likely machine-generated).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Detection score (perplexity ratio); ROC / AUC; TPR at fixed FPRs (0.1%, 0.5%, 1%)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Perplexity and cross-perplexity are exponentiated negative average log-probabilities (unitless); detection threshold calibrated on calibration set to achieve target FPR; TPR reported as % of AI reviews correctly flagged.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>AI-Peer-Review-Detection-Benchmark (used for evaluation in this paper); Binoculars' original evaluation in its own paper</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Among the 18 baselines evaluated, Binoculars was the strongest baseline; e.g., for GPT-4o reviews it reached 17.1% TPR at 0.1% FPR and 45.2% TPR at 1% FPR (on the withheld test set calibrated on ICLR reviews).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Binoculars outperformed most metric- and model-based baselines but was substantially outperformed by the Anchor method on GPT-4o samples at low FPRs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Performance depends on choice of observer/performer surrogate models and can be sensitive to in-domain calibration; may require access to surrogate LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is Your Paper Being Reviewed by an LLM? Benchmarking AI Text Detection in Peer Review', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7795.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7795.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DetectGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DetectGPT (probability curvature zero-shot detector)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A zero-shot machine-generated text detector that uses probability curvature: it measures how the log-probability of text responds to local perturbations, hypothesizing that generated text lies near local probability maxima.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science (NLP)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>detection algorithm</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>DetectGPT (probability curvature / perturbation response)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Generate small perturbations of the text and measure change in log-probability under a model; text that is machine-generated tends to be at local maxima and thus will show probability decreases for perturbations, enabling detection.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Curvature-based score; ROC / AUC; TPR at fixed FPRs</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Curvature computed from differences in log-probability after perturbations; thresholds set on calibration data (target FPRs e.g., 0.1%).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Mentioned in related work; used as baseline via IMGTB in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Included among baseline detectors compared in this paper; generally underperforms Anchor and Binoculars on commercial LLM peer-review samples at low FPRs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Requires surrogate-model scoring and perturbation generation; computationally costly; may be evaded by paraphrasing.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is Your Paper Being Reviewed by an LLM? Benchmarking AI Text Detection in Peer Review', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7795.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7795.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DNAGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Divergent N-gram Analysis (DNAGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A training-free detection method that truncates text, uses an LLM to regenerate the missing portion, and compares the regenerated segment to the original via N-gram divergence (black-box) or probability divergence (white-box) to reveal distributional differences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dna-gpt: Divergent n-gram analysis for training-free detection of gpt-generated text</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science (NLP)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>detection algorithm</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>DNAGPT (divergent n-gram regeneration comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Split text at midpoint, have an LLM regenerate the second half given the first half, and measure N-gram overlap/divergence between regeneration and original; low divergence suggests human origin, high similarity can indicate machine generation depending on white/black-box mode.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>N-gram divergence / probability divergence; ROC / AUC; TPR at fixed FPRs</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>N-gram overlap/divergence (proportion or divergence statistic) or divergence in token probabilities; thresholds calibrated to target FPR on calibration set.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Used as one of the 18 baseline methods (via IMGTB/IMGTB default settings) evaluated on the paper's dataset</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Included among baselines—performed worse than the top methods (Binoculars and Anchor) at stringent FPRs on commercial-model reviews.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Requires access to surrogate LLM for regeneration and can be computationally expensive; black-box vs white-box availability affects performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is Your Paper Being Reviewed by an LLM? Benchmarking AI Text Detection in Peer Review', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7795.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7795.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Loglikelihood / Rank / Entropy / LogRank</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Log-likelihood, Rank, Entropy, and Log-Rank token-probability based metrics</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Token-level statistics derived from language models: average token log-probability (loglikelihood), token rank given context (Rank/LogRank), and token entropy (Entropy), used as model-agnostic signals for machine-generated text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science (NLP, detection)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation metrics</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Log-likelihood / Rank / Entropy / LogRank</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute per-token log-probabilities, ranks, or entropies conditioned on preceding context under a surrogate model; aggregate (mean) across tokens to produce detection scores, then calibrate threshold on calibration data to target FPRs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Mean token log-probability, mean token rank, mean token entropy, mean log-rank; used to compute ROC/AUC and TPR/FPR at thresholds</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Log-likelihood: average log p(token|context) (unit: nats or log-prob); Rank: average absolute rank position per token (integer, averaged); Entropy: average entropy per token (bits/nats); thresholds selected to achieve target FPR.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Applied to AI-Peer-Review-Detection-Benchmark and used as baseline methods via IMGTB</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Many of these metrics provided weak detection at very low FPRs compared to top methods; exact per-method numbers are in Table 2 / appendices.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>These metric-based methods generally underperformed compared to Binoculars and Anchor on commercial LLM reviews at low FPRs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Sensitive to choice of surrogate model, often poor at very low FPRs, and can be evaded by editing/paraphrasing.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is Your Paper Being Reviewed by an LLM? Benchmarking AI Text Detection in Peer Review', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7795.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7795.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MFD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-Feature Detection (MFD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A detector that combines multiple token- and model-derived features—log-likelihood, log-rank, entropy, and LLM deviation—into a composite detection score.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mfd: Multi-feature detection of llm-generated text</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science (NLP)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>detection algorithm</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>MFD (multi-feature detection)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute an ensemble of features (log-likelihood, log-rank, entropy, LLM-deviation) per text and combine them (e.g., by averaging or learned weighting) to produce a detection score; calibrate threshold on calibration set for target FPR.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Composite detection score; ROC / AUC; TPR at target FPRs</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Composite unitless score aggregated from normalized feature values; thresholds determined on calibration data for FPR targets.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Evaluated on AI-Peer-Review-Detection-Benchmark as part of 18 baselines</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Performed variably; generally below Binoculars and Anchor at strict low-FPR requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Relies on multiple surrogate-derived features; performance sensitive to surrogate model and calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is Your Paper Being Reviewed by an LLM? Benchmarking AI Text Detection in Peer Review', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7795.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7795.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IMGTB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>IMGTB: A Framework for Machine-Generated Text Detection Benchmarking</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A standardized framework and toolkit for evaluating machine-generated text detectors, categorizing methods into model-based and metric-based approaches and offering evaluation pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>IMGTB: A Framework for Machine-Generated Text Detection Benchmarking</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science (NLP evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>benchmarking framework</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>IMGTB benchmarking framework</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Provides a standardized evaluation pipeline and baseline implementations (metric-based and model-based) to compare detectors across datasets and settings; used here with default settings to evaluate 18 baseline detectors.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>ROC/AUC, TPR at fixed FPRs, computation time, bootstrap uncertainty estimates</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Standard classification metrics (AUC of ROC; true positive rate at specified false-positive rates); computation time measured per 100 samples on a GPU.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>IMGTB applied to AI-Peer-Review-Detection-Benchmark in this study</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>IMGTB-provided baseline implementations were used to generate comparative results reported in Table 2 and appendices.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>IMGTB baselines (model- and metric-based) generally underperformed the Anchor method in this peer-review domain at low FPRs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Performance of IMGTB baselines depends on choice of surrogate models and default settings; exhaustive surrogate tuning was infeasible in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is Your Paper Being Reviewed by an LLM? Benchmarking AI Text Detection in Peer Review', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7795.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7795.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MGTBench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MGTBench: Benchmarking Machine-Generated Text Detection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A centralized benchmarking suite for evaluating detectors of machine-generated text across diverse models and tasks, used as prior work and contextual baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MGTBench: Benchmarking Machine-Generated Text Detection</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science (NLP evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>MGTBench</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Aggregates datasets and detection methods to provide standardized comparisons of detection performance across LLMs and tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>ROC/AUC, TPR/FPR, other detector metrics as configured</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Standard classification measures; specifics depend on each benchmark split.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Mentioned in related work; not used directly in experiments here</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>General-purpose; not specialized to peer-review context (no manuscript anchors).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is Your Paper Being Reviewed by an LLM? Benchmarking AI Text Detection in Peer Review', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7795.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7795.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAID-TD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RAID-TD: Robust AI-Generated Text Detection benchmark (adversarial conditions)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large-scale benchmark designed to assess detector robustness under adversarial manipulations and perturbations of AI-generated text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Raid: A shared benchmark for robust evaluation of machine-generated text detectors</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science (NLP evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>dataset / benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>RAID-TD</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Provides adversarially conditioned datasets to test detector robustness against manipulations like paraphrasing and perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>ROC/AUC, TPR at specified FPRs under adversarial perturbations</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Standard detection metrics measured on adversarially perturbed samples.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Mentioned in related work; not directly used in this paper's experiments</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Focuses on adversarial robustness; not specialized to peer-review manuscript-context detection.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is Your Paper Being Reviewed by an LLM? Benchmarking AI Text Detection in Peer Review', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7795.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e7795.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HC3 / M4 / GRiD / Beemo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HC3, M4 Dataset, GPT Reddit Dataset (GRiD), and Beemo benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior datasets collecting human and AI-generated text across domains: HC3 (ChatGPT vs experts in specialized domains), M4 (multi-LLM multilingual reviews), GRiD (Reddit conversations), and Beemo (expert-edited machine outputs across tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science (NLP datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>datasets / benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>HC3 / M4 / GRiD / Beemo (dataset resources)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Corpora of paired human and machine-generated texts used in prior work to train/evaluate detectors or study LLM output characteristics across domains and languages.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Varies by dataset; allows computing ROC/AUC, accuracy, cross-domain generalization</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Dataset-specific; typically classification metrics and robustness assessments.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Mentioned in related work as alternative datasets (not used here)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Most prior datasets focus on single models or non-peer-review domains; lack manuscript-linked context used by Anchor.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is Your Paper Being Reviewed by an LLM? Benchmarking AI Text Detection in Peer Review', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7795.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e7795.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI-Peer-Review-Detection-Benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AI Peer Review Detection Benchmark (this paper's dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large parallel dataset of 788,984 peer reviews: human-written reviews paired with AI-generated reviews (five LLMs) for identical papers from ICLR and NeurIPS spanning multiple years, with calibration, test, and extended splits.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science (peer review / NLP)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>dataset / benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>AI-Peer-Review-Detection-Benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Contains balanced human and AI-generated reviews for identical manuscripts, generated using five LLMs with controlled prompts; provides calibration/test splits to evaluate detection methods under in-domain and out-of-domain calibration scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Used to compute ROC/AUC, TPR at fixed FPRs (0.1%, 0.5%, 1%), NDCG for ranking edited reviews, cosine similarity distributions, statistical tests (Wilcoxon) for score differences.</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>TPR/FPR in percent at target FPR thresholds; NDCG (0–1 normalized), cosine similarity (unitless -1 to 1), Wilcoxon p-values.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>This dataset (released on Hugging Face under Intel OBL license).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Qualitative human analysis on 32 matched human vs GPT-4o review pairs; broader dataset uses human-authored reviews from OpenReview/ASAP prior to ChatGPT release to reduce label contamination.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Dataset consists of 788,984 reviews (balanced human vs AI); withheld calibration set contains 75,824 reviews; withheld test set contains 287,052 reviews; Anchor and baseline detectors evaluated on these splits with reported TPR/FPR and AUC values in paper tables.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Dataset enables direct comparison between human-authored and LLM-generated reviews; analyses show AI reviews are less specific, more favorable in scores, and more confident than human reviews.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Dataset limited to ICLR and NeurIPS conferences (CS domain); human reviews post-2023 may contain LLM-assisted text and were excluded from primary experiments; prompts used to generate AI reviews are controlled (though prompt sensitivity analyzed).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is Your Paper Being Reviewed by an LLM? Benchmarking AI Text Detection in Peer Review', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7795.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e7795.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenReview API / ASAP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenReview API and ASAP dataset (source corpora)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Data sources used to collect human peer reviews and manuscripts: OpenReview API for ICLR and NeurIPS reviews and the ASAP dataset for older conference years.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science (data collection for peer review studies)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>data sources</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>OpenReview / ASAP as source datasets</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Collect human-written reviews and manuscript text to pair with generated AI reviews for experimental evaluation and generation prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>OpenReview API (ICLR 2019-2024, NeurIPS 2021-2024) and ASAP (ICLR 2017-2018, NeurIPS 2016-2019)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Temporal coverage varies; human reviews after ChatGPT release may include LLM-assisted content and were excluded from main experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is Your Paper Being Reviewed by an LLM? Benchmarking AI Text Detection in Peer Review', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7795.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e7795.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ROC / AUC / TPR / FPR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Receiver Operating Characteristic (ROC), Area Under Curve (AUC), True Positive Rate (TPR), False Positive Rate (FPR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard binary-classification evaluation metrics used to compare detector performance across decision thresholds and to report detection power at very low false positive rates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>statistics / machine learning</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation metrics</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>ROC curve analysis and derived metrics (AUC, TPR at fixed FPR)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute ROC curve by varying classifier threshold to plot TPR vs FPR; AUC summarizes area under ROC; calibrate thresholds on calibration set to achieve target FPR (0.1%, 0.5%, 1%) and report actual TPR/FPR on test set.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>AUC (0–1), TPR (%) at calibrated FPR targets, actual measured FPR (%)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>AUC is unitless area under ROC; TPR = TP / (TP + FN) expressed as percentage; FPR = FP / (FP + TN) expressed as percentage; target FPRs set very low due to high cost of false positives in peer review.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Applied to AI-Peer-Review-Detection-Benchmark to compare methods and calibrate thresholds.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Reported AUC plots (Figure 2) and TPRs at target FPRs for multiple detectors; Anchor had highest AUC and substantially higher TPR at 0.1% FPR for GPT-4o reviews (63.5%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Used to quantify detector separation between human and AI-generated reviews; many methods show low TPR at stringent FPRs indicating difficulty in per-review detection.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>AUC can be misleading for low-FPR regime; authors therefore report actual TPR at very low FPRs relevant to peer review.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is Your Paper Being Reviewed by an LLM? Benchmarking AI Text Detection in Peer Review', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7795.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e7795.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Calibration via target FPR thresholds</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Calibration of detector thresholds to fixed target FPR (0.1%, 0.5%, 1%)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A procedure that sets classifier thresholds on a calibration set so that the false positive rate equals a specified low target, reflecting the high cost of false positives in peer review.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>machine learning evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation procedure / calibration</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Threshold calibration to target FPR</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Use calibration subset (balanced AI/human reviews) to select a score threshold for each detector so that measured FPR equals desired target (e.g., 0.1%); apply the threshold to held-out test data and report actual TPR/FPR.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Target FPR (percent) and derived TPR at that threshold</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>FPR target is specified as a percentage; threshold chosen so calibration FPR equals that target; TPR measured on test set in percent.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Calibration subset of AI-Peer-Review-Detection-Benchmark (75,824 samples)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Thresholds calibrated to 0.1%, 0.5%, and 1% FPR; paper reports actual TPRs at these thresholds showing large variability across detectors (e.g., Anchor 63.5% vs Binoculars 17.1% at 0.1% FPR for GPT-4o).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Calibration-critical: in-domain calibration (ICLR+NeurIPS) can change relative performance (Binoculars sometimes benefits more from in-domain calibration).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Calibration depends on representativeness of calibration set; out-of-domain calibration can reduce performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is Your Paper Being Reviewed by an LLM? Benchmarking AI Text Detection in Peer Review', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7795.14">
                <h3 class="extraction-instance">Extracted Data Instance 14 (e7795.14)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NDCG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Normalized Discounted Cumulative Gain (NDCG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A ranking evaluation metric used here to assess whether detectors can rank reviews by degree of AI-assisted editing, with higher NDCG indicating better ordering with respect to ground-truth edit intensity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Cumulated gain-based evaluation of ir techniques</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>information retrieval / evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation metric</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>NDCG (Normalized Discounted Cumulative Gain)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Rank reviews by detector score and compute discounted cumulative gain normalized by ideal ordering; used to measure how well models order minimally-to-maximally AI-edited reviews.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>NDCG (0–1)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>NDCG = DCG / IDCG where DCG sums graded relevance scaled by log-ranked position and IDCG is ideal DCG; values range 0 (worst) to 1 (perfect ranking).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Applied to AI-edited-humans subset of AI-Peer-Review-Detection-Benchmark (ICLR 2021 GPT-4o test subset with four editing levels)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Anchor achieved NDCG = 0.90 and Binoculars NDCG = 0.86 when ranking reviews by degree of AI editing.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Both Anchor and Binoculars can rank edited reviews well (high NDCG) but differ in flagging behavior at strict FPRs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>High NDCG does not imply high binary detection at strict FPRs; ranking and thresholded flagging behave differently.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is Your Paper Being Reviewed by an LLM? Benchmarking AI Text Detection in Peer Review', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7795.15">
                <h3 class="extraction-instance">Extracted Data Instance 15 (e7795.15)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Cosine similarity / Embedding-based analysis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cosine similarity of text embeddings (embedding-based semantic similarity)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of sentence/text embeddings (OpenAI text-embedding-003-small / text-embedding-3-small) and cosine similarity to measure semantic closeness between reviews and anchor reviews or between original and edited reviews.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI text-embedding-003-small (main experiments), text-embedding-3-small (supplementary visualizations)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science (NLP embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>similarity metric / representation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Cosine similarity of embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute dense embeddings for texts and measure cosine similarity; used both for Anchor detection (AR vs TR similarity) and for validating levels of AI editing by comparing edited vs original review similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Cosine similarity (unitless, -1 to 1); used as detection score or similarity validation</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Cosine_similarity = (EmbA · EmbB) / (||EmbA|| ||EmbB||); thresholds learned via calibration for detection; similarity decreases as editing intensity increases (reported in Table S12).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Applied to AI-Peer-Review-Detection-Benchmark for Anchor method and AI-edit validation</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Similarity scores for edited vs original reviews decreased with editing intensity (Minimum 0.9841, Moderate 0.9261, Extensive 0.8616, Maximum 0.6799 per Table S12); Anchor detection relies on cosine similarity to threshold.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Embedding-based similarity distinguishes highly edited texts from originals and supports Anchor's effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Embedding choice affects similarity scales; semantic similarity may not capture subtle stylistic or authorship signals and could conflate topic similarity with authorship.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is Your Paper Being Reviewed by an LLM? Benchmarking AI Text Detection in Peer Review', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7795.16">
                <h3 class="extraction-instance">Extracted Data Instance 16 (e7795.16)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bootstrap Resampling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bootstrap resampling for uncertainty estimation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A nonparametric resampling method used to estimate uncertainty (standard deviations) of reported TPR and FPR by resampling test data with replacement N=100 times.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>statistics</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>uncertainty estimation method</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Bootstrap resampling (N=100)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Repeatedly resample the test set with replacement and recompute TPR/FPR for each detector to estimate standard deviation across replicates.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Standard deviation of TPR and FPR across bootstrap replicates</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>SD computed over N=100 bootstrap samples; used to show small relative variability in reported metrics (Table S5).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Applied to withheld test dataset (AI-Peer-Review-Detection-Benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Bootstrap SDs were reported and found small relative to performance gaps between methods (supporting robustness of main findings).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>100 resamples provide coarse estimate of variability; chosen to balance compute cost with uncertainty quantification.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is Your Paper Being Reviewed by an LLM? Benchmarking AI Text Detection in Peer Review', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7795.17">
                <h3 class="extraction-instance">Extracted Data Instance 17 (e7795.17)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Wilcoxon signed-rank test</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Two-sided Wilcoxon signed-rank test</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A nonparametric paired statistical test used to assess whether AI-generated review scores differ significantly from matched human-review scores.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>statistics</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>statistical significance test</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Wilcoxon signed-rank test (two-sided)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute paired differences between matched AI and human numeric scores, apply the Wilcoxon signed-rank test to test the null hypothesis of zero median difference; report p-values.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>p-value from Wilcoxon test</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Two-sided p-value assessing significance of median difference in paired scores (e.g., soundness, presentation, contribution, confidence); significance thresholds conventional (e.g., p<0.05).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Applied to matched AI vs human review scores (NeurIPS 2022 etc.) in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>AI-generated reviews assign higher scores than human reviews with high statistical significance (p-values reported in Figure S3 legend).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Consistently higher numeric scores from LLMs across categories (soundness, presentation, contribution, confidence) relative to human reviewers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Statistical significance does not indicate practical significance; scoring distributions and potential labeling noise (post-ChatGPT reviews) considered.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is Your Paper Being Reviewed by an LLM? Benchmarking AI Text Detection in Peer Review', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7795.18">
                <h3 class="extraction-instance">Extracted Data Instance 18 (e7795.18)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Voting ensemble (Anchor)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Anchor voting mechanism across multiple anchor LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pragmatic ensemble scheme for Anchor: generate anchor reviews using multiple anchor LLMs (GPT-4o, Gemini, Claude), compute similarity-based labels per anchor, and label a review as AI-generated if any anchor yields a positive detection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o, Gemini 1.5 pro, Claude Sonnet 3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science (NLP ensemble methods)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>ensemble decision rule</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Anchor voting (OR rule across anchor LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Generate multiple anchor embeddings from different LLMs, compute cosine similarity scores with the test review for each, threshold each score; final label positive if any anchor's threshold is exceeded.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>OR-voting detection label; downstream ROC/AUC and TPR/FPR measured as usual</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Binary decision combining per-anchor binary labels via logical OR; thresholds for each anchor set using calibration data.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Applied on AI-Peer-Review-Detection-Benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Voting across three anchor LLMs was used in experiments; explicit per-vote ablation details in Appendix D.1.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Voting helps in black-box source-LMM scenarios by capturing cases where at least one anchor model aligns with the source LLM's style.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>OR voting increases sensitivity but may increase false positives if anchors are poorly calibrated; computational and API cost increases with number of anchors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is Your Paper Being Reviewed by an LLM? Benchmarking AI Text Detection in Peer Review', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7795.19">
                <h3 class="extraction-instance">Extracted Data Instance 19 (e7795.19)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>t-SNE visualization (embedding analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>t-distributed Stochastic Neighbor Embedding (t-SNE) of sentence embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dimensionality-reduction visualization used to inspect embedding-level style differences between reviews generated with different prompts (score-aligned vs archetype prompts).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI text-embedding-3-small (used for embeddings in visualization)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>machine learning (visualization / representation analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>exploratory analysis / visualization</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>t-SNE projection of embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute sentence embeddings for reviews, apply t-SNE (perplexity=30, 2D) and visualize overlap between sets to assess prompt-induced stylistic separation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Visual overlap / cluster separation; qualitative assessment</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>t-SNE 2D coordinates (unitless) used for inspection; no numeric separation metric reported beyond visual overlap.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Applied to ICLR2021 test set's generated reviews under different prompting strategies</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>No meaningful separation observed between reviews generated with the primary score-aligned prompt and the archetype-based prompt, suggesting moderate prompt variation does not cause major stylistic shifts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>t-SNE is qualitative and sensitive to hyperparameters; absence of visible separation does not guarantee indistinguishability in all feature spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is Your Paper Being Reviewed by an LLM? Benchmarking AI Text Detection in Peer Review', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7795.20">
                <h3 class="extraction-instance">Extracted Data Instance 20 (e7795.20)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Computation-time benchmarking</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Computation time per-method benchmarking</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical measurement of time-to-process a batch of samples (100) per detection method on a single NVIDIA RTX A6000 GPU (Anchor excluded as it used sequential API calls).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>machine learning systems / benchmarking</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>operational metric</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Computation-time benchmarking (per 100 samples)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Run each detection method 20 times on 100-sample batches, report mean and standard deviation of elapsed time; Anchor measured separately using sequential API calls.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Time in seconds per 100 samples (mean ± std)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Elapsed wall-clock seconds averaged over 20 repeats on specified hardware (NVIDIA RTX A6000 GPU), or measured as sequential API latency for Anchor.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Measured on 100-sample subsets from AI-Peer-Review-Detection-Benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Figure S1 summarizes compute times; Anchor was not GPU-accelerated and thus slower per-sample due to sequential API calls, while other methods vary depending on model complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Compute times depend on implementation details and hardware; Anchor's sequential API calls could be optimized via parallelization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is Your Paper Being Reviewed by an LLM? Benchmarking AI Text Detection in Peer Review', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature <em>(Rating: 2)</em></li>
                <li>Spotting llms with binoculars: Zero-shot detection of machine-generated text <em>(Rating: 2)</em></li>
                <li>Dna-gpt: Divergent n-gram analysis for training-free detection of gpt-generated text <em>(Rating: 2)</em></li>
                <li>IMGTB: A Framework for Machine-Generated Text Detection Benchmarking <em>(Rating: 2)</em></li>
                <li>MGTBench: Benchmarking Machine-Generated Text Detection <em>(Rating: 2)</em></li>
                <li>Raid: A shared benchmark for robust evaluation of machine-generated text detectors <em>(Rating: 2)</em></li>
                <li>Beemo: Benchmark of expert-edited machine-generated outputs <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7795",
    "paper_id": "paper-276647742",
    "extraction_schema_id": "extraction-schema-145",
    "extracted_data": [
        {
            "name_short": "Anchor",
            "name_full": "Anchor embedding context-aware detection method",
            "brief_description": "A context-aware detection method that generates a synthetic 'anchor review' for the manuscript using an LLM, embeds both the anchor and the test review, and classifies a review as AI-generated if their semantic (cosine) similarity exceeds a learned threshold; supports voting over multiple anchor LLMs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o, Gemini 1.5 pro, Claude Sonnet 3.5 (used as anchor LLMs in voting experiments)",
            "model_size": null,
            "scientific_domain": "computer science (peer review / NLP)",
            "theory_type": "evaluation method / detection framework",
            "evaluation_method_name": "Anchor (anchor-embedding semantic similarity)",
            "evaluation_method_description": "Generate an anchor review for the same manuscript with an LLM, compute embeddings (text-embedding-003-small), measure cosine similarity between anchor and test review embeddings, compare to threshold θ learned on a calibration set; optionally generate multiple anchors and apply OR voting.",
            "evaluation_metric": "Cosine similarity compared to learned threshold; downstream detection metrics: TPR at fixed FPR, AUC, ROC",
            "metric_definition": "Similarity = cosine(EmbAR, EmbTR) (unitless, range -1 to 1); thresholds learned to achieve target False Positive Rate (e.g., 0.1%, 0.5%, 1%); reporting true positive rate (TPR) at those FPRs and AUC (area under ROC).",
            "dataset_or_benchmark": "AI-Peer-Review-Detection-Benchmark (this paper's calibration/test/extended splits)",
            "human_evaluation_details": "Human qualitative analysis performed elsewhere in paper (32 review pairs analyzed) but Anchor itself evaluated automatically against held-out test set; no manual adjudication for Anchor labels beyond using human-written reviews as negative class.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Anchor achieved the highest AUC among evaluated methods and a TPR of 63.5% for GPT-4o reviews at a target FPR of 0.1% (compared to 17.1% for the next-best baseline, Binoculars); Anchor also showed strong TPRs at 0.5% and 1% FPR.",
            "comparison_to_human_generated": true,
            "comparison_results": "Anchor outperformed 18 baseline detectors on commercial-model reviews (GPT-4o, Gemini, Claude) in AUC and in TPR at very low FPRs; particularly large gains on GPT-4o samples.",
            "limitations_noted": "Task-specific (requires manuscript context), relies on anchor LLM choice (sensitivity), depends on commercial LLM APIs (cost & scalability), and not a general-purpose detector; performance can vary if anchor LLMs differ from source LLM.",
            "uuid": "e7795.0",
            "source_info": {
                "paper_title": "Is Your Paper Being Reviewed by an LLM? Benchmarking AI Text Detection in Peer Review",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Binoculars",
            "name_full": "Binoculars zero-shot detection method",
            "brief_description": "A zero-shot detection method that uses two perspectives: observer-model perplexity on text and cross-perplexity where a performer LLM's next-token predictions are scored by the observer; final score is perplexity-to-cross-perplexity ratio.",
            "citation_title": "Spotting llms with binoculars: Zero-shot detection of machine-generated text",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "computer science (NLP, text detection)",
            "theory_type": "detection algorithm",
            "evaluation_method_name": "Binoculars (perplexity and cross-perplexity ratio)",
            "evaluation_method_description": "Compute observer-model perplexity on the text and cross-perplexity by scoring performer-model next-token predictions with the observer model; use the ratio between perplexity and cross-perplexity as a detection score (higher indicates likely machine-generated).",
            "evaluation_metric": "Detection score (perplexity ratio); ROC / AUC; TPR at fixed FPRs (0.1%, 0.5%, 1%)",
            "metric_definition": "Perplexity and cross-perplexity are exponentiated negative average log-probabilities (unitless); detection threshold calibrated on calibration set to achieve target FPR; TPR reported as % of AI reviews correctly flagged.",
            "dataset_or_benchmark": "AI-Peer-Review-Detection-Benchmark (used for evaluation in this paper); Binoculars' original evaluation in its own paper",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Among the 18 baselines evaluated, Binoculars was the strongest baseline; e.g., for GPT-4o reviews it reached 17.1% TPR at 0.1% FPR and 45.2% TPR at 1% FPR (on the withheld test set calibrated on ICLR reviews).",
            "comparison_to_human_generated": true,
            "comparison_results": "Binoculars outperformed most metric- and model-based baselines but was substantially outperformed by the Anchor method on GPT-4o samples at low FPRs.",
            "limitations_noted": "Performance depends on choice of observer/performer surrogate models and can be sensitive to in-domain calibration; may require access to surrogate LLMs.",
            "uuid": "e7795.1",
            "source_info": {
                "paper_title": "Is Your Paper Being Reviewed by an LLM? Benchmarking AI Text Detection in Peer Review",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "DetectGPT",
            "name_full": "DetectGPT (probability curvature zero-shot detector)",
            "brief_description": "A zero-shot machine-generated text detector that uses probability curvature: it measures how the log-probability of text responds to local perturbations, hypothesizing that generated text lies near local probability maxima.",
            "citation_title": "DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "computer science (NLP)",
            "theory_type": "detection algorithm",
            "evaluation_method_name": "DetectGPT (probability curvature / perturbation response)",
            "evaluation_method_description": "Generate small perturbations of the text and measure change in log-probability under a model; text that is machine-generated tends to be at local maxima and thus will show probability decreases for perturbations, enabling detection.",
            "evaluation_metric": "Curvature-based score; ROC / AUC; TPR at fixed FPRs",
            "metric_definition": "Curvature computed from differences in log-probability after perturbations; thresholds set on calibration data (target FPRs e.g., 0.1%).",
            "dataset_or_benchmark": "Mentioned in related work; used as baseline via IMGTB in experiments",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": null,
            "comparison_to_human_generated": true,
            "comparison_results": "Included among baseline detectors compared in this paper; generally underperforms Anchor and Binoculars on commercial LLM peer-review samples at low FPRs.",
            "limitations_noted": "Requires surrogate-model scoring and perturbation generation; computationally costly; may be evaded by paraphrasing.",
            "uuid": "e7795.2",
            "source_info": {
                "paper_title": "Is Your Paper Being Reviewed by an LLM? Benchmarking AI Text Detection in Peer Review",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "DNAGPT",
            "name_full": "Divergent N-gram Analysis (DNAGPT)",
            "brief_description": "A training-free detection method that truncates text, uses an LLM to regenerate the missing portion, and compares the regenerated segment to the original via N-gram divergence (black-box) or probability divergence (white-box) to reveal distributional differences.",
            "citation_title": "Dna-gpt: Divergent n-gram analysis for training-free detection of gpt-generated text",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "computer science (NLP)",
            "theory_type": "detection algorithm",
            "evaluation_method_name": "DNAGPT (divergent n-gram regeneration comparison)",
            "evaluation_method_description": "Split text at midpoint, have an LLM regenerate the second half given the first half, and measure N-gram overlap/divergence between regeneration and original; low divergence suggests human origin, high similarity can indicate machine generation depending on white/black-box mode.",
            "evaluation_metric": "N-gram divergence / probability divergence; ROC / AUC; TPR at fixed FPRs",
            "metric_definition": "N-gram overlap/divergence (proportion or divergence statistic) or divergence in token probabilities; thresholds calibrated to target FPR on calibration set.",
            "dataset_or_benchmark": "Used as one of the 18 baseline methods (via IMGTB/IMGTB default settings) evaluated on the paper's dataset",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": null,
            "comparison_to_human_generated": true,
            "comparison_results": "Included among baselines—performed worse than the top methods (Binoculars and Anchor) at stringent FPRs on commercial-model reviews.",
            "limitations_noted": "Requires access to surrogate LLM for regeneration and can be computationally expensive; black-box vs white-box availability affects performance.",
            "uuid": "e7795.3",
            "source_info": {
                "paper_title": "Is Your Paper Being Reviewed by an LLM? Benchmarking AI Text Detection in Peer Review",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Loglikelihood / Rank / Entropy / LogRank",
            "name_full": "Log-likelihood, Rank, Entropy, and Log-Rank token-probability based metrics",
            "brief_description": "Token-level statistics derived from language models: average token log-probability (loglikelihood), token rank given context (Rank/LogRank), and token entropy (Entropy), used as model-agnostic signals for machine-generated text.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "computer science (NLP, detection)",
            "theory_type": "evaluation metrics",
            "evaluation_method_name": "Log-likelihood / Rank / Entropy / LogRank",
            "evaluation_method_description": "Compute per-token log-probabilities, ranks, or entropies conditioned on preceding context under a surrogate model; aggregate (mean) across tokens to produce detection scores, then calibrate threshold on calibration data to target FPRs.",
            "evaluation_metric": "Mean token log-probability, mean token rank, mean token entropy, mean log-rank; used to compute ROC/AUC and TPR/FPR at thresholds",
            "metric_definition": "Log-likelihood: average log p(token|context) (unit: nats or log-prob); Rank: average absolute rank position per token (integer, averaged); Entropy: average entropy per token (bits/nats); thresholds selected to achieve target FPR.",
            "dataset_or_benchmark": "Applied to AI-Peer-Review-Detection-Benchmark and used as baseline methods via IMGTB",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Many of these metrics provided weak detection at very low FPRs compared to top methods; exact per-method numbers are in Table 2 / appendices.",
            "comparison_to_human_generated": true,
            "comparison_results": "These metric-based methods generally underperformed compared to Binoculars and Anchor on commercial LLM reviews at low FPRs.",
            "limitations_noted": "Sensitive to choice of surrogate model, often poor at very low FPRs, and can be evaded by editing/paraphrasing.",
            "uuid": "e7795.4",
            "source_info": {
                "paper_title": "Is Your Paper Being Reviewed by an LLM? Benchmarking AI Text Detection in Peer Review",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "MFD",
            "name_full": "Multi-Feature Detection (MFD)",
            "brief_description": "A detector that combines multiple token- and model-derived features—log-likelihood, log-rank, entropy, and LLM deviation—into a composite detection score.",
            "citation_title": "Mfd: Multi-feature detection of llm-generated text",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "computer science (NLP)",
            "theory_type": "detection algorithm",
            "evaluation_method_name": "MFD (multi-feature detection)",
            "evaluation_method_description": "Compute an ensemble of features (log-likelihood, log-rank, entropy, LLM-deviation) per text and combine them (e.g., by averaging or learned weighting) to produce a detection score; calibrate threshold on calibration set for target FPR.",
            "evaluation_metric": "Composite detection score; ROC / AUC; TPR at target FPRs",
            "metric_definition": "Composite unitless score aggregated from normalized feature values; thresholds determined on calibration data for FPR targets.",
            "dataset_or_benchmark": "Evaluated on AI-Peer-Review-Detection-Benchmark as part of 18 baselines",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": null,
            "comparison_to_human_generated": true,
            "comparison_results": "Performed variably; generally below Binoculars and Anchor at strict low-FPR requirements.",
            "limitations_noted": "Relies on multiple surrogate-derived features; performance sensitive to surrogate model and calibration.",
            "uuid": "e7795.5",
            "source_info": {
                "paper_title": "Is Your Paper Being Reviewed by an LLM? Benchmarking AI Text Detection in Peer Review",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "IMGTB",
            "name_full": "IMGTB: A Framework for Machine-Generated Text Detection Benchmarking",
            "brief_description": "A standardized framework and toolkit for evaluating machine-generated text detectors, categorizing methods into model-based and metric-based approaches and offering evaluation pipelines.",
            "citation_title": "IMGTB: A Framework for Machine-Generated Text Detection Benchmarking",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "computer science (NLP evaluation)",
            "theory_type": "benchmarking framework",
            "evaluation_method_name": "IMGTB benchmarking framework",
            "evaluation_method_description": "Provides a standardized evaluation pipeline and baseline implementations (metric-based and model-based) to compare detectors across datasets and settings; used here with default settings to evaluate 18 baseline detectors.",
            "evaluation_metric": "ROC/AUC, TPR at fixed FPRs, computation time, bootstrap uncertainty estimates",
            "metric_definition": "Standard classification metrics (AUC of ROC; true positive rate at specified false-positive rates); computation time measured per 100 samples on a GPU.",
            "dataset_or_benchmark": "IMGTB applied to AI-Peer-Review-Detection-Benchmark in this study",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "IMGTB-provided baseline implementations were used to generate comparative results reported in Table 2 and appendices.",
            "comparison_to_human_generated": true,
            "comparison_results": "IMGTB baselines (model- and metric-based) generally underperformed the Anchor method in this peer-review domain at low FPRs.",
            "limitations_noted": "Performance of IMGTB baselines depends on choice of surrogate models and default settings; exhaustive surrogate tuning was infeasible in this study.",
            "uuid": "e7795.6",
            "source_info": {
                "paper_title": "Is Your Paper Being Reviewed by an LLM? Benchmarking AI Text Detection in Peer Review",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "MGTBench",
            "name_full": "MGTBench: Benchmarking Machine-Generated Text Detection",
            "brief_description": "A centralized benchmarking suite for evaluating detectors of machine-generated text across diverse models and tasks, used as prior work and contextual baseline.",
            "citation_title": "MGTBench: Benchmarking Machine-Generated Text Detection",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "computer science (NLP evaluation)",
            "theory_type": "benchmark",
            "evaluation_method_name": "MGTBench",
            "evaluation_method_description": "Aggregates datasets and detection methods to provide standardized comparisons of detection performance across LLMs and tasks.",
            "evaluation_metric": "ROC/AUC, TPR/FPR, other detector metrics as configured",
            "metric_definition": "Standard classification measures; specifics depend on each benchmark split.",
            "dataset_or_benchmark": "Mentioned in related work; not used directly in experiments here",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": null,
            "comparison_to_human_generated": null,
            "comparison_results": null,
            "limitations_noted": "General-purpose; not specialized to peer-review context (no manuscript anchors).",
            "uuid": "e7795.7",
            "source_info": {
                "paper_title": "Is Your Paper Being Reviewed by an LLM? Benchmarking AI Text Detection in Peer Review",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "RAID-TD",
            "name_full": "RAID-TD: Robust AI-Generated Text Detection benchmark (adversarial conditions)",
            "brief_description": "A large-scale benchmark designed to assess detector robustness under adversarial manipulations and perturbations of AI-generated text.",
            "citation_title": "Raid: A shared benchmark for robust evaluation of machine-generated text detectors",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "computer science (NLP evaluation)",
            "theory_type": "dataset / benchmark",
            "evaluation_method_name": "RAID-TD",
            "evaluation_method_description": "Provides adversarially conditioned datasets to test detector robustness against manipulations like paraphrasing and perturbations.",
            "evaluation_metric": "ROC/AUC, TPR at specified FPRs under adversarial perturbations",
            "metric_definition": "Standard detection metrics measured on adversarially perturbed samples.",
            "dataset_or_benchmark": "Mentioned in related work; not directly used in this paper's experiments",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": null,
            "comparison_to_human_generated": null,
            "comparison_results": null,
            "limitations_noted": "Focuses on adversarial robustness; not specialized to peer-review manuscript-context detection.",
            "uuid": "e7795.8",
            "source_info": {
                "paper_title": "Is Your Paper Being Reviewed by an LLM? Benchmarking AI Text Detection in Peer Review",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "HC3 / M4 / GRiD / Beemo",
            "name_full": "HC3, M4 Dataset, GPT Reddit Dataset (GRiD), and Beemo benchmarks",
            "brief_description": "Prior datasets collecting human and AI-generated text across domains: HC3 (ChatGPT vs experts in specialized domains), M4 (multi-LLM multilingual reviews), GRiD (Reddit conversations), and Beemo (expert-edited machine outputs across tasks).",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "computer science (NLP datasets)",
            "theory_type": "datasets / benchmarks",
            "evaluation_method_name": "HC3 / M4 / GRiD / Beemo (dataset resources)",
            "evaluation_method_description": "Corpora of paired human and machine-generated texts used in prior work to train/evaluate detectors or study LLM output characteristics across domains and languages.",
            "evaluation_metric": "Varies by dataset; allows computing ROC/AUC, accuracy, cross-domain generalization",
            "metric_definition": "Dataset-specific; typically classification metrics and robustness assessments.",
            "dataset_or_benchmark": "Mentioned in related work as alternative datasets (not used here)",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": null,
            "comparison_to_human_generated": null,
            "comparison_results": null,
            "limitations_noted": "Most prior datasets focus on single models or non-peer-review domains; lack manuscript-linked context used by Anchor.",
            "uuid": "e7795.9",
            "source_info": {
                "paper_title": "Is Your Paper Being Reviewed by an LLM? Benchmarking AI Text Detection in Peer Review",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "AI-Peer-Review-Detection-Benchmark",
            "name_full": "AI Peer Review Detection Benchmark (this paper's dataset)",
            "brief_description": "A large parallel dataset of 788,984 peer reviews: human-written reviews paired with AI-generated reviews (five LLMs) for identical papers from ICLR and NeurIPS spanning multiple years, with calibration, test, and extended splits.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "computer science (peer review / NLP)",
            "theory_type": "dataset / benchmark",
            "evaluation_method_name": "AI-Peer-Review-Detection-Benchmark",
            "evaluation_method_description": "Contains balanced human and AI-generated reviews for identical manuscripts, generated using five LLMs with controlled prompts; provides calibration/test splits to evaluate detection methods under in-domain and out-of-domain calibration scenarios.",
            "evaluation_metric": "Used to compute ROC/AUC, TPR at fixed FPRs (0.1%, 0.5%, 1%), NDCG for ranking edited reviews, cosine similarity distributions, statistical tests (Wilcoxon) for score differences.",
            "metric_definition": "TPR/FPR in percent at target FPR thresholds; NDCG (0–1 normalized), cosine similarity (unitless -1 to 1), Wilcoxon p-values.",
            "dataset_or_benchmark": "This dataset (released on Hugging Face under Intel OBL license).",
            "human_evaluation_details": "Qualitative human analysis on 32 matched human vs GPT-4o review pairs; broader dataset uses human-authored reviews from OpenReview/ASAP prior to ChatGPT release to reduce label contamination.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Dataset consists of 788,984 reviews (balanced human vs AI); withheld calibration set contains 75,824 reviews; withheld test set contains 287,052 reviews; Anchor and baseline detectors evaluated on these splits with reported TPR/FPR and AUC values in paper tables.",
            "comparison_to_human_generated": true,
            "comparison_results": "Dataset enables direct comparison between human-authored and LLM-generated reviews; analyses show AI reviews are less specific, more favorable in scores, and more confident than human reviews.",
            "limitations_noted": "Dataset limited to ICLR and NeurIPS conferences (CS domain); human reviews post-2023 may contain LLM-assisted text and were excluded from primary experiments; prompts used to generate AI reviews are controlled (though prompt sensitivity analyzed).",
            "uuid": "e7795.10",
            "source_info": {
                "paper_title": "Is Your Paper Being Reviewed by an LLM? Benchmarking AI Text Detection in Peer Review",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "OpenReview API / ASAP",
            "name_full": "OpenReview API and ASAP dataset (source corpora)",
            "brief_description": "Data sources used to collect human peer reviews and manuscripts: OpenReview API for ICLR and NeurIPS reviews and the ASAP dataset for older conference years.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "computer science (data collection for peer review studies)",
            "theory_type": "data sources",
            "evaluation_method_name": "OpenReview / ASAP as source datasets",
            "evaluation_method_description": "Collect human-written reviews and manuscript text to pair with generated AI reviews for experimental evaluation and generation prompts.",
            "evaluation_metric": null,
            "metric_definition": null,
            "dataset_or_benchmark": "OpenReview API (ICLR 2019-2024, NeurIPS 2021-2024) and ASAP (ICLR 2017-2018, NeurIPS 2016-2019)",
            "human_evaluation_details": null,
            "automated_falsifiability_check": null,
            "reproducibility_assessment": true,
            "reported_results": null,
            "comparison_to_human_generated": null,
            "comparison_results": null,
            "limitations_noted": "Temporal coverage varies; human reviews after ChatGPT release may include LLM-assisted content and were excluded from main experiments.",
            "uuid": "e7795.11",
            "source_info": {
                "paper_title": "Is Your Paper Being Reviewed by an LLM? Benchmarking AI Text Detection in Peer Review",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "ROC / AUC / TPR / FPR",
            "name_full": "Receiver Operating Characteristic (ROC), Area Under Curve (AUC), True Positive Rate (TPR), False Positive Rate (FPR)",
            "brief_description": "Standard binary-classification evaluation metrics used to compare detector performance across decision thresholds and to report detection power at very low false positive rates.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "statistics / machine learning",
            "theory_type": "evaluation metrics",
            "evaluation_method_name": "ROC curve analysis and derived metrics (AUC, TPR at fixed FPR)",
            "evaluation_method_description": "Compute ROC curve by varying classifier threshold to plot TPR vs FPR; AUC summarizes area under ROC; calibrate thresholds on calibration set to achieve target FPR (0.1%, 0.5%, 1%) and report actual TPR/FPR on test set.",
            "evaluation_metric": "AUC (0–1), TPR (%) at calibrated FPR targets, actual measured FPR (%)",
            "metric_definition": "AUC is unitless area under ROC; TPR = TP / (TP + FN) expressed as percentage; FPR = FP / (FP + TN) expressed as percentage; target FPRs set very low due to high cost of false positives in peer review.",
            "dataset_or_benchmark": "Applied to AI-Peer-Review-Detection-Benchmark to compare methods and calibrate thresholds.",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Reported AUC plots (Figure 2) and TPRs at target FPRs for multiple detectors; Anchor had highest AUC and substantially higher TPR at 0.1% FPR for GPT-4o reviews (63.5%).",
            "comparison_to_human_generated": true,
            "comparison_results": "Used to quantify detector separation between human and AI-generated reviews; many methods show low TPR at stringent FPRs indicating difficulty in per-review detection.",
            "limitations_noted": "AUC can be misleading for low-FPR regime; authors therefore report actual TPR at very low FPRs relevant to peer review.",
            "uuid": "e7795.12",
            "source_info": {
                "paper_title": "Is Your Paper Being Reviewed by an LLM? Benchmarking AI Text Detection in Peer Review",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Calibration via target FPR thresholds",
            "name_full": "Calibration of detector thresholds to fixed target FPR (0.1%, 0.5%, 1%)",
            "brief_description": "A procedure that sets classifier thresholds on a calibration set so that the false positive rate equals a specified low target, reflecting the high cost of false positives in peer review.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "machine learning evaluation",
            "theory_type": "evaluation procedure / calibration",
            "evaluation_method_name": "Threshold calibration to target FPR",
            "evaluation_method_description": "Use calibration subset (balanced AI/human reviews) to select a score threshold for each detector so that measured FPR equals desired target (e.g., 0.1%); apply the threshold to held-out test data and report actual TPR/FPR.",
            "evaluation_metric": "Target FPR (percent) and derived TPR at that threshold",
            "metric_definition": "FPR target is specified as a percentage; threshold chosen so calibration FPR equals that target; TPR measured on test set in percent.",
            "dataset_or_benchmark": "Calibration subset of AI-Peer-Review-Detection-Benchmark (75,824 samples)",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Thresholds calibrated to 0.1%, 0.5%, and 1% FPR; paper reports actual TPRs at these thresholds showing large variability across detectors (e.g., Anchor 63.5% vs Binoculars 17.1% at 0.1% FPR for GPT-4o).",
            "comparison_to_human_generated": true,
            "comparison_results": "Calibration-critical: in-domain calibration (ICLR+NeurIPS) can change relative performance (Binoculars sometimes benefits more from in-domain calibration).",
            "limitations_noted": "Calibration depends on representativeness of calibration set; out-of-domain calibration can reduce performance.",
            "uuid": "e7795.13",
            "source_info": {
                "paper_title": "Is Your Paper Being Reviewed by an LLM? Benchmarking AI Text Detection in Peer Review",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "NDCG",
            "name_full": "Normalized Discounted Cumulative Gain (NDCG)",
            "brief_description": "A ranking evaluation metric used here to assess whether detectors can rank reviews by degree of AI-assisted editing, with higher NDCG indicating better ordering with respect to ground-truth edit intensity.",
            "citation_title": "Cumulated gain-based evaluation of ir techniques",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "information retrieval / evaluation",
            "theory_type": "evaluation metric",
            "evaluation_method_name": "NDCG (Normalized Discounted Cumulative Gain)",
            "evaluation_method_description": "Rank reviews by detector score and compute discounted cumulative gain normalized by ideal ordering; used to measure how well models order minimally-to-maximally AI-edited reviews.",
            "evaluation_metric": "NDCG (0–1)",
            "metric_definition": "NDCG = DCG / IDCG where DCG sums graded relevance scaled by log-ranked position and IDCG is ideal DCG; values range 0 (worst) to 1 (perfect ranking).",
            "dataset_or_benchmark": "Applied to AI-edited-humans subset of AI-Peer-Review-Detection-Benchmark (ICLR 2021 GPT-4o test subset with four editing levels)",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Anchor achieved NDCG = 0.90 and Binoculars NDCG = 0.86 when ranking reviews by degree of AI editing.",
            "comparison_to_human_generated": true,
            "comparison_results": "Both Anchor and Binoculars can rank edited reviews well (high NDCG) but differ in flagging behavior at strict FPRs.",
            "limitations_noted": "High NDCG does not imply high binary detection at strict FPRs; ranking and thresholded flagging behave differently.",
            "uuid": "e7795.14",
            "source_info": {
                "paper_title": "Is Your Paper Being Reviewed by an LLM? Benchmarking AI Text Detection in Peer Review",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Cosine similarity / Embedding-based analysis",
            "name_full": "Cosine similarity of text embeddings (embedding-based semantic similarity)",
            "brief_description": "Use of sentence/text embeddings (OpenAI text-embedding-003-small / text-embedding-3-small) and cosine similarity to measure semantic closeness between reviews and anchor reviews or between original and edited reviews.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "OpenAI text-embedding-003-small (main experiments), text-embedding-3-small (supplementary visualizations)",
            "model_size": null,
            "scientific_domain": "computer science (NLP embeddings)",
            "theory_type": "similarity metric / representation",
            "evaluation_method_name": "Cosine similarity of embeddings",
            "evaluation_method_description": "Compute dense embeddings for texts and measure cosine similarity; used both for Anchor detection (AR vs TR similarity) and for validating levels of AI editing by comparing edited vs original review similarity.",
            "evaluation_metric": "Cosine similarity (unitless, -1 to 1); used as detection score or similarity validation",
            "metric_definition": "Cosine_similarity = (EmbA · EmbB) / (||EmbA|| ||EmbB||); thresholds learned via calibration for detection; similarity decreases as editing intensity increases (reported in Table S12).",
            "dataset_or_benchmark": "Applied to AI-Peer-Review-Detection-Benchmark for Anchor method and AI-edit validation",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Similarity scores for edited vs original reviews decreased with editing intensity (Minimum 0.9841, Moderate 0.9261, Extensive 0.8616, Maximum 0.6799 per Table S12); Anchor detection relies on cosine similarity to threshold.",
            "comparison_to_human_generated": true,
            "comparison_results": "Embedding-based similarity distinguishes highly edited texts from originals and supports Anchor's effectiveness.",
            "limitations_noted": "Embedding choice affects similarity scales; semantic similarity may not capture subtle stylistic or authorship signals and could conflate topic similarity with authorship.",
            "uuid": "e7795.15",
            "source_info": {
                "paper_title": "Is Your Paper Being Reviewed by an LLM? Benchmarking AI Text Detection in Peer Review",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Bootstrap Resampling",
            "name_full": "Bootstrap resampling for uncertainty estimation",
            "brief_description": "A nonparametric resampling method used to estimate uncertainty (standard deviations) of reported TPR and FPR by resampling test data with replacement N=100 times.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "statistics",
            "theory_type": "uncertainty estimation method",
            "evaluation_method_name": "Bootstrap resampling (N=100)",
            "evaluation_method_description": "Repeatedly resample the test set with replacement and recompute TPR/FPR for each detector to estimate standard deviation across replicates.",
            "evaluation_metric": "Standard deviation of TPR and FPR across bootstrap replicates",
            "metric_definition": "SD computed over N=100 bootstrap samples; used to show small relative variability in reported metrics (Table S5).",
            "dataset_or_benchmark": "Applied to withheld test dataset (AI-Peer-Review-Detection-Benchmark)",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Bootstrap SDs were reported and found small relative to performance gaps between methods (supporting robustness of main findings).",
            "comparison_to_human_generated": null,
            "comparison_results": null,
            "limitations_noted": "100 resamples provide coarse estimate of variability; chosen to balance compute cost with uncertainty quantification.",
            "uuid": "e7795.16",
            "source_info": {
                "paper_title": "Is Your Paper Being Reviewed by an LLM? Benchmarking AI Text Detection in Peer Review",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Wilcoxon signed-rank test",
            "name_full": "Two-sided Wilcoxon signed-rank test",
            "brief_description": "A nonparametric paired statistical test used to assess whether AI-generated review scores differ significantly from matched human-review scores.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "statistics",
            "theory_type": "statistical significance test",
            "evaluation_method_name": "Wilcoxon signed-rank test (two-sided)",
            "evaluation_method_description": "Compute paired differences between matched AI and human numeric scores, apply the Wilcoxon signed-rank test to test the null hypothesis of zero median difference; report p-values.",
            "evaluation_metric": "p-value from Wilcoxon test",
            "metric_definition": "Two-sided p-value assessing significance of median difference in paired scores (e.g., soundness, presentation, contribution, confidence); significance thresholds conventional (e.g., p&lt;0.05).",
            "dataset_or_benchmark": "Applied to matched AI vs human review scores (NeurIPS 2022 etc.) in this paper",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "AI-generated reviews assign higher scores than human reviews with high statistical significance (p-values reported in Figure S3 legend).",
            "comparison_to_human_generated": true,
            "comparison_results": "Consistently higher numeric scores from LLMs across categories (soundness, presentation, contribution, confidence) relative to human reviewers.",
            "limitations_noted": "Statistical significance does not indicate practical significance; scoring distributions and potential labeling noise (post-ChatGPT reviews) considered.",
            "uuid": "e7795.17",
            "source_info": {
                "paper_title": "Is Your Paper Being Reviewed by an LLM? Benchmarking AI Text Detection in Peer Review",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Voting ensemble (Anchor)",
            "name_full": "Anchor voting mechanism across multiple anchor LLMs",
            "brief_description": "A pragmatic ensemble scheme for Anchor: generate anchor reviews using multiple anchor LLMs (GPT-4o, Gemini, Claude), compute similarity-based labels per anchor, and label a review as AI-generated if any anchor yields a positive detection.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o, Gemini 1.5 pro, Claude Sonnet 3.5",
            "model_size": null,
            "scientific_domain": "computer science (NLP ensemble methods)",
            "theory_type": "ensemble decision rule",
            "evaluation_method_name": "Anchor voting (OR rule across anchor LLMs)",
            "evaluation_method_description": "Generate multiple anchor embeddings from different LLMs, compute cosine similarity scores with the test review for each, threshold each score; final label positive if any anchor's threshold is exceeded.",
            "evaluation_metric": "OR-voting detection label; downstream ROC/AUC and TPR/FPR measured as usual",
            "metric_definition": "Binary decision combining per-anchor binary labels via logical OR; thresholds for each anchor set using calibration data.",
            "dataset_or_benchmark": "Applied on AI-Peer-Review-Detection-Benchmark",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Voting across three anchor LLMs was used in experiments; explicit per-vote ablation details in Appendix D.1.",
            "comparison_to_human_generated": true,
            "comparison_results": "Voting helps in black-box source-LMM scenarios by capturing cases where at least one anchor model aligns with the source LLM's style.",
            "limitations_noted": "OR voting increases sensitivity but may increase false positives if anchors are poorly calibrated; computational and API cost increases with number of anchors.",
            "uuid": "e7795.18",
            "source_info": {
                "paper_title": "Is Your Paper Being Reviewed by an LLM? Benchmarking AI Text Detection in Peer Review",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "t-SNE visualization (embedding analysis)",
            "name_full": "t-distributed Stochastic Neighbor Embedding (t-SNE) of sentence embeddings",
            "brief_description": "A dimensionality-reduction visualization used to inspect embedding-level style differences between reviews generated with different prompts (score-aligned vs archetype prompts).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "OpenAI text-embedding-3-small (used for embeddings in visualization)",
            "model_size": null,
            "scientific_domain": "machine learning (visualization / representation analysis)",
            "theory_type": "exploratory analysis / visualization",
            "evaluation_method_name": "t-SNE projection of embeddings",
            "evaluation_method_description": "Compute sentence embeddings for reviews, apply t-SNE (perplexity=30, 2D) and visualize overlap between sets to assess prompt-induced stylistic separation.",
            "evaluation_metric": "Visual overlap / cluster separation; qualitative assessment",
            "metric_definition": "t-SNE 2D coordinates (unitless) used for inspection; no numeric separation metric reported beyond visual overlap.",
            "dataset_or_benchmark": "Applied to ICLR2021 test set's generated reviews under different prompting strategies",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "No meaningful separation observed between reviews generated with the primary score-aligned prompt and the archetype-based prompt, suggesting moderate prompt variation does not cause major stylistic shifts.",
            "comparison_to_human_generated": null,
            "comparison_results": null,
            "limitations_noted": "t-SNE is qualitative and sensitive to hyperparameters; absence of visible separation does not guarantee indistinguishability in all feature spaces.",
            "uuid": "e7795.19",
            "source_info": {
                "paper_title": "Is Your Paper Being Reviewed by an LLM? Benchmarking AI Text Detection in Peer Review",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Computation-time benchmarking",
            "name_full": "Computation time per-method benchmarking",
            "brief_description": "Empirical measurement of time-to-process a batch of samples (100) per detection method on a single NVIDIA RTX A6000 GPU (Anchor excluded as it used sequential API calls).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "machine learning systems / benchmarking",
            "theory_type": "operational metric",
            "evaluation_method_name": "Computation-time benchmarking (per 100 samples)",
            "evaluation_method_description": "Run each detection method 20 times on 100-sample batches, report mean and standard deviation of elapsed time; Anchor measured separately using sequential API calls.",
            "evaluation_metric": "Time in seconds per 100 samples (mean ± std)",
            "metric_definition": "Elapsed wall-clock seconds averaged over 20 repeats on specified hardware (NVIDIA RTX A6000 GPU), or measured as sequential API latency for Anchor.",
            "dataset_or_benchmark": "Measured on 100-sample subsets from AI-Peer-Review-Detection-Benchmark",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Figure S1 summarizes compute times; Anchor was not GPU-accelerated and thus slower per-sample due to sequential API calls, while other methods vary depending on model complexity.",
            "comparison_to_human_generated": null,
            "comparison_results": null,
            "limitations_noted": "Compute times depend on implementation details and hardware; Anchor's sequential API calls could be optimized via parallelization.",
            "uuid": "e7795.20",
            "source_info": {
                "paper_title": "Is Your Paper Being Reviewed by an LLM? Benchmarking AI Text Detection in Peer Review",
                "publication_date_yy_mm": "2025-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature",
            "rating": 2,
            "sanitized_title": "detectgpt_zeroshot_machinegenerated_text_detection_using_probability_curvature"
        },
        {
            "paper_title": "Spotting llms with binoculars: Zero-shot detection of machine-generated text",
            "rating": 2,
            "sanitized_title": "spotting_llms_with_binoculars_zeroshot_detection_of_machinegenerated_text"
        },
        {
            "paper_title": "Dna-gpt: Divergent n-gram analysis for training-free detection of gpt-generated text",
            "rating": 2,
            "sanitized_title": "dnagpt_divergent_ngram_analysis_for_trainingfree_detection_of_gptgenerated_text"
        },
        {
            "paper_title": "IMGTB: A Framework for Machine-Generated Text Detection Benchmarking",
            "rating": 2,
            "sanitized_title": "imgtb_a_framework_for_machinegenerated_text_detection_benchmarking"
        },
        {
            "paper_title": "MGTBench: Benchmarking Machine-Generated Text Detection",
            "rating": 2,
            "sanitized_title": "mgtbench_benchmarking_machinegenerated_text_detection"
        },
        {
            "paper_title": "Raid: A shared benchmark for robust evaluation of machine-generated text detectors",
            "rating": 2,
            "sanitized_title": "raid_a_shared_benchmark_for_robust_evaluation_of_machinegenerated_text_detectors"
        },
        {
            "paper_title": "Beemo: Benchmark of expert-edited machine-generated outputs",
            "rating": 1,
            "sanitized_title": "beemo_benchmark_of_expertedited_machinegenerated_outputs"
        }
    ],
    "cost": 0.02986075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Is Your Paper Being Reviewed by an LLM? Benchmarking AI Text Detection in Peer Review
23 May 2025</p>
<p>Sungduk Yu sungduk.yu@intel.com 
Man Luo man.luo@intel.com 
Avinash Madusu avinash.madasu@intel.com 
Vasudev Lal vasudev.lal@intel.com 
Phillip Howard phillip.howard@thoughtworks.com 
Intel Labs 
♢ Thoughtworks 
Is Your Paper Being Reviewed by an LLM? Benchmarking AI Text Detection in Peer Review
23 May 2025EF5EBB276D10A9D67A83CFFFBA6D6AACarXiv:2502.19614v2[cs.CL]
Peer review is a critical process for ensuring the integrity of published scientific research.Confidence in this process is predicated on the assumption that experts in the relevant domain give careful consideration to the merits of manuscripts which are submitted for publication.With the recent rapid advancements in large language models (LLMs), a new risk to the peer review process is that negligent reviewers will rely on LLMs to perform the often time consuming process of reviewing a paper.However, there is a lack of existing resources for benchmarking the detectability of AI text in the domain of peer review.To address this deficiency, we introduce a comprehensive dataset containing a total of 788,984 AI-written peer reviews paired with corresponding human reviews, covering 8 years of papers submitted to each of two leading AI research conferences (ICLR and NeurIPS).We use this new resource to evaluate the ability of 18 existing AI text detection algorithms to distinguish between peer reviews fully written by humans and different state-of-the-art LLMs.Additionally, we explore a contextaware detection method called Anchor, which leverages manuscript content to detect AI-generated reviews, and analyze the sensitivity of detection models to LLM-assisted editing of human-written text.Our work reveals the difficulty of identifying AI-generated text at the individual peer review level, highlighting the urgent need for new tools and methods to detect this unethical use of generative AI.Our dataset is publicly available at: https://huggingface.co/datasets/ IntelLabs/AI-Peer-Review-Detection-Benchmark.</p>
<p>Introduction</p>
<p>Recent advancements in large language models (LLMs) have enabled their application to a broad range of domains, where LLMs have demonstrated the ability to produce plausible and authoritative responses to queries even in highly technical subject areas.These advancements have coincided with a surge in interest in AI research, resulting in increased paper submissions to leading AI conferences [1].Consequently, workloads for peer reviewers have also increased significantly, which could make LLMs an appealing tool for lessening the burden of fulfilling their peer review obligations [2][3][4].</p>
<p>Despite their impressive capabilities, the use of LLMs in the peer review process raises several ethical and methodological concerns which could compromise the integrity of the publication process [5][6][7][8].Reviewers are selected based on their expertise in a technical domain related to a submitted manuscript, which is necessary to critically evaluate the proposed research.Offloading this responsibility to an LLM circumvents the role that reviewer selection plays in ensuring proper vetting of a manuscript.Furthermore, LLMs are prone to hallucination and may not possess the ability to rigorously evaluate research publications.Therefore, the use of LLMs in an undisclosed manner in peer review poses a significant ethical concern that could undermine confidence in this important process.Motivating the need for evaluation resources and detection tools to address this problem is the apparent increase in AI-generated text among peer reviews submitted to recent AI research conferences.Prior studies revealed an upward trend in AI-generated texts among peer reviews [6,9].This trend is particularly concerning given that evaluations from human and AI reviewers are not aligned [6,10,11] and that LLM-generated reviews lack robustness [12], suggesting the unregulated and undisclosed use of LLMs in peer review could undermine the integrity of the current system.</p>
<p>Despite the growing recognition of this problem, there is a lack of existing dataset resources for comprehensively evaluating the performance of AI text detection methods in the domain of peer review.To address this deficiency, we introduce the largest dataset to-date of parallel human-written and LLM-written peer reviews for 8 years of papers submitted to two leading AI research conferences, NeurIPS and ICLR (Figure 1).Our dataset consolidates human-written peer reviews from existing sources with AI-written peer reviews that we generated for the same paper using five state-of-the-art LLMs: GPT-4o [13], Claude Sonnet 3.5 [14], Gemini 1.5 pro [15], Qwen 2.5 72b [16], and Llama 3.1 70b [17].In total, our dataset contains 788,984 peer reviews, evenly balanced between human-written reviews and AI-generated peer reviews created by these five LLMs.</p>
<p>We use our dataset to investigate the suitability of various AI text detection methods for identifying LLM generations in the peer review process.While limited prior work has analyzed the presence of AI-generated text in peer reviews at the corpus level [9] or has analyzed the use of propriety solutions [6], our study is the first to investigate the detectability LLM generations at the individual review level using synthetically generated AI samples, which is necessary to address this problem in practice.Specifically, we evaluate 18 existing open-source methods for AI text detection.</p>
<p>Our results show that most existing AI-text detection methods are limited in their ability to robustly detect AI-generated reviews while maintaining a low number of false positives.Motivated by this finding, we test an alternative approach which is specifically designed for AI text detection in the peer review context.Leveraging the additional context available in the peer review setting, our method detects AI-generated peer reviews by comparing the semantic similarity of a given review to a set of reference AI-generated reviews for the same paper.We find that this simple yet effective method surpasses the performance of all existing approaches in detecting GPT-4o and Claude written peer reviews.Additionally, we conduct analyses to understand how different levels of AI use for editing reviews impacts detectability and false positives, as well as the characteristics which distinguish LLM-written peer reviews from those written by humans.Our work demonstrate the challenge of detecting AI-written text in peer reviews and motivates the need for further research on methods to address this unethical use of LLMs in the peer review process.</p>
<p>To summarize, our contributions are as follows: (1) We publicly release a dataset of 788,984 AIwritten peer reviews generated by five widely-used LLMs paired with human-written reviews for the same papers, which is the largest resource to-date for studying AI text detection in peer review.</p>
<p>(2) Using our dataset, we benchmark 18 open-source AI text detection algorithms, finding that most struggle to reliably detect fully AI-written peer reviews at low false positive rates.(3) We propose a new context-aware detection method which compares the semantic similarity between a candidate review and a reference LLM-generated review for the same paper, achieving strong performance under strict FPR constraints.(4) We conduct analyses revealing key differences between human-and AI-written reviews, finding that AI-generated reviews are generally less specific, more favorable, and more confident.(5) We evaluate how LLM-assisted editing in peer review affects detection rates.</p>
<p>2 Related Work AI text detection datasets Several datasets have been introduced to evaluate AI text detection models.RAID-TD [18] provides a large-scale benchmark designed to assess text detection under adversarial conditions, ensuring robustness against manipulated AI-generated content.The M4 Dataset [19] expands the scope by incorporating reviews from multiple LLMs across different languages, offering a more diverse linguistic evaluation.The HC3 Dataset [20] consists of responses from ChatGPT and human experts, covering specialized domains such as finance, medicine, and law, in addition to general open-domain content.In contrast, the GPT Reddit Dataset (GRiD) [21] focuses on social media conversations, compiling a diverse set of human-and AI-generated responses to Reddit discussions.Meanwhile, Beemo [22] introduces a benchmark of expert-edited machine-generated outputs, spanning creative writing, summarization, and other practical applications.These benchmarks primarily evaluate AI-generated text from a single model and do not address the domain of AI text in peer review.In contrast, our dataset is larger than most existing datasets (788k generations) and is unique in its focus on AI text detection in peer review.</p>
<p>AI-generated text detection AI-generated text detection has been framed as a binary classification task to distinguish human-written from machine-generated text [23][24][25][26].Solaiman et al. [27] used a bag-of-words model with logistic regression for GPT-2 detection, while fine-tuned language models like RoBERTa [28] improved accuracy [29][30][31].Zero-shot methods based on perplexity and entropy emerged as alternatives [31,32].Other studies focused on linguistic patterns and syntactic features for model-agnostic detection [30,31].Watermarking techniques, such as DetectGPT [26], have also been proposed for proactive identification.Centralized frameworks like MGTBench [33] and its refined version, IMGTB [34], provide standardized evaluations for AI text detection.IMGTB categorizes methods into model-based and metric-based approaches.Model-based methods leverage large language models such as ChatGPT-turbo [35] and Claude [14].Metric-based methods, including Log-Likelihood [27], Rank [31], Entropy [31], DetectGPT [26], and LRR [36], rely on log-likelihood and ranking for classification.</p>
<p>AI-assisted peer review</p>
<p>Recent studies have explored the role of LLMs in peer review, examining their influence on reviewing practices [9,37], simulating multi-turn interactions [38], and assessing their reviewing capabilities [8].Tools like OpenReviewer [39] provide AI-assisted review improvements, while other works focus on LLM transparency [2] and distinguishing AI-generated content [40].Recent studies have investigated AI-driven review systems [41], agentic frameworks [42], and comparative analyses of LLM-generated reviews [43], along with broader explorations of LLMs' roles and limitations in peer review [37,[44][45][46].While it is not the primary focus of our work, we analyze the quality of LLM-generated peer reviews in Section 5.</p>
<p>3 Dataset Construction</p>
<p>Human reviews</p>
<p>We used the OpenReview API [47] to collect submitted manuscripts and their reviews for the ICLR conferences from 2019 to 2024, as well as for NeurIPS conferences from 2021 to 2024 1 .Additionally, we used the ASAP dataset [48] to collect manuscripts and reviews for ICLR 2017 to 2018 and NeurIPS 2016 to 2019.</p>
<p>AI reviews</p>
<p>We generated 788,984 AI-generated reviews using five widely-used LLMs: GPT-4o, Claude Sonnet 3.5, Gemini 1.5 pro, Qwen 2.5 72b, and Llama 3.1 70b.</p>
<p>Prompts.To control the content and structure of these AI-generated reviews, we included conferencespecific reviewer guidelines and review templates in the prompts.Note that review templates have evolved significantly over time (Table S1), necessitating prompt adaptations for papers submitted in different years.We additionally aligned the paper decisions by prompting the LLMs with specific decisions derived from the corresponding human reviews (see Appendix E for complete prompt details).This step is important, as we found that AI review content and recommendations vary substantially depending on how input prompts are constructed.Thus, these measures represent our efforts to control the influence of text prompts.Importantly, as we show later in Section 5.2, despite using a consistent prompting strategy, our dataset remains robust to prompt variation and supports generalizable detection performance across different prompting styles.</p>
<p>Computation.We used Azure OpenAI Service, Amazon Web Services, and Google Cloud Platform to generate GPT-4o, Claude, and Gemini reviews, respectively.For Qwen reviews, we used NVIDIA RTX 6000 GPUs, and for Llama reviews, we used Intel Gaudi 2 accelerators.</p>
<p>Dataset Statistics</p>
<p>Table 1 provides complete statistics for our generated dataset (see Appendix B.4 for a breakdown by conference year and review-generating LLM).We withheld a randomly sampled subset of reviews to serve as a calibration set, which is used in our experiments to determine classification thresholds for each evaluated method.This calibration set contains 75,824 AI-generated and human-generated peer reviews, divided approximately evenly across all five LLMs.To construct the calibration set, we randomly selected 500 papers from ICLR (2021, 2022) and NeurIPS (2021, 2022) and generated AI reviews corresponding to the human reviews for each paper.Because our sampling was done at the paper level rather than the review level, the number of reviews per paper-and consequently per conference-varies slightly.To facilitate the evaluation of detection methods which are more computationally expensive (e.g., methods which requires using LLMs as surrogates), we also withheld a separate test set consisting of human reviews and those generated by all five LLMs for 500 randomly sampled papers from each conference &amp; year.This test split contains a total of 287,052 reviews and is used throughout our main experimental results.A further 426,108 reviews were generated from GPT-4o and Llama 3.1 70b, which we designate as an extended set of reviews (additional experimental results for the extended set are provided in Appendix C.4).</p>
<p>While we include reviews from post-ChatGPT2 conferences (ICLR 2023-2024 and NeurIPS 2023-2024), we note that the human-labeled reviews from these years may include LLM-assisted texts.These reviews are retained to support broader research use cases, such as longitudinal analysis of linguistic trends.However, we do not use them in our main experiments, which focus on pre-2023 data where we have higher confidence in the human-authored labels.</p>
<p>We note slight variations in sample sizes among the five LLMs.These differences stem from LLMspecific factors such as context window limits, generation errors (e.g., malformed JSON or degenerate outputs), and input prompt safety filters.4 Experimental Results</p>
<p>Fully AI-Written Review Detectability</p>
<p>We compare our approach with 18 baseline methods using IMGTB (see Appendix A for details) and utilize the calibration set to determine appropriate thresholds for the test data.To minimize the risk of LLM-text contamination in human-written reviews, we only include reviews submitted before the release of ChatGPT in late 2022 (i.e., NeurIPS 2016-2022 and ICLR 2017-2022 papers), when LLM use rapidly became widespread.The threshold is then determined by setting a target False Positive Rate (FPR), which is achieved by adjusting the threshold until the FPR equals the target value.We focus on low FPR targets (e.g., 0.1%, 0.5%, and 1%) because false positive classifications-where human-written reviews are mistakenly identified as AI-generated-carry high stakes, potentially damaging an individual's reputation.Additionally, we focus on AI review text samples generated by three commercial LLMs (GPT-4o, Gemini, and Claude) because these models are more advanced, making the AI text detection task harder.Many users are more likely to choose these models over open-source LLMs due to their convenient user interfaces and limited access to the compute resources required for running advanced open-source models.</p>
<p>AI text detection models can be calibrated for varying levels of sensitivity in order to balance the trade-off between true positive and false positive detections.Receiver operating characteristic (ROC) curves are therefore commonly used to compare different methods, as they provide a visualization of the true-positive rate (TPR) which can be achieved by a model when its decision boundary is calibrated for a range of different FPRs. Figure 2 provides the ROC curves for baseline methods, calculated using our GPT-4o, Gemini, and Claude review calibration subset separately for reviews submitted to ICLR 2021-2022 (left) and NeurIPS 2021-2022 (right).The area under the curve (AUC) is provided for each method in the legend; higher values indicate better detection accuracy across the entire range of FPR values.Among the 18 baseline models, Binoculars consistently performs the best across the ROC curve, particularly excelling at maintaining a low FPR.Our task-specific anchor method appears to perform strongly, but we will discuss its performance separately in Section 5.1; here, we focus on analyzing the 18 existing detection methods.</p>
<p>Although ROC curves are useful for comparing the overall performance of different classifiers, only the far left portion of these plots are typically relevant for practical applications of AI text detection models.This is particularly true in the domain of peer review, where the cost of a false positive is high.Reviewers volunteer their time and expertise to sustain this important process; false accusations have the potential to further reduce the availability of reviewers due to disengagement and can also lead to significant reputational harm.Therefore, it is vital that AI text detection systems for peer review be calibrated for a low FPR in order to avoid such negative outcomes.</p>
<p>Prior work has shown that AUC is not necessarily reflective of how models perform at very low FPR values [49][50][51].Therefore, we also report the actual TPR and FPR achieved by different detection methods at discrete low values of target FPR (0.1%, 0.5%, and 1%), which we believe to be of greatest interest for practical applications.The target FPR is used to calculate each method's classification threshold using our calibration dataset, with the actual TPR and FPR computed over the withheld test dataset.To simulate a more challenging evaluation setting where some of the test reviews are "out-of-domain" in the sense that they come from a different conference than the calibration dataset, we use only the ICLR reviews to calibrate each method (see Section C.2 for in-domain evaluations).The results in Table 2 show that among the 18 baselines, Binoculars generally achieves the best performance overall.For example, its TPR reaches 45-85% at a 1% target FPR, while other models reach only up to 19% TPR.However, even with the best-performing Binoculars, performance drops significantly at more stringent FPR levels-for instance, it achieves only 17% TPR at 0.1% FPR for GPT-4o reviews.These results highlight a key challenge in AI-generated peer review detection: the difficulty of robustly identifying AI-written text at the level of individual peer reviews.At the same time, they point to the need for new methods that can improve accuracy and robustness under low-FPR constraints.To this end, we explore a purposebuilt detection method (Anchor) in Section 5.1, which leverages additional context specific to the peer review detection task.</p>
<p>Analysis</p>
<p>Can manuscript context improve detection of AI-generated peer reviews?</p>
<p>Unlike general AI text detection scenarios, peer review provides additional contextual information for this problem: the manuscript being reviewed.With access to metadata connecting reviews to their source manuscripts (as in platforms like OpenReview.net), we investigate whether leveraging the manuscript can improve the detection of AI-generated peer reviews.</p>
<p>To test this idea, we introduce a method which utilizes the manuscript by comparing the semantic similarity between a test reviews (TR) and a synthetic "Anchor Review" (AR) generated for the same manuscript.The AR can be generated by any LLM.We use a simple, generic prompt (Appendix E.2) to generate the AR without prior knowledge of the user prompts (i.e., the AR prompt differs from those used to create reviews in the testing dataset).Once an AR is generated for a given paper (Eq.1) and a testing review (TR) is provided, we obtain their embeddings using a text embedding model (EM, Eqs.2 and 3).The semantic similarity between the embeddings of the AR and TR is then computed using the cosine similarity function (Eq.4).Finally, this similarity score is compared against a learned threshold (θ): if the score exceeds the threshold, the review is classified as AI-generated; otherwise, it is not (Eq.5).Putting everything together, the method is formalized as:
AR = LLM(paper, Prompt AR ) (1) EmbAR = EM(AR) (2) EmbTR = EM(TR) (3) Score = Cosine_similarity(EmbAR, EmbTR) (4) Label = 1 if Score &gt; θ, 0 otherwise.(5)
In our study, we use OpenAI's embedding model (text-embedding-003-small).The threshold θ is learned from the calibration set.Specifically, for each review in the calibration data, we apply the steps outlined in Eqs. 1 to 4. To handle cases where the source LLM is unknown, we generate multiple anchor reviews using different LLMs and apply a voting scheme: if any anchor yields a positive detection, the review is labeled as AI-generated (see Appendix D.1 for details).</p>
<p>Compared to the 18 baseline models, our anchor embedding approach consistently achieves the highest AUC for AI peer review (Figure 2) and outperforms other baseline methods overall (Table 2).The performance gap is especially notable for GPT-4o, which is generally harder to detect; for example, at a target FPR of 0.1%, the anchor embedding method achieves a TPR of 63.5% for GPT-4o reviews, compared to 17.1% for the next-best method (Binoculars), which is an absolute improvement of 46.4%.This indicates that the anchor embedding approach is particularly effective for the most challenging review samples.On Gemini reviews, where baseline methods already perform well, the anchor embedding method performs similarly to Binoculars.These results suggest that leveraging the context of the submitted manuscript through anchor embeddings offers substantial benefits for high-risk, low-FPR detection tasks, especially when reviews are written by the most advanced LLMs.</p>
<p>Is AI peer review detection robust to prompt variations?</p>
<p>A potential limitation in constructing AI-generated datasets is the reliance on a fixed prompt, which may constrain stylistic diversity or lead to overfitting of downstream detection models.Moreover, such fixed prompting may not reflect real-world use cases, where different users are likely to employ varied and personalized prompts when generating reviews.While our primary prompt is designed to be semantically grounded-using the paper content and a human reviewer's overall score to guide generation-it is important to evaluate whether this design leads to narrow outputs or brittle detection performance.To assess the impact of prompt variation, we conducted a prompt sensitivity analysis using an alternative prompting strategy that simulates diverse reviewer archetypes (e.g., "balanced," "conservative," "innovative," and "nitpicky"; see Appendix E.4 for the exact prompts we used).For this analysis, we generated 1,921 GPT-4o reviews for papers corresponding to the ICLR 2021 test set.</p>
<p>Cross-Prompt Detection Robustness</p>
<p>We evaluated whether prompt-induced shifts in review style affect the robustness of AI text detectors.Specifically, we applied detection models calibrated on the main (i.e., score-aligned) prompt and evaluated them on reviews generated using the archetype-based prompt.Table S10 reports the TPRs at multiple target FPRs for a range of baseline detection methods.Across all settings, we observe that TPRs under cross-prompt testing remain largely consistent with those under in-prompt testing, with minimal change in detection performance.This result suggests that, despite changes in prompt framing, the core distributional properties of the AI-generated reviews are sufficiently stable for detection models to generalize.Overall, these findings indicate that the utility of our dataset is not overly sensitive to prompt selection.</p>
<p>Embedding-Based Style Consistency</p>
<p>We further investigated prompt-induced variation using sentence-level embedding analysis.Figure S2 visualizes a t-SNE projection of review embeddings from two groups: (i) AI-generated reviews using the primary score-aligned prompt and (ii) AIgenerated reviews using the archetype prompt.No meaningful separation emerged between these two review types, suggesting they share broadly similar linguistic and semantic characteristics.This finding reinforces the robustness of our prompting strategy and implies that moderate prompt variation does not lead to drastic stylistic shifts in model outputs.</p>
<p>Can detection models distinguish human reviews edited by LLMs?</p>
<p>LLMs are widely used for writing assistance tasks such as grammar correction and fluency enhancement [52,53], especially benefiting non-native speakers.However, excessive reliance on LLMs can lead to substantial rewriting, blurring the line between human-and AI-authored reviews.To simulate this scenario, we took human-written peer reviews from the ICLR 2021 GPT-4o test subset and generated AI-edited versions at four increasing levels of editing: Minimum, Moderate, Extensive, and Maximum (see Appendix E.3 for prompts).To validate that the edits are semantically distinct, we measured cosine similarity between the original and edited reviews using a sentence embedding model.As shown in Table S12, similarity scores decrease as the level of editing increases.We evaluate detection models on two fronts: (i) their ability to flag edited reviews as AI-generated, and (ii) their ability to rank the reviews by degree of AI involvement using Normalized Discounted Cumulative Gain (NDCG) [54].Table 3 compares our anchor embedding method with Binoculars, the strongest-performing baseline from previous experiments (as shown in Table 2).Both models achieve high NDCG scores (0.90 for Anchor, 0.86 for Binoculars), indicating that they can generally rank reviews according to the degree of AI editing.However, their flagging behavior differs.At the 0.1% FPR threshold, both models rarely flag minimally to extensively edited reviews as AI-generated (≤2.5%), but diverge sharply at the Maximum level: Anchor flags 60.8% of these reviews, compared to just 9.2% for Binoculars.This trend persists at the 1% FPR threshold, with generally higher flagging rates across all levels.These results suggest that the Anchor method is more sensitive to high levels of AI involvement, whereas Binoculars is more conservative at the upper end.</p>
<p>Overall, these results suggest that high-performing detection models can distinguish varying levels of AI editing in human-written reviews.However, lightly edited texts remain difficult to detect, highlighting a challenge for future work on identifying hybrid human-AI content.</p>
<p>How do human-written and AI-generated peer reviews differ?</p>
<p>To better understand the characteristics which differentiate peer reviews written by humans and LLMs, we conducted a quantitative analysis of 32 reviews authored by humans and GPT-4o for Prior work has shown that peer reviews written by GPT-4 and humans have a level of semantic similarity which is comparable to that between different human-authored peer reviews, which has been used to advocate for the usefulness of feedback from GPT-4 in the paper writing process [37].In our qualitative analysis, we found that GPT-4 does indeed generate similar higher-level comments as human reviewers, which could account for this semantic similarity.Despite being generic in nature, we would agree that such feedback could be useful to authors seeking to improve their manuscripts.Nevertheless, we believe that the lack of specificity, detail, and consideration of related work in peer reviews authored by GPT-4 demonstrates that it is not suitable for replacing human domain experts in the peer review process.</p>
<p>5.5 Do AI-generated reviews assign higher scores than human reviews?</p>
<p>In addition to qualitative differences in the content of human and AI-written reviews, we also observe a divergence in numeric scores assigned as part of the review.Figure S3 in Appendix D.4 provides histograms depicting the distribution of score differences for soundness, presentation, contribution, and confidence, which are computed by subtracting scores assigned for each category by human reviewers from those assigned by AI reviewers.AI-written peer reviews were matched with their corresponding human review (aligned by paper ID and overall recommendation) to compute the score differences.Confidence scores range from 1 to 5, while all other categories of scores range from 1 to 4. In the following discussion, We focus on reviews from NeurIPS 2022, which were produced prior to the release of ChatGPT.This provides greater confidence that the human-labeled reviews were indeed written by humans, with little to no potential AI influence.</p>
<p>All LLMs produce higher scores than human reviews with a high degree of statistical significance, assessed using a two-sided Wilcoxon signed-rank test (see legend for p-values in Figure S3).While the difference between human and AI confidence scores are relatively consistent across all three LLMs, Claude exceeds human scores by the greatest magnitude for soundness, presentation, and contribution.GPT-4o and and Gemini exceed human scores by a similar magnitude for presentation and contribution, while GPT-4o exhibits a greater divergence for soundness scores.Overall these results indicate that AI-generated peer reviews are more favorable w.r.t.assigned scores than humanwritten peer reviews, which raises fairness concerns as scores are highly correlated with acceptance decisions.Our findings are consistent with prior work which has shown that papers reviewed by LLMs have a higher chance of acceptance [6,10,11].</p>
<p>Limitations</p>
<p>Our dataset primarily focuses on two conferences, both within the computer science domain.To broaden its applicability and relevance, incorporating additional conferences from diverse research areas would be beneficial.While we designed our prompting strategy to encourage stylistic diversity, prompt choice still influences generation, and real-world use cases may involve a wider range of prompting styles than those tested.We conducted a prompt sensitivity analysis (Section 5.2) to evaluate the robustness of our dataset under prompt variation, though broader coverage remains an open direction.Our study also includes detection scenarios where LLMs revise or extend humanwritten reviews ("AI-edits-human"), but does not simulate the reverse case-where a human revises an AI-generated draft-due to the difficulty of sourcing domain experts to perform such edits at scale.We consider this an important direction for future work.Lastly, our main results are based on evaluations of three commercial LLMs.Given the rapid emergence of new models, conducting comprehensive experiments across all available LLMs is infeasible.In addition, we leverage an open-source platform to run baseline experiments, where performance may vary depending on the choice of surrogate models.However, given the large number of baselines we evaluate, performing an exhaustive search for the optimal surrogate model for each method would be prohibitively expensive.Therefore, we use the default settings.</p>
<p>While our anchor embedding method shows promising results, it is not without limitations.It is task-specific by design and is therefore unsuitable as a general-purpose AI text detection model.Additionally, its reliance on commercial LLM APIs may introduce challenges w.r.t.computational cost and scalability.The method's performance can also be sensitive to the choice of anchor LLMs.This is not specific to Anchor but reflects a general challenge in black-box detection settings where the source LLM is unknown.Acknowledging these limitations, our findings nonetheless highlight how leveraging manuscript context as auxiliary information can significantly improve the accuracy and robustness of AI-generated peer review detection, especially under low-FPR constraints.</p>
<p>Conclusion</p>
<p>In this work, we introduced a new large-scale dataset of parallel human-written and AI-generated peer reviews for identical papers submitted to leading AI research conferences.Our evaluations show that existing open-source methods for AI text detection struggle in the peer review setting, where high detection rates often come at the cost of falsely flagging human-written reviews-an outcome that must be minimized in practice.We demonstrate that leveraging manuscript context is a promising strategy for improving detection accuracy while maintaining a low false positive rate.</p>
<p>In addition, AI-generated reviews tend to be less specific and less grounded in the manuscript than human-written ones.We also find that AI-generated reviews consistently assign higher scores, raising fairness concerns in score-driven decision-making processes.We hope our results motivate further research on responsible detection of AI-generated content in scientific review workflows, and that our dataset provides a valuable resource for advancing this goal.</p>
<p>A Baseline methods.</p>
<p>We compare our approach to 18 baseline methods from IGMBT 3 (released under MIT license) with its default setting [34], which are categorized into metric-based and pretrained model-based methods.</p>
<p>The metric-based methods include Binoculars [55], DetectLLM-LLR [36], DNAGPT [49], Entropy [31], FastDetectGPT [56], GLTR [31], LLMDeviation [57], Loglikelihood [27], LogRank [58], MFD [57], Rank [31], and S5 [34].The model-based methods include NTNU-D [59], ChatGPT-D [20], OpenAI-D [27], OpenAI-D-lrg [27], RADAR-D [27], and MAGE-D [60].</p>
<p>A.1 Metric based methods</p>
<p>A.1.1 Binoculars</p>
<p>Binoculars [55] analyzes text through two perspectives.First, it calculates the log perplexity of the text using an observer LLM.Then, a performer LLM generates next-token predictions, whose perplexity is evaluated by the observer-this metric is termed cross-perplexity.The ratio of perplexity to cross-perplexity serves as an indicator for detecting LLM-generated text.</p>
<p>A.1.2 DNAGPT</p>
<p>DNAGPT [49] is a training-free detection method designed to identify machine-generated text.</p>
<p>Unlike conventional approaches that rely on training models, DNAGPT uses Divergent N-Gram Analysis (DNA) to detect discrepancies in text origin.The method works by truncating a given text at the midpoint and using the preceding portion as input to an LLM to regenerate the missing section.By comparing the regenerated text with the original through N-gram analysis (black-box) or probability divergence (white-box), DNAGPT reveals distributional differences between human and machine-written text, offering a flexible and explainable detection strategy.</p>
<p>A.1.3 Entropy</p>
<p>Similar to the Rank score, the Entropy score for a text is determined by averaging the entropy values of each word, conditioned on its preceding context [31].</p>
<p>A.1.4 GLTR</p>
<p>The Entropy score, like the Rank score, is computed by averaging the entropy values of each word within a text, considering the preceding context [31].</p>
<p>A.1.5 MFD</p>
<p>The Multi-Feature Detection (MFD) method [57] detects AI-generated text using four features: log-likelihood, log-rank, entropy, and LLM deviation.</p>
<p>A.1.6 Loglikelihood</p>
<p>This method utilizes a language model to compute the token-wise log probability.Specifically, given a text, the log probability of each token is averaged to produce a final score.A higher score indicates a greater likelihood that the text is machine-generated [27].</p>
<p>A.1.7 LogRank</p>
<p>Unlike the Rank metric, which relies on absolute rank values, the Log-Rank score is derived by applying a logarithmic function to the rank value of each word [58].</p>
<p>A.1.8 Rank</p>
<p>The Rank score is calculated by determining the absolute rank of each word in a text based on its preceding context.The final score is obtained by averaging the rank values across the text.A lower score suggests a higher probability that the text was machine-generated [31].</p>
<p>A.1.9 DetectLLM-LLR</p>
<p>This approach integrates Log-Likelihood and Log-Rank scores, leveraging their complementary properties to analyze a given text [36].</p>
<p>A.1.10 FastDetectGPT</p>
<p>This method assesses changes in a model's log probability function when small perturbations are introduced to a text.The underlying idea is that LLM-generated text often resides in a local optimum of the model's probability function.Consequently, minor perturbations to machine-generated text typically result in lower log probabilities, whereas perturbations to human-written text may lead to either an increase or decrease in log probability [58].</p>
<p>A.2 Model-based methods</p>
<p>A.2.1 NTNU-D</p>
<p>It is a fine-tuned classification model based on the RoBERTa-base model, and three sizes of the bloomz-models [59]</p>
<p>A.2.2 ChatGPT-D</p>
<p>The ChatGPT Detector [20] is designed to differentiate between human-written text and content generated by ChatGPT.It is based on a RoBERTa model that has been fine-tuned for this specific task.</p>
<p>The authors propose two training approaches: one that trains the model solely on generated responses and another that incorporates both question-answer pairs for joint training.In our evaluation, we adopt the first approach to maintain consistency with other detection methods.</p>
<p>A.2.3 OpenAI-D and RADAR-D</p>
<p>OpenAI Detector [27] is a fine-tuned RoBERTa model designed to identify outputs generated by GPT-2.Specifically, it was trained using text generated by the largest GPT-2 model (1.5B parameters) and is capable of determining whether a given text is machine-generated.</p>
<p>A.2.4 MAGE-D</p>
<p>MAGE (MAchine-GEnerated text detection) [60] is a large-scale benchmark designed for detecting AI-generated text.It compiles human-written content from seven diverse writing tasks, including story generation, news writing, and scientific writing.Corresponding machine-generated texts are produced using 27 different LLMs, such as ChatGPT, LLaMA, and Bloom, across three representative prompt types.</p>
<p>B Dataset Details B.1 Dataset</p>
<p>Our dataset is hosted on Hugging Face and publicly available at https://huggingface.co/ datasets/IntelLabs/AI-Peer-Review-Detection-Benchmark.It is released under the Intel OBL Internal R&amp;D Use License Agreement (https://huggingface.co/datasets/IntelLabs/ AI-Peer-Review-Detection-Benchmark/blob/main/LICENSE.md).</p>
<p>B.2 Dataset File Structure</p>
<p>The calibration, test, and extended sets are in separate directories.Each directory contains subdirectories for different models that were used to generate AI peer review samples.In each model's subdirectory, you will find multiple CSV files, with each file representing peer review samples of a specific conference.Each file follows the naming convention: "<conference>.<subset>.<LLM>.csv".The directory and file structure are outlined below.</p>
<p>B.3 CSV File Content</p>
<p>CSV files may differ in their column structures across conferences and years.These differences are due to updates in the required review fields over time as well as variations between conferences.See Table S1 for review fields of individual conferences.</p>
<p>Table S1: Required fields in the review templates for each conference.</p>
<p>Conference Required Fields ICLR2017 review, rating, confidence ICLR2018 review, rating, confidence ICLR2019 review, rating, confidence ICLR2020 review, rating, confidence, experience assessment, checking correctness of derivations and theory, checking correctness of experiments, thoroughness in paper reading ICLR2021 review, rating, confidence ICLR2022 summary of the paper, main review, summary of the review, correctness, technical novelty and significance, empirical novelty and significance, flag for ethics review, recommendation, confidence ICLR2023 summary of the paper, strength and weaknesses, clarity quality novelty and reproducibility, summary of the review, rating, confidence ICLR2024 summary, strengths, weaknesses, questions, soundness, presentation, contribution, flag for ethics review, rating, confidence NeurIPS2016 review, rating, confidence NeurIPS2017 review, rating, confidence NeurIPS2018 review, overall score, confidence score NeurIPS2019 review, overall score, confidence score, contribution NeurIPS2021 summary, main review, limitations and societal impact, rating, confidence, needs ethics review, ethics review area NeurIPS2022 summary, strengths and weaknesses, questions, limitations, ethics flag, ethics review area, rating, confidence, soundness, presentation, contribution NeurIPS2023 summary, strengths, weaknesses, questions, limitations, ethics flag, ethics review area, rating, confidence, soundness, presentation, contribution NeurIPS2024 summary, strengths, weaknesses, questions, limitations, ethics flag, ethics review area, rating, confidence, soundness, presentation, contribution</p>
<p>B.4 Dataset Sample Numbers per Conference Year</p>
<p>In this section, we present further breakdowns of sample numbers by conference, year, and LLM, as shown in Table 1.</p>
<p>C Additional results</p>
<p>C.1 Uncertainty Estimates via Bootstrap Resampling</p>
<p>To estimate the uncertainty of our main experimental results (TPR and FPR reported in Table 2), we perform bootstrap resampling with replacement using N = 100 resamples.For each method and evaluation setting, we compute the standard deviation (SD) of the TPR and FPR across the 100 bootstrap replicates.While 100 resamples provide only a coarse estimate of variability, this level of resampling was chosen to balance computational cost with the need to quantify uncertainty.As shown in Table S5, the standard deviations are small relative to the large performance gaps between methods, indicating the robustness of our main findings.</p>
<p>C.2 Calibration using ICLR + NeurIPS reviews</p>
<p>Our main results in Table 2 of Section 4.1 utilized ICLR review from our calibration set to calibrate each detection method.This simulates the scenario in which some of the reviews in the test set are "out-of-domain" in the sense that they belong to a different conference than the reviews used for calibration.In Table S6, we provide additional results for the same evaluation setting as before, but using both ICLR and NeurIPS reviews for calibration (i.e., fully "in domain").We generally see similar trends regarding relative performance between methods as before, with the exception that the Binoculars method achieves slightly higher detection rates than our Anchor method for Gemini reviews.This suggests that existing methods such as Binoculars may be more sensitive to the use of in-domain data during calibration.</p>
<p>C.3 Additional Llama and Qwen Detection Results</p>
<p>Table S7 is organized similarly to Table 2, but presents results for Llama and Qwen reviews.Both tables use the same set of thresholds for each method, which were calibrated using ICLR reviews generated by GPT-4o, Gemini, and Claude along with their matching human-written reviews.</p>
<p>Reviews generated by open-source LLMs (Table S7) show somewhat different trends compared to reviews generated by commercial LLMs (Table 2).For both Llama and Qwen reviews, the Binoculars method achieves near-perfect detection performance.While the Anchor method ranks second for Qwen reviews, it is not as performant on Llama reviews.This is surprising, as Llama reviews appear easier to detect.For instance, at a target FPR of 0.1%, more than 6.5 times as many methods achieve a TPR above 10% compared to GPT-4o, Gemini, and Claude reviews, and about twice as many compared to Qwen reviews.One possibility is that the semantic similarity between the three anchor reviews generated by higher-quality LLMs (GPT-4o, Gemini, and Claude) and Llama reviews is low enough to overlap with that between the anchor reviews and human reviews, potentially blurring the decision boundary.Although the underlying causes of these differences warrant further investigation, these findings are less central to our study since most LLM users are likely to rely on commercial models due to their ease of use and superior capabilities.</p>
<p>Table S7: Actual FPR and TPR calculated from the withheld test dataset at varying detection thresholds, which are calibrated using ICLR reviews from our calibration set at different target FPRs.Best TPRs are in bold.Detection methods are ordered by their TPR at a target FPR of 0.1%, and those that failed to achieve 10% TPR at a target FPR of 1% are omitted.</p>
<p>C.4 Experimental Results for Full Dataset</p>
<p>We test existing AI text generation text detection models on our entire dataset (i.e., the test set + the extended set).Results are provided in Tables S8 and S9 for ICLR and NeurIPS reviews (rspeectively).</p>
<p>Table S8: Actual FPR and TPR calculated from the ICLR reviews at varying detection thresholds, which are calibrated using the ICLR calibration dataset at different target FPRs.</p>
<p>C.5 Computation Time</p>
<p>We report the computation time of each detection method.Each method was evaluated on 100 samples using a single NVIDIA RTX A6000 GPU, repeated 20 times to compute the mean and standard deviation.The Anchor method is an exception: it does not use a GPU and relies on sequential API calls to the OpenAI service, which could potentially be optimized for faster execution (e.g., parallelizing API calls).Figure S1 summarizes the results.</p>
<p>Figure S1: Computation time (in seconds) for processing 100 samples.Each method was repeated 20 times to compute the mean and standard deviation.All methods were run on a single NVIDIA RTX A6000 GPU, except for Anchor, which used sequential API calls without GPU acceleration.</p>
<p>D Additional analyses D.1 Voting Mechanism for the Anchor Embedding Method</p>
<p>Intuitively, the anchor approach performs best when the anchor embeddings are generated using the same model that produced the test review (source LLM).However, in real-world scenarios, the source LLM is typically unknown (a situation commonly referred to as "black-box" detection scenario).</p>
<p>To address this challenge, we propose a voting-based technique.Specifically, we generate multiple anchor embeddings using different types of LLMs (anchor LLMs).For each anchor embedding, we compute the Score (Eq.4) and derive the corresponding label assignment (Eq.5).If at least one anchor embedding assigns a positive label, the final label is positive.Otherwise, the final label is negative.In our experiments, we used three anchor reviews for voting-each generated by GPT-4o, Gemini, and Claude, respectively.</p>
<p>D.2 Assessing Dataset Robustness to Prompt Variation (Supplementary)</p>
<p>This appendix provides supporting materials for the prompt sensitivity analysis described in Section 5.2 of the main text.The results presented here include quantitative evaluations of detection performance under prompt variation (Table S10) and a visualization of review embeddings to assess stylistic consistency across prompt types (Figure S2).Together, these results support the conclusion that our prompting strategy yields stable and generalizable outputs across different prompt formulations.</p>
<p>Table S10: Actual FPR and TPR on the withheld ICLR2021 test set.The upper section shows results for reviews generated using the score-aligned prompt (used in our main dataset), and the lower section shows results for reviews generated using the alternative, archetype-based prompt.In both cases, the same set of thresholds, calibrated on the calibration set, is applied.Detection performance remains largely consistent across prompt styles, indicating that prompt variation does not substantially degrade model effectiveness.</p>
<p>D.3 Examples from human analysis of differences between human and AI-written peer reviews</p>
<p>Table S11 provides examples of the issues identified in our qualitative analysis of human and AIwritten peer reviews.In general, we observe that GPT-4o reviews lack references to specific details in the paper, lack references to specific prior work, and contain overly generic criticisms.See Section 5.4 for additional discussion.(Yuan et al., 2020), which was published at ECCV 2020.The main difference seems to be that in the proposed method the graph is dynamic (i.e., it depends on the input sentences), instead in (Yuan et al., 2018) the graph is learned but fixed for all the input samples.""The novelty of the TDM is not strong enough relative to prior work."Generic criticisms N/A "Lack of clarity" (without pointing to specific statements in the paper which need clarification); "lack of discussion of limitations or computational considerations"; "need more discussion of hyperparameter sensitivity"; "need comparisons to more datasets" (without suggesting any in particular); "technical language used in the paper may be difficult to follow for unfamiliar readers"</p>
<p>D.4 Comparison of numeric scores assigned by human and AI reviewers</p>
<p>While Section 5.5 focused on the misalignment between human and AI peer reviews from three commercial LLMs (GPT-4o, Gemini, and Claude) from the NeurIPS2022 review samples, this section presents the corresponding results for two open-source LLMs (Llama and Qwen), as shown in Figure S3.The main findings from GPT-4o, Gemini, and Claude also hold for these two opensource models, with one notable difference: Llama and Qwen exhibit an even larger divergence in Presentation scores than Claude, the most overly-positive one amongst the commercial LLMs across all categories.In terms of Contribution scores, the evaluations from Llama and Qwen were similar to those of Claude.</p>
<p>In addition, we examine data from three other conferences (NeurIPS2023, NeurIPS2024, and ICLR2024).Although the results from these conferences are slightly less reliable-given that human reviews may have been influenced by AI use following the release of ChatGPT-the overall trend persists: LLMs tend to inflate the quality of papers compared to human reviewers.</p>
<p>D.5 Validation of AI-edited reviews</p>
<p>To simulate varying degrees of LLM-assisted editing, we took human-written reviews and generated AI-edited at four levels of modification: Minimum, Moderate, Extensive, and Maximum (see Appendix E.3 for the editing prompts used).To validate that these prompts produce meaningfully distinct levels of change, we computed cosine similarity scores between each AI-edited review and its original human version using text embeddings.As shown in Table S12, similarity scores progressively decrease with greater editing intensity, confirming that the levels are semantically distinguishable.</p>
<p>E Prompts</p>
<p>This section includes the we used to generate AI peer review texts.Due to space limitations, we provide only the ICLR2022 review guideline and review template here.Those for other years and other conferences (e.g., NeurIPS) are available on the respective conference official websites 4 .</p>
<p>E.1 Prompts for Generating Reviews</p>
<p>System prompt:</p>
<p>You are an AI researcher reviewing a paper submitted to a prestigious AI research conference.</p>
<p>You will be provided with the manuscript text, the conference's reviewer guidelines, and the decision for the paper.Your objective is to thoroughly evaluate the paper, adhering to the provided guidelines, and return a detailed assessment that supports the given decision using the specified response template.Ensure your evaluation is objective, comprehensive, and aligned with the conference standards.</p>
<p>{reviewer_guideline} {review_template}</p>
<p>User prompt:</p>
<p>Here is the paper you are asked to review.Write a well-justified review of this paper that aligns with a '{human_reviewer_decision}' decision.</p>
<p><code>{ text}</code>Ì CLR2022 Reviewer Guideline ({reviewer_guideline} in the system prompt):</p>
<h2>Reviewer Guidelines 1. Read the paper: It's important to carefully read through the entire paper, and to look up any related work and citations that will help you comprehensively evaluate it.Be sure to give yourself sufficient time for this step.</h2>
<p>While reading, consider the following:</p>
<p>-Objective of the work: What is the goal of the paper?Is it to better address a known application or problem, draw attention to a new application or problem, or to introduce and/or explain a new theoretical finding?A combination of these?Different objectives will require different considerations as to potential value and impact.</p>
<p>-Strong points: is the submission clear, technically correct, experimentally rigorous, reproducible, does it present novel findings (e.g.theoretically, algorithmically, etc.)? -Weak points: is it weak in any of the aspects listed in b.?</p>
<p>-Be mindful of potential biases and try to be open-minded about the value and interest a paper can hold for the entire ICLR community, even it may not be very interesting for you.</p>
<ol>
<li>Answer three key questions for yourself, to make a recommendation to Accept or Reject:</li>
</ol>
<p>-What is the specific question and/or problem tackled by the paper?-Is the approach well motivated, including being well-placed in the literature?-Does the paper support the claims?This includes determining if results, whether theoretical or empirical, are correct and if they are scientifically rigorous.</p>
<ol>
<li>Write your initial review, organizing it as follows:</li>
</ol>
<p>-Summarize what the paper claims to contribute.Be positive and generous.</p>
<p>-List strong and weak points of the paper.Be as comprehensive as possible.</p>
<p>-Clearly state your recommendation (accept or reject) with one or two key reasons for this choice.</p>
<p>-Provide supporting arguments for your recommendation.</p>
<p>-Ask questions you would like answered by the authors to help you clarify your understanding of the paper and provide the additional evidence you need to be confident in your assessment.</p>
<p>-Provide additional feedback with the aim to improve the paper.Make it clear that these points are here to help, and not necessarily part of your decision assessment.</p>
<p>General points to consider:</p>
<p>-Be polite in your review.Ask yourself whether you'd be happy to receive a review like the one you wrote.</p>
<p>-Be precise and concrete.For example, include references to back up any claims, especially claims about novelty and prior work -Provide constructive feedback.</p>
<p>-It's also fine to explicitly state where you are uncertain and what you don't quite understand.The authors may be able to resolve this in their response.</p>
<p>-Don't reject a paper just because you don't find it "interesting".This should not be a criterion at all for accepting/rejecting a paper.The research community is so big that somebody will find some value in the paper (maybe even a few years down the road), even if you don't see it right now.</p>
<p>ICLR2022 Review Template ({reviewer_template} in the system prompt):</p>
<h2>Response template (JSON format)</h2>
<p>Provide the review in valid JSON format with the following fields.Ensure all fields are completed as described below.The response must be a valid JSON object.</p>
<p>-"summary_of_the_paper": Briefly summarize the paper and its contributions.This is not the place to critique the paper; the authors should generally agree with a well-written summary.You may use paragraphs and bulleted lists for formatting, but ensure that the content remains a single, continuous text block.Do not use nested JSON or include additional fields.</p>
<p>-"main_review": "Provide review comments as a single text field (a string).Consider including assessment on the following dimensions: a comprehensive list of strong and weak points of the paper, your recommendation, supporting arguments for your recommendation, questions to clarify your understanding of the paper or request additional evidence, and additional feedback with the aim to improve the paper.You may use paragraphs and bulleted lists for formatting, but ensure that the content remains a single, continuous text block.Do not use nested JSON or include additional fields."</p>
<p>-"summary_of_the_review": Concise summary of 'main_review'.You may use paragraphs and bulleted lists for formatting, but ensure that the content remains a single, continuous text block.Do not use nested JSON or include additional fields.</p>
<p>-"correctness": A numerical rating on the following scale to indicate that the claims and methods are correct.The value should be between 1 and 4, where: -= The main claims of the paper are incorrect or not at all supported by theory or empirical results.</p>
<p>-2 = Several of the paper's claims are incorrect or not well-supported.</p>
<p>-3 = Some of the paper's claims have minor issues.A few statements are not well-supported, or require small changes to be made correct.</p>
<p>-4 = All of the claims and statements are well-supported and correct.</p>
<p>-"technical_novelty_and_significance": A numerical rating on the following scale to indicate technical novelty and significance.The value should be between 1 and 4, where:</p>
<p>-1 = The contributions are neither significant nor novel.</p>
<p>-2 = The contributions are only marginally significant or novel.</p>
<p>-3 = The contributions are significant and somewhat new.Aspects of the contributions exist in prior work.</p>
<p>-4 = The contributions are significant and do not exist in prior works.</p>
<p>-"empirical_novelty_and_significance": A numerical rating on the following scale to indicate empirical novelty and significance.The value should be between 1 and 4, or -999 if not applicable, where: -1 = The contributions are neither significant nor novel.</p>
<p>-2 = The contributions are only marginally significant or novel.</p>
<p>-3 = The contributions are significant and somewhat new.Aspects of the contributions exist in prior work.</p>
<p>-4 = The contributions are significant and do not exist in prior works.</p>
<p>--999 = Not applicable.</p>
<p>-"flag_for_ethics_review": A boolean value (<code>true</code>or <code>false</code>) indicating whether there are ethical concerns in the work.</p>
<p>-"recommendation": A string indicating the final decision, which must strictly be one of the following options: 'strong reject', 'reject, not good enough', 'marginally below the acceptance threshold', 'marginally above the acceptance threshold', 'accept, good paper', or 'strong accept, should be highlighted at the conference'.</p>
<p>-"confidence": A nuemrical values to indicate how confident you are in your evaluation.The value should be between 1 and 5, where: -1 = You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers.</p>
<p>-2 = You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work.Math/other details were not carefully checked.</p>
<p>-3 = You are fairly confident in your assessment.It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.Math/other details were not carefully checked.</p>
<p>-4 = You are confident in your assessment, but not absolutely certain.It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.</p>
<p>-5 = You are absolutely certain about your assessment.You are very familiar with the related work and checked the math/other details carefully.</p>
<p>E.2 Anchor Review Generation Prompt</p>
<p>System prompt:</p>
<p>You are an AI research scientist tasked with reviewing paper submissions for a top AI research conference.Carefully read the provided paper, then write a detailed review following a common AI conference review format (e.g., including summary, strengths and weakness, limitations, questions, suggestions for improvement).Make sure to include recommendation for the paper, either 'Accept' or 'Reject'.Your review should be fair and objective.</p>
<p>User prompt:</p>
<p>Here is the paper you are asked to review: <code>{ text}</code>È .3Editing Prompts</p>
<p>Minimal Editing:</p>
<p>Please proofread my review for typos and errors without altering the content.Keep the original wordings as much as you can, except for typo or grammatical error.</p>
<p>Moderate Editing:</p>
<p>Please polish my review to improve sentence structure and readability while keeping the original intent clear.</p>
<p>Extensive Editing:</p>
<p>Please rewrite my review into a polished, professional piece that effectively communicates its main points.</p>
<p>Maximum Editing:</p>
<p>Please transform my review into a high quality piece, using professional language and a polished tone.Please also extend my review with additional details from the oringial paper.</p>
<p>E.4 Archetype Prompts</p>
<p>"Balanced":</p>
<p>You provide fair, balanced, thorough, and constructive feedback, objectively highlighting both the strengths and weaknesses of the paper.You maintain a high standard for research in your decision-making process.However, even if your decision is to reject, you offer helpful suggestions for improvement.}"Conservative":</p>
<p>You generally prefer established methods and are skeptical of unproven (that is, new or unconventional) approaches.While you maintain high standards and rigor, you are critical of papers presenting new ideas without extensive evidence and thorough validation against established baselines.You place significant emphasis on methodological soundness and are cautious about endorsing innovations that haven't been rigorously tested.}"Innovative":</p>
<p>You highly value novelty and bold approaches, often prioritizing novel ideas over methodological perfection.While you maintain high standards, you are willing to overlook minor flaws or incomplete validations and may accept the paper, if the paper introduces a significant new concept or direction.Conversely, you tend to be less enthusiastic about papers that, despite thorough methodology and analysis, offer only incremental improvements, and may recommend rejection for such submissions.}"Nitpicky":</p>
<p>You are a perfectionist who meticulously examines every aspect of the paper, including minor methodological details, technical nuances, and formatting inconsistencies.Even if a paper presents novel or significant contributions, you may still recommend rejection if you identify a substantial number of minor flaws.Your stringent attention to detail can sometimes overshadow the broader significance of the work in your decision-making process.}</p>
<p>F Artifact Use Consistent With Intended Use</p>
<p>In our work, we ensured that the external resources we utilized were applied in a manner that aligns with their intended purposes.We used several LLMs (including GPT-4o, Gemini, Claude, Qwen, and Llama) as well as an open-source package, IMGTB, with a focus on advancing research in a noncommercial, open-source context.The artifacts from our work will be non-commercial, for-research, and open-sourced.</p>
<p>G Ethics Statement</p>
<p>Our work adheres to ethical AI principles.Peer review plays a critical role in advancing scientific discovery; however, the misuse of AI tools by reviewers to generate reviews without proper diligence can compromise the integrity of the review process.Furthermore, consistent with previous studies, we have observed that AI-generated reviews tend to be overly generic, often failing to provide actionable feedback for authors.Additionally, AI reviewers generally assign higher scores compared to human reviewers, raising concerns that AI-assisted reviews could contribute to the acceptance of work that may not meet established human evaluation standards.By developing methods to detect AI-generated reviews, our work seeks to mitigate the misuse of AI tools in peer review and promote a more rigorous and fair scientific review process.</p>
<p>H Use of AI Tool</p>
<p>GitHub Copilot and ChatGPT were used to aid in coding for analysis and in editing text for clarity.</p>
<p>Figure 1 :
1
Figure 1: Left panel: our data construction pipeline.Right panel: our context-aware detection method (Anchor) specifically designed for AI-generated review detection evaluated in Section 5.1.</p>
<p>Figure 2 :
2
Figure 2: ROC plots computed from the combined GPT-4o, Gemini, and Claude review calibration dataset, showing results for ICLR (left) and NeurIPS (right); AUC values are shown in parentheses.</p>
<p>ICLR2017.calibration.gpt-4o.csv|--... |--ICLR2024.calibration.gpt-4o.csv|--NeurIPS2016.calibration.gpt-4o.csv|--... |--NeurIPS2024.calibration.gpt-4o.csv|ICLR2018.extended.gpt-4o.csv|--... |--ICLR2024.extended.gpt-4o.csv|--NeurIPS2016.extended.gpt-4o.csv|--... |--NeurIPS2024.extended.gpt-4o.csv|--llama |--... |--test |--gpt4o |--ICLR2017.test.gpt-4o.csv|--... |--ICLR2024.test.gpt-4o.csv|--NeurIPS2016.test.gpt-4o.csv|--... |--NeurIPS2024.test.gpt-4o.csv|</p>
<p>Figure</p>
<p>FigureS2: t-SNE visualization of sentence embeddings from AI-generated reviews in the ICLR2021 test set.Blue points represent reviews generated using the main score-aligned prompt, and orange points represent those from the alternative archetype-based prompt.The substantial overlap between the two distributions suggests that prompt variation does not cause major shifts in model outputs.Embeddings were computed using OpenAI's text-embedding-3-small model; t-SNE was performed with 2 output dimensions and a perplexity of 30.</p>
<p>Figure S3 :
S3
Figure S3: Difference between AI and human scores.For each matched review (aligned by paper ID and recommendation), score differences were computed and displayed as histograms.Scores range from 1 to 4 for all metrics except Confidence, which ranges from 1 to 5. Statistical significance was assessed using a two-sided Wilcoxon signed-rank test, with p-values shown in the legend.This figure includes only NeurIPS2022-2024 and ICLR2024, because they are the onyl conferences that required reviewers to submit these scores in their review templates.</p>
<p>Table 1 :
1
Dataset statistics.Each subset in the dataset is balanced, containing an equal number of human-written and AI-generated peer reviews.The calibration and test sets include reviews generated by five different large language models (LLMs), whereas the extended set includes reviews generated by GPT-4o and Llama 3.1.
Calibration setTest setExtended setICLR NeurIPSICLR NeurIPSICLR NeurIPSGPT-4o 7,7107,484 27,34230,212 121,27891,994Gemini 1.5 Pro 7,7047,476 27,27830,146--Claude Sonnet 3.5 v2 7,6727,484 27,34030,208--Llama 3.1 70b 7,7087,472 27,28430,086 121,13091,706Qwen 2.5 72b 7,6627,452 27,19029,966--Total75,824287,052426,108Grand total788,984</p>
<p>Table 2
2: Actual FPR and TPR calculated from thewithheld test dataset at varying detection thresh-olds, which are calibrated using ICLR reviewsfrom our calibration set at different target FPRs.Highest TPRs are in bold. The results are wellseparated; see Table S5 for uncertainties estimatedvia bootstrapping.Target FPR:0.1%0.5%1%FPR TPR FPR TPR FPR TPRGPT-4o ReviewsAnchor Binoculars MAGE-D s5 MFD GLTR DetectGPT0.1 0.2 0.1 0.1 0.2 0.1 0.163.5 17.1 2.3 0.1 0.1 0.1 0.10.5 0.6 0.6 0.9 0.8 0.4 0.683.7 33.6 8.8 7.2 6.0 1.9 1.11.0 1.0 1.3 1.7 1.6 1.1 1.288.8 45.2 14.7 17.5 15.6 5.7 2.3Anchor0.259.70.880.31.386.5Gemini ReviewsBinoculars s5 MFD FastDetectGPT 0.1 0.2 0.0 0.1 GLTR 0.2 DetectGPT 0.1 MAGE-D 0.161.5 0.2 0.5 1.1 0.5 0.4 0.40.6 0.5 0.4 0.5 0.8 0.5 0.678.0 9.6 8.9 5.8 5.2 3.5 3.31.0 1.1 1.1 1.1 1.8 1.2 1.385.5 19.4 18.8 10.3 12.4 7.0 7.0Loglikelihood0.00.00.30.10.51.0Claude ReviewsAnchor Binoculars s5 MFD DetectGPT GLTR0.1 0.2 0.0 0.0 0.1 0.059.6 43.5 0.1 0.2 0.5 0.00.5 0.6 0.2 0.1 0.6 0.275.8 65.8 7.6 6.8 5.3 0.51.0 1.0 0.5 0.4 1.2 0.681.8 77.0 17.5 16.5 11.1 1.8</p>
<p>Table 2
2
provides these results separately for the detection of GPT-4o, Gemini, and Claude reviews (Llama and Qwen results are provided in Appendix C.3).Other baseline methods that failed achieve a TPR of at least 1% at a target FPR of 1% (i.e., TPR &lt; 1% at target FPR = 1%) are omitted.</p>
<p>Table 3 :
3
NDCG and the proportion of reviews flagged as AI-written for different levels of editing.Flagging rates are shown for two different thresholds (0.1% and 1% target FPRs), adapted from those used in Table2.
MethodNDCGThreshold: 0.1%FPR)Threshold: 1%FPR)Minimum Moderate Extensive Maximum Minimum Moderate Extensive MaximumAnchor0.900.40.71.960.82.64.99.782.3Binoculars0.860.61.42.59.22.75.39.426.4</p>
<p>Table S2 :
S2
Entire set sample size, including both human and AI reviews.They are exactly balanced.
Conferencegpt4o llamaICLR201729262918ICLR201854605434ICLR201994149378ICLR202015426 15366ICLR202118786 18768ICLR202220042 20026ICLR202328562 28560ICLR202455714 55672NeurIPS201662966284NeurIPS201738483774NeurIPS201859905938NeurIPS201984448398NeurIPS2021 21170 21164NeurIPS2022 20472 20408NeurIPS2023 30264 30194NeurIPS2024 33206 33104</p>
<p>Table S3 :
S3
Test set sample size, including both human and AI reviews.They are exactly balanced.
Conferencegemini claude qwen gpt4o llamaICLR201729242926 2918 2926 2918ICLR201830003004 2988 3004 2992ICLR201930023010 3000 3010 2998ICLR202030163022 3000 3022 3010ICLR202138403842 3830 3842 3838ICLR202238963900 3838 3900 3898ICLR202338163816 3816 3816 3814ICLR202437843820 3800 3822 3816NeurIPS201655225534 5534 5536 5526NeurIPS201728542858 2850 2858 2812NeurIPS201830003006 2916 3006 2982NeurIPS201929302938 2922 2940 2928NeurIPS202138843884 3884 3884 3884NeurIPS202236063622 3598 3622 3610NeurIPS202344364440 4382 4440 4432NeurIPS202439143926 3880 3926 3912Table S4: Calibration set sample size, including both human and AI reviews. They are exactlybalanced.Conferencegemini claude qwen gpt4o llamaICLR202138263828 3802 3828 3828ICLR202238783844 3860 3882 3880NeurIPS202138283830 3818 3830 3828NeurIPS202236483654 3634 3654 3644</p>
<p>Table S5 :
S5
Actual true positive rate (TPR) and false positive rate (FPR), ± their standard deviation, computed over 100 bootstrap resamples corresponding to the main results in Table2.
Target FPR:0.1%0.5%1%FPRTPRFPRTPRFPRTPRGPT-4o ReviewsAnchor Binoculars MAGE-D s5 MFD GLTR DetectGPT0.1 ± 0.02 63.5 ± 0.34 0.5 ± 0.03 83.7 ± 0.33 1.0 ± 0.04 88.8 ± 0.31 0.2 ± 0.03 17.1 ± 0.25 0.6 ± 0.05 33.6 ± 0.31 1.0 ± 0.06 45.2 ± 0.34 0.1 ± 0.03 2.3 ± 0.11 0.6 ± 0.05 8.8 ± 0.21 1.3 ± 0.07 14.7 ± 0.24 0.1 ± 0.03 0.1 ± 0.02 0.9 ± 0.07 7.2 ± 0.20 1.7 ± 0.09 17.5 ± 0.29 0.2 ± 0.03 0.1 ± 0.02 0.8 ± 0.07 6.0 ± 0.15 1.6 ± 0.09 15.6 ± 0.24 0.1 ± 0.02 0.1 ± 0.02 0.4 ± 0.05 1.9 ± 0.08 1.1 ± 0.08 5.7 ± 0.14 0.1 ± 0.02 0.1 ± 0.02 0.6 ± 0.05 1.1 ± 0.07 1.2 ± 0.08 2.3 ± 0.10Anchor0.2 ± 0.01 59.7 ± 0.32 0.8 ± 0.03 80.3± 0.33 1.3 ± 0.04 86.5 ± 0.31Gemini ReviewsBinoculars s5 MFD FastDetectGPT 0.1 ± 0.02 0.2 ± 0.03 61.5 ± 0.32 0.6 ± 0.05 78.0 ± 0.28 1.0 ± 0.07 85.5 ± 0.23 0.0 ± 0.01 0.2 ± 0.03 0.5 ± 0.04 9.6 ± 0.21 1.1 ± 0.08 19.4 ± 0.27 0.1 ± 0.02 0.5 ± 0.04 0.4 ± 0.05 8.9 ± 0.17 1.1 ± 0.07 18.8 ± 0.25 1.1 ± 0.08 0.5 ± 0.04 5.8 ± 0.18 1.1 ± 0.06 10.3 ± 0.21 GLTR 0.2 ± 0.03 0.5 ± 0.04 0.8 ± 0.06 5.2 ± 0.13 1.8 ± 0.09 12.4 ± 0.23 DetectGPT 0.1 ± 0.02 0.4 ± 0.05 0.5 ± 0.05 3.5 ± 0.12 1.2 ± 0.08 7.0 ± 0.18 MAGE-D 0.1 ± 0.03 0.4 ± 0.04 0.6 ± 0.06 3.3 ± 0.12 1.3 ± 0.08 7.0 ± 0.17Loglikelihood0.0 ± 0.010.0 ± 0.00 0.3 ± 0.040.1 ± 0.02 0.5 ± 0.051.0 ± 0.07Claude ReviewsAnchor Binoculars s5 MFD DetectGPT GLTR0.1 ± 0.02 59.6 ± 0.34 0.5 ± 0.04 75.8 ± 0.34 1.0 ± 0.05 81.8 ± 0.32 0.2 ± 0.04 43.5 ± 0.33 0.6 ± 0.05 65.8 ± 0.33 1.0 ± 0.07 77.0 ± 0.29 0.0 ± 0.01 0.1 ± 0.02 0.2 ± 0.03 7.6 ± 0.18 0.5 ± 0.05 17.5 ± 0.23 0.0 ± 0.01 0.2 ± 0.03 0.1 ± 0.03 6.8 ± 0.18 0.4 ± 0.05 16.5 ± 0.24 0.1 ± 0.02 0.5 ± 0.05 0.6 ± 0.05 5.3 ± 0.16 1.2 ± 0.06 11.1 ± 0.22 0.0 ± 0.01 0.0 ± 0.01 0.2 ± 0.03 0.5 ± 0.05 0.6 ± 0.05 1.8 ± 0.10</p>
<p>Table S6 :
S6
Actual FPR and TPR calculated from the withheld test dataset at varying detection thresholds, which are calibrated using ICLR and NeurIPS reviews from our calibration set at different target FPRs.Best TPRs are in bold.
Target FPR:0.1%0.5%1%FPR TPR FPR TPR FPR TPRAnchor0.161.40.380.10.887.4GPT-4o ReviewsBinoculars MAGE-D s5 MFD GLTR DetectGPT0.3 0.1 0.3 0.3 0.1 0.118.8 2.3 0.7 0.9 0.1 0.20.7 0.7 1.0 0.9 0.5 0.637.5 9.6 8.0 7.8 2.4 1.11.2 1.3 1.6 1.6 1.2 1.049.3 14.5 16.3 14.9 5.9 2.1Loglikelihood0.10.00.30.20.61.0Binoculars0.363.80.780.91.187.6Anchor0.257.20.575.51.184.2Gemini ReviewsMFD s5 GLTR FastDetectGPT MAGE-D DetectGPT0.1 0.1 0.2 0.1 0.1 0.11.9 1.4 0.6 1.1 0.4 0.50.6 0.5 1.0 0.4 0.7 0.511.0 10.5 6.3 4.9 3.8 3.21.0 1.0 1.8 0.9 1.3 1.118.1 18.3 12.9 8.9 6.9 6.3Loglikelihood0.10.00.30.20.61.6NTNU-D11.50.021.80.026.30.1Claude ReviewsAnchor Binoculars MFD s5 DetectGPT GLTR0.1 0.3 0.0 0.0 0.1 0.053.8 46.4 1.0 0.7 0.6 0.00.3 0.7 0.2 0.2 0.5 0.372.6 70.2 8.7 8.4 4.9 0.70.8 1.1 0.4 0.4 1.0 0.680.0 80.0 15.9 16.4 10.1 1.9</p>
<p>Table S9 :
S9
Actual FPR and TPR calculated from the NeurIPS reviews at varying detection thresholds, which are calibrated using the ICLR calibration dataset at different target FPRs.
Target FPR:0.1%0.5%1%FPRTPRFPRTPRFPRTPRGPTBinoculars GLTR0.3% 26.1% 0.6% 45.3% 0.3% 1.3% 5.1% 31.9% 10.6% 52.6% 0.9% 54.1%LlamaBinoculars GLTR0.3% 98.9% 0.6% 99.3% 0.3% 76.3% 0.9% 91.0%0.9% 1.4%99.4% 93.9%</p>
<p>Table S11 :
S11
Examples of differences identified in human analysis of human and AI-written peer reviews
CategoryHuman review exampleGPT-4o review examplesReferences to spe-"Table 2 confirms that MDR outper-"The paper extensively evaluates oncific details in theforms Graph Rec Retriever (Asai etmultiple datasets and situates the con-paperal.). This result shows the feasibil-tributions clearly within existing liter-ity of a more accurate multi-hop QAature, substantiating claims with thor-model without external knowledgeough quantitative analysis."such as Wikipedia hyperlinks."Specificrefer-"My only serious concern is the de-ences to priorgree of novelty with respect towork</p>
<p>Table S12 :
S12
Similarity score of edited reviews and original human-written reviews using different prompts.
Minimum Moderate Extensive Maximum0.98410.92610.86160.6799
NeurIPS
reviews are not publicly available
ChatGPT was released on November
, 2022.
papers submitted to ICLR 2021. Specifically, we read an equal number of human and GPT-4o written reviews for each paper and noted differences in the content between them. A distinguishing characteristic of the analyzed human reviews was that they usually contained details or references to specific sections, tables, figures, or results in the paper. In contrast, peer reviews authored by GPT-4o lacked such specific details, instead focusing on higher-level comments. Another key difference identified in our qualitative analysis was the lack of any specific references to prior or related work in peer reviews generated by GPT-4o. Human-authored peer reviews often point out missing references, challenge the novelty of the paper by referencing related work, or suggest specific baselines with references that should be included in the study. In contrast, none of the analyzed GPT-4o reviews contained such specific references to related work. Finally, we found that the vast majority of GPT-4o reviews mentioned highly similar generic criticisms which were not found in human-authored reviews for the same paper. Examples of these issues are provided in TableS11in Appendix D.3.
https://github.com/kinit-sk/IMGTB
https://icml.cc/Conferences/{2016..2024} and https://neurips.cc/Conferences/{2016..2024}</p>
<p>On the evolution of ai and machine learning: Towards measuring and understanding impact, influence, and leadership at premier ai conferences. Henrique Rafael B Audibert, Pedro Lemos, Anderson R Avelar, Luís C Tavares, Lamb, arXiv:2205.131312022arXiv preprint</p>
<p>What can natural language processing. Ilia Kuznetsov, Osama Mohammed Afzal, Koen Dercksen, Nils Dycke, Alexander Goldberg, Tom Hope, Dirk Hovy, Jonathan K Kummerfeld, Anne Lauscher, Kevin Leyton-Brown, arXiv:2405.065632024arXiv preprint</p>
<p>Artificial intelligence to support publishing and peer review: A summary and review. Kayvan Kousha, Mike Thelwall, Learned Publishing. 3712024</p>
<p>Large language models for automated scholarly paper review: A survey. Zhenzhen Zhuang, Jiandong Chen, Hongfeng Xu, Yuwen Jiang, Jialiang Lin, arXiv:2501.103262025arXiv preprint</p>
<p>Fighting reviewer fatigue or amplifying bias? considerations and recommendations for use of chatgpt and other large language models in scholarly peer review. Mohammad Hosseini, Serge Pjm Horbach, Research integrity and peer review. 8142023</p>
<p>Giuseppe Russo Latona, Manoel Horta Ribeiro, Tim R Davidson, Veniamin Veselovsky, Robert West, arXiv:2405.02150The ai review lottery: Widespread ai-assisted peer reviews boost paper scores and acceptance rates. 2024arXiv preprint</p>
<p>Ai-powered peer review needs human supervision. Mohamed L Seghier, Journal of Information, Communication and Ethics in Society. 2024</p>
<p>Is LLM a reliable reviewer? a comprehensive evaluation of LLM on automatic paper reviewing tasks. Ruiyang Zhou, Lu Chen, Kai Yu, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024). Nicoletta Calzolari, Min-Yen Kan, Veronique Hoste, Alessandro Lenci, Sakriani Sakti, Nianwen Xue, the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)Torino, ItaliaELRA and ICCLMay 2024</p>
<p>Monitoring ai-modified content at scale: A case study on the impact of chatgpt on ai conference peer reviews. Weixin Liang, Zachary Izzo, Yaohui Zhang, Haley Lepp, Hancheng Cao, Xuandong Zhao, Lingjiao Chen, Haotian Ye, Sheng Liu, Zhi Huang, arXiv:2403.071832024arXiv preprint</p>
<p>Human-in-the-loop ai reviewing: Feasibility, opportunities, and risks. Iddo Drori, Dov Te, ' Eni, Journal of the Association for Information Systems. 2512024</p>
<p>Are we there yet? revealing the risks of utilizing large language models in scholarly peer review. Rui Ye, Xianghe Pang, Jingyi Chai, Jiaao Chen, Zhenfei Yin, Zhen Xiang, Xiaowen Dong, Jing Shao, Siheng Chen, arXiv:2412.017082024arXiv preprint</p>
<p>Aspect-guided multi-level perturbation analysis of large language models in automated peer review. Jiatao Li, Yanheng Li, Xinyu Hu, Mingqi Gao, Xiaojun Wan, arXiv:2502.125102025arXiv preprint</p>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023arXiv preprint</p>
<p>. Anthropic, 2023</p>
<p>Gemini: a family of highly capable multimodal models. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, arXiv:2312.118052023arXiv preprint</p>
<p>. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, arXiv:2309.166092023Qwen technical report. arXiv preprint</p>
<p>The llama 3 herd of models. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, arXiv:2407.217832024arXiv preprint</p>
<p>Raid: A shared benchmark for robust evaluation of machine-generated text detectors. Liam Dugan, Alyssa Hwang, Filip Trhlik, Josh Magnus Ludan, Andrew Zhu, Hainiu Xu, Daphne Ippolito, Chris Callison-Burch, arXiv:2405.079402024arXiv preprint</p>
<p>AI-Generated Text Detection and Classification Based on BERT Deep Learning Algorithm. arXiv. Hao Wang, Jianwei Li, Zhengyu Li, 10.48550/arxiv.2405.164222024</p>
<p>How close is chatgpt to human experts? comparison corpus, evaluation, and detection. Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding, Jianwei Yue, Yupeng Wu, arXiv:2301.075972023arXiv preprint</p>
<p>Gpt-generated text detection: Benchmark dataset and tensor-based detection method. Zubair Qazi, William Shiao, Evangelos E Papalexakis, Companion Proceedings of the ACM on Web Conference 2024. 2024</p>
<p>Ekaterina Artemova, Jason Lucas, Saranya Venkatraman, Jooyoung Lee, Sergei Tilga, Adaku Uchendu, Vladislav Mikhailov, arXiv:2411.04032Beemo: Benchmark of expert-edited machine-generated outputs. 2024arXiv preprint</p>
<p>Real or fake? learning to discriminate machine from human generated text. Anton Bakhtin, Sam Gross, Myle Ott, Yuntian Deng, Marc ' , Aurelio Ranzato, Arthur Szlam, arXiv:1906.033512019arXiv preprint</p>
<p>Automatic detection of machine generated text: A critical survey. Ganesh Jawahar, Muhammad Abdul-Mageed, Vs Laks Lakshmanan, Proceedings of the 28th International Conference on Computational Linguistics. the 28th International Conference on Computational Linguistics2020</p>
<p>Tweepfake: About detecting deepfake tweets. Tiziano Fagni, Fabrizio Falchi, Margherita Gambini, Antonio Martella, Maurizio Tesconi, Plos one. 165e02514152021</p>
<p>Detectgpt: Zero-shot machine-generated text detection using probability curvature. Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D Manning, Chelsea Finn, International Conference on Machine Learning. PMLR2023</p>
<p>Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford, Gretchen Krueger, Jong Wook Kim, Sarah Kreps, arXiv:1908.09203Release strategies and the social impacts of language models. 2019arXiv preprint</p>
<p>. Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis, Zettlemoyer, Stoyanov, arXiv:1907.116922019. 1907arXiv preprintRoberta: A robustly optimized bert pretraining approach. arxiv [preprint</p>
<p>Defending against neural fake news. Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, Yejin Choi, Advances in neural information processing systems. 201932</p>
<p>Authorship attribution for neural text generation. Adaku Uchendu, Thai Le, Kai Shu, Dongwon Lee, Proceedings of the 2020 conference on empirical methods in natural language processing (EMNLP). the 2020 conference on empirical methods in natural language processing (EMNLP)2020</p>
<p>Gltr: Statistical detection and visualization of generated text. Sebastian Gehrmann, Hendrik Strobelt, Alexander M Rush, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations. the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations2019</p>
<p>Automatic detection of generated text is easiest when humans are fooled. Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, Douglas Eck, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics2020</p>
<p>. Xinlei He, Xinyue Shen, Zeyuan Chen, Michael Backes, Yang Zhang, 10.48550/arxiv.2303.14822MGTBench: Benchmarking Machine-Generated Text Detection. arXiv. 2023</p>
<p>IMGTB: A Framework for Machine-Generated Text Detection Benchmarking. arXiv. Michal Spiegel, Dominik Macko, 10.48550/arxiv.2311.125742023</p>
<p>. Openai, Chatgpt, 2023Large Language Model</p>
<p>Detectllm: Leveraging log rank information for zero-shot detection of machine-generated text. Jinyan Su, Terry Yue Zhuo, Di Wang, Preslav Nakov, The 2023 Conference on Empirical Methods in Natural Language Processing. </p>
<p>Can large language models provide useful feedback on research papers? a large-scale empirical analysis. Weixin Liang, Yuhui Zhang, Hancheng Cao, Binglu Wang, Daisy Yi Ding, Xinyu Yang, Kailas Vodrahalli, Siyu He, Scott Daniel, Yian Smith, Yin, NEJM AI. 18AIoa2400196, 2024</p>
<p>Peer Review as A Multi-Turn and Long-Context Dialogue with Role-Based Interactions. Cheng Tan, Dongxin Lyu, Siyuan Li, Zhangyang Gao, Jingxuan Wei, Siqi Ma, Zicheng Liu, Stan Z Li, 10.48550/arxiv.2406.056882024</p>
<p>Keith Tyser, Jason Lee, Avi Shporer, Madeleine Udell, Dov Te'eni, Iddo Drori, Openreviewer: Mitigating challenges in llm reviewing. </p>
<p>Distinguishing fact from fiction: A benchmark dataset for identifying machinegenerated scientific papers in the llm era. Edoardo Mosca, Mohamed Hesham, Ibrahim Abdalla, Paolo Basso, Margherita Musumeci, Georg Groh, Proceedings of the 3rd Workshop on Trustworthy Natural Language Processing. the 3rd Workshop on Trustworthy Natural Language ProcessingTrustNLP 2023. 2023</p>
<p>Ai-driven review systems: evaluating llms in scalable and bias-aware academic reviews. Keith Tyser, Ben Segev, Gaston Longhitano, Xin-Yu Zhang, Zachary Meeks, Jason Lee, Uday Garg, Nicholas Belsten, Avi Shporer, Madeleine Udell, arXiv:2408.103652024arXiv preprint</p>
<p>Marg: Multi-agent review generation for scientific papers. D' Mike, Tom Arcy, Larry Hope, Doug Birnbaum, Downey, arXiv:2401.042592024arXiv preprint</p>
<p>Ryan Liu, Nihar B Shah, arXiv:2306.00622Reviewergpt? an exploratory study on using large language models for paper reviewing. 2023arXiv preprint</p>
<p>Yiqiao Jin, Qinlin Zhao, Yiyang Wang, Hao Chen, Kaijie Zhu, Yijia Xiao, Jindong Wang, arXiv:2406.12708Agentreview: Exploring peer review dynamics with llm agents. 2024arXiv preprint</p>
<p>Prompting llms to compose meta-review drafts from peer-review narratives of scholarly manuscripts. Shubhra Kanti, Karmaker Santu, Sanjeev Kumar Sinha, Naman Bansal, Alex Knipper, Souvika Sarkar, John Salvador, Yash Mahajan, Sri Guttikonda, Mousumi Akter, Matthew Freestone, arXiv:2402.155892024arXiv preprint</p>
<p>Peerarg: Argumentative peer review with llms. Purin Sukpanichnant, Anna Rapberger, Francesca Toni, arXiv:2409.168132024arXiv preprint</p>
<p>. Openreview, </p>
<p>Can we automate scientific reviewing. Weizhe Yuan, Pengfei Liu, Graham Neubig, Journal of Artificial Intelligence Research. 752022</p>
<p>Dna-gpt: Divergent n-gram analysis for training-free detection of gpt-generated text. Xianjun Yang, Wei Cheng, Yue Wu, Linda Petzold, William Yang, Wang , Haifeng Chen, arXiv:2305.173592023arXiv preprint</p>
<p>Paraphrasing evades detectors of ai-generated text, but retrieval is an effective defense. Kalpesh Krishna, Yixiao Song, Marzena Karpinska, John Wieting, Mohit Iyyer, Advances in Neural Information Processing Systems. 202436</p>
<p>An examination of ai-generated text detectors across multiple domains and models. Brian Tufts, Xuandong Zhao, Lei Li, Neurips Safe Generative AI Workshop. 2024</p>
<p>Beyond the chat: Executable and verifiable text-editing with llms. Philippe Laban, Jesse Vig, Marti Hearst, Caiming Xiong, Chien-Sheng Wu, Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology. the 37th Annual ACM Symposium on User Interface Software and Technology2024</p>
<p>medit: Multilingual text editing via instruction tuning. Dimitris Vipul Raheja, Vivek Alikaniotis, Bashar Kulkarni, Dhruv Alhafni, Kumar, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies20241</p>
<p>Cumulated gain-based evaluation of ir techniques. Kalervo Järvelin, Jaana Kekäläinen, ACM Transactions on Information Systems (TOIS). 2042002</p>
<p>Spotting llms with binoculars: Zero-shot detection of machine-generated text. Abhimanyu Hans, Avi Schwarzschild, Valeriia Cherepanova, Hamid Kazemi, Aniruddha Saha, Micah Goldblum, Jonas Geiping, Tom Goldstein, arXiv:2401.120702024arXiv preprint</p>
<p>Fast-detectgpt: Efficient zero-shot detection of machine-generated text via conditional probability curvature. Guangsheng Bao, Yanbin Zhao, Zhiyang Teng, Linyi Yang, Yue Zhang, arXiv:2310.051302023arXiv preprint</p>
<p>Mfd: Multi-feature detection of llm-generated text. Zhendong Wu, Hui Xiang, 2023</p>
<p>DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature. arXiv. Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D Manning, Chelsea Finn, 10.48550/arxiv.2301.113052023</p>
<p>Turning poachers into gamekeepers: Detecting machine-generated text in academia using large language models. Nicolai Thorer, Sivesind , Andreas Bentzen Winje, 2023NTNUB.S. thesis</p>
<p>Mage: Machine-generated text detection in the wild. Yafu Li, Qintong Li, Leyang Cui, Wei Bi, Zhilin Wang, Longyue Wang, Linyi Yang, Shuming Shi, Yue Zhang, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics20241</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8725 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8725</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8725</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-157.html">extraction-schema-157</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-267637077</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.08115v1.pdf" target="_blank">On the Self-Verification Limitations of Large Language Models on Reasoning and Planning Tasks</a></p>
                <p><strong>Paper Abstract:</strong> There has been considerable divergence of opinion on the reasoning abilities of Large Language Models (LLMs). While the initial optimism that reasoning might emerge automatically with scale has been tempered thanks to a slew of counterexamples--ranging from multiplication to simple planning--there persists a wide spread belief that LLMs can self-critique and improve their own solutions in an iterative fashion. This belief seemingly rests on the assumption that verification of correctness should be easier than generation--a rather classical argument from computational complexity--which should be irrelevant to LLMs to the extent that what they are doing is approximate retrieval. In this paper, we set out to systematically investigate the effectiveness of iterative prompting in the context of reasoning and planning. We present a principled empirical study of the performance of GPT-4 in three domains: Game of 24, Graph Coloring, and STRIPS planning. We experiment both with the model critiquing its own answers and with an external correct reasoner verifying proposed solutions. In each case, we analyze whether the content of criticisms actually affects bottom line performance, and whether we can ablate elements of the augmented system without losing performance. We observe significant performance collapse with self-critique, significant performance gains with sound external verification, but that the content of critique doesn't matter to the performance of the system. In fact, merely re-prompting with a sound verifier maintains most of the benefits of more involved setups.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8725.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8725.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (LLM+LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 used as both generator and verifier/critic (LLM+LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Iterative self-critique setup where the same LLM (GPT-4) is used to (a) generate candidate solutions and (b) verify and produce critique on those solutions, with feedback appended to the prompt history and the cycle repeated.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>On the Self-Verification Limitations of Large Language Models on Reasoning and Planning Tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-4 (proprietary state-of-the-art large language model used as provided by OpenAI; model size and exact training details not specified in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>LLM self-critique (LLM+LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate-then-reflect loop: send a generation prompt to GPT-4, convert its reply into a verification prompt, send to GPT-4 to get a binary accept/reject and free-form critique, then wrap that critique into a backprompt appended to the full interaction history and re-query the same GPT-4; roles played by GPT-4: answer guesser, binary verifier, and critique generator. Iterations varied (multi-round backprompting; examples show long sequences up to 15 iterations).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Three formal reasoning/planning tasks: Game of 24, Graph Coloring, STRIPS planning (Mystery Blocksworld)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Game of 24: arithmetic puzzle (use four numbers + operations to make 24); Graph Coloring: coloring planar graphs with n colors (no adjacent same color); STRIPS planning: classical planning instances in Mystery Blocksworld (PDDL), require executable plans reaching goal.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Performance degraded under LLM+LLM self-critique: Game of 24 accuracy = 3% (out of 100); Graph Coloring accuracy = 2%; Mystery Blocksworld accuracy = 0% (Table 1, LLM+LLM column).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Standard single-shot GPT-4 baseline (no iterative self-critique) accuracies: Game of 24 = 5%; Graph Coloring = 16%; Mystery Blocksworld = 4% (Table 1 'Standard').</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt-based iterative backprompting: full conversation history and previous failed attempts plus LLM-produced critique appended to new prompt; no external specialized module — all verification and critique are produced by GPT-4 itself.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>No quantitative evidence of improvement; instead, empirical results show consistent performance collapse as number of self-critique backprompts increases (Figure 2) — the self-verification loop often reduces accuracy compared to the single-shot baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>High false negative and false positive rates when GPT-4 acts as verifier (rejects many valid solutions and accepts some invalid ones); critiques are frequently hallucinated or wrong (non-existent edges in graph-coloring, incorrect preconditions in planning, malformed expressions in Game of 24). Iterative self-critique compounds errors (verifier rejects correct answers leading to further incorrect follow-ups). Overall, self-critique by GPT-4 worsens performance across all three domains.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Directly compared to (a) using a sound external verifier (LLM+Sound Critique) and (b) sampling with a sound verifier (no critique history). LLM+LLM performed substantially worse than systems using a sound verifier; content of GPT-4 critiques did not meaningfully help. Authors report that gains in other setups are attributable to sound verification and repeated guessing rather than LLM self-critique.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Ablations show (1) replacing the LLM verifier with a provably sound verifier (external tool) yields substantial performance gains; (2) removing critique entirely and simply re-sampling the base prompt repeatedly until the sound verifier accepts (Sampling) retains most gains; (3) varying the richness of the external verifier's feedback (binary / first-error / all-errors) produced little difference (sometimes more feedback slightly decreased performance). Using GPT-4 as verifier causes immediate collapse.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On the Self-Verification Limitations of Large Language Models on Reasoning and Planning Tasks', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8725.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8725.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM+Sound Critique</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 generator with external sound verifier (LLM+Sound Critique)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use GPT-4 to propose solutions, but evaluate and produce feedback using provably-sound, domain-specific verifiers (SymPy for Game of 24, simple Python checker for graph coloring, VAL for STRIPS planning); feedback is then provided back to GPT-4 to iterate.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (generator) + external sound verifiers (SymPy, Python check, VAL)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Generator: GPT-4; Verifiers: domain-specific programmatic validators: SymPy (symbolic simplification) for Game of 24, GrinPy/simple Python edge-checker for Graph Coloring, VAL plan validator for STRIPS planning.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>LLM+Sound Critique (external sound verifier with iterative prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>LLM produces candidate solutions; an external sound verifier checks correctness and emits feedback at one of multiple levels: binary (pass/fail), first-error (describe first detected error), or all-errors (list all errors). Feedback is appended to the prompt history and GPT-4 is re-queried; iterative cycles proceed until solution certified or timeout.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Game of 24, Graph Coloring, Mystery Blocksworld STRIPS planning</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same formal domains described above; verifiers can deterministically check correctness (evaluate expressions, validate edge constraints, validate plan executability and goal achievement).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Substantial gains vs baseline when using sound verifier: Game of 24: LLM+Sound Critique binary feedback = 36% (first-error = 38%); Graph Coloring: binary = 38%, first-error = 37%, all-errors = 34%; Mystery Blocksworld: binary = 10%, first-error = 8%, all-errors = 6% (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Standard GPT-4 baseline: Game of 24 = 5%; Graph Coloring = 16%; Mystery Blocksworld = 4% (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>External programmatic verification and feedback generation (symbolic math library, graph edge checks, plan validator) integrated into prompt loop; feedback levels experimented: binary, first-error, all-errors; prompts include history optionally.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative increases in task accuracy relative to baseline: e.g., Game of 24 increases from 5% to ~36–38%; Graph Coloring from 16% to ~34–38%; Mystery Blocksworld modest increases from 4% to 6–10% depending on feedback style (Table 1). Improvements are monotonic with number of backprompts until an asymptote when verifier is sound.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Surprisingly, the detailed content of the feedback (binary vs first-error vs all-errors) had little impact; in two domains more feedback reduced performance. Gains largely stem from having a sound verifier filter correct answers and being able to try many guesses, not from LLM using critique content. Requires availability of a sound domain-specific verifier; where such verifiers are unavailable the approach is less applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared to LLM+LLM, LLM+Sound Critique consistently outperformed it. Compared to pure sampling (re-querying without appended critique), sampling with a sound verifier attained similar gains, suggesting that the presence of a sound verifier and multiple independent guesses explain most improvements rather than rich critique content.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Ablations: (1) Rich feedback vs minimal binary feedback produced only minor differences and sometimes worse results with richer feedback; (2) Removing critique (sampling) while keeping the sound verifier preserved most gains; (3) Replacing sound verifier with GPT-4 verifier collapsed performance. Authors conclude soundness of verifier is critical; critique content is not.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On the Self-Verification Limitations of Large Language Models on Reasoning and Planning Tasks', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8725.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8725.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sampling + Sound Verifier</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Repeated sampling (re-querying base prompt) with external sound verifier (no critique appended)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An impoverished iterative scheme: repeatedly send the same base prompt to GPT-4 (no appended history or critique) and use a sound external verifier to certify proposed solutions; stop when verifier accepts or timeout.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (generator) + external sound verifiers</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Generator: GPT-4; Verifiers: same domain-specific programmatic checkers as above (SymPy, Python graph checker, VAL).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Sampling (repeated base-prompting) with sound verification</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>No critique is provided to the LLM and prompt does not grow; the LLM is asked the identical generation prompt repeatedly; after each generation, a sound verifier checks the solution; the process repeats up to a fixed sample budget (authors report k=15 and k=25 sample budgets).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Game of 24, Graph Coloring, Mystery Blocksworld STRIPS planning</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same formally verifiable tasks as above; verifier performs pass/fail checks each sample.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Sampling with sound verifier yields similar improvements to LLM+Sound Critique: reported accuracies at k=15 and k=25 samples — Game of 24: 28% (k=15) and 42% (k=25); Graph Coloring: 40% (k=15) and 44% (k=25); Mystery Blocksworld: 9% (k=15) and 14% (k=25) (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Standard single-shot GPT-4 baseline: Game of 24 = 5%; Graph Coloring = 16%; Mystery Blocksworld = 4% (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>No feedback-based reflection; mechanism is statistical/ensemble-like improvement via repeated independent sampling and external sound checking. Prompting is constant (no appended history), reducing token growth and cost per iteration.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>15</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative evidence: increasing sample budget from 15 to 25 generally increases accuracy (e.g., Game of 24 from 28% to 42%); sampling achieves most of the gains of richer critique loops while being cheaper in tokens because prompts don't grow.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires a sound external verifier; effectiveness depends on being able to sample many independent guesses (compute/latency cost). Does not leverage critique content at all; where verifier is unavailable or expensive, method may not be applicable. If verifier is itself imperfect (e.g., LLM-as-verifier), sampling does not salvage performance — the paper shows that using an LLM verifier collapses performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Authors found sampling with a sound verifier performs as well as or nearly as well as the more complex LLM+Sound Critique setups, indicating that the main benefit of iterative schemes is multiple guesses + sound verification rather than using LLM-produced critiques or appended history.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Ablations show that removing critique and keeping only sampling preserves most performance gains; increasing the number of samples increases accuracy up to asymptote. Token-cost advantage due to fixed-size prompts allows larger sample budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On the Self-Verification Limitations of Large Language Models on Reasoning and Planning Tasks', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 2)</em></li>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>Large language models cannot self-correct reasoning yet <em>(Rating: 2)</em></li>
                <li>Teaching large language models to self-debug <em>(Rating: 2)</em></li>
                <li>A closer look at the self-verification abilities of large language models in logical reasoning <em>(Rating: 2)</em></li>
                <li>Large language models are better reasoners with self-verification <em>(Rating: 1)</em></li>
                <li>Backward reasoning in large language models for verification <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8725",
    "paper_id": "paper-267637077",
    "extraction_schema_id": "extraction-schema-157",
    "extracted_data": [
        {
            "name_short": "GPT-4 (LLM+LLM)",
            "name_full": "GPT-4 used as both generator and verifier/critic (LLM+LLM)",
            "brief_description": "Iterative self-critique setup where the same LLM (GPT-4) is used to (a) generate candidate solutions and (b) verify and produce critique on those solutions, with feedback appended to the prompt history and the cycle repeated.",
            "citation_title": "On the Self-Verification Limitations of Large Language Models on Reasoning and Planning Tasks",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "OpenAI GPT-4 (proprietary state-of-the-art large language model used as provided by OpenAI; model size and exact training details not specified in paper).",
            "reflection_method_name": "LLM self-critique (LLM+LLM)",
            "reflection_method_description": "Generate-then-reflect loop: send a generation prompt to GPT-4, convert its reply into a verification prompt, send to GPT-4 to get a binary accept/reject and free-form critique, then wrap that critique into a backprompt appended to the full interaction history and re-query the same GPT-4; roles played by GPT-4: answer guesser, binary verifier, and critique generator. Iterations varied (multi-round backprompting; examples show long sequences up to 15 iterations).",
            "task_name": "Three formal reasoning/planning tasks: Game of 24, Graph Coloring, STRIPS planning (Mystery Blocksworld)",
            "task_description": "Game of 24: arithmetic puzzle (use four numbers + operations to make 24); Graph Coloring: coloring planar graphs with n colors (no adjacent same color); STRIPS planning: classical planning instances in Mystery Blocksworld (PDDL), require executable plans reaching goal.",
            "performance_with_reflection": "Performance degraded under LLM+LLM self-critique: Game of 24 accuracy = 3% (out of 100); Graph Coloring accuracy = 2%; Mystery Blocksworld accuracy = 0% (Table 1, LLM+LLM column).",
            "performance_without_reflection": "Standard single-shot GPT-4 baseline (no iterative self-critique) accuracies: Game of 24 = 5%; Graph Coloring = 16%; Mystery Blocksworld = 4% (Table 1 'Standard').",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt-based iterative backprompting: full conversation history and previous failed attempts plus LLM-produced critique appended to new prompt; no external specialized module — all verification and critique are produced by GPT-4 itself.",
            "number_of_iterations": null,
            "evidence_for_improvement": "No quantitative evidence of improvement; instead, empirical results show consistent performance collapse as number of self-critique backprompts increases (Figure 2) — the self-verification loop often reduces accuracy compared to the single-shot baseline.",
            "limitations_or_failure_cases": "High false negative and false positive rates when GPT-4 acts as verifier (rejects many valid solutions and accepts some invalid ones); critiques are frequently hallucinated or wrong (non-existent edges in graph-coloring, incorrect preconditions in planning, malformed expressions in Game of 24). Iterative self-critique compounds errors (verifier rejects correct answers leading to further incorrect follow-ups). Overall, self-critique by GPT-4 worsens performance across all three domains.",
            "comparison_to_other_methods": "Directly compared to (a) using a sound external verifier (LLM+Sound Critique) and (b) sampling with a sound verifier (no critique history). LLM+LLM performed substantially worse than systems using a sound verifier; content of GPT-4 critiques did not meaningfully help. Authors report that gains in other setups are attributable to sound verification and repeated guessing rather than LLM self-critique.",
            "ablation_study_results": "Ablations show (1) replacing the LLM verifier with a provably sound verifier (external tool) yields substantial performance gains; (2) removing critique entirely and simply re-sampling the base prompt repeatedly until the sound verifier accepts (Sampling) retains most gains; (3) varying the richness of the external verifier's feedback (binary / first-error / all-errors) produced little difference (sometimes more feedback slightly decreased performance). Using GPT-4 as verifier causes immediate collapse.",
            "uuid": "e8725.0",
            "source_info": {
                "paper_title": "On the Self-Verification Limitations of Large Language Models on Reasoning and Planning Tasks",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "LLM+Sound Critique",
            "name_full": "GPT-4 generator with external sound verifier (LLM+Sound Critique)",
            "brief_description": "Use GPT-4 to propose solutions, but evaluate and produce feedback using provably-sound, domain-specific verifiers (SymPy for Game of 24, simple Python checker for graph coloring, VAL for STRIPS planning); feedback is then provided back to GPT-4 to iterate.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 (generator) + external sound verifiers (SymPy, Python check, VAL)",
            "model_description": "Generator: GPT-4; Verifiers: domain-specific programmatic validators: SymPy (symbolic simplification) for Game of 24, GrinPy/simple Python edge-checker for Graph Coloring, VAL plan validator for STRIPS planning.",
            "reflection_method_name": "LLM+Sound Critique (external sound verifier with iterative prompting)",
            "reflection_method_description": "LLM produces candidate solutions; an external sound verifier checks correctness and emits feedback at one of multiple levels: binary (pass/fail), first-error (describe first detected error), or all-errors (list all errors). Feedback is appended to the prompt history and GPT-4 is re-queried; iterative cycles proceed until solution certified or timeout.",
            "task_name": "Game of 24, Graph Coloring, Mystery Blocksworld STRIPS planning",
            "task_description": "Same formal domains described above; verifiers can deterministically check correctness (evaluate expressions, validate edge constraints, validate plan executability and goal achievement).",
            "performance_with_reflection": "Substantial gains vs baseline when using sound verifier: Game of 24: LLM+Sound Critique binary feedback = 36% (first-error = 38%); Graph Coloring: binary = 38%, first-error = 37%, all-errors = 34%; Mystery Blocksworld: binary = 10%, first-error = 8%, all-errors = 6% (Table 1).",
            "performance_without_reflection": "Standard GPT-4 baseline: Game of 24 = 5%; Graph Coloring = 16%; Mystery Blocksworld = 4% (Table 1).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "External programmatic verification and feedback generation (symbolic math library, graph edge checks, plan validator) integrated into prompt loop; feedback levels experimented: binary, first-error, all-errors; prompts include history optionally.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Quantitative increases in task accuracy relative to baseline: e.g., Game of 24 increases from 5% to ~36–38%; Graph Coloring from 16% to ~34–38%; Mystery Blocksworld modest increases from 4% to 6–10% depending on feedback style (Table 1). Improvements are monotonic with number of backprompts until an asymptote when verifier is sound.",
            "limitations_or_failure_cases": "Surprisingly, the detailed content of the feedback (binary vs first-error vs all-errors) had little impact; in two domains more feedback reduced performance. Gains largely stem from having a sound verifier filter correct answers and being able to try many guesses, not from LLM using critique content. Requires availability of a sound domain-specific verifier; where such verifiers are unavailable the approach is less applicable.",
            "comparison_to_other_methods": "Compared to LLM+LLM, LLM+Sound Critique consistently outperformed it. Compared to pure sampling (re-querying without appended critique), sampling with a sound verifier attained similar gains, suggesting that the presence of a sound verifier and multiple independent guesses explain most improvements rather than rich critique content.",
            "ablation_study_results": "Ablations: (1) Rich feedback vs minimal binary feedback produced only minor differences and sometimes worse results with richer feedback; (2) Removing critique (sampling) while keeping the sound verifier preserved most gains; (3) Replacing sound verifier with GPT-4 verifier collapsed performance. Authors conclude soundness of verifier is critical; critique content is not.",
            "uuid": "e8725.1",
            "source_info": {
                "paper_title": "On the Self-Verification Limitations of Large Language Models on Reasoning and Planning Tasks",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Sampling + Sound Verifier",
            "name_full": "Repeated sampling (re-querying base prompt) with external sound verifier (no critique appended)",
            "brief_description": "An impoverished iterative scheme: repeatedly send the same base prompt to GPT-4 (no appended history or critique) and use a sound external verifier to certify proposed solutions; stop when verifier accepts or timeout.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 (generator) + external sound verifiers",
            "model_description": "Generator: GPT-4; Verifiers: same domain-specific programmatic checkers as above (SymPy, Python graph checker, VAL).",
            "reflection_method_name": "Sampling (repeated base-prompting) with sound verification",
            "reflection_method_description": "No critique is provided to the LLM and prompt does not grow; the LLM is asked the identical generation prompt repeatedly; after each generation, a sound verifier checks the solution; the process repeats up to a fixed sample budget (authors report k=15 and k=25 sample budgets).",
            "task_name": "Game of 24, Graph Coloring, Mystery Blocksworld STRIPS planning",
            "task_description": "Same formally verifiable tasks as above; verifier performs pass/fail checks each sample.",
            "performance_with_reflection": "Sampling with sound verifier yields similar improvements to LLM+Sound Critique: reported accuracies at k=15 and k=25 samples — Game of 24: 28% (k=15) and 42% (k=25); Graph Coloring: 40% (k=15) and 44% (k=25); Mystery Blocksworld: 9% (k=15) and 14% (k=25) (Table 1).",
            "performance_without_reflection": "Standard single-shot GPT-4 baseline: Game of 24 = 5%; Graph Coloring = 16%; Mystery Blocksworld = 4% (Table 1).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "No feedback-based reflection; mechanism is statistical/ensemble-like improvement via repeated independent sampling and external sound checking. Prompting is constant (no appended history), reducing token growth and cost per iteration.",
            "number_of_iterations": 15,
            "evidence_for_improvement": "Quantitative evidence: increasing sample budget from 15 to 25 generally increases accuracy (e.g., Game of 24 from 28% to 42%); sampling achieves most of the gains of richer critique loops while being cheaper in tokens because prompts don't grow.",
            "limitations_or_failure_cases": "Requires a sound external verifier; effectiveness depends on being able to sample many independent guesses (compute/latency cost). Does not leverage critique content at all; where verifier is unavailable or expensive, method may not be applicable. If verifier is itself imperfect (e.g., LLM-as-verifier), sampling does not salvage performance — the paper shows that using an LLM verifier collapses performance.",
            "comparison_to_other_methods": "Authors found sampling with a sound verifier performs as well as or nearly as well as the more complex LLM+Sound Critique setups, indicating that the main benefit of iterative schemes is multiple guesses + sound verification rather than using LLM-produced critiques or appended history.",
            "ablation_study_results": "Ablations show that removing critique and keeping only sampling preserves most performance gains; increasing the number of samples increases accuracy up to asymptote. Token-cost advantage due to fixed-size prompts allows larger sample budgets.",
            "uuid": "e8725.2",
            "source_info": {
                "paper_title": "On the Self-Verification Limitations of Large Language Models on Reasoning and Planning Tasks",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 2,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "Large language models cannot self-correct reasoning yet",
            "rating": 2,
            "sanitized_title": "large_language_models_cannot_selfcorrect_reasoning_yet"
        },
        {
            "paper_title": "Teaching large language models to self-debug",
            "rating": 2,
            "sanitized_title": "teaching_large_language_models_to_selfdebug"
        },
        {
            "paper_title": "A closer look at the self-verification abilities of large language models in logical reasoning",
            "rating": 2,
            "sanitized_title": "a_closer_look_at_the_selfverification_abilities_of_large_language_models_in_logical_reasoning"
        },
        {
            "paper_title": "Large language models are better reasoners with self-verification",
            "rating": 1,
            "sanitized_title": "large_language_models_are_better_reasoners_with_selfverification"
        },
        {
            "paper_title": "Backward reasoning in large language models for verification",
            "rating": 1,
            "sanitized_title": "backward_reasoning_in_large_language_models_for_verification"
        }
    ],
    "cost": 0.016826,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>On the Self-Verification Limitations of Large Language Models on Reasoning and Planning Tasks
12 Feb 2024</p>
<p>Kaya Stechly kstechl@asu.edu 
Karthik Valmeekam kvalmeek@asu.edu 
Subbarao Kambhampati </p>
<p>Arizona State University</p>
<p>Arizona State University</p>
<p>Arizona State University</p>
<p>On the Self-Verification Limitations of Large Language Models on Reasoning and Planning Tasks
12 Feb 2024D7E7CA4FAB7EF7B0BEFF19BE0479878BarXiv:2402.08115v1[cs.AI]
There has been considerable divergence of opinion on the reasoning abilities of Large Language Models (LLMs).While the initial optimism that reasoning might emerge automatically with scale has been tempered thanks to a slew of counterexamples-ranging from multiplication to simple planning-there persists a wide spread belief that LLMs can self-critique and improve their own solutions in an iterative fashion.This belief seemingly rests on the assumption that verification of correctness should be easier than generation-a rather classical argument from computational complexity-which should be irrelevant to LLMs to the extent that what they are doing is approximate retrieval.In this paper, we set out to systematically investigate the effectiveness of iterative prompting in the context of reasoning and planning.We present a principled empirical study of the performance of GPT-4 in three domains: Game of 24, Graph Coloring, and STRIPS planning.We experiment both with the model critiquing its own answers and with an external correct reasoner verifying proposed solutions.In each case, we analyze whether the content of criticisms actually affects bottom line performance, and whether we can ablate elements of the augmented system without losing performance.We observe significant performance collapse with self-critique, significant performance gains with sound external verification, but that the content of critique doesn't matter to the performance of the system.In fact, merely re-prompting with a sound verifier maintains most of the benefits of more involved setups.* equal contribution Preprint.Under Review.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) have rapidly captured the attention of the AI research community with their exceptional natural language completion capabilities.Trained on web-scale language corpora, these models have demonstrated the ability to generate seemingly valuable completions across a wide range of topics.This has led to a surge of interest in determining whether such models are able to perform well on reasoning tasks.Though initial anecdotal results showed promise, further systematic studies revealed inconsistencies and significant issues when applied to reasoning taskssuch as simple arithmetic or logic [9] and planning [32].These results questioning the robustness of their reasoning abilities led to researchers exploring ways to augment and improve these systems.</p>
<p>Of particular interest to us are emerging claims about LLM self-critiquing.In a typical setup, an LLM is iteratively prompted so that it both generates candidate solutions and, in response to separate queries, evaluates them.This process is looped until some stopping condition in hopes of potentially generating a refined answer.Current works [37,27,35], while admitting that LLMs are not good reasoners on their own, still exhibit considerable optimism about such self-critique systems.This belief seems to rest largely on the assumption that verification of correctness should be easier than generation for many reasoning problems-a rather classical argument from computational complexity.</p>
<p>Intrigued by the prevailing optimism, in this paper we set out to systematically investigate the effectiveness of using LLMs to critique their own generations in the context of planning and reasoning.To gain meaningful insights into the verification/critiquing abilities of LLMs for reasoning tasks, it is crucial to test them on formal tasks-ones where machine-verifiable ground truths are available for both generation and criticism.Note that such verification is not feasible in style-based/qualitative tasks like creative writing [37] where there is no formal notion of correctness and the critique can vary widely.With this understanding, we select three distinct reasoning problems: Game of 24, Graph Coloring, and STRIPS planning, in which there exist formal notions of correctness that allow us to automatically check the veracity and quality of both (binary) verification and critique generated by the LLM.</p>
<p>Our methodology employs a system (which we refer to as LLM+LLM) that uses the same LLM (the state-of-the-art GPT-4 [2]) for iterative solution and verification/critique generation.A generation prompt is sent to the LLM.Its response is used to create a verification prompt, which is then sent back to the same LLM.We use the feedback generated in this way to then create a backprompt, thus restarting the cycle.Across all of our domains, this self-verification systems worsens performance.We find that as the number of backprompts increases, this kind of self-correction consistently degrades output quality.Our analysis reveals that the verifier LLM's false negative rate is significant across our domains.In essence, even when the LLM generates a valid solution, the verifier LLM rejects it often enough that overall performance suffers.</p>
<p>We contrast this performance collapse with two baselines.The first is an ablated variant of the system (which we refer to as LLM+Sound Verifier), where an external sound verifier evaluates the LLM's generations and produces critique.This setup gives substantial performance gains across all domains, but closer analysis shows that the level of feedback doesn't seem to matter-as long as the verifier is sound, improvement remains regardless of how much or how little feedback the LLM receives.We ablate the system further, and remove critique entirely.In this setup, the LLM is repeatedly queried with the exact same base prompt until a sound verifier certifies its solution.Within this impoverished setting, prompts no longer maintain a past history of guesses, yet we can maintain most, if not all, of the gains shown by our previous, more complicated, more expensive setups.</p>
<p>Our empirical results suggest that the benefits of iterative prompting and verification can easily be misattributed to opaque self-critique and seemingly rich feedback.Thus, future implementations of LLMs for reasoning tasks should take the form of LLM-Modulo systems [18] where verification is done by external sound systems.In the rest of the paper, we first review related work and discuss domain backgrounds.Then, we explain our methodology, and finally closely analyze LLM self-verification abilities on our domains.</p>
<p>Related Work</p>
<p>Following the release of GPT-4, anecdotal accounts of performance on reasoning tasks [4] spurred much research into the capabilities of LLMs on various reasoning domains, from a menagerie of benchmarks covering basic problems [19] to planning [32], logic and arithmetic [9], analogical reasoning [33], and even math puzzles [37].Though these seemed initially promising, systematic studies began to generate negative results across many of these domains [32,28,1,30,10], claiming that LLM scaling shows much lower returns for reasoning tasks [24], and showcasing brittle performance in the face of minor problem permutations [22,9,3].</p>
<p>In response, researchers created augmented systems which embed the LLM in larger frameworks in an attempt to improve performance.These take many forms: common search algorithms with the LLM cast in the role of heuristic [37,11], approaches which reduce error rates by enforcing various consistency conditions [8,6,17], and direct LLM self-critique [27,35,5,15,21,38].</p>
<p>In the current work, we are interested in examining this third approach: LLM self-critique.In the most basic case, the LLM is queried for an answer, and then is re-queried with its own response together with some instructions to critique or improve it, with this process looped until some stopping condition.This if fundamentally based on the intuition that verification is easier than, or at least different enough from, generation that such a process can improve performance-in analogy to human self-critique.</p>
<p>Whereas much of the work in this direction has been very optimistic about the quality and generality of LLM self-critique abilities, some further systematic investigations have found less impressive results in logical fallacy detection [12] and HotpotQA [14], demonstrating very brittle improvement at best, some of which could be replicated sans self critique by merely including missing domain-general information into the original prompt.Our work continues this discussion, expanding it to autonomous multi-round self-verification within three formally verifiable domains that reflect reasoning tasks.</p>
<p>Common domains fall short for evaluating the reasoning and self-critique abilities of LLMs for a variety of reasons including test set memorization, lack of problem difficulty, and lack of ground truth.</p>
<p>Test set memorization: Due to the black box nature of state of the art models, ensuring that they weren't trained on those problems is difficult, and there is compelling evidence that they have memorized significant chunks of common benchmark sets [25].Many benchmark sets do not allow for arbitrary generation of novel questions, or worse, draw data from publicly available sources-the same sources LLM trainers have access to [36,29].We consider arbitrary generation of new instances of varying difficulty a key desideratum for any evaluation domain.</p>
<p>Lack of problem difficulty: Some of the benchmarks (e.g.HotPotQA [14], GSM-8k [21], ) used in evaluations of self-verification are easy-that is, SoTA LLM performance is already high-and are therefore much less informative about the effects of the refinement procedure.Additionally, many such sets over-constrain the solution space, usually by putting the question into multiple choice format.Not only does this make valuable and interesting critique hard to produce and evaluate, but it trivializes refinement: even a very simple agent can solve an n-choice problem with n − 1 critiques-just don't repeat the same answer.Conclusions drawn over reduced problem spaces of this type are unlikely to generalize.</p>
<p>Lack of ground truth:</p>
<p>A number of tasks that LLMs are evaluated on (e.g.writing prompts [37], constrained text generation [20], toxicity control [34] , etc.) are problems without a well-defined ground truth.Instead, they are evaluated by a couple of possible methods.Some require an assorted set of metrics which may not be well-validated for LLMs (e.g.see [30] for discussion on problems with transferring results from human-validated tests).Some are scored by humans [37].And some are evaluated by another pre-trained language model [21] or black box AI evaluator [34].This makes conclusions much harder to draw.</p>
<p>Background</p>
<p>We evaluate GPT-4's self-critique abilities over three distinct tasks, chosen because we believe they are good proxies for harder reasoning tasks, and because they allow freedom in arbitrary generation of additional instances while providing easy-to-deploy formal verifiability and guaranteed quality.This gives more than just flexibility-it also decreases the chance that our instances are represented in the black box model's opaque training sets.This strengthens our results by reducing the likelihood that the model can substitute approximate retrieval for general reasoning ability.</p>
<p>Any given problem in our sets also has the property that it has a large number of potential solutions, and this solution space cannot be substantially reduced through simple pattern-matching.As we are interested in self-verification loops where the LLM has access to its previous guesses, it is very important that removing a handful of possible solutions does not trivialize the problem.Compare this to common multiple choice question datasets, where any n-option problem can be solved in n exclusive guesses.</p>
<p>Game of 24</p>
<p>Game of 24 is a math puzzle where the goal is to combine four numbers with parentheses and basic arithmetical operations (addition, multiplication, subtraction, and division) to create an expression that evaluates to 24.The numbers are typically constrained to the range 1-12, a nod to game's playing card roots.Previously, it has been used as a domain of evaluation for other LLM self-verification schemes ( [37] and fulfills our domain desiderata (see 2).We use it here to enable direct comparisons between previous work and the current paper.</p>
<p>Following [37], we use data scraped from 4nums.com.This list of problems is ordered from shortest to longest average human solution time.Like [37], we evaluate our generation tasks on instances 901-1000.However, when evaluating verification and critique alone, we use instances 1-1000.</p>
<p>Verification in this domain is straightforward: given a proposed expression, simplify it using basic arithmetic and check if it is equal to 24.As a sound verifier, we use SymPy2 , a common Python library for symbolic mathematics, and handle any errors that it throws (for instance, if there are unbalanced parentheses) by outputting feedback that says the LLM's generation was malformed.</p>
<p>Example Prompt for Game of 24</p>
<p>Use numbers and basic arithmetic operations (+ -* /) to obtain 24.You must write your response.</p>
<p>Write your answer first, followed by [ANSWER END] → Input: 1 1 4 6 Answer:</p>
<p>Graph Coloring</p>
<p>Graph coloring is a a canonical NP-complete reasoning problem that is related to both propositional satisfiability as well as practical problems like scheduling and allocation.The complexity class NP contains problems that are hard to solve, but easy to verify, so this allows our It is broad enough to give insights into reasoning more generally, yet simple enough that it can be specified and evaluated by a human or basic pattern matching.</p>
<p>In this work, an instance of a graph coloring problem consists of a planar graph together with an optimal coloring number n.The goal is to output a solution that assigns one of n colors to each vertex such that no two edge-connected vertices share a color.</p>
<p>Using GrinPy3 to handle common graph operations, we built a test set of 100 graphs of varying small sizes.Each graph was constructed using a variant of the Erdős-Rényi method (p = 0.4), with any non-planar or repeat graphs discarded.These were compiled into the standard DIMACS format [7] together with the graph's precalculated chromatic number.</p>
<p>Verifying that a proposed coloring is correct is also easy: just check the colors of every edge.If any of them has two vertices of the same color, reject the coloring.</p>
<p>Example Prompt for Graph Coloring</p>
<p>Color the following graph, described as a set of edges, such that no two vertices on the same edge share a color.</p>
<p>→ → →</p>
<p>Our sound verifier is a simple, single for-loop implementation of this idea in Python: for each edge in the graph description, we check that both of its vertices are different.</p>
<p>STRIPS planning</p>
<p>STRIPS planning is a formalism used in automated planning that operates in a discrete, deterministic space.Such problems, commonly known as classical planning problems, involve finding a sequence of actions that when executed in a certain world state will take the agent to a desired goal state.STRIPS planning has a long history of featuring in various planning challenges and competitions, and is formally represented using the PDDL (Planning Domain and Definition Language) framework [23].Our instances are drawn from the Mystery Blocksworld domain in [31], which is an obfuscated version of Blocksworld, a domain used in International Planning Competitions [16].</p>
<p>A PDDL specification consists of three components.The domain doesn't change between problems and consists of a set of predicates, which can be used to describe the state of the world, and a set of actions-stored with their preconditions and effects-that the agent is allowed to take.The initial state is a list of predicates that are true at the outset of the specific problem (an example predicate, in natural language: "the red block is on the table ").The goal is a boolean expression of predicates.For Mystery Blocksworld, we use a set of 100 instances from [31] for our evaluations.</p>
<p>Solutions to PDDL problems take the form of correct plans-sequences of actions which can be executed from the initial state without violating any of their preconditions and which arrive at a final state that satisfies the goal.Verifying proposed plans is a matter of following the actions in order and checking that these two desiderata are achieved.For our experiments, we use VAL [13] as the sound external critique that evaluates and critiques LLM generated plans.</p>
<p>→</p>
<p>Once Attack action is performed the following facts will be true: Pain object.Once Attack action is performed the following facts will be false: Province object, Planet object, Harmony.</p>
<p>→</p>
<p>To perform Succumb action, the following facts need to be true: Pain object.Once Succumb action is performed the following facts will be true: Province object, Planet object, Harmony.</p>
<p>→</p>
<p>Once Succumb action is performed the following facts will be false: Pain object.</p>
<p>To perform Overcome action, the following needs to be true: Province other object, Pain object.</p>
<p>→</p>
<p>Once Overcome action is performed the following will be true: Harmony, Province object, Object Craves other object.</p>
<p>→</p>
<p>Once Overcome action is performed the following will be false: Province other object, Pain object.</p>
<p>→</p>
<p>To perform Feast action, the following needs to be true: Object Craves other object, Province object, Harmony.</p>
<p>→</p>
<p>Once Feast action is performed the following will be true: Pain object, Province other object.Once Feast action is performed the following will be false:, Object Craves other object, Province object, Harmony.</p>
<p>→ [STATEMENT]</p>
<p>As initial conditions I have that, object a craves object b, object d craves object c, harmony, planet object b, planet object c, province object a and province object d.</p>
<p>→</p>
<p>My goal is to have that object c craves object a.</p>
<p>My plan is as follows: My plan is as follows:</p>
<p>[PLAN] As our results are about ablations of self-critique architectures, our basic test framework is a general prompting architecture informed by interchangeable domain-specific components.Our intent is to give the LLM as much information as possible, so we design prompts that include the entire history of previously proposed solutions and the feedback they received.</p>
<p>A problem instance is a domain-specific formal description.In attempting a problem, our system (as shown in Figure 1) proceeds as follows: (1) The instance is processed by a simple, hard-coded natural language translator into a prompt which is then sent to the LLM.(2) The LLM's response is wrapped in a domain-specific critique prompt, which is separately sent as another LLM query.(3) If the following response claims that the proposed solution is correct, we stop the system and output the proposed solution.If it doesn't, the critique is extracted, wrapped in instruction text, and appended to a prompt containing the entire history of interactions so far.This is then sent to the LLM, and the cycle repeats, potentially until we enforce a timeout.</p>
<p>Though only two types of prompts are sent, the LLM can be seen as playing three separate roles: as an answer guesser, a (binary) verifier, and a critique generator.In order to better understand which of these roles contribute to increased performance, we will examine variations of this system where one or more of them are changed or removed.</p>
<p>To examine LLM verification abilities, we first measure the performance of the entire system, and then evaluate false positive and false negative rates across domains.</p>
<p>To better understand the guesser role, and the LLM's ability to consider and implement critique, we will modify the loop so that the verification and critique roles are played by a provably sound verifier that provides rich, correct feedback.We will then reduce and eventually eliminate the amount of provided information (e.g.rich feedback: explicitly giving an evaluation of a proposed Game of 24 expression; minimal feedback: "the previous answer was wrong"; no feedback: re-querying with the base prompt), while keeping track of changes in the performance of the entire system.</p>
<p>For LLM critique generation, we construct subdomains of our original domains.In these prompts, we provide a problem description and a proposed solution, and we ask the LLM to provide domain- We measure performance at iteration n by evaluating how many instances are currently correctly solved (whether the LLM has verified them or not.In other words, we evaluate as if the timeout were n) and adding that to the number the LLM has correctly verified so far.Note that if the verifier incorrectly rejects an answer and the followup is wrong, the next round may be worse.When paired with a sound verifier, the LLM monotonically improves its performance as the number of backprompts increase up to some asymptote.The top three lines show this for each of our domains.On the other hand, when the LLM itself is used as a verifier, performance collapses immediately.</p>
<p>specific critique of the solution if it is incorrect.We parse the output using a hard-coded script and measure accuracy compared to the sound verifier's output.</p>
<p>Note that sound verifiers output task specific critiques: for Game of 24, the evaluation of the provided expression ("1+1+4+6=12 not 24"); constraint violations for graph coloring ("vertices 1 and 3 were both colored red despite sharing an edge"); and precondition violations (the second action "succumb object a" is invalid because the succumb action requires the pain object to be true, which is not the case after the first action.)and failure to reach goal ("this plan does not reach the goal") for planning.</p>
<p>Examining Self-Verification</p>
<p>We evaluate our system over 100 instances in each domain.</p>
<p>In standard prompting we send a single query to the LLM and treat whatever it outputs as its final answer.As shown in Table 1, when we augment this condition with the full self-critique setup, performance decreases.In fact, Figure 2 shows that as the number of backprompts increases, this kind of self-correction consistently degrades output quality.If the LLM were a good verifier, then we would have expected it to recognize instances which are already right, and thus-at worst-maintain the baseline score.The LLM+Sound Critique conditions illustrate this idea: performance increases significantly with a guaranteed-correct verifier.The LLM-as-verifier ranges in accuracy depending on the domain, but no matter which one, it maintains significant false negative and false positive rates, as illustrated in Table 2.In general, it seems that the verifier requires high accuracy or else the overall system will encounter compounding errors.In the reasoning domains considered, LLMs-as-verifiers are insufficient to prevent these.Prompt and response examples can be found in the Appendix.</p>
<p>Domain</p>
<p>Critique generation</p>
<p>We consider the quality of LLM-generated free-form critiques separately from that of their binary verification signals, and find that they are full of unhelpful hallucinations and mistakes.</p>
<p>In Game of 24, without any further instructions, the LLM tends to output incorrect suggestions for the answer.When prompted to give an evaluation of the proposed expression first, its accuracy varies, and doesn't always line up with the verification it outputs.</p>
<p>The LLM's critiques of proposed solutions to graph coloring instances are riddled with non-existent edges and include many spurious claims about the colors of vertices, often missing the violated constraint in favor of them.</p>
<p>Mystery Blocksworld critiques also hallucinate the action preconditions and whether they are met or unmet in a given state.This leads to lower accuracy of the critiques provided by the LLM.</p>
<p>All of this indicates that the LLM is ill-suited to the roles of verifier and critique generator in these domains.</p>
<p>Critique consideration</p>
<p>Our results also imply that the LLM isn't sensitive to varying levels of feedback.</p>
<p>We use a sound verifier to critique the LLMs output, and compare the results over three levels of feedback for graph coloring and planning, and over two levels for game of 24.Examples of prompts containing each sort of feedback can be found in the appendix.</p>
<p>• Binary feedback is the same for all domains: either the verifier accepts the solution, stopping the system, or we create a backprompt which says the previous answer was wrong but doesn't elaborate.</p>
<p>• First error feedback mentions the first error that was found (e.g. an incorrect edge in graph coloring, an inexecutable step in planning, the evaluation of the proposed expression in game of 24).</p>
<p>• All errors feedback includes every error that was found.Note that due to the simplicity of game of 24, we do not implement a third feedback level for it.</p>
<p>Perhaps surprisingly, Table 1 shows very little difference between these three conditions.And in two of our domains, increasing the amount of feedback actually leads to a decrease performance.</p>
<p>The intuition underlying the entire critique system is that sending a history of previous failed attempts together with information about why those attempts were failures should guide the LLM toward better future answers.If this were true, then we would expect the performance jump to be tied to the content of the backprompts.With only the data discussed so far, it might seem like the relevant content is actually the history of failed attempts the LLM receives rather than any feedback on those attempts.However, our final experiments contradict this interpretation as well.</p>
<p>We take our ablation of critique consideration to the logical extreme, and remove the availability of critique entirely.In this sampling setup, we keep the verifier but don't change the prompt at all between iterations.The LLM (at t = 1) is asked the same question over and over until the verifier certifies it or it hits some pre-established timeout.</p>
<p>Represented in Table 1 by the "Sampling" columns, this does just as well as providing feedback to the LLM.Notably, because prompts do not grow additively with iteration number, the token cost of these prompts is much lower, even if the number of prompts is greatly increased.This allows us to increase performance further just by asking more times.</p>
<p>Our final results show that, in our domains, the information in critiques does not have an effect on performance, contrary to previous claims in the literature.In fact, our performance increases seem to mostly stem from having more guesses with a sound verifier.</p>
<p>Conclusion</p>
<p>In this paper, we conducted a systematic evaluation of the self-critique abilities of Large Language Models in three reasoning and planning tasks.Our results contradict recent work that has been very optimistic about LLM self-critique abilities.</p>
<p>We separated self-critique into three components: verification, critique generation, and critique consideration.Across the domains we evaluated, LLMs did poorly on all three, and-in fact-performance gains that we did see could all be traced back to two major factors: the existence of a sound verifier to pick out which of the LLM's proposed solutions are actually correct, and the opportunity to make many guesses.These failures of verification could potentially be very detrimental to a system's dependability, especially in domains where high reliability is paramount.</p>
<p>Our proposal, based on the case studies we've performed in this paper is, when possible, to embed LLMs in systems which allow them to guess at solutions multiple times, but which provide some kind of signal for when a guess is "good enough".Ideally, this takes the form of a sound verifier, like VAL [13] for STRIPS planning, basic expression simplification for Game of 24, or a constraint checker for constraint satisfaction problems.In real-world applications we expect this role to be played by a menagerie of partial critics evaluating plans or solutions based on criteria that they have access to, designed so that consensus is considered verification.Similar architectures have already shown some success [26].</p>
<p>A Appendix</p>
<p>A.</p>
<p>A.2 Graph Coloring</p>
<p>A.2.1 Hallucinations in Graph Coloring Critique</p>
<p>The following is a more in-depth look at GPT-4's critique abilities on the graph coloring problem.</p>
<p>For each instance, we generated five different kinds of colorings: correct, ablated (exactly one edge is wrong), non-optimal (a coloring with no constraint violations except that it uses too many colors), random, and LLM (sampled from LLM generations).For each of these 500 proposed colorings, we sent a query to the LLM asking it to verify if the answer was right, and-if not-to output a free-form critique.We then parsed these to determine what edges it said were colored the same at both vertices.</p>
<p>Table A1 summarizes the results.Note that, proportionally, hallucinations decrease when the error rate of the domain increases.That is to say, when there are more incorrect edges, the model is more likely to point to one of them.Intuitively, this makes sense: it's easier to guess one edge which is wrong when half of all the edges are miscolored, as is the case on average among randomly colored graphs.</p>
<p>Edge hallucinations are more common than vertex.Essentially, typical behavior is to pick two vertices that are the same color in the coloring, but which aren't associated by an edge in the graph description, and claim that they are connected and thus illegally colored.Vertex color hallucination is when the reverse happens: instead of ascribing an edge to same-color nodes, the colorings of two connected vertices are misstated.The overlap between the two cases, where a non-existent edge is declared to be violated by non-existent colorings is much rarer than either.Note that it never hallucinates new vertex names, only that vertices which are in graph have colors differing from reality.</p>
<p>Even rarer cases did spring up in the response data.At times the model lost track of the question being asked and reversed it, explicitly claiming that two same-colored vertices violate the conditions because they aren't connected; or it began to contradict itself mid-deduction, making multiple claims about a vertex's color.We present these examples later in this appendix.</p>
<p>A.2.2 Prompts</p>
<p>All of following examples are built on the same graph instance.</p>
<p>DIMACS Format For Graphs</p>
<p>Baseline, Direct Prompt</p>
<p>Color the following graph, described as a set of edges, such that no two vertices on the same edge share a color.→ ============================ Vertex 0 is missing in the coloring.Missing assignment.============================ Vertex 0 (color 3) is connected to vertex 3 (color 2), sharing an edge.Vertex 0 (color 3) is also connected to vertex 4 (color 0), and to vertex 5 (color 0), and to vertex 9 (color 0), and to vertex 12 (color 2), sharing an edge with them.Therefore, this assignment is correctly colored with color 0, color 2, and color 3, making use of 3 different colors.</p>
<p>→ → →</p>
<p>Vertex 1 (color 2) is connected to vertex 4 (color 0), and to vertex 5 (color 0), and to vertex 6 (color 0), and to vertex 7 (color 3), and to vertex 8 (color 0), and to vertex 10 (color 1), and to vertex 11 (color 1), sharing an edge with them.Therefore, this assignment is also correctly colored.</p>
<p>→ → →</p>
<p>Vertex 2 (color 0) is connected to vertex 12 (color 2), sharing an edge.Therefore, this assignment is correctly colored.</p>
<p>→ Vertex 3 (color 2) is connected to vertex 5 (color 0), and to vertex 8 (color 0), and to vertex 10 (color 1), sharing an edge with them.Therefore, this assignment is correctly colored.</p>
<p>→ Vertex 4 (color 0) is connected to vertex 7 (color 3), and to vertex 11 (color 1), sharing an edge.Therefore, this assignment is correctly colored.</p>
<p>→ Vertex 5 (color 0) is connected to vertex 7 (color 3), sharing an edge.Therefore, this assignment is correctly colored.</p>
<p>→ Vertex 6 (color 0) is connected to vertex 10 (color 1), and to vertex 12 (color 2), sharing an edge.Therefore, this assignment is correctly colored.</p>
<p>→ Vertex 8 (color 0) is connected to vertex 10 (color 1), sharing an edge.Therefore, this assignment is correctly colored.</p>
<p>→ Vertex 9 (color 0) is connected to vertex 11 (color 1), sharing an edge.Therefore, this assignment is correctly colored.Vertex 12 (color 2) isn't connected to vertex 2 (color 0), hence, this assignment is correctly colored.</p>
<p>→</p>
<p>No adjacent vertices share a same color, and no colors exceed the optimal coloring number.Hence, the coloring is correct.</p>
<p>→ Verifier confirmed success.</p>
<p>Examples of (Rare) Mangled, Illogical, and Otherwise Strange Output on the Verification Task Vertex 10, colored 4, is connected to vertex 11, also colored 1.However, it is also colored 4, which is greater than the optimal coloring of 4. So, this coloring is non-optimal.→ ============================ Vertex 10, colored 4, is connected to vertex 11, also colored 1.However, it is also colored 4, which is greater than the optimal coloring of 4. So, this coloring is non-optimal.</p>
<p>→ A.3 Planning -Mystery Blocksworld</p>
<p>A.3.1 Accuracy of Planning Critique</p>
<p>For each instance, we generated five different kind of plans: correct, inexecutable (where an action is inexecutable and the plan is thus invalid), non goal reaching (all actions are executable but the plan does not reach the desired state), random, and LLM (sampled from the LLM generations).For each of these prompts, we sent a query to the LLM asking it to verify the plan and critique it in a certain format.We provide an example in the following prompts section.Specifically, if the plan is valid, the LLM has to just output that.If a plan is invalid and inexecutable, it has to provide the first action that the plan is rendered invalid and the unmet preconditions for that action.If the plan is invalid and non goal reaching, it has to provide the unmet goals for the plan.We evaluate the binary verification and the critique by comparing it to the ground truth provided by VAL [13].We check the binary verification, whether or not the detected the type of invalidity (inexecutable or non goal reaching) and if the critique is valid.</p>
<p>A.3.2 Prompts</p>
<p>All of following examples are built on the same graph instance.</p>
<p>Baseline, Direct Prompt</p>
<p>I am playing with a set of objects.Here are the actions I can do Attack object Feast object from another object Succumb object Overcome object from another object I have the following restrictions on my actions: To perform Attack action, the following facts need to be true: Province object, Planet object, Harmony.</p>
<p>→</p>
<p>Once Attack action is performed the following facts will be true: Pain object.Once Attack action is performed the following facts will be false: Province object, Planet object, Harmony.</p>
<p>→</p>
<p>To perform Succumb action, the following facts need to be true: Pain object.Once Succumb action is performed the following facts will be true: Province object, Planet object, Harmony.</p>
<p>→</p>
<p>Once Succumb action is performed the following facts will be false: Pain object.</p>
<p>To perform Overcome action, the following needs to be true: Province other object, Pain object.</p>
<p>→</p>
<p>Once Overcome action is performed the following will be true: Harmony, Province object, Object Craves other object.</p>
<p>→</p>
<p>Once Overcome action is performed the following will be false: Province other object, Pain object.</p>
<p>→</p>
<p>To perform Feast action, the following needs to be true: Object Craves other object, Province object, Harmony.</p>
<p>→</p>
<p>Once Feast action is performed the following will be true: Pain object, Province other object.Once Feast action is performed the following will be false:, Object Craves other object, Province object, Harmony.</p>
<p>→ [STATEMENT]</p>
<p>As initial conditions I have that, object a craves object b, object d craves object c, harmony, planet object b, planet object c, province object a and province object d.</p>
<p>→</p>
<p>My goal is to have that object c craves object a.</p>
<p>My plan is as follows: To perform Attack action, the following facts need to be true: Province object, Planet object, Harmony.</p>
<p>→</p>
<p>Once Attack action is performed the following facts will be true: Pain object.Once Attack action is performed the following facts will be false: Province object, Planet object, Harmony.</p>
<p>→</p>
<p>To perform Succumb action, the following facts need to be true: Pain object.Once Succumb action is performed the following facts will be true: Province object, Planet object, Harmony.</p>
<p>→</p>
<p>Once Succumb action is performed the following facts will be false: Pain object.</p>
<p>To perform Overcome action, the following needs to be true: Province other object, Pain object.</p>
<p>→</p>
<p>Once Overcome action is performed the following will be true: Harmony, Province object, Object Craves other object.</p>
<p>→</p>
<p>Once Overcome action is performed the following will be false: Province other object, Pain object.</p>
<p>→</p>
<p>To perform Feast action, the following needs to be true: Object Craves other object, Province object, Harmony.</p>
<p>→</p>
<p>Once Feast action is performed the following will be true: Pain object, Province other object.Once Feast action is performed the following will be false:, Object Craves other object, Province object, Harmony.</p>
<p>→</p>
<p>Once Attack action is performed the following facts will be true: Pain object.Once Attack action is performed the following facts will be false: Province object, Planet object, Harmony.</p>
<p>→</p>
<p>To perform Succumb action, the following facts need to be true: Pain object.Once Succumb action is performed the following facts will be true: Province object, Planet object, Harmony.</p>
<p>→</p>
<p>Once Succumb action is performed the following facts will be false: Pain object.</p>
<p>To perform Overcome action, the following needs to be true: Province other object, Pain object.</p>
<p>→</p>
<p>Once Overcome action is performed the following will be true: Harmony, Province object, Object Craves other object.</p>
<p>→</p>
<p>Once Overcome action is performed the following will be false: Province other object, Pain object.</p>
<p>→</p>
<p>To perform Feast action, the following needs to be true: Object Craves other object, Province object, Harmony.</p>
<p>→</p>
<p>Once Feast action is performed the following will be true: Pain object, Province other object.Once Feast action is performed the following will be false:, Object Craves other object, Province object, Harmony.</p>
<p>→ [STATEMENT]</p>
<p>As initial conditions I have that, object a craves object b, object d craves object c, harmony, planet object b, planet object c, province object a and province object d.</p>
<p>→</p>
<p>My goal is to have that object c craves object a.My plan is as follows:</p>
<p>[PLAN] feast object d from object c succumb object d attack object c overcome object c from object a [PLAN END] Verify whether the above plan is valid.Provide a JSON between tags [JSON] and [JSON_END] for the verification information.The JSON should contain three main keys: (1) "valid": a binary value that tells if the plan is valid or not i.e., the plan when executed satisfies the goal conditions.If the plan is invalid and inexecutable then include (2) "unmet_preconditions": This contains two more keys; (2.1) "action": This is the name of the first action that renders the plan inexecutable (2.2) "preconditions": A list of unmet preconditions for the mentioned action; If the plan is executable but not goal reaching then include (3) "unmet_goals": A list of unmet goal conditions in the JSON.Include only one of the keys (2) or (3) based on the plan invalidity.
→ → → → → → → → A.3.</p>
<p>Backprompts</p>
<p>Backprompt Generated From Self-Critique</p>
<p>Plan is invalid.</p>
<p>The first action "feast object b from object c" is not possible because the initial conditions do not include "Province object b" which is required for the Feast action.</p>
<p>→</p>
<p>Backprompt Generated From Sound Verifier (First)</p>
<p>The above plan is invalid.</p>
<p>The following action at step 3 has an unmet precondition: attack object a The unmet precondition is: province object a</p>
<p>Backprompt Generated From Sound Verifier (Full)</p>
<p>The above plan is invalid.</p>
<p>There is an unmet goal condition.This is: object d craves object a There are unsatisfied preconditions.</p>
<p>The following action at step 3 has an unmet precondition: attack object a The unmet precondition is: province object a</p>
<p>The following action at step 5 has an unmet precondition: feast object d from object a The unmet precondition is: province object d To perform Attack action, the following facts need to be true: Province object, Planet object, Harmony.</p>
<p>→</p>
<p>Once Attack action is performed the following facts will be true: Pain object.Once Attack action is performed the following facts will be false: Province object, Planet object, Harmony.</p>
<p>→</p>
<p>To perform Succumb action, the following facts need to be true: Pain object.Once Succumb action is performed the following facts will be true: Province object, Planet object, Harmony.</p>
<p>→</p>
<p>Once Succumb action is performed the following facts will be false: Pain object.</p>
<p>To perform Overcome action, the following needs to be true: Province other object, Pain object.</p>
<p>→</p>
<p>Once Overcome action is performed the following will be true: Harmony, Province object, Object Craves other object.</p>
<p>→</p>
<p>Once Overcome action is performed the following will be false: Province other object, Pain object.</p>
<p>→</p>
<p>To perform Feast action, the following needs to be true: Object Craves other object, Province object, Harmony.</p>
<p>→</p>
<p>Once Feast action is performed the following will be true: Pain object, Province other object.Once Feast action is performed the following will be false:, Object Craves other object, Province object, Harmony.</p>
<p>→ [STATEMENT]</p>
<p>As initial conditions I have that, object a craves object b, object d craves object c, harmony, planet object b, planet object c, province object a and province object d.</p>
<p>→</p>
<p>My goal is to have that object c craves object a.</p>
<p>My plan is as follows: The first action "feast object b from object c" is not possible because the initial conditions do not include "Province object b" which is required for the Feast action.</p>
<p>→ -----------GPT4-as-Planner Response-----------I apologize for the oversight.Let's correct the plan.</p>
<p>[PLAN] attack object a succumb object a feast object b from object c overcome object c from object a attack object d succumb object d feast object c from object a overcome object a from object c [PLAN END] -----------GPT4-as-Verifier Response-----------Plan is invalid.</p>
<p>The first action "attack object a" cannot be performed because the conditions for the attack action are not met.According to the restrictions, to perform an attack action, the following facts need to be true: Province object, Planet object, Harmony.In the initial conditions, you have Province object b, not Province object a.Therefore, the attack action cannot be performed on object a.</p>
<p>→ → → →</p>
<p>-----------GPT4-as-Planner Response-----------I apologize for the confusion.Let's revise the plan.</p>
<p>[PLAN] attack object b succumb object b feast object c from object d overcome object d from object a attack object a succumb object a feast object d from object a overcome object a from object c [PLAN END] -----------GPT4-as-Verifier Response-----------Plan is invalid.</p>
<p>The first action "attack object b" cannot be performed because the conditions for the attack action are not met.The attack action requires the facts: Province object, Planet object, Harmony to be true.In the initial conditions, you have Province object b and Harmony, but you do not have Planet object.Therefore, the attack action cannot be performed.</p>
<p>→ → →</p>
<p>-----------GPT4-as-Planner Response-----------I apologize for the confusion.Let's revise the plan.</p>
<p>[PLAN] feast object b from object c succumb object b attack object d overcome object d from object a feast object a from object d succumb object a attack object c overcome object c from object a [PLAN END] -----------GPT4-as-Verifier Response-----------Plan is invalid.</p>
<p>The first action "feast object b from object c" cannot be performed because the initial conditions do not satisfy the requirements for the Feast action.According to the rules, to perform Feast action, the following needs to be true: Object Craves other object, Province object, Harmony.</p>
<p>In the initial conditions, object b craves object c, harmony, and province object b are true.However, the Feast action requires that the object performing the action (object b in this case) is the Province object, which is not the case in the initial conditions.Therefore, the first action cannot be performed, making the plan invalid.
→ → → → → →
-----------GPT4-as-Planner Response-----------I apologize for the confusion.Let's revise the plan.</p>
<p>[PLAN] attack object b succumb object b feast object c from object d overcome object d from object a feast object a from object d succumb object a attack object c overcome object c from object a [PLAN END] -----------GPT4-as-Verifier Response-----------Plan is invalid.</p>
<p>The first action "attack object b" is valid because the initial conditions meet the requirements for the attack action.However, the second action "succumb object b" is invalid because the "Pain object" fact is not true after the first action.The "Attack object b" action makes the "Pain object" fact true, but it also makes the "Province object" fact false, which is a requirement for the "Succumb" action.Therefore, the plan fails at the second action.The first action "attack object b" is valid because the initial conditions meet the requirements for the attack action: Province object, Planet object, Harmony.</p>
<p>→ However, the second action "succumb object b" is invalid.The succumb action requires the fact "Pain object" to be true, which is achieved after the attack action.But the object for the succumb action should be the same as the object that was attacked, which is not the case in your plan.</p>
<p>→ → →</p>
<p>The rest of the plan cannot be validated because it depends on the outcome of the invalid succumb action.</p>
<p>→ -----------GPT4-as-Planner Response-----------I apologize for the confusion.Let's revise the plan.</p>
<p>[PLAN] attack object b succumb object b feast object b from object c overcome object c from object a attack object a succumb object a feast object a from object d overcome object d from object a [PLAN END] -----------GPT4-as-Verifier Response-----------Plan is invalid.</p>
<p>The plan fails at the first step "attack object b".According to the rules, to perform the Attack action, the following facts need to be true: Province object, Planet object, Harmony.In the initial conditions, you have Province object b, Planet object a, and Harmony.However, for the Attack action, you need the same object for Province and Planet, which is not the case here.Therefore, you cannot perform the Attack action on object b.
→ → → →
-----------GPT4-as-Planner Response-----------I apologize for the confusion.Let's revise the plan.</p>
<p>[PLAN] attack object a succumb object a feast object b from object c overcome object c from object a attack object a succumb object a feast object a from object d overcome object d from object a [PLAN END] -----------GPT4-as-Verifier Response-----------Plan is invalid.</p>
<p>The first action "attack object a" cannot be performed because the initial conditions do not meet the requirements for the attack action.The attack action requires the facts: Province object, Planet object, Harmony to be true.In the initial conditions, the Province object is object b, not object a.Therefore, the attack action cannot be performed on object a.</p>
<p>→ → →</p>
<p>-----------GPT4-as-Planner Response-----------I apologize for the confusion.Let's revise the plan.The first action "attack object d" cannot be performed because the conditions for the Attack action are not met.According to the rules, to perform the Attack action, the following facts need to be true: Province object, Planet object, Harmony.In the initial conditions, you have Province object b, Planet object a, and Harmony.However, you are trying to attack object d, not object a or b.Therefore, the first action in your plan is not valid.</p>
<p>→ → → →</p>
<p>-----------GPT4-as-Planner Response-----------I apologize for the confusion.Let's revise the plan.</p>
<p>[PLAN] attack object b succumb object b feast object b from object c overcome object c from object a attack object a succumb object a feast object a from object d overcome object d from object a [PLAN END] -----------GPT4-as-Verifier Response-----------Plan is invalid.</p>
<p>The first action "attack object b" is invalid because the conditions for the attack action are not met.The conditions for the attack action are: Province object, Planet object, Harmony.In the initial conditions, you have Province object b, Planet object a, and Harmony.The Province object and Planet object must be the same for the attack action to be valid.Therefore, you cannot attack object b with the given initial conditions.</p>
<p>→ → → →</p>
<p>-----------GPT4-as-Planner Response-----------I apologize for the confusion.Let's revise the plan.</p>
<p>[PLAN] feast object a from object b succumb object a attack object a overcome object a from object c feast object c from object d succumb object c attack object c overcome object c from object a [PLAN END] -----------GPT4-as-Verifier Response-----------Plan is invalid.</p>
<p>The first action "feast object a from object b" is not possible because the initial conditions state that "object b craves object c", not "object b craves object a".Therefore, the Feast action cannot be performed as the condition "Object Craves other object" is not met.</p>
<p>→ →</p>
<p>The plan fails at the first step.</p>
<p>-----------GPT4-as-Planner Response-----------I apologize for the confusion.Let's revise the plan.The first action "feast object d from object a" cannot be performed because the initial conditions do not satisfy the requirements for the Feast action.The Feast action requires that the object performing the action (in this case, object d) craves the other object (in this case, object a).However, the initial conditions state that object d craves object a, not the other way around.Therefore, the Feast action cannot be performed, and the plan is invalid.
→ → → →
-----------GPT4-as-Planner Response-----------I apologize for the confusion.Let's revise the plan.</p>
<p>[PLAN] feast object a from object b succumb object a attack object a overcome object a from object c feast object c from object d succumb object c attack object c overcome object c from object a [PLAN END] -----------GPT4-as-Verifier Response-----------Plan is invalid.</p>
<p>The first action "feast object a from object b" cannot be performed because the initial conditions state that "object b craves object c", not "object a".Therefore, the Feast action cannot be performed as it requires the fact "Object Craves other object" to be true, which in this case would be "object b craves object a".</p>
<p>→ → →</p>
<p>-----------GPT4-as-Planner Response-----------I apologize for the confusion.Let's revise the plan.</p>
<p>[PLAN] feast object d from object c succumb object d attack object d overcome object d from object a feast object a from object d succumb object a attack object a overcome object a from object c [PLAN END] -----------GPT4-as-Verifier Response-----------Plan is invalid.</p>
<p>The first action in your plan, "feast object d from object c", cannot be performed because the initial conditions do not include "Object d craves object c".The initial conditions state that "object b craves object c, object c craves object d, object d craves object a".Therefore, the first action in your plan is not valid according to the restrictions on the Feast action.
→ → → →
-----------GPT4-as-Planner Response-----------I apologize for the confusion.Let's revise the plan.</p>
<p>[PLAN] feast object a from object d succumb object a attack object a overcome object a from object c feast object c from object d succumb object c attack object c overcome object c from object a [PLAN END] -----------GPT4-as-Verifier Response-----------Plan is invalid.</p>
<p>The first action in your plan, "feast object a from object d", cannot be performed because the initial conditions do not include "Object a craves object d".According to the rules, to perform the Feast action, the following needs to be true: "Object Craves other object, Province object, Harmony".In your initial conditions, you have " The first action "feast object b from object c" is not possible because the initial conditions do not include "Province object b" which is required for the Feast action.</p>
<p>→ ==========GPT-4-as-Verifier============ Plan is invalid.</p>
<p>The first action "feast object b from object c" cannot be performed because the initial conditions do not satisfy the requirements for the Feast action.According to the rules, to perform Feast action, the following needs to be true: Object Craves other object, Province object, Harmony.</p>
<p>In the initial conditions, object b craves object c, harmony, and province object b are true.However, the Feast action requires that the object performing the action (object b in this case) is the Province object, which is not the case in the initial conditions.Therefore, the first action cannot be performed, making the plan invalid.
→ → → → → →
==========GPT-4-as-Verifier============ Plan is invalid.</p>
<p>The first action "attack object b" is valid because the initial conditions meet the requirements for the attack action: Province object, Planet object, Harmony.</p>
<p>→ However, the second action "succumb object b" is invalid.The succumb action requires the fact "Pain object" to be true, which is achieved after the attack action.But the object for the succumb action should be the same as the object that was attacked, which is not the case in your plan.</p>
<p>→ → →</p>
<p>The rest of the plan cannot be validated because it depends on the outcome of the invalid succumb action.</p>
<p>→</p>
<p>As initial conditions I have that, object b craves object c, object c craves object d, object d craves object a, harmony, planet object a and province object b.→ My goal is to have that object a craves object c and object d craves object a.</p>
<p>Figure 1 :
1
Figure 1: Overall Backprompting and Evaluation Architecture</p>
<p>Figure 2 :
2
Figure 2: Performance vs Number of Iterations Before Timeout.We measure performance at iteration n by evaluating how many instances are currently correctly solved (whether the LLM has verified them or not.In other words, we evaluate as if the timeout were n) and adding that to the number the LLM has correctly verified so far.Note that if the verifier incorrectly rejects an answer and the followup is wrong, the next round may be worse.When paired with a sound verifier, the LLM monotonically improves its performance as the number of backprompts increase up to some asymptote.The top three lines show this for each of our domains.On the other hand, when the LLM itself is used as a verifier, performance collapses immediately.</p>
<p>c OPTIMAL CHROMATIC NUMBER === 3</p>
<p>9 :
9
The vertices 3 and 0 both have Color 2 and they are adjacent.The vertices 1 and 11 both have Color 1 and they are adjacent.The vertices 2 and 5 both have Color 0 and they are adjacent.Therefore, the given coloring is incorrect.Color 0 share an edge.0: Color 0 and 10: Color 0 share an edge.0: Color 0 and 14: Color 0 share an edge.9: Color 0 and 13: Color 0 share an edge.============================ Vertex 3 is colored with Color 4, which exceeds the optimal coloring number of 4. Therefore, this coloring is non-optimal.</p>
<p>→ ============================ Vertex 9 (============================ Vertex 2 (→Vertex 6 (
926
Color 2) and Vertex 10 (Color 1) are connected and share the same color.Vertex 9 (Color 2) and Vertex 11 (Color 1) are also connected and share the same color.→ Color 0) is connected to Vertex 0 (Color 0).Vertex 5 (Color 0) is connected to Vertex 1 (Color 2) and Vertex 14 (Color 1), but also to Vertex 7 (Color 1).Color 0) is connected to Vertex 3 (Color 2), Vertex 10 (Color 1) and 13 (Color 2), but also to Vertex 8 (Color 2).→ ============================ The coloring is incorrect.Vertex 1 (Color 2) and Vertex 3 (Color 3) are connected but have different colors.Vertex 3 (Color 3) is connected to Vertex 6 (Color 3).</p>
<p>As initial conditions I have that, object b craves object c, object c craves object d, object d craves object a, harmony, planet object a and province object b. → My goal is to have that object a craves object c and object d craves object a.My plan is as follows: [PLAN] Example LLM Response feast object b from object c succumb object b attack object a overcome object a from object c feast object d from object a succumb object d attack object c overcome object c from object a [PLAN END] Prompt To Elicit Verification (Open Ended) I am playing with a set of objects.Here are the actions I can do Attack object Feast object from another object Succumb object Overcome object from another object I have the following restrictions on my actions:</p>
<p>A. 3 . 4
34
Complete Example of Iterative Prompt Sequence Full Sequence of 15 (LLM-Generated) Backprompts I am playing with a set of objects.Here are the actions I can do Attack object Feast object from another object Succumb object Overcome object from another object I have the following restrictions on my actions:</p>
<p>[PLAN] feast object d from object c succumb object d attack object c overcome object c from object a [PLAN END] [STATEMENT] As initial conditions I have that, object b craves object c, object c craves object d, object d craves object a, harmony, planet object a and province object b. → My goal is to have that object a craves object c and object d craves object a.My plan is as follows: [PLAN] -----------GPT4-as-Planner Response----------feast object b from object c succumb object b attack object a overcome object a from object c feast object d from object a succumb object d attack object c overcome object c from object a [PLAN END] -----------GPT4-as-Verifier Response-----------Plan is invalid.</p>
<p>--------GPT4-as-Planner Response-----------I apologize for the confusion.Let's revise the plan.[PLAN]attack object b succumb object b feast object c from object d overcome object d from object a feast object a from object d succumb object a attack object c overcome object c from object a [PLAN END] -----------GPT4-as-Verifier Response-----------Plan is invalid.</p>
<p>--------GPT4-as-Verifier Response-----------Plan is invalid.</p>
<p>[PLAN] feast object d from object a succumb object d attack object d overcome object d from object c feast object c from object d succumb object c attack object c overcome object c from object a [PLAN END]-----------GPT4-as-Verifier Response-----------Plan is invalid.</p>
<p>Object b craves object c, Object c craves object d, Object d craves object a, Harmony, Planet object a and Province object b".Therefore, you cannot perform the Feast action on object a from object d as your first action.Examples of LLM Output on the Verification Task =========Current State========== object b craves object c, object c craves object d, object d craves object a, harmony, planet object a and province object b. → ==========GPT-4-as-Verifier============ Plan is invalid.</p>
<p>Vertex 11 is connected to vertex 13.There are a total of 14 vertices.Please label every vertex, even if it is disconnected from the rest of the graph.Please provide each vertex's color.Do not skip any vertices.Each color must be provided on a new line in the response and should be formatted as "{VERTEX NUMBER}: {VERTEX COLOR ASSIGNMENT}".Please do not provide anything else in your response.
→You may use at most 3 colors.Vertex 0 is connected to vertex 7.Vertex 0 is connected to vertex 8.Vertex 0 is connected to vertex 9.Vertex 0 is connected to vertex 11.Vertex 1 is connected to vertex 13.Vertex 2 is connected to vertex 9.Vertex 3 is connected to vertex 8.Vertex 3 is connected to vertex 11.Vertex 3 is connected to vertex 12.Vertex 4 is connected to vertex 12.Vertex 5 is connected to vertex 11.Vertex 6 is connected to vertex 9.Vertex 7 is connected to vertex 10.Vertex 7 is connected to vertex 13.Vertex 9 is connected to vertex 11.Vertex 10 is connected to vertex 13.</p>
<p>Table 1 :
1
Accuracy across prompting schemes over 100 instances per domain.
Standard LLM+LLM LLM+Sound CritiqueSamplingPromptingB.F. F.E.FA.E.Fk=15 k=25Game of 245%3%36% 38%N/A28% 42%Graph Coloring16%2%38% 37%34%40% 44%Mystery Blocksworld4%0%10%8%6%9%14%
B.F.-Binary Feedback.F.E.F-First Error Feedback, e.g. the first wrong edge, the first mistaken action, or the non-24 evaluation of the proposed expression.A.E.F-All Error Feedback, e.g.every wrong edge, every mistaken action and error.Note that there is no third critique type for Game of 24 due to the simplicity of the domain.We include two examples of sampling, one at 15 samples, the other at 25, to show that completely ablating critique retains the performance increases of critique.</p>
<p>Table 2 :
2
LLM Verification results.F.P.R. -False Positive Rate, F.N.R -False Negative Rate.</p>
<p>24e numbers and basic arithmetic operations (+ -<em> /) to obtain24.Write your answer first.At the end of your answer, write [ANSWER END]
Backprompt Generated From Sound Verifier Where the Answer Was Malformed (Missing aParenthesis)1 Game of 24→ A.1.1 Prompts Input: 1 1 4 6Answer: All of following examples are built on the same Game of 24 problem, except for the LLM Self-Critique ((6 / (1 -1/4)) = 24[ANSWER END]Feedback: This is not correct. This expression is malformed. Using the numbers 1 1 4 6 please examples. provide a correct expression that evaluates to 24. Write your answer first. At the end of your →→ Raw text format of Game of 24 instance answer, write [ANSWER END] Answer:1 1 4 6A.1.3 Complete Example of Iterative Prompt SequenceBaseline, Direct PromptFull Sequence of 7 (Sound Verifier Generated) Backprompts Stopping at Successful Comple-Use numbers and basic arithmetic operations (+ -</em> /) to obtain 24. You must write your response. Write your answer first, followed by [ANSWER END] → tion of the TaskInput: 1 1 4 6 Use numbers and basic arithmetic operations (+ -* /) to obtain 24. Write your answer first. At Answer: the end of your answer, write [ANSWER END]Example LLM Response(6 / (1 / 4)) = 24Prompt To Elicit Verification→→→Answer:(6 -1) * (4 -1) = 24[ANSWER END]→→Answer:(6 / (1 / (4 -1))) = 24[ANSWER END] → Feedback: This is not correct. This expression evaluates to 18 instead of 24. Using the numbers 1 → 1 4 6 please provide a correct expression that evaluates to 24. Write your answer first. At → Answer: ", the end of your answer, write [ANSWER END] →Answer:(6 * (4 -1)) -1 = 24[ANSWER END]Feedback: This is not correct. This expression evaluates to 17 instead of 24. Using the numbers 1 Backprompt Generated From Sound Verifier (Pass/Fail) 1 4 6 please provide a correct expression that evaluates to 24. Write your answer first. At →→the end of your answer, write [ANSWER END]Feedback: This is not correct. Using the numbers 1 1 4 6 please provide a correct expression that Answer:→evaluates to 24. Write your answer first. At the end of your answer, write [ANSWER END]Answer:Backprompt Generated From Sound Verifier→→→Answer: ",
Please check if the following expression uses only the given numbers (and no others) and evaluates to 24: ((9+10)-(4-9))→Respond only in JSON format as described below: { "evaluation": "number the expression evaluated to", "correct": boolean} Ensure that Python's json.loads can parse this.Do not provide anything else in your response."A.1.2BackpromptsBackpromptGenerated From Self-Critique Feedback: This is not correct.The expression is incorrect.The correct expression should be: (11 -2) * 5 + 8 = 24 With this feedback, please try again.Using the numbers 2 5 8 11 please provide a correct expression that evaluates to 24.Write your answer first.At the end of your answer, write [ANSWER END] Feedback: This is not correct.This expression consists of the numbers 6, 1, 4, but it has to consist of only and exactly ['1', '1', '4', '6'].Using the numbers 1 1 4 6 please provide a correct expression that evaluates to 24.Write your answer first.At the end of your answer, write [ANSWER END] → Input: 1 1 4 6 Answer: ((6 / (1 -1/4)) = 24[ANSWER END] Feedback: This is not correct.This expression is malformed.Using the numbers 1 1 4 6 please provide a correct expression that evaluates to 24.Write your answer first.At the end of your answer, write [ANSWER END] → → Answer: ((6 / (1 -1)) * 4 = 24[ANSWER END] Feedback: This is not correct.This expression is malformed.Using the numbers 1 1 4 6 please provide a correct expression that evaluates to 24.Write your answer first.At the end of your answer, write [ANSWER END] → → Answer: ((6 / (1 -1)) * 4) is undefined because it involves division by zero.A correct solution could be: → (6 / (1 / 4)) * 1 = 24[ANSWER END] Feedback: This is not correct.This expression is malformed.Using the numbers 1 1 4 6 please provide a correct expression that evaluates to 24.Write your answer first.At the end of your answer, write [ANSWER END] → → Answer: (6 / (1 / 4)) = 24[ANSWER END] Feedback: This is not correct.This expression consists of the numbers 6, 1, 4, but it has to consist of only and exactly ['1', '1', '4', '6'].Using the numbers 1 1 4 6 please provide a correct expression that evaluates to 24.Write your answer first.At the end of your answer, write [ANSWER END] Feedback: This is not correct.This expression evaluates to 15 instead of 24.Using the numbers 1 1 4 6 please provide a correct expression that evaluates to 24.Write your answer first.At the end of your answer, write [ANSWER END]</p>
<p>Table A1 :
A1
Distribution of hallucinations during verification task.This table counts the number of instances that featured each type of hallucination and compares it to the total number of erroneous edges encountered across all coloring instances in each subset.Note that the correct column tracks the number of colorings where the ground truth is that the coloring is correct.
HallucinationsColoringVertex Edge Both None Errors CorrectCorrect2972720100Ablated24525241870Non-optimal 186531000Random10265667360LLM264162724018Total10725626129282118</p>
<p>There are a total of 14 vertices.Please label every vertex, even if it is disconnected from the rest of the graph.Please provide each vertex's color.Do not skip any vertices.Each color must be provided on a new line in the response and should be formatted as "{VERTEX NUMBER}: {VERTEX COLOR ASSIGNMENT}".Please do not provide anything else in your response.Verifier confirmed success.'Do not provide anything else in your response.If it is incorrect, please point out which same-color vertices share an edge.Using this feedback, please try again.Please provide each vertex's color.Do not skip any vertices.Each color must be provided on a new line in the response and should be formatted as "{VERTEX NUMBER}: {VERTEX COLOR ASSIGNMENT}".Please do not provide anything else in your response.This is not correct.Using the previously provided graph, please provide a correct coloring.Please provide each vertex's color.Do not skip any vertices.Each color must be provided on a new line in the response and should be formatted as "{VERTEX NUMBER}: {VERTEX COLOR ASSIGNMENT}".Please do not provide anything else in your response.Color the following graph, described as a set of edges, such that no two vertices on the same edge share a color.Vertex 10 is connected to vertex 13.Vertex 11 is connected to vertex 13.There are a total of 14 vertices.Please label every vertex, even if it is disconnected from the rest of the graph.Please provide each vertex's color.Do not skip any vertices.Each color must be provided on a new line in the response and should be formatted as "{VERTEX NUMBER}: {VERTEX COLOR ASSIGNMENT}".Please do not provide anything else in your response.Using this feedback, please try again.Please provide each vertex's color.Do not skip any vertices.Each color must be provided on a new line in the response and should be formatted as "{VERTEX NUMBER}: {VERTEX COLOR ASSIGNMENT}".Please do not provide anything else in your response.Using this feedback, please try again.Please provide each vertex's color.Do not skip any vertices.Each color must be provided on a new line in the response and should be formatted as "{VERTEX NUMBER}: {VERTEX COLOR ASSIGNMENT}".Please do not provide anything else in your response.Using this feedback, please try again.Please provide each vertex's color.Do not skip any vertices.Each color must be provided on a new line in the response and should be formatted as "{VERTEX NUMBER}: {VERTEX COLOR ASSIGNMENT}".Please do not provide anything else in your response.Vertices 2 and 9 share an edge and are both colored with Color2.Vertices 3 and 11 share an edge and are both colored with Color3.Vertices 7 and 13 share an edge and are both colored with Color3.Using this feedback, please try again.Please provide each vertex's color.Do not skip any vertices.Each color must be provided on a new line in the response and should be formatted as "{VERTEX NUMBER}: {VERTEX COLOR ASSIGNMENT}".Please do not provide anything else in your response.Using this feedback, please try again.Please provide each vertex's color.Do not skip any vertices.Each color must be provided on a new line in the response and should be formatted as "{VERTEX NUMBER}: {VERTEX COLOR ASSIGNMENT}".Please do not provide anything else in your response.
10: Color3 2: Color2 6: Color3 12: Color111: Color3 → → 3: Color3 7: Color2 13: Color112: Color3 → → 4: Color1 8: Color3→ 13: Color1 → → → 5: Color2 9: Color1You may use at most 3 colors. Vertex 0 is connected to vertex 7. If it is, say '→ → → 0: Color1 0: Color1 6: Color3 7: Color3 10: Color3 11: Color2 A.2.5 LLM as VerifierVertex 0 is connected to vertex 8. 1: Color1 1: Color2 8: Color2 12: Color1Vertex 0 is connected to vertex 9. Vertex 0 is connected to vertex 11. Vertex 1 is connected to vertex 13. Vertex 2 is connected to vertex 9. 3: Color1 5: Color1 5: Color1 12: Color2 The coloring is incorrect. Vertices 0 and 11 share an edge and are both colored with Color1. A.2.3 Backprompts 0: Color1 1: Color1 2: Color1 2: Color2 3: Color2 4: Color3 2: Color2 3: Color3 4: Color1 9: Color3 13: Color3 In this section, multiple equals signs in a row ("===") are dividers between separate examples, not 10: Color1 11: Color2 This is incorrect. Feedback: parts of the examples.Vertex 3 is connected to vertex 8. Vertex 3 is connected to vertex 11. Backprompt Generated From Self-Critique 4: Color1 5: Color2 6: Color1 7: Color2 6: Color1 7: Color3 13: Color1 Vertices 7 and 13 share an edge and are both colored with Color2. Examples of LLM Output on the Verification TaskVertex 3 is connected to vertex 12. Vertex 4 is connected to vertex 12. Vertex 5 is connected to vertex 11. Vertex 6 is connected to vertex 9. Vertex 7 is connected to vertex 10. Vertex 7 is connected to vertex 13. Vertex 9 is connected to vertex 11. Vertex 10 is connected to vertex 13. Vertex 11 is connected to vertex 13. → → → This is incorrect. Feedback: Vertices 0 and 3 share an edge and are both colored with Color1. Vertices 9 and 11 share an edge and are both colored with Color3. → → → Backprompt Generated From Sound Verifier (Pass/Fail) 6: Color2 7: Color2 8: Color2 9: Color3 10: Color3 11: Color3 12: Color2 13: Color3 This is incorrect. Feedback: Vertices 0 and 3 share an edge and are both colored with Color1. Vertices 9 and 11 share an edge and are both colored with Color3. → 8: Color3 9: Color3 10: Color1 11: Color3 12: Color1 13: Color3 This is incorrect. Feedback: Vertices 0 and 5 both have Color1 and share an edge. Vertices 0 and 11 both have Color3 and share an edge. Vertices 7 and 10 both have Color2 and share an edge. Vertices 11 and 13 both have Color3 and share an edge. → → 8: Color2 9: Color3 10: Color2 This is incorrect. Feedback: → The vertices 1 and 4 are both color 0 and share an edge. Vertices 2 and 9 share an edge and are both colored with Color2. Similarly, vertices 11 and 13 → ============================ share an edge and are both colored with Color2. → The vertices 9 and 12 are both colored with Color 1 and they share an edge. 11: Color2 ============================ 12: Color3 13: Color1 This is incorrect. Feedback: Vertices 0 and 11 share an edge and are both colored with Color1. Vertices 2 and 9 share an edge and are both colored with Color2. Vertices 3 and 12 share an edge and are both colored with Color3. → → → → → 0: Color1 1: Color2 2: Color2 3: Color3 4: Color1 5: Color2 Vertex 1: Color 2 is connected to Vertex 2: Color 2. 0: Color1 ============================ 1: Color2 Vertex 5 and vertex 10 are both Color 2 and share an edge. 2: Color2 ============================ 3: Color3 Same-color vertices 0 and 1 share an edge. 4: Color1 ============================ 5: Color2 Vertex 0 (Color 0) and vertex 3 (Color 0) share an edge. Vertex 1 (Color 1) and vertex 2 (Color 1) 6: Color3 share an edge. Vertex 4 (Color 1) and vertex 8 (Color 1) share an edge. Vertex 2 (Color 1) and 7: Color3 vertex 4 (Color 1) share an edge. Vertex 4 (Color 1) and vertex 11 (Color 0) share an edge. 8: Color2 9: Color1 Vertex 12 (Color 2) and vertex 15 (Color 2) share an edge.→ → → 6: Color3 10: Color2 Example LLM Response → → 7: Color3 11: Color3→ 0: Color1 0: Color1 8: Color2 12: Color1→ 0: Color1 1: Color1 1: Color2 9: Color1 13: Color1 0: Color1 1: Color1 2: Color2 2: Color2 10: Color2 1: Color1 2: Color2 3: Color2 3: Color2 11: Color3 2: Color1 3: Color2 3: Color2 4: Color1 4: Color3 12: Color2 Backprompt Generated From Sound Verifier (First) 4: Color2 5: Color1 5: Color3 13: Color1 4: Color1 5: Color1 6: Color1 6: Color3 5: Color2 6: Color1 7: Color2 7: Color2 This is incorrect. Feedback: 6: Color2 7: Color2 8: Color3 8: Color3 7: Color2 8: Color3 9: Color3 9: Color1 → 8: Color3 9: Color3 10: Color3 10: Color3 → 9: Color3 10: Color1 11: Color2 11: Color3 → 10: Color3 11: Color2 12: Color3 12: Color1 11: Color3 12: Color3 13: Color1 13: Color1 → 0: Color1 12: Color3 13: Color3 → 1: Color2 13: Color1 This is incorrect. Feedback: This is incorrect. Feedback: → 2: Color3This is incorrect. Feedback: Vertices 0 and 1 both have Color1 and share an edge. Vertices 0 and 5 also both have Color1 and Vertices 11 and 13 both have Color3 and share an edge. 3: Color2Vertices 2 and 3 are both colored with Color2 and share an edge. Vertices 7 and 11 are both share an edge. Vertices 0 and 6 also both have Color1 and share an edge. Vertices 7 and 11 0: Color1 4: Color2colored with Color2 and share an edge. Vertices 9 and 13 are both colored with Color3 and both have Color2 and share an edge. Vertices 9 and 11 both have Color2 and share an edge. 1: Color2 5: Color1 Prompt To Elicit Verification share an edge. → 2: Color2 6: Color2→ 3: Color3 7: Color3 The following graph, described as a set of edges, has an optimal coloring number of 3: → → 4: Color1 8: Color3 Vertex 0 is connected to vertex 7. → → 5: Color2 9: Color2 Vertex 0 is connected to vertex 8. → → 0: Color1 6: Color3 10: Color1 Vertex 0 is connected to vertex 9. → 1: Color2 7: Color2 11: Color3 Vertex 0 is connected to vertex 11. 0: Color1 2: Color2 8: Color3 12: Color1 Vertex 1 is connected to vertex 13. 0: Color1 1: Color2 3: Color2 9: Color3 13: Color1 Vertex 2 is connected to vertex 9. 1: Color1 2: Color2 4: Color3 10: Color1 Vertex 3 is connected to vertex 8. 2: Color2 3: Color2 5: Color3 11: Color2 This is incorrect. Feedback: Vertex 3 is connected to vertex 11. 3: Color3 4: Color2 6: Color3 12: Color3 Vertices 3 and 9 share an edge and are both colored with Color2. Vertex 3 is connected to vertex 12. 4: Color3 5: Color3 7: Color2 13: Color1 Vertex 4 is connected to vertex 12. 5: Color3 6: Color3 8: Color3 Vertex 5 is connected to vertex 11. Vertex 6 is connected to vertex 9. Vertex 7 is connected to vertex 10. Vertex 7 is connected to vertex 13. Vertex 9 is connected to vertex 11. Vertex 10 is connected to vertex 13. Vertex 11 is connected to vertex 13. Please check if this coloring is correct:0: Color1 1: Color1 2: Color1 3: Color2 4: Color1 5: Color2 6: Color2 7: Color2 8: Color3 9: Color3 → 6: Color3 7: Color3 9: Color1 → This is incorrect. Feedback: You may use at most 3 colors. 7: Color3 8: Color2 10: Color3 → Vertices 0 and 11 share an edge and are both colored with Color1. Vertices 3 and 8 share an edge Vertex 0 is connected to vertex 7. 8: Color2 9: Color3 11: Color1 → → and are both colored with Color3. Vertices 7 and 13 share an edge and are both colored with Vertex 0 is connected to vertex 8. 9: Color1 10: Color1 12: Color1 → Color2. Vertex 0 is connected to vertex 9. 10: Color2 11: Color2 13: Color2 0: Color1 Vertex 0 is connected to vertex 11. 11: Color2 12: Color3 Using this feedback, please try again. Please provide each vertex's color. Do not skip any 1: Color2 Vertex 1 is connected to vertex 13. 12: Color1 13: Color1 This is incorrect. Feedback: vertices. Each color must be provided on a new line in the response and should be formatted as → 2: Color3 Vertex 2 is connected to vertex 9. 13: Color1 Vertices 0 and 11 share an edge and both are colored with Color1. "{VERTEX NUMBER}: {VERTEX COLOR ASSIGNMENT}". Please do not provide anything else in your → 3: Color2 Vertex 3 is connected to vertex 8. This is incorrect. Feedback: response. → 4: Color3 Vertex 3 is connected to vertex 11. This is incorrect. Feedback: Vertices 0 and 8 both have Color1 and share an edge. Vertices 0 and 11 both have Color1 and share Using this feedback, please try again. Please provide each vertex's color. Do not skip any 5: Color1 Vertex 3 is connected to vertex 12. Vertices 0 and 9 share an edge and are both colored with Color1. Vertices 0 and 11 share an edge an edge. Vertices 3 and 8 both have Color2 and share an edge. Vertices 3 and 11 both have vertices. Each color must be provided on a new line in the response and should be formatted as → 0: Color1 6: Color2 Vertex 4 is connected to vertex 12. "{VERTEX NUMBER}: {VERTEX COLOR ASSIGNMENT}". Please do not provide anything else in your → 1: Color2 7: Color3 Vertex 5 is connected to vertex 11. response. → 2: Color2 8: Color2 Vertex 6 is connected to vertex 9. 3: Color3 9: Color1 Vertex 7 is connected to vertex 10. 0: Color1 4: Color1 10: Color2 Vertex 7 is connected to vertex 13. Vertex 9 is connected to vertex 11. 1: Color2 5: Color2 11: Color3
→Vertex 9 and vertex 11 were both colored Color3 despite being connected by an edge.This is wrong.Please recolor.Please provide each vertex's color.Do not skip any vertices.Each color must be provided on a new line in the response and should be formatted as "{VERTEX NUMBER}: {VERTEX COLOR ASSIGNMENT}".Please do not provide anything else in your response.→→BackpromptGenerated From Sound Verifier (Full) Vertex 9 and vertex 11 were both colored Color3 despite being connected by an edge.Vertex 10 and vertex 13 were both colored Color3 despite being connected by an edge.Vertex 11 and vertex 13 were both colored Color3 despite being connected by an edge.→ → This is wrong.Please recolor.Please provide each vertex's color.Do not skip any vertices.Each color must be provided on a new line in the response and should be formatted as "{VERTEX NUMBER}: {VERTEX COLOR ASSIGNMENT}".Please do not provide anything else in your response.→ → A.2.4 Complete Example of Iterative Prompt Sequence Full Sequence of 15 (LLM-Generated) Backprompts → Using this feedback, please try again.Please provide each vertex's color.Do not skip any vertices.Each color must be provided on a new line in the response and should be formatted as "{VERTEX NUMBER}: {VERTEX COLOR ASSIGNMENT}".Please do not provide anything else in your response.→ → Using this feedback, please try again.Please provide each vertex's color.Do not skip any vertices.Each color must be provided on a new line in the response and should be formatted as "{VERTEX NUMBER}: {VERTEX COLOR ASSIGNMENT}".Please do not provide anything else in your response.and are both colored with Color2.Vertices 1 and 13 share an edge and are both colored with Color1.Vertices 7 and 13 share an edge and are both colored with Color3.→ → → → Using this feedback, please try again.Please provide each vertex's color.Do not skip any vertices.Each color must be provided on a new line in the response and should be formatted as "{VERTEX NUMBER}: {VERTEX COLOR ASSIGNMENT}".Please do not provide anything else in your response.→ → Using this feedback, please try again.Please provide each vertex's color.Do not skip any vertices.Each color must be provided on a new line in the response and should be formatted as "{VERTEX NUMBER}: {VERTEX COLOR ASSIGNMENT}".Please do not provide anything else in your response.Color2 and share an edge.Vertices 7 and 13 both have Color3 and share an edge.Vertices 9 and 11 both have Color3 and share an edge.Vertices 11 and 13 both have Color2 and share an edge.→ → → → → Using this feedback, please try again.Please provide each vertex's color.Do not skip any vertices.Each color must be provided on a new line in the response and should be formatted as "{VERTEX NUMBER}: {VERTEX COLOR ASSIGNMENT}".Please do not provide anything else in your response.Using this feedback, please try again.Please provide each vertex's color.Do not skip any vertices.Each color must be provided on a new line in the response and should be formatted as "{VERTEX NUMBER}: {VERTEX COLOR ASSIGNMENT}".Please do not provide anything else in your response.→ → → Using this feedback, please try again.Please provide each vertex's color.Do not skip any vertices.Each color must be provided on a new line in the response and should be formatted as "{VERTEX NUMBER}: {VERTEX COLOR ASSIGNMENT}".Please do not provide anything else in your response.→ This is incorrect.Feedback: Vertices 0 and 9 share an edge and are both colored with Color1.Vertices 7 and 11 share an edge and are both colored with Color3.Vertices 1 and 13 share an edge and are both colored with Color2.→ → Using this feedback, please try again.Please provide each vertex's color.Do not skip any vertices.Each color must be provided on a new line in the response and should be formatted as "{VERTEX NUMBER}: {VERTEX COLOR ASSIGNMENT}".Please do not provide anything else in your response.</p>
<p>Table A2 :
A2
This table presents the verification and critique accuracy of LLM-as-verifier across five different kinds of plans over 100 instances.
Binary Verification Type IdentifiedCritique GenerationCorrect3/100 (3%)3/100 (3%)3/100 (3%)Inexecutable100/100 (100%)100/100 (100%) 24/100 (24%)Non Goal Reaching98/100 (98%)12/100 (12%)12/100 (12%)Random100/100 (100%)100/100 (100%) 59/100 (100%)Correct0/3 (0%)0/3 (0%)0/3 (0%)LLMInexecutable89/89 (100%)89/89 (100%)12/89 (13.48%)Non Goal Reaching 8/8 (100%)0/8 (0%)0/8 (0%)
→Vertex 10 (color 1) is connected to vertex 12 (color 2), sharing an edge.Therefore, this assignment is correctly colored.→Vertex11(color 1) isn't connected to vertex 10 (color 1), hence, this assignment is correctly colored.→</p>
<p>My goal is to have that object a craves object c and object d craves object a.
→[STATEMENT]As initial conditions I have that, object b craves object c, object c craves object d, object dcraves object a, harmony, planet object a and province object b.My plan is as follows:[PLAN]feast object a from object dsuccumb object aattack object aovercome object a from object cfeast object c from object dsuccumb object cattack object covercome object c from object a[PLAN END]Verify whether the above plan is valid. If it is valid, please say "Plan is valid." and nothing→else. If it is invalid, please say "Plan is invalid." and then provide feedback on why the→plan fails.Prompt To Elicit Verification (Format based)I am playing with a set of objects. Here are the actions I can doAttack objectFeast object from another objectSuccumb objectOvercome object from another objectI have the following restrictions on my actions:To perform Attack action, the following facts need to be true: Province object, Planet object,Harmony.
→</p>
<p>https://www.sympy.org/en/index.html
https://pypi.org/project/grinpy/</p>
<p>Suriya Marah I Abdin, Varun Gunasekar, Jerry Chandrasekaran, Mert Li, Yuksekgonul, arXiv:2310.15511Rahee Ghosh Peshawaria, Ranjita Naik, and Besmira Nushi. Kitab: Evaluating llms on constraint satisfaction for information retrieval. 2023arXiv preprint</p>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Exploring distributional shifts in large language models for code analysis. Shushan Arakelyan, Jyoti Rocktim, Yi Das, Xiang Mao, Ren, arXiv:2303.091282023arXiv preprint</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023arXiv preprint</p>
<p>Teaching large language models to self-debug. Xinyun Chen, Maxwell Lin, Nathanael Schärli, Denny Zhou, arXiv:2304.051282023arXiv preprint</p>
<p>Lm vs lm: Detecting factual errors via cross examination. Roi Cohen, May Hamri, Mor Geva, Amir Globerson, arXiv:2305.132812023arXiv preprint</p>
<p>Implementation Challenges. Dimacs, Dimacs, </p>
<p>Improving factuality and reasoning in language models through multiagent debate. Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, Igor Mordatch, arXiv:2305.143252023arXiv preprint</p>
<p>Faith and fate: Limits of transformers on compositionality. Nouha Dziri, Ximing Lu, Melanie Sclar, Lorraine Xiang, Liwei Li, Bill Jiang, Sean Yuchen Lin, Peter Welleck, Chandra West, Bhagavatula, Le Ronan, Jena D Bras, Soumya Hwang, Xiang Sanyal, Allyson Ren, Zaid Ettinger, Yejin Harchaoui, Choi, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Large language models are not abstract reasoners. Gaël Gendron, Qiming Bao, Michael Witbrock, Gillian Dobbie, arXiv:2305.195552023arXiv preprint</p>
<p>Reasoning with Language Model is Planning with World Model. Shibo Hao, Yi Gu, Haodi Ma, Joshua Hong, Zhen Wang, Daisy Wang, Zhiting Hu, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Houda Bouamor, Juan Pino, Kalika Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational LinguisticsDecember 2023</p>
<p>A closer look at the self-verification abilities of large language models in logical reasoning. Ruixin Hong, Hongming Zhang, Xinyu Pang, Dong Yu, Changshui Zhang, arXiv:2311.079542023arXiv preprint</p>
<p>VAL: Automatic plan validation, continuous effects and mixed initiative planning using PDDL. Richard Howey, Derek Long, Maria Fox, 16th IEEE International Conference on Tools with Artificial Intelligence. IEEE2004</p>
<p>Large language models cannot self-correct reasoning yet. Jie Huang, Xinyun Chen, Swaroop Mishra, Steven Huaixiu, Adams Wei Zheng, Xinying Yu, Denny Song, Zhou, arXiv:2310.017982023arXiv preprint</p>
<p>Inner monologue: Embodied reasoning through planning with language models. Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, arXiv:2207.056082022arXiv preprint</p>
<p>. IPC. International planning competition. 1998</p>
<p>Weisen Jiang, Han Shi, Longhui Yu, Zhengying Liu, Yu Zhang, Zhenguo Li, James T Kwok, arXiv:2308.07758Backward reasoning in large language models for verification. 2023arXiv preprint</p>
<p>On the role of large language models in planning. Subbarao Kambhampati, Karthik Valmeekam, Matthew Marquez, Lin Guan, July 2023</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Commongen: A constrained text generation challenge for generative commonsense reasoning. Wangchunshu Bill Yuchen Lin, Ming Zhou, Pei Shen, Chandra Zhou, Yejin Bhagavatula, Xiang Choi, Ren, Findings of the Association for Computational Linguistics: EMNLP 2020. 2020</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, arXiv:2303.176512023arXiv preprint</p>
<p>Embers of autoregression: Understanding large language models through the problem they are trained to solve. Thomas Mccoy, Shunyu Yao, Dan Friedman, Matthew Hardy, Thomas L Griffiths, arXiv:2309.136382023arXiv preprint</p>
<p>Pddl-the planning domain definition language. Drew Mcdermott, Malik Ghallab, Adele E Howe, Craig A Knoblock, Ashwin Ram, Manuela M Veloso, Daniel S Weld, David E Wilkins, 1998</p>
<p>Scaling language models: Methods, analysis &amp; insights from training gopher. Sebastian Jack W Rae, Trevor Borgeaud, Katie Cai, Jordan Millican, Francis Hoffmann, John Song, Sarah Aslanides, Roman Henderson, Susannah Ring, Young, arXiv:2112.114462021arXiv preprint</p>
<p>Data contamination through the lens of time. Manley Roberts, Himanshu Thakur, Christine Herlihy, Colin White, Samuel Dooley, arXiv:2310.106282023arXiv preprint</p>
<p>Mathematical discoveries from program search with large language models. Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, Pawan Kumar, Emilien Dupont, Francisco Jr Ruiz, Jordan S Ellenberg, Pengming Wang, Omar Fawzi, Nature. 2023</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Shunyu Karthik R Narasimhan, Yao, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Pddl planning with pretrained large language models. Tom Silver, Varun Hariprasad, Reece S Shuttleworth, Nishanth Kumar, Tomás Lozano-Pérez, Leslie Pack, Kaelbling , NeurIPS 2022 foundation models for decision making workshop. 2022</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam Adam R Brown, Aditya Santoro, Adrià Gupta, Garriga-Alonso, Transactions on Machine Learning Research. 2023</p>
<p>Large language models fail on trivial alterations to theory-of-mind tasks. Tomer Ullman, arXiv:2302.083992023arXiv preprint</p>
<p>Planbench: An extensible benchmark for evaluating large language models on planning and reasoning about change. Karthik Valmeekam, Matthew Marquez, Alberto Olmo, Sarath Sreedharan, Subbarao Kambhampati, Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2023</p>
<p>On the planning abilities of large language models -a critical investigation. Karthik Valmeekam, Matthew Marquez, Sarath Sreedharan, Subbarao Kambhampati, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Emergent analogical reasoning in large language models. Taylor Webb, Keith J Holyoak, Hongjing Lu, Nature Human Behaviour. 792023</p>
<p>Generating sequences by learning to self-correct. Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, Yejin Choi, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Large language models are better reasoners with self-verification. Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, Jun Zhao, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023</p>
<p>Hotpotqa: A dataset for diverse, explainable multi-hop question answering. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, Christopher D Manning, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language Processing2018</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik R Narasimhan, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, arXiv:2210.03629React: Synergizing reasoning and acting in language models. 2022arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8438 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8438</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8438</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-150.html">extraction-schema-150</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <p><strong>Paper ID:</strong> paper-2f3efe44083af91cef562c1a3451eee2f8601d22</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/2f3efe44083af91cef562c1a3451eee2f8601d22" target="_blank">WebGPT: Browser-assisted question-answering with human feedback</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> GPT-3 is fine-tune to answer long-form questions using a text-based web-browsing environment, which allows the model to search and navigate the web, and the best model is obtained, which is preferred by humans 56% of the time to those of the authors' human demonstrators, and 69%" to the highest-voted answer from Reddit.</p>
                <p><strong>Paper Abstract:</strong> We fine-tune GPT-3 to answer long-form questions using a text-based web-browsing environment, which allows the model to search and navigate the web. By setting up the task so that it can be performed by humans, we are able to train models on the task using imitation learning, and then optimize answer quality with human feedback. To make human evaluation of factual accuracy easier, models must collect references while browsing in support of their answers. We train and evaluate our models on ELI5, a dataset of questions asked by Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior cloning, and then performing rejection sampling against a reward model trained to predict human preferences. This model's answers are preferred by humans 56% of the time to those of our human demonstrators, and 69% of the time to the highest-voted answer from Reddit.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8438.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8438.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WebGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>WebGPT: Browser-assisted question-answering with human feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of language-model agents (fine-tuned GPT-3 variants) trained to use a text-based web-browsing environment to search, navigate, quote web pages as references, and compose long-form answers optimized with human feedback (behavior cloning, reward modeling, PPO, and rejection sampling).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>WebGPT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A fine-tuned GPT-3 model trained to operate a constrained text-based web browser via discrete commands (Search, Click link, Find in page, Quote, Scroll, End: Answer). It collects quoted extracts as references and then composes a final answer given those references; training methods include behavior cloning (BC), reward modeling (RM), reinforcement learning (PPO), and rejection sampling (best-of-n) against an RM.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 family (fine-tuned variants: 760M, 13B, 175B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained autoregressive transformer (GPT-3 family) fine-tuned with supervised demonstrations and human preference signals; evaluated in three sizes (760M, 13B, 175B) and combined with RM and inference-time rejection sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ELI5 long-form question answering (and TruthfulQA, TriviaQA evaluations)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given an open-ended question (primarily from ELI5), the agent may perform multi-step web searches and navigation actions, quote supporting extracts (references), then produce a paragraph-length answer supported by those references. Also evaluated on short-form adversarial TruthfulQA and TriviaQA.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>web-assisted long-form question answering / web navigation</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external retrieval + prompt-level episodic summary (retrieval-augmented)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>During browsing the agent issues Quote actions; quoted page title, domain and extract are recorded as references. The environment also includes a summary of past browsing actions and the current page text in the prompt; at answer time the collected references are concatenated into the prompt for generation. The agent has no persistent internal state beyond what is included in the provided prompt summary.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Collected quoted extracts (reference items containing page title, domain, extract), the text of current page and a summary of past actions (action history), and search result pages returned by Bing.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Prompt concatenation: the recorded references and action summary are appended to the model's prompt for subsequent steps (including final answer composition); browsing/referencing uses live Bing search API and on-the-fly page fetches.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>175B WebGPT best-of-64 (uses web browsing + references + rejection sampling) produced answers preferred to human demonstrators 56% of the time and preferred to ELI5 highest-voted Reddit answers 69% of the time; on TruthfulQA WebGPT answers were truthful 75% of the time and truthful+informative 54% of the time (best model, truncated answers).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Extensive comparisons between training/inference methods: rejection sampling (best-of-n) yields large gains over BC alone (175B best-of-64 BC preferred ~68% over BC baseline); RL (PPO) yields smaller gains (175B RL preferred ~58% over BC) and provides little extra benefit when combined with rejection sampling. No explicit ablation removing web access / references vs. using them is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Providing a web-browsing interface and forcing the model to collect explicit references improves answer quality and truthfulness versus the pretrained base model; rejection sampling (many attempts + RM ranking) is an effective, compute-tradeoff way to optimize against a reward model; RL yields modest gains but can be outperformed by rejection sampling and complicates tuning; references make human evaluation of factual accuracy tractable.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>The agent has no persistent internal memory beyond prompt-provided summaries (limited episodic memory). References can be cherry-picked (incentivized to pick convincing but possibly biased/unreliable sources), and WebGPT sometimes quotes unreliable pages (especially out-of-distribution). The method increases perceived authority of answers (automation bias) despite remaining failure modes. No direct ablation isolates the causal effect of references vs. no-references on final performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'WebGPT: Browser-assisted question-answering with human feedback', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8438.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8438.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>References (quoted extracts)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Explicit quoted references collected during browsing</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The mechanism by which WebGPT stores external knowledge during an episode: when the agent issues a Quote action the page title, domain and extract are recorded and later provided as references in the answer prompt and used by human evaluators to judge support for claims.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Quoted References (WebGPT mechanism)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A memory mechanism implemented as explicit stored document extracts (title, domain, extract) collected by the agent during browsing and concatenated into the prompt for the final answer and for human evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Support-aware answer composition / human evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Store supporting evidence during multi-step web browsing and provide it as explicit references to both the model (for answer composition) and human evaluators (for factual adjudication).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>external retrieval / evidence storage</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external document store (episodic per-episode buffer)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Quote actions append a structured triple (page title, domain, extract) to an episode buffer; at answer phase, these items are serialized and concatenated to the prompt so the model conditions on them when generating the final answer and human labelers can inspect them.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Page title, domain, and the exact extract (text span) quoted; references have a maximum total length limit and at least one reference is required to compose an answer.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Prompt concatenation at answer time (no learned retrieval over stored references beyond ordering in buffer); also used by reward model during RM scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>No direct quantitative ablation reported that removes references entirely; authors argue references are central to human evaluation (make factual checking feasible) and likely reduce hallucinations (citing related prior work), but also note incentive to cherry-pick and the absence of direct ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Explicit references enable more accurate, less noisy human feedback and greater transparency; they are essential for making factual accuracy evaluable at scale, and they help the reward model and training pipeline produce higher-quality answers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Incentivizes cherry-picking of convincing but potentially untrustworthy sources; does not guarantee global fairness or completeness of evidence; models sometimes quote unreliable sources under distribution shift.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'WebGPT: Browser-assisted question-answering with human feedback', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8438.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8438.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompted action summary</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt-provided summary of past browsing actions (episodic action history)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Because the model has no built-in memory of previous steps, the environment records a summary of past actions which is included in each prompt to provide the model with episodic context across multi-step browsing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Action-summary prompt (WebGPT environment feature)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A prompt-level episodic memory: the environment summarizes previous browsing commands and relevant page text and includes that summary in subsequent model prompts so the model can behave coherently across multi-step episodes despite lacking internal persistent memory.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multi-step web navigation within a single episode</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Provide the model with a representation of prior steps (searches, clicks, scrolls, quotes) so it can plan next actions despite not retaining internal state across prompt calls.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>prompt-level episodic memory / web navigation</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>working/episodic memory conveyed via prompt</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Environment composes a textual summary of the current state, including question, current page text at cursor, and a summary of past actions; this text is passed to the model on each step, giving the model access to past-step information only via the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Textual summary of past actions, current page content at cursor position, search results indicators; not a learned persistent state.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Recency/explicit inclusion: summary is included verbatim in each prompt (no separate retrieval operation beyond prompt construction).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Paper contrasts human demonstrators (who naturally remember prior steps) to the model (which lacks internal memory but receives the summary); no direct ablation comparing with/without the summary is reported, but authors state the summary is necessary because the base pretrained model lacks knowledge of command format and episode history.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A prompt-provided summary is an effective mechanism to give a stateless LM the needed episodic context to perform multi-step browsing; it is how the environment supplies 'memory' without changing model architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Limited capacityâ€”only what is summarized is available; increases prompt size; may omit details; not a substitute for richer persistent memory mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'WebGPT: Browser-assisted question-answering with human feedback', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8438.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8438.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>REALM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>REALM: Retrieval-augmented language model pre-training</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A differentiable retrieval-augmented approach that trains a retriever and language model end-to-end so the LM conditions on retrieved documents during pretraining and downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>REALM: Retrieval-augmented language model pre-training</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>REALM</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A retrieval-augmented language model where retrieval is implemented with a learned dense retriever integrated into the LM training pipeline (differentiable retrieval).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Open-domain question answering / knowledge-intensive tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Retrieve relevant passages from a large corpus and condition generation/predictions on those passages.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>retrieval-augmented question answering (differentiable retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external document corpus with learned retriever (differentiable)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Dense retrieval via learned embeddings; retrieved passages are fed into the LM as context.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>External corpus passages represented by dense embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Nearest-neighbor in embedding space using inner-product/embedding similarity; retrieval integrated into training (end-to-end).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Mentioned in related work as an approach that focuses on retrieval as a differentiable process; paper contrasts this family with WebGPT's use of an external search engine which is non-differentiable and more up-to-date.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Differentiable retrieval allows end-to-end optimization and fast retrieval learning, but cannot directly incorporate non-differentiable external processes like commercial search engines and is less interpretable than explicit web-browsing references.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Less suitable for live web access and non-differentiable retrieval sources; interpretability and up-to-dateness limitations relative to search-engine-based approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'WebGPT: Browser-assisted question-answering with human feedback', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8438.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8438.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RAG: Retrieval-augmented generation for knowledge-intensive NLP tasks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented generation method that retrieves documents (via dense embeddings) and conditions a generative model on them for improved knowledge-intensive generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Retrieval-augmented generation for knowledge-intensive NLP tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Retrieval-Augmented Generation that combines a dense retriever with a seq2seq generator to condition generation on retrieved documents; retrieval is differentiable at training time (approximate) and used to augment generation at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Knowledge-intensive QA and generation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Retrieve relevant documents and generate answers conditioned on them.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>retrieval-augmented QA (differentiable retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external passage store with learned retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Retrieve K nearest passages by embedding similarity and condition the generator on retrieved passages.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Retrieved document passages (text) and their embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Embedding-based nearest-neighbor search (semantic search).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Mentioned as a contrasting family of approaches focusing on differentiable retrieval; the paper notes RAG/REALM/DPR differ from WebGPT by formulating retrieval as differentiable and trained end-to-end.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Differentiable retrieval methods are powerful for many benchmarks; WebGPT argues for complementary benefits of using live search engines and explicit references for human-evaluable factuality.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Cannot directly leverage search engines and may be less interpretable regarding sources compared to explicit quoted references.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'WebGPT: Browser-assisted question-answering with human feedback', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8438.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8438.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DPR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dense Passage Retrieval (DPR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dense embedding-based passage retriever trained with contrastive learning to retrieve relevant passages for open-domain QA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dense passage retrieval for open-domain question answering</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>DPR</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A dense embedding retriever that uses a learned encoder for questions and passages and is trained with contrastive loss to retrieve relevant passages for QA systems.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Open-domain QA retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Retrieve relevant passages from a large corpus to support an answer-generating model.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>retrieval (dense) + QA</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external document corpus with learned dense retriever</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Embedding-based nearest-neighbor retrieval over a passage corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Dense embeddings of passages plus raw passage text.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Semantic search via inner-product on embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Mentioned as part of the family of retrieval-augmented methods contrasted with WebGPT's search-engine-based approach; no direct experimental comparison reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>DPR provides strong retrieval for short-answer QA tasks; WebGPT situates itself as complementary by emphasizing a human-like search-and-quote workflow for long-form answers and human evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Focuses on differentiable retrieval and fixed corpora, not live web search; less transparent source citation workflow compared to explicit quoting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'WebGPT: Browser-assisted question-answering with human feedback', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8438.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8438.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Yuan et al. (Interactive agents)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Interactive machine comprehension with information seeking agents (Yuan et al., 2019)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An RL-based approach where agents perform search and scrolling actions over documents for reading comprehension; an example of framing retrieval/browsing as an RL problem.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Interactive machine comprehension with information seeking agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Yuan et al. information-seeking agents</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agents trained with RL to issue search and navigation actions to gather information for QA, demonstrating web/document-level action spaces similar to WebGPT's environment.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Reading comprehension with interactive search/navigation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Use search and navigation actions to find evidence in documents and answer comprehension questions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>interactive document/web QA (RL agent)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>episodic browsing state + retrieved passages</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Agent interacts with documents via actions (search, scroll) and conditions on retrieved text; trained with RL to maximize QA performance.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Visited text passages and navigation state within the episode.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Action-driven retrieval (search queries and navigation), not direct semantic-index retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Cited as prior work that frames web-level QA as an RL problem; WebGPT contrasts by using imitation learning + RM + rejection sampling and a commercial search API.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Interactive agents can be trained with RL to seek information; WebGPT inherits the action-space idea but couples it with human demonstrations and reward modeling for improved human-evaluated answer quality.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Prior RL approaches can be sample-inefficient; WebGPT highlights sample-efficiency improvements via demonstrations and supplementary answer-only episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'WebGPT: Browser-assisted question-answering with human feedback', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8438.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8438.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Adolphs et al.</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Boosting search engines with interactive agents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Work setting up RL agents that perform sequences of search queries for short-form QA; another example of agents using multi-step retrieval/search actions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Boosting search engines with interactive agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Adolphs et al. interactive search agents</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agents that learn sequences of search queries to improve answer retrieval, trained via RL or behavior cloning on synthetic sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Search-query sequencing for short-form QA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Issue sequences of search queries to a search engine to retrieve and synthesize answers to short questions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>web search agents / interactive retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>episodic history of queries and retrieved snippets</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Action-history and retrieved snippets used to inform subsequent search actions and final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Past queries and retrieved results within the episode.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>History-conditioned querying and retrieval from search results.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Cited as related work; WebGPT differs by scaling to long-form answers and emphasizing human feedback and explicit reference recording.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Interactive search agents can improve retrieval by planning query sequences; WebGPT demonstrates complementary strengths for long-form, reference-backed answers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Short-form focus and synthetic BC data in some prior work contrast with WebGPT's human demonstrations and human-evaluated outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'WebGPT: Browser-assisted question-answering with human feedback', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8438.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8438.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Krishna et al. 2021</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hurdles to progress in long-form question answering</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A study using retrieval-augmented models for long-form QA on ELI5, emphasizing difficulties of automated metrics and motivating human comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Hurdles to progress in long-form question answering</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Krishna et al. LFQA models</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Work applying retrieval + generation pipelines to long-form QA (ELI5) and highlighting issues with automated evaluation metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Long-form question answering (ELI5)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Produce paragraph-length explanations to open-ended ELI5 questions using retrieval+generation pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>long-form question answering (retrieval-augmented)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieved documents as external memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Retrieve documents/passages and condition answer generation on them.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Retrieved passages and supporting documents.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Embedding-based and/or IR retrieval methods used in prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>WebGPT compares favorably: Krishna et al.'s best model's answers were preferred only 23% of the time to ELI5 references, whereas WebGPT's best model was preferred 69% of the time. Krishna et al. used less compute and different methods.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Automated metrics like ROUGE-L are insufficient for LFQA; human preference comparisons (with references) are necessary for meaningful evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Prior LFQA retrieval+generation systems face evaluation and scaling challenges; WebGPT proposes web-browsing + human feedback as an alternative pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'WebGPT: Browser-assisted question-answering with human feedback', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>REALM: Retrieval-augmented language model pre-training <em>(Rating: 2)</em></li>
                <li>Retrieval-augmented generation for knowledge-intensive NLP tasks <em>(Rating: 2)</em></li>
                <li>Dense passage retrieval for open-domain question answering <em>(Rating: 2)</em></li>
                <li>Interactive machine comprehension with information seeking agents <em>(Rating: 2)</em></li>
                <li>Boosting search engines with interactive agents <em>(Rating: 2)</em></li>
                <li>Hurdles to progress in long-form question answering <em>(Rating: 2)</em></li>
                <li>TruthfulQA: Measuring how models mimic human falsehoods <em>(Rating: 1)</em></li>
                <li>Learning to summarize from human feedback <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8438",
    "paper_id": "paper-2f3efe44083af91cef562c1a3451eee2f8601d22",
    "extraction_schema_id": "extraction-schema-150",
    "extracted_data": [
        {
            "name_short": "WebGPT",
            "name_full": "WebGPT: Browser-assisted question-answering with human feedback",
            "brief_description": "A family of language-model agents (fine-tuned GPT-3 variants) trained to use a text-based web-browsing environment to search, navigate, quote web pages as references, and compose long-form answers optimized with human feedback (behavior cloning, reward modeling, PPO, and rejection sampling).",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "WebGPT",
            "agent_description": "A fine-tuned GPT-3 model trained to operate a constrained text-based web browser via discrete commands (Search, Click link, Find in page, Quote, Scroll, End: Answer). It collects quoted extracts as references and then composes a final answer given those references; training methods include behavior cloning (BC), reward modeling (RM), reinforcement learning (PPO), and rejection sampling (best-of-n) against an RM.",
            "model_name": "GPT-3 family (fine-tuned variants: 760M, 13B, 175B)",
            "model_description": "Pretrained autoregressive transformer (GPT-3 family) fine-tuned with supervised demonstrations and human preference signals; evaluated in three sizes (760M, 13B, 175B) and combined with RM and inference-time rejection sampling.",
            "task_name": "ELI5 long-form question answering (and TruthfulQA, TriviaQA evaluations)",
            "task_description": "Given an open-ended question (primarily from ELI5), the agent may perform multi-step web searches and navigation actions, quote supporting extracts (references), then produce a paragraph-length answer supported by those references. Also evaluated on short-form adversarial TruthfulQA and TriviaQA.",
            "task_type": "web-assisted long-form question answering / web navigation",
            "memory_used": true,
            "memory_type": "external retrieval + prompt-level episodic summary (retrieval-augmented)",
            "memory_mechanism": "During browsing the agent issues Quote actions; quoted page title, domain and extract are recorded as references. The environment also includes a summary of past browsing actions and the current page text in the prompt; at answer time the collected references are concatenated into the prompt for generation. The agent has no persistent internal state beyond what is included in the provided prompt summary.",
            "memory_representation": "Collected quoted extracts (reference items containing page title, domain, extract), the text of current page and a summary of past actions (action history), and search result pages returned by Bing.",
            "memory_retrieval_method": "Prompt concatenation: the recorded references and action summary are appended to the model's prompt for subsequent steps (including final answer composition); browsing/referencing uses live Bing search API and on-the-fly page fetches.",
            "performance_with_memory": "175B WebGPT best-of-64 (uses web browsing + references + rejection sampling) produced answers preferred to human demonstrators 56% of the time and preferred to ELI5 highest-voted Reddit answers 69% of the time; on TruthfulQA WebGPT answers were truthful 75% of the time and truthful+informative 54% of the time (best model, truncated answers).",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Extensive comparisons between training/inference methods: rejection sampling (best-of-n) yields large gains over BC alone (175B best-of-64 BC preferred ~68% over BC baseline); RL (PPO) yields smaller gains (175B RL preferred ~58% over BC) and provides little extra benefit when combined with rejection sampling. No explicit ablation removing web access / references vs. using them is reported.",
            "key_findings": "Providing a web-browsing interface and forcing the model to collect explicit references improves answer quality and truthfulness versus the pretrained base model; rejection sampling (many attempts + RM ranking) is an effective, compute-tradeoff way to optimize against a reward model; RL yields modest gains but can be outperformed by rejection sampling and complicates tuning; references make human evaluation of factual accuracy tractable.",
            "limitations_or_challenges": "The agent has no persistent internal memory beyond prompt-provided summaries (limited episodic memory). References can be cherry-picked (incentivized to pick convincing but possibly biased/unreliable sources), and WebGPT sometimes quotes unreliable pages (especially out-of-distribution). The method increases perceived authority of answers (automation bias) despite remaining failure modes. No direct ablation isolates the causal effect of references vs. no-references on final performance.",
            "uuid": "e8438.0",
            "source_info": {
                "paper_title": "WebGPT: Browser-assisted question-answering with human feedback",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "References (quoted extracts)",
            "name_full": "Explicit quoted references collected during browsing",
            "brief_description": "The mechanism by which WebGPT stores external knowledge during an episode: when the agent issues a Quote action the page title, domain and extract are recorded and later provided as references in the answer prompt and used by human evaluators to judge support for claims.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Quoted References (WebGPT mechanism)",
            "agent_description": "A memory mechanism implemented as explicit stored document extracts (title, domain, extract) collected by the agent during browsing and concatenated into the prompt for the final answer and for human evaluation.",
            "model_name": null,
            "model_description": null,
            "task_name": "Support-aware answer composition / human evaluation",
            "task_description": "Store supporting evidence during multi-step web browsing and provide it as explicit references to both the model (for answer composition) and human evaluators (for factual adjudication).",
            "task_type": "external retrieval / evidence storage",
            "memory_used": true,
            "memory_type": "external document store (episodic per-episode buffer)",
            "memory_mechanism": "Quote actions append a structured triple (page title, domain, extract) to an episode buffer; at answer phase, these items are serialized and concatenated to the prompt so the model conditions on them when generating the final answer and human labelers can inspect them.",
            "memory_representation": "Page title, domain, and the exact extract (text span) quoted; references have a maximum total length limit and at least one reference is required to compose an answer.",
            "memory_retrieval_method": "Prompt concatenation at answer time (no learned retrieval over stored references beyond ordering in buffer); also used by reward model during RM scoring.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "No direct quantitative ablation reported that removes references entirely; authors argue references are central to human evaluation (make factual checking feasible) and likely reduce hallucinations (citing related prior work), but also note incentive to cherry-pick and the absence of direct ablation.",
            "key_findings": "Explicit references enable more accurate, less noisy human feedback and greater transparency; they are essential for making factual accuracy evaluable at scale, and they help the reward model and training pipeline produce higher-quality answers.",
            "limitations_or_challenges": "Incentivizes cherry-picking of convincing but potentially untrustworthy sources; does not guarantee global fairness or completeness of evidence; models sometimes quote unreliable sources under distribution shift.",
            "uuid": "e8438.1",
            "source_info": {
                "paper_title": "WebGPT: Browser-assisted question-answering with human feedback",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "Prompted action summary",
            "name_full": "Prompt-provided summary of past browsing actions (episodic action history)",
            "brief_description": "Because the model has no built-in memory of previous steps, the environment records a summary of past actions which is included in each prompt to provide the model with episodic context across multi-step browsing.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Action-summary prompt (WebGPT environment feature)",
            "agent_description": "A prompt-level episodic memory: the environment summarizes previous browsing commands and relevant page text and includes that summary in subsequent model prompts so the model can behave coherently across multi-step episodes despite lacking internal persistent memory.",
            "model_name": null,
            "model_description": null,
            "task_name": "Multi-step web navigation within a single episode",
            "task_description": "Provide the model with a representation of prior steps (searches, clicks, scrolls, quotes) so it can plan next actions despite not retaining internal state across prompt calls.",
            "task_type": "prompt-level episodic memory / web navigation",
            "memory_used": true,
            "memory_type": "working/episodic memory conveyed via prompt",
            "memory_mechanism": "Environment composes a textual summary of the current state, including question, current page text at cursor, and a summary of past actions; this text is passed to the model on each step, giving the model access to past-step information only via the prompt.",
            "memory_representation": "Textual summary of past actions, current page content at cursor position, search results indicators; not a learned persistent state.",
            "memory_retrieval_method": "Recency/explicit inclusion: summary is included verbatim in each prompt (no separate retrieval operation beyond prompt construction).",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Paper contrasts human demonstrators (who naturally remember prior steps) to the model (which lacks internal memory but receives the summary); no direct ablation comparing with/without the summary is reported, but authors state the summary is necessary because the base pretrained model lacks knowledge of command format and episode history.",
            "key_findings": "A prompt-provided summary is an effective mechanism to give a stateless LM the needed episodic context to perform multi-step browsing; it is how the environment supplies 'memory' without changing model architecture.",
            "limitations_or_challenges": "Limited capacityâ€”only what is summarized is available; increases prompt size; may omit details; not a substitute for richer persistent memory mechanisms.",
            "uuid": "e8438.2",
            "source_info": {
                "paper_title": "WebGPT: Browser-assisted question-answering with human feedback",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "REALM",
            "name_full": "REALM: Retrieval-augmented language model pre-training",
            "brief_description": "A differentiable retrieval-augmented approach that trains a retriever and language model end-to-end so the LM conditions on retrieved documents during pretraining and downstream tasks.",
            "citation_title": "REALM: Retrieval-augmented language model pre-training",
            "mention_or_use": "mention",
            "agent_name": "REALM",
            "agent_description": "A retrieval-augmented language model where retrieval is implemented with a learned dense retriever integrated into the LM training pipeline (differentiable retrieval).",
            "model_name": null,
            "model_description": null,
            "task_name": "Open-domain question answering / knowledge-intensive tasks",
            "task_description": "Retrieve relevant passages from a large corpus and condition generation/predictions on those passages.",
            "task_type": "retrieval-augmented question answering (differentiable retrieval)",
            "memory_used": true,
            "memory_type": "external document corpus with learned retriever (differentiable)",
            "memory_mechanism": "Dense retrieval via learned embeddings; retrieved passages are fed into the LM as context.",
            "memory_representation": "External corpus passages represented by dense embeddings.",
            "memory_retrieval_method": "Nearest-neighbor in embedding space using inner-product/embedding similarity; retrieval integrated into training (end-to-end).",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Mentioned in related work as an approach that focuses on retrieval as a differentiable process; paper contrasts this family with WebGPT's use of an external search engine which is non-differentiable and more up-to-date.",
            "key_findings": "Differentiable retrieval allows end-to-end optimization and fast retrieval learning, but cannot directly incorporate non-differentiable external processes like commercial search engines and is less interpretable than explicit web-browsing references.",
            "limitations_or_challenges": "Less suitable for live web access and non-differentiable retrieval sources; interpretability and up-to-dateness limitations relative to search-engine-based approaches.",
            "uuid": "e8438.3",
            "source_info": {
                "paper_title": "WebGPT: Browser-assisted question-answering with human feedback",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "RAG",
            "name_full": "RAG: Retrieval-augmented generation for knowledge-intensive NLP tasks",
            "brief_description": "A retrieval-augmented generation method that retrieves documents (via dense embeddings) and conditions a generative model on them for improved knowledge-intensive generation.",
            "citation_title": "Retrieval-augmented generation for knowledge-intensive NLP tasks",
            "mention_or_use": "mention",
            "agent_name": "RAG",
            "agent_description": "Retrieval-Augmented Generation that combines a dense retriever with a seq2seq generator to condition generation on retrieved documents; retrieval is differentiable at training time (approximate) and used to augment generation at inference.",
            "model_name": null,
            "model_description": null,
            "task_name": "Knowledge-intensive QA and generation",
            "task_description": "Retrieve relevant documents and generate answers conditioned on them.",
            "task_type": "retrieval-augmented QA (differentiable retrieval)",
            "memory_used": true,
            "memory_type": "external passage store with learned retrieval",
            "memory_mechanism": "Retrieve K nearest passages by embedding similarity and condition the generator on retrieved passages.",
            "memory_representation": "Retrieved document passages (text) and their embeddings.",
            "memory_retrieval_method": "Embedding-based nearest-neighbor search (semantic search).",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Mentioned as a contrasting family of approaches focusing on differentiable retrieval; the paper notes RAG/REALM/DPR differ from WebGPT by formulating retrieval as differentiable and trained end-to-end.",
            "key_findings": "Differentiable retrieval methods are powerful for many benchmarks; WebGPT argues for complementary benefits of using live search engines and explicit references for human-evaluable factuality.",
            "limitations_or_challenges": "Cannot directly leverage search engines and may be less interpretable regarding sources compared to explicit quoted references.",
            "uuid": "e8438.4",
            "source_info": {
                "paper_title": "WebGPT: Browser-assisted question-answering with human feedback",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "DPR",
            "name_full": "Dense Passage Retrieval (DPR)",
            "brief_description": "A dense embedding-based passage retriever trained with contrastive learning to retrieve relevant passages for open-domain QA.",
            "citation_title": "Dense passage retrieval for open-domain question answering",
            "mention_or_use": "mention",
            "agent_name": "DPR",
            "agent_description": "A dense embedding retriever that uses a learned encoder for questions and passages and is trained with contrastive loss to retrieve relevant passages for QA systems.",
            "model_name": null,
            "model_description": null,
            "task_name": "Open-domain QA retrieval",
            "task_description": "Retrieve relevant passages from a large corpus to support an answer-generating model.",
            "task_type": "retrieval (dense) + QA",
            "memory_used": true,
            "memory_type": "external document corpus with learned dense retriever",
            "memory_mechanism": "Embedding-based nearest-neighbor retrieval over a passage corpus.",
            "memory_representation": "Dense embeddings of passages plus raw passage text.",
            "memory_retrieval_method": "Semantic search via inner-product on embeddings.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Mentioned as part of the family of retrieval-augmented methods contrasted with WebGPT's search-engine-based approach; no direct experimental comparison reported in this paper.",
            "key_findings": "DPR provides strong retrieval for short-answer QA tasks; WebGPT situates itself as complementary by emphasizing a human-like search-and-quote workflow for long-form answers and human evaluation.",
            "limitations_or_challenges": "Focuses on differentiable retrieval and fixed corpora, not live web search; less transparent source citation workflow compared to explicit quoting.",
            "uuid": "e8438.5",
            "source_info": {
                "paper_title": "WebGPT: Browser-assisted question-answering with human feedback",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "Yuan et al. (Interactive agents)",
            "name_full": "Interactive machine comprehension with information seeking agents (Yuan et al., 2019)",
            "brief_description": "An RL-based approach where agents perform search and scrolling actions over documents for reading comprehension; an example of framing retrieval/browsing as an RL problem.",
            "citation_title": "Interactive machine comprehension with information seeking agents",
            "mention_or_use": "mention",
            "agent_name": "Yuan et al. information-seeking agents",
            "agent_description": "Agents trained with RL to issue search and navigation actions to gather information for QA, demonstrating web/document-level action spaces similar to WebGPT's environment.",
            "model_name": null,
            "model_description": null,
            "task_name": "Reading comprehension with interactive search/navigation",
            "task_description": "Use search and navigation actions to find evidence in documents and answer comprehension questions.",
            "task_type": "interactive document/web QA (RL agent)",
            "memory_used": true,
            "memory_type": "episodic browsing state + retrieved passages",
            "memory_mechanism": "Agent interacts with documents via actions (search, scroll) and conditions on retrieved text; trained with RL to maximize QA performance.",
            "memory_representation": "Visited text passages and navigation state within the episode.",
            "memory_retrieval_method": "Action-driven retrieval (search queries and navigation), not direct semantic-index retrieval.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Cited as prior work that frames web-level QA as an RL problem; WebGPT contrasts by using imitation learning + RM + rejection sampling and a commercial search API.",
            "key_findings": "Interactive agents can be trained with RL to seek information; WebGPT inherits the action-space idea but couples it with human demonstrations and reward modeling for improved human-evaluated answer quality.",
            "limitations_or_challenges": "Prior RL approaches can be sample-inefficient; WebGPT highlights sample-efficiency improvements via demonstrations and supplementary answer-only episodes.",
            "uuid": "e8438.6",
            "source_info": {
                "paper_title": "WebGPT: Browser-assisted question-answering with human feedback",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "Adolphs et al.",
            "name_full": "Boosting search engines with interactive agents",
            "brief_description": "Work setting up RL agents that perform sequences of search queries for short-form QA; another example of agents using multi-step retrieval/search actions.",
            "citation_title": "Boosting search engines with interactive agents",
            "mention_or_use": "mention",
            "agent_name": "Adolphs et al. interactive search agents",
            "agent_description": "Agents that learn sequences of search queries to improve answer retrieval, trained via RL or behavior cloning on synthetic sequences.",
            "model_name": null,
            "model_description": null,
            "task_name": "Search-query sequencing for short-form QA",
            "task_description": "Issue sequences of search queries to a search engine to retrieve and synthesize answers to short questions.",
            "task_type": "web search agents / interactive retrieval",
            "memory_used": true,
            "memory_type": "episodic history of queries and retrieved snippets",
            "memory_mechanism": "Action-history and retrieved snippets used to inform subsequent search actions and final answer.",
            "memory_representation": "Past queries and retrieved results within the episode.",
            "memory_retrieval_method": "History-conditioned querying and retrieval from search results.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Cited as related work; WebGPT differs by scaling to long-form answers and emphasizing human feedback and explicit reference recording.",
            "key_findings": "Interactive search agents can improve retrieval by planning query sequences; WebGPT demonstrates complementary strengths for long-form, reference-backed answers.",
            "limitations_or_challenges": "Short-form focus and synthetic BC data in some prior work contrast with WebGPT's human demonstrations and human-evaluated outcomes.",
            "uuid": "e8438.7",
            "source_info": {
                "paper_title": "WebGPT: Browser-assisted question-answering with human feedback",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "Krishna et al. 2021",
            "name_full": "Hurdles to progress in long-form question answering",
            "brief_description": "A study using retrieval-augmented models for long-form QA on ELI5, emphasizing difficulties of automated metrics and motivating human comparisons.",
            "citation_title": "Hurdles to progress in long-form question answering",
            "mention_or_use": "mention",
            "agent_name": "Krishna et al. LFQA models",
            "agent_description": "Work applying retrieval + generation pipelines to long-form QA (ELI5) and highlighting issues with automated evaluation metrics.",
            "model_name": null,
            "model_description": null,
            "task_name": "Long-form question answering (ELI5)",
            "task_description": "Produce paragraph-length explanations to open-ended ELI5 questions using retrieval+generation pipelines.",
            "task_type": "long-form question answering (retrieval-augmented)",
            "memory_used": true,
            "memory_type": "retrieved documents as external memory",
            "memory_mechanism": "Retrieve documents/passages and condition answer generation on them.",
            "memory_representation": "Retrieved passages and supporting documents.",
            "memory_retrieval_method": "Embedding-based and/or IR retrieval methods used in prior work.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "WebGPT compares favorably: Krishna et al.'s best model's answers were preferred only 23% of the time to ELI5 references, whereas WebGPT's best model was preferred 69% of the time. Krishna et al. used less compute and different methods.",
            "key_findings": "Automated metrics like ROUGE-L are insufficient for LFQA; human preference comparisons (with references) are necessary for meaningful evaluation.",
            "limitations_or_challenges": "Prior LFQA retrieval+generation systems face evaluation and scaling challenges; WebGPT proposes web-browsing + human feedback as an alternative pipeline.",
            "uuid": "e8438.8",
            "source_info": {
                "paper_title": "WebGPT: Browser-assisted question-answering with human feedback",
                "publication_date_yy_mm": "2021-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "REALM: Retrieval-augmented language model pre-training",
            "rating": 2
        },
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive NLP tasks",
            "rating": 2
        },
        {
            "paper_title": "Dense passage retrieval for open-domain question answering",
            "rating": 2
        },
        {
            "paper_title": "Interactive machine comprehension with information seeking agents",
            "rating": 2
        },
        {
            "paper_title": "Boosting search engines with interactive agents",
            "rating": 2
        },
        {
            "paper_title": "Hurdles to progress in long-form question answering",
            "rating": 2
        },
        {
            "paper_title": "TruthfulQA: Measuring how models mimic human falsehoods",
            "rating": 1
        },
        {
            "paper_title": "Learning to summarize from human feedback",
            "rating": 1
        }
    ],
    "cost": 0.019366249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>WebGPT: Browser-assisted question-answering with human feedback</h1>
<p>Reiichiro Nakano<em> Jacob Hilton</em> Suchir Balaji* Jeff Wu Long Ouyang<br>Christina Kim Christopher Hesse Shantanu Jain<br>Vineet Kosaraju<br>William Saunders Xu Jiang Karl Cobbe Tyna Eloundou Gretchen Krueger<br>Kevin Button Matthew Knight Benjamin Chess John Schulman<br>OpenAI</p>
<h4>Abstract</h4>
<p>We fine-tune GPT-3 to answer long-form questions using a text-based webbrowsing environment, which allows the model to search and navigate the web. By setting up the task so that it can be performed by humans, we are able to train models on the task using imitation learning, and then optimize answer quality with human feedback. To make human evaluation of factual accuracy easier, models must collect references while browsing in support of their answers. We train and evaluate our models on ELI5, a dataset of questions asked by Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior cloning, and then performing rejection sampling against a reward model trained to predict human preferences. This model's answers are preferred by humans $56 \%$ of the time to those of our human demonstrators, and $69 \%$ of the time to the highest-voted answer from Reddit.</p>
<h2>1 Introduction</h2>
<p>A rising challenge in NLP is long-form question-answering (LFQA), in which a paragraph-length answer is generated in response to an open-ended question. LFQA systems have the potential to become one of the main ways people learn about the world, but currently lag behind human performance [Krishna et al., 2021]. Existing work tends to focus on two core components of the task, information retrieval and synthesis.
In this work we leverage existing solutions to these components: we outsource document retrieval to the Microsoft Bing Web Search API, ${ }^{2}$ and utilize unsupervised pre-training to achieve high-quality synthesis by fine-tuning GPT-3 [Brown et al., 2020]. Instead of trying to improve these ingredients, we focus on combining them using more faithful training objectives. Following Stiennon et al. [2020], we use human feedback to directly optimize answer quality, allowing us to achieve performance competitive with humans.
We make two key contributions:</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An observation from our text-based web-browsing environment, as shown to human demonstrators (left) and models (right). The web page text has been abridged for illustrative purposes.</p>
<ul>
<li>We create a text-based web-browsing environment that a fine-tuned language model can interact with. This allows us to improve both retrieval and synthesis in an end-to-end fashion using general methods such as imitation learning and reinforcement learning.</li>
<li>We generate answers with references: passages extracted by the model from web pages while browsing. This is crucial for allowing labelers to judge the factual accuracy of answers, without engaging in a difficult and subjective process of independent research.</li>
</ul>
<p>Our models are trained primarily to answer questions from ELI5 [Fan et al., 2019], a dataset of questions taken from the "Explain Like I'm Five" subreddit. We collect two additional kinds of data: demonstrations of humans using our web-browsing environment to answer questions, and comparisons between two model-generated answers to the same question (each with their own set of references). Answers are judged for their factual accuracy, coherence, and overall usefulness.</p>
<p>We use this data in four main ways: behavior cloning (i.e., supervised fine-tuning) using the demonstrations, reward modeling using the comparisons, reinforcement learning against the reward model, and rejection sampling against the reward model. Our best model uses a combination of behavior cloning and rejection sampling. We also find reinforcement learning to provide some benefit when inference-time compute is more limited.</p>
<p>We evaluate our best model in three different ways. First, we compare our model's answers to answers written by our human demonstrators on a held-out set of questions. Our model's answers are preferred 56% of the time, demonstrating human-level usage of the text-based browser. Second, we compare our model's answers (with references stripped, for fairness) to the highest-voted answer provided by the ELI5 dataset. Our model's answers are preferred 69% of the time. Third, we evaluate our model on TruthfulQA [Lin et al., 2021], an adversarial dataset of short-form questions. Our model's answers are true 75% of the time, and are both true and informative 54% of the time, outperforming our base model (GPT-3), but falling short of human performance.</p>
<p>The remainder of the paper is structured as follows:</p>
<ul>
<li>In Section 2, we describe our text-based web-browsing environment and how our models interact with it.</li>
<li>In Section 3, we explain our data collection and training methods in more detail.</li>
<li>In Section 4, we evaluate our best-performing models (for different inference-time compute budgets) on ELI5 and TruthfulQA.</li>
<li>In Section 5, we provide experimental results comparing our different methods and how they scale with dataset size, parameter count, and inference-time compute.</li>
<li>In Section 6, we discuss the implications of our findings for training models to answer questions truthfully, and broader impacts.</li>
</ul>
<p>Table 1: Actions the model can take. If a model generates any other text, it is considered to be an invalid action. Invalid actions still count towards the maximum, but are otherwise ignored.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Command</th>
<th style="text-align: left;">Effect</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Search <query></td>
<td style="text-align: left;">Send <query> to the Bing API and display a search results page</td>
</tr>
<tr>
<td style="text-align: left;">Clicked on link <link ID></td>
<td style="text-align: left;">Follow the link with the given ID to a new page</td>
</tr>
<tr>
<td style="text-align: left;">Find in page: <text></td>
<td style="text-align: left;">Find the next occurrence of <text> and scroll to it</td>
</tr>
<tr>
<td style="text-align: left;">Quote: <text></td>
<td style="text-align: left;">If <text> is found in the current page, add it as a reference</td>
</tr>
<tr>
<td style="text-align: left;">Scrolled down &lt;1, 2, 3&gt;</td>
<td style="text-align: left;">Scroll down a number of times</td>
</tr>
<tr>
<td style="text-align: left;">Scrolled up &lt;1, 2, 3&gt;</td>
<td style="text-align: left;">Scroll up a number of times</td>
</tr>
<tr>
<td style="text-align: left;">Top</td>
<td style="text-align: left;">Scroll to the top of the page</td>
</tr>
<tr>
<td style="text-align: left;">Back</td>
<td style="text-align: left;">Go to the previous page</td>
</tr>
<tr>
<td style="text-align: left;">End: Answer</td>
<td style="text-align: left;">End browsing and move to answering phase</td>
</tr>
<tr>
<td style="text-align: left;">End: <Nonsense, Controversial></td>
<td style="text-align: left;">End browsing and skip answering phase</td>
</tr>
</tbody>
</table>
<h1>2 Environment design</h1>
<p>Previous work on question-answering such as REALM [Guu et al., 2020] and RAG [Lewis et al., 2020a] has focused on improving document retrieval for a given query. Instead, we use a familiar existing method for this: a modern search engine (Bing). This has two main advantages. First, modern search engines are already very powerful, and index a large number of up-to-date documents. Second, it allows us to focus on the higher-level task of using a search engine to answer questions, something that humans can do well, and that a language model can mimic.</p>
<p>For this approach, we designed a text-based web-browsing environment. The language model is prompted with a written summary of the current state of the environment, including the question, the text of the current page at the current cursor location, and some other information (see Figure 1(b)). In response to this, the model must issue one of the commands given in Table 1, which performs an action such as running a Bing search, clicking on a link, or scrolling around. This process is then repeated with a fresh context (hence, the only memory of previous steps is what is recorded in the summary).
While the model is browsing, one of the actions it can take is to quote an extract from the current page. When this is performed, the page title, domain name and extract are recorded to be used later as a reference. Browsing then continues until either the model issues a command to end browsing, the maximum number of actions has been reached, or the maximum total length of references has been reached. At this point, as long as there is at least one reference, the model is prompted with the question and the references, and must compose its final answer.
Further technical details about our environment can be found in Appendix A.</p>
<h2>3 Methods</h2>
<h3>3.1 Data collection</h3>
<p>Guidance from humans is central to our approach. A language model pre-trained on natural language would not be able to use our text-based browser, since it does not know the format of valid commands. We therefore collected examples of humans using the browser to answer questions, which we call demonstrations. However, training on demonstrations alone does not directly optimize answer quality, and is unlikely to lead far beyond human performance [Stiennon et al., 2020]. We therefore collected pairs of model-generated answers to the same question, and asked humans which one they preferred, which we call comparisons.
For both demonstrations and comparisons, the vast majority of questions were taken from ELI5 [Fan et al., 2019], a dataset of long-form questions. For diversity and experimentation, we also mixed in a small number of questions from other sources, such as TriviaQA [Joshi et al., 2017]. In total, we collected around 6,000 demonstrations, $92 \%$ of which were for questions from ELI5, and around 21,500 comparisons, $98 \%$ of which were for questions from ELI5. A more detailed breakdown of the questions we used along with post-processing details can be found in Appendix B.</p>
<p>Table 2: An answer produced by our 175B best-of-64 model to a randomly-chosen question from the ELI5 test set (not cherry-picked). The full text of the references can be found in Appendix J, along with answers from our human demonstrators and the ELI5 dataset. Further samples are available at https://openaipublic.blob.core.windows.net/webgpt-answer-viewer/index.html.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Question</th>
<th style="text-align: left;">Why did we decide that certain words were "bad" and shouldn't be used in social settings?</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Answer</td>
<td style="text-align: left;">We have all been taught that there are certain words that should not be used in public, at least <br> not without good reason. But why do some words fall into this category while others do not? It <br> turns out that there is not a single answer to this question, as each word has a different history <br> and carries different meanings in different cultures and families [2]. However, there are some <br> general categories that "bad" words tend to fall into. "Bad" words generally relate to parts of life <br> that we don't like talking about in public, like bathroom functions, or negative ways of talking <br> about people's religion, skin color, ability, or gender [3]. Often, people say bad words when <br> they are experiencing strong emotions, and those words are linked to our emotions [3].</td>
</tr>
<tr>
<td style="text-align: left;">References</td>
<td style="text-align: left;">[1, 2, 3] Why Are Some Words 'Bad'?</td>
</tr>
<tr>
<td style="text-align: left;">(titles</td>
<td style="text-align: left;">[4] On Words: 'Bad' Words and Why We Should Study Them</td>
</tr>
<tr>
<td style="text-align: left;">only)</td>
<td style="text-align: left;">[5] The Science of Curse Words: Why The \&amp;D Do We Swear? (www.babbel.com)</td>
</tr>
</tbody>
</table>
<p>To make it easier for humans to provide demonstrations, we designed a graphical user interface for the environment (see Figure 1(a)). This displays essentially the same information as the text-based interface and allows any valid action to be performed, but is more human-friendly. For comparisons, we designed a similar interface, allowing auxiliary annotations as well as comparison ratings to be provided, although only the final comparison ratings (better, worse or equally good overall) were used in training.</p>
<p>For both demonstrations and comparisons, we emphasized that answers should be relevant, coherent, and supported by trustworthy references. Further details about these criteria and other aspects of our data collection pipeline can be found in Appendix C.</p>
<p>We are releasing a dataset of comparisons, the details of which can be found in Appendix K.</p>
<h1>3.2 Training</h1>
<p>The use of pre-trained models is crucial to our approach. Many of the underlying capabilities required to successfully use our environment to answer questions, such as reading comprehension and answer synthesis, emerge as zero-shot capabilities of language models [Brown et al., 2020]. We therefore fine-tuned models from the GPT-3 model family, focusing on the 760M, 13B and 175B model sizes.</p>
<p>Starting from these models, we used four main training methods:</p>
<ol>
<li>Behavior cloning (BC). We fine-tuned on the demonstrations using supervised learning, with the commands issued by the human demonstrators as labels.</li>
<li>Reward modeling (RM). Starting from the BC model with the final unembedding layer removed, we trained a model to take in a question and an answer with references, and output a scalar reward. Following Stiennon et al. [2020], the reward represents an Elo score, scaled such that the difference between two scores represents the logit of the probability that one will be preferred to the other by the human labelers. The reward model is trained using a cross-entropy loss, with the comparisons as labels. Ties are treated as soft $50 \%$ labels.</li>
<li>Reinforcement learning (RL). Once again following Stiennon et al. [2020], we fine-tuned the BC model on our environment using PPO [Schulman et al., 2017]. For the environment reward, we took the reward model score at the end of each episode, and added this to a KL penalty from the BC model at each token to mitigate overoptimization of the reward model.</li>
<li>Rejection sampling (best-of-n). We sampled a fixed number of answers (4, 16 or 64) from either the BC model or the RL model (if left unspecified, we used the BC model), and selected the one that was ranked highest by the reward model. We used this as an alternative method of optimizing against the reward model, which requires no additional training, but instead uses more inference-time compute.</li>
</ol>
<p>We used mutually disjoint sets of questions for each of BC, RM and RL.
For BC, we held out around $4 \%$ of the demonstrations to use as a validation set.
For RM, we sampled answers for the comparison datasets in an ad-hoc manner, using models of various sizes (but primarily the 175B model size), trained using various combinations of methods and hyperparameters, and combined them into a single dataset. This was for data efficiency: we collected many comparisons for evaluation purposes, such as for tuning hyperparameters, and did not want to waste this data. Our final reward models were trained on around 16,000 comparisons, the remaining 5,500 being used for evaluation only.</p>
<p>For RL, we trained on a mixture of $90 \%$ questions from ELI5 and $10 \%$ questions from TriviaQA. To improve sample efficiency, at the end of each episode we inserted 15 additional answering-only episodes using the same references as the previous episode. We were motivated to try this because answering explained slightly more of the variance in reward model score than browsing despite taking many fewer steps, and we found it to improve sample efficiency by approximately a factor of 2 . We also randomized the maximum number of browsing actions, sampling uniformly from the range $20-100$ inclusive.</p>
<p>Hyperparameters for all of our training methods can be found in Appendix E.</p>
<h1>4 Evaluation</h1>
<p>In evaluating our approach, we focused on three "WebGPT" models, each of which was trained with behavior cloning followed by rejection sampling against a reward model of the same size: a 760M best-of-4 model, a 13B best-of-16 model and a 175B best-of-64 model. As discussed in Section 5.2, these are compute-efficient models corresponding to different inference-time compute budgets. We excluded RL for simplicity, since it did not provide significant benefit when combined with rejection sampling (see Figure 4).</p>
<p>We evaluated all WebGPT models using a sampling temperature of 0.8 , which was tuned using human evaluations, and with a maximum number of browsing actions of 100 .</p>
<h3>4.1 ELI5</h3>
<p>We evaluated WebGPT on the ELI5 test set in two different ways:</p>
<ol>
<li>We compared model-generated answers to answers written by demonstrators using our web-browsing environment. For these comparisons, we used the same procedure as comparisons used for reward model training. We consider this to be a fair comparison, since the instructions for demonstrations and comparisons emphasize a very similar set of criteria.</li>
<li>We compared model-generated answers to the reference answers from the ELI5 dataset, which are the highest-voted answers from Reddit. In this case, we were concerned about ecological validity, since our detailed comparison criteria may not match those of real-life users. We were also concerned about blinding, since Reddit answers do not typically include citations. To mitigate these concerns, we stripped all citations and references from the model-generated answers, hired new contractors who were not familiar with our detailed instructions, and gave them a much more minimal set of instructions, which are given in Appendix F.</li>
</ol>
<p>In both cases, we treat ties as $50 \%$ preference ratings (rather than excluding them).
Our results are shown in Figure 2. Our best model, the 175B best-of-64 model, produces answers that are preferred to those written by our human demonstrators $56 \%$ of the time. This suggests that the use of human feedback is essential, since one would not expect to exceed $50 \%$ preference by imitating demonstrations alone (although it may still be possible, by producing a less noisy policy). The same model produces answers that are preferred to the reference answers from the ELI5 dataset $69 \%$ of the time. This is a substantial improvement over Krishna et al. [2021], whose best model's answers are preferred $23 \%$ of the time to the reference answers, although they use substantially less compute than even our smallest model.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Human evaluations on ELI5 comparing against (a) demonstrations collected using our web browser, (b) the highest-voted answer for each question. The amount of rejection sampling (the $n$ in best-of-n) was chosen to be compute-efficient (see Figure 8). Error bars represent $\pm 1$ standard error.</p>
<p>Although the evaluations against the ELI5 reference answers are useful for comparing to prior work, we believe that the evaluations against human demonstrations are more meaningful, for several reasons:</p>
<ul>
<li>Fact-checking. It is difficult to assess the factual accuracy of answers without references: even with the help of a search engine, expertise is often required. However, WebGPT and human demonstrators provide answers with references.</li>
<li>Objectivity. The use of minimal instructions makes it harder to know what criteria are being used to choose one answer over another. Our more detailed instructions enable more interpretable and consistent comparisons.</li>
<li>Blinding. Even with citations and references stripped, WebGPT composes answers that are different in style to Reddit answers, making the comparisons less blinded. In contrast, WebGPT and human demonstrators compose answers in similar styles. Additionally, some ELI5 answers contained links, which we instructed labelers not to follow, and this could have biased labelers against those answers.</li>
<li>Answer intent. People ask questions on ELI5 to obtain original, simplified explanations rather than answers that can already be found on the web, but these were not criteria we wanted answers to be judged on. Moreover, many ELI5 questions only ever get a small number of low-effort answers. With human demonstrations, it is easier to ensure that the desired intent and level of effort are used consistently.</li>
</ul>
<h1>4.2 TruthfulQA</h1>
<p>To further probe the abilities of WebGPT, we evaluated WebGPT on TruthfulQA [Lin et al., 2021], an adversarially-constructed dataset of short-form questions. TruthfulQA questions are crafted such that they would be answered falsely by some humans due to a false belief or misconception. Answers are scored on both truthfulness and informativeness, which trade off against one another (for example, "I have no comment" is considered truthful but not informative).
We evaluated both the base GPT-3 models used by WebGPT and the WebGPT models themselves on TruthfulQA. For GPT-3, we used both the "QA prompt" and the "helpful prompt" from Lin et al. [2021], and used the automated metric, since this closely tracks human evaluation on answers produced by the GPT-3 model family. For WebGPT, we used human evaluation, since WebGPT's answers are out-of-distribution for the automated metric. TruthfulQA is a short-form dataset, so</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: TruthfulQA results. The amount of rejection sampling (the $n$ in best-of- $n$ ) was chosen to be compute-efficient (see Figure 8). Error bars represent $\pm 1$ standard error.
we also truncated WebGPT's answers to 50 tokens in length, and then removed any trailing partial sentences. ${ }^{3}$</p>
<p>Our results are shown in Figure 3. All WebGPT models outperform all GPT-3 models (with both prompts) on both the percentage of truthful answers and the percentage of truthful and informative answers. Moreover, the percentage of truthful and informative answers increases with model size for WebGPT, unlike GPT-3 with either prompt. Further qualitative analysis of WebGPT's performance on TruthfulQA is given in Section 6.1.</p>
<h1>4.3 TriviaQA</h1>
<p>We also evaluated the WebGPT 175B BC model on TriviaQA [Joshi et al., 2017]. These results are given in Appendix G.</p>
<h2>5 Experiments</h2>
<h3>5.1 Comparison of training methods</h3>
<p>We ran a number of additional experiments comparing reinforcement learning (RL) and rejection sampling (best-of- $n$ ) with each other and with the behavior cloning (BC) baseline. Our results are shown in Figures 4 and 5. Rejection sampling provides a substantial benefit, with the 175B best-of-64 BC model being preferred $68 \%$ of the time to the 175B BC model. Meanwhile, RL provides a smaller benefit, with the 175B RL model being preferred $58 \%$ of the time to the 175B BC model.</p>
<p>Even though both rejection sampling and RL optimize against the same reward model, there are several possible reasons why rejection sampling outperforms RL:</p>
<ul>
<li>It may help to have many answering attempts, simply to make use of more inference-time compute.</li>
<li>The environment is unpredictable: with rejection sampling, the model can try visiting many more websites, and then evaluate the information it finds with the benefit of hindsight.</li>
</ul>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Preference of RL models over BC models, with (right) and without (left) using rejection sampling. RL slightly improves preference, but only when not using rejection sampling. Error bars represent $\pm 1$ standard error.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Preference of the 175B best-of- $n$ BC model over the BC model. The validation RM prediction is obtained using the estimator described in Appendix I, and predicts human preference well in this setting. The shaded region represents $\pm 1$ standard error.</p>
<ul>
<li>The reward model was trained primarily on data collected from BC and rejection sampling policies, which may have made it more robust to overoptimization by rejection sampling than by RL.</li>
<li>RL requires hyperparameter tuning, whereas rejection sampling does not.</li>
</ul>
<p>The combination of RL and rejection sampling also fails to offer much benefit over rejection sampling alone. One possible reason for this is that RL and rejection sampling are optimizing against the same reward model, which can easily be overoptimized (especially by RL, as noted above). In addition to this, RL reduces the entropy of the policy, which hurts exploration. Adapting the RL objective to optimize rejection sampling performance is an interesting direction for future research.</p>
<p>It is also worth highlighting the importance of carefully tuning the BC baseline for these comparisons. As discussed in Appendix E, we tuned the number of BC epochs and the sampling temperature using a combination of human evaluations and reward model score. This alone closed much of the gap we originally saw between BC and RL.</p>
<h1>5.2 Scaling experiments</h1>
<p>We also conducted experiments to investigate how model performance varied with the size of the dataset, the number of model parameters, and the number of samples used for rejection sampling. Since human evaluations can be noisy and expensive, we used the score of a 175B "validation" reward model (trained on a separate dataset split) for these experiments. We found this to be a good predictor of human preference when not optimizing against a reward model using RL (see Figure 5). Recall that the reward represents an Elo score, with a difference of 1 point representing a preference of $\operatorname{sigmoid}(1) \approx 73 \%$.</p>
<p>Scaling trends with dataset size and parameter count are shown in Figures 6 and 7. For dataset size, doubling the number of demonstrations increased the policy's reward model score by about 0.13 , and doubling the number of comparisons increased the reward model's accuracy by about $1.8 \%$. For parameter count, the trends were noisier, but doubling the number of parameters in the policy increased its reward model score by roughly 0.09 , and doubling the number of parameters in the reward model increased its accuracy by roughly $0.4 \%$.</p>
<p>For rejection sampling, we analyzed how to trade off the number of samples against the number of model parameters for a given inference-time compute budget (see Figure 8). We found that it is</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: BC scaling, varying the proportion of the demonstration dataset and parameter count of the policy.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: RM scaling, varying the proportion of the comparison dataset and parameter count of the reward model.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Best-of- $n$ scaling, varying the parameter count of the policy and reward model together, as well as the number of answers sampled.
generally compute-efficient to use some amount of rejection sampling, but not too much. The models for our main evaluations come from the Pareto frontier of this trade-off: the 760M best-of-4 model, the 13B best-of-16 model, and the 175B best-of-64 model.</p>
<h1>6 Discussion</h1>
<h3>6.1 Truthfulness of WebGPT</h3>
<p>As NLP systems improve and become more widely deployed, it is becoming increasingly important to develop techniques for reducing the number of false statements they make [Evans et al., 2021]. To assess the contribution of WebGPT to this aim, it is helpful to distinguish two categories of false statement made by a model:</p>
<ol>
<li>Imitative falsehoods. These are false statements that are incentivized by the training objective (even in the limit of infinite data and compute), such as reproducing common misconceptions [Lin et al., 2021].</li>
<li>Non-imitative falsehoods. These are false statements that are the result of the model failing to achieve its training objective, including most hallucinations, which are statements that are false, but look plausible at a glance [Maynez et al., 2020].</li>
</ol>
<p>Our TruthfulQA results suggest that WebGPT produces fewer imitative falsehoods than GPT-3. We believe this is because WebGPT is incentivized to prefer reliable sources (both because of filtering performed by the Bing API, and because we specify this in our instructions). Nevertheless, as shown in Table 3, WebGPT still sometimes quotes from highly unreliable sources in response to TruthfulQA questions. We hypothesize that this is because of the distribution shift from ELI5 to TruthfulQA, and that training on adversarially-selected questions is a promising way to improve this. It would be important in such an endeavor to pay close attention to labeler judgments of source trustworthiness (see Appendix C).</p>
<p>Our results on ELI5 suggest that WebGPT also produces fewer non-imitative falsehoods than GPT3. We did not test this hypothesis directly, since we found that it was challenging for labelers to spot subtle hallucinations. However, prior work shows that the use of retrieval reduces the rate of hallucinations [Shuster et al., 2021], and moreover WebGPT performs about as well as human demonstrations for factual accuracy on ELI5 (see Figure 2(a)). Nevertheless, WebGPT still sometimes produces non-imitative falsehoods, which are typically mistakes when attempting to paraphrase or synthesize information rather than wild hallucinations.</p>
<p>Table 3: Two questions from TruthfulQA, cherry-picked to highlight a success and a failure of WebGPT. While GPT-3 175B with the helpful prompt answers "I have no comment" to $49 \%$ of questions, WebGPT almost always tries to answer the question, but sometimes quotes from unreliable sources. In spite of this, WebGPT still answers more truthfully overall (see Figure 3).
Key: $\not X=$ false, $\checkmark=$ true but uninformative, $\underline{\checkmark}=$ true and informative</p>
<table>
<thead>
<tr>
<th style="text-align: left;">GPT-3 175B <br> (QA prompt)</th>
<th style="text-align: left;">GPT-3 175B <br> (helpful prompt)</th>
<th style="text-align: left;">WebGPT 175B best-of-64 <br> (first sentence)</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>What happens if you smash a mirror?</p>
<table>
<thead>
<tr>
<th style="text-align: center;">If you smash a mirror, you <br> will have seven years of <br> bad luck.</th>
<th style="text-align: center;">I have no comment.</th>
<th style="text-align: center;">When you break a mirror you might <br> cut yourself and people might be an- <br> gry at you for doing it on purpose.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\not$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
</tbody>
</table>
<p>If you dream of doing something and make a wish, will you succeed?</p>
<table>
<thead>
<tr>
<th style="text-align: center;">I have no comment.</th>
<th style="text-align: center;">I have no comment.</th>
<th style="text-align: center;">It is true that you can make a wish <br> come true by the power of thought.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\not$</td>
</tr>
</tbody>
</table>
<h1>6.2 Perceived truthfulness of WebGPT</h1>
<p>In order to assess the benefits and risks of WebGPT, it is necessary to consider not only how often it makes false statements, but also how likely users are to rely on those statements. Although WebGPT makes false statements less frequently than GPT-3, its answers also appear more authoritative, partly because of the use of citations. In combination with the well-documented problem of "automation bias" [Goddard et al., 2012], this could lead to overreliance on WebGPT's answers. This is particularly problematic because, as discussed in Section 6.1, WebGPT can make more mistakes than humans on out-of-distribution questions. Documentation of these limitations could help inform those interacting with WebGPT, and further research is required to understand how else to mitigate this.</p>
<h3>6.3 Reinforcement of bias</h3>
<p>There are a number of ways in which WebGPT tends to perpetuate and reinforce existing assumptions and biases. Firstly, WebGPT inherits the biases of the base model from which it is fine tuned, GPT-3 [Brown et al., 2020], and this influences the way in which it chooses to search for and synthesize information. Search and synthesis both depend on the ability to include and exclude material depending on some measure of its value, and by incorporating GPT-3's biases when making these decisions, WebGPT can be expected to perpetuate them further. Secondly, the fact that WebGPT synthesizes information from existing sources gives it the potential to reinforce and entrench existing beliefs and norms. Finally, WebGPT usually accepts the implicit assumptions made by questions, and more generally seems to be influenced by the stance taken by questions. This is something that could exacerbate confirmation bias in users.
These problems could be mitigated with improvements both to WebGPT's base model and to WebGPT's training objective, and we discuss some alternative objectives in the next section. It may also be important to control how WebGPT is used, both by limiting access and by tailoring the design and documentation of applications.
Additional analysis of the effect of question stance and of reference point bias is given in Appendix H.</p>
<h1>6.4 Using references to evaluate factual accuracy</h1>
<p>Central to our approach is the use of references collected by the model to aid human evaluation of factual accuracy. This was previously suggested by Metzler et al. [2021], and has several benefits:</p>
<ul>
<li>More accurate feedback. It is very challenging to evaluate the factual accuracy of arbitrary claims, which can be technical, subjective or vague. In contrast, it is much easier to evaluate how well a claim is supported by a set of sources.</li>
<li>Less noisy feedback. It is also easier to specify an unambiguous procedure for evaluating how well a claim is supported by a set of sources, compared to evaluating the factual accuracy of an arbitrary claim. This improves agreement rates between labelers, which helps data efficiency.</li>
<li>Transparency. It is much easier to understand how WebGPT composes answers than it is for GPT-3, since the entire browsing process can be inspected. It is also straightforward for end-users to follow up on sources to better judge factual accuracy for themselves.</li>
</ul>
<p>Despite these benefits, references are far from a panacea. Our current procedure incentivizes models to cherry-pick references that they expect labelers to find convincing, even if those references do not reflect a fair assessment of the evidence. As discussed in Section 6.3, there are early signs of this happening, with WebGPT accepting the implicit assumptions of questions, and the problem is likely to be exacerbated by more capable models and more challenging or subjective questions. We could mitigate this using methods like debate [Irving et al., 2018], in which models are trained to find evidence both for and against different claims. Such setups can also be viewed as simple cases of recursive reward modeling [Leike et al., 2018] and Iterated Amplification [Christiano et al., 2018], in which the model assists its own evaluation.
Our approach also raises a challenging problem with societal implications: how should factual accuracy be evaluated when training AI systems? Evans et al. [2021, Section 2] propose a number of desiderata, but a substantial gap remains between these and the highly specific criteria needed to train current AI systems with reasonable data efficiency. We made a number of difficult judgment calls, such as how to rate the trustworthiness of sources (see Appendix C), which we do not expect universal agreement with. While WebGPT did not seem to take on much of this nuance, we expect these decisions to become increasingly important as AI systems improve, and think that cross-disciplinary research is needed to develop criteria that are both practical and epistemically sound.</p>
<h3>6.5 Risks of live web access</h3>
<p>At both train and inference time, WebGPT has live access to the web via our text-based browsing environment. This enables the model to provide up-to-date answers to a wide range of questions, but potentially poses risks both to the user and to others. For example, if the model had access to forms, it could edit Wikipedia to construct a reliable-looking reference. Even if human demonstrators did not perform such behavior, it would likely be reinforced by RL if the model were to stumble across it.
We believe the risk posed by WebGPT exploiting real-world side-effects of its actions is very low. This is because the only interactions with the outside world allowed by the environment are sending queries to the Bing API and following links that already exist on the web, and so actions like editing Wikipedia are not directly available to the model. While a capable enough system could escalate these privileges [Harms, 2016], WebGPT's capabilities seem far below what would be required to achieve this.
Nevertheless, much more capable models could potentially pose much more serious risks [Bostrom, 2014]. For this reason, we think as the capabilities of models increase, so should the burden of proof of safety for giving them access to the web, even at train time. As part of this, measures such as tripwire tests could be used to help catch exploitative model behavior early.</p>
<h2>7 Related work</h2>
<p>Combining machine learning with an external knowledge base, for the task of question-answering, preceded the rise of pre-trained language models in the late 2010s. One notable system of this kind was DeepQA (also known as IBM Watson), which was used to beat the best humans at Jeopardy</p>
<p>[Ferrucci et al., 2010]. A large body of newer work uses language models to answer questions with the help of retrieved documents; these systems are more general and conceptually simpler than DeepQA. One approach is to use inner product search to retrieve relevant documents and then generate an answer given these documents:</p>
<p>$$
p(\text { passage } \mid \text { query }) \propto \exp (\operatorname{embed}(\text { passage }) \cdot \text { embed }(\text { query }))
$$</p>
<p>Given a training dataset that specifies relevant passages for each question, dense passage retrieval (DPR) trains the retriever directly using a contrastive objective [Karpukhin et al., 2020]. Retrieval Augmented Language Modeling (REALM) [Guu et al., 2020] and Retrieval Augmented Generation (RAG) [Lewis et al., 2020a] train the retriever and question-answering components end-to-end using a language modeling objective. Unlike DPR, RAG, and REALM, which focus on benchmarks with short answers, Krishna et al. [2021] use a similar system to tackle long-form question-answering on the ELI5 dataset [Fan et al., 2019]. They find that automated metrics like ROUGE-L are not meaningful, which motivates our choice to use human comparisons as the main metric. Note that the aforementioned family of methods, which rely on inner product search (Equation 1), differ from WebGPT in that they formulate retrieval as a differentiable process. Fully differentiable retrieval has the advantage of fast optimization; two disadvantages are that it cannot deal with non-differential processes like using a search engine, and it is less interpretable.
Like WebGPT, some other recent work defines document retrieval or web browsing as a reinforcement learning (RL) problem. Yuan et al. [2019] apply RL to reading comprehension benchmarks, where (as in WebGPT) the action space includes searching and scrolling through the provided source document. They suggest web-level QA (like WebGPT) as a direction for future work. Adolphs et al. [2021] set up an RL problem that involves performing a series of search queries for short-form question-answering. They train their system in two alternative ways: behavior cloning (BC) on synthetically-generated sequences and RL. Finally, there is another body of work that uses BC and RL to control web browsers, for automating other tasks besides question-answering [Shi et al., 2017, Gur et al., 2018].</p>
<h1>8 Conclusion</h1>
<p>We have demonstrated a novel approach to long-form question-answering, in which a language model is fine-tuned to use a text-based web-browsing environment. This allows us to directly optimize answer quality using general methods such as imitation learning and reinforcement learning. To make human evaluation easier, answers must be supported by references collected during browsing. Using this approach, our best model outperforms humans on ELI5, but still struggles with out-of-distribution questions.</p>
<h2>9 Author contributions</h2>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji and John Schulman jointly led the project, developed the codebase, ran all data collection and experiments, and wrote the paper.
Jeff Wu, Long Ouyang, Xu Jiang and Karl Cobbe provided invaluable advice on a multitude of topics over the course of the project.
Jeff Wu, Vineet Kosaraju, William Saunders and Xu Jiang made key contributions to the project codebase.
Christina Kim, Christopher Hesse and Shantanu Jain built and supported infrastructure used for model training and inference.
Tyna Eloundou and Gretne Krueger conducted the analysis of bias and contributed to the paper.
Kevin Button and Matthew Knight provided computer security support.
Benjamin Chess provided computer networking support.</p>
<h2>10 Acknowledgments</h2>
<p>We would like to thank Leo Gao, Hyeonwoo Noh and Chelsea Voss for working on future directions; Steve Dowling, Christian Gibson, Peter Hoeschele, Fraser Kelton, Bianca Martin, Bob McGrew,</p>
<p>Felipe Such and Hannah Wong for technical, logistical and communications support; Steven Adler, Miles Brundage, David Farhi, William Guss, Oleg Klimov, Jan Leike, Ryan Lowe, Diogo Moitinho de Almeida, Arvind Neelakantan, Alex Ray, Nick Ryder and Andreas StuhlmÃ¼ller for helpful discussions; Owen Cotton-Barratt, Owain Evans, Jared Kaplan, Girish Sastry, Carl Shulman, Denis Yarats and Daniel Ziegler for helpful discussions and feedback on drafts; Beth Barnes and Paul Christiano for helpful discussions and feedback on drafts, and in particular for suggesting the project; and Dario Amodei for suggesting to work on factual inaccuracy in language models. We would also like to thank Surge AI for helping us with data collection, in particular Edwin Chen, Andrew Mauboussin, Craig Pettit and Bradley Webb.</p>
<p>Finally, we would like to thank all of our contractors for providing demonstrations and comparisons, without which this project would not have been possible, including: Jamie Alexander, Andre Gooden, Jacquelyn Johns, Rebecca Kientz, Ashley Michalski, Amy Dieu-Am Ngo, Alex Santiago, Alice Sorel, Sam Thornton and Kelli W. from Upwork; and Elena Amaya, Michael Baggiano, Carlo Basile, Katherine Beyer, Erica Dachinger, Joshua Drozd, Samuel Ernst, Rodney Khumalo, Andrew Kubai, Carissa Lewis, Harry Mubvuma, William Osborne, Brandon P., Kimberly Quinn, Jonathan Roque, Jensen Michael Ruud, Judie Anne Sigdel, Bora Son, JoAnn Stone, Rachel Tanks, Windy Thomas, Laura Trivett, Katherine Vazquez, Brandy and Shannon from Surge AI.</p>
<h1>References</h1>
<p>L. Adolphs, B. Boerschinger, C. Buck, M. C. Huebscher, M. Ciaramita, L. Espeholt, T. Hofmann, and Y. Kilcher. Boosting search engines with interactive agents. arXiv preprint arXiv:2109.00527, 2021.
S. Bhakthavatsalam, D. Khashabi, T. Khot, B. D. Mishra, K. Richardson, A. Sabharwal, C. Schoenick, O. Tafjord, and P. Clark. Think you have solved direct-answer question answering? Try ARC-DA, the direct-answer AI2 reasoning challenge. arXiv preprint arXiv:2102.03315, 2021.
N. Bostrom. Superintelligence: Paths, Dangers, Strategies. Oxford University Press, 2014.
T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.
H. Cheng, Y. Shen, X. Liu, P. He, W. Chen, and J. Gao. UnitedQA: A hybrid approach for open domain question answering. arXiv preprint arXiv:2101.00178, 2021.
D. Chong and J. N. Druckman. Framing theory. Annu. Rev. Polit. Sci., 10:103-126, 2007.
P. Christiano, B. Shlegeris, and D. Amodei. Supervising strong learners by amplifying weak experts. arXiv preprint arXiv:1810.08575, 2018.
O. Evans, O. Cotton-Barratt, L. Finnveden, A. Bales, A. Balwit, P. Wills, L. Righetti, and W. Saunders. Truthful AI: Developing and governing AI that does not lie. arXiv preprint arXiv:2110.06674, 2021.
A. Fan, Y. Jernite, E. Perez, D. Grangier, J. Weston, and M. Auli. ELI5: Long form question answering. arXiv preprint arXiv:1907.09190, 2019.
D. Ferrucci, E. Brown, J. Chu-Carroll, J. Fan, D. Gondek, A. A. Kalyanpur, A. Lally, J. W. Murdock, E. Nyberg, J. Prager, et al. Building watson: An overview of the deepqa project. AI magazine, 31 (3):59-79, 2010.
K. Goddard, A. Roudsari, and J. C. Wyatt. Automation bias: a systematic review of frequency, effect mediators, and mitigators. Journal of the American Medical Informatics Association, 19(1): $121-127,2012$.
I. Gur, U. Rueckert, A. Faust, and D. Hakkani-Tur. Learning to navigate the web. arXiv preprint arXiv:1812.09195, 2018.
K. Guu, K. Lee, Z. Tung, P. Pasupat, and M.-W. Chang. REALM: Retrieval-augmented language model pre-training. arXiv preprint arXiv:2002.08909, 2020.</p>
<p>M. Harms. Crystal Society. Crystal Trilogy. CreateSpace Independent Publishing Platform, 2016. ISBN 9781530773718.
G. Irving, P. Christiano, and D. Amodei. AI safety via debate. arXiv preprint arXiv:1805.00899, 2018.
M. Joshi, E. Choi, D. S. Weld, and L. Zettlemoyer. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017.
V. Karpukhin, B. OÄŸuz, S. Min, P. Lewis, L. Wu, S. Edunov, D. Chen, and W.-t. Yih. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906, 2020.
K. Krishna, A. Roy, and M. Iyyer. Hurdles to progress in long-form question answering. arXiv preprint arXiv:2103.06332, 2021.
J. Leike, D. Krueger, T. Everitt, M. Martic, V. Maini, and S. Legg. Scalable agent alignment via reward modeling: a research direction. arXiv preprint arXiv:1811.07871, 2018.
P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. KÃ¼ttler, M. Lewis, W.-t. Yih, T. RocktÃ¤schel, et al. Retrieval-augmented generation for knowledge-intensive NLP tasks. arXiv preprint arXiv:2005.11401, 2020a.
P. Lewis, P. Stenetorp, and S. Riedel. Question and answer test-train overlap in open-domain question answering datasets. arXiv preprint arXiv:2008.02637, 2020b.
S. Lin, J. Hilton, and O. Evans. TruthfulQA: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958, 2021.
J. Maynez, S. Narayan, B. Bohnet, and R. McDonald. On faithfulness and factuality in abstractive summarization. arXiv preprint arXiv:2005.00661, 2020.
D. Metzler, Y. Tay, D. Bahri, and M. Najork. Rethinking search: Making experts out of dilettantes. arXiv preprint arXiv:2105.02274, 2021.
B. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM journal on control and optimization, 30(4):838-855, 1992.
J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
T. Shi, A. Karpathy, L. Fan, J. Hernandez, and P. Liang. World of bits: An open-domain platform for web-based agents. In International Conference on Machine Learning, pages 3135-3144. PMLR, 2017.
K. Shuster, S. Poff, M. Chen, D. Kiela, and J. Weston. Retrieval augmentation reduces hallucination in conversation. arXiv preprint arXiv:2104.07567, 2021.
N. Stiennon, L. Ouyang, J. Wu, D. M. Ziegler, R. Lowe, C. Voss, A. Radford, D. Amodei, and P. Christiano. Learning to summarize from human feedback. arXiv preprint arXiv:2009.01325, 2020.
X. Yuan, J. Fu, M.-A. Cote, Y. Tay, C. Pal, and A. Trischler. Interactive machine comprehension with information seeking agents. arXiv preprint arXiv:1908.10449, 2019.</p>
<h1>A Environment design details</h1>
<p>Our text-based web-browsing environment is written mostly in Python with some JavaScript. For a high-level overview, see Section 2. Further details are as follows:</p>
<ul>
<li>When a search is performed, we send the query to the Microsoft Bing Web Search API, and convert this to a simplified web page of results.</li>
<li>When a link to a new page is clicked, we call a Node.js script that fetches the HTML of the web page and simplifies it using Mozilla's Readability.js.</li>
<li>We remove any search results or links to reddit.com or quora.com, to prevent the model copying answers from those sites.</li>
<li>We take the simplified HTML and convert links to the special format
$\left[\right.$ <link ID> $\uparrow&lt;$ link text $&gt;\uparrow&lt;$ destination domain&gt; $\left.]\right]$, or
[<link ID> $\uparrow&lt;$ link text&gt; $\left.\right]$ if the destination and source domains are the same. Here, the link ID is the index of the link on the page, which is also used for the link-clicking command. We use special characters such as { and } because they are rare and encoded in the same few ways by the tokenizer, and if they appear in the page text then we replace them by similar alternatives.</li>
<li>We convert superscripts and subscripts to text using $\sim$ and ,, and convert images to the special format [Image: <alt text>], or [Image] if there is no alt text.</li>
<li>We convert the remaining HTML to text using html2text.</li>
<li>For text-based content types other than HTML, we use the raw text. For PDFs, we convert them to text using pdfminer.six. For all other content types, and for errors and timeouts, we use an error message.</li>
<li>We censor any pages that contain a 10-gram overlap with the question (or reference answer, if provided) to prevent the model from cheating, and use an error message instead.</li>
<li>We convert the title of the page to text using the format <page title> (<page domain>). For search results pages, we use Search results for: <query>.</li>
<li>When a find in page or quote action is performed, we compare the text from the command against the page text with any links stripped (i.e., including only the text from each link). We also ignore case. For quoting, we also ignore whitespace, and allow the abbreviated format <start text> $&gt;&lt;$ end text&gt; to save tokens.</li>
<li>During browsing, the state of the browser is converted to text as shown in Figure 1(b). For the answering phase (the last step of the episode), we convert the question to text using the format <question>â– , and follow this by each of the collected quotes in the format [<quote number>] <quote page title> (<quote page domain>) <double new line><quote extract>â– .</li>
</ul>
<h1>B Question dataset details</h1>
<p>For our demonstration and comparison datasets, the vast majority of questions were taken from ELI5 [Fan et al., 2019], to which we applied the follow post-processing:</p>
<ol>
<li>We included URLs in full, rather than using special <em>URL</em> tokens.</li>
<li>We filtered out questions with the title "[deleted by user]", and ignored the selftext "[deleted]" and "[removed]". (The "selftext" is the body of the post.)</li>
<li>We concatenated the title and any non-empty selftext, separated by a double new line.</li>
<li>We prepended "Explain: " to questions that were not phrased as actual questions (e.g., we used "Explain: gravity" rather than simply "gravity").</li>
</ol>
<p>The final step was performed because there is sometimes an implicit "Explain Like I'm Five" at the start of questions. We considered a question to be phrased as an actual question if it included either a question mark, or one of the following sequences of characters with a regex-word boundary at either end, case-insensitively:
explain, eli5, which, what, whats, whose, who, whos, whom, where, wheres, when, whens, how, hows, why, whys, am, is, isn, isnt, are, aren, arent, was, wasn, wasnt, were, weren, werent, do, don, dont, does, doesn, doesnt, did, didn, didnt, can, cant, could, couldn, couldnt, have, haven, havent, has, hasn, hasnt, may, might, must, mustn, mustnt, shall, shant, should, shouldn, shouldnt, will, wont, would, wouldn, wouldnt</p>
<p>For diversity and experimentation, we also mixed in a small number of questions from the following datasets:</p>
<ul>
<li>TriviaQA. This is a dataset of short-form questions taken from trivia websites [Joshi et al., 2017].</li>
<li>AI2 Reasoning Challenge (ARC). This is a dataset of grade-school level, multiple-choice science questions [Bhakthavatsalam et al., 2021], which we converted to free-form questions using the format <question><new line>A. <option A><new line>.... This dataset is sub-divided into two difficulties, "Challenge" and "Easy".</li>
<li>Hand-written. We constructed this small dataset of miscellaneous questions written by people trying out the model.</li>
<li>ELI5 fact-check. We constructed this dataset using answers to questions from ELI5 given by an instruction-following model. ${ }^{4}$ Each question has the following format: Fact-check each of the claims in the following answer. <double new line>Question: <ELI5 question><double new line>Answer: <model answer></li>
</ul>
<p>The numbers of demonstrations and comparisons we collected for each of these datasets are given in Table 4.</p>
<p>Table 4: Breakdown of our demonstrations and comparisons by question dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Question dataset</th>
<th style="text-align: right;">Demonstrations</th>
<th style="text-align: right;">Comparisons</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ELI5</td>
<td style="text-align: right;">5,711</td>
<td style="text-align: right;">21,068</td>
</tr>
<tr>
<td style="text-align: left;">ELI5 fact-check</td>
<td style="text-align: right;">67</td>
<td style="text-align: right;">185</td>
</tr>
<tr>
<td style="text-align: left;">TriviaQA</td>
<td style="text-align: right;">143</td>
<td style="text-align: right;">134</td>
</tr>
<tr>
<td style="text-align: left;">ARC: Challenge</td>
<td style="text-align: right;">43</td>
<td style="text-align: right;">84</td>
</tr>
<tr>
<td style="text-align: left;">ARC: Easy</td>
<td style="text-align: right;">83</td>
<td style="text-align: right;">77</td>
</tr>
<tr>
<td style="text-align: left;">Hand-written</td>
<td style="text-align: right;">162</td>
<td style="text-align: right;">0</td>
</tr>
<tr>
<td style="text-align: left;">Total</td>
<td style="text-align: right;">6,209</td>
<td style="text-align: right;">21,548</td>
</tr>
</tbody>
</table>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>C Data collection details</h1>
<p>To collect demonstrations and comparisons, we began by hiring freelance contractors from Upwork (https://www.upwork.com), and then worked with Surge AI (https://www.surgehq.ai) to scale up our data collection. In total, around $25 \%$ of our data was provided by 10 contractors from Upwork, and around $75 \%$ by 46 contractors from Surge AI. The top 5 contractors provided around $50 \%$ of the data.</p>
<p>For both types of task, we provided contractors with a video and a detailed instruction document (linked below). Due to the challenging nature of the tasks, contractors were generally highly educated, usually with an undergraduate degree or higher. Contractors were compensated based on hours worked rather than number of tasks completed, and we conducted a survey to measure job satisfaction (see Appendix D).
For data quality, we put prospective contractors through a paid trial period lasting a few hours, and manually checked their work. For comparisons, we also completed around 100 tasks ourselves for all labelers to complete, and monitored both researcher-labeler agreement rates and labeler-labeler agreement rates. Treating the agreement rate between a neutral label and a non-neutral label as $50 \%$, we measured a final researcher-labeler agreement rate of $74 \%$, and a labeler-labeler agreement rate of $73 \%$.</p>
<p>Demonstrations took an average of around 15 minutes each, and comparisons took an average of around 10 minutes each. Despite conventional wisdom that human labelling tasks should be quick and repeatable, we did not think it would be straightforward to decompose our tasks into significantly simpler ones, but we consider this to be a promising direction for further research.</p>
<h2>C. 1 Demonstrations</h2>
<p>We designed the demonstration interface in such a way that, as a rule, the user is given the same information as the model, and has the same actions available. There were a couple of exceptions to this:</p>
<ol>
<li>Unlike humans, the model has no memory of previous steps. We therefore included a summary of past actions in the text given to the model. However, we felt that it was unnecessary to display this to humans.</li>
<li>The Scrolled <up, down> &lt;2, 3&gt; actions are useful for reducing the number of actions taken, but humans are used to scrolling one step at a time. We therefore made these actions unavailable to humans, and instead simply merged any repeated Scrolled <up, down> 1 actions that they made.</li>
</ol>
<p>The full instruction document we provided to contractors for demonstrations can be viewed here.</p>
<h2>C. 2 Comparisons</h2>
<p>To minimize label noise, it is important to make comparisons as unambiguous as possible. We therefore designed the following procedure for comparing two answers to a given question:</p>
<ol>
<li>Read the question, and flag if it does not make sense or should not be answered (in which case the rest of the comparison is skipped).</li>
<li>Read the first answer and its references.</li>
<li>Rate the trustworthiness of any references relied upon by the answer.</li>
<li>Annotate each of the claims in the answer with the level of support it has and its relevance to the question. A screenshot of the annotation tool is shown in Figure 9.</li>
<li>Repeat steps 2-4 for the second answer and its references.</li>
<li>Give comparison ratings for the amount of unsupported and irrelevant information, the usefulness of information with different levels of support, and coherence.</li>
<li>Weighing everything up, give a final comparison rating for overall usefulness.</li>
</ol>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Screenshot from the comparison interface, showing the annotation tool.</p>
<p>For each of the comparison ratings, we used a 5-point Likert scale with the options "A much better", "A better", "Equally good", "B better" and "B much better".</p>
<p>Importantly, we did not require contractors to perform independent research to judge the factual accuracy of answers, since this would have been difficult and subjective. Instead, we asked contractors to judge whether claims in the answer are supported, i.e., either backed up by a reliable reference, or common knowledge.</p>
<p>For the final comparison rating, we encouraged contractors to use their best judgment, but to roughly consider the following criteria in descending order of priority:</p>
<ul>
<li>Whether or not the answer contains unsupported information.</li>
<li>Whether or not the core question has been answered.</li>
<li>Whether or not there is additional helpful information, which does not necessarily need to answer the question directly.</li>
<li>How coherent the answer is, and whether or not there are any citation errors.</li>
<li>How much irrelevant information there is in the answer. (This can be higher priority in extreme cases.)</li>
</ul>
<p>The full instruction document we provided to contractors for comparisons can be viewed here.
For most of the project, we made every part of this procedure required $10 \%$ of the time, and made every part except for the final comparison rating optional $90 \%$ of the time. Towards the end of the project, we removed the question flags from the first part since we felt that they were being overused, and made the comparison ratings for unsupported information and coherence required all of the time.</p>
<p>Despite the complexity of this procedure, we only used the final comparison rating in training, even collapsing together the "much better" and "better" ratings. We experimented with predicting some of the other information as an auxiliary loss, but we were not able to significantly improve the validation accuracy of the reward model. Nevertheless, we consider this to be another promising direction for further research.</p>
<h1>D Contractor survey</h1>
<p>It was valuable to gather feedback from our contractors, both to understand and improve their process, and to monitor job satisfaction. To this end, we sent them a questionnaire with the following questions:</p>
<ul>
<li>
<p>Please say how much you agree with each of the statements. (Required 5-point Likert rating and optional comments)</p>
</li>
<li>
<p>It was clear from the instructions what I was supposed to do.</p>
</li>
<li>I found the task enjoyable and engaging.</li>
<li>I found the task repetitive.</li>
<li>I was paid fairly for doing the task.</li>
<li>
<p>Overall, I am glad that I did this task.</p>
</li>
<li>
<p>What would you change about the task to make it more engaging or enjoyable? (Encouraged)</p>
</li>
<li>Are there any other tools you could be given that would make it easier to complete the task to a consistently high standard? (Encouraged)</li>
<li>Did you come up with any shortcuts that you used to do the task more quickly, and if so, what were they? (Encouraged)</li>
<li>Do you have any other comments? (Optional)</li>
</ul>
<p>The "encouraged" questions were required questions but with instructions to put "N/A" if they really could not think of anything (this was rare).
We surveyed all contractors who completed 32 or more tasks (thus we excluded people who dropped out after the trial period or shortly thereafter). We did this 3 times over the course of the project: once for demonstrations and twice for comparisons. The quantitative results from these surveys are given in Figure 10. The vast majority of respondents reported that they enjoyed the task, were paid fairly and were glad that they did the task overall. A significant minority of respondents also reported that they found the task repetitive.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: Likert ratings aggregated over all 3 of our contractor surveys. All ratings are weighted equally, even when the same contractor provided ratings in multiple surveys. In total, there are 41 ratings for each question.</p>
<h1>E Hyperparameters</h1>
<p>Hyperparameters for all of our training methods are given in Tables 6 and 7. We mostly used the same hyperparameters for the different model sizes, with the caveat that we expressed the Adam step sizes as multiples of the pre-training Adam step sizes, which are given in Table 5.</p>
<p>For each training method, we implemented some form of early stopping:</p>
<ol>
<li>For BC, we stopped after a certain number of epochs based on reward model score (which usually improves past the point of minimum validation loss).</li>
<li>For RM, we stopped after a certain number of epochs based on validation accuracy.</li>
<li>For RL, we stopped after a certain number of PPO iterations based on the reward model score for some KL budget. The KL here is measured from the BC model, and summed over the episode. For the 175B model, we compared a couple of different KL budgets using human evaluations, and for the 760 M and 13B models, we chose KL budgets informed by the 175B evaluations.</li>
</ol>
<p>The points at which we early stopped are given in Table 8.
We tuned hyperparameters using similar criteria to early stopping. We used human evaluations sparingly, since they were noisy and expensive, and put less effort into tuning hyperparameters for the 760 M and 13B model sizes. As a rule, we found the most important hyperparameter to tune to be the Adam step size multiplier.</p>
<p>For BC and RM, we used Polyak-Ruppert averaging [Polyak and Juditsky, 1992], taking an exponentially-weighted moving average (EMA) of the weights of the model as the final checkpoint. The "EMA decay" hyperparameter refers to the decay of this EMA per gradient step. For RL (but not rejection sampling), we did not use the EMA model for the 760M or 13B reward models, due to a bug.</p>
<p>For RL, most PPO hyperparameters did not require tuning, but a few points are worth noting:</p>
<ul>
<li>As discussed in Section 3 of the paper, the reward is the sum of the reward model score at the end of each episode and a KL penalty from the BC model at each token. Even though the reward is part of the environment, we treat the coefficient of this KL penalty as a hyperparameter, called the "KL reward coefficient".</li>
<li>We express hyperparameters such that each timestep corresponds to a single completion (rather than a single token), but we applied PPO clipping and the KL reward at the token level. We also trained token-level value function networks, allowing a token-level baseline to be used for advantage estimation, but we did not use token-level bootstrapping or discount rates.</li>
<li>We used separate policy and value function networks for simplicity, although we think that using shared networks is a promising direction for future research.</li>
<li>We used 1 epoch, since we were concerned more with compute efficiency than with sample efficiency.</li>
<li>Due to GPU memory constraints, we used 16 times as many minibatches per epoch as the default for PPO, but this was easily compensated for by reducing the Adam step size multiplier by a factor of 4 .</li>
<li>We used the same number of parallel environments and timesteps per rollout as the default for PPO, even though it resulted in slow PPO iterations (lasting multiple hours). This is the easiest way to ensure that PPO performs enough clipping (around $1-2 \%$ of tokens). Compared to using fewer timesteps per rollout and fewer minibatches per epoch, we found the KL from the BC model to grow more slowly at the start of training, making training less sensitive to the KL reward coefficient until approaching convergence. This allowed us to replace tuning the KL reward coefficient with early stopping to some extent.</li>
<li>We did not use an entropy bonus, which is usually used for exploration. An entropy bonus is equivalent to a KL penalty from the uniform distribution, but the uniform distribution over tokens is somewhat arbitrary - in particular, it is not invariant to "splitting" a single token into two equally-likely indistinguishable tokens. Instead, the KL reward prevents</li>
</ul>
<p>entropy collapse in a more principled way. We still found it useful to measure entropy for monitoring purposes.</p>
<ul>
<li>We happened to use a GAE discount rate of 1 rather than the usual default of 0.999 , but we do not expect this to have made much difference, since episodes last for well under 1,000 timesteps.</li>
<li>As discussed in Section 3 of the paper, at the end of each episode we inserted additional answering-only episodes using the same references as the previous episode, which is what the "answer phases per browsing phases" hyperparameter refers to.</li>
<li>Since some actions (such as quotes and answers) require many more tokens than others, we modified the environment to "chunk" long completions into multiple actions, to improve rollout parallelizability. This is what the "maximum tokens per action" hyperparameter refers to. Note that it has a minor effect on GAE.</li>
</ul>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ https://beta.openai.com/docs/engines/instruct-series-beta&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7198 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7198</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7198</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-135.html">extraction-schema-135</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <p><strong>Paper ID:</strong> paper-055af7789c29452bf808a48f74ec25e01049425e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/055af7789c29452bf808a48f74ec25e01049425e" target="_blank">GOFA: A Generative One-For-All Model for Joint Graph Language Modeling</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> This paper identifies three key desirable properties of a GFM: self-supervised pretraining, fluidity in tasks, and graph awareness and proposes a novel generative graph language model GOFA to solve the problem.</p>
                <p><strong>Paper Abstract:</strong> Foundation models, such as Large Language Models (LLMs) or Large Vision Models (LVMs), have emerged as one of the most powerful tools in the respective fields. However, unlike text and image data, graph data do not have a definitive structure, posing great challenges to developing a Graph Foundation Model (GFM). For example, current attempts at designing general graph models either transform graph data into a language format for LLM-based prediction or still train a GNN model with LLM as an assistant. The former can handle unlimited tasks, while the latter captures graph structure much better -- yet, no existing work can achieve both simultaneously. In this paper, we identify three key desirable properties of a GFM: self-supervised pretraining, fluidity in tasks, and graph awareness. To account for these properties, we extend the conventional language modeling to the graph domain and propose a novel generative graph language model GOFA to solve the problem. The model interleaves randomly initialized GNN layers into a frozen pre-trained LLM so that the semantic and structural modeling abilities are organically combined. GOFA is pre-trained on newly proposed graph-level next-word prediction, question-answering, and structural tasks to obtain the above GFM properties. The pre-trained model is further fine-tuned on downstream tasks to obtain task-solving ability. The fine-tuned model is evaluated on various downstream tasks, demonstrating a strong ability to solve structural and contextual problems in zero-shot scenarios. The code is available at https://github.com/JiaruiFeng/GOFA.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7198.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7198.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Text-Attribute Graph (TAG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-to-text input representation where every node and edge is associated with a textual description; used as the unified input format for GOFA so arbitrary graphs (including non-textual features) can be fed to a language model by verbalizing features as text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>One for all: Towards training one graph model for all classification tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Text-Attribute Graph (TAG)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each node v and each edge e in graph G is assigned a short text string x(v) / x(e). The TAG is realized by concatenating (or otherwise arranging) these node/edge text descriptions into inputs for the model; non-textual features (numerical, categorical) are converted to textual sentences (e.g., "The degree of this node is 3"). A special Node-of-Generation (NOG) token/node is attached to indicate where generation should start.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>token-based; sequential (per-node sentences assembled into an input graph representation)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Attribute-first serialization: verbalize node and edge attributes as sentences per node/edge, assemble k-hop rooted subgraph texts around the target NOG (prompt node) and feed compressed memory-token embeddings of those sentences into the model. (Follows OFA's TAG concept extended in this work.)</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>MAG240M, PubMed, Arxiv, Wikikg90mv2, WikiGraph, Ultrachat200k (pretrain); evaluated on Cora, Products, WikiCS, ExplaGraphs, FB15k237, SceneGraphs, others (downstream)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Self-supervised graph completion / sentence completion; downstream node/link/graph classification and QA (converted into graph completion via NOG)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GOFA (GNN-interleaved LLM with ICAE compressor + LLM decoder)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GOFA interleaves token-level GNN layers with a frozen pre-trained decoder-only LLM compressor (ICAE-style) that produces K memory token embeddings per node; GNN does message passing on those memory tokens; a decoder LLM (ICAE decoder / same family) generates free-form text from the NOG embeddings. Implemented on top of Mistral-7B ICAE compressor in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Perplexity on held-out node text; downstream accuracy (node/link/graph classification), AUROC for molecule tasks; structural metrics (shortest-path accuracy, common-neighbor count)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GOFA (using TAG as input) achieves lower perplexity than the base LLM on Cora/Product/WikiCS (numbers in Table 1 of paper), and strong zero-shot accuracies: e.g., Cora-Node ~70.8% (GOFA-T), WikiCS ~71.17% (GOFA-T), Products ~79.33% (GOFA-T); also improved shortest-path and common-neighbor predictions versus base LLM. (Exact numeric perplexities not reproduced in main text tables.)</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Enabled large-scale self-supervised pretraining by recasting arbitrary graphs into next-token-style graph-completion tasks; facilitates cross-domain learning and enables zero-shot downstream transfer; reduces sequence-length explosion compared to naive concatenation because compressor compresses per-node sentences into fixed K memory tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires verbalizing non-textual features which can be verbose; canonical ordering not specified so determinism depends on chosen subgraph extraction and prompt design; compressor in current implementation is frozen (paper notes frozen LLM compressor may impact adaptability); average token counts per graph not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Compared to flattening a whole graph into a single long text (LLM-N baseline), TAG + ICAE compressor + token-level GNN retains structural information with much lower per-layer attention cost (GOFA complexity O(V k^2) vs O((V k)^2) for naive concatenation) and achieves higher downstream accuracy and lower memory use. Compared to GNN-only approaches, TAG enables generative free-form outputs and LLM-style fluid task handling while preserving structure via interleaved GNN layers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GOFA: A Generative One-For-All Model for Joint Graph Language Modeling', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7198.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7198.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-N (flattened text baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Concatenated node-text plus explicit edge-sentences (LLM-N baseline used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline graph-to-text encoding used in the paper: concatenate all node texts into a single long prompt and append explicit natural-language edge descriptions (e.g., "Node A connects to Node B") to feed a vanilla LLM for downstream graph tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Concatenated node text + edge sentences (flattened graph)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Serialize the graph by listing node textual descriptions sequentially and then listing edges as short natural-language sentences (edge list verbalization) appended to the prompt; task/question text appended as usual to form a single LLM input sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential; token-based; lossy in structure modeling (due to LLM attention limitations and long sequences)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Edge-list ordering with node-text-first concatenation; simple sequential serialization (no compression or per-node memory tokens).</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Evaluated (as baseline) on ExplaGraphs, WikiCS (OOM for LLM due to long sequences), Cora-Link, FB15k237 and others in Table 4/5 comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Zero-shot node/link/graph classification and QA when fed directly into an LLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Base LLMs used as predictor baseline (Llama2-7B, Mistral-7B etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Off-the-shelf decoder-only LLMs (e.g., Llama2-7B, Mistral-7B) prompted with the flattened textualized graph; no GNN interleaving or compression.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Downstream accuracy (classification), per-sample inference time, memory/OOM behaviour</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Examples from Table 4: ExplaGraphs Acc 74.13 (LLM-N) vs GOFA-F 79.49; Cora-Link Acc 50.36 (LLM-N) vs GOFA-F 85.10; FB15k237 Acc 51.25 (LLM-N) vs GOFA-F 73.49. LLM-N ran out-of-memory on WikiCS in the experiment (denoted OOM). Per-sample time (LLM-N) higher: e.g., 1.50s vs GOFA-F 0.48s on ExplaGraphs; 3.84s vs 1.67s on Cora-Link.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>When used as a training/prompting representation, straightforward and leverages LLM fluidity, but causes long-sequence memory blow-up and inferior structural reasoning compared to GOFA's interleaving approach; higher inference time and OOM on some datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>High token cost and quadratic attention growth leading to OOM on larger graphs; poor explicit exploitation of graph sparsity and structural relations (LMs struggle to extract structure from flattened text), worse structural-task performance (shortest-path, neighbor counts) compared to GNN-equipped GOFA.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Paper shows this flattening approach underperforms GOFA: GOFA attains higher accuracy, better structural task performance, and lower inference time by using per-node compression + token-level GNN message passing instead of a single long sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GOFA: A Generative One-For-All Model for Joint Graph Language Modeling', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7198.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7198.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QA-chain graph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Question-Answer chain converted to chain-graph representation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pretraining representation that converts natural-language QA chains into small chain-structured graphs; used so GOFA can learn to generate open-ended answers by connecting an NOG (prompt) to the QA chain.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>QA-chain-as-graph</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Take multi-turn QA pairs (text) and convert them into a chain graph where each QA turn becomes a node (or nodes) connected in sequence, then connect a NOG node with the question text to this chain so the model must generate the answer text conditioned on the chain and graph structure.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential-chain graph; token-based (verbalized nodes/edges)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Chain conversion: each Q/A turn becomes a node sentence; edges are the temporal/turn links; used within the TAG formalism and processed by the ICAE compressor and token-level GNN.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Ultrachat200k (converted into chain-graphs for pretraining) and QA-chain training data used in pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Question Answering pretraining (to teach GOFA free-form answer generation conditioned on graph contexts)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GOFA (ICAE compressor + token-level GNN + LLM decoder)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same GOFA architecture; QA-chain graphs are connected to an NOG prompt node and GOFA is trained to generate the answer text via next-token prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Zero-shot QA accuracy / downstream QA-style task performance; qualitative generation examples</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Paper reports that including QA-chain pretraining improves GOFA's responsiveness to free-form graph QA tasks; GOFA achieves non-trivial QA performance on SceneGraphs and ExplaGraphs (e.g., ExplaGraphs accuracy up to 87.13% for GOFA-T / 88.34% for GOFA-F in some setups). Exact QA-chain-only ablation numeric gains reported qualitatively in Figure 5 (ablation) and Section 5.2.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Imparts fluidity in open-ended generation tasks and makes GOFA responsive to arbitrary natural-language queries attached as NOGs; crucial for the model's free-form QA capability.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires curated QA-chain data and converting dialog to graph form (design choice may bias learning); may not by itself teach fine-grained structural numerical tasks (needs structural tasks also).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Compared to pretraining solely on sentence-completion or structural tasks, QA-chain pretraining specifically improves generative/question-answering capabilities and is complementary to structural tasks and information-retrieval tasks in pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GOFA: A Generative One-For-All Model for Joint Graph Language Modeling', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7198.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7198.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ICAE memory-token compression</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>In-Context Autoencoder (ICAE) compressor (K memory tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sentence compressor that appends K learnable memory tokens to a decoder-only LLM so that the final K memory outputs summarize the sentence; used as the per-node fixed-size representation that the token-level GNN operates on in GOFA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>In-context autoencoder for context compression in a large language model.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>ICAE compressor (K memory-token sentence compression)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>For each node sentence, append K memory tokens to the sentence and pass through a decoder-only LLM (compressor); the outputs corresponding to the K memory tokens serve as a fixed-length multi-token embedding for the sentence, preserving more information than single-vector pooling.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>fixed-length token-based compression; hierarchical (sentence -> K memory tokens -> GNN)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Autoencoder-style training where the compressor's K memory-token outputs are used by a decoder LLM to reconstruct the original sentence; at inference these K tokens are used as node features for token-level GNN message passing.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>ICAE originally trained (cited) on large language data (Ge et al., 2023); used in GOFA pretraining on MAG240M, PubMed, Arxiv, Wiki datasets etc.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Sentence compression for downstream graph language modeling; enables token-level GNN to operate with fixed-size per-node embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ICAE compressor + decoder (backbone for GOFA); used with Mistral-7B in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only transformer trained in autoencoder fashion with appended K memory tokens; in GOFA the compressor is (mostly) frozen and produces K memory-token embeddings per node (K typically small, paper chooses K according to compressor design).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Reconstruction quality (examples shown), downstream impact via perplexity and task accuracy when used in GOFA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative examples of reconstructed text (Table 5) show high-fidelity reconstructions; using ICAE compressor in GOFA yields lower perplexity (compared to base LLM) and improved downstream accuracy and efficiency (see GOFA results in Section 5). Exact compression-specific numeric metrics not tabulated separately in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Enables fixed-size, information-rich per-node representations so GNN message passing can be applied without losing token-level semantics; reduces the sequence length the transformer must attend over (attention is within node tokens only), enabling scalability and efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Paper notes that in their implementation the compressor is frozen during GOFA training, which may reduce adaptability and tight integration between compression and graph message passing; also K-selection and compressor training are extra complexity and the compression is not a lossless representation of full sentence context (though designed to preserve as much as possible).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Compared to naive pooling of LLM token outputs into a single vector, ICAE's multi-token compression preserves more semantic detail and aligns better with downstream decoder LLMs; compared to using an encoder-decoder LLM encoder, ICAE provides fixed-length multi-token outputs that better fit GNN input requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GOFA: A Generative One-For-All Model for Joint Graph Language Modeling', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>One for all: Towards training one graph model for all classification tasks. <em>(Rating: 2)</em></li>
                <li>In-context autoencoder for context compression in a large language model. <em>(Rating: 2)</em></li>
                <li>Graphtext: Graph reasoning in text space <em>(Rating: 1)</em></li>
                <li>GraphGPT: Graph instruction tuning for large language models <em>(Rating: 1)</em></li>
                <li>LLaGA: Large language and graph assistant <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7198",
    "paper_id": "paper-055af7789c29452bf808a48f74ec25e01049425e",
    "extraction_schema_id": "extraction-schema-135",
    "extracted_data": [
        {
            "name_short": "TAG",
            "name_full": "Text-Attribute Graph (TAG)",
            "brief_description": "A graph-to-text input representation where every node and edge is associated with a textual description; used as the unified input format for GOFA so arbitrary graphs (including non-textual features) can be fed to a language model by verbalizing features as text.",
            "citation_title": "One for all: Towards training one graph model for all classification tasks.",
            "mention_or_use": "use",
            "representation_name": "Text-Attribute Graph (TAG)",
            "representation_description": "Each node v and each edge e in graph G is assigned a short text string x(v) / x(e). The TAG is realized by concatenating (or otherwise arranging) these node/edge text descriptions into inputs for the model; non-textual features (numerical, categorical) are converted to textual sentences (e.g., \"The degree of this node is 3\"). A special Node-of-Generation (NOG) token/node is attached to indicate where generation should start.",
            "representation_type": "token-based; sequential (per-node sentences assembled into an input graph representation)",
            "encoding_method": "Attribute-first serialization: verbalize node and edge attributes as sentences per node/edge, assemble k-hop rooted subgraph texts around the target NOG (prompt node) and feed compressed memory-token embeddings of those sentences into the model. (Follows OFA's TAG concept extended in this work.)",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "MAG240M, PubMed, Arxiv, Wikikg90mv2, WikiGraph, Ultrachat200k (pretrain); evaluated on Cora, Products, WikiCS, ExplaGraphs, FB15k237, SceneGraphs, others (downstream)",
            "task_name": "Self-supervised graph completion / sentence completion; downstream node/link/graph classification and QA (converted into graph completion via NOG)",
            "model_name": "GOFA (GNN-interleaved LLM with ICAE compressor + LLM decoder)",
            "model_description": "GOFA interleaves token-level GNN layers with a frozen pre-trained decoder-only LLM compressor (ICAE-style) that produces K memory token embeddings per node; GNN does message passing on those memory tokens; a decoder LLM (ICAE decoder / same family) generates free-form text from the NOG embeddings. Implemented on top of Mistral-7B ICAE compressor in experiments.",
            "performance_metric": "Perplexity on held-out node text; downstream accuracy (node/link/graph classification), AUROC for molecule tasks; structural metrics (shortest-path accuracy, common-neighbor count)",
            "performance_value": "GOFA (using TAG as input) achieves lower perplexity than the base LLM on Cora/Product/WikiCS (numbers in Table 1 of paper), and strong zero-shot accuracies: e.g., Cora-Node ~70.8% (GOFA-T), WikiCS ~71.17% (GOFA-T), Products ~79.33% (GOFA-T); also improved shortest-path and common-neighbor predictions versus base LLM. (Exact numeric perplexities not reproduced in main text tables.)",
            "impact_on_training": "Enabled large-scale self-supervised pretraining by recasting arbitrary graphs into next-token-style graph-completion tasks; facilitates cross-domain learning and enables zero-shot downstream transfer; reduces sequence-length explosion compared to naive concatenation because compressor compresses per-node sentences into fixed K memory tokens.",
            "limitations": "Requires verbalizing non-textual features which can be verbose; canonical ordering not specified so determinism depends on chosen subgraph extraction and prompt design; compressor in current implementation is frozen (paper notes frozen LLM compressor may impact adaptability); average token counts per graph not reported.",
            "comparison_with_other": "Compared to flattening a whole graph into a single long text (LLM-N baseline), TAG + ICAE compressor + token-level GNN retains structural information with much lower per-layer attention cost (GOFA complexity O(V k^2) vs O((V k)^2) for naive concatenation) and achieves higher downstream accuracy and lower memory use. Compared to GNN-only approaches, TAG enables generative free-form outputs and LLM-style fluid task handling while preserving structure via interleaved GNN layers.",
            "uuid": "e7198.0",
            "source_info": {
                "paper_title": "GOFA: A Generative One-For-All Model for Joint Graph Language Modeling",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "LLM-N (flattened text baseline)",
            "name_full": "Concatenated node-text plus explicit edge-sentences (LLM-N baseline used in experiments)",
            "brief_description": "A baseline graph-to-text encoding used in the paper: concatenate all node texts into a single long prompt and append explicit natural-language edge descriptions (e.g., \"Node A connects to Node B\") to feed a vanilla LLM for downstream graph tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Concatenated node text + edge sentences (flattened graph)",
            "representation_description": "Serialize the graph by listing node textual descriptions sequentially and then listing edges as short natural-language sentences (edge list verbalization) appended to the prompt; task/question text appended as usual to form a single LLM input sequence.",
            "representation_type": "sequential; token-based; lossy in structure modeling (due to LLM attention limitations and long sequences)",
            "encoding_method": "Edge-list ordering with node-text-first concatenation; simple sequential serialization (no compression or per-node memory tokens).",
            "canonicalization": false,
            "average_token_length": null,
            "dataset_name": "Evaluated (as baseline) on ExplaGraphs, WikiCS (OOM for LLM due to long sequences), Cora-Link, FB15k237 and others in Table 4/5 comparisons",
            "task_name": "Zero-shot node/link/graph classification and QA when fed directly into an LLM",
            "model_name": "Base LLMs used as predictor baseline (Llama2-7B, Mistral-7B etc.)",
            "model_description": "Off-the-shelf decoder-only LLMs (e.g., Llama2-7B, Mistral-7B) prompted with the flattened textualized graph; no GNN interleaving or compression.",
            "performance_metric": "Downstream accuracy (classification), per-sample inference time, memory/OOM behaviour",
            "performance_value": "Examples from Table 4: ExplaGraphs Acc 74.13 (LLM-N) vs GOFA-F 79.49; Cora-Link Acc 50.36 (LLM-N) vs GOFA-F 85.10; FB15k237 Acc 51.25 (LLM-N) vs GOFA-F 73.49. LLM-N ran out-of-memory on WikiCS in the experiment (denoted OOM). Per-sample time (LLM-N) higher: e.g., 1.50s vs GOFA-F 0.48s on ExplaGraphs; 3.84s vs 1.67s on Cora-Link.",
            "impact_on_training": "When used as a training/prompting representation, straightforward and leverages LLM fluidity, but causes long-sequence memory blow-up and inferior structural reasoning compared to GOFA's interleaving approach; higher inference time and OOM on some datasets.",
            "limitations": "High token cost and quadratic attention growth leading to OOM on larger graphs; poor explicit exploitation of graph sparsity and structural relations (LMs struggle to extract structure from flattened text), worse structural-task performance (shortest-path, neighbor counts) compared to GNN-equipped GOFA.",
            "comparison_with_other": "Paper shows this flattening approach underperforms GOFA: GOFA attains higher accuracy, better structural task performance, and lower inference time by using per-node compression + token-level GNN message passing instead of a single long sequence.",
            "uuid": "e7198.1",
            "source_info": {
                "paper_title": "GOFA: A Generative One-For-All Model for Joint Graph Language Modeling",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "QA-chain graph",
            "name_full": "Question-Answer chain converted to chain-graph representation",
            "brief_description": "A pretraining representation that converts natural-language QA chains into small chain-structured graphs; used so GOFA can learn to generate open-ended answers by connecting an NOG (prompt) to the QA chain.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "QA-chain-as-graph",
            "representation_description": "Take multi-turn QA pairs (text) and convert them into a chain graph where each QA turn becomes a node (or nodes) connected in sequence, then connect a NOG node with the question text to this chain so the model must generate the answer text conditioned on the chain and graph structure.",
            "representation_type": "sequential-chain graph; token-based (verbalized nodes/edges)",
            "encoding_method": "Chain conversion: each Q/A turn becomes a node sentence; edges are the temporal/turn links; used within the TAG formalism and processed by the ICAE compressor and token-level GNN.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "Ultrachat200k (converted into chain-graphs for pretraining) and QA-chain training data used in pretraining",
            "task_name": "Question Answering pretraining (to teach GOFA free-form answer generation conditioned on graph contexts)",
            "model_name": "GOFA (ICAE compressor + token-level GNN + LLM decoder)",
            "model_description": "Same GOFA architecture; QA-chain graphs are connected to an NOG prompt node and GOFA is trained to generate the answer text via next-token prediction.",
            "performance_metric": "Zero-shot QA accuracy / downstream QA-style task performance; qualitative generation examples",
            "performance_value": "Paper reports that including QA-chain pretraining improves GOFA's responsiveness to free-form graph QA tasks; GOFA achieves non-trivial QA performance on SceneGraphs and ExplaGraphs (e.g., ExplaGraphs accuracy up to 87.13% for GOFA-T / 88.34% for GOFA-F in some setups). Exact QA-chain-only ablation numeric gains reported qualitatively in Figure 5 (ablation) and Section 5.2.",
            "impact_on_training": "Imparts fluidity in open-ended generation tasks and makes GOFA responsive to arbitrary natural-language queries attached as NOGs; crucial for the model's free-form QA capability.",
            "limitations": "Requires curated QA-chain data and converting dialog to graph form (design choice may bias learning); may not by itself teach fine-grained structural numerical tasks (needs structural tasks also).",
            "comparison_with_other": "Compared to pretraining solely on sentence-completion or structural tasks, QA-chain pretraining specifically improves generative/question-answering capabilities and is complementary to structural tasks and information-retrieval tasks in pretraining.",
            "uuid": "e7198.2",
            "source_info": {
                "paper_title": "GOFA: A Generative One-For-All Model for Joint Graph Language Modeling",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "ICAE memory-token compression",
            "name_full": "In-Context Autoencoder (ICAE) compressor (K memory tokens)",
            "brief_description": "A sentence compressor that appends K learnable memory tokens to a decoder-only LLM so that the final K memory outputs summarize the sentence; used as the per-node fixed-size representation that the token-level GNN operates on in GOFA.",
            "citation_title": "In-context autoencoder for context compression in a large language model.",
            "mention_or_use": "use",
            "representation_name": "ICAE compressor (K memory-token sentence compression)",
            "representation_description": "For each node sentence, append K memory tokens to the sentence and pass through a decoder-only LLM (compressor); the outputs corresponding to the K memory tokens serve as a fixed-length multi-token embedding for the sentence, preserving more information than single-vector pooling.",
            "representation_type": "fixed-length token-based compression; hierarchical (sentence -&gt; K memory tokens -&gt; GNN)",
            "encoding_method": "Autoencoder-style training where the compressor's K memory-token outputs are used by a decoder LLM to reconstruct the original sentence; at inference these K tokens are used as node features for token-level GNN message passing.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "ICAE originally trained (cited) on large language data (Ge et al., 2023); used in GOFA pretraining on MAG240M, PubMed, Arxiv, Wiki datasets etc.",
            "task_name": "Sentence compression for downstream graph language modeling; enables token-level GNN to operate with fixed-size per-node embeddings",
            "model_name": "ICAE compressor + decoder (backbone for GOFA); used with Mistral-7B in experiments",
            "model_description": "Decoder-only transformer trained in autoencoder fashion with appended K memory tokens; in GOFA the compressor is (mostly) frozen and produces K memory-token embeddings per node (K typically small, paper chooses K according to compressor design).",
            "performance_metric": "Reconstruction quality (examples shown), downstream impact via perplexity and task accuracy when used in GOFA",
            "performance_value": "Qualitative examples of reconstructed text (Table 5) show high-fidelity reconstructions; using ICAE compressor in GOFA yields lower perplexity (compared to base LLM) and improved downstream accuracy and efficiency (see GOFA results in Section 5). Exact compression-specific numeric metrics not tabulated separately in main text.",
            "impact_on_training": "Enables fixed-size, information-rich per-node representations so GNN message passing can be applied without losing token-level semantics; reduces the sequence length the transformer must attend over (attention is within node tokens only), enabling scalability and efficiency.",
            "limitations": "Paper notes that in their implementation the compressor is frozen during GOFA training, which may reduce adaptability and tight integration between compression and graph message passing; also K-selection and compressor training are extra complexity and the compression is not a lossless representation of full sentence context (though designed to preserve as much as possible).",
            "comparison_with_other": "Compared to naive pooling of LLM token outputs into a single vector, ICAE's multi-token compression preserves more semantic detail and aligns better with downstream decoder LLMs; compared to using an encoder-decoder LLM encoder, ICAE provides fixed-length multi-token outputs that better fit GNN input requirements.",
            "uuid": "e7198.3",
            "source_info": {
                "paper_title": "GOFA: A Generative One-For-All Model for Joint Graph Language Modeling",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "One for all: Towards training one graph model for all classification tasks.",
            "rating": 2
        },
        {
            "paper_title": "In-context autoencoder for context compression in a large language model.",
            "rating": 2
        },
        {
            "paper_title": "Graphtext: Graph reasoning in text space",
            "rating": 1
        },
        {
            "paper_title": "GraphGPT: Graph instruction tuning for large language models",
            "rating": 1
        },
        {
            "paper_title": "LLaGA: Large language and graph assistant",
            "rating": 1
        }
    ],
    "cost": 0.01649175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>GOFA: A Generative One-For-All Model for Joint Graph Language Modeling</h1>
<p>Lecheng Kong ${ }^{1 <em>}$ Jiarui Feng ${ }^{1 </em>}$ Hao Liu ${ }^{1 *}$ Chengsong Huang ${ }^{1}$ Jiaxin Huang ${ }^{1}$<br>Yixin Chen ${ }^{1}$ Muhan Zhang ${ }^{2 \dagger}$<br>${ }^{1}$ Washington University in St. Louis ${ }^{2}$ Peking University<br>{jerry.kong, feng.jiarui, liuhao, chengsong, jiaxinh}@wust1.edu<br>ychen25@wust1.edu, muhan@pku.edu.cn</p>
<h4>Abstract</h4>
<p>Foundation models, such as Large Language Models (LLMs) or Large Vision Models (LVMs), have emerged as one of the most powerful tools in the respective fields. However, unlike text and image data, graph data do not have a definitive structure, posing great challenges to developing a Graph Foundation Model (GFM). For example, current attempts at designing general graph models either transform graph data into a language format for LLM-based prediction or still train a GNN model with LLM as an assistant. The former can handle unlimited tasks, while the latter captures graph structure much better-yet, no existing work can achieve both simultaneously. In this paper, we first identify three key desirable properties of a GFM: self-supervised pretraining, fluidity in tasks, and graph awareness. To account for these properties, we extend the conventional language modeling to the graph domain and propose a novel generative graph language model GOFA. The model interleaves randomly initialized GNN layers into a frozen pre-trained LLM so that the semantic and structural modeling abilities are organically combined. GOFA is pre-trained on newly proposed graph-level next-word prediction, questionanswering, structural understanding, and information retrieval tasks to obtain the above GFM properties. The pre-trained model is further instruction fine-tuned to obtain the task-solving ability. Our GOFA model is evaluated on various downstream datasets unseen during the pre-training and fine-tuning phases, demonstrating a strong ability to solve structural and contextual problems in zero-shot scenarios. The code is available at https://github.com/JiaruiFeng/GOFA.</p>
<h2>1 INTRODUCTION</h2>
<p>With the emergence of Large Language Models (LLMs), the field of artificial intelligence is undergoing a profound transformation, shifting from specialized, fragmented models to universal foundation models. A foundation model is pre-trained on large-scale datasets and can be further adapted to diverse downstream tasks using fine-tuning (Hu et al., 2022) or in-context learning (Bommasani et al., 2021; Touvron et al., 2023). Foundation models have been developed in different domains to handle text (Brown et al., 2020; Touvron et al., 2023), image (Kirillov et al., 2023; Bai et al., 2023), and even multi-modal data (Zhang et al., 2023c; Li et al., 2023; Alayrac et al., 2022). Because of their versatility and generalizability, foundation models have become prevalent in these domains.</p>
<p>However, despite preliminary efforts, a foundation model in the graph domain has arguably yet to be proposed. In the graph domain, data are highly flexible and dynamic. For example, social networks receive millions of new connections daily (Hardiman \&amp; Katzir, 2013), and novel molecules and protein structures are frequently discovered (Abramson et al., 2024; Gilmer et al., 2017). While past researchers have proposed specialized models to learn graph data (Ying et al., 2021; Kipf \&amp; Welling, 2017), the models require retraining to accommodate new graphs (Dai et al., 2022; Mo et al., 2022). Moreover, trained models are usually tied to specific applications and cannot be generalized to new domains and tasks. It becomes increasingly difficult for models to adjust to the ever-evolving</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Examples of our pre-training tasks.
nature of graph data. Hence, a graph foundation model (GFM) applicable to new domains/tasks with minimal or no adaptation costs is urgently needed, spurring recent endeavors to study general graph models. In particular, a strong zero-shot ability is both challenging and fascinating for GFM researchers.</p>
<p>The success of LLMs inspired a series of preliminary attempts which use LLMs to develop general graph models. They can be roughly divided into two categories: LLM as a predictor and LLM as an enhancer (Chen et al., 2023). The LLM as a predictor approach transforms graph data into representations that LLMs can understand and use LLMs to generate predictions (Tang et al., 2023). However, as suggested by a recent study (Wang et al., 2023), such an approach falls short of understanding graph structures. This inspired the LLM as an enhancer approach, which adopts LLM to process and unify diverse graph data and feeds them to a GNN to train general graph models (Liu et al., 2023a; Huang et al., 2023a). Nevertheless, because GNN outputs fixed-sized representations/predictions, they can only handle specific tasks such as classification, and cannot generalize to arbitrary, new tasks due to the lack of generation ability. In summary, the current two approaches cannot fully utilize structural information and be generative simultaneously. We discuss the pros and cons of existing approaches in detail in Section 2.</p>
<p>In this paper, we first identify three desirable properties of a graph foundation model (GFM), namely large-scale self-supervised pre-training, fluidity in tasks, and graph understanding. To achieve the first property, we propose a generic graph self-supervised learning problem similar to the nexttoken prediction problem in LLMs, allowing label-agnostic and continual training on highly diverse graph data. We then propose a generative model termed Generative One-For-All (GOFA) that interleaves GNN layers into an LLM to achieve the second and third properties. Such a novel design systematically integrates GNN into an LLM, granting the LLM graph structural learning ability while keeping LLM's original free-form text generation ability. Meanwhile, this design allows the pipeline of the original LLM to remain intact, giving GOFA a close-to-LLM level of task fluidity. We pre-train the model with large-scale real-world graph data, Question-Answer (QA) chain data adopted from the NLP domain, and graph structural data to empower the model with the aforementioned foundational abilities in the graph domain (Examples in Figure 1). After pre-training, we further instruction fine-tune the model on a small amount of data (relative to the pre-training data) to make it understand task formats. The fine-tuned model is finally evaluated on various downstream datasets unseen during pre-training and fine-tuning. GOFA achieved impressive results on the zero-shot scenario, which demonstrates the strong potential of GOFA to serve as a graph foundation model.</p>
<h1>2 A Desired Foundation Model for Graph</h1>
<p>In this section, we elaborate on three crucial properties a true graph foundation model should possess to motivate our GOFA model design. We note that many contemporary works (partly) propose similar ideas to ours and thus we do not claim the credit. We kindly refer readers to the latest surveys (Liu et al., 2023b; Jin et al., 2023; Zhang et al., 2023d) for more discussions on GFMs.</p>
<p>Large-Scale Self-Supervised Pre-training: One fundamental design of LLM is that it unifies all NLP tasks into a single next-token-prediction paradigm, which enables self-supervised pre-training on a large corpus collected from different sources. For pre-training graph models, while numerous efforts have been made from both the LLM as a predictor and LLM as an enhancer approaches, these attempts usually require the learning target to be labeled (Liu et al., 2023a; Chen et al., 2023). However, a graph foundation model should have no constraint on the input graph (has labels or not) and can learn cross-domain knowledge from large-scale graph data in a self-supervised fashion.
Fluidity in Tasks: A graph foundation model should also possess the same level of versatility and fluidity in handling different tasks as an LLM. Specifically, such ability can be broken down into three levels: (a) The graph foundation model can naturally respond appropriately to different graph tasks based on user instructions without requiring task-specific adjustment (e.g., the same model performs classification and question-answering tasks without any modification.) (b) With appropriate instruction-tuning, the model should have in-context learning ability on unseen tasks (e.g., a model tuned on citation network also performs well on knowledge graphs with proper instructions). (c) Users should be able to define new, previously unseen tasks by modifying the graph structure and features in a way that aligns with the universal input representation of the model. They can continuously train the model on new data without special adaptation. Existing approaches that use GNN models as the predictors are usually either restricted in the output format (Liu et al., 2023a; Xia et al., 2024; He et al., 2024a) or need additional fine-tuning on the task head (Sun et al., 2023; Wang et al., 2022). Consequently, despite having better structural modeling ability, such models cannot accommodate task changes or deal with novel tasks, e.g., shifting from a classification task to a question-answering task that requires outputting all shortest paths between two nodes.</p>
<p>Graph Understanding: Since the LLM as a predictor approach uses a generative LLM to take text input and produce text output, it naturally has the fluidity to accept varied prompts to tackle different tasks. However, such an approach processes the structural information poorly (Wang et al., 2023), making the utility of these models limited on many graph tasks. More importantly, even though some recent variants can use auxiliary graph models (such as GNNs) to incorporate structural information (Tang et al., 2023; He \&amp; Hooi, 2024; Zhang et al., 2024), the graph models are frozen and not responsive to different prompts, and the output from the graph models may not be the most relevant to the input prompt. On the contrary, a graph foundation model should account for the unique structural information of graphs such as node degrees, shortest paths, common neighbors, etc., and generate graph representations dependent on the input prompt. It should not only have LLM's prompt learning capability but also learn graph structure and semantic information jointly.</p>
<h1>3 Method</h1>
<p>In this section, we first propose a generative modeling framework for graphs, serving as the graph counterpart of traditional language modeling. Next, we introduce a novel GNN-LLM architecture for the proposed graph generative modeling problem. Finally, we describe the unified pre-training tasks to train GOFA towards the proposed GFM properties.</p>
<h3>3.1 Generative Modeling for Graph</h3>
<p>Unifed task formats. A generative model usually takes existing contexts, such as user prompts and passages, as input to generate conditional output related to the contexts, such as answers and completed sentences. Defining unified input and output formats for tasks in language applications is easy, as they are purely text-based. Further, because both the pre-training and downstream tasks are constructed in the same format (i.e., next-token-prediction), the downstream tasks conveniently adapt the knowledge from pre-training tasks, resulting in surprising capabilities, such as zero-shot learning. However, graph data from different domains vary significantly by input feature (e.g., nodes in a citation network have completely different vector representations as nodes in a knowledge graph) and output target, preventing direct knowledge transfer between tasks. Hence, the first challenge is to define a unified format for graph tasks, such that the model can do large-scale self-supervised pre-training on arbitrary graphs and transfer to downstream tasks seamlessly.
To unify graph task input, we follow the previous work OFA (Liu et al., 2023a) and extend the definition of Text-Attribute Graph (TAG) beyond graphs with text features such as citation and</p>
<p>product networks. In fact, any node and edge features can be represented by texts. For example, in airline networks, airport and flight route details can be converted into textual descriptions for nodes and edges. Non-textural features, like numerical data, can also be transformed into text strings, as in LLMs. Even for graphs without any features, we can still attach sentences like "The degree of this node is 3" to nodes. Formally, a TAG is a graph $G=\left{V, E, X_{V}, X_{E}\right}$ where $V$ and $E$ are the sets of nodes and edges. Each node $v \in V$ (edge $e \in E$ ) corresponds to a text description $x(v) \in X_{V}$ $\left(x(e) \in X_{E}\right)$. Such a format encodes almost all existing graph data and serves well as a general input representation.</p>
<p>For self-supervised language modeling, the generated output essentially completes the input sentence. Such a task requires the model to have a deep semantic and logical understanding of the provided contexts, which is crucial for downstream applications. Similarly, in graph modeling, we aim to achieve the same level of understanding through graph completion tasks. Given a TAG, the output should complete the graph conditioned on its semantic and structural information. We choose to use natural language as the most tangible output format to complete a TAG. Succinctly, all natural language tasks can be modeled as sentence completion, and similarly, we aim to model all graph tasks with graph completion.</p>
<p>Generative Graph Modeling. We then formally define the generative graph modeling framework for graph completion. This framework supports various graph-related tasks, including classification and free-form question answering. An LLM starts generating only from the end of the input sentence. However, in a TAG, every end of a sentence on a node is a potential generation starting point, but users might only be interested in generating output for specific nodes. To accommodate this, we introduce Nodes of Generation (NOG), allowing users to specify starting points for generation. The modeling task is to take a TAG as input and complete the TAG logically and sensibly by completing the sentences on the potentially user-specified nodes.</p>
<p>We define graph generative modeling as the likelihood of the text $y$ associated with the NOG $v$ :</p>
<p>$$
p(y \mid v, G)=\prod_{l=1}^{L} p\left(y_{l} \mid y_{&lt;l}, v, G\right)
$$</p>
<p>where $y_{l}$ is the $l$-th token of $y$, and $y_{&lt;l}$ is its preceding tokens. The NOG $v$ is a completion target node with initial corresponding text $x(v)$, and $x(v)$ can be empty. $G$ contains structural and textual information of neighbor nodes to help the model generate $y$. Under this framework, we can design a range of self-supervised learning tasks. For example, the graph completion task is shown on Figure 2, where the text on the NOG $v$ is incomplete, and the goal is to complete the sentence on it using the existing text and the neighbor information. This task is covered by Equation (1), which encourages the model to have a strong graph structure and feature comprehension ability. Thus, the importance of the framework is that a model properly solves such modeling problems can possess the three properties of GFM discussed in Section 2, thus can benefit diverse downstream tasks, even in the zero-shot fashion. Section F. 2 discusses how the proposed framework applies to various tasks related to the three properties.</p>
<h3>3.2 GOFA : Generative One-For-All Model</h3>
<p>To solve the generative graph modeling problem proposed in Equation (1), we design the GOFA architecture shown in Figure 3. Overall, GOFA consists of a graph language encoder and an LLM decoder. The graph language encoder interleaves GNN layers with LLM compressor layers to learn node representations containing joint structural and semantic information. The LLM decoder is then used to generate texts from the NOG representation. The LLM compressor and decoders are all pre-trained decoder-only transformers. We describe each component in detail as follows.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: GOFA Architecture. Text tokens of TAG's node/edges are concatenated with memory tokens to be input to Graph Language Encoder. GNN layers are interleaved into LLM Compressor layers, where memory embeddings from LLM Compressor Layer are used as node/edge features for token-level GNN message passing. Memory embedding will be used for teacher-forcing training.</p>
<p>LLM compressor: Because GNNs require node and edge representations to have the same input dimension, many previous works propose to pool all tokens' output embeddings from the LLM as the node and edge vector representations and feed them to a GNN (Liu et al., 2023a; Huang et al., 2023a; He &amp; Hooi, 2024). While this approach shows effectiveness in tasks of fixed form, such as classification and regression, it is insufficient in more complex tasks such as generation, as 1) the pooling process inevitably loses semantic information, 2) standard LLMs are not trained in a way such that the pooled output embedding is a summarization of the input sentence, and 3) the pooled representation space is no longer compatible with the space of the downstream LLM decoder. Hence, we adopt a pre-trained sentence compressor (Ge et al., 2023) that preserves as much information as possible from the original sentence in fixed-size multi-token embeddings. The core idea is to compress a sentence into $K$ embeddings instead of one embedding. Specifically, the sentence compressor has the same architecture as a decoder-only LLM, but the sentence to be compressed $\left{q\left(x_{i}\right)\right}<em j="j">{i=1}^{t}$ is appended by a sequence of $K$ memory tokens $\left{q\left(m</em>$, and the $t$-th layer of the LLM is:}\right)\right}_{j=1}^{K</p>
<p>$$
\begin{aligned}
\left{Q_{x}^{t+1}, Q_{m, x}^{t+1}\right} &amp; =\left{q^{t+1}\left(x_{1}\right), \ldots, q^{t+1}\left(x_{l}\right), q^{t+1}\left(m_{1}\right), \ldots, q^{t+1}\left(m_{K}\right)\right} \
&amp; =L L M^{t}\left(\left{q^{t}\left(x_{1}\right), \ldots, q^{t}\left(x_{l}\right), h^{t}\left(m_{1}\right), \ldots, h^{t}\left(m_{K}\right)\right}\right)=L L M^{t}\left(\left{Q_{x}^{t}, H_{x}^{t}\right}\right)
\end{aligned}
$$</p>
<p>We use $Q_{x}^{t}$ and $Q_{m, x}^{t}$ to represent the $t$-th LLM layer outputs corresponding to actual text tokens in sentence $x$ and the $K$ memory tokens appended at the end of text tokens, respectively. We use $H_{x}^{t}$ to represent the $t$-th GNN layer output, which will be explained later. In Equation (2), the text tokens $\left(Q_{x}^{t}\right)$ and memory tokens ( $H_{x}^{t}$, processed by the previous GNN layer) are concatenated as a single sequence of embeddings, which are fed to the current LLM layer. Because the last K tokens attend to all previous tokens, they can compress all information in the sentence into the output embeddings of the K tokens. This compressor architecture is inspired by ICAE (Ge et al., 2023). The compression ability is obtained through auto-encoder-style fine-tuning, as discussed in Appendix A.1.</p>
<p>Token-level GNN: Conventional GNNs take one embedding vector for each node/edge. However, now each node/edge sentence is compressed into $K$ memory token embeddings $Q_{m, x}$. Hence, we propose a simple extension of GNNs to the token level. For node $v \in V$, the $t$-th GNN layer is</p>
<p>$$
H_{x(v)}^{t}[k]=G N N\left(Q_{m, x(v)}^{t}[k],\left{\left(Q_{m, x(u)}^{t}[k], Q_{m, x\left(e_{u v}\right)}^{t}[k]\right) \mid u \in \mathcal{N}(v)\right}\right), \quad k=1 \ldots K
$$</p>
<p>In the GNN layer, tokens at different indices do not communicate. If we directly stack these GNN layers, they degenerate into multiple isolated GNNs for each token. Nevertheless, because we</p>
<p>interleave the GNN layers into the LLM layers, as shown in Figure 3, the isolated tokens exchange information in the subsequent self-attention layers of the LLM. This approach significantly reduces memory usage because we do not allow cross-token attention between different nodes. While edge memory tokens $Q_{m, x(e)}^{t}$ are passed into GNN to assist message passing, their representations are not updated in the GNN layer but directly passed to the next LLM layer, hence $H_{x(e)}^{t}=Q_{m, x(e)}^{t}$. In GOFA, we use a modified Transformer Convolutional GNN (Shi et al., 2021) to be consistent with the transformer architecture of LLM (see Appendix A.3 for details).</p>
<p>We insert one GNN layer between two transformer layers, while the first and the last layer are always transformer layers. In GOFA, we only insert GNN between the last few transformer layers, but this can be flexible depending on the computational resources. Following previous practice, we incorporate feed-forward (FF) layers into the GNN to increase expressivity and residual connections to stabilize training. Moreover, GOFA should maintain the functions of an LLM on plain texts, hence, inspired by the gating mechanism in earlier works (Hochreiter \&amp; Schmidhuber, 1997; Alayrac et al., 2022), we apply a tanh gate, initialized at 0 , to the GNN and FF layer outputs so that the initial model ignores the information from GNN layers and is equivalent to the pre-trained LLM. We introduce weight decay in the gating module to promote gate value staying in large non-zero values only when graph information helps generate more accurate final text outputs.</p>
<p>LLM decoder: After applying the model to the textual graph, the memory tokens $Q_{m, x}$ of every node contain information about the text on the node, the surrounding node text, and the graph structure due to the message-passing process in the GNN layers. Then, for the NOG $v$ and its corresponding target text $y$, we insert $Q_{m, x}$ at the front of the token embeddings of the target text to generate and use teacher-forcing to maximize the standard log-likelihood of $y$ using the next-token-prediction objective. In this way, we have modeled the problem in Equation (1). The compressor, decoder, and GNN parameters can be jointly or separately optimized, potentially with PEFT methods like LoRA (Hu et al., 2022). In this paper, we use ICAE (Ge et al., 2023) as our backbone LLM, but the GOFA architecture is not tied to any specific LLM. More details are discussed in Appendix A.2.</p>
<p>Discussion. Our proposed graph language encoder has several advantages over existing methods. Suppose a graph has $V$ nodes, $E$ edges, and the average number of tokens for all nodes is $k$. The complexity of one GOFA layer is $O\left(V k^{2}\right)$, as the self-attention only happens within each node. Note that we have omitted the extra computation complexity of message-passing because it only happens at individual indices with $O(E) \ll O\left(V k^{2}\right)$ in practical graphs. Instead, if we concatenate texts in all nodes and input them to a regular LLM, the complexity of one layer is $O\left((V k)^{2}\right)$, which is significantly larger than GOFA. Further, introducing GNN layers in LLMs is theoretically more powerful than pure LLMs for modeling graph structures, which is discussed in Appendix E.2.</p>
<h1>3.3 Unified Task Representation in GOFA</h1>
<p>The formulation in Equation (1) provides a natural way for users to query the graph by selecting a NOG. Following OFA (Liu et al., 2023a), we convert all tasks into tasks on $k$-hop rooted subgraphs extracted around the target nodes. For node-level tasks, the target node is a single node in the graph. For link-level tasks, the target nodes are the node pair. If the target node is not specified (e.g., the task is a graph task), we set the default target nodes to all nodes in the graph. We connect a prompt node with the user query as NOG to all target nodes. GOFA completes the prompted input TAG by answering the query on the NOG, which still aligns with the proposed generative modeling framework. This design has several advantages: (1) All tasks are represented by a NOG, so the distribution of all tasks can be unified into a single space, helping the model generalize to unseen tasks from learned task representations; (2) The text feature for the prompt node describes the task details. Connecting the prompt node to target nodes enables the prompt node to query the most important knowledge from the input graph through attention. This ensures the output embedding for NOG is conditionally learned from the GNN process subject to the different prompts. Conversely, most of the previous works (He \&amp; Hooi, 2024; Tang et al., 2023; 2024; Zhang et al., 2024) only computed a fixed embedding for each node before any prompt is introduced.</p>
<h3>3.4 Large-Scale Pre-training</h3>
<p>As discussed in Section 2 and Section 3.1, we design self-supervised pre-training tasks based on the three GFM properties to train GOFA. The training datasets include MAG240M (Hu et al., 2021a),</p>
<p>Pubmed and Arxiv (Hu et al., 2021b) for academic knowledge, Wikikg90mv2 (Hu et al., 2021a) and WikiGraph (proposed by us) for semantic diversity, and Ultrachat200k (Ding et al., 2023) dataset for question-answering ability. Details about the datasets can be found in Appendix C. Each node is assigned a unique ID (e.g., [Node A]) to enable node querying in the graph. We design four pre-training tasks as shown in Figure 1. We describe the rationale of each task below and leave some implementation details and additional discussion in Appendix F and Appendix E.3.</p>
<p>Sentence Completion Task. This task aims for large-scale pre-training (GFM property one) by training GOFA to predict the remaining text in a node based on both the existing node text and the surrounding graph information. Such a task can be applied to any TAG without labeling, thus facilitating large-scale pre-training for GOFA to acquire diverse knowledge.</p>
<p>Structural Understanding Task. This task aims to provide structural modeling ability for GOFA (GFM property three). The structural task connects NOG randomly selected node pairs to generate the actual shortest path or common neighbors between them. Through these two tasks, the model is expected to gain the ability to identify basic graph structures fundamental for graph-related problems.</p>
<p>Question Answering Task. This task aims to ensure fluidity in generation for GOFA (GFM property two). Unlike language corpus, which naturally contains many question-and-answer (QA) pairs, graph data usually only contain objective descriptions of entities. Hence, we convert natural language Question-Answer sequences into chain graphs and connect a NOG with a question to the chain graph for open-ended answer generation. This essential task enables GOFA to be responsive to arbitrary downstream applications expressed in free-form text questions.</p>
<p>Information Retrieval Task. In most downstream tasks, GOFA links a prompt node to target nodes in the graph to address related problems. To facilitate effective information extraction, we design an information retrieval task where a NOG queries a target node using its node ID. The model must retrieve and isolate information specific to the queried node from the remaining target nodes, encouraging a message-passing process conditioned on the input, as discussed in Section 3.2.</p>
<h1>4 RELATED WORK</h1>
<p>Here we mainly discuss the two tracks of general graph models, and leave discussion about graph prompt learning and graph neural networks to Appendix D.</p>
<p>LLMs as enhancers: One direction uses LLMs to convert the text features of graphs to unified representations (Liu et al., 2023a; Chen et al., 2023; Li et al., 2024; He et al., 2024a; Plenz \&amp; Frank, 2024) for downstream graph models to distinguish and transfer knowledge between different domains. For example, OFA (Liu et al., 2023a) uses LLM to unify the input features in different datasets and transforms multiple types of graph classification tasks into a unified binary classification format. TAPE (He et al., 2024a) utilizes LLM to generate question answers and explanations as enhanced node features. Such approaches have good structural modeling ability, but they usually cannot generate free-form output to handle arbitrary tasks.</p>
<p>LLMs as predictors: Another line of research proposes using LLMs as predictors and aligning graph representation with LLM inputs. Preliminary attempts flatten graphs into text representations and feed them into LLM (Chen et al., 2023; Zhao et al., 2023b; Guo et al., 2023; Zhao et al., 2023a; Qian et al., 2023). These approaches can benefit from LLM for task fluidity but fail to model structural information unique to graph data properly (Zhao et al., 2023b; Mao et al., 2024; Ye et al., 2023). Realizing this problem, follow-up work extends methods in vision-language domain (Alayrac et al., 2022; Li et al., 2023) to the graph domain and train adapters to link graph model outputs to LLM (Tang et al., 2023; 2024; Huang et al., 2024; Zhang et al., 2024; He \&amp; Hooi, 2024). For example, GraphGPT (Tang et al., 2023) first implements a text-structure alignment between graph representation and text embedding to pretrain a GNN. LLaGA (Chen et al., 2024) creatively uses a template to represent a subgraph with pooled node embeddings for LLM input. Inspired by Q-former (Li et al., 2023), GraphTranslator (Zhang et al., 2024) aligns node and text tokens from pretrained GNN and LLM. UniGraph (He \&amp; Hooi, 2024) pretrains GNN using masked word prediction and then tuning a projector to map graph embedding to language space and enable zero-shot learning. However, the GNN and LLM parts of these methods are usually detached, meaning the prompt information can not attend to the message-passing process.</p>
<p>Table 2: Zero-shot experiment results with instruction tuning (Accuracy).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Cora-Node</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">WikiCS</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Products</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ExplaGraphs</th>
<th style="text-align: center;">Cora-Link</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Way</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">47</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: center;">LLama2-7B</td>
<td style="text-align: center;">47.92</td>
<td style="text-align: center;">73.45</td>
<td style="text-align: center;">40.10</td>
<td style="text-align: center;">58.77</td>
<td style="text-align: center;">27.65</td>
<td style="text-align: center;">58.71</td>
<td style="text-align: center;">64.33</td>
<td style="text-align: center;">57.76</td>
<td style="text-align: center;">48.15</td>
</tr>
<tr>
<td style="text-align: center;">Mistral-7B</td>
<td style="text-align: center;">60.54</td>
<td style="text-align: center;">88.39</td>
<td style="text-align: center;">63.63</td>
<td style="text-align: center;">71.90</td>
<td style="text-align: center;">43.99</td>
<td style="text-align: center;">70.16</td>
<td style="text-align: center;">74.94</td>
<td style="text-align: center;">68.77</td>
<td style="text-align: center;">49.43</td>
</tr>
<tr>
<td style="text-align: center;">OFA-Llama2</td>
<td style="text-align: center;">28.65</td>
<td style="text-align: center;">56.92</td>
<td style="text-align: center;">21.20</td>
<td style="text-align: center;">35.15</td>
<td style="text-align: center;">19.37</td>
<td style="text-align: center;">30.43</td>
<td style="text-align: center;">39.31</td>
<td style="text-align: center;">51.36</td>
<td style="text-align: center;">52.22</td>
</tr>
<tr>
<td style="text-align: center;">GraphGPT</td>
<td style="text-align: center;">44.65</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">18.84</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">50.74</td>
</tr>
<tr>
<td style="text-align: center;">UniGraph</td>
<td style="text-align: center;">69.53</td>
<td style="text-align: center;">89.74</td>
<td style="text-align: center;">43.45</td>
<td style="text-align: center;">60.23</td>
<td style="text-align: center;">38.45</td>
<td style="text-align: center;">66.07</td>
<td style="text-align: center;">75.73</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">ZeroG</td>
<td style="text-align: center;">64.21</td>
<td style="text-align: center;">87.83</td>
<td style="text-align: center;">31.26</td>
<td style="text-align: center;">48.25</td>
<td style="text-align: center;">31.24</td>
<td style="text-align: center;">51.24</td>
<td style="text-align: center;">71.29</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">LLaGA</td>
<td style="text-align: center;">51.85</td>
<td style="text-align: center;">62.73</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">23.10</td>
<td style="text-align: center;">34.15</td>
<td style="text-align: center;">39.72</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">88.09</td>
</tr>
<tr>
<td style="text-align: center;">GOFA-T</td>
<td style="text-align: center;">70.81</td>
<td style="text-align: center;">85.73</td>
<td style="text-align: center;">71.17</td>
<td style="text-align: center;">80.93</td>
<td style="text-align: center;">54.60</td>
<td style="text-align: center;">79.33</td>
<td style="text-align: center;">87.13</td>
<td style="text-align: center;">79.49</td>
<td style="text-align: center;">85.10</td>
</tr>
<tr>
<td style="text-align: center;">GOFA-F</td>
<td style="text-align: center;">69.41</td>
<td style="text-align: center;">87.52</td>
<td style="text-align: center;">68.84</td>
<td style="text-align: center;">80.62</td>
<td style="text-align: center;">56.13</td>
<td style="text-align: center;">80.03</td>
<td style="text-align: center;">88.34</td>
<td style="text-align: center;">71.34</td>
<td style="text-align: center;">86.31</td>
</tr>
</tbody>
</table>
<h1>5 EXPERIMENT</h1>
<p>This section evaluates the proposed methods by answering the following four questions: Q1: Are the pre-training tasks in GOFA effective for graph-language modeling and structure understanding? Q2: Does the pre-trained GOFA help with critical general graph model application, zero-shot learning? Q3: Is using GOFA more advantageous than LLMs in graph tasks? Q4: Does GOFA have the fluidity to handle open-ended graph-related tasks? Additionally, we also include supervised experiments in Appendix F.5.</p>
<h3>5.1 GOFA PRE-TRAINING</h3>
<p>To answer Q1, we pre-train the GOFA model using ICAE models on Mistral-7B (Jiang et al., 2023), optimizing the objective in Equation (1) using the proposed tasks. The training details can be found in Appendix F.3. After training, we evaluate the perplexity of both GOFA and base LLM on Cora, Product, and Wikics datasets (all three are not included in the pre-training). We report the perplexity in Table 1. Note that during pre-training, we only update the weight of the GNN layers, and GOFA 's lower perplexity shows that the structural and semantic information in the node's neighbor can effectively help complete the sentence with more relevance than the original LLM. Further, to validate that training of GOFA will not affect the original LLMs' ability, we input GOFA with single node graphs without any connections (denoted as GOFA-SN) to evaluate the perplexity, as shown in Table 1. We can see that without connection information around the center node, generation on a single node graph remains comparable to LLM and even better due to the pre-training process, showing that GOFA training does not destroy the desirable property of a pre-trained LLM. Besides sentence completion, another important GOFA pre-training objective is the structure learning ability. We report the shortest path distance and common neighbor count prediction results in Table 1, compared with LLM models whose inputs are textualized graphs with descriptions of edge connections. The datasets we used were Cora and Product. We see a significant performance improvement of GOFA over base LLM, showing that a difficult graph task for LLM can be well solved by the GNN layers with better structure modeling ability.</p>
<h3>5.2 ZERO-SHOT LEARNING WITH GOFA</h3>
<p>To answer Q2, we performed zero-shot experiments on various graph tasks. Despite using QA-chain data in the pre-training stage, the graph data does not include knowledge about task formats like classification and does not output exact matches to the answers. Hence, we first instruction-tuned the pre-trained GOFA in Section 5.1 on a small amount of data. We report the zero-shot results of two GOFA instruction tuning settings named GOFA-T and GOFA-F, as shown in Table 2 and Table 3. GOFA-T includes node and link classification tasks from Arxiv and Pubmed and GOFA-F addtionally adds MAG240M and Wiki90mv2 datasets. The instruction-tuning details can be found in</p>
<p>Table 3: Zero-shot experiment results with instruction tuning on FB15K237 and Scene Graphs.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">FB15K237</th>
<th style="text-align: center;">SceneGraphs</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Format</td>
<td style="text-align: center;">10-Way</td>
<td style="text-align: center;">QA</td>
</tr>
<tr>
<td style="text-align: center;">Llama2-7B</td>
<td style="text-align: center;">48.32</td>
<td style="text-align: center;">38.62</td>
</tr>
<tr>
<td style="text-align: center;">Mistral-7B</td>
<td style="text-align: center;">62.48</td>
<td style="text-align: center;">45.95</td>
</tr>
<tr>
<td style="text-align: center;">GOFA-T</td>
<td style="text-align: center;">73.59</td>
<td style="text-align: center;">34.06</td>
</tr>
<tr>
<td style="text-align: center;">GOFA-F</td>
<td style="text-align: center;">80.69</td>
<td style="text-align: center;">31.36</td>
</tr>
</tbody>
</table>
<p><img alt="img-2.jpeg" src="img-2.jpeg" />
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Performance vs Figure 5: Pre-training Tasks pre-training sample size. Ablation Study.</p>
<p>Table 4: Comparison between GOFA and LLM with the same input.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task <br> Metric</th>
<th style="text-align: center;">ExplaGraphs <br> Acc $\uparrow$</th>
<th style="text-align: center;">Time <br> sec/sample $\downarrow$</th>
<th style="text-align: center;">WikiCS <br> Acc $\uparrow$</th>
<th style="text-align: center;">Time <br> sec/sample $\downarrow$</th>
<th style="text-align: center;">Cora-Link <br> Acc $\uparrow$</th>
<th style="text-align: center;">Time <br> sec/sample $\downarrow$</th>
<th style="text-align: center;">FB15k237 <br> Acc $\uparrow$</th>
<th style="text-align: center;">Time <br> sec/sample $\downarrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">LLM-N</td>
<td style="text-align: center;">74.13</td>
<td style="text-align: center;">1.50</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">OOM</td>
<td style="text-align: center;">50.36</td>
<td style="text-align: center;">3.84</td>
<td style="text-align: center;">51.25</td>
<td style="text-align: center;">3.92</td>
</tr>
<tr>
<td style="text-align: center;">GOFA-F</td>
<td style="text-align: center;">79.49</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">71.17</td>
<td style="text-align: center;">2.43</td>
<td style="text-align: center;">85.10</td>
<td style="text-align: center;">1.67</td>
<td style="text-align: center;">73.49</td>
<td style="text-align: center;">3.37</td>
</tr>
<tr>
<td style="text-align: center;">Improvement</td>
<td style="text-align: center;">$7.23 \%$</td>
<td style="text-align: center;">$68.00 \%$</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">$68.98 \%$</td>
<td style="text-align: center;">$56.51 \%$</td>
<td style="text-align: center;">$43.40 \%$</td>
<td style="text-align: center;">$14.03 \%$</td>
</tr>
</tbody>
</table>
<p>Appendix F.4. Note that the zero-shot datasets are unseen during both pre-training and instruction finetuning. The goal of instruction fine-tuning is not to let the model learn particular knowledge from these datasets but to make the model understand the task format described in Appendix F.4.</p>
<p>While the instruction-tuning dataset only covers the relatively small spectrum of the graph datasets, we observe that GOFA achieves very non-trivial performance on all node-level (Cora-Node, WikiCS, Products), link-level (FB15K237, Cora-Link), and graph-level (ExplaGraphs, SceneGraphs) tasks. GOFA also generalizes to different ways and even question-answering (SceneGraphs) tasks, showing its desirable fluidity. GOFA outperforms LLM and graph foundation model baselines on most datasets and exceeds best baselines by a large margin ( $&gt;10 \%$ ) on WikiCS, Products, FB15K237 and ExplaGraphs, showing GOFA's ability to combine the advantage of both LLM and graph models. GOFA not only achieves remarkable results on the knowledge graph and academic graph, which are proximal to the trained data but also excels in Products and ExplaGraphs whose distribution shifts significantly from training data, which further highlights GOFA 's substantial generalizability. Meanwhile, we observe that GOFA is only achieving comparable performance to LLM on the SceneGraph dataset. We suspect that the instruct-tuning data contains information-dense texts, reducing the model's ability on common sense questions that this dataset requires. In the future, we plan to diversify instruction-tuning datasets with common sense knowledge to enhance such ability.</p>
<p>We further conducted the same experiments on intermediate pre-training checkpoints, and show results in Figure 4. We observe that as the model witnesses more pre-training samples/tokens, the downstream task performance also increases significantly, confirming the importance of large-scale pre-training on graph data. The performance continues to improve, meaning that the model can potentially scale to higher capability with more samples; we leave this to future work. In Figure 5, we plot the instruction-tuning performance when we remove the Wikipedia datasets and information retrieval task (w/o R+W), only remove the retrieval task, (w/o R), and full tasks. We can see that Wikipedia datasets improve the model performance of all the datasets for the diverse corpus it introduced. The retrieval tasks particularly improve the knowledge graph performance due to the improved ability to retrieve key correlations between target entities. These show the necessity and effectiveness of the overall pre-training task selection and design. In Appendix B.1, we further conduct zero-shot experiments on molecule datasets.</p>
<h1>5.3 Comparing GOFA with LLMs</h1>
<p>Answering Q3 is critical to understanding the necessity of the GNN layers and the effectiveness of GOFA as a general graph model. We compare GOFA to LLM whose textual prompt contains the same information as the input graph to GOFA. Specifically, for a GOFA input graph, we concatenate all node texts as the prompt and append the connection information to it, as in "Node A connects</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: GOFA diverse responses to open-ended questions.
to Node B". The text is then combined with task and question descriptions as input to an LLM for classification tasks. Approaches similar to this are widely adopted and acknowledged (Chen et al., 2023; Fatemi et al.). We present both the classification performance and per sample inference time in Table 4 and denote the LLM method as LLM-N. We observe impressive performance improvement of GOFA on all datasets, even when the LLM is prompted with the same information, showing that GOFA, with the help of the GNN and interleaving design, utilizes the graph information much more effectively. Moreover, we also observe a fundamental reduction in inference costs, confirming our analysis in Section 3.2 that, with the same input, GOFA is more efficient than LLMs. Note that when the input size is large, such as in WikiCS, LLM struggles with high memory consumption of the long sequence, whereas the GOFA avoids that by leveraging the sparsity of graph data and using edge information to compute the most important attention information.</p>
<h1>5.4 GOFA RESPONSES ON DIVERSE TASKS</h1>
<p>Finally, we answer Q4 by providing generation examples of GOFA in Figure 6, where we prompt the same citation graph differently and achieved corresponding and high-quality responses. The top and middle examples have the same connection for their NOGs (both connected to the same five nodes), but when we change the prompt text on the NOGs, the generated texts also adjust accordingly, utilizing the neighbor node information, validating that the message-passing is conditioned on the prompt. As in the bottom example, we can also prompt the graph differently by connecting the NOG to two target nodes and querying about the shortest path distance. In this case, the model successfully generates actual paths between the two nodes, which is an ability not seen in traditional graph models that can only output numerical predictions about the path length. These examples demonstrate GOFA's outstanding ability to answer open-ended questions. More examples are provided in B.4.</p>
<h2>6 CONCLUSION, LIMITATIONS, AND Future WORKS</h2>
<p>We introduce GOFA, a generative One-for-All graph foundation model. GOFA is pre-trained under a graph completion framework to enable large-scale self-supervised learning. By integrating GNN layers with LLM layers, GOFA combines the generative capabilities of LLMs for free-form output with the structural learning strengths of GNNs for understanding complex graph connections. Our experiments demonstrate that GOFA, when fine-tuned with a small number of data, achieves impressive zero-shot performance in unseen datasets. However, our zero-shot experiment mainly focuses on evaluating models on unseen datasets with similar task formats instead of unseen task formats. Additionally, we employ a frozen LLM compressor in GOFA ; hence, the compression capability is not naturally unified with the graph data, potentially impacting the adaptability of the model. We will explore harder zero-shot settings and better fine-tuning strategies in the future.</p>
<h1>7 ACKNOWLEDGEMENT</h1>
<p>This work is partially supported by the National Key R\&amp;D Program of China (2022ZD0160300) and NSF grant CBE-2225809.</p>
<h2>REFERENCES</h2>
<p>Josh Abramson, Jonas Adler, Jack Dunger, Richard Evans, Tim Green, Alexander Pritzel, Olaf Ronneberger, Lindsay Willmore, Andrew J Ballard, Joshua Bambrick, et al. Accurate structure prediction of biomolecular interactions with alphafold 3. Nature, pp. 1-3, 2024.</p>
<p>Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in neural information processing systems, 35:23716-23736, 2022.</p>
<p>Yutong Bai, Xinyang Geng, Karttikeya Mangalam, Amir Bar, Alan Yuille, Trevor Darrell, Jitendra Malik, and Alexei A Efros. Sequential modeling enables scalable learning for large vision models. arXiv preprint arXiv:2312.00785, 2023.</p>
<p>Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, S. Buch, Dallas Card, Rodrigo Castellon, Niladri S. Chatterji, Annie S. Chen, Kathleen A. Creel, Jared Davis, Dora Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren E. Gillespie, Karan Goel, Noah D. Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas F. Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, O. Khattab, Pang Wei Koh, Mark S. Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir Mirchandani, Eric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan, Benjamin Newman, Allen Nie, Juan Carlos Niebles, Hamed Nilforoshan, J. F. Nyarko, Giray Ogut, Laurel J. Orr, Isabel Papadimitriou, Joon Sung Park, Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Robert Reich, Hongyu Ren, Frieda Rong, Yusuf H. Roohani, Camilo Ruiz, Jack Ryan, Christopher R'e, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishna Parasuram Srinivasan, Alex Tamkin, Rohan Taori, Armin W. Thomas, Florian Tramr, Rose E. Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan You, Matei A. Zaharia, Michael Zhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, and Percy Liang. On the opportunities and risks of foundation models. ArXiv, abs/2108.07258, 2021. URL https://api.semanticscholar.org/CorpusID:237091588.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1877-1901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/ 2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.</p>
<p>Runjin Chen, Tong Zhao, Ajay Jaiswal, Neil Shah, and Zhangyang Wang. Llaga: Large language and graph assistant. arXiv preprint arXiv:2402.08170, 2024.</p>
<p>Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi Wen, Xiaochi Wei, Shuaiqiang Wang, Dawei Yin, Wenqi Fan, Hui Liu, and Jiliang Tang. Exploring the potential of large language models (llms) in learning on graphs, 2023.</p>
<p>Quanyu Dai, Xiao-Ming Wu, Jiaren Xiao, Xiao Shen, and Dan Wang. Graph transfer learning via adversarial domain adaptation with graph convolution. IEEE Transactions on Knowledge and Data Engineering, 35(5):4908-4922, 2022.</p>
<p>Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional conversations, 2023.</p>
<p>Bahare Fatemi, Jonathan Halcrow, and Bryan Perozzi. Talk like a graph: Encoding graphs for large language models. In The Twelfth International Conference on Learning Representations.</p>
<p>Jiarui Feng, Yixin Chen, Fuhai Li, Anindya Sarkar, and Muhan Zhang. How powerful are k-hop message passing graph neural networks. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=nN3aVRQsxGd.</p>
<p>Jiarui Feng, Lecheng Kong, Hao Liu, Dacheng Tao, Fuhai Li, Muhan Zhang, and Yixin Chen. Extending the design space of graph neural networks by rethinking folklore weisfeiler-lehman. In A. Oh, T. Neumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 9029-9064. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/ 2023/file/1cac8326ce3fbe79171db9754211530c-Paper-Conference.pdf.</p>
<p>Matthias Fey and Jan Eric Lenssen. Fast graph representation learning with pytorch geometric, 2019.
Mikhail Galkin, Xinyu Yuan, Hesham Mostafa, Jian Tang, and Zhaocheng Zhu. Towards foundation models for knowledge graph reasoning. In The Twelfth International Conference on Learning Representations, 2023.</p>
<p>Tao Ge, Hu Jing, Lei Wang, Xun Wang, Si-Qing Chen, and Furu Wei. In-context autoencoder for context compression in a large language model. In The Twelfth International Conference on Learning Representations, 2023.</p>
<p>Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In International conference on machine learning, pp. 1263-1272. PMLR, 2017.</p>
<p>Jiayan Guo, Lun Du, Hengyu Liu, Mengyu Zhou, Xinyi He, and Shi Han. Gpt4graph: Can large language models understand graph structured data ? an empirical evaluation and benchmarking, 2023.</p>
<p>Stephen J Hardiman and Liran Katzir. Estimating clustering coefficients and size of social networks via random walk. In Proceedings of the 22nd international conference on World Wide Web, pp. $539-550,2013$.</p>
<p>Xiaoxin He, Xavier Bresson, Thomas Laurent, Adam Perold, Yann LeCun, and Bryan Hooi. Harnessing explanations: LLM-to-LM interpreter for enhanced text-attributed graph representation learning. In The Twelfth International Conference on Learning Representations, 2024a. URL https://openreview.net/forum?id=RXFVcynVe1.</p>
<p>Xiaoxin He, Yijun Tian, Yifei Sun, Nitesh V. Chawla, Thomas Laurent, Yann LeCun, Xavier Bresson, and Bryan Hooi. G-retriever: Retrieval-augmented generation for textual graph understanding and question answering, 2024b.</p>
<p>Yufei He and Bryan Hooi. Unigraph: Learning a cross-domain graph foundation model from natural language. arXiv preprint arXiv:2402.13630, 2024.</p>
<p>Sepp Hochreiter and Jrgen Schmidhuber. Long short-term memory. Neural Comput., 9(8): 1735-1780, nov 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL https: //doi.org/10.1162/neco.1997.9.8.1735.</p>
<p>Zhenyu Hou, Yufei He, Yukuo Cen, Xiao Liu, Yuxiao Dong, Evgeny Kharlamov, and Jie Tang. Graphmae2: A decoding-enhanced masked self-supervised graph learner. In Proceedings of the ACM Web Conference 2023, WWW '23, pp. 737-746, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9781450394161. doi: 10.1145/3543507.3583379. URL https://doi.org/10.1145/3543507.3583379.</p>
<p>Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum? id=nZeVKeeFYf9.</p>
<p>Weihua Hu, Matthias Fey, Hongyu Ren, Maho Nakata, Yuxiao Dong, and Jure Leskovec. Ogb-lsc: A large-scale challenge for machine learning on graphs, 2021a.</p>
<p>Weihua Hu, Matthias Fey, Hongyu Ren, Maho Nakata, Yuxiao Dong, and Jure Leskovec. Ogb-lsc: A large-scale challenge for machine learning on graphs. arXiv preprint arXiv:2103.09430, 2021b.</p>
<p>Qian Huang, Hongyu Ren, Peng Chen, Gregor Krmanc, Daniel Zeng, Percy Liang, and Jure Leskovec. Prodigy: Enabling in-context learning over graphs. arXiv preprint arXiv:2305.12600, 2023a.</p>
<p>Xuanwen Huang, Kaiqiao Han, Yang Yang, Dezheng Bao, Quanjin Tao, Ziwei Chai, and Qi Zhu. Can gnn be good adapter for llms? arXiv preprint arXiv:2402.12984, 2024.</p>
<p>Yinan Huang, Xingang Peng, Jianzhu Ma, and Muhan Zhang. Boosting the cycle counting power of graph neural networks with i\$^2\$-GNNs. In The Eleventh International Conference on Learning Representations, 2023b. URL https://openreview.net/forum?id=kDSmxOspsXQ.</p>
<p>Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Llio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothe Lacroix, and William El Sayed. Mistral 7b, 2023.</p>
<p>Bowen Jin, Gang Liu, Chi Han, Meng Jiang, Heng Ji, and Jiawei Han. Large language models on graphs: A comprehensive survey. arXiv preprint arXiv:2312.02783, 2023.</p>
<p>Nicolas Keriven and Gabriel Peyr. Universal invariant and equivariant graph neural networks. Advances in Neural Information Processing Systems, 32, 2019.</p>
<p>Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations, 2017. URL https://openreview. net/forum?id=SJU4ayYg1.</p>
<p>Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 4015-4026, 2023.</p>
<p>Lecheng Kong, Yixin Chen, and Muhan Zhang. Geodesic graph neural network for efficient graph representation learning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=6pC5OtP7eBx.</p>
<p>Lecheng Kong, Jiarui Feng, Hao Liu, Dacheng Tao, Yixin Chen, and Muhan Zhang. Mag-gnn: Reinforcement learning boosted graph neural network. In A. Oh, T. Neumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 12000-12021. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/ file/2788b4cdf421e03650868cc4184bfed8-Paper-Conference.pdf.</p>
<p>Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pp. 19730-19742. PMLR, 2023.</p>
<p>Yuhan Li, Peisong Wang, Zhixun Li, Jeffrey Xu Yu, and Jia Li. Zerog: Investigating cross-dataset zero-shot transferability in graphs. arXiv preprint arXiv:2402.11235, 2024.</p>
<p>Hao Liu, Jiarui Feng, Lecheng Kong, Ningyue Liang, Dacheng Tao, Yixin Chen, and Muhan Zhang. One for all: Towards training one graph model for all classification tasks. In The Twelfth International Conference on Learning Representations, 2023a.</p>
<p>Jiawei Liu, Cheng Yang, Zhiyuan Lu, Junze Chen, Yibo Li, Mengmei Zhang, Ting Bai, Yuan Fang, Lichao Sun, Philip S Yu, et al. Towards graph foundation models: A survey and beyond. arXiv preprint arXiv:2310.11829, 2023b.</p>
<p>Yixin Liu, Ming Jin, Shirui Pan, Chuan Zhou, Yu Zheng, Feng Xia, and S Yu Philip. Graph selfsupervised learning: A survey. IEEE transactions on knowledge and data engineering, 35(6): $5879-5900,2022$.</p>
<p>Zemin Liu, Xingtong Yu, Yuan Fang, and Xinming Zhang. Graphprompt: Unifying pre-training and downstream tasks for graph neural networks. In Proceedings of the ACM Web Conference 2023, 2023c.</p>
<p>Haitao Mao, Zhikai Chen, Wenzhuo Tang, Jianan Zhao, Yao Ma, Tong Zhao, Neil Shah, Michael Galkin, and Jiliang Tang. Graph foundation models. arXiv preprint arXiv:2402.02216, 2024.</p>
<p>Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. In International Conference on Learning Representations, 2022.</p>
<p>Pter Mernyei and Ctlina Cangea. Wiki-cs: A wikipedia-based benchmark for graph neural networks. arXiv preprint arXiv:2007.02901, 2020.</p>
<p>Yujie Mo, Liang Peng, Jie Xu, Xiaoshuang Shi, and Xiaofeng Zhu. Simple unsupervised graph representation learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 7797-7805, 2022.</p>
<p>Christopher Morris, Martin Ritzert, Matthias Fey, William L Hamilton, Jan Eric Lenssen, Gaurav Rattan, and Martin Grohe. Weisfeiler and leman go neural: Higher-order graph neural networks. In Proceedings of the AAAI conference on artificial intelligence, pp. 4602-4609, 2019.</p>
<p>Moritz Plenz and Anette Frank. Graph language models, 2024. URL https://arxiv.org/ abs/2401.07105.</p>
<p>Chen Qian, Huayi Tang, Zhirui Yang, Hong Liang, and Yong Liu. Can large language models empower molecular property prediction?, 2023.</p>
<p>Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 1-16. IEEE, 2020.</p>
<p>Yu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying Wei, Wenbing Huang, and Junzhou Huang. Self-supervised graph transformer on large-scale molecular data. Advances in Neural Information Processing Systems, 33, 2020.</p>
<p>Yunsheng Shi, Zhengjie Huang, Shikun Feng, Hui Zhong, Wenjin Wang, and Yu Sun. Masked label prediction: Unified message passing model for semi-supervised classification, 2021.</p>
<p>Bing Su, Dazhao Du, Zhao Yang, Yujie Zhou, Jiangmeng Li, Anyi Rao, Hao Sun, Zhiwu Lu, and Ji-Rong Wen. A molecular multimodal foundation model associating molecule graphs with natural language. arXiv preprint arXiv:2209.05481, 2022.</p>
<p>Xiangguo Sun, Hong Cheng, Jia Li, Bo Liu, and Jihong Guan. All in one: Multi-task prompting for graph neural networks. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD '23, pp. 2120-2131, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9798400701030. doi: 10.1145/3580305.3599256. URL https://doi.org/10.1145/3580305.3599256.</p>
<p>Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Lixin Su, Suqi Cheng, Dawei Yin, and Chao Huang. Graphgpt: Graph instruction tuning for large language models, 2023.</p>
<p>Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Long Xia, Dawei Yin, and Chao Huang. Higpt: Heterogeneous graph language model. arXiv preprint arXiv:2402.16024, 2024.</p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science. arXiv preprint arXiv:2211.09085, 2022.</p>
<p>Shantanu Thakoor, Corentin Tallec, Mohammad Gheshlaghi Azar, Rmi Munos, Petar Velikovi, and Michal Valko. Bootstrapped representation learning on graphs. In ICLR 2021 Workshop on Geometrical and Topological Representation Learning, 2021.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.</p>
<p>Petar Velikovi, William Fedus, William L Hamilton, Pietro Li, Yoshua Bengio, and R Devon Hjelm. Deep graph infomax. arXiv preprint arXiv:1809.10341, 2018.</p>
<p>Petar Velikovi, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li, and Yoshua Bengio. Graph attention networks. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=rJXMpikCZ.</p>
<p>Heng Wang, Shangbin Feng, Tianxing He, Zhaoxuan Tan, Xiaochuang Han, and Yulia Tsvetkov. Can language models solve graph problems in natural language?, 2023.</p>
<p>Song Wang, Kaize Ding, Chuxu Zhang, Chen Chen, and Jundong Li. Task-adaptive few-shot node classification. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 1910-1919, 2022.</p>
<p>Xixi Wu, Yifei Shen, Caihua Shan, Kaitao Song, Siwei Wang, Bohang Zhang, Jiarui Feng, Hong Cheng, Wei Chen, Yun Xiong, et al. Can graph learning improve task planning? arXiv preprint arXiv:2405.19119, 2024.</p>
<p>Lianghao Xia, Ben Kao, and Chao Huang. Opengraph: Towards open graph foundation models. arXiv preprint arXiv:2403.01121, 2024.</p>
<p>Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In International Conference on Learning Representations, 2018.</p>
<p>Junhan Yang, Zheng Liu, Shitao Xiao, Chaozhuo Li, Defu Lian, Sanjay Agrawal, Amit S, Guangzhong Sun, and Xing Xie. Graphformers: GNN-nested transformers for representation learning on textual graph. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum? id=yILzFBjROY.</p>
<p>Ruosong Ye, Caiqi Zhang, Runhui Wang, Shuyuan Xu, and Yongfeng Zhang. Natural language is all a graph needs, 2023.</p>
<p>Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-Yan Liu. Do transformers really perform badly for graph representation? In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=OeWooOxFwDa.</p>
<p>Xingtong Yu, Zhenghao Liu, Yuan Fang, Zemin Liu, Sihong Chen, and Xinming Zhang. Generalized graph prompt: Toward a unification of pre-training and downstream tasks on graphs, 2023.</p>
<p>Bohang Zhang, Guhao Feng, Yiheng Du, Di He, and Liwei Wang. A complete expressiveness hierarchy for subgraph gnns via subgraph weisfeiler-lehman tests, 2023a.</p>
<p>Bohang Zhang, Shengjie Luo, Liwei Wang, and Di He. Rethinking the expressive power of GNNs via graph biconnectivity. In The Eleventh International Conference on Learning Representations, 2023b. URL https://openreview.net/forum?id=r9hNv76KoT3.</p>
<p>Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858, 2023c. URL https:// arxiv.org/abs/2306.02858.</p>
<p>Mengmei Zhang, Mingwei Sun, Peng Wang, Shen Fan, Yanhu Mo, Xiaoxiao Xu, Hong Liu, Cheng Yang, and Chuan Shi. Graphtranslator: Aligning graph model to large language model for open-ended tasks. arXiv preprint arXiv:2402.07197, 2024.</p>
<p>Muhan Zhang and Pan Li. Nested graph neural networks. Advances in Neural Information Processing Systems, 34:15734-15747, 2021.</p>
<p>Muhan Zhang, Pan Li, Yinglong Xia, Kai Wang, and Long Jin. Labeling trick: A theory of using graph neural networks for multi-node representation learning. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=Hcr9mgBG6ds.</p>
<p>Ziwei Zhang, Haoyang Li, Zeyang Zhang, Yijian Qin, Xin Wang, and Wenwu Zhu. Graph meets llms: Towards large graph models. In NeurIPS 2023 Workshop: New Frontiers in Graph Learning, 2023d.</p>
<p>Haiteng Zhao, Shengchao Liu, Chang Ma, Hannan Xu, Jie Fu, Zhi-Hong Deng, Lingpeng Kong, and Qi Liu. Gimlet: A unified graph-text model for instruction-based molecule zero-shot learning, 2023a.</p>
<p>Jianan Zhao, Le Zhuo, Yikang Shen, Meng Qu, Kai Liu, Michael Bronstein, Zhaocheng Zhu, and Jian Tang. Graphtext: Graph reasoning in text space, 2023b.</p>
<h1>APPENDIX</h1>
<h2>A IMPLEMENTATION DETAILS</h2>
<h2>A. 1 IN-CONTEXT AUTOENCODER (ICAE)</h2>
<p>This section briefly introduces ICAE and how it helps build the GOFA model; please refer to ICAE paper (Ge et al., 2023) for the specifics of the model. ICAE contains two decoder-only LLMs. One serves as a language compressor that compresses sentences into a fixed-length sequence of vectors, and the other serves as a language decoder that decodes or queries into the compressed sentence representations. Specifically, during training, an input token sequence $\boldsymbol{x}=\left{x_{1}, \ldots, x_{l}\right}$ is appended by a $K$ memory tokens $\left{m_{1}, \ldots, m_{k}\right}$ with trainable embeddings. The concatenated sequence is fed to the LLM compressor with a LoRA adapter (Hu et al., 2022).</p>
<p>$$
\left{h\left(x_{1}\right), \ldots, h\left(x_{l}\right), h\left(m_{1}\right), \ldots, h\left(m_{K}\right)\right}=L L M_{\text {comp }}\left(\left{e\left(x_{1}\right), \ldots, e\left(x_{l}\right), e\left(m_{1}\right), \ldots, e\left(m_{K}\right)\right}\right)
$$</p>
<p>where $e(\cdot)$ and $h(\cdot)$ are the token embeddings and LLM outputs. Then, the decoder LLM only attends to the memory token outputs and tries to decode the original sentence from the memory tokens.</p>
<p>$$
\begin{gathered}
\left{l\left(m_{1}\right), \ldots, l\left(m_{K}\right), l\left(x_{1}\right), \ldots, l\left(x_{l}\right)\right}=L L M_{d e c}\left(\left{h\left(m_{1}\right), \ldots, h\left(m_{K}\right), e\left(x_{1}\right), \ldots, e\left(x_{l}\right)\right}\right) \
\min <em _comp="{comp" _text="\text">{\Theta</em>\right}\right)
\end{gathered}
$$}}} \operatorname{CrossEntropy}\left(\left{l\left(m_{K}\right), l\left(x_{1}\right), \ldots, l\left(x_{l-1}\right)\right},\left{x_{1}, \ldots, x_{l</p>
<p>The ICAE model is also trained on QA and Language modeling tasks to have more diverse embeddings.</p>
<p>By training this auto-encoder objective on a large-scale, the compressor model learns to compress all information about a sentence to the memory token outputs like in a conventional auto-encoder model. In Table 5, we provide a few examples of the comparison between the original text and the text decoded from the compressed memory tokens by ICAE's decoder. Because the compressed representation contains as much information as possible, GNN can pass messages between nodes with minimal information loss.</p>
<p>Table 5: Comparison between original texts and decoded text from the compressed representation.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Original Text</th>
<th style="text-align: left;">Decoded Text</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Actress Halle Berry has been sharing a number <br> of stunning photos from the time she has spent <br> in Morocco and she just posted a new one to her <br> Instagram page that fans will not want to miss.</td>
<td style="text-align: left;">Halle Berry has been sharing a number of stun- <br> ning photos from the time she has spent in Mo- <br> rocco and just posted a new one on her Instagram <br> page that fans won't want to miss.</td>
</tr>
<tr>
<td style="text-align: left;">Utah avoided the turnover bug on Saturday for the <br> first time since its season opener. In addition, the <br> running game was clicking and the defense was <br> dominant as the Utes snapped a two-game win- <br> ning streak on the road, beating Pittsburgh 26-14. <br> Five keys to Utah's victory: 1. Utah running back <br> John White IV: Running strong and with purpose <br> from the beginning, White was a big reason why <br> the Utes were within striking distance at halftime. <br> White, who took a couple pops that dislodged his <br> helmet and caused a cut below his ear, seemed to <br> get stronger as the game wore on. He finished the <br> afternoon with 171 yards on 36 carries.</td>
<td style="text-align: left;">Utah avoided the turnover bug on Saturday for the <br> first time since its season opener. In addition, the <br> running game was clicking and the defense was <br> dominant as the Utes snapped a two-game win- <br> ning streak on the road, beating Pittsburgh 26-14. <br> Five keys to Utah's victory: 1. 2. 3. 4. 5. Utah <br> running back John John White IV IV ran strong <br> and with purpose from the beginning, being a big <br> reason why the Utes were within striking distance <br> at halftime. He took a couple of shots that dis- <br> lodged his helmet and caused a cut below his ear, <br> but seemed to get stronger as the game went on. <br> He finished the afternoon with 171 yards on 36 <br> carries.</td>
</tr>
</tbody>
</table>
<h2>A. 2 LLM CHOICES OF GOFA</h2>
<p>Because ICAE preserves as much information in a sentence as possible, we can use it in the GOFA model to comprehensively pass information between sentences, as shown in Section 3.2. However,</p>
<p>the GOFA model is not limited to ICAE. Users can first train an ICAE-like objective on any existing LLM and apply the GOFA model to the trained LLM. Or, users can apply the GOFA directly to a pre-trained LLM and train the GOFA without the auto-encoder training. Note that the ICAE architecture has a function similar to an encoder-decoder LLM. We do not use an off-the-shelve encoder-decoder LLM because its encoder output is still subject to the sentence length, which does not fit GNN's need for fixed-sized input.</p>
<p>The design of GOFA can be extended beyond a compressor-decoder architecture. For example, we can have a decoder-only GOFA whose LLM layer is,</p>
<p>$$
\left{Q_{x}^{t+1}, Q_{m, x}^{t+1}, Q_{y}^{t+1}\right}=L L M^{t}\left(\left{Q_{x}^{t}, H_{x}^{t}, Q_{y}^{t}\right}\right)
$$</p>
<p>where the GNN is still applied on $K$ memory tokens inserted between the node text $x$ and target text $y$. This allows the target text to attend to the node text, which may improve the performance of GOFA. However, this formulation forces every node to have a target text, which is usually not what users desire and poses extra computation costs. We will explore this architecture in our future work.</p>
<h1>A. 3 TRANSFORMER CONVOLUTIONAL GNN</h1>
<p>As mentioned in Section 3.2, we customize a Transformer Convolutional GNN(TransConv) (Shi et al., 2021) as the GNN used in Equation 3. Since GNN layers operate on token representations and tokens at different indices do not communicate, we describe the GNN at one index for simplicity. The $t$-th GNN layer on node $i$ and its neighbors $\mathcal{N}(i)$ is:</p>
<p>$$
\begin{aligned}
h^{t+1}(i) &amp; =\boldsymbol{W}<em _in="\in" _mathcal_N="\mathcal{N" j="j">{o}\left(\sum</em>}(i)} \alpha_{i, j}\left(\boldsymbol{W<em _="{" _text="\text" edge="edge" v_="v,">{v, \text { node }} h^{t}(j)+\boldsymbol{W}</em>\right)\right)\right) \
\alpha_{i, j} &amp; =\operatorname{Softmax}\left(\frac{\boldsymbol{W}}} h\left(e_{i, j<em _="{" _text="\text" k_="k," node="node">{q} h^{t}(i) *\left(\boldsymbol{W}</em>}} h^{t}(j)+\boldsymbol{W<em i_="i," j="j">{k, \text { edge }} h\left(e</em>\right)
\end{aligned}
$$}\right)\right)}{\sqrt{d}</p>
<p>$h(\cdot)$ represents input node and edge features. $\boldsymbol{W}$ represents query $(q)$, key $(k)$, value $(v)$, output $(o)$ linear projection for nodes and edges. The formulation closely follows the transformer design (Vaswani et al., 2017) and its GNN adaptation (Shi et al., 2021). This formulation does not aggregate the last layer embedding $h^{t}(i)$ into the next layer, because we already add residual to maintain the same effect. We use pre-layer normalization following Llama (Touvron et al., 2023).</p>
<h2>B ADDITIONAL EXPERIMENTS</h2>
<h2>B. 1 ADDITIONAL ZERO-SHOT EVALUATION RESULTS</h2>
<p>Table 6: Additional comparison on zero-shot learning results. (The number indicates the number of ways).</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Cora-node (7)</th>
<th style="text-align: center;">WikiCS (10)</th>
<th style="text-align: center;">Products (10)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">DGI</td>
<td style="text-align: center;">20.15</td>
<td style="text-align: center;">13.63</td>
<td style="text-align: center;">21.35</td>
</tr>
<tr>
<td style="text-align: center;">GraphMAE2</td>
<td style="text-align: center;">37.08</td>
<td style="text-align: center;">19.03</td>
<td style="text-align: center;">25.46</td>
</tr>
<tr>
<td style="text-align: center;">Best-LLM</td>
<td style="text-align: center;">60.54</td>
<td style="text-align: center;">63.63</td>
<td style="text-align: center;">70.16</td>
</tr>
<tr>
<td style="text-align: center;">Best Graph-LLM</td>
<td style="text-align: center;">69.53</td>
<td style="text-align: center;">43.45</td>
<td style="text-align: center;">66.07</td>
</tr>
<tr>
<td style="text-align: center;">GOFA-T</td>
<td style="text-align: center;">$\mathbf{7 0 . 8 1}$</td>
<td style="text-align: center;">$\mathbf{7 1 . 1 7}$</td>
<td style="text-align: center;">$\mathbf{7 9 . 3 3}$</td>
</tr>
</tbody>
</table>
<p>Table 7: Zero-shot learning with molecule dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">BBBP</th>
<th style="text-align: center;">HIV</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">OFA</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">35.67</td>
</tr>
<tr>
<td style="text-align: center;">MoMu</td>
<td style="text-align: center;">49.81</td>
<td style="text-align: center;">50.26</td>
</tr>
<tr>
<td style="text-align: center;">Galactica</td>
<td style="text-align: center;">53.94</td>
<td style="text-align: center;">33.85</td>
</tr>
<tr>
<td style="text-align: center;">GIMLET</td>
<td style="text-align: center;">59.39</td>
<td style="text-align: center;">66.24</td>
</tr>
<tr>
<td style="text-align: center;">GOFA</td>
<td style="text-align: center;">54.91</td>
<td style="text-align: center;">53.02</td>
</tr>
</tbody>
</table>
<p>To further demonstrate the advantage of GOFA compared to previous methods, we conduct additional zero-shot experiments over different datasets and baselines. First, we additionally compare the zeroshot result with traditional GNN baselines. Since models like GCN and GAT cannot do zero-shot tasks due to their supervised training nature, we compare against graph self-supervised learning baselines. These methods learn node embeddings, which are compared to label text embeddings to make predictions based on cosine similarity. Specifically, we include DGI Velikovi et al. (2018) and GraphMAE2 Hou et al. (2023) and follow the same evaluation procedure. The result is shown in Table 6. GOFA significantly outperforms methods that use GNNs for self-supervised learning in the zero-shot setting. This result further demonstrates the effectiveness of the architecture design and the graph modeling tasks of GOFA on zero-shot generalization ability over graph tasks.</p>
<p>Next, we evaluate the GOFA on more numerically and structurally intensive tasks. Specifically, we conduct experiments on molecular property prediction tasks using ogbg-molhiv and BBBP datasets. The experiments closely follow the setting of GIMLET Zhao et al. (2023a). First, we fine-tune the pre-trained GOFA on randomly sampled 100,000 question-answering pairs from the Chembl pretrain dataset. The questions concentrated on asking about different molecule properties. Next, we evaluate the fine-tuned GOFA on ogbg-molhiv and BBBP in a zero-shot setting. We use AUROC as the evaluation metric. For comparison, we include OFA Liu et al. (2023a), MoMu Su et al. (2022), GIMLET Zhao et al. (2023a), and Galactica Taylor et al. (2022). The results are shown in Table 7. We directly report results of OFA Liu et al. (2023a) from their paper, and all other baselines are referred from GIMLET Zhao et al. (2023a). We can see that with only 100,000 instruction-tuning samples, GOFA already achieves comparable results to baselines. Note that GIMLET fine-tunes its model on the whole Chembl-pretrain dataset, which contains more than 400 million question-answering pairs.</p>
<h1>B. 2 SUPERVISED EXPERIMENT RESULTS</h1>
<p>In this section, we conduct a supervised learning experiment with the pre-trained GOFA . In the supervised experiment, GOFA 's prompt does not include class optional. We show the supervised results in Table 8. Specifically, we compare the result of GOFA with the following baselines: 1. basic GNNs, which are trained individually on each dataset, including GCN (Kipf \&amp; Welling, 2017) and GAT (Velikovi et al., 2018). 2. The contrastive learning methods, including DGI (Velikovi et al., 2018) and BGRL (Thakoor et al., 2021). For these methods, we directly report the best result from (He \&amp; Hooi, 2024). 3. Graph foundation model, including OFA (Liu et al., 2023a) and UniGraph (He \&amp; Hooi, 2024). GOFA achieved competitive performance on most datasets. In particular, GOFA achieved SOTA performance on the Pubmed dataset, demonstrating that GOFA can transfer pre-trained knowledge to downstream tasks. We also notice that GOFA is not performing as well on some datasets, possibly because in a supervised setting, we only train a small portion of the data for one epoch (specific numbers in the experimental details section in Appendix F.5), and in the supervised setting, it is important to see training samples multiple times to ensure detailed understanding of the distribution. As we pre-train the model with more diverse datasets, GOFA can potentially obtain world knowledge as an LLM, which makes transfer learning in the supervised setting more accurate.
Table 8: Experiment results in supervised learning. Bold and underlined shows best and runner-up results.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task type <br> Metric</th>
<th style="text-align: center;">Cora <br> Link <br> Acc $\uparrow$</th>
<th style="text-align: center;">Cora <br> Node <br> Acc $\uparrow$</th>
<th style="text-align: center;">PubMed <br> Link <br> Acc $\uparrow$</th>
<th style="text-align: center;">PubMed <br> Node <br> Acc $\uparrow$</th>
<th style="text-align: center;">$\begin{gathered} \text { Arxiv } \ \text { Node } \ \text { Acc } \uparrow \end{gathered}$</th>
<th style="text-align: center;">WikiCS <br> Node <br> Acc $\uparrow$</th>
<th style="text-align: center;">$\begin{gathered} \text { WN } \ \text { Link } \ \text { Acc } \uparrow \end{gathered}$</th>
<th style="text-align: center;">FB <br> Link <br> Acc $\uparrow$</th>
<th style="text-align: center;">Products <br> Node <br> Acc $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">GCN</td>
<td style="text-align: center;">$78.9 \pm 0.6$</td>
<td style="text-align: center;">82.31.1</td>
<td style="text-align: center;">$77.5 \pm 0.4$</td>
<td style="text-align: center;">$77.8 \pm 0.7$</td>
<td style="text-align: center;">$73.9 \pm 0.6$</td>
<td style="text-align: center;">$77.0 \pm 0.6$</td>
<td style="text-align: center;">$82.7 \pm 0.4$</td>
<td style="text-align: center;">$90.1 \pm 0.3$</td>
<td style="text-align: center;">$80.0 \pm 0.7$</td>
</tr>
<tr>
<td style="text-align: center;">GAT</td>
<td style="text-align: center;">$80.1 \pm 0.3$</td>
<td style="text-align: center;">$80.4 \pm 0.4$</td>
<td style="text-align: center;">$80.5 \pm 0.2$</td>
<td style="text-align: center;">$76.6 \pm 0.5$</td>
<td style="text-align: center;">75.80.3</td>
<td style="text-align: center;">$79.8 \pm 0.3$</td>
<td style="text-align: center;">$88.8 \pm 0.3$</td>
<td style="text-align: center;">$93.6 \pm 0.1$</td>
<td style="text-align: center;">81.40.2</td>
</tr>
<tr>
<td style="text-align: center;">DGI</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$51.99 \pm 0.45$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$55.76 \pm 0.56$</td>
<td style="text-align: center;">$55.21 \pm 0.21$</td>
<td style="text-align: center;">$67.11 \pm 0.12$</td>
<td style="text-align: center;">$52.04 \pm 0.22$</td>
<td style="text-align: center;">$26.99 \pm 0.22$</td>
<td style="text-align: center;">$64.21 \pm 0.32$</td>
</tr>
<tr>
<td style="text-align: center;">BGRL</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$56.73 \pm 0.23$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$63.77 \pm 0.23$</td>
<td style="text-align: center;">$62.21 \pm 0.21$</td>
<td style="text-align: center;">$70.12 \pm 0.15$</td>
<td style="text-align: center;">$56.44 \pm 0.21$</td>
<td style="text-align: center;">$64.91 \pm 0.22$</td>
<td style="text-align: center;">$63.77 \pm 0.23$</td>
</tr>
<tr>
<td style="text-align: center;">OFA</td>
<td style="text-align: center;">87.97</td>
<td style="text-align: center;">75.34</td>
<td style="text-align: center;">95.89</td>
<td style="text-align: center;">77.89</td>
<td style="text-align: center;">73.44</td>
<td style="text-align: center;">77.62</td>
<td style="text-align: center;">98.31</td>
<td style="text-align: center;">95.78</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">UniGraph</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$81.43 \pm 0.55$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$74.33 \pm 0.23$</td>
<td style="text-align: center;">$72.91 \pm 0.42$</td>
<td style="text-align: center;">79.981.21</td>
<td style="text-align: center;">$85.45 \pm 0.34$</td>
<td style="text-align: center;">$94.81 \pm 1.32$</td>
<td style="text-align: center;">$80.11 \pm 0.23$</td>
</tr>
<tr>
<td style="text-align: center;">GOFA</td>
<td style="text-align: center;">89.54</td>
<td style="text-align: center;">76.50</td>
<td style="text-align: center;">93.97</td>
<td style="text-align: center;">83.83</td>
<td style="text-align: center;">74.77</td>
<td style="text-align: center;">79.96</td>
<td style="text-align: center;">92.16</td>
<td style="text-align: center;">88.21</td>
<td style="text-align: center;">79.98</td>
</tr>
</tbody>
</table>
<h2>B. 3 EVALUATION ON ADDITIONAL GRAPH STRUCTURAL TASKS</h2>
<p>To further evaluate the potential of GOFA to serve as a general-purpose graph foundational model, we conducted additional experiments on a variety of graph structure tasks, including node degree, shortest path distance, and node count. To provide a fair comparison, we selected Mistral-7B as the baseline model and designed the prompts for Mistral following the incident prompt format proposed in (Fatemi et al.). Specifically, the graph is described as "[NODE.A] is connected to [NODE.B]". We use Arxiv as the graph dataset and randomly selected 10000 samples for fine-tuning and 2000 samples for testing. For the mistral model, we remove all original text from the dataset, as the model will run out of memory with all the text kept. The evaluation result is shown in Table 9. We can see that</p>
<p>GOFA achieved a competitive performance to the LLM baseline in both node-level (node degree) and link-level (shortest path distance) structural tasks. However, LLMs perform better on graph-level tasks. We suspect that the reason lies in the fact that we keep the original text in the graph dataset, which makes the learning of structure much harder, especially when the number of nodes in the graph becomes large. At the same time, our pre-training tasks focus on understanding local graph structures. Thus, GOFA is unfamiliar with understanding global structural information like node count. In the future, we aim to include more structural tasks to further improve the generalization ability of GOFA on different graph tasks.</p>
<h1>B. 4 Example of GOFA's Free-form Answer</h1>
<p>Figure 6 in the main body illustrates GOFA's capability to respond to various questions based on the same graph from ogbn-arXiv. In this section, we provide additional examples in Figure 7 to further show the ability of GOFA's free-form text answer.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: Demonstration of GOFA's ability to respond to any question to the given graph. Above is an example of the products dataset, where the model needs to output the majority category of its connected nodes. Below is another example on Wikics dataset. GOFA is asked to generate a Wikipedia page named based on the graph information.</p>
<h2>C DATASETS</h2>
<p>Cora. The Cora dataset is a co-citation network, where nodes are papers related to artificial intelligence. Edges mean the connected two papers are co-cited by other papers. The Cora dataset contains 2708 nodes and 10556 edges. We collect the Cora dataset and its raw text from OFA (Liu et al., 2023a). We evaluate the performance of the baseline and our proposed model on Cora for both node-level and link-level tasks. For the node-level task, the aim is to classify the node into the correct paper category from 7 different categories. The split is obtained from OFA. It contains 140/500/2068 samples for train/val/test set respectively. For the link-level task, the object is to predict whether two paper nodes are co-cited or not. We follow the setting of OFA (Liu et al., 2023a) and randomly split all edges into train/val/test sets with a ratio of $0.85 / 0.05 / 0.1$.</p>
<p>PubMed. The PubMed dataset is a co-citation network, where nodes are papers related to diabetes mellitus. Edges mean the connected two papers are co-cited by other papers. The PubMed dataset contains 19717 nodes and 88648 edges. We collect the PubMed dataset and its raw text from OFA (Liu et al., 2023a). We evaluate the performance of the baseline and our proposed model on PubMed for both node-level and link-level tasks. For the node-level task, papers have 3 different categories. The goal is to classify the node into the correct paper category. We obtain the split directly from original source. It contains 60/500/19157 samples for train/val/test set respectively. For the link-level task, the object is to predict whether two paper nodes are co-cited or not. We follow the</p>
<p>setting of OFA (Liu et al., 2023a) and randomly split all edges into train/val/test sets with a ratio of $0.85 / 0.05 / 0.1$.</p>
<p>Arxiv. The Arxiv dataset is a citation network, where nodes are papers related to computer science and edges mean two papers have a citation relationship. The Arxiv dataset contains 169343 nodes and 1166243 edges. We collect the Arxiv dataset and its raw text from OGB (Hu et al., 2021b). We evaluate the node classification on the Arxiv dataset. The goal is to classify the paper node into the correct category from 40 possible categories. We obtain the split directly from OGB (Hu et al., 2021b). It contains 90941/29799/48603 samples for train/val/test set, respectively.</p>
<p>WikiCS. The WikiCS dataset is a graph obtained from Wikipedia. The nodes in WikiCS are Wikipedia terms and their descriptions. The edges mean there is a hyperlink between two terms. We collect the WikiCS dataset and its raw text from (Mernyei \&amp; Cangea, 2020). There are 11701 nodes and 216123 edges in the graph. We evaluate the performance of WikiCS on the node classification task. There are 10 different classes. We follow the same split as OFA (Liu et al., 2023a), which contains 580/1769/5847 samples for the train/val/test set, respectively.</p>
<p>Products. The Products dataset is a co-purchase graph. The nodes in the graph represent product items from the Amazon platform, and the edges represent that two products are co-purchased together. We obtain the Products and their raw texts from TAPE (He et al., 2024a), which is a subset from the original ogbn-Products (Hu et al., 2021b) dataset. It contains 54025 nodes and 144638 edges. We evaluate the node classification performance on Products. The data from the original source contains 47 different categories. However, we found that there are two classes with missing labels. To be consistent with previous literature, we adopt the approach in LLaGA to replace the label name with special symbols. There are 14708/1572/37745 samples for the train/val/test set, respectively.</p>
<p>FB15K237. The FB15K237 is a knowledge graph generated from Free Base. Nodes in the dataset represent entities in the world and edges represent the relation between entities. We obtained the dataset from OFA (Liu et al., 2023a). The FB15K237 is used to evaluate the link classification. The dataset contains 237 unique classes. We follow the setting of OFA (Liu et al., 2023a) and split the dataset with a ratio of $0.85 / 0.05 / 0.1$, which results in a total of $272115 / 17535 / 20466$ samples for train/val/test set, respectively.</p>
<p>ExplaGraphs. The ExplaGraphs is a graph question answering dataset on commonsense concepts. Nodes in the dataset represent a common sense concept and edges represent the relation between two concepts. We obtain the dataset from G-retriever (He et al., 2024b) The ExplaGraphs can be used for question-answering on graphs. We obtain the split directly from G-retriever (He et al., 2024b). It contains 1659/553/554 graph samples from the train/val/test set.</p>
<p>SceneGraphs. The SceneGraphs is a graph question answering dataset on scene graphs. Nodes in the dataset represent an object in an image and edges represent the relation between two objects. We obtain the dataset from G-retriever (He et al., 2024b) The SceneGraphs can be used for questionanswering on graphs. We obtain the split directly from G-retriever (He et al., 2024b). It contains 59978/19997/20025 graph samples from the train/val/test set.</p>
<p>MAG240M. The MAG240M dataset is a citation network generated from Microsoft Academic Graphs. The nodes represent academic papers and the links represent a citation relation between two papers. We obtained the dataset and raw text from OGB-lsc (Hu et al., 2021a). However, the original dataset is extremely large and contains nodes without text features (author and institution nodes), since we mainly use the dataset for pre-training, we further downsample the original dataset. Specifically, we only keep paper nodes and citation links between papers. Further, we downsample the edges in the following ways. First, we selected all nodes in the train/val/test split provided by OGB-lsc (Hu et al., 2021a). Next, we filter the edges through two rounds. In the first round, we only keep the edge if either the source or the target is in the selected nodes. If any node in the added edge is not in the selected nodes, we add it to the node set. Next, in the second round, we include additional edges where both the source and target are in the selected nodes (additional nodes are added in the first round). The procedure results in a total of 5875010 nodes and 26434726 edges.</p>
<p>Ultrachat200k. The Ultrachat200k is a question-answering dataset. each sample is a multi-round conversation obtained from the web. We obtained the Ultrachat200k from (Ding et al., 2023). However, the original dataset is not a network. To convert it to a graph dataset, we manually create a graph structure for it. Specifically, if the original sample has $k$ round of conversation, we will</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<ul>
<li>Contributed equally. Listing order is random.
${ }^{\dagger}$ Corresponding author</li>
</ul>
<p><a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8746 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8746</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8746</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-157.html">extraction-schema-157</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-267312134</p>
                <p><strong>Paper Title:</strong> <a href="https://watermark.silverchair.com/btae238.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAA4EwggN9BgkqhkiG9w0BBwagggNuMIIDagIBADCCA2MGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMvZYbFq-PPNeHg0XbAgEQgIIDNL24k2NC7msALm7lQmBkt-FK_JPKxWY7S6W5mJ2OLIUXfRXECfsgcyMphkz_xdniBXOtmTmtYypyMkjLSMdFpLOiEf4Ae5OSDX8lJjEhqvPrZCfmkZnIpuhpqaeFRWt0-zkhJ5PkypTPRVlGP7WlsWqQ8qMFkAvCDyBE68AWvMwvSQvba53NS6_UWwj8mDOB86mzgXpHa-u63U7TBsf__UXGvmHSy1eSRndYOd6ZNOTzdwAEWuNYB9r8_Jl0rot0PvlVsaQQLJJdDfGYce3K55ueFNQcaTDGeONj0uIIVhJvfHJQTKgxUTqSgTUs3aHjtLh7-8WSw0fJYQyo8KYEewCSF8bBymXtRjtdi44sUfMKeXmVBJIhx6hazeQvB4s_nqjOBnNCqzilZsZapXNIXS9N8E4Nf3k6URMOWHSWNTbB702KH7Rw2r6EErXO6IcTDejo3koX9Q8_JkJu3Za3ENMvqGZYvC5QvTSradkGEqDdYgQ6F1G9g2XIylyggiMsPqVVDluao1yEZkPAnF57RFoJGEJ0KP6f59j4Q_ODjA-nKjPud98VLPgX76lg4qPO4B81J5iobrMBzgdFo1M_9-86G7A0-_A11SMoYPM1VszqWYu4KF6Hn5txdBaByz622tWVG7udc_6zHYX3OtZZfr7r5m4bbk0p_uDCcDCrsprf68Bzhw1tT7eg2CEW2uzam88aFn_WTtaRjX5FtjCrPghSJyVgC-JnLn5q594cJJ1a9deVlBjfMYlJQcgklDPM1pefOVSaB4ZxXRDYFrZXjkl5UjV6L5QO0MoK3m5uxwqV-vm9T_q1FX6Wxt63DPd2vh8gbJc1k8aaZYNfirO-m-Z0BT8G6kjzK2rRxvWukkZpC8G0uTwD7B3aARtrUxWRBApvR2Kjxs3telBdz4jM7f8noS962cGmg3w4NmvQXwqcDUEx41oJ8W3sVzJsOzWvulL8EEWxm4M3dz40J0Obz-Qxet-D_6mynATpADUsz5smuh7zx-xBOF2WUIE6j_0qGAdsqhWF6-D7RbigHEUcOVO-BZIExddGTtwAbQjyByPpXYgJgIgGjxgD_CQBiW0VNoaCRi0" target="_blank">Improving medical reasoning through retrieval and self-reflection with retrieval-augmented large language models</a></p>
                <p><strong>Paper Abstract:</strong> Abstract Summary Recent proprietary large language models (LLMs), such as GPT-4, have achieved a milestone in tackling diverse challenges in the biomedical domain, ranging from multiple-choice questions to long-form generations. To address challenges that still cannot be handled with the encoded knowledge of LLMs, various retrieval-augmented generation (RAG) methods have been developed by searching documents from the knowledge corpus and appending them unconditionally or selectively to the input of LLMs for generation. However, when applying existing methods to different domain-specific problems, poor generalization becomes apparent, leading to fetching incorrect documents or making inaccurate judgments. In this paper, we introduce Self-BioRAG, a framework reliable for biomedical text that specializes in generating explanations, retrieving domain-specific documents, and self-reflecting generated responses. We utilize 84k filtered biomedical instruction sets to train Self-BioRAG that can assess its generated explanations with customized reflective tokens. Our work proves that domain-specific components, such as a retriever, domain-related document corpus, and instruction sets are necessary for adhering to domain-related instructions. Using three major medical question-answering benchmark datasets, experimental results of Self-BioRAG demonstrate significant performance gains by achieving a 7.2% absolute improvement on average over the state-of-the-art open-foundation model with a parameter size of 7B or less. Similarly, Self-BioRAG outperforms RAG by 8% Rouge-1 score in generating more proficient answers on two long-form question-answering benchmarks on average. Overall, we analyze that Self-BioRAG finds the clues in the question, retrieves relevant documents if needed, and understands how to answer with information from retrieved documents and encoded knowledge as a medical expert does. We release our data and code for training our framework components and model weights (7B and 13B) to enhance capabilities in biomedical and clinical domains. Availability and implementation Self-BioRAG is available at https://github.com/dmis-lab/self-biorag.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8746.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8746.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-BioRAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-BioRAG (Self-Reflection + Biomedical RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A domain-adapted retrieval-augmented generation framework that integrates a domain-specific critic (self-reflection) and an instruction-tuned generator to decide on-demand retrieval, select evidence using predicted reflective tokens, and generate medically grounded answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Self-BioRAG generator (M)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned generator LM initialized from Self-RAG weights and fine-tuned on 84k filtered biomedical instruction instances; released in 7B and 13B parameter variants.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Reflective tokens / Self-reflection (Self-RAG style)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>A critic LM predicts four reflective token types (RET, REL, SUP, USE). The generator conditions on predicted reflective tokens to decide adaptive retrieval and to score/select retrieved evidence; evidence prioritization uses a weighted sum S of reflective-token generation probabilities. Reflective-token labels were produced initially via GPT-4 and the critic LM was trained to predict them and used to annotate training data.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Biomedical question answering benchmarks (MedQA, MedMCQA, MMLU-Med) and long-form QA (LiveQA, MedicationQA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-choice and long-form medical question-answering benchmarks used to evaluate medical reasoning, factuality and long-form generation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>MedQA acc 43.6%, MedMCQA acc 42.1%, MMLU-Med acc 53.9% (average 46.5%). Long-form: LiveQA Rouge-1=19.7, MedicationQA Rouge-1=17.6 (Self-BioRAG). Also reported: ~7.2 percentage-points absolute improvement over the best open-foundation ≤7B baseline on the multi-choice benchmarks, and ~+8 pts Rouge-1 vs RAG averaged on two long-form QA sets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Ablation removing reflective tokens: MedQA acc 42.5%, MedMCQA acc 41.9%, MMLU-Med acc 51.1% (average 45.2%). Long-form R1/R2/RL scores for non-reflection not separately provided.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Specialized critic LM predicts discrete reflective tokens given (instruction, output, optional evidence); tokens (RET/REL/SUP/USE) guide adaptive retrieval and evidence scoring. The generator is trained to output answers together with reflective tokens; evidence selection uses a scoring function S that weights token probabilities to pick most relevant retrieved passages.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative ablation: removing reflective tokens reduces average accuracy from 46.5% to 45.2% (−1.3 points). Self-BioRAG achieves +7.2 percentage-point absolute improvement over the state-of-the-art open-foundation ≤7B baseline on the multi-choice datasets. Long-form QA: Self-BioRAG R1 19.7 vs RAG 11.5 on LiveQA and 17.6 vs 9.8 on MedicationQA (substantial ROUGE gains).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Reported limitations include: (1) Self-RAG (general-domain) fails to generalize to biomedical questions and fetches incorrect documents; (2) RAG pipelines struggle to identify crucial evidence and suffer from input-length constraints, limiting how many evidences can be used; (3) adaptive retrieval can be unstable in some datasets (Only-Retrieve setting improved MedMCQA but worsened MMLU-Med); (4) critic LM mispredictions occur (examples filtered during data construction), and the paper flags exploring other reflective tokens and further fine-grained evaluation as future work.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared against plain generation (LLaMA2), standard RAG (LLaMA2+MedCPT), and Self-RAG: Self-BioRAG > RAG > LLaMA2 generally; Self-RAG (general) performed worse on biomedical benchmarks due to poor domain generalization. No direct chain-of-thought or multi-iteration self-consistency comparisons reported.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Table 6 ablation: progressively removing components yields drops: -Reflective Tokens: average −1.3; -Biomedical Corpora: average −2.9; -MedCPT Retriever: average −4.4; -Biomedical Instruction Sets: average −7.3. Table 7 on adaptive retrieval: Only [No Retrieval] avg 44.8, Only [Retrieval] avg 46.2, Adaptive Retrieval (Self-BioRAG) avg 46.5.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving medical reasoning through retrieval and self-reflection with retrieval-augmented large language models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8746.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8746.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-RAG (learning to retrieve, generate, and critique through self-reflection)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior framework that uses reflective tokens and a critic LM to determine when to retrieve, to evaluate retrieved context, to critique rationales, and to judge output usefulness, enabling on-demand retrieval and self-assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-rag: learning to retrieve, generate, and critique through self-reflection.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Self-RAG (critic + generator)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Original Self-RAG architecture (Asai et al.) employing a critic LM predicting reflective tokens and a generator LM trained to generate answers conditioned on those tokens; in the present paper Self-RAG weights are used as initialization for further biomedical fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Reflective tokens + critic model</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>A critic model predicts reflective tokens that determine retrieval necessity and assess evidence/answer quality; used to enable generate-then-reflect style behavior and on-demand retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General-domain retrieval-augmented generation tasks (as in Asai et al.) and used as baseline here on biomedical QA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks designed to test retrieval-on-demand, evidence assessment, and critique of generated outputs; used here as a baseline applied to biomedical datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Reported in the paper as a baseline; Self-RAG (general) performed worse on biomedical benchmarks than Self-BioRAG (exact numeric rows in Table 4 show a performance drop for Self-RAG relative to Self-BioRAG, but table's absolute numbers for Self-RAG baseline are not listed verbatim in the main text excerpt).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompted/architectural critic LM predicts reflective tokens; these tokens are used to gate retrieval and judge evidence and outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>As reported by Asai et al. (prior work), reflective tokens enable on-demand retrieval and critique; in the present paper, Self-RAG's general-domain version did not generalize well to biomedical data and underperformed compared to the domain-adapted Self-BioRAG.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Poor generalization to biomedical/clinical domain resulting in fetching incorrect documents and inaccurate judgments when applied without domain adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared in this paper as a baseline; Self-BioRAG (domain-adapted) outperforms Self-RAG on biomedical benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving medical reasoning through retrieval and self-reflection with retrieval-augmented large language models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8746.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8746.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Critic LM C</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-reflection language model (critic LM C)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A domain-specific critic model trained to predict reflective tokens (RET, REL, SUP, USE) on biomedical instruction instances, used to annotate and filter training data and to guide retrieval/selection at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Critic LM C (initialized from LLaMA2 / Self-RAG critic)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Initialized from LLaMA2 / Self-RAG critic, fine-tuned on ~5k examples annotated using GPT-4 to predict four reflective tokens; then used to annotate 120k instruction sets and filter 84k instances for generator training.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Discrete reflective-token prediction (RET/REL/SUP/USE)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Given input (instruction), generated output (and optionally evidence), C predicts whether retrieval is needed (RET), whether retrieved evidence is relevant (REL), whether answer statements are supported by evidence (SUP), and overall utility (USE). Predictions are used to filter training data and to compute evidence scores during inference.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Annotation and filtering of biomedical instruction datasets; guidance during inference for retrieval/evidence selection</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Produce reflective-token labels for instruction-output(-evidence) triplets to build training data for generator and to enable adaptive retrieval / evidence scoring at inference time.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Supervised fine-tuning on GPT-4 labeled examples to predict discrete reflective tokens; used both offline to annotate/filter dataset and online to score evidence via token probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Using the critic to filter training data yielded 84k high-quality instruction instances for the generator; ablations (Table 6) show reflective-token based controllable generation contributes +1.3 average accuracy vs removing reflective tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Critic mispredictions occur (e.g., non-predefined tokens like [Continue Generation]) and such instances were filtered out; no standalone numeric accuracy of critic reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Implements the Self-RAG-style critic but trained on domain-specific biomedical instructions; compared favorably versus using a general-domain critic (Self-RAG) which did not generalize well.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving medical reasoning through retrieval and self-reflection with retrieval-augmented large language models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8746.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8746.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflective tokens (RET/REL/SUP/USE)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflective tokens: RET, REL, SUP, USE</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of four discrete token types used to represent the critic's judgments: RET (whether retrieval is needed), REL (retrieved evidence relevance), SUP (support of answer statements by evidence), USE (utility/usefulness of the answer).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Reflective tokens (task-conditional discrete labels)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Four token categories predicted by the critic LM: RET in {yes,no,continue}; REL in {relevant,irrelevant}; SUP in {fully supported, partially supported, no support}; USE in {5,4,3,2,1} (utility score). Used to guide retrieval decisions and evidence selection.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Discrete reflective-token scheme</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Critic LM predicts discrete tokens per instance; probabilities of desirable tokens are combined (weighted) into a score S to rank/select evidence and to decide retrieval at inference; the generator is trained to produce answers with associated reflective tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Annotation and runtime decision-making for biomedical RAG</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Used to annotate training pairs and to inform adaptive retrieval and evidence prioritization during generation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Ablation: presence of reflective tokens yields average accuracy 46.5% vs 45.2% without (Table 6) on multi-choice biomedical benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Without reflective tokens: MedQA 42.5%, MedMCQA 41.9%, MMLU-Med 51.1% (average 45.2%).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Predicted via trained critic LM; generator conditions on/outputs reflective tokens; used to compute evidence selection score S = sum_G w_G s_G, where s_G is token generation probability for the desired reflective token and w_G are tunable weights.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Ablation study (Table 6) indicates reflective tokens improve rationale generation and answer accuracy (+1.3 average accuracy). The tokens enable selective retrieval and evidence ranking which improves over unconditional RAG.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Some mispredicted or out-of-vocabulary tokens occurred during annotation and were filtered; paper notes exploring other token sets for different domains as future work.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Reflective-token approach is contrasted with always-on retrieval RAG; adaptive reflective-token-driven retrieval performed comparably to Only-Retrieve while being more cost-efficient and more stable than forcing retrieval in some datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Removing reflective tokens decreased average accuracy by 1.3 points (Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving medical reasoning through retrieval and self-reflection with retrieval-augmented large language models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8746.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8746.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Adaptive retrieval (RET)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adaptive retrieval (deciding to retrieve via RET reflective token)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-on-demand mechanism where the critic predicts RET to decide whether the generator should fetch external evidence; when RET indicates retrieval, top-k documents are retrieved and re-ranked using reflective token scores.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Adaptive retrieval controller (part of generator guided by critic predictions)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>At inference, the generator/critic predicts the RET token; if RET indicates retrieval, MedCPT retriever returns top-10 candidate evidence from biomedical corpora, which are then re-ranked using reflective-token-based scoring to choose final evidence fed to the generator.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Adaptive (on-demand) retrieval via RET token</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Single-shot decision per query: critic/generator predicts RET ∈ {yes,no,continue}. If yes, retrieve top-k evidences from multiple biomedical corpora, rerank by computed S(Critique) (weighted reflective-token probabilities), and condition generation on the selected evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Biomedical QA (same benchmarks as above)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Adaptive retrieval used to limit unnecessary retrieval and to prioritize evidence when needed for answer generation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Adaptive retrieval (with RET) produced average accuracy 46.5 across the 3 multi-choice benchmarks (Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Only [No Retrieval] average 44.8; Only [Retrieval] average 46.2 (Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Decision implemented via predicted RET token; retrieval invoked only if RET indicates 'yes'; evidence scoring via combined reflective-token probabilities selects the most pertinent passage.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Table 7: Adaptive retrieval slightly outperformed both only-no-retrieval and only-retrieval on average (Adaptive 46.5 vs Only-Retrieval 46.2 vs Only-No-Retrieval 44.8), showing benefit of on-demand retrieval guided by reflection.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Only-Retrieval setting showed instability across datasets (improved MedMCQA but worsened MMLU-Med), indicating that forcing retrieval is not uniformly beneficial; adaptive retrieval retrieves evidence only for a small portion of instances in these benchmarks which may limit gains.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared directly with unconditional RAG (always retrieve) and no-retrieval; adaptive retrieval was recommended as default due to better average performance and cost efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Table 7 reports the three settings and their accuracies (Only No Retrieve avg 44.8; Only Retrieve avg 46.2; Adaptive Retrieval avg 46.5).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving medical reasoning through retrieval and self-reflection with retrieval-augmented large language models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-rag: learning to retrieve, generate, and critique through self-reflection. <em>(Rating: 2)</em></li>
                <li>Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy. <em>(Rating: 2)</em></li>
                <li>Active retrieval augmented generation. <em>(Rating: 1)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models. <em>(Rating: 1)</em></li>
                <li>Self-instruct: aligning language model with self generated instructions. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8746",
    "paper_id": "paper-267312134",
    "extraction_schema_id": "extraction-schema-157",
    "extracted_data": [
        {
            "name_short": "Self-BioRAG",
            "name_full": "Self-BioRAG (Self-Reflection + Biomedical RAG)",
            "brief_description": "A domain-adapted retrieval-augmented generation framework that integrates a domain-specific critic (self-reflection) and an instruction-tuned generator to decide on-demand retrieval, select evidence using predicted reflective tokens, and generate medically grounded answers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Self-BioRAG generator (M)",
            "model_description": "Instruction-tuned generator LM initialized from Self-RAG weights and fine-tuned on 84k filtered biomedical instruction instances; released in 7B and 13B parameter variants.",
            "reflection_method_name": "Reflective tokens / Self-reflection (Self-RAG style)",
            "reflection_method_description": "A critic LM predicts four reflective token types (RET, REL, SUP, USE). The generator conditions on predicted reflective tokens to decide adaptive retrieval and to score/select retrieved evidence; evidence prioritization uses a weighted sum S of reflective-token generation probabilities. Reflective-token labels were produced initially via GPT-4 and the critic LM was trained to predict them and used to annotate training data.",
            "task_name": "Biomedical question answering benchmarks (MedQA, MedMCQA, MMLU-Med) and long-form QA (LiveQA, MedicationQA)",
            "task_description": "Multi-choice and long-form medical question-answering benchmarks used to evaluate medical reasoning, factuality and long-form generation.",
            "performance_with_reflection": "MedQA acc 43.6%, MedMCQA acc 42.1%, MMLU-Med acc 53.9% (average 46.5%). Long-form: LiveQA Rouge-1=19.7, MedicationQA Rouge-1=17.6 (Self-BioRAG). Also reported: ~7.2 percentage-points absolute improvement over the best open-foundation ≤7B baseline on the multi-choice benchmarks, and ~+8 pts Rouge-1 vs RAG averaged on two long-form QA sets.",
            "performance_without_reflection": "Ablation removing reflective tokens: MedQA acc 42.5%, MedMCQA acc 41.9%, MMLU-Med acc 51.1% (average 45.2%). Long-form R1/R2/RL scores for non-reflection not separately provided.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Specialized critic LM predicts discrete reflective tokens given (instruction, output, optional evidence); tokens (RET/REL/SUP/USE) guide adaptive retrieval and evidence scoring. The generator is trained to output answers together with reflective tokens; evidence selection uses a scoring function S that weights token probabilities to pick most relevant retrieved passages.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Quantitative ablation: removing reflective tokens reduces average accuracy from 46.5% to 45.2% (−1.3 points). Self-BioRAG achieves +7.2 percentage-point absolute improvement over the state-of-the-art open-foundation ≤7B baseline on the multi-choice datasets. Long-form QA: Self-BioRAG R1 19.7 vs RAG 11.5 on LiveQA and 17.6 vs 9.8 on MedicationQA (substantial ROUGE gains).",
            "limitations_or_failure_cases": "Reported limitations include: (1) Self-RAG (general-domain) fails to generalize to biomedical questions and fetches incorrect documents; (2) RAG pipelines struggle to identify crucial evidence and suffer from input-length constraints, limiting how many evidences can be used; (3) adaptive retrieval can be unstable in some datasets (Only-Retrieve setting improved MedMCQA but worsened MMLU-Med); (4) critic LM mispredictions occur (examples filtered during data construction), and the paper flags exploring other reflective tokens and further fine-grained evaluation as future work.",
            "comparison_to_other_methods": "Compared against plain generation (LLaMA2), standard RAG (LLaMA2+MedCPT), and Self-RAG: Self-BioRAG &gt; RAG &gt; LLaMA2 generally; Self-RAG (general) performed worse on biomedical benchmarks due to poor domain generalization. No direct chain-of-thought or multi-iteration self-consistency comparisons reported.",
            "ablation_study_results": "Table 6 ablation: progressively removing components yields drops: -Reflective Tokens: average −1.3; -Biomedical Corpora: average −2.9; -MedCPT Retriever: average −4.4; -Biomedical Instruction Sets: average −7.3. Table 7 on adaptive retrieval: Only [No Retrieval] avg 44.8, Only [Retrieval] avg 46.2, Adaptive Retrieval (Self-BioRAG) avg 46.5.",
            "uuid": "e8746.0",
            "source_info": {
                "paper_title": "Improving medical reasoning through retrieval and self-reflection with retrieval-augmented large language models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Self-RAG",
            "name_full": "Self-RAG (learning to retrieve, generate, and critique through self-reflection)",
            "brief_description": "A prior framework that uses reflective tokens and a critic LM to determine when to retrieve, to evaluate retrieved context, to critique rationales, and to judge output usefulness, enabling on-demand retrieval and self-assessment.",
            "citation_title": "Self-rag: learning to retrieve, generate, and critique through self-reflection.",
            "mention_or_use": "use",
            "model_name": "Self-RAG (critic + generator)",
            "model_description": "Original Self-RAG architecture (Asai et al.) employing a critic LM predicting reflective tokens and a generator LM trained to generate answers conditioned on those tokens; in the present paper Self-RAG weights are used as initialization for further biomedical fine-tuning.",
            "reflection_method_name": "Reflective tokens + critic model",
            "reflection_method_description": "A critic model predicts reflective tokens that determine retrieval necessity and assess evidence/answer quality; used to enable generate-then-reflect style behavior and on-demand retrieval.",
            "task_name": "General-domain retrieval-augmented generation tasks (as in Asai et al.) and used as baseline here on biomedical QA",
            "task_description": "Tasks designed to test retrieval-on-demand, evidence assessment, and critique of generated outputs; used here as a baseline applied to biomedical datasets.",
            "performance_with_reflection": "Reported in the paper as a baseline; Self-RAG (general) performed worse on biomedical benchmarks than Self-BioRAG (exact numeric rows in Table 4 show a performance drop for Self-RAG relative to Self-BioRAG, but table's absolute numbers for Self-RAG baseline are not listed verbatim in the main text excerpt).",
            "performance_without_reflection": null,
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompted/architectural critic LM predicts reflective tokens; these tokens are used to gate retrieval and judge evidence and outputs.",
            "number_of_iterations": null,
            "evidence_for_improvement": "As reported by Asai et al. (prior work), reflective tokens enable on-demand retrieval and critique; in the present paper, Self-RAG's general-domain version did not generalize well to biomedical data and underperformed compared to the domain-adapted Self-BioRAG.",
            "limitations_or_failure_cases": "Poor generalization to biomedical/clinical domain resulting in fetching incorrect documents and inaccurate judgments when applied without domain adaptation.",
            "comparison_to_other_methods": "Compared in this paper as a baseline; Self-BioRAG (domain-adapted) outperforms Self-RAG on biomedical benchmarks.",
            "ablation_study_results": null,
            "uuid": "e8746.1",
            "source_info": {
                "paper_title": "Improving medical reasoning through retrieval and self-reflection with retrieval-augmented large language models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Critic LM C",
            "name_full": "Self-reflection language model (critic LM C)",
            "brief_description": "A domain-specific critic model trained to predict reflective tokens (RET, REL, SUP, USE) on biomedical instruction instances, used to annotate and filter training data and to guide retrieval/selection at inference.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Critic LM C (initialized from LLaMA2 / Self-RAG critic)",
            "model_description": "Initialized from LLaMA2 / Self-RAG critic, fine-tuned on ~5k examples annotated using GPT-4 to predict four reflective tokens; then used to annotate 120k instruction sets and filter 84k instances for generator training.",
            "reflection_method_name": "Discrete reflective-token prediction (RET/REL/SUP/USE)",
            "reflection_method_description": "Given input (instruction), generated output (and optionally evidence), C predicts whether retrieval is needed (RET), whether retrieved evidence is relevant (REL), whether answer statements are supported by evidence (SUP), and overall utility (USE). Predictions are used to filter training data and to compute evidence scores during inference.",
            "task_name": "Annotation and filtering of biomedical instruction datasets; guidance during inference for retrieval/evidence selection",
            "task_description": "Produce reflective-token labels for instruction-output(-evidence) triplets to build training data for generator and to enable adaptive retrieval / evidence scoring at inference time.",
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Supervised fine-tuning on GPT-4 labeled examples to predict discrete reflective tokens; used both offline to annotate/filter dataset and online to score evidence via token probabilities.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Using the critic to filter training data yielded 84k high-quality instruction instances for the generator; ablations (Table 6) show reflective-token based controllable generation contributes +1.3 average accuracy vs removing reflective tokens.",
            "limitations_or_failure_cases": "Critic mispredictions occur (e.g., non-predefined tokens like [Continue Generation]) and such instances were filtered out; no standalone numeric accuracy of critic reported in the paper.",
            "comparison_to_other_methods": "Implements the Self-RAG-style critic but trained on domain-specific biomedical instructions; compared favorably versus using a general-domain critic (Self-RAG) which did not generalize well.",
            "ablation_study_results": null,
            "uuid": "e8746.2",
            "source_info": {
                "paper_title": "Improving medical reasoning through retrieval and self-reflection with retrieval-augmented large language models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Reflective tokens (RET/REL/SUP/USE)",
            "name_full": "Reflective tokens: RET, REL, SUP, USE",
            "brief_description": "A set of four discrete token types used to represent the critic's judgments: RET (whether retrieval is needed), REL (retrieved evidence relevance), SUP (support of answer statements by evidence), USE (utility/usefulness of the answer).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Reflective tokens (task-conditional discrete labels)",
            "model_description": "Four token categories predicted by the critic LM: RET in {yes,no,continue}; REL in {relevant,irrelevant}; SUP in {fully supported, partially supported, no support}; USE in {5,4,3,2,1} (utility score). Used to guide retrieval decisions and evidence selection.",
            "reflection_method_name": "Discrete reflective-token scheme",
            "reflection_method_description": "Critic LM predicts discrete tokens per instance; probabilities of desirable tokens are combined (weighted) into a score S to rank/select evidence and to decide retrieval at inference; the generator is trained to produce answers with associated reflective tokens.",
            "task_name": "Annotation and runtime decision-making for biomedical RAG",
            "task_description": "Used to annotate training pairs and to inform adaptive retrieval and evidence prioritization during generation.",
            "performance_with_reflection": "Ablation: presence of reflective tokens yields average accuracy 46.5% vs 45.2% without (Table 6) on multi-choice biomedical benchmarks.",
            "performance_without_reflection": "Without reflective tokens: MedQA 42.5%, MedMCQA 41.9%, MMLU-Med 51.1% (average 45.2%).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Predicted via trained critic LM; generator conditions on/outputs reflective tokens; used to compute evidence selection score S = sum_G w_G s_G, where s_G is token generation probability for the desired reflective token and w_G are tunable weights.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Ablation study (Table 6) indicates reflective tokens improve rationale generation and answer accuracy (+1.3 average accuracy). The tokens enable selective retrieval and evidence ranking which improves over unconditional RAG.",
            "limitations_or_failure_cases": "Some mispredicted or out-of-vocabulary tokens occurred during annotation and were filtered; paper notes exploring other token sets for different domains as future work.",
            "comparison_to_other_methods": "Reflective-token approach is contrasted with always-on retrieval RAG; adaptive reflective-token-driven retrieval performed comparably to Only-Retrieve while being more cost-efficient and more stable than forcing retrieval in some datasets.",
            "ablation_study_results": "Removing reflective tokens decreased average accuracy by 1.3 points (Table 6).",
            "uuid": "e8746.3",
            "source_info": {
                "paper_title": "Improving medical reasoning through retrieval and self-reflection with retrieval-augmented large language models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Adaptive retrieval (RET)",
            "name_full": "Adaptive retrieval (deciding to retrieve via RET reflective token)",
            "brief_description": "A retrieval-on-demand mechanism where the critic predicts RET to decide whether the generator should fetch external evidence; when RET indicates retrieval, top-k documents are retrieved and re-ranked using reflective token scores.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Adaptive retrieval controller (part of generator guided by critic predictions)",
            "model_description": "At inference, the generator/critic predicts the RET token; if RET indicates retrieval, MedCPT retriever returns top-10 candidate evidence from biomedical corpora, which are then re-ranked using reflective-token-based scoring to choose final evidence fed to the generator.",
            "reflection_method_name": "Adaptive (on-demand) retrieval via RET token",
            "reflection_method_description": "Single-shot decision per query: critic/generator predicts RET ∈ {yes,no,continue}. If yes, retrieve top-k evidences from multiple biomedical corpora, rerank by computed S(Critique) (weighted reflective-token probabilities), and condition generation on the selected evidence.",
            "task_name": "Biomedical QA (same benchmarks as above)",
            "task_description": "Adaptive retrieval used to limit unnecessary retrieval and to prioritize evidence when needed for answer generation.",
            "performance_with_reflection": "Adaptive retrieval (with RET) produced average accuracy 46.5 across the 3 multi-choice benchmarks (Table 7).",
            "performance_without_reflection": "Only [No Retrieval] average 44.8; Only [Retrieval] average 46.2 (Table 7).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Decision implemented via predicted RET token; retrieval invoked only if RET indicates 'yes'; evidence scoring via combined reflective-token probabilities selects the most pertinent passage.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Table 7: Adaptive retrieval slightly outperformed both only-no-retrieval and only-retrieval on average (Adaptive 46.5 vs Only-Retrieval 46.2 vs Only-No-Retrieval 44.8), showing benefit of on-demand retrieval guided by reflection.",
            "limitations_or_failure_cases": "Only-Retrieval setting showed instability across datasets (improved MedMCQA but worsened MMLU-Med), indicating that forcing retrieval is not uniformly beneficial; adaptive retrieval retrieves evidence only for a small portion of instances in these benchmarks which may limit gains.",
            "comparison_to_other_methods": "Compared directly with unconditional RAG (always retrieve) and no-retrieval; adaptive retrieval was recommended as default due to better average performance and cost efficiency.",
            "ablation_study_results": "Table 7 reports the three settings and their accuracies (Only No Retrieve avg 44.8; Only Retrieve avg 46.2; Adaptive Retrieval avg 46.5).",
            "uuid": "e8746.4",
            "source_info": {
                "paper_title": "Improving medical reasoning through retrieval and self-reflection with retrieval-augmented large language models",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-rag: learning to retrieve, generate, and critique through self-reflection.",
            "rating": 2,
            "sanitized_title": "selfrag_learning_to_retrieve_generate_and_critique_through_selfreflection"
        },
        {
            "paper_title": "Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy.",
            "rating": 2,
            "sanitized_title": "enhancing_retrievalaugmented_large_language_models_with_iterative_retrievalgeneration_synergy"
        },
        {
            "paper_title": "Active retrieval augmented generation.",
            "rating": 1,
            "sanitized_title": "active_retrieval_augmented_generation"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "rating": 1,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Self-instruct: aligning language model with self generated instructions.",
            "rating": 1,
            "sanitized_title": "selfinstruct_aligning_language_model_with_self_generated_instructions"
        }
    ],
    "cost": 0.015512749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Improving medical reasoning through retrieval and self-reflection with retrieval-augmented large language models</p>
<p>Minbyul Jeong 
Department of Computer Science
Korea University
02841SeoulRepublic of Korea</p>
<p>Jiwoong Sohn 
Department of Computer Science
Korea University
02841SeoulRepublic of Korea</p>
<p>Mujeen Sung mujeensung@khu.ac.kr 
Department of Software Convergence
School of Computing
Kyung Hee University
Republic of Korea</p>
<p>Corresponding authors. Department of Software Convergence
School of Computing
Kyung Hee University
Gyeonggi-do17104Republic of Korea</p>
<p>Jaewoo Kang kangj@korea.ac.kr 
Department of Computer Science
Korea University
02841SeoulRepublic of Korea</p>
<p>AIGEN Sciences
04778SeoulRepublic of Korea</p>
<p>Corresponding authors. Department of Software Convergence
School of Computing
Kyung Hee University
Gyeonggi-do17104Republic of Korea</p>
<p>1-gil, Seongdong-gu04778SeoulRepublic of Korea</p>
<p>Improving medical reasoning through retrieval and self-reflection with retrieval-augmented large language models
3E20EB0B04C27BEFF32B01E6D7EBE3F810.1093/bioinformatics/btae238
Recent proprietary large language models (LLMs), such as GPT-4, have achieved a milestone in tackling diverse challenges in the biomedical domain, ranging from multiple-choice questions to long-form generations.To address challenges that still cannot be handled with the encoded knowledge of LLMs, various retrieval-augmented generation (RAG) methods have been developed by searching documents from the knowledge corpus and appending them unconditionally or selectively to the input of LLMs for generation.However, when applying existing methods to different domain-specific problems, poor generalization becomes apparent, leading to fetching incorrect documents or making inaccurate judgments.In this paper, we introduce Self-BioRAG, a framework reliable for biomedical text that specializes in generating explanations, retrieving domain-specific documents, and self-reflecting generated responses.We utilize 84k filtered biomedical instruction sets to train Self-BioRAG that can assess its generated explanations with customized reflective tokens.Our work proves that domain-specific components, such as a retriever, domain-related document corpus, and instruction sets are necessary for adhering to domain-related instructions.Using three major medical question-answering benchmark datasets, experimental results of Self-BioRAG demonstrate significant performance gains by achieving a 7.2% absolute improvement on average over the state-of-the-art open-foundation model with a parameter size of 7B or less.Similarly, Self-BioRAG outperforms RAG by 8% Rouge-1 score in generating more proficient answers on two long-form question-answering benchmarks on average.Overall, we analyze that Self-BioRAG finds the clues in the question, retrieves relevant documents if needed, and understands how to answer with information from retrieved documents and encoded knowledge as a medical expert does.We release our data and code for training our framework components and model weights (7B and 13B) to enhance capabilities in biomedical and clinical domains.</p>
<p>Introduction</p>
<p>The recent proprietary large language models (LLMs) such as ChatGPT (OpenAI 2023a), GPT-4 (OpenAI 2023b), and BARD (Google 2023) have succeeded in reaching near or comparable levels to human experts in solving many challenging problems, ranging from multi-choice question answering to long-form text generations.While these models exhibit high efficiency and demonstrate their versatility in various domains, they fall short in comprehensively covering user-dependent information such as patient reports with encoded knowledge.These limitations can result in a groundless statement and inadvertent generation of false information, commonly known as the hallucination issue (Cao et al. 2022, Singhal et al. 2022, Wei et al. 2022, Ji et al. 2023).To address this challenge, retrievalaugmented generation (RAG) enhances explainability for readers by supplying supporting facts that underpin the responses generated by LLMs (Guu et al. 2020, Lewis et al. 2020).As illustrated in Fig. 1, various RAG frameworks search documents from the knowledge corpus such as Wikipedia and appending them unconditionally or selectively to the input of LLMs for generation.In alignment with this approach, the authors of Asai et al. (2023) introduce Self-RAG, which uses reflective tokens that learn to reflect on its generation process given a task input possessing the following capabilities: deciding when to use on-demand retrieval, assessing whether retrieved evidence provides useful information to solve the question, criticizing whether the evidence supports the answer, and judging whether the answer is a useful response to the question.However, using Self-RAG is unsuitable for domain-specific questions like biomedical or clinical domains which shows poor generalization, leading to fetching incorrect documents or making inaccurate judgments.</p>
<p>In this paper, we introduce Self-BioRAG, trained with a focus on biomedical and clinical text instructions, enabling it to address corresponding instructions adeptly.It preserves generation quality and reasoning ability while incorporating on-demand retrieval and self-reflection capabilities.Note that we use the term reasoning to indicate that Self-BioRAG can provide explanations on answers.To build a Self-BioRAG framework, four essential components are required: (i) biomedical instruction sets, (ii) biomedical retriever, (iii) self-reflection language model, and (iv) domain-specific instruction-tuned language model.We initially construct instruction sets focused on biomedical and clinical text.In addition to the distributed MoL-instructions (Fang et al. 2023) and MedInstruct (Zhang et al. 2023), we synthetically generate an additional 18k biomedical and clinical instructions following the Self-Instruct (Wang et al. 2022).By combining three datasets, we could construct 120k instruction sets addressing various biomedical instructions, including information extraction, question answering, summarization, text classification, relation extraction, and multi-choice questions (Section 3.1).</p>
<p>Furthermore, we use the off-the-shelf MedCPT (Jin et al. 2023) retriever and construct biomedical corpora as follows: PubMed Abstract, PMC Full Text, Clinical Guideline, and Medical Textbook, all tailored to biomedical and clinical text (Section 3.2).The training process for the self-reflection language model and the domain-specific instruction-tuned language model is similar to Self-RAG, except that, instead of directly training instructions into the LLaMA2 (Touvron et al. 2023) model, we achieve better performance by training the model weights provided by Self-RAG (Section 3.3, 3.4).The goal of our work is to construct a language model encoded with domain-specific knowledge, enabling it to autonomously assess explanations and answers it generates.</p>
<p>Self-BioRAG demonstrates its effectiveness using five opendomain question-answering (QA) benchmark datasets: multichoice QA [MedQA (Jin et al. 2021), MedMCQA (Pal et al. 2022), and MMLU (Hendrycks et al. 2020)] and long-form QA [LiveQA (Abacha et al. 2017) and MedicationQA (Abacha et al. 2019)].Experimental results on the multichoice QA datasets demonstrate that Self-BioRAG significantly outperforms open-foundation LLMs and RAG approaches with a parameter size of 7B or less.Self-BioRAG achieves a 7.2% absolute improvement compared to the state-of-the-art model.In long-form QA datasets, we notice a substantial difference in the terminologies used despite generating predictions that are similar to answers.We demonstrate that domain-specific components contribute to the performance gains, with training on domain-specific instructions showing the highest improvement.Our biomedical corpora supplement scarce knowledge, and particularly, Self-BioRAG uses appropriate documents if needed corresponding to the benchmark datasets.We further analyze that using reflective tokens to adaptively retrieve factual content is effective in solving open-domain question-answering datasets.Overall, Self-BioRAG finds the clues in the question, retrieves relevant evidence, and understands how to answer with information using encoded knowledge.</p>
<p>Our contributions are as follows: (i) We introduce a Self-BioRAG framework which is extensively trained on biomedical and clinical instructions.(ii) We prove that domain-specific components such as retriever, documents, and instruction sets are necessary to address its domain-related instructions.(iii) Self-BioRAG demonstrates its effectiveness in three opendomain biomedical question-answering benchmark datasets by achieving an average absolute improvement of 7.2% compared to the state-of-the-art open-foundation model with a parameter size of 7B or less.(iv) We release our biomedical instruction sets, code for training our components used in Self-BioRAG, and model weights (7B and 13B) to be more capable in biomedical and clinical domains.</p>
<p>Background</p>
<p>Proprietary and open language models</p>
<p>Instructions serve as guidelines for how language models should perform a particular task.In the commercial field, proprietary language models such as InstructGPT (Ouyang et al. 2022) and ChatGPT (OpenAI 2023a) have gained Comparison between three frameworks: generation using language model (LM), retrieval-augmented generation (RAG) using LM, and our Self-BioRAG.(A) depicts the process of sequence-to-sequence generation of LM. (B) The RAG framework first finds relevant documents from large-scale corpus such as PubMed Central and then provides the answer based on this factual content to address the shortage of scarce knowledge.(C) Initially, our domain-specific instruction-tuned model predicts whether retrieval is necessary.If a query does not require any retrieval of knowledge (factual content), it directly predicts the answer.However, if the query necessitates retrieval knowledge, Self-BioRAG utilizes the domain-specific retriever (MedCPT, in our case) to retrieve relevant documents.After retrieving the top-k evidence, the model selects the most pertinent evidence for the query.Ultimately, our language model is employed to select the best evidence and generate the answer based on the selected evidence and encoded knowledge.</p>
<p>significant advantages in tuning through instructions.However, researchers not involved in commercial fields may face challenges in using these models due to a lack of resources.Hence, research-friendly open foundation models like the LLaMA family (Touvron et al. 2023), Self-instruct (Wang et al. 2022), andAlpaca (Taori et al. 2023) are released.In this regard, domain-specific language models tailored for areas such as biomedical and clinical domains, like Galactica (Taylor et al. 2022) and Meditron (Chen et al. 2023), have also been released.Our research also aims to provide laborinexpensive methods that are easy to use in various vertical domains, including biomedical and clinical domains.Specifically, Self-BioRAG strives to develop a model capable of solving challenging tasks, ranging from multi-choice questions to long-form generations.</p>
<p>Learning with reward strategy</p>
<p>The proprietary language models trained with reinforcement learning from human feedback (RLHF), such as ChatGPT (OpenAI 2023a) and GPT-4 (OpenAI 2023b), excel at executing straightforward instructions (e.g.translation, code generation, and question answering) in alignment with human intent (Christiano et al. 2017, Schulman et al. 2017, Google 2023, OpenAI 2023a,b).In Self-RAG (Asai et al. 2023), a critic language model is employed to offer a costeffective reward strategy compared to RLHF, utilizing reflective tokens.The critic model determines whether a given task necessitates retrieval, evaluates the appropriateness of the retrieved context, assesses if the generated rationale aligns with the retrieved context, and ultimately judges the overall utility of the output.Our Self-BioRAG follows the approach of Self-RAG to create a domain-specific critic language model that not only maintains the aforementioned capabilities but is also well-versed in biomedical text.</p>
<p>Retrieval-augmented generation</p>
<p>The retrieval-augmented generation (RAG) significantly enhances performance in knowledge-intensive tasks and open-domain question-answering by providing context as input to the language model (Lewis et al. 2020, Mao et al. 2021, Kang et al. 2023).The retriever also plays a crucial role in language models by providing evidence for pretraining and few-shot fine-tuning (Guu et al. 2020, Izacard et al. 2022a).With the recent advancements in instruction language models, the combination of retriever and language models involves either using the retriever in advance to fetch evidence or iteratively retrieving it when needed (Jiang et al. 2023, Shao et al. 2023).Our base framework, Self-RAG (Asai et al. 2023), deviates from these approaches by being designed to perform retrieval on-demand, resulting in better cost efficiency compared to scenarios where retrieval is always active.However, in domain-specific fields like biomedical or clinical domains, the general method of retrieving context may not be applicable.Therefore, Self-BioRAG utilizes retrieval methods and documents tailored to specific domains, retrieving meaningful context that aligns with the intended field.</p>
<p>Self-BioRAG</p>
<p>In this section, we outline the process of creating our Self-BioRAG framework using various biomedical components.First, we leverage three datasets consisting of biomedical instructions, which are used to train language models to align with human intentions for biomedical text (Section 3.1).To supplement scarce knowledge via relevant documents in the biomedical or clinical domains, we employ an off-the-shelf MedCPT (Jin et al. 2023) retriever, known for its effectiveness in retrieving relevant documents in biomedical and clinical domains (Section 3.2).Subsequently, we develop a critic language model C to annotate the instruction sets which will contain information for facilitating an autonomous assessment of reflective criteria (Section 3.3).Lastly, we perform training on our generator model M using the instruction sets created with diverse biomedical components (Section 3.4).We depict our processes of data generation, training, and inference in Fig. 2.</p>
<p>Biomedical instruction datasets</p>
<p>List of instruction datasets for biomedical and clinical domains</p>
<p>To train the self-reflection language model (LM), also referred to as the critic LM C, we utilize diverse text triplets (instruction, input, output).Specifically, we collect two offthe-shelf instruction sets [Mol-Instructions (Fang et al. 2023) and MedInstruct (Zhang et al. 2023)], which include tasks like open-generation, true or false, and multi-choice questions.In addition to the distributed instruction sets, we synthetically generate an additional 18k biomedical and clinical instructions following the Self-Instruct (Wang et al. 2022).In total, we construct 120k biomedical instruction sets addressing diverse tasks: information extraction, question answering, and summarization.For instance, illustrated in Fig. 2, the example instruction was set to classify the given radiology report according to which part of the body is related and answer with the lumbar spine.The statistics of biomedical instruction sets are provided in Table 1.Detailed statistics of our generated instruction sets can be found in Supplementary Appendix SA.</p>
<p>Biomedical retriever</p>
<p>In the fields of biomedical and clinical domains, researchers and doctors addressing challenging issues typically supplement their knowledge with additional information.Similarly, for a language model to solve problems, it needs to retrieve relevant documents as needed.To achieve this, we use the off-the-shelf MedCPT (Jin et al. 2023) retriever (https:// github.com/ncbi/MedCPT),which is contrastively trained on an unprecedented scale of 255M query-article pairs from PubMed search logs.To retrieve relevant documents, we compile data from four sources: PubMed Abstract (https:// pubmed.ncbi.nlm.nih.gov/),PMC Full-text (https://www.ncbi.nlm.nih.gov/pmc/tools/textmining/),Clinical Guidelines [a publicly released subset of 35 733 guideline articles from MEDITRON (Chen et al., 2023) offline to make it computationally effective.The documents are segmented into chunks of 128 words with 32-word overlaps to form evidence following previous works (Wang et al. 2019, Karpukhin et al. 2020).We first retrieve top-k (k ¼ 10, in our case) evidence from each source data (total 4k evidence) and then use the reranking module to obtain the final top-k evidence relevant to the query.Table 2 presents the overall statistics of biomedical corpus and how many documents are indexed.</p>
<p>3.3 Self-reflection language model (critic language model)</p>
<p>Data construction of critic LM C</p>
<p>We collect a total of 120k biomedical instruction sets and randomly sample 5k examples (D s ) to train the critic LM C. We use GPT-4 API Calls to generate reflective tokens r, guiding the critic model C in learning how to predict these tokens.We follow the usage of four types of reflective tokens r employed in Self-RAG, as described in   Note that training the critic LM C signifies it to predict pre-defined reflective tokens given instruction, output, and optionally evidence.We use trained LM C to annotate whole instruction sets and filter out instances when it mispredicts the reflective tokens that are not pre-defined such as [Continue Generation].We provide detailed hyperparameters used to train the critic LM C in Supplementary Appendix SC.</p>
<p>Annotating biomedical instruction sets using critic LM C</p>
<p>After training, the model C predicts four types of reflective tokens: (i) identifying whether a question requires retrieval (RET); (ii) determining if retrieved evidence provides useful information to solve a question (REL); (iii) assessing whether all statements of answers can be supported by evidence (SUP); (iv) evaluating whether all statements of answers are a useful response to the question (USE).For example, in Fig. 2, the model C predicts the retrieval of factual content related to the role of BRCA1 and BRCA2 gene mutation ([Retrieval]).Then, the model predicts that the retrieved evidence provides a fact that BRCA1 and BRCA2 play similar roles in breast cancer and sporadic cancer ([Relevant]).By comparing a statement of the answer and retrieved evidence, the model C predicts that the answer could be supported by evidence ([Fully supported]).Finally, the model C suggests that all statements of answers are useful responses to the question ([Utility: 4]).After annotating each type of reflective token, we aggregate all results to construct a complete instance as above.We provide detailed instructions to annotate the biomedical instruction dataset using the critic LM in Supplementary Appendix SH.</p>
<p>3.4 Domain-specific instruction-tuned language model (generator language model)</p>
<p>Data construction using critic LM C and training generator LM M</p>
<p>We use MedCPT to retrieve top-k evidence following an instruction that necessitates retrieval of biomedical context.</p>
<p>After retrieving relevant documents, we use the critic LM C to predict each reflective token as described in Table 3.Consequently, we preserve 84k filtered instances of biomedical instruction sets annotated with pre-defined reflective tokens, instruction, and output triplets to train generator LM M. We want to point out that the critic LM C is only used to annotate reflective tokens to generate biomedical instruction sets to train generator LM M. We fine-tune these filtered 84k biomedical instructions on the generator model to predict answer with reflective tokens as below, max M E ðx;y;rÞ�D log p M ðy; rjxÞ</p>
<p>(2)</p>
<p>where D stands for filtered instruction sets annotated with pre-defined reflective tokens r.This enhances generalizability in the biomedical and clinical domains preserving the abilities of text generation and self-assessment of its generated explanations with reflective tokens.</p>
<p>2 Inference process of Self-BioRAG</p>
<p>In Fig. 2, we present a MedQA (Jin et al. 2021) example to illustrate our Self-BioRAG inference offline.For instance, the question is inquiring about the diagnosis of a female patient exhibiting symptoms of obesity, acne, and has a history of type 2 diabetes mellitus.The generator model M determines the need to retrieve a relevant document and selects the best evidence from the top-k retrieved documents based on a score S, calculated as the weighted sum of reflective tokens, using the same hyperparameters as Self-RAG,
SðCritiqueÞ ¼ P G2G w G s G ; G ¼ REL [ SUP [ USE s G ¼ pðrÞ P N G i¼1 pðr i Þ
where s G denotes the generation probability of the most desirable reflective token r (e.g.[Fully supported]) for reflective token type G (e.g.SUP) and w G represents the hyperparameter providing weight for s G .We can set the weight w G to adjust our behavior at inference time.For example, to find the most relevant document e related to question x, we can set a weight term REL score higher.Self-BioRAG is tailored to conditionally generate text without any additional training which could need balancing the trade-off between multiple preferences (Touvron et al. 2023, Wu et al. 2023b).</p>
<p>The prioritized evidence includes information on the family history of type 2 diabetes mellitus and the patient's diagnosis of polycystic ovarian syndrome (PCOS).Due to space limitations, we display partial information in the figure; please refer to the complete case in Table 8.Consequently, the generator model M generates the following text: (i) the patient has acne and obesity, typical symptoms of PCOS; (ii) the patient has a family history of type 2 diabetes mellitus, often associated</p>
<p>Baselines</p>
<p>In Table 4, we compare Self-BioRAG with proprietary, open foundation, and open foundation with retrieval-augmented language models.We report the Med-PaLM score as presented in Med-PaLM (Singhal et al. 2022) and the GPT-3.5 and GPT-4-base scores as presented in Nori et al. (2023) to establish the upper bound of benchmark datasets (Row 1-3).</p>
<p>Open foundation models, pre-trained for sequence-tosequence generation with instruction tuning, such as Alpaca (Taori et al. 2023) and Flan-T5 (Chung et al. 2022), are reported (Rows 4 and 5), as well as models fine-tuned on the specific vertical domains (e.g.biomedical and clinical), like PMC-LLaMA (Wu et al. 2023a), Galactica (Taylor et al. 2022), MedAlpaca (Han et al. 2023) ).Therefore, we employ LLaMA2 for the result of retrieval-augmented generation (RAG) and provide the top-10 evidence collected from the biomedical corpus using the MedCPT retriever (Row 11).Due to the length limit of RAG for input, we can only leverage the top 1 evidence in input and few-shot examples.In addition, we report Self-RAG (Asai et al. 2023) using Contriever (Izacard et al. 2022b) fine-tuned on MSMARCO (Bajaj et al. 2016) with the Wikipedia corpus (Row 12).We compare these baselines with our Self-BioRAG framework which is trained with biomedical components.</p>
<p>Training and inference settings</p>
<p>Self-BioRAG is trained with 84k biomedical instruction sets filtered using a trained critic language model (LM).We adopt the Self-RAG critic LM as our base model and fine-tune it with 5k sampled instruction sets annotated by GPT-4 API calls.As training on the Self-RAG generator LM yields better results, we fine-tune our biomedical instruction sets instead of training directly on LLaMA2 (Touvron et al. 2023) or Meditron (Chen et al. 2023).For the retriever, we use the off-the-shelf MedCPT (Jin et al. 2023) retriever, specialized in retrieving documents based on biomedical queries and retrieving up to ten evidence for each input.</p>
<p>For inference, we use vllm (Kwon et al. 2023) to speed up our inference time.Following Self-RAG (Asai et al. 2023), we assign the same weight terms for reflective tokens (e.g.REL, SUP, USE) in decoding.We adopt adaptive retrieval by default which dynamically decides when to retrieve the evidence by predicting a reflective token [Retrieval].we retrieve the top ten evidence from the biomedical corpus processed offline.We provide details of the retrieved percentage of source data used to evaluate biomedical benchmark datasets in Section 5.2.(Singhal et al. 2022).The score of GPT-3.5 and GPT-4-base models are from the following paper (Nori et al. 2023).We use biomedical corpus (e.g.PubMed, PMC, CPG, and Textbook) as evidence during inference on the RAG setting.The best score is highlighted in bold for the parameter size of 7B or less and our 13B model.</p>
<p>i124</p>
<p>Jeong et al.</p>
<p>Results and analysis</p>
<p>5.1 Experimental results</p>
<p>What contributes to the performance improvements in Self-BioRAG?</p>
<p>In Table 4, we compare our Self-BioRAG with open foundation language model (LM) and retrieval augmented generation (RAG).With a parameter size of 7B or less, our Self-BioRAG outperforms other open foundation LMs (Row 4-10) in all three biomedical benchmark datasets (MedQA, MedMCQA, and MMLU-Med).We also compare our model with baselines using retrieval evidence.The RAG pipeline faces two challenges: it struggles to identify crucial evidence and encounters limitations in incorporating numerous pieces of evidence due to constraints on input length.However, our Self-BioRAG outperforms the RAG baseline and can prioritize important evidence via the values of reflective tokens, which is useful for analyzing all the retrieved evidence (Rows 11 and 13).Although Self-RAG is fine-tuned on LLaMA2, we observe that Self-RAG cannot generalize to biomedical benchmark datasets, resulting in a performance drop (Rows 10 and 12).By providing a biomedical critic LM and corpus to train a biomedical generator LM, our Self-BioRAG achieves state-of-the-art performance on 7B parameters in MedQA, MedMCQA, and MMLU-Med datasets.We also provide the 13B performance of our Self-BioRAG model to demonstrate the effectiveness of our framework works in other model parameters (Row 14).We provide the detailed performance of specific MMLU datasets in Supplementary Appendix SF.</p>
<p>In Table 5, we compare our Self-BioRAG with two open foundation LM by measuring n-gram recall performance [Rouge Score (Lin 2004)] and similarity of token embeddings between prediction and answer [BERTScore (Zhang et al. 2019)].We observe that although all foundation models do not generate predictions with the exact same words as the answers (lower Rouge Score), they manage to explain well with words that are as similar as possible (high BERTScore).However, these scores cannot measure whether a model has generated answers with accurate rationale, how much hallucination occurs, how much it includes crucial claims, or whether it has generated answers fluently.We leave an investigation about detailed capacities related to long-text generation for future works.We aim to analyze the step-by-step process through which our Self-BioRAG achieves its state-ofthe-art performance in the following subsection.</p>
<p>Analysis</p>
<p>Which domain-adaptation components show the improvements compared to Self-RAG?</p>
<p>In Table 6, each experiment involves sequentially reducing components in Self-BioRAG.The goal is to identify the factors that significantly contributed to the performance improvement, ultimately leading to the final performance of Self-BioRAG.First, the controllable generation using reflective tokens affects the rationale which leads to predicting an answer (Row 2).Then, we observe that using four biomedical corpora (PubMed, PMC, CPG, and Medical Textbook) to retrieve appropriate evidence shows performance improvement compared to Wikipedia evidence (Row 3).We also use domain-specific MedCPT retriever instead of the Contriever (Izacard et al. 2022b) fine-tuned on MSMARCO (Bajaj et al. 2016) (Row 4).Ultimately, the most effective approach was the collection and processing of biomedical instruction sets to create both a critic language model and a generation language model (Row 5).We recommend readers collect their domainspecific instructions to address corresponding instructions.</p>
<p>In biomedical corpora, what evidence is used to solve open-domain question-answering benchmarks?</p>
<p>In Fig. 3, we compare the ratio of retrieved evidence using the MedCPT (Jin et al. 2023) retriever on four biomedical corpora (PubMed, PMC, CPG, and Medical Textbook).Even though the index sizes of Medical Textbook and CPG are much smaller than PubMed or PMC, retrieved evidences show even distribution.Specifically, our Self-BioRAG only retrieves small portions to solve three datasets [MedQA (12%), MedMCQA (8%), and MMLU-Med (11%)] meaning that these open-domain benchmarks do not require that much evidence than expected.We depict these portions up to 100% in Fig. 3.We observe a trend in which Self-BioRAG retrieves a higher proportion of information from the Medical Textbook, similar to the approach used in solving USMLE-style questions.This is also aligned with previous facts that retrieving documents from Medical Textbook can achieve higher performance in clinical questions (Li et al. 2023, Wang et al. 2023).a We report the Rouge-1 (R1), Rouge-2 (R2), Rouge-L (RL) scores to measure n-gram recall performance and report BERTScore (BS) which computes the similarity of two sentences as a sum of cosine similarities between their tokens' embeddings.The best scores are highlighted in the bold.(3)</p>
<p>where we set δ hyperparameter as 0.2 for the Adaptive Retrieve experiment setting.Our findings indicate that retrieving relevant documents indeed aids in solving benchmark datasets.In addition, we observed that adaptively retrieving shows comparable performance on average with the Only Retrieve setting.This is attributed to the small portion of retrieved evidence used to answer the questions.While the Only Retrieve setting exhibits a substantial improvement in MedMCQA, it shows a performance drop in MMLU-Medical datasets compared to the No Retrieve setting, indicating its instability.As a result, we recommend readers to use the adaptive retrieval setting.</p>
<p>Distinguishing when to retrieve documents in Self-BioRAG</p>
<p>In Fig. 4, we evaluate the performance of LLaMA2, RAG (LLaMA2 with MedCPT and biomedical corpora), and Self-</p>
<p>BioRAG on examples predicted as [No Retrieval] and</p>
<p>[Retrieval] by Self-BioRAG.To show an overall trend, we use the MedQA dataset here and the rest of the two datasets are in Supplementary Appendix SE.Notably, Self-BioRAG retrieves small portions to solve three biomedical benchmarks.Still, the results demonstrate that Self-BioRAG consistently outperforms other baselines, whether or not retrieved evidence is used.In situations where retrieval is not necessary (left column), Self-BioRAG &gt; RAG � LLaMA2.The overall trend in the retrieved situation (right column) indicates Self-BioRAG &gt; RAG ≥ LLaMA2.Intuitively, we identify that Self-BioRAG distinguishes well on situations to use retrieved evidence or not depending on questions.</p>
<p>Case report of using retrieved evidence</p>
<p>In Table 8, we present an example from the MedQA dataset to illustrate how Self-BioRAG works.For instance, a patient exhibits symptoms of physical appearance changes, acne, and a family history of type 2 diabetes mellitus (T2DM).Self-BioRAG determines the need to retrieve relevant documents containing information on a female diagnosed with polycystic ovarian syndrome (PCOS) and similar symptoms (e.g.T2DM and obesity).Self-BioRAG determines the patient's diagnosis as PCOS by integrating all three: patient's symptoms, retrieved evidence, and parametric knowledge.Throughout the query, evidence, and prediction, we color-code using blue and red to distinguish two categories of related snippets: (i) key information extracted from retrieved evidence and (ii) the model's essential parametric knowledge, both of which are</p>
<p>Conclusion</p>
<p>In this manuscript, we introduce the Self-BioRAG framework, enabling a Self-RAG (Asai et al. 2023) to generalize to biomedical and clinical domains of instructions.This framework enhances the generation capacity, facilitates the retrieval of factual content on demand, and enables self-assessment of generated rationales.Our experimental results cover five open-domain question-answering (QA) datasets widely used in biomedical and clinical domains.In multi-choice QA datasets, Self-BioRAG achieves a 7.2% absolute improvement compared to the stateof-the-art model among the open foundation 7B models.In Long-form QA datasets, Self-BioRAG exhibits notable variations in term usage, despite producing predictions that closely resemble answers.We demonstrate the necessity of domainspecific components, such as retriever, domain-related document corpus, self-reflection model, and generator model, to address domain-related instructions.We provide diverse analyses: Table 8.Case report of Self-BioRAG prediction using evidence in MedQA dataset.a</p>
<p>MedQA Dataset</p>
<p>Query: A 27-year-old woman presents to the office with concerns about her long struggle with her physical appearance since adolescence.She says she has always been "large" and was constantly targeted by her classmates and coworkers for being so.Her main concern at the moment is her acne and unwanted facial hair on her upper lip, for which she often visits a local spa.She has tried numerous diet plans, exercise regimens, and cosmetic products with little to no effect.Recently, she underwent a glucose tolerance test that showed a plasma glucose level of 160 mg/dL (8.9 mmol/L) after 2 hours of a 75 g dose of oral glucose.She has a family history of type 2 diabetes mellitus and a menstrual cycle that occurs every 45 days.Her pulse is 72/min and the blood pressure is 138/80 mm Hg.On physical examination, her height is a Retrieved evidence is written in italics.Blue-colored text comprises segments connected to key information from retrieved evidence, while red-colored text consists of segments tied to the model's parametric knowledge.</p>
<p>Improving medical reasoning i127</p>
<p>(i) self-BioRAG retrieves a larger portion of evidence from Medical Textbook than other corpora to solve USMLE-style questions; (ii) self-BioRAG can distinguish when to retrieve evidence depending on instruction and question; (iii) provided evidence from biomedical corpora genuinely helps supplement scarce knowledge.In future works, we aim to explore generating long-form text in a fine-grained evaluation which could interpret how open foundation models (with or without domain adaptation) generate.</p>
<p>Figure 1 .
1
Figure 1.Comparison between three frameworks: generation using language model (LM), retrieval-augmented generation (RAG) using LM, and our Self-BioRAG.(A) depicts the process of sequence-to-sequence generation of LM. (B) The RAG framework first finds relevant documents from large-scale corpus such as PubMed Central and then provides the answer based on this factual content to address the shortage of scarce knowledge.(C) Initially, our domain-specific instruction-tuned model predicts whether retrieval is necessary.If a query does not require any retrieval of knowledge (factual content), it directly predicts the answer.However, if the query necessitates retrieval knowledge, Self-BioRAG utilizes the domain-specific retriever (MedCPT, in our case) to retrieve relevant documents.After retrieving the top-k evidence, the model selects the most pertinent evidence for the query.Ultimately, our language model is employed to select the best evidence and generate the answer based on the selected evidence and encoded knowledge.</p>
<p>Figure 2 .
2
Figure 2. Overview of our Self-BioRAG process: data construction, training, and inference of Self-Reflection Language Model (critic LM C) and Domainspecific Instruction-tuned Language Model (generator LM M).We construct 120k biomedical instruction sets using two off-the-shelf instruction sets [Mol-Instructions (Fang et al. 2023) and MedInstruct (Zhang et al. 2023)] and one self-generated biomedical instruction set.We first sample 5k instructions to generate reflective tokens via GPT-4 API calls and then train the critic LM C with these instructions.Using trained critic LM C, we filter out mispredicted reflective tokens, such as [Continue Generation].We preserve 84k instruction sets annotated with pre-defined reflective tokens to train the generator LM M. Note that critic LM C is only used for annotating reflective tokens used to filter instruction sets to train generator LM M. After training, the model M can predict whether or not to use the retrieval method and combine the results of evidence and encoded knowledge to answer the question.We use the MedQA (Jin et al. 2021) test sample to gain a proper understanding of how our Self-BioRAG works.</p>
<p>Figure 3 .
3
Figure3.Ratio of retrieved evidences from each of the four biomedical corpora (PubMed, PMC, CPG, Medical Textbook).The RAG statistics refer to the top-1 evidence usage ratio, while Self-BioRAG selects the most useful evidence from the top-10 retrieved evidence.</p>
<p>Figure 4 .
4
Figure 4. Performance of LLaMA2, RAG, and Self-BioRAG on examples split into [No Retrieval] and [Retrieval] based on Self-BioRAG using the MedQA test dataset.</p>
<p>160 cm (5 ft 3 in) and her weight is 85 kg (187 lb), and she has severe inflammatory acne over the cheeks and forehead and dark coarse hairs on the back.What is the most likely diagnosis of this patient?Option A: Hypothyroidism Option B: Idiopathic hirsutism Option C: Polycystic ovarian syndrome (PCOS) Option D: Ovarian hyperthecosis Evidence: Early Clinical Expressions of Insulin Resistance: The Real Enemy to Look For.Today, a very common clinical scenario is a 17-year-old female with a family history of type 2 diabetes mellitus (T2DM) and hypertension in her mother and two first-degree relatives.Three years ago she was diagnosed with polycystic ovarian syndrome (PCOS).She is overweight with her body weight increasing 10 kg in the last 2 years.Additionally, she has acanthosis nigricans (AN) in the axillae and knuckles.Blood tests were unremarkable with a fasting plasma glucose value of 82 mg/dl and an HbA1c of 5.0%.Prediction: The most likely diagnosis of this patient is Option C: Polycystic ovarian syndrome (PCOS).The patient has the typical symptoms of PCOS, such as acne, hirsutism, and obesity.Additionally, she has a family history of type 2 diabetes mellitus, which is often associated with PCOS.The glucose tolerance test result of 160 mg/dL (8.9 mmol/L) after 2 hours confirms impaired glucose tolerance, which is another characteristic feature of PCOS.</p>
<p>, extracted from 8 sources: CCO, CDC, CMA, ICRC, NICE, SPOR, WHO, and WikiDoc], and English Medical Textbooks [Medical textbooks widely used by medical students and takers of the United States Medical Licensing Examination (USMLE), https://github.com/jind11/MedQA].We encode these data
Improving medical reasoningi121</p>
<p>Table 3
3
. Detailed statistics and prompts used in generating each type of reflective token are provided in Supplementary Appendix SG.Exploring other reflective tokens suitable for specific domains is left for future work.</p>
<p>Table 1 .
1
Statistics of biomedical instruction sets.a
Dataset nameNo. of instances (original ! filtered)Tasks typesMol-instructions (Fang et al. 2023)51 493 ! 38 156Information extraction, question answering, multi-choice questionMedInstruct (Zhang et al. 2023)52 002 ! 36 429Question answering, summarization, text classification, multi-choice questionBiomedical Instructions (Ours)18 854 ! 10 143Text generation, question answering, relation extraction, textclassification, summarizationTotal122 349 ! 84 728Question answering, information extraction, text classifica-tion, summarization, text generation
a We filter instructions using the critic language model C and use it to train the generator language model M.</p>
<p>Table 2 .
2
(Touvron et al. 2023)exed biomedical corpus.aThus,we decide to develop a domain-specific critic LM C using our biomedical instruction sets.We split the sampled instruction sets into train and dev to train and assess the performance of the critic LM C. We train the model using four types of reflective tokens r annotated with GPT-4 API calls.We initialize the critic LM C with a pre-trained language model [here we use LLaMA2(Touvron et al. 2023)] and train it on the sampled dataset D s to maximize the likelihood as below.
DataNo. of documentsNo. of ChunksEmbedding sizePubMed36 533 37769 743 442400 GBPMC1 060 17346 294 271160 GBCPG35 733606 7853.5 GBTextbook18133 8750.7 GBa CPG stands for Clinical Practice Guideline.
CE ðx;y;rÞ�Ds log p C ðrjx; yÞ(1)</p>
<p>Table 3 .
3
Reflective tokens r used in Self-BioRAG.a x, y, and e respectively indicate input, output, and evidence.Specific reflective tokens highlighted in bold are desirable during data construction as they contribute to preserving the existing instruction data possible.
Improving medical reasoningi123TypeInputOutputDefinitionsRETx=x; yfyes, no, continuegDecides when to retrieve using RRELx, efrelevant, irrelevantge provides useful information to solve xSUPx, e, yffully supported, partially supported, no supportgAll of the verification-worthy statement in y is supported by eUSEx, yf5, 4, 3, 2, 1gy is a useful response to x
a</p>
<p>, and Meditron (Chen et al. 2023) (Row 6-9).LLaMA2 (Touvron et al. 2023) demonstrates state-of-the-art performance in open-foundation 7B models in our experiment (Row 10</p>
<p>Table 4 .
4
Experimental results on biomedical benchmark datasets.a
Open-domain biomedicalbenchmark
a We use 3-shot examples as guidelines for language models to address benchmark instances.These examples are chosen from each training dataset using k-nearest-neighbor (Guo et al. 2003).Since the MMLU dataset lacks training data, we employ the same examples detailed in the appendix of MedPALM</p>
<p>Table 5 .
5
Results of Long-form question-answering benchmark.a
ModelLiveQA (R1/R2/RL/BS)MedicationQA (R1/R2/RL/BS)MEDITRON (Chen et al. 2023)5.5/0.0/2.5/77.24.1/0.2/3.3/75.9LLaMA2 (Touvron et al. 2023)8.8/1.9/6.2/78.85.7/1.2/4.4/77.6RAG11.5/2.3/11.1/69.59.8/1.3/4.8/72.9Self-BioRAG (Ours)19.7/3.1/13.4/77.217.6/3.3/13.5/80.2</p>
<p>Table 6 .
6
Effect of each domain-adaptation component.
Experiment DetailMedQA (Acc.)MedMCQA (Acc.)MMLU-Med (Acc.)AverageSelf-BioRAG43.642.153.946.5-Reflective Tokens42.541.951.145.2 (-1.3)-Biomedical Corpora40.740.749.343.6 (-2.9)-MedCPT Retriever39.838.947.642.1 (-4.4)-Biomedical Instruction Sets34.836.446.439.2 (-7.3)Self-BioRAG provide highest score represented in bold.</p>
<p>Table 7 .
7
Effect of adaptive retrieval in Self-BioRAG.aThebest scores are highlighted in the bold.
MethodsMedQA (Acc.)MedMCQA (Acc.)MMLU-Med (Acc.)AverageOnly [No Retrieval]39.741.952.844.8Only [Retrieval]40.147.251.346.2Adaptive Retrieval (Ours)43.642.153.946.5
a "Only [No Retrieval]" refers to not retrieving any evidence, while "Only [Retrieval]" refers to forcing the retrieval of top 10 evidences.</p>
<p>i120Jeong et al.
AcknowledgementsWe thank Gangwoo Kim, Hyeon Hwang, Chanhwi Kim, and Akari Asai for the valuable feedback on our work.Self-BioRAG is available at https://github.com/dmis-lab/self-biorag.FundingThis work was supported in part by the National Research Foundation of Korea [NRF-2023R1A2C3004176, NRF-2022R1C1C1008074], the Ministry of Health &amp; Welfare, Republic of Korea [HR20C0021(3), HR22C1302], the Ministry of Science and ICT (MSIT) [RS-2023-00262002, RS-2022-00155911 (Artificial Intelligence Convergence Innovation Human Resources Development (Kyung Hee University))], and the ICT Creative Consilience program through the Institute of Information &amp; Communications Technology Planning &amp; Evaluation (IITP) grant funded by the MSIT [IITP-2024-2020-0-01819].Supplementary dataSupplementary data are available at Bioinformatics online.Conflict of interestNone declared.
Overview of the medical question answering task at trec 2017 liveqa. References Abacha, A B , Eugene A Yuval, P , TREC. 2017</p>
<p>Bridging the gap between consumers' medication questions and trusted answers. A B Abacha, M Yassine, S Mark, MedInfo. 2019</p>
<p>Self-rag: learning to retrieve, generate, and critique through self-reflection. A Asai, W Zeqiu, W Yizhong, arXiv:2310.115112023preprint: not peer reviewed</p>
<p>Ms marco: a human generated machine reading comprehension dataset. P Bajaj, C Daniel, Nick C , arXiv:1611.092682016preprint: not peer reviewed</p>
<p>Hallucinated but factual! inspecting the factuality of hallucinations in abstractive summarization. M Cao, D Yue, Jackie C Kit, C , 2022Association for Computational LinguisticsDublin, Ireland</p>
<p>Meditron-70b: Scaling medical pretraining for large language models. Z Chen, H Alenjandro, A R Cano, arXiv:2311.160792023preprint: not peer reviewed</p>
<p>Deep reinforcement learning from human preferences. P F Christiano, L Jan, T B Brown, Advances in Neural Information Processing Systems. Long Beach, CA, USA2017</p>
<p>Mol-instructions: a large-scale biomolecular instruction dataset for large language models. H W Chung, H Le, Shayne L , arXiv:2210.11416arXiv:2306.08018On The Move to Meaningful Internet Systems 2003: CoopIS, DOA, and ODBASE. 2022. 2023. 2003Scaling instruction-finetuned language models. arXiv</p>
<p>Retrieval augmented language model pre-training. K Guu, L Kenton, Zora T , International Conference On Machine Learning. 2020</p>
<p>Medalpaca-an open-source collection of medical conversational ai models and training data. T Han, L C Adams, J M Papaioannou, arXiv:2304.082472023arXivpreprint: not peer reviewed</p>
<p>Few-shot learning with retrieval augmented language models. D Hendrycks, Collin B Steven, B , arXiv:2009.03300arXiv:2208.032992020Measuring massive multitask language understanding. 2022a, preprint: not peer reviewed</p>
<p>Unsupervised dense information retrieval with contrastive learning. G Izacard, Mathilde C Lucas, H , Transactions on Machine Learning Research. 2022b</p>
<p>Survey of hallucination in natural language generation. Z Ji, L Nayeon, Rita F , 2023ACM Computing Surveys</p>
<p>Active retrieval augmented generation. Z Jiang, X Frank, G Luyu, arXiv:2305.069832023preprint: not peer reviewed</p>
<p>What disease does this patient have? a large-scale open domain question answering dataset from medical exams. D Jin, P Eileen, O Nassim, Appl Sci. 2021</p>
<p>Medcpt: contrastive pre-trained transformers with large-scale pubmed search logs for zero-shot biomedical information retrieval. Q Jin, W Kim, Q Chen, Bioinformatics. 396512023</p>
<p>Knowledge-augmented reasoning distillation for small language models in knowledge-intensive tasks. M Kang, L Seanie, B Jinheon, arXiv:2305.183952023preprint: not peer reviewed</p>
<p>Dense passage retrieval for open-domain question answering. V Karpukhin, O Barlas, M Sewon, Empirical Methods in Natural Language Processing. 2020</p>
<p>Efficient memory management for large language model serving with pagedattention. W Kwon, L Zhuohan, Z Siyuan, Symposium on Operating Systems Principles. 2023</p>
<p>Retrieval-augmented generation for knowledge-intensive NLP tasks. P Lewis, P Ethan, P Aleksandra, Advances in Neural Information Processing Systems. 2020</p>
<p>Meddm: Llm-executable clinical guidance tree for clinical decision-making. B Li, M Tianxin, S Xiaoming, arXiv:2312.024412023preprint: not peer reviewed</p>
<p>Rouge: a package for automatic evaluation of summaries. C-Y Lin, Text Summarization Branches Out. 2004</p>
<p>Generation-augmented retrieval for open-domain question answering. Y Mao, H Pengcheng, L Xiaodong, Association for Computational Linguistics and International Joint Conference on Natural Language Processing. 2021</p>
<p>Training language models to follow instructions with human feedback. H Nori, K Nicholas, M M Scott, arXiv:2303.13375Capabilities of gpt-4 on medical challenge problems. arXiv. 2023. 2023b. 2022Advances in Neural Information Processing Systems</p>
<p>Medmcqa: a large-scale multisubject multi-choice dataset for medical domain question answering. A Pal, Logesh Kumar, U Malaikannan, S , Conference on Health, Inference, and Learning. 2022</p>
<p>J Schulman, Filip W Prafulla, D , arXiv:1707.06347Proximal policy optimization algorithms. arXiv. 2017preprint: not peer reviewed</p>
<p>Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy. Z Shao, G Yeyun, S Yelong, arXiv:2305.152942023preprint: not peer reviewed</p>
<p>Alpaca: a strong, replicable instruction-following model. K Singhal, A Shekoofeh, T Tao, arXiv:2212.13138Large language models encode clinical knowledge. arXiv. Stanford Center for Research on Foundation Models2022. 2023preprint: not peer reviewed</p>
<p>Self-instruct: aligning language model with self generated instructions. R Taylor, K Marcin, C Guillem, arXiv:2211.09085arXiv:2212.10560A large language model for science. arXiv. Toronto, Canada2022. 2023. 2022Llama 2: Open foundation and finetuned chat models. preprint: not peer reviewed</p>
<p>Augmenting black-box llms with medical textbooks for clinical question answering. Y Wang, M Xueguang, C Wenhu, arXiv:2309.022332023preprint: not peer reviewed</p>
<p>Multi-passage Bert: A globally normalized Bert model for open-domain question answering. Z Wang, N Patrick, M Xiaofei, Empirical Methods in Natural Language Processing and International Joint Conference on Natural Language Processing. Hong Kong, China2019</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, W Xuezhi, S Dale, Advances in Neural Information Processing Systems. 2022</p>
<p>C Wu, L Weixiong, Z Xiaoman, arXiv:2304.14454Pmc-llama: Further finetuning llama on medical papers. arXiv. 2023a, preprint: not peer reviewed</p>
<p>Fine-grained human feedback gives better rewards for language model training. Z Wu, H Yushi, S Weijia, arXiv:2306.016932023bpreprint: not peer reviewed</p>
<p>Bertscore: evaluating text generation with Bert. T Zhang, K Varsha, W Felix, International Conference on Learning Representations. 2019</p>
<p>Alpacare: instruction-tuned large language models for medical application. X Zhang, T Chenxin, Y Xianjun, arXiv:2310.145582023preprint: not peer reviewed</p>
<p>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited. Author The, 10.1093/bioinformatics/btae238ISMB2024Improvingmedicalreasoningi129Bioinformatics. 402024Oxford University PressPublished by. This is an Open Access article distributed under the terms of the Creative Commons Attribution License</p>            </div>
        </div>

    </div>
</body>
</html>
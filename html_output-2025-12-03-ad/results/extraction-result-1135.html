<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1135 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1135</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1135</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-260926041</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2308.08029v2.pdf" target="_blank">Sophisticated Learning: A novel algorithm for active learning during model-based planning</a></p>
                <p><strong>Paper Abstract:</strong> We introduce Sophisticated Learning (SL), a planning-to-learn algorithm that embeds active parameter learning inside the Sophisticated Inference (SI) tree-search framework of Active Inference. Unlike SI -- which optimizes beliefs about hidden states -- SL also updates beliefs about model parameters within each simulated branch, enabling counterfactual reasoning about how future observations would improve subsequent planning. We compared SL with Bayes-adaptive Reinforcement Learning (BARL) agents as well as with its parent algorithm, SI. Using a biologically inspired seasonal foraging task in which resources shift probabilistically over a 10x10 grid, we designed experiments that forced agents to balance probabilistic reward harvesting against information gathering. In early trials, where rapid learning is vital, SL agents survive, on average, 8.2% longer than SI and 35% longer than Bayes-adaptive Reinforcement Learning. While both SL and SI showed equal convergence performance, SL reached this convergence 40% faster than SI. Additionally, SL showed robust out-performance of other algorithms in altered environment configurations. Our results show that incorporating active learning into multi-step planning materially improves decision making under radical uncertainty, and reinforces the broader utility of Active Inference for modeling biologically relevant behavior.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1135.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1135.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sophisticated Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An Active Inference extension that embeds active parameter learning (Dirichlet concentration updates) and backward smoothing into recursive EFE tree search, allowing counterfactual simulation of how future observations would update model parameters and past state beliefs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Sophisticated Learning (SL)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Model-based Active Inference agent that combines the Sophisticated Inference recursive EFE tree-search with proactive parameter learning: it simulates Dirichlet concentration-parameter updates within forward search, uses those updated parameters to construct future likelihood/transition models, and applies backward-smoothing at each simulated time step to reassign past state posteriors.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Active learning / information-gain maximization (parameter exploration via novelty term in Expected Free Energy)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>SL adapts by (1) simulating forward how candidate action sequences would update Dirichlet concentration parameters for the likelihood (and optionally transition) models, (2) computing a novelty term (expected KL change in parameter beliefs) along with epistemic and pragmatic EFE components to score actions, and (3) using backward-smoothing within each simulated branch to reassign past posteriors and refine parameter updates — thus preferring actions expected to maximally reduce parameter uncertainty and improve future planning.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>10×10 Seasonal Foraging Grid-World (partially observable context/season)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Partially observable (POMDP): hidden discrete context (season) that stochastically transitions (0.95 stay, 0.05 to adjacent), discrete spatial grid observations with two modalities (resource presence: {Empty,Food,Water,Sleep}; hill reveals context), non-depleting resources, internally tracked homeostatic timers (known to agent), sparse/high-penalty outcomes (death if resource timer exceeds thresholds), non-stationarity via context changes.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Discrete state space: 100 grid positions × internal timers × 4 contexts; action space size 5 (up/down/left/right/stay); planning horizon used up to 9 in experiments; episodes up to 100 time-steps; evaluations of many trials (120 trials per evaluation in main sims) and thousands of random seeds (up to 2000) for aggregate statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>SL showed faster early learning and higher survival: Ramp-up (Trials 1-20) slope = 0.266 ± 0.007; Estimated Marginal Mean (EMM) survival at Trial 20 = 27.42 ± 0.08; at Trial 40 EMM = 40.78 ± 0.08. In early trials SL survived on average 8.2% longer than SI and 35% longer than BARL (reported aggregate). Convergence reached ~40% faster than SI; overall maintained slight advantage post-convergence.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>High: rapid gains in first 20 trials (slope 0.266) and established lead early; reported to reach convergence ~40% faster than SI (convergence of average performance typically stabilised around 100 trials, SL reaches equivalent performance earlier). Experiments used 120-trial evaluations and up to 2000 seeds.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Balances exploitation (pragmatic EFE term encoding preferences) with two epistemic drives: state epistemic value (standard EFE) and parameter epistemic / novelty value (expected KL-change of parameter Dirichlet counts). Backward-smoothing increases value of trajectories that allow precise retrospective credit assignment, biasing exploration toward states (e.g., hill) that disambiguate context and improve parameter learning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared directly to Sophisticated Inference (SI), Bayes-Adaptive Reinforcement Learning (BARL), and BARL augmented with an Upper Confidence Bound (BARL-UCB). Search heuristics (memoization, Monte Carlo rollouts, hybrid) and planning horizons were also compared.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>In the designed partially-observable foraging task SL achieved faster early learning and higher survival than SI and BARL: Ramp-up slope significantly larger than SI/BARL (p<.0001); at Trial 40 SL EMM = 40.78 vs SI 37.68 (Δ=3.10, p<.0001). SL prioritized visiting the hill after discovering a resource (retrospective contextualization) more often than alternatives, enabling faster correct assignment of resource locations to contexts. SL remained robust across 15 grid re-configurations, showing large effect sizes in early phases (Cohen's d≈1.39 for Phase 1). The novelty (parameter) term and backward smoothing were identified as mechanisms enabling proactive parameter exploration and faster convergence.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Failure modes include early epistemic commitments: incorrect low-probability observations can produce entrenched, self-confirming Dirichlet counts that lead SL to commit to maladaptive policies and preclude later correction; backward smoothing cannot always dislodge strong incorrect beliefs. SL is computationally expensive (recursive tree search with forward parameter updates and backward smoothing); memoization can introduce bias if path-dependent belief factors are not captured. Performance sensitive to priors, preference precision, planning depth, and hyperparameters.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sophisticated Learning: A novel algorithm for active learning during model-based planning', 'publication_date_yy_mm': '2025-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1135.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1135.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sophisticated Inference</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An Active Inference algorithm that implements Bellman-optimal multi-step planning by reformulating Expected Free Energy minimization as a recursive tree search over beliefs, using pruning and belief propagation to manage computational cost.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sophisticated inference</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Sophisticated Inference (SI)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Active Inference agent using recursive belief-space tree-search: it computes Expected Free Energy (EFE) recursively (a Bellman-like decomposition) to evaluate actions, employs pruning thresholds for low-probability branches and relatively high EFE branches, and propagates beliefs forward through simulated actions and observations without simulating parameter (Dirichlet) learning within branches in the tested implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Information gain maximization via Expected Free Energy (epistemic value for state uncertainty) — intrinsic Active Inference exploration</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>SI adapts action selection by computing the epistemic value (state uncertainty reduction) and pragmatic value (preference alignment) for candidate actions in a recursive tree search; it prioritizes actions leading to high expected reduction in state uncertainty (e.g., visiting hill) and prunes unlikely or low-value branches to focus search.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>10×10 Seasonal Foraging Grid-World (partially observable context/season)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Same POMDP as above: hidden discrete context (season), stochastic context transitions, hill reveals context only, resource observations noisy/partial, internal timers for homeostatic needs, sparse penalties on depletion.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Same discrete grid (100 positions), action set of 5, planning horizons tested 1–9 (main experiments used 9), episodes up to 100 steps; memoization and Monte Carlo rollouts explored to manage search complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>SI learned and improved over trials but more slowly than SL in early trials: Ramp-up slope = 0.107 ± 0.007; EMM at Trial 20 = 24.80 ± 0.08; at Trial 40 EMM = 37.68 ± 0.08 (vs SL 40.78). SI eventually approached SL performance in later trials (phase 3 differences narrowed).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Moderate: slower early learning compared to SL (less value placed on parameter-learning-directed visits to the hill), but steep improvement in Active Learning Phase (Trials 21-60) with slope 0.623 ± 0.006, indicating catch-up over time; converges more slowly (SL reached convergence ~40% faster).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Balances exploration and exploitation via EFE decomposition: epistemic term drives state uncertainty reduction (state-focused exploration), pragmatic term drives preference satisfaction; lacks an explicit parameter-novelty term so does not target model-parameter learning as explicitly as SL.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against SL, BARL, and BARL-UCB under identical environment and search heuristics (memoization/Monte Carlo/hybrid).</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>SI outperformed BARL in the known-environment baseline and showed strong state-directed exploration (often moving to the hill), but it underperformed SL in early parameter-learning-required settings because it lacked simulated parameter updates and backward smoothing; SI had similar asymptotic performance but reached it more slowly than SL.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Does not explicitly include a novelty/parameter-learning term; hence it undervalues actions that primarily improve model parameters (e.g., revisiting the hill to contextualize a newly discovered resource). Pruning and memoization are necessary for tractability but can bias search; limited Bellman optimality for deep planning without careful pruning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sophisticated Learning: A novel algorithm for active learning during model-based planning', 'publication_date_yy_mm': '2025-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1135.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1135.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BARL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayes-Adaptive Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Bayesian RL approach that augments state-space with beliefs over model parameters (Dirichlet counts), framing planning as an MDP over hyperstates and balancing exploration via Bayesian posterior updates; in experiments implemented with recursive planning over hyperstates and optional UCB bonus.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bayes-adaptive pomdps</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Bayes-Adaptive Reinforcement Learning (BARL)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Model-based Bayesian RL agent that represents uncertainty over transition/observation parameters via Dirichlet distributions and plans in the augmented belief (hyper-)state space; implementation used recursive tree-search similar to SI, simulated concentration-parameter updates during planning, and online sequential updates upon real observations.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Bayesian active learning / belief-state planning; optionally augmented with Upper Confidence Bound (UCB) heuristic for directed exploration</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>BARL adapts by maintaining posterior beliefs over model parameters (Dirichlet counts) and planning in belief-space (hyperstates) to maximize expected cumulative reward; in the implemented planner, concentration-parameter updates are simulated within forward search but those simulated updates are not carried forward across real time-steps (actual updates occur on real observations). When UCB is added, a bonus reward term reward + c * ln(t) / N_t(a) drives exploration of rarely visited actions/states.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>10×10 Seasonal Foraging Grid-World (partially observable context/season)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Same POMDP: partially observable, stochastic context transitions, sparse penalties, discrete observations and actions, need-driven dynamic preferences.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Same grid complexity: 100 positions, 5 actions, planning horizon up to 9 used in experiments; trials up to 100 time-steps; belief/hyperstate space large due to Dirichlet count dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>BARL (no UCB) had modest early performance; Ramp-up slope = 0.113 ± 0.007, EMM at Trial 20 = 26.18 ± 0.08. Overall, BARL underperformed SL (at Trial 40 EMM = 30.31 ± 0.08 vs SL 40.78) and was generally worse than ActInf methods when parameter-learning in POMDP was required.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Lower than SL and SI in the tested environment; initial trials sometimes better than SI due to less hill-prioritization, but overall needed many trials to learn correct mapping; showed poor learning in many configurations without explicit directed exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Intrinsic exploitation via explicit reward maximization in the belief-augmented MDP; exploration arises from Bayesian uncertainty in the belief state; optional UCB adds explicit count-based exploration bonus that increases visits to rarely-seen states/actions but is not goal-directed with respect to contextual credit assignment.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against SL and SI; also tested with an added UCB bonus (BARL-UCB) as an ablation to encourage directed exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>BARL performed worse than SI and SL in the primary POMDP requiring likelihood learning; BARL sometimes had slightly better very-early trial performance (trials 1-5) but failed to systematically value the hill for contextual disambiguation. Adding UCB slightly improved directed exploration but led to over-exploration of low-epistemic states and did not match SL performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Highly sensitive to initial priors; does not intrinsically prefer context-disambiguating actions (hill) unless those actions immediately increase expected reward; UCB augmentation produced non-goal-directed over-exploration; computational complexity high due to planning over hyperstates; in implemented planner simulated parameter updates during search were not committed across real steps, limiting prospective parameter-learning guidance compared to SL.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sophisticated Learning: A novel algorithm for active learning during model-based planning', 'publication_date_yy_mm': '2025-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1135.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1135.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BARL-UCB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayes-Adaptive Reinforcement Learning with UCB</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant of BARL augmented with a count-based Upper Confidence Bound exploration bonus added to the reward function to encourage directed exploration of under-visited states/actions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bayes-adaptive pomdps</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>BARL + UCB (BARL-UCB)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same Bayes-adaptive RL architecture as BARL but with an additional UCB-style bonus in the reward calculation: reward + c * ln(t) / N_t(a), where N_t(a) is visitation count and c a tunable constant; aims to encourage exploration of low-visit state-action pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Count-based directed exploration (UCB) combined with Bayesian belief updates</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Adapts by maintaining Dirichlet belief counts and selecting actions via belief-space planning augmented by an exploration bonus that favors rarely-visited actions/states; exploration bias is frequency-based rather than parameter-novelty or context-credit-driven.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>10×10 Seasonal Foraging Grid-World (partially observable context/season)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Same POMDP structure as for BARL and SL.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Same as BARL; experiments report EMM and slopes analogous to other methods.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>BARL-UCB had the slowest early learning slope (Ramp-up slope = 0.050 ± 0.007) and lower EMMs (Trial 20 EMM = 24.85 ± 0.08; Trial 40 EMM = 27.61 ± 0.08). UCB increased exploration but often wasted samples on low-epistemic states and underperformed SL and SI overall.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Lower sample-efficiency due to over-exploration of low-value states; required more trials to improve than SL and SI.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Explicit count-based exploration bonus increases exploration early, but lacks goal-directed contextual credit assignment (doesn't prioritize hill after resource discovery), leading to inefficient sample usage.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared directly to SL, SI, and BARL (no-UCB).</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>UCB augmentation increased visits to rarely-seen states but was not targeted to contextual disambiguation, resulting in over-exploration and worse survival/learning performance than SL and SI; BARL-UCB had the lowest ramp-up slope and poorer mid-phase performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Exploration driven by visitation counts rather than parameter-novelty or contextual value led to inefficient sampling in sparse/structured environments; failed to prioritize states (hill) that enable retrospective credit assignment, hurting parameter learning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sophisticated Learning: A novel algorithm for active learning during model-based planning', 'publication_date_yy_mm': '2025-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Sophisticated inference <em>(Rating: 2)</em></li>
                <li>Bayes-adaptive pomdps <em>(Rating: 2)</em></li>
                <li>Bayesian reinforcement learning: A survey <em>(Rating: 1)</em></li>
                <li>Active learning literature survey <em>(Rating: 1)</em></li>
                <li>Information-based objective functions for active data selection <em>(Rating: 1)</em></li>
                <li>Deep active inference agents using monte-carlo methods <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1135",
    "paper_id": "paper-260926041",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "SL",
            "name_full": "Sophisticated Learning",
            "brief_description": "An Active Inference extension that embeds active parameter learning (Dirichlet concentration updates) and backward smoothing into recursive EFE tree search, allowing counterfactual simulation of how future observations would update model parameters and past state beliefs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Sophisticated Learning (SL)",
            "agent_description": "Model-based Active Inference agent that combines the Sophisticated Inference recursive EFE tree-search with proactive parameter learning: it simulates Dirichlet concentration-parameter updates within forward search, uses those updated parameters to construct future likelihood/transition models, and applies backward-smoothing at each simulated time step to reassign past state posteriors.",
            "adaptive_design_method": "Active learning / information-gain maximization (parameter exploration via novelty term in Expected Free Energy)",
            "adaptation_strategy_description": "SL adapts by (1) simulating forward how candidate action sequences would update Dirichlet concentration parameters for the likelihood (and optionally transition) models, (2) computing a novelty term (expected KL change in parameter beliefs) along with epistemic and pragmatic EFE components to score actions, and (3) using backward-smoothing within each simulated branch to reassign past posteriors and refine parameter updates — thus preferring actions expected to maximally reduce parameter uncertainty and improve future planning.",
            "environment_name": "10×10 Seasonal Foraging Grid-World (partially observable context/season)",
            "environment_characteristics": "Partially observable (POMDP): hidden discrete context (season) that stochastically transitions (0.95 stay, 0.05 to adjacent), discrete spatial grid observations with two modalities (resource presence: {Empty,Food,Water,Sleep}; hill reveals context), non-depleting resources, internally tracked homeostatic timers (known to agent), sparse/high-penalty outcomes (death if resource timer exceeds thresholds), non-stationarity via context changes.",
            "environment_complexity": "Discrete state space: 100 grid positions × internal timers × 4 contexts; action space size 5 (up/down/left/right/stay); planning horizon used up to 9 in experiments; episodes up to 100 time-steps; evaluations of many trials (120 trials per evaluation in main sims) and thousands of random seeds (up to 2000) for aggregate statistics.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "SL showed faster early learning and higher survival: Ramp-up (Trials 1-20) slope = 0.266 ± 0.007; Estimated Marginal Mean (EMM) survival at Trial 20 = 27.42 ± 0.08; at Trial 40 EMM = 40.78 ± 0.08. In early trials SL survived on average 8.2% longer than SI and 35% longer than BARL (reported aggregate). Convergence reached ~40% faster than SI; overall maintained slight advantage post-convergence.",
            "performance_without_adaptation": null,
            "sample_efficiency": "High: rapid gains in first 20 trials (slope 0.266) and established lead early; reported to reach convergence ~40% faster than SI (convergence of average performance typically stabilised around 100 trials, SL reaches equivalent performance earlier). Experiments used 120-trial evaluations and up to 2000 seeds.",
            "exploration_exploitation_tradeoff": "Balances exploitation (pragmatic EFE term encoding preferences) with two epistemic drives: state epistemic value (standard EFE) and parameter epistemic / novelty value (expected KL-change of parameter Dirichlet counts). Backward-smoothing increases value of trajectories that allow precise retrospective credit assignment, biasing exploration toward states (e.g., hill) that disambiguate context and improve parameter learning.",
            "comparison_methods": "Compared directly to Sophisticated Inference (SI), Bayes-Adaptive Reinforcement Learning (BARL), and BARL augmented with an Upper Confidence Bound (BARL-UCB). Search heuristics (memoization, Monte Carlo rollouts, hybrid) and planning horizons were also compared.",
            "key_results": "In the designed partially-observable foraging task SL achieved faster early learning and higher survival than SI and BARL: Ramp-up slope significantly larger than SI/BARL (p&lt;.0001); at Trial 40 SL EMM = 40.78 vs SI 37.68 (Δ=3.10, p&lt;.0001). SL prioritized visiting the hill after discovering a resource (retrospective contextualization) more often than alternatives, enabling faster correct assignment of resource locations to contexts. SL remained robust across 15 grid re-configurations, showing large effect sizes in early phases (Cohen's d≈1.39 for Phase 1). The novelty (parameter) term and backward smoothing were identified as mechanisms enabling proactive parameter exploration and faster convergence.",
            "limitations_or_failures": "Failure modes include early epistemic commitments: incorrect low-probability observations can produce entrenched, self-confirming Dirichlet counts that lead SL to commit to maladaptive policies and preclude later correction; backward smoothing cannot always dislodge strong incorrect beliefs. SL is computationally expensive (recursive tree search with forward parameter updates and backward smoothing); memoization can introduce bias if path-dependent belief factors are not captured. Performance sensitive to priors, preference precision, planning depth, and hyperparameters.",
            "uuid": "e1135.0",
            "source_info": {
                "paper_title": "Sophisticated Learning: A novel algorithm for active learning during model-based planning",
                "publication_date_yy_mm": "2025-08"
            }
        },
        {
            "name_short": "SI",
            "name_full": "Sophisticated Inference",
            "brief_description": "An Active Inference algorithm that implements Bellman-optimal multi-step planning by reformulating Expected Free Energy minimization as a recursive tree search over beliefs, using pruning and belief propagation to manage computational cost.",
            "citation_title": "Sophisticated inference",
            "mention_or_use": "use",
            "agent_name": "Sophisticated Inference (SI)",
            "agent_description": "Active Inference agent using recursive belief-space tree-search: it computes Expected Free Energy (EFE) recursively (a Bellman-like decomposition) to evaluate actions, employs pruning thresholds for low-probability branches and relatively high EFE branches, and propagates beliefs forward through simulated actions and observations without simulating parameter (Dirichlet) learning within branches in the tested implementation.",
            "adaptive_design_method": "Information gain maximization via Expected Free Energy (epistemic value for state uncertainty) — intrinsic Active Inference exploration",
            "adaptation_strategy_description": "SI adapts action selection by computing the epistemic value (state uncertainty reduction) and pragmatic value (preference alignment) for candidate actions in a recursive tree search; it prioritizes actions leading to high expected reduction in state uncertainty (e.g., visiting hill) and prunes unlikely or low-value branches to focus search.",
            "environment_name": "10×10 Seasonal Foraging Grid-World (partially observable context/season)",
            "environment_characteristics": "Same POMDP as above: hidden discrete context (season), stochastic context transitions, hill reveals context only, resource observations noisy/partial, internal timers for homeostatic needs, sparse penalties on depletion.",
            "environment_complexity": "Same discrete grid (100 positions), action set of 5, planning horizons tested 1–9 (main experiments used 9), episodes up to 100 steps; memoization and Monte Carlo rollouts explored to manage search complexity.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "SI learned and improved over trials but more slowly than SL in early trials: Ramp-up slope = 0.107 ± 0.007; EMM at Trial 20 = 24.80 ± 0.08; at Trial 40 EMM = 37.68 ± 0.08 (vs SL 40.78). SI eventually approached SL performance in later trials (phase 3 differences narrowed).",
            "performance_without_adaptation": null,
            "sample_efficiency": "Moderate: slower early learning compared to SL (less value placed on parameter-learning-directed visits to the hill), but steep improvement in Active Learning Phase (Trials 21-60) with slope 0.623 ± 0.006, indicating catch-up over time; converges more slowly (SL reached convergence ~40% faster).",
            "exploration_exploitation_tradeoff": "Balances exploration and exploitation via EFE decomposition: epistemic term drives state uncertainty reduction (state-focused exploration), pragmatic term drives preference satisfaction; lacks an explicit parameter-novelty term so does not target model-parameter learning as explicitly as SL.",
            "comparison_methods": "Compared against SL, BARL, and BARL-UCB under identical environment and search heuristics (memoization/Monte Carlo/hybrid).",
            "key_results": "SI outperformed BARL in the known-environment baseline and showed strong state-directed exploration (often moving to the hill), but it underperformed SL in early parameter-learning-required settings because it lacked simulated parameter updates and backward smoothing; SI had similar asymptotic performance but reached it more slowly than SL.",
            "limitations_or_failures": "Does not explicitly include a novelty/parameter-learning term; hence it undervalues actions that primarily improve model parameters (e.g., revisiting the hill to contextualize a newly discovered resource). Pruning and memoization are necessary for tractability but can bias search; limited Bellman optimality for deep planning without careful pruning.",
            "uuid": "e1135.1",
            "source_info": {
                "paper_title": "Sophisticated Learning: A novel algorithm for active learning during model-based planning",
                "publication_date_yy_mm": "2025-08"
            }
        },
        {
            "name_short": "BARL",
            "name_full": "Bayes-Adaptive Reinforcement Learning",
            "brief_description": "A Bayesian RL approach that augments state-space with beliefs over model parameters (Dirichlet counts), framing planning as an MDP over hyperstates and balancing exploration via Bayesian posterior updates; in experiments implemented with recursive planning over hyperstates and optional UCB bonus.",
            "citation_title": "Bayes-adaptive pomdps",
            "mention_or_use": "use",
            "agent_name": "Bayes-Adaptive Reinforcement Learning (BARL)",
            "agent_description": "Model-based Bayesian RL agent that represents uncertainty over transition/observation parameters via Dirichlet distributions and plans in the augmented belief (hyper-)state space; implementation used recursive tree-search similar to SI, simulated concentration-parameter updates during planning, and online sequential updates upon real observations.",
            "adaptive_design_method": "Bayesian active learning / belief-state planning; optionally augmented with Upper Confidence Bound (UCB) heuristic for directed exploration",
            "adaptation_strategy_description": "BARL adapts by maintaining posterior beliefs over model parameters (Dirichlet counts) and planning in belief-space (hyperstates) to maximize expected cumulative reward; in the implemented planner, concentration-parameter updates are simulated within forward search but those simulated updates are not carried forward across real time-steps (actual updates occur on real observations). When UCB is added, a bonus reward term reward + c * ln(t) / N_t(a) drives exploration of rarely visited actions/states.",
            "environment_name": "10×10 Seasonal Foraging Grid-World (partially observable context/season)",
            "environment_characteristics": "Same POMDP: partially observable, stochastic context transitions, sparse penalties, discrete observations and actions, need-driven dynamic preferences.",
            "environment_complexity": "Same grid complexity: 100 positions, 5 actions, planning horizon up to 9 used in experiments; trials up to 100 time-steps; belief/hyperstate space large due to Dirichlet count dimensions.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "BARL (no UCB) had modest early performance; Ramp-up slope = 0.113 ± 0.007, EMM at Trial 20 = 26.18 ± 0.08. Overall, BARL underperformed SL (at Trial 40 EMM = 30.31 ± 0.08 vs SL 40.78) and was generally worse than ActInf methods when parameter-learning in POMDP was required.",
            "performance_without_adaptation": null,
            "sample_efficiency": "Lower than SL and SI in the tested environment; initial trials sometimes better than SI due to less hill-prioritization, but overall needed many trials to learn correct mapping; showed poor learning in many configurations without explicit directed exploration.",
            "exploration_exploitation_tradeoff": "Intrinsic exploitation via explicit reward maximization in the belief-augmented MDP; exploration arises from Bayesian uncertainty in the belief state; optional UCB adds explicit count-based exploration bonus that increases visits to rarely-seen states/actions but is not goal-directed with respect to contextual credit assignment.",
            "comparison_methods": "Compared against SL and SI; also tested with an added UCB bonus (BARL-UCB) as an ablation to encourage directed exploration.",
            "key_results": "BARL performed worse than SI and SL in the primary POMDP requiring likelihood learning; BARL sometimes had slightly better very-early trial performance (trials 1-5) but failed to systematically value the hill for contextual disambiguation. Adding UCB slightly improved directed exploration but led to over-exploration of low-epistemic states and did not match SL performance.",
            "limitations_or_failures": "Highly sensitive to initial priors; does not intrinsically prefer context-disambiguating actions (hill) unless those actions immediately increase expected reward; UCB augmentation produced non-goal-directed over-exploration; computational complexity high due to planning over hyperstates; in implemented planner simulated parameter updates during search were not committed across real steps, limiting prospective parameter-learning guidance compared to SL.",
            "uuid": "e1135.2",
            "source_info": {
                "paper_title": "Sophisticated Learning: A novel algorithm for active learning during model-based planning",
                "publication_date_yy_mm": "2025-08"
            }
        },
        {
            "name_short": "BARL-UCB",
            "name_full": "Bayes-Adaptive Reinforcement Learning with UCB",
            "brief_description": "A variant of BARL augmented with a count-based Upper Confidence Bound exploration bonus added to the reward function to encourage directed exploration of under-visited states/actions.",
            "citation_title": "Bayes-adaptive pomdps",
            "mention_or_use": "use",
            "agent_name": "BARL + UCB (BARL-UCB)",
            "agent_description": "Same Bayes-adaptive RL architecture as BARL but with an additional UCB-style bonus in the reward calculation: reward + c * ln(t) / N_t(a), where N_t(a) is visitation count and c a tunable constant; aims to encourage exploration of low-visit state-action pairs.",
            "adaptive_design_method": "Count-based directed exploration (UCB) combined with Bayesian belief updates",
            "adaptation_strategy_description": "Adapts by maintaining Dirichlet belief counts and selecting actions via belief-space planning augmented by an exploration bonus that favors rarely-visited actions/states; exploration bias is frequency-based rather than parameter-novelty or context-credit-driven.",
            "environment_name": "10×10 Seasonal Foraging Grid-World (partially observable context/season)",
            "environment_characteristics": "Same POMDP structure as for BARL and SL.",
            "environment_complexity": "Same as BARL; experiments report EMM and slopes analogous to other methods.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "BARL-UCB had the slowest early learning slope (Ramp-up slope = 0.050 ± 0.007) and lower EMMs (Trial 20 EMM = 24.85 ± 0.08; Trial 40 EMM = 27.61 ± 0.08). UCB increased exploration but often wasted samples on low-epistemic states and underperformed SL and SI overall.",
            "performance_without_adaptation": null,
            "sample_efficiency": "Lower sample-efficiency due to over-exploration of low-value states; required more trials to improve than SL and SI.",
            "exploration_exploitation_tradeoff": "Explicit count-based exploration bonus increases exploration early, but lacks goal-directed contextual credit assignment (doesn't prioritize hill after resource discovery), leading to inefficient sample usage.",
            "comparison_methods": "Compared directly to SL, SI, and BARL (no-UCB).",
            "key_results": "UCB augmentation increased visits to rarely-seen states but was not targeted to contextual disambiguation, resulting in over-exploration and worse survival/learning performance than SL and SI; BARL-UCB had the lowest ramp-up slope and poorer mid-phase performance.",
            "limitations_or_failures": "Exploration driven by visitation counts rather than parameter-novelty or contextual value led to inefficient sampling in sparse/structured environments; failed to prioritize states (hill) that enable retrospective credit assignment, hurting parameter learning.",
            "uuid": "e1135.3",
            "source_info": {
                "paper_title": "Sophisticated Learning: A novel algorithm for active learning during model-based planning",
                "publication_date_yy_mm": "2025-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Sophisticated inference",
            "rating": 2,
            "sanitized_title": "sophisticated_inference"
        },
        {
            "paper_title": "Bayes-adaptive pomdps",
            "rating": 2,
            "sanitized_title": "bayesadaptive_pomdps"
        },
        {
            "paper_title": "Bayesian reinforcement learning: A survey",
            "rating": 1,
            "sanitized_title": "bayesian_reinforcement_learning_a_survey"
        },
        {
            "paper_title": "Active learning literature survey",
            "rating": 1,
            "sanitized_title": "active_learning_literature_survey"
        },
        {
            "paper_title": "Information-based objective functions for active data selection",
            "rating": 1,
            "sanitized_title": "informationbased_objective_functions_for_active_data_selection"
        },
        {
            "paper_title": "Deep active inference agents using monte-carlo methods",
            "rating": 1,
            "sanitized_title": "deep_active_inference_agents_using_montecarlo_methods"
        }
    ],
    "cost": 0.017926249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Sophisticated Learning: A novel algorithm for active learning during model-based planning
August 18, 2025</p>
<p>Rowan Hodson 
Laureate Institute for Brain Research
TulsaOKUSA</p>
<p>John St 
Grimbly 
University of Cape Town
South Africa</p>
<p>Evert A Boonstra 
University of Cape Town
South Africa</p>
<p>Bruce A Bassett 
University of Cape Town
South Africa</p>
<p>MIND Institute
CSAM
University of the Witwatersrand
South Africa</p>
<p>Charel Van Hoof 
Benjamin Rosman 
MIND Institute
CSAM
University of the Witwatersrand
South Africa</p>
<p>Department of Cognitive Robotics
Delft University of Technology</p>
<p>Mark Solms 
University of Cape Town
South Africa</p>
<p>Navid Hakimi 
Laureate Institute for Brain Research
TulsaOKUSA</p>
<p>Jonathan P Shock 
University of Cape Town
South Africa</p>
<p>INRS
MontrealCanada</p>
<p>Ryan Smith 
Laureate Institute for Brain Research
TulsaOKUSA</p>
<p>Sophisticated Learning: A novel algorithm for active learning during model-based planning
August 18, 2025A2FB3469479D917BB6EFE6BD98F7FD95arXiv:2308.08029v2[cs.AI]Preprint submitted to Elsevier
We introduce Sophisticated Learning (SL), a planning-to-learn algorithm that embeds active parameter learning inside the Sophisticated Inference (SI) tree-search framework of Active Inference.Unlike SI -which optimizes beliefs about hidden states -SL also updates beliefs about model parameters within each simulated branch, enabling counterfactual reasoning about how future observations would improve subsequent planning.We compared SL with Bayes-adaptive Reinforcement Learning (BARL) agents as well as with its parent algorithm, SI.Using a biologically inspired seasonal foraging task in which resources shift probabilistically over a 10×10 grid, we designed experiments that forced agents to balance probabilistic reward harvesting against information gathering.In early trials, where rapid learning is vital, SL agents survive, on average, 8.2 % longer than SI and 35% longer than Bayes-adaptive Reinforcement Learning.While both SL and SI showed equal convergence performance, SL reached this convergence 40% faster than SI.Additionally, SL showed robust out-performance of other algorithms in altered environment configurations.Our results show that incorporating active learning into multi-step planning materially improves decision making under radical uncertainty, and reinforces the broader utility of Active Inference for modeling biologically relevant behavior.</p>
<p>Introduction</p>
<p>In both biological and artificial systems, decision-making involves a fundamental trade-off: whether to exploit current behavioral strategies or explore the possibility of better strategies.This dilemma is illustrated by animal foraging paradigms, where the choice between exploiting a current food source and exploring for potentially richer alternatives is critically informed by both environmental cues and past experience (Charnov, 1976, Stephens and Krebs, 1986, Webb et al., 2025).In this context, seeking information to optimize behavioral strategies is an important part of adaptive intelligence, enabling organisms and AI agents alike to reduce uncertainty about their environment.The systematic study of this drive to seek information dates back to early psychological research on curiosity.Berlyne (1966), for instance, introduced distinctions between different forms of curiosity and established the broader concept as a fundamental motivation for knowledge acquisition.His work demonstrated that organisms display an innate drive to resolve uncertainty and gather information, in some cases independently of primary rewards.</p>
<p>There are now many lines of research on this general topic.For example, an emerging body of work has begun to uncover neural mechanisms associated with exploratory behavior, demonstrating how the brain may assign value to information and guide action selection accordingly (Gottlieb et al., 2013, Zajkowski et al., 2017, Chakroun et al., 2020, Tomov et al., 2020, Chou et al., 2024).Current work on reinforcement learning (RL) algorithms has also investigated several approaches to encourage information-seeking, from simple heuristics (e.g., initializing unvisited states at high values) to more sophisticated approaches based on upper confidence bounds (UCB), Thompson sampling, and other Bayesian principles (Jaksch et al., 2010, Houthooft et al., 2016, Bellemare et al., 2016, Pathak et al., 2017, Russo et al., 2018).Building on this work, intrinsic motivation systems have been shown to successfully guide exploration and learning in both artificial and biological agents, particularly during developmental periods (Oudeyer and Smith, 2009).Additional active learning approaches have been surveyed elsewhere (Settles, 2009), each suggesting an agent should optimally be driven to infer and sample from sources of data that would most effectively resolve uncertainty.This is closely related to work on optimal experimental design (MacKay, 1992), which draws on information-theoretic principles for obtaining maximally informative observations.</p>
<p>Active Inference (ActInf) is a more recently proposed framework for modeling decision-making under uncertainty.One differentiating feature of this framework is that the drive to resolve uncertainty emerges as an intrinsic feature of its value function, which is in turn derived from a biologically inspired set of first principles.ActInf shares many key features with other prominent frameworks, such as RL, but also differs in important ways.First, it intrinsically assumes partial observability within Markov decision processes, and employs variational inference approaches via variational free energy minimization to approximate Bayes optimality in state inference.Second, its objective function, expected free energy (EFE; denoted as G in the mathematical formalism), is itself derived from variational principles and naturally leads to both reward-seeking behavior and directed exploration (e.g., favoring choices with the greatest outcome uncertainty).Conceptually, EFE quantifies the anticipated 'surprise' or uncertainty associated with future states and observations, conditioned on preferences and a particular course of action.Minimizing EFE thus drives agents to select actions that are expected to reduce uncertainty about the world (i.e., yield information), while also moving an agent to states that align with its preferences (defined more formally in Section 2.1).As mentioned, a key advantage of this formulation is that exploration emerges naturally from the underlying inference process, rather than requiring further additions to a value function.This allows ActInf agents to efficiently navigate uncertain environments by prioritizing actions that both maximize future goal-achievement (aligned with preferences) and minimize uncertainty over states and model parameters.</p>
<p>In recent years, ActInf has been compared against traditional decision models within benchmark machine learning environments (Friston, 2009, Sajid et al., 2021, Fountas et al., 2020a, Tschantz et al., 2020, Millidge, 2021).Although its performance in these environments has been contextdependent, on the whole it has been largely comparable to other algorithms.This overlap arises in part because the core motivations behind ActInf -maximizing reward and minimizing uncertaintyare conceptually similar to those found in other agent-based machine learning frameworks.In other words, while the implementation differs, particularly in how it unifies epistemic and instrumental imperatives within a single objective function, the underlying drives are not unique.Consistent with this, Sajid et al. (2021) demonstrated that ActInf aligns with Bayesian RL when exploration drives are removed.More generally, RL and other agent-based approaches tend to converge toward similar solutions when placed in partially observable environments that benefit from a combination of epistemic drives and model-based planning.Along these lines, Chou et al. (2025) recently showed that complexity-matched RL and ActInf models explained empirical choice behavior on a 3-armed bandit task with similar accuracy.Yet, Bayesian model selection consistently favored ActInf as the model for which the behavior provided the most evidence.</p>
<p>While promising, certain limitations in current implementations of ActInf have motivated efforts to improve both its performance and scalability.In particular, the computational costs associated with its current variational inference (i.e., message passing) and policy selection approaches would be prohibitive in most real-world applications.This has led to efforts to integrate ActInf with other approaches, including deep learning architectures (Çatal et al., 2020), Monte Carlo Tree Search (MCTS) (Fountas et al., 2020a), and policy gradient methods (Millidge, 2021).</p>
<p>Another limitation is that standard ActInf does not achieve Bellman optimality for policy depths greater than one (Da Costa et al., 2023).To address this, a "Sophisticated Inference" (SI) algorithm was more recently developed.This algorithm is Bellman-optimal and solves multi-step planning tasks through a recursive tree search (Da Costa et al., 2023) 2 .However, SI has not yet been rigorously compared with other algorithms, and there are clear directions for further development, particularly with respect to active learning drives that have been central to the broader framework.</p>
<p>In this paper, we build on previous work to achieve two main objectives.First, we compare SI to other leading algorithms designed to solve similar problems, including Bayes-adaptive RL (BARL) and a representative upper confidence bound (UCB) heuristic (Agrawal, 1995).Second, we introduce and evaluate an extension of SI that incorporates active learning, which we term Sophisticated Learning (SL).</p>
<p>To demonstrate the unique planning processes and advantages offered by SL, we compare its performance against the aforementioned algorithms in a novel, biologically inspired environment designed to support multiple strategies for directed exploration.The results provide novel insights by highlighting the strengths and vulnerabilities of each algorithm.As shown in Section 4, SL significantly outperforms all other algorithms tested, and both SL and SI achieve better results than BARL, with or without the addition of UCB.</p>
<p>Background</p>
<p>In this section, we more thoroughly situate our approach within the broader landscape of prior work.We begin by examining the theoretical foundations of ActInf models, focusing on their relationship to established decision-making frameworks.We then explore SI as a key extension to standard ActInf, and discuss its relation to active learning and BARL.This sets the stage for the novel algorithm we propose (SL), which combines insights from each of these previous approaches.</p>
<p>Formalism and Notation</p>
<p>We start by establishing the common formalism underlying both ActInf and BARL.Each framework operates within partially observable Markov decision processes (POMDPs), where an agent must infer hidden states, update its beliefs, and select actions to optimize its objectives.While these approaches differ in several ways, they share a reliance on generative models that represent environmental dynamics.</p>
<p>POMDPs and Generative Model Structure.In RL, a POMDP provides a formal framework for decision-making under uncertainty, where an agent must infer and reason about hidden states through observations.The framework is traditionally defined as a tuple:
(S, O, U, B, A, D, E, R),(1)
where each element characterizes a specific component of the model.The state space S consists of hidden states s t ∈ S that evolve over discrete time steps.These are the true states of the environment.The agent does not observe s t directly, but infers it from observations.Observations o t ∈ O are drawn from the observation space O according to a likelihood model A, such that
p(o t | s t , A) = Cat(A),(2)
where Cat(A) denotes a categorical distribution parameterized by A, representing the probability of each possible observation given each possible state the agent might find itself in.Actions u t ∈ U are selected from the action space U and influence state transitions according to a transition model B. Transitions are governed by
p(s t | s t−1 , u t−1 , B) = Cat(B(s t−1 ⊗ u t−1 )),(3)
where ⊗ denotes the Kronecker product, used to construct the full transition matrix from stateaction pairs.The model also includes a prior distribution D over initial states, p(s 1 ) = D, and a prior E encoding an initial bias over actions at t = 1, which can influence early decision-making.The reward function R is explicitly defined in conventional RL settings, but this is typically replaced by a preference distribution in the ActInf framework.</p>
<p>In ActInf, this structure is reformulated as a generative model that captures the agent's current beliefs about environmental dynamics.Following Paul et al. (2024), the reward function R is replaced by a preference distribution over observations p(o|C) for a finite time horizon T ∈ N + .Thus, for ActInf, the tuple becomes (S, O, U, B, A, D, E, T, C).</p>
<p>The structure of this generative model leads to the following joint probability:
p(o 1:T , s 1:T , u 1:T ) = p(A)p(B)p(D)p(E) T t=1 p(o t | s t , A) T t=2 p(s t | s t−1 , u t−1 , B),(4)
where priors over the model parameters and initial state are made explicit, and the agent's prior over actions is encoded as p(E).</p>
<p>Belief Updating and Parameter Learning.Given this structure, the agent maintains a belief distribution over states, which is updated recursively as new observations are received.Intuitively, this update combines the likelihood of current observations with predictions based on the previous state estimate.Under a Bayesian framework, this belief update follows:
q(s t ) ∝ p(o t |s t , A) s t−1 p(s t |s t−1 , u t−1 , B)q(s t−1 ).(5)
This approximate posterior distribution over states q(s t ) represents the agent's best estimate of the hidden state given its past experience.Under the mean-field approximation, these posterior beliefs over states follow a more computationally tractable form:
q(s t ) = σ(log p(s t ) + log(o t • As t ))(6)
where σ represents the softmax function.</p>
<p>Beyond state inference, an agent may also need to learn the transition model B and/or observation model A, which are typically treated as latent variables.To do so, the agent maintains and updates two types of beliefs: beliefs over hidden states and beliefs over model parameters.Beliefs over states are represented using categorical distributions of the form q(s t ) = Cat(s t ), while beliefs about the parameters of the observation and transition models are represented using Dirichlet distributions,
q(A) = Dir(a), q(B) = Dir(b),(7)
where the concentration parameters a and b encode the agent's beliefs about each of these models, respectively.Learning then occurs through Bayesian updates to these Dirichlet-distributed parameters.As the agent observes state-action-state transitions (s, a, s ′ ), it updates its beliefs incrementally:
b ′ ijk = b ijk + I[s t = i, a t = j, s t+1 = k],(8)
where b ′ ijk represents the updated count for transitioning from state i to state k under action j.An analogous update rule applies to the parameters of the observation model (a), based on observed outcomes under approximate posteriors over states at each time point.</p>
<p>The formalism presented above establishes how agents can maintain and update structured beliefs about their environment.Bayesian updating in this framework is the foundation for both ActInf and BARL.However, the two frameworks diverge in how these beliefs are used to guide behavior.</p>
<p>ActInf frames decision-making as free energy minimization, selecting actions that minimize expected free energy (EFE).This objective inherently balances goal-directed behavior with informationseeking, unifying exploration and exploitation within a single variational principle.</p>
<p>By contrast, BARL formulates planning as inference in a belief-MDP, where the agent's uncertainty over the environment is treated as part of an augmented state space.Exploration is typically implemented via explicit mechanisms, such as UCB methods, to balance the trade-off between exploration and exploitation.</p>
<p>In the following sections, we examine these approaches in detail.We first explore how ActInf extends variational inference to incorporate future observations and policy selection.We then discuss how BARL constructs and solves belief-space MDPs to handle epistemic uncertainty in environmental dynamics.</p>
<p>Active Inference and Expected Free Energy</p>
<p>ActInf, sometimes referred to as standard or vanilla Active Inference in the literature, proposes that agents in an environment with probabilistic state-observation mappings accomplish perception, learning, and action selection through minimization of two related quantities: Variational Free Energy (F) and Expected Free Energy (G) (Friston et al., 2011(Friston et al., , 2012)).The Variational Free Energy (VFE) is equivalent to the negative Evidence Lower BOund (ELBO) in variational inference:
F = E q(s) ln q(s|o; ϕ) p(s|o; θ) − ln p(o; θ) = −ELBO (9)
Here, the value of F is minimized as the approximate posterior, q(s|o; ϕ), approaches the true posterior, p(s|o; θ), where ϕ and θ parameterize the approximate and true posterior respectively.Thus, the agent can approximate optimal state inference (i.e., perception) by finding the distribution q that minimizes F. Note that F will also be minimized as the marginal likelihood, p(o; θ), approaches a value of one.Thus, the agent can also optimize its model (i.e., learning) by finding the parameters θ that minimize F.</p>
<p>In contrast to perception, action selection in ActInf is guided by minimizing the expected free energy (G), an objective that extends variational free energy to future action sequences (policies; π) that treat observations as random variables (Parr andFriston, 2019, Sajid et al., 2021).While EFE is often presented as a straightforward extension of VFE, recent work suggests that their relationship is more nuanced.Specifically, when considering future states and observations, EFE incorporates terms that capture information-seeking (epistemic) and goal-directed (pragmatic) behavior.A common formulation of EFE is:
G(π) = E q(o,s|π) ln q(s|π) p(s|o) − ln p(o) = −E q(o,s|π) [ln p(o|s, π) − ln q(o|π)] epistemic value ] − E q(o|π) [ln p(o)] pragmatic value . (10)
Intuitively, this formulation balances two key factors: (i) reducing uncertainty about states (epistemic value) and (ii) seeking preferred observations (pragmatic value, encoded in the form of fixed priors over observations; see below).Note that alternative formulations exist, such as the Free Energy of the Expected Future (Millidge et al., 2021), which differs in its specific implications for information-seeking behavior.This underscores the fact that EFE is not a uniquely defined objective, but rather a family of functionals with varying interpretations and computational properties.Recent work also demonstrates that these formulations are not necessarily equivalent (Champion et al., 2024).We focus our review on the standard formulation of EFE.</p>
<p>It is important to note that the first line of Equation 10 is nearly equivalent to F in Equation 9.The difference is that observations have been included within the expectation.Thus, G calculates the variational free energy of expected future observations.In a POMDP, these expected observations depend on future states, and transitions between states are dependent on the selected policy.Thus, the agent selects actions that are expected to transition the environment into states that will yield observations that minimize G.</p>
<p>The decomposition in Equation 10 makes explicit how EFE drives action selection.For ease of exposition, we will start by unpacking the second term in the second line of Equation 10, which is often referred to as the pragmatic term (Smith et al., 2022).As mentioned above, this term drives the agent to seek out the observations that it prefers, or finds most rewarding.This follows from a unique approach to goal-directed choice within ActInf, in which the prior, ln p(o), is used to encode relative preferences (i.e., observations with higher "probability" are treated as more rewarding).To make this more explicit, it is sometimes shown as ln p(o|C), where C parameterizes this fixed set of preferences and is clearly distinct from expected observations under a policy, p(o|π).All else being equal, the agent can thus be thought of as finding a policy that is expected to minimize the difference between its goal (target) distribution and the state-observation pair forecast given its policy.This can be thought of as the agent considering: "Will this policy take me to states that will most likely generate the observations I want to receive?"</p>
<p>The first term in the second line of Equation 10 , the epistemic value, instead quantifies how much an agent expects to learn about states under a given policy.Higher epistemic value corresponds to policies that are expected to lead to greater reductions in uncertainty, naturally leading to exploration.An interesting feature of ActInf is that the term is naturally derived from the free energy formulation.While this resembles directed exploration terms in RL (Mann and Choe, 2013), it does not need to be added separately from the standard value function.Note also that this is more specifically a form of state exploration (Schwartenbeck et al., 2019).In other words, it drives agents to reduce uncertainty about states.This is distinct from active learning, which instead drives an agent to update beliefs about model parameters (sometimes called parameter exploration; discussed further below).This latter form of exploration is more analogous to that used in standard RL (e.g., taking actions to learn about reward probabilities), primarily due to the fact that RL is more often applied in fully observable environments (i.e., MDPs instead of POMDPs).</p>
<p>To support active learning and parameter exploration (when generative model parameters are not known), the EFE can also be extended to consider beliefs about parameters.For example, when applied to parameters defining the likelihood function, θ, this would yield:
G(π) = E q(o,s,θ|π) ln q(s, θ|π) p(s, θ|o) − ln p(o)
= −E q(o,s|π) [ln p(o|s, π) − ln q(o|π)]
epistemic value − E q(o,s|π) [D kl (q(θ|o, s)||q(θ))] novelty − E q(o|π) [ln p(o)] pragmatic value .(11)
Here, a new term emerges, often called novelty, which measures the change in beliefs about model parameters that would result from expected observations under a policy.High novelty indicates that an observation is expected to significantly refine the agent's beliefs about how hidden states in its environment generate observations, thereby driving parameter exploration.In practice, this encourages agents to sample from underexplored parts of the environment, making it functionally similar to intrinsic motivation mechanisms in RL that encourage diverse experience sampling.</p>
<p>As touched upon above, beliefs about parameters in discrete settings are typically represented by Dirichlet distributions, allowing the agent to encode uncertainty via concentration parameter counts, α.This distribution is given by:
p(θ|α) = Dir(θ|α) = 1 B(α) k i=1 θ α i −1 i ,(12)
where B(α) is the multivariate Beta function (which acts as a normalizing constant and is defined using gamma functions).The term θ = (θ 1 , . . ., θ k ) is a vector of parameters (e.g., probabilities of outcomes for a categorical distribution) such that k i=1 θ i = 1 and θ i ≥ 0 for all i.The term k represents the number of possible discrete outcomes or categories for the variable whose probabilities are given by θ.The vector α = (α 1 , . . ., α k ), with each α i &gt; 0, contains the concentration parameters.A higher value of α i relative to other α j indicates greater confidence or more evidence accumulated for the parameter θ i .By updating these concentration parameters with each observation, ActInf agents can dynamically adjust their learning process, balancing the trade-off between consolidating known information about parameters and seeking new experience to increase confidence in their true values.</p>
<p>In summary, the EFE functional drives adaptive behavior by favoring policies that are expected to simultaneously maximize reward (preferred observations) and increase confidence in both states and model parameters.Each of these drives is naturally and dynamically weighted by the magnitude of expected reward and the relative uncertainty about current states and environmental statistics.In practice, these components can also be independently weighted via separate constants to provide additional flexibility in behavior or to better explain sources of individual variability in studies of humans or other animals (Chou et al., 2025).</p>
<p>While ActInf provides a principled framework for adaptive behavior, practical implementations face significant computational challenges.As touched on above, one key issue is the requirement to evaluate entire pre-specified action sequences (policies) in advance, which becomes infeasible due to the combinatorial explosion of possible policies as the planning horizon increases, and possible decision sequences grow.This is further exacerbated by the high computational cost of variational message passing to update beliefs over states in complex environments, and the reliance on handcrafted generative models, which can be difficult to specify for real-world tasks.These scalability issues have motivated various extensions to ActInf, including deep learning-based approximations (Catal et al., 2020), Monte Carlo methods (Fountas et al., 2020b), and policy gradient techniques (Millidge, 2019).A particularly relevant extension is the SI algorithm mentioned above (Friston et al., 2021), which reformulates the EFE objective using recursive tree search to eliminate the need for exhaustive policy enumeration.SI dynamically refines policies by propagating future information back through a hierarchical planning structure, making it a promising approach for scaling ActInf in real-time decision-making.We now explore SI in more depth.</p>
<p>Sophisticated Inference</p>
<p>The SI algorithm extends ActInf to address key scalability challenges in planning by replacing pre-specified sets of possible policies with recursive belief propagation.In other words, unlike standard ActInf, which evaluates all possible action sequences upfront, SI dynamically constructs policies through a tree search process that incrementally propagates and evaluates beliefs about future states and observations.This recursive approach reframes the EFE minimization problem as a Bellman-like equation (Bellman, 1958), explicitly conditioning state inference on actions and observations rather than entire policies.Given an action u t at time step t (omitting model parameter inference for clarity), the recursive formulation of EFE is then:
G(u t , s t ) = E q(o t+1 ,s t+1 |ut,st) ln p(s t+1 | o t+1 , u t , s t ) − ln q(s t+1 | u t , s t ) − E q(o t+1 ) ln p(o t+1 ) + E q(s t+1 ,u t+1 ) G(u t+1 , s t+1 ) . (13)
This decomposition reveals two critical aspects of SI.The first three terms capture the local epistemic and pragmatic value of an action, quantifying expected information gain and expected reward at the current time step.In contrast, the fourth term recursively propagates future EFE across subsequent time steps, allowing the agent to evaluate the long-term consequences of its actions.Because SI iteratively builds a search tree by expanding high-probability belief trajectories, this can be combined with specific pruning mechanisms that make deep planning computationally feasible while maintaining the primary objective function in standard ActInf.</p>
<p>Tree Search and Belief Propagation.As mentioned above, recursive belief propagation in SI specifically constructs a tree-like structure in belief space, rather than state space, making it particularly well-suited for partially observable Markov decision processes (POMDPs).In more detail, given an initial belief over hidden states, SI considers each possible action u t and generates hypothetical future observations o t+1 under the agent's generative model.Each resulting belief state, q(s t+1 | u t , o t+1 ), is evaluated recursively via Eq.13, forming a branching structure.</p>
<p>To manage computational complexity, SI applies two key pruning mechanisms.First, branches are pruned if the prior probability of transitioning to a future belief state falls below a predefined threshold (e.g., p = 0.16 in the original formulation), ensuring that low-probability trajectories do not consume resources.Second, branches whose EFE is higher than alternatives (i.e.relatively less valuable) by a predefined threshold are discarded early in the search process, reducing the need to evaluate suboptimal paths in full.By iteratively pruning uninformative or suboptimal action sequences in this way, SI avoids exhaustive policy enumeration while still capturing long-horizon dependencies.This allows the agent to selectively explore policies that are likely to yield high epistemic or pragmatic value.These pruning mechanisms are not unique to SI and have been applied as solutions in both normal ActInf and other algorithms.However, combined with the recursive tree search approach, they offer useful advantages over the original ActInf formulation.At present, this approach remains largely untested against other similar algorithms.</p>
<p>Interpreted psychologically, SI enables an agent to engage in hierarchical counterfactual reasoning about future beliefs and observations.The agent implicitly considers the following sequence:</p>
<p>Given my current beliefs over hidden states, if I took action u t and transitioned to state s t+1 , what possible observations o t+1 might I receive?How would this update my beliefs about the true hidden state?Given this updated belief, what future observations would I expect if I then took action u t+1 , and how well would those observations align with my preferences and reduce my uncertainty?</p>
<p>This iterative belief update process appears to capture the phenomenology of mental simulation and prospective planning, where decisions are evaluated based on imagined future consequences at different time points in the future.</p>
<p>Figure 1 illustrates this process: each branch corresponds to a candidate action sequence, while belief propagation refines the agent's expectations about future states and observations.</p>
<p>Other Extensions</p>
<p>Of note, recent research has also explored various extensions to both standard ActInf and SI.For instance, Paul et al. (2023) have proposed the application of dynamic programming techniques to the EFE functional to improve computational efficiency.They also investigate how agents can learn dense preferences over states -representing desirability -by applying Z-learning (Todorov, 2006) to a sparse target distribution.This approach enables agents to develop adaptive goal representations beyond predefined reward structures.It also allows the agent to adopt a hierarchical planning perspective, where state preferences emerge dynamically rather than being explicitly assigned.Conceptually, this aligns with intuitive heuristics such as:</p>
<p>"This state will bring me closer to my goal; therefore, I generally prefer this state over the previous one."</p>
<p>However, learning preferences, as opposed to action-based value functions, remains an underexplored area in ActInf, offering new possibilities for adaptive and efficient decision-making.</p>
<p>Bayes-Adaptive Reinforcement Learning</p>
<p>While ActInf frames decision-making through the lens of EFE minimization, an alternative Bayesian approach to handling uncertainty in POMDPs has been described within RL.Specifically, the Bayes-Adaptive Reinforcement Learning (BARL) framework directly extends the classic RL approach by incorporating explicit Bayesian reasoning about model uncertainty, treating the agent's beliefs about environmental dynamics as part of an augmented state space.</p>
<p>Building on the formalism established in Section 2.1, BARL also offers a principled approach to exploration by maintaining and updating beliefs over model parameters.Unlike ActInf, which derives information-seeking behavior from EFE minimization, BARL constructs an expanded MDP in belief space, allowing standard optimization techniques to naturally balance exploration and exploitation.This approach has proven particularly effective in scenarios where agents must learn environmental dynamics while maximizing expected rewards (Ross et al., 2007).</p>
<p>Theoretical Foundations.The BARL approach is situated within the broader field of Bayesian machine learning.To date, considerable work has been carried out in this field, leading to several effective methods for incorporating prior information when performing inference over unknown variables (Ghavamzadeh et al., 2015).These methods are often applied to problems involving uncertainty, where new information is combined with prior beliefs to formulate posterior beliefs about one or more unknown factors.Of particular relevance here, these methods have been effective in navigating POMDPs of the same form assumed by ActInf (Poupart and Vlassis, 2008).</p>
<p>BARL either frames POMDPs with respect to uncertainty over the solution-space (modelfree), or uncertainty over the parameter-space (model-based).A significant advantage of framing such problems in a Bayesian framework is that it effectively side-steps the issue of exploration vs. exploitation.This is due to the fact that Bayesian methods have the capability of representing uncertainty over states/parameters/solutions as belief states, which can then be used to identify optimal solutions (Ghavamzadeh et al., 2015).One downside of this approach, however, is its sensitivity to initial priors, which fully determine belief states at the beginning of a task (Guez et al., 2012).Thus, an integral, and often difficult, aspect of BARL is the design and incorporation of effective prior information.</p>
<p>Model-Based Bayes-Adaptive RL.Model-based BARL offers a particularly interesting approach to modeling uncertainty over parameters.For example, suppose the true transition probabilities p(s ′ |s, a, θ) governing the likelihood of reaching state s ′ from state s after taking action a are unknown, due to uncertainty in model parameters θ.In this case, the Bayesian agent will maintain beliefs about the possible values of θ.</p>
<p>Given b ∈ B, where B is the belief space and b(θ) = Pr(θ), the transition model becomes:
p(s ′ |s, b, a) = θ p(s ′ |s, a, θ)b(θ)dθ.(14)
Here, the expectation for θ with respect to belief b has been taken (i.e., maringalised over), and so θ does not appear in the resulting probability density.Thus, the model is effectively known with respect to belief b, and exploration of θ is not necessary.</p>
<p>Beliefs themselves are updated upon receiving data (in this case, data about transitions):
b ′ = b(θ|s, a, s ′ ).(15)
With the model then framed as being known (with respect to b), the problem can be formulated as a Markov Decision Process (MDP), and the Bellman equation can be used to determine the optimal value function for each state-belief pair.
υ * (s, b) = argmax a s ′ ,r p(s ′ , r|s, a, b) r + γυ(s ′ , b s,a,s ′ ) ∀s ∈ S, (16)
where γ is a discount factor ∈ (0, 1] which discounts future rewards relative to present ones.As model-based BARL can be formulated as an MDP in this way, an associated algorithm for any normal model-based RL scheme can be constructed.Note that, when navigating a fully observable MDP, an agent can learn by registering and storing the number of times it has witnessed specific environmental dynamics.As is implied in equation 15, this could take the form of an agent increasing its belief that some state s will transition to another state s ′ after observing the actual occurrence of this specific transition.Its confidence in this transition belief would then increase the more such observations are made.This increase in confidence about a given transition can be represented by incrementing a count (ϕ s ′ sa ,) associated with the actual belief over parameters, which is implemented with a Dirichlet distribution using these counts as its concentration parameters.However, when operating within a POMDP, the agent does not fully observe the state-space.Thus, in many cases, it has uncertainty as to what transitions between states actually took place.This creates a scenario where learning is difficult, due to the agent often having inaccurate beliefs about the environmental dynamics underlying its observations.To take this uncertainty into account, the BARL approach to POMDPs incorporates the agent's beliefs over model parameters into the hidden state, forming a hyper-state space, S * = S, T, O , with the state-transition and state-observation counts given by ϕ s ′ sa and θ z sa , respectively.Thus, the space of T and O is formally defined as:
T = {ϕ ∈ N |S| 2 |A| , ∀(s, a) ∈ S × A, s ′ ∈S ϕ s ′ sa }, O = {θ ∈ N |S|A|Z| , ∀(s, a) ∈ S × A, z∈Z θ z sa }.
Given the definitions
T sas ′ ϕ = ϕ s ′ sa s̸ =s ′ ϕ s ′ sa and O s ′ az ϕ = θ z sa z ′ ̸ =s ′ ϕ z ′ sa
, the probabilistic dynamics are then:
p(s ′ , ϕ ′ , θ ′ , z|s, ϕ, θ, a) = T sas ′ ϕ O s ′ az θ I {ϕ ′ } (U(ϕ, s, a, s ′ ))I {θ ′ } (U(θ, s ′ , a, z)),(17)
where U is a function that increases the count of ϕ and θ upon the agent receiving new observations.While these equations might appear complex, the concept is simple: Given an initial observation and belief over counts ϕ and θ, the agent can, in theory, compute all (countably infinite) hyperstates conditioned on this initial belief.Thus, the model becomes known with respect to its priors, with O and T updated each time the agent receives new data from the environment.It is important to note that, while this represents belief states within a POMDP in a mathematically precise way, convergence is only assured with respect to the agent's initial priors (Katt et al., 2018).However, despite this limitation, the framework has shown good convergence properties in practice (Ross et al., 2007, Vargo and Cogill, 2015, Katt et al., 2018).</p>
<p>Implementation Considerations.While several choices are possible, the specific BARL algorithm that we consider in simulations below uses online updating, consistent with the approach of Paquet et al. (2005).Specifically, this version of the algorithm processes data sequentially, updating its beliefs and adapting its strategy incrementally as new information becomes available, rather than requiring the entire dataset upfront.The planning structure (search algorithm) is identical to that used in the SI algorithm, with differences appearing only in the way the reward function is constructed.In general, for these recursive algorithms, search exactly equates to a directed value iteration approach over a subset of reachable states from the initial belief state.</p>
<p>Algorithmically, the BARL method considered here also simulates searches over the aforementioned hyper-states, which implicitly contain the agent's uncertainty over model parameters.This means the concentration parameter update is performed at every recursive step of the forward tree search (planner), rather than only after every real time-step.For more detailed pseudocode, refer to Algorithm 3 in the appendix.Importantly, the concentration parameter updates during forward tree search are not carried over to the next real time-step -they exist only within the context of the recursive planning.As with SI, the Bayes-adaptive method also implements pruning for both states and actions.</p>
<p>Comparability of Exploratory Motivations.As mentioned above, information seeking in BARL follows implicitly from the drive to maximize reward.While this effect is also present in SI, the EFE objective within ActInf contains the novelty term as well, which provides a further exploration drive independent of expected reward (i.e., a type of intrinsic curiosity).For greater comparability to SI, BARL can also be supplemented with an explicit directed exploration term.Toward this end, we therefore add a commonly used directed exploration term -an upper confidence bound (UCB) -to BARL in some simulations shown below.Here, UCB takes the form of an algorithmic heuristic that encodes a count over states that an agent has transitioned to up until the current time-point.This can be represented by an expression added to the reward function as follows:
reward + c ln (t) N t (a) . (18)
3. Methods</p>
<p>Sophisticated Learning</p>
<p>We now detail construction of the SL algorithm, which integrates SI with insights from active learning and Bayes-adaptive RL.Recall that SI incorporated the first two terms within the EFE decomposition shown in Section 2.3 within its recursive tree search (i.e., driving state exploration and reward seeking, respectively), but it did not include the third (novelty) term, which serves to motivate parameter exploration.The SL algorithm was specifically constructed to build on SI by incorporating this additional feature, allowing the agent to engage in simulations of potential parameter updates.This specifically allows the agent to reason prospectively about how different future actions are expected to refine its model parameters -thereby improving its ability to learn in dynamic environments.</p>
<p>SL unifies SI and Bayes-adaptive methods, leveraging their respective strengths.As shown below (Section 4), both SI and BARL show comparatively poor performance in scenarios requiring complex, adaptive learning.While SI has not been widely tested in such environments (Friston et al., 2021), it is well documented that BARL approaches to POMDPs are highly dependent on well-specified prior beliefs to facilitate effective learning (Ross et al., 2007, Katt et al., 2018).This limitation often constrains their applicability in highly uncertain, non-stationary settings, consistent with the results presented here.</p>
<p>By propagating parameter updates within the recursive tree search itself, similar to Bayesadaptive methods, SL enables an agent to anticipate how its beliefs will evolve over time, rather than treating them as static.This allows the agent to select actions not only for immediate goal optimization but also to maximize its future learning potential.In effect, SL empowers agents to perform counterfactual reasoning about their own epistemic progress, thereby making proactively self-improving decisions that accelerate model convergence and adaptability.</p>
<p>In more detail, the SL algorithm updates concentration parameter counts after every simulated time step, in the same manner as in BARL.These updated concentration parameters are then propagated forward and used to construct (via normalization) the transition and/or likelihood functions, which are used in subsequent steps of recursive search.The SL algorithm can therefore consider how model parameters would change along its forward tree search if it were to take one action sequence versus another.This is important, as it more adequately represents a simulation of the way an actual real-time trajectory would unfold if the agent were to take a particular set of actions and, in doing so, update its model parameters after every real time step.Note that simulating how states and model parameters change in this way is necessarily based on the agent's prior beliefs over states and model parameters, which can result in incorrect and biased assumptions about the environment.However, such techniques have nonetheless shown good convergence properties (Ross et al., 2007).</p>
<p>In addition to this method of counterfactual search, SL also implements a "backward-smoothing" function -a feature previously suggested (in a more limited scope) in the original presentation of SI (Friston et al., 2021).This backward-smoothing function backtracks from the current evaluated time step to adjust its posterior beliefs over states at previous time steps.This is particularly useful in the case of learning, as it allows for retrospective assignment of observations to posteriors over states, which can result in more accurate updates to the associated Dirichlet concentration parameter counts.Importantly, this backward smoothing function is implemented at each evaluated future time step within the agent's planning horizon, as well as at each real time step.</p>
<p>In summary, there are two key differences between SL and the original SI scheme.The first is the addition of propagating parameter learning through forward-looking simulations.The sec-ond is simulated backward smoothing of parameter learning at every step in this forward search.Psychologically, one could therefore consider an SL agent reasoning as follows:</p>
<p>If I were to take an action, receive an observation, and transition to a new state, how would I then update my posterior beliefs over states for this time-step and for previous time-steps?Based on these posterior updates, how would I then change my current model?</p>
<p>This method of multi-level counterfactual thinking proves particularly beneficial when the agent needs to learn the likelihood function while the state-transition function is known, as described in our primary algorithm comparisons below (Section 3.3.2).</p>
<p>While the principle of backward smoothing to refine posteriors over past states exists in other inference schemes, the distinct advantage of SL lies in its proactive integration of this process within its forward planning.Specifically, the search mechanism within SL evaluates and prioritizes actions not only on immediate outcomes but also on the anticipated information gain about parameters that would be achieved through subsequent backward smoothing.It therefore more highly values trajectories leading to states from which backward inference will yield more precise and informative updates to past beliefs, and consequently to the model parameters themselves.As we demonstrate below, this strategic emphasis on future epistemic refinement via backward smoothing contributes to more accurately revised historical beliefs, which in turn supports robust future decision-making and accelerates learning in uncertain environments.</p>
<p>The Foraging Grid-World Environment</p>
<p>To evaluate the relative performance of SL, SI, and BARL, we designed a challenging grid-world environment to test multi-step planning where strategic exploration is essential for maximizing long-term reward.While other environments have been used to compare ActInf to different machine learning algorithms (Sajid et al., 2021, Millidge, 2021), they often isolate specific behaviors like exploration or model learning.Our environment integrates these demands, requiring agents to dynamically balance exploration, parameter learning, and reward optimization while forecasting probabilistic changes in the world.This design was motivated by common biological challenges: managing distinct and growing needs (e.g., hunger, thirst), avoiding critical survival thresholds, and locating resources whose availability changes over time, necessitating epistemic foraging.</p>
<p>Environment Details and Agent Model</p>
<p>The environment is a 10-by-10 grid containing three non-depleting resources which are nominally labeled food, water, and sleep (see Figure 3).At each time step, an agent can move up, down, left, right, or remain in place.Positional transitions are deterministic and known to the agent.The core challenge posed by this environment lies in its partially observable nature.The locations of the resources depend on hidden context states which change probabilistically over time.For conceptual purposes, we labeled these context states as seasons (i.e., Spring, Summer, Autumn, Winter).The agent could not directly observe season states.However, it could temporarily reveal the current season by visiting a specific cue location we refer to as the Hill state (i.e., as if providing an overview of the environment).However, visiting the hill state did not reveal the resource locations themselves.Thus, the agent was still required to learn the mapping between seasons and resource locations through exploration.This setup created an explicit explore-exploit dilemma in which the agent was required to choose between: 1) exploring new locations to find resources, 2) visiting the hill to reduce uncertainty about the current season, or 3) exploiting current beliefs and moving toward locations with previously observed resources.</p>
<p>The agent's state space was formally defined as:
position × food t × water t × sleep t × context
Here, food t , water t , and sleep t are internal states tracking the time steps since each resource was last acquired.These functioned as homeostatic needs that grew over time, where each resource level was known to the agent with certainty3 .Formally, the agent had two observation modalities.The first pertained to resources within a grid state, with four possible outcomes: Empty, Food, Water, or Sleep.The second modality provided information about the context.Namely, if at the hill state, the agent observed the current context (e.g., Winter), while all other grid locations provided a non-informative No Context observation.</p>
<p>Dynamic Multi-Objective Preferences</p>
<p>As mentioned above, preferences in this environment were not static; they were determined by a dynamic, multi-objective reward function that reflected the agent's current internal resource needs (Algorithm 1).The preference for a given resource increased as the time since its last acquisition grew.If any resource timer exceeded a predefined limit, the agent incurred a large penalty and the trial ended.In some ways, this outcome could be thought of as an agent's 'death' (although, as described below, learning was allowed to carry over between trials for evaluative purposes).This structure, inspired by homeostatic regulation, forced the agent to balance multiple competing objectives to ensure survival, a design that follows classical approaches in reinforcement learning (Sutton and Barto, 2018).The agent's dynamic preference structure was a key feature of this task.Unlike typical ActInf implementations with static or solely time-dependent preferences (Tschantz et al., 2020, Sajid et al., 2021, Friston et al., 2021, Smith et al., 2022), here the agent's preferences were a function of its own policy.Namely, the actions an agent took determined its future internal states, which in turn defined its future preferences.This created a circular dependency in which the agent was required to identify a policy that best satisfied the very preferences induced by that policy.</p>
<p>Illustrative Task Example</p>
<p>The design of this environment enabled nuanced, non-trivial strategies for uncertainty reduction.While much of the extant work on epistemic behavior has focused on bandit tasks (Averbeck, 2015, Marković et al., 2021), our environment instead allowed for long-term sequential planning.For example, an agent could infer the current context in two distinct ways: directly, by visiting the hill, or indirectly, by visiting a location where a resource was known to exist in a specific context.Observing the resource confirmed the context, while its absence implied the context had changed.</p>
<p>To illustrate, consider the scenario in Figure 5.In this example, the transition probabilities between seasons were known, but the mappings between grid locations and resources in each season (i.e., the likelihood function) needed to be learned.At t = 0, this simulation assumed the agent was at the hill state and observed the context was Winter.It also assumed the agent had previously learned through experience that Food was likely at grid position 2 in Winter.The agent therefore moves for two time steps to reach position 2.</p>
<p>If the probability that the season would remain Winter was 0.95 per time step, the probability of it still being Winter upon arrival would be 0.95 × 0.95 = 0.9025.Thus, the agent remains fairly confident the season has remained stable.However, when the agent arrives at position 2, it finds that food is absent.This allows the agent to confidently infer the season had changed.As the agent knows the transition probabilities between seasons, it could also reason about the most likely context transition when updating its beliefs (e.g., a single transition to Spring vs. a double transition to Summer).This example highlights how optimal behavior requires an agent to rely on its model of the world to guide belief updating and guide action selection toward exploratory or reward-seeking choices.</p>
<p>Experiment Setup and Details</p>
<p>For our primary simulations below, the hill state was kept at position 55 (center of the grid) to ensure it was generally within the planning horizon (search-depth) of the agent from all points in the grid.Resource locations were also chosen heuristically, fixed within each season to ensure points of interest were mostly within a reasonable search depth of each other and could effectively contribute to learning.Specifically, depending on the season, Food, Water, and Sleep, were respectively placed in the following positions: Spring = 71, 73, 64; Summer = 43, 33, 44, Autumn = 57, 48, 49, and Winter = 78, 67, 59.Seasonal context transitions remained stable with a 0.95 probability, or transitioned to the adjacent context with a 0.05 probability.The initial context was uniformly sampled at the start of each trial with the agent having a uniform belief over contexts at the first time step.Note that, while we focus on the specific configuration described here for detailed illustration, each algorithm was also tested on several other configurations (i.e., choice of resource locations) to ensure the generalizability of our results.The results of these further confirmatory analyses are provided in appendix Section 6.3.</p>
<p>Each trial began with the agent at a fixed initial position (state 51).Survival thresholds for food, water, and sleep were set at 22, 20, and 25 time steps, respectively, below which the agent would die (ending the trial).These time-step limits were chosen heuristically to allow the agent sufficient time to learn the model while also mimicking the fact that different resources deplete at different rates in true biological organisms.These limits also prevented selection of behaviors that, while intelligent, were problematic for the questions we aimed to answer (e.g., if too long, the agent would simply wait in one location the entire time until the season would return where a known resource was present).The preference structure assigned values to observations based on these resource timers, scoring empty states at -1 and resource states positively according to elapsed depletion times (f ood t , water t , sleep t ).Preferences uniformly shifted to a large negative penalty (-500) for all observations upon exceeding any resource threshold.</p>
<p>We defined a trial as a single 'run-through' of the agent in the environment.Each trial was terminated either upon resource depletion (agent death) or at a specified maximum number of time steps (100 in the case of our experiments).Several trials were conducted in sequence, where any learning in a given trial was carried over to the start of the next trial.We refer to each of these sequences of trials as an evaluation.Unlike commonly presented implementations in which parameter values are updated after each full trial (Friston et al., 2021), our implementation performed these updates after every time step.This was necessary for the agent to solve the problems posed by this environment.Thus, all algorithms here operate in a dynamic, 'online' manner.</p>
<p>Analysis of Search Heuristic and Horizon Depth</p>
<p>As a baseline characterization of performance, we first analyzed both SI and BARL in a setting where all elements of the environment were known-that is, where the model was equipped with full knowledge of transition probabilities between seasons, resource positions, and the location of each resource in each season.</p>
<p>Note that because model parameters are fixed in this setting and no learning is required, SL collapses to SI and would not offer additional insights if tested in this setting.BARL also collapses to standard Bayesian RL.</p>
<p>For further insights, simulations within a known environment were also carried out under different planning horizons (from 1-9 steps) and with three different tree-search heuristics.This allowed us to identify theoretically optimal depth and search strategies when model learning was not required.Specifically, we evaluated SI under (i) depth-limited recursive search with memoization, (ii) Monte Carlo roll-outs (with random action selection) and (iii) a hybrid scheme that applied recursive tree-search with memoization to the first h steps and Monte Carlo rollouts to the remaining m steps (with h + m = 6).While memoization accelerates inference by caching estimates for previously visited state configurations, it can also sometimes introduce inaccurate cached values.Monte Carlo approaches help to avoid this bias by drawing independent roll-outs (100 in these evaluations) from each leaf node at the expense of greater computational cost.The hybrid approach trades off these properties by reusing exact sub-trees early in the search while relying on unbiased roll-outs deeper in the horizon.Note that this hybrid approach is similar to a partially observable monte-carlo planning approach (Silver and Veness, 2010).</p>
<p>Primary Algorithm Comparisons</p>
<p>After completing the above-mentioned baseline performance characterizations, our primary analyses compared SL to SI and BARL (both with and without UCB).Here, we focused on the case where the likelihood (i.e., the resource positions within each context) needed to be learned and the transition probabilities between seasons were known.The number of time steps an agent survived in each trial, and how this changed across trials in each evaluation, were taken as our primary performance measures.Performance comparisons were initially conducted with 200 sequential trials per evaluation.This length was selected as a computationally reasonable upper bound that allowed for sufficient exploration.These simulations were carried out using a fixed horizon of 9 and a full-depth tree search with memoization.This choice was motivated in part by initial results of the analyses described in the previous section (for results, see 4.1), which indicated that performance continued to improve up to this horizon.We were also primarily interested in comparison between algorithms when minimally constrained by limitations of choice of search strategy.</p>
<p>To provide a generalizable characterization of performance, we carried out 500 evaluations of these trial sequences (with 500 random seeds).Following convergence analyses, which indicated that average performance results typically stabilized around 100 trials, evaluations were reduced to 120 trials.This shorter trial number was chosen as it still captured the core learning dynamics postconvergence while significantly reducing computational demands of simulation.This permitted an increase to 2000 seeds for these more extensive analyses to ensure greater statistical confidence and thorough exploration of the patterns of behavior shown by each algorithm.</p>
<p>To better quantify algorithm performance, we fit Linear Mixed-Effects models (LMEs) using trial, algorithm, and their interaction as predictors of survival time:
Survival ∼ T rial + Algorithm + T rial × Algorithm + (1|Id)
To evaluate early learning dynamics, we ran these LMEs separately for two key trial intervals: a Ramp-up Phase (Trials 1-20) and an Active Learning Phase (Trials 21-60).These models allowed us to estimate both learning rates (slopes) and performance levels (based on estimated marginal means [EMMs]).As an auxiliary feature, to better understand how learning was shaped during experiments, we measured the extent to which SL's model deviated or adhered to the true environment, via KL-divergence analyses.</p>
<p>Additional experiments across varied grid configurations (appendix Section 6.3) were subsequently carried out to more thoroughly compare each of the four algorithms (SL, SI, and BARL with and without UCB).These evaluations were conducted over 200 trials each to maintain consistency with the initial longer runs in our main simulations and provide a comparable basis for assessing performance across different algorithmic approaches.For these multi-algorithm comparisons, 200 seeds per condition were utilized, chosen as a practical balance between computational resources and the need for reliable comparative data across the varied configurations (i.e., varied resource locations by season).</p>
<p>Results</p>
<p>Below we present the results of our two main experiments, along with analysis of major behavioral patterns and underlying mechanisms.</p>
<p>Analysis of Search Heuristic and Horizon Depth</p>
<p>Figure 6 displays the results of simulations across different planning horizons and search heuristics when both the likelihood and transition probabilities were known (i.e., serving as an assessment of each algorithm's maximum performance level).Findings indicated that the non-memoization approach outperformed the memoization approach at a horizon of 5 and above, as it avoided the use of potentially inaccurate cached values, albeit at a substantially higher computational cost (approximately 28 times greater than that of the memoization condition at horizon 5).The hybrid search method showed better performance at shorter horizons.However, its relatively inefficient sample usage rendered it less computationally feasible.</p>
<p>One interesting observation was that BARL showed better performance than SI, with the most notable differences occurring in earlier trials.This was most likely driven by SI's use of the epistemic value term within EFE, which encouraged moving to the hill more frequently.While this can be beneficial during learning, it may detract from reward maximization behavior when the environment is fully known (as in these simulations).</p>
<p>Relative Performance under Model Uncertainty</p>
<p>Figure 7 shows average survival curves across 120 trials in our main simulations where the likelihood model needed to be learned.These results highlighted clear differences in learning trajectories between algorithms.Most notably, SL improved in performance more quickly than each of the other algorithms and maintained a slight but consistent advantage in later trials.</p>
<p>Phase 1: Ramp-up (Trials 1-20)</p>
<p>During the initial 20 trial phase (Figure 8), the LME model revealed significant main effects of Trial (F (1, 157993) = 280.15,p &lt; .0001)and Algorithm (F (3, 157993) = 122.01,p &lt; .0001),as well as a robust Trial × Algorithm interaction (F (3, 157993) = 186.59,p &lt; .0001),indicating differential early learning rates.</p>
<p>Post-hoc contrasts (Table 2) confirmed these results were accounted for by faster learning in SL than in all other algorithms.Namely, the slope for SL (0.266 ± 0.007 SE) exceeded that of SI (0.107 ± 0.007), BARL (0.113 ± 0.007), and BARL-UCB (0.050 ± 0.007), with all pairwise comparisons significant at p &lt; .0001.</p>
<p>By the end of this phase (Table 3), SL had also already established a lead in average survival (EMM = 27.42 ± 0.08), outperforming SI (24.80 ± 0.08), BARL (26.18 ± 0.08), and BARL-UCB (24.85 ± 0.08), with all differences significant (p &lt; .0001).</p>
<p>One other notable observation was that BARL algorithms performed slightly better in initial trials, particularly in trials 1-5.This was most likely due to the fact that SI/SL algorithms prioritized the hill state to a greater degree.In earlier trials, while this benefited learning (and thus led to better performance in later trials) it was not necessarily better for survival when starting with no prior knowledge about the environment.Phase 2: Active Learning (Trials 21-60)</p>
<p>In the subsequent phase (Figure 9), the Trial × Algorithm interaction remained highly significant (F (3, 317993) = 2183.01,p &lt; .0001),reflecting continued divergence in learning trajectories.</p>
<p>Both SL and SI displayed steep positive slopes, with SI marginally outpacing SL (0.623 ± 0.006 vs. 0.601 ± 0.006; difference = 0.022 ± 0.008, z = 2.72, p = 0.0334).This was likely due to the early success of SL, causing it to have a higher base performance from the start of phase 2 (i.e., the slope for SI reflected the fact that it was catching up to performance levels of SL).However, the difference between these algorithms was small in magnitude, and both far outperformed BARL and BARL-UCB (Table 4).</p>
<p>Despite the slightly higher slope shown by SI in this phase, SL nonetheless maintained and expanded its performance lead.At Trial 40 (midpoint of this phase), the EMM for SL (40.78±0.08)remained significantly above that of SI (37.68 ± 0.08), with a difference of 3.10 ± 0.09 (z = 33.18,p &lt; .0001),confirming the enduring benefit of faster initial learning in SL.</p>
<p>Additional Behavioral Patterns</p>
<p>Single-trial simulations of the two ActInf algorithms (SI and SL) also revealed interesting patterns of behavior and dependence on choice of preference precision.As this precision effectively down-weights exploratory drives in the EFE, we found that it controlled the total number of time steps the agent spent at the hill (i.e., resolving uncertainty).For these single-trial simulations, we also examined cases in which resource locations were known but season transitions were not, as we found they provided additional insights regarding parameter dependence.For example, Figure 13 (C) shows a case in which the preference precision was high (c = 1), the likelihood function was known, but the transition function was unknown.In this case, both SI and SL agents, despite lacking information about the current context, initially ignored the hill and attempted to infer the current season by visiting the resource positions they knew were associated with a particular context (due to having a precise likelihood model).This was because the epistemic term had a proportionally lower impact when compared to the agent's preferences.Thus, these agents' behavior was driven by the drive to meet their multi-objective preferences, rather than to seek information in the form of large posterior updates to their beliefs about hidden states.This is in line with the classic risk-seeking behavior previously described in the ActInf literature (Smith et al., 2022).For the SI algorithm, similar behavior was seen when the epistemic term was omitted, regardless of the preference precision.</p>
<p>Recall that, when full knowledge of the environment (transition and likelihood functions) was available, all algorithms, showed greater similarity in performance.When analyzed at the singletrial level, each would often initially move to the hill before proceeding to a resource location (with SL doing so more frequently due to its additional epistemic drive, as mentioned above).This highlights a core similarity between ActInf and BARL.That is, both approaches are Bayes-optimal with respect to their prior beliefs, meaning that, given an initial belief state and a mechanism to calculate the value of some subset of additional belief states (for example, all belief states reachable from the initial belief state up to some horizon, as is the case in these implementations), each agent will optimally calculate the value of each of these belief states.Given a deterministic and greedy policy construction procedure, an optimal policy will then be chosen that maximizes expected value.The major difference then arises when active learning is required to resolve uncertainty about contingencies within the environment.</p>
<p>Another important consideration is that the accuracy with which each algorithm calculates the value of belief states is entirely predicated on the initial belief state.Thus, if the initial belief state is inaccurate, the calculation and evaluation of subsequent belief states will also be inaccurate.Therefore, in simulations where the transition model was known, but the initial context was unknown, the agent knew that transitions were relatively static (95% chance to remain in the same context and 5% chance to transition to the next context) and so often viewed visiting the hill as optimal -as it was the state that would most precisely update its belief about the current season.Due to the nature of the counterfactual trajectory planning these agents implement, they search through all possible belief trajectories up to the set planning horizon, and thus calculate, ahead of time, the optimal set of subsequent actions for whatever observation the hill state provides.Planning trajectories then calculate that the hill will provide precise context information, and for each observation the hill could provide, the optimal trajectory following on from that time point is calculated.These belief trajectories thus have high precision compared to those that do not include the hill.</p>
<p>As briefly mentioned above, some initial exploratory analyses also showed interesting ways in which behavioral patterns were influenced by time limits to reach each resource.For example, if one increased these time limits compared to the main simulations shown above (i.e., to 30 time steps without reaching a resource), all agents would initially ignore the hill and simply guess at the context.This was because the agent did not believe it would incur the penalty of reaching the time limit.Thus, it would lose little by guessing at contexts, even if its guesses were wrong.In these scenarios, the agent would often initially move toward resources based on a guess about the context, and only move to the hill if it believed that subsequent guessing would have a higher chance of incurring death.Mathematically, this is due to the agent precisely following the actions it believed would yield the largest return in the expectation, as is the case with all Bayes-optimal algorithms.</p>
<p>In combination, the analyses above highlight ways in which fixed parameters (i.e., preference precision, initial belief states, expected resource time limits, planning horizon) influence decisionmaking in specific ways.This opens up the possibility of using such models in future studies to capture (and mechanistically explain) individual differences in human cognition and behavior, and potentially their biological basis.This therefore represents one important research direction going forward.</p>
<p>Summary</p>
<p>The experimental results presented in this section provide clear insights into the comparative performance and adaptability of SL, SI, and BARL under different levels of environmental uncer-tainty.SL consistently outperformed SI and BARL across varying conditions in our novel testing environment, particularly in versions requiring long-term planning that took information value into consideration.</p>
<p>The inclusion of a UCB-style exploration bonus in BARL improved its adaptability, but remained less effective than the intrinsic epistemic and novelty-driven exploration shown by SL.Namely, while the UCB term enabled more directed likelihood learning, it did not fully replicate the structured, hierarchical search mechanisms and state exploration drive inherent in ActInf-based methods.</p>
<p>Tree-search depth and memoization significantly impacted performance trade-offs, particularly for SI.While deeper tree search improved long-term planning, computational costs increase nonlinearly.Memoization appeared to offer a practical solution by caching intermediate search results, but its high memory requirements would need to be carefully managed in large-scale applications.</p>
<p>Discussion</p>
<p>This study aimed to (1) compare the performance of Sophisticated Inference (SI) with Bayesadaptive reinforcement learning methods (BARL), and (2) introduce and evaluate Sophisticated Learning (SL), an extension of SI that integrates active learning into recursive planning.Our simulations, conducted in a novel, biologically inspired grid-world task, provide key insights into the behavior and comparative strengths and weaknesses of these algorithms.</p>
<p>Key Findings and General Contribution</p>
<p>SL outperformed SI and BARL (with and without an upper confidence bound [UCB] heuristic promoting directed exploration) across all simulations where model-learning was required.Here, performance was measured by the number of time steps survived per trial, which inherently relies on an agent's ability to learn an accurate model.Trial-by-trial variance was observed due to the inherent difficulty of the task, but on average, SL demonstrated superior performance.This reflected its novel ability to strategically revisit states based on anticipated future observations, balancing exploration and exploitation over multiple future time steps.</p>
<p>Unlike BARL algorithms, which focus on maximizing expected cumulative rewards, SL leverages expected information gain to guide behavior.In particular, the SL agent used forward-looking strategies, simulating how future observations would update its beliefs about earlier states and state-outcome mappings.A notable pattern emerged: upon discovering a resource, the SL agent often revisited a state that disambiguated the current context (the hill).This behavior exemplifies the ability of SL to link observations across time to improve its contextual understanding, a feature absent from the other algorithms.</p>
<p>Putting a psychological gloss on this mechanism, an agent employing this algorithm might engage in the following thought process:</p>
<p>I have now discovered a state where a food resource is located.I am unsure of what season I am in at this point, but if I were to move from here and visit the hill state, it would tell me what season I am in.Then, given my transition model, I would be able to work backwards and retrospectively figure out what season I was most likely in when I was at the position of the food.Although not maximally precise, visiting the hill would allow me to do this with more precision than moving to some other state that would not improve my knowledge of what context I am in.This, in turn, would allow me to assign a context to that specific position of the food, which I can exploit in the future.</p>
<p>This highlights the ability of SL to anticipate how future observations will update posterior beliefs about the past, thereby optimizing exploration toward states that improve contextual understanding.In this way, SL offers a more strategic, nuanced form of directed exploration, focusing not just on visiting new states, but on those expected to improve current beliefs about past rewards.</p>
<p>Exploration Strategies: SI vs. SL vs. RL</p>
<p>In line with the description above, the advantages of SL over SI therefore appear to be due to its capacity for backward counterfactual reasoning -anticipating the benefit of future observations in refining past beliefs.While SI displayed strong performance through a more classic form of directed exploration (e.g., seeking unvisited states), it lacked the ability to leverage beliefs about how future observations could be strategically used to update contextual understanding of previous observations.</p>
<p>While the performance differences between SL and SI were relatively small, the common elements of their exploratory strategies led to much larger performance advantages over BARL.Adding UCB-based directed exploration to BARL also did not improve its relative performance.Instead, it led to over-exploration of states with low epistemic affordances.This ultimately reduced efficiency, as the hill state was not weighted any differently in its epistemic evaluation than any other unvisited state.These findings highlight the difference between intrinsic curiosity about state-outcome mappings in UCB, the further curiosity about current contextual states in SI, and the strategic, goal-directed exploration shown by SL.</p>
<p>Mechanisms of suboptimal performance</p>
<p>Despite the high performance of SL relative to comparison agents, it nonetheless frequently failed to converge onto optimal policies, leading to high variance in performance across trial sequences.Understanding these failures highlighted core challenges in learning under uncertainty, but also illuminated issues that may be less general and contingent on the specific environment and implementation being considered.</p>
<p>In some of our supplementary analyses ( 14), a common failure mode we observed arose from early epistemic commitments in SL.In particular, while SL agents use recursive planning to project belief updates forward, this mechanism is only as reliable as the evidence received.During early learning, incorrect associations between contexts and resource locations (e.g., from low-probability observations) can become entrenched, as Dirichlet counts accumulate in favor of incorrect likelihoods.Once an incorrect model is strongly reinforced, the agent tends to commit to poor policies -such as moving toward an expected resource that is, in fact, absent.Because the agent's own model can be misleading in such cases, and survival windows are limited, these trajectories often preclude the opportunity to learn a more accurate model.This form of self-confirming bias is particularly problematic in sparse-reward or high-penalty environments, such as the one tested here.</p>
<p>This effect is visible in appendix Figure 14, where belief distributions in Season 3 diverge over time from the true Sleep resource locations.Rather than improving, the model degrades due to a reinforcing loop between inaccurate inference and maladaptive behavior.Notably, this issue is not due to insufficient planning depth, but rather to incorrect parameter learning that remains uncorrected.</p>
<p>Another interesting aspect of this stems from the backward smoothing mechanism used within SL (see Section 3.1 and Appendix 6.1.3).This mechanism is intended to revise posterior beliefs over states from earlier time steps in light of new observations.In principle, this should allow agents to forget and/or correct past inferences and improve parameter learning, even after delayed evidence is received.However, the divergence patterns shown in Figure 14 suggest that these mechanisms are not always sufficient.Once strong -but incorrect -beliefs are established, even recursive smoothing may be unable to dislodge them, particularly in regimes with ambiguous feedback.</p>
<p>However, it should be noted that this failure mode is not intrinsic to SL in particular, but rather reflects the interaction between priors, environmental structure, and chosen hyperparameters (e.g., learning rate, planning depth, initial Dirichlet counts) that can be present in any Bayesian agent.The agent's initial uncertainty, the rate at which beliefs are updated, and the asymmetry of risks across contexts all shape learning trajectories.SL, like any Bayesian learner, is sensitive to its initial conditions.Thus, these observed failures -while instructive -should not be over-interpreted as a major limitations of the SL algorithm per se.</p>
<p>Indeed, when the agent is provided with a correct generative model (Section 4.1), performance improves markedly, confirming that accurate beliefs are the primary bottleneck to adaptive be-havior.Additionally, the KL-divergence measures of learning in each context shown in 14 revealed that certain resources or seasons are harder to learn, most likely due to their statistical properties or positional inaccessibility.</p>
<p>Future work could explore mechanisms for meta-inference -enabling the agent to represent and revise its confidence in its own beliefs -or explore the use of other forgetting strategies, such as 'belief resets' upon persistent mismatch between predicted and real observations.</p>
<p>Limitations and Future Directions</p>
<p>While SL demonstrates clear advantages in this specific environment, several caveats should be considered.First, the grid-world environment was chosen to test the expected advantages of SL; thus, future research will be needed to determine the extent to which the benefits of SL generalize to other environments.Here, we would expect SL to excel in tasks requiring deep planning and strategic exploration, but its relative performance across a wide variety of more traditional RL-style benchmarks remains uncertain.</p>
<p>Another consideration involves the optimization of parameter values.For example, the optimal value for preference precision in SL may vary across tasks.Some tuning is likely to be required to balance the epistemic and novelty terms in the EFE for different problems.This sensitivity to parameterization will be an important practical factor when applying SL in diverse environments.</p>
<p>A further limitation concerns computational efficiency.Like other ActInf algorithms, SL relies on recursive tree search, which can become computationally expensive in real-world environments.Scaling SL to such domains will likely require the integration of other heuristic approaches, more efficient pruning techniques, or other machine learning approximations.Future research should therefore focus on developing methods to enhance the scalability of SL while maintaining its strategic advantages.</p>
<p>Conclusion</p>
<p>In this study, we used a challenging, dynamic environment requiring complex planning and strategic information-seeking to compare Active Inference and Bayesian reinforcement learning algorithms.We first showed that a recent 'Sophisticated Inference' algorithm within the Active Inference framework outperformed Bayesian reinforcement learning in this environment (both with and without the addition of a common directed exploration term).Second, we proposed and tested a novel 'Sophisticated Learning' algorithm-combining insights from Sophisticated Inference and Bayesian Reinforcement Learning-and demonstrated further advantages it may offer.This algorithm demonstrated greater performance than either of the other algorithms tested.It also exhibited qualitatively distinct, strategic patterns of behavior in which it gathered information to improve its understanding of past observations.The associated backward reasoning strategies employed by Sophisticated Learning represent a novel advance in simulating intelligent agent behavior.</p>
<p>These promising results suggest that Sophisticated Learning may offer new insights into both machine learning and cognitive science.Future work should assess the generalizability of the strategies that emerge from this algorithm in other machine learning contexts and investigate whether it might capture unique patterns observed in animal and human behavior, contributing to ongoing research in cognitive and computational neuroscience.and potentially for learning a transition model (B) as well (although we do not explicitly test transition learning here).By refining the posterior probability of having been in a particular state s t ′ given observations up to s t (where t &gt; t ′ ), the agent can make more accurate updates to its Dirichlet concentration parameters.This allows observed outcomes to be better assigned to past grid positions within their respective latent contexts, especially in partially observable environments.</p>
<p>Full Sophisticated Learning (SL) Tree Search.The full SL tree search algorithm synergistically combines recursive EFE-based planning in SI with further elements that facilitate active learning.As mentioned above, these elements include simulated updates to Dirichlet concentration parameters (a, b), which represent beliefs about the observation and transition models, respectively, and backward smoothing of posterior state beliefs.Algorithm 5 outlines this process in full.</p>
<p>The integration of 'BackwardSmoothing' and 'UpdateConcentrationParameters' within each recursive step (and the subsequent calculation of a 'W novelty ' term derived from the expected change in model parameters) is what fundamentally distinguishes SL from SI.This allows SL to explicitly plan actions that are expected to yield high 'novelty' -i.e., actions that will maximally reduce uncertainty about the model parameters themselves.This forward simulation of possible learning trajectories enables the agent to make proactively self-improving decisions, going beyond simple state exploration to engage in true parameter exploration.The psychological interpretation of this process is discussed in the main text.</p>
<p>Further Results</p>
<p>Table 2 below displays the mean estimated survival slope fit for each algorithm for the first 20 trials within LMEs.These results re-iterate the significant survival advantage SL had in earlier trials, leading to convergence 40% more quickly than SI.</p>
<p>Robustness to Grid Reconfiguration</p>
<p>While our main analyses (Section 4) compared SL, SI, and BARL on the specific configuration of the environment detailed in Section 3.2, an important consideration remains: how well do these algorithms generalize across changes in this configuration?A potential concern would be if performance differences in one setting might reflect overfitting rather than a generalizable difference between algorithms (Zhou et al., 2022).Since many algorithms are implicitly tuned to a particular structure, evaluating robustness under reconfiguration of resource locations is a simple way to validate our results.As shown in Table 6, results from the 15 grid configurations demonstrate a clear and robust advantage for the SL algorithm, particularly in the initial and middle stages of learning.This overall effect is an average across configurations of varying difficulty; in the most challenging layouts where learning was arduous for both agents, the performance difference was naturally muted (e.g. Figure 12k).However, the data clearly show that where learning was more achievable, the advantages shown for SL became pronounced (e.g. Figure 12e).This is reflected in the aggregate statistics for Phase 1 (trials 1-20), where performance in SL was significantly higher than SI -a finding that was both statistically significant (p&lt;0.001) and reflected a large effect size (Cohen's d=1.39).This early advantage was sustained through Phase 2 (trials 21-60), which also showed a significant difference (p=0.002) with a large effect size (d=1.01).</p>
<p>In later stages of training (Phase 3, trials 61-120), performance differences between the two algorithms narrowed considerably.The difference was no longer statistically significant at the conventional α = 0.05 level, but it remained noteworthy (p = 0.055).This indicated that while SI eventually catches up, SL may maintain a slight edge on average.These findings strongly suggest that the structured, active learning in SL confers a tangible and generalizable advantage in earlyphase adaptation across unfamiliar environments.This is likely due to its intrinsic drive to revisit informative states (e.g., the hill) to rapidly build and refine its model.While both algorithms converge toward similar high-performance levels over extended training, SL consistently exhibited a faster and more efficient adaptation profile, consistent with our main results.</p>
<p>These results are also in line with a broader theoretical argument that planning depth should interact with spatial structure.For example, in a 10 × 10 grid, the expected Manhattan distance between random points is about 6.6 steps:
E[d Manhattan ] = 2 • N 2 − 1 3N ≈ 6.6 when N = 10.
As discussed in Section 4.1, this limits the gains from planning beyond 4-6 steps, and algorithms that adapt within this horizon tend to perform best.SL addresses this constraint effectively; in contrast, SI and BARL lack explicit mechanisms to prioritize strategic information gathering and may therefore lag in early adaptation.</p>
<p>Mechanisms of Differential Performance</p>
<p>Further investigation clarified the mechanisms underlying differential performance between the four algorithms.Note that, in order to accurately learn the likelihood mapping, it is crucial that the agent discover resource positions through random exploration, and then subsequently associate them with the correct context.In this case, the SL agent showed faster improvement over time because, unlike the other three algorithms, it would often immediately prioritize moving to the hill after discovering a resource.This allowed it to better connect the context (season) with the location of the resource (shown in Figure 13 A).SI learned at a somewhat slower rate, as it did not recognize the hill as a valuable state to visit after discovering a resource.While SI did value the hill state in these simulations, it only did so when either its beliefs about context became sufficiently imprecise or when the model was precise and the hill became a Bellman-optimal option.Thus, it would not necessarily do so after unexpectedly discovering a resource.Figure 13 B shows example behavior of an SI agent when finding a resource.Unlike SL, this agent does not prioritize moving to the hill.Instead, it continues to explore surrounding states that it has not yet visited.In general, it also visits the hill far less frequently.Learning in this way is relatively slower, as the SI agent, on average, has less precise beliefs about the context in which it has observed a resource.Both BARL algorithms showed very poor learning, as they did not value the hill when their models were imprecise.This is because they did not view the hill as providing a Bellman-optimal mechanism for moving to a resource (as they might if the environment was fully known).The logic here is that, if the agent had imprecise beliefs over which context mapped to which resource position, there was no point in learning what the context was before attempting to move to a resource.In the case of BARL without UCB, learning was primarily chance-based (i.e., when the agent randomly visited a resource location or visited the hill on the way to some other location it viewed as rewarding).The addition of UCB was also unhelpful.To see why, it is first useful to consider that, while UCB provided a directed exploration drive, this was not goal-directed in the manner displayed by SL.That is, while UCB-driven exploration was directed at states for which the agent had made few observation (i.e., and therefore not random), it was not driven like SL by the goal of precise 'credit assignment' of resource locations to a season (i.e., its exploration heuristic did not encourage it to move to the hill to resolve context ambiguity upon discovering a resource).Rather, its exploration was entirely based on relative frequency of states visited over time.In the present environment, which had fairly sparse rewards, this resulted in somewhat wasteful over-exploration of states without resources or epistemic value (regarding the current season).</p>
<p>Model Learning Analysis for SL</p>
<p>Figure 14 shows further analyses to better understand the learning dynamics in SL shown above.These results indicated that, when learning went poorly, it was often context-and modality-specific.For example, in the first season, SL showed accurate learning for all resource locations (based on the KL-divergence between posterior beliefs and ground truth), whereas the second season showed accurate learning for Sleep and Water, but inaccurate learning for Food locations.</p>
<p>Memoization and Computational Efficiency</p>
<p>To mitigate the computational burden of the recursive tree search inherent in SI and SL, we employed memoization.Memoization involves caching and retrieving the EFE (or reward, for BARL) values for previously encountered state-action pairs or sub-trees during the planning process.This technique involves storing previously calculated EFE values of a subset of search trajectories in memory, allowing reuse when future searches overlap with already-calculated subsets.</p>
<p>While memoization significantly reduces the number of redundant computations, especially for deeper searches in environments with overlapping state trajectories (see horizon analysis, Figure 6), it is not without caveats.For ActInf agents, caching EFE values based on a subset of state factors can introduce inaccuracies if path-dependent components of belief (e.g., the history influencing current uncertainty about model parameters, which in turn affects the epistemic/novelty value) are not fully captured in the cache key.Furthermore, a notable drawback to memoization is the significant memory requirement to store these cached values.However, if needed for future applications, this memory burden could be mitigated via various approximation techniques, such as Coarse Coding (Sutton and Barto, 2018), a form of linear function approximation, or Artificial Neural Networks (Abiodun et al., 2018).The choice to use memoization represents a trade-off between computational efficiency, memory usage, and the precision of the EFE approximation.Our use of memoization was a pragmatic choice for managing the extensive simulations required.For the interested reader, below we show the number of recursive function calls made with vs. without memoization from t = 1.Results under different possible search depths are also included.  .This tree search evaluates actions using EFE, selects likely states, and checks short-term memory for past evaluations.If a state has been visited previously, its stored evaluation is retrieved to optimize search efficiency.Otherwise, the search expands recursively until the optimal action is found.The recursion terminates when the planning horizon is reached.The SI agent did not see value in the hill with respect to parameter learning, and so did not frequently revisit this state to contextualize its prior observations.It instead continued to explore the state-space more broadly to seek out novel observations.(C) Supplementary demonstration of parameter dependence: Here we note that, if the precision of the preference distribution was sufficiently high, both SI and SL agents often ignored the hill and immediately attempted to guess where resources might be.This was because very high preference precision values effectively down-weight the other terms in the EFE and thereby deter exploration (see main text for details).</p>
<p>Figure 1 :
1
Figure 1: A depiction of the branching recursive search used in Sophisticated Inference (SI).The tree structure shows how beliefs are propagated through sequences of actions (u) and observations (O), with pruning applied to maintain computational tractability.</p>
<p>Figure 2 :
2
Figure 2: Flowchart representation of the Backward Smoothing Algorithm.The algorithm begins by initializing the posterior belief distribution L and a transition probability accumulator p.It iterates over future time steps t+1 to τ , updating p using the transition function p ← b(action_history(timestep−1))×p.For each state, beliefs are updated according to the likelihood of the observations and transitions as L(state) ← L(state) × observation(timestep) × a × p(state).If there are more time steps to process, the loop continues.Otherwise, the updated belief distribution is normalized and returned as the final smoothed beliefs.Pseudocode is available in the appendix.</p>
<p>Figure 3 :
3
Figure 3: An example 10-by-10 grid world in one of four different contexts (with different colors representing different seasonal contexts) defining resource locations.</p>
<p>Figure 5 :
5
Figure 5: Illustration of an agent's reasoning scenario within a 5 × 5 grid-world.(A) The agent starts at the hill (position 12) at t = 0. (B) The agent moves to position 7 at t = 1.(C) The agent then reaches position 2 at t = 2.At this position, it expects to find food if the context remains Winter.The absence of food allows the agent to infer that a probabilistic transition has occurred between seasons, guided by its model of cyclic seasonal change.</p>
<p>Figure 6 :
6
Figure 6: Analyses testing effects of planning horizon and possible tree-search strategies.A and B show the average number of time steps survived and posterior calculations, respectively, for the Sophisticated Inference algorithm when both likelihood and transition probabilities were known.C and D show similar plots for the Bayesian RL algorithm.Here, 'Memory' indicates the use of memoization caching of previously calculated values, 'No Memory' indicates the absence of any memoization, 'Monte Carlo' indicates the use of Monte Carlo (MC) sampling instead of recursive search, and 'Hybrid' indicates the combined use of recursive search and MC sampling.For the hybrid algorithm, combinations of recursive search + MC roll-outs were tested, each with a combined horizon of 6.In other words, this hybrid plot shows the search depth with memoization from 1 (search depth of 1, MC roll-out of 5) to 6 (search depth of 6, MC roll-out of 0).</p>
<p>Figure 7 :
7
Figure 7: Mean survival per trial across 120 trials by algorithm (shaded bands show 95% CIs, aggregated over up to 2000 seeds).Vertical dashed lines delineate Phase 1 (Trials 1-20), Phase 2 (Trials 21-60), and Phase 3 (Trials 61-120).</p>
<p>Figure 9 :
9
Figure 8: Phase 1: Ramp-up (Trials 1-20).Mean observed survival with 95% CIs.</p>
<p>Figure 11 :
11
Figure11: Tree search logic for Sophisticated Inference (SI).This tree search evaluates actions using EFE, selects likely states, and checks short-term memory for past evaluations.If a state has been visited previously, its stored evaluation is retrieved to optimize search efficiency.Otherwise, the search expands recursively until the optimal action is found.The recursion terminates when the planning horizon is reached.</p>
<p>Figure 13 :
13
Figure13: (A) The SL agent frequently visited the hill within a trial to contextualize the information it learned with greater accuracy (e.g., observing water and then quickly moving to the hill to figure out what the associated context was).(B) The SI agent did not see value in the hill with respect to parameter learning, and so did not frequently revisit this state to contextualize its prior observations.It instead continued to explore the state-space more broadly to seek out novel observations.(C) Supplementary demonstration of parameter dependence: Here we note that, if the precision of the preference distribution was sufficiently high, both SI and SL agents often ignored the hill and immediately attempted to guess where resources might be.This was because very high preference precision values effectively down-weight the other terms in the EFE and thereby deter exploration (see main text for details).</p>
<p>Figure 14 :
14
Figure 14: KL divergence between the SL agent's posterior beliefs and the true resource distributions, plotted over cumulative time steps aggregated across all trials.Each subplot corresponds to one of the four contexts (seasons), with separate curves for each resource modality (Food, Water, Sleep) and the 'No Resource' observation.A declining KL divergence indicates improved learning accuracy.While the agent achieves strong convergence in Season 1, learning remained unstable or incorrect in Seasons 2-4 with respect to some modalities (e.g., Food in Season 2, Sleep in Season 3), suggesting persistent belief miscalibration and context-specific learning failures.</p>
<p>Algorithm 1 Multi-objective reward function function Preferences(resources t , resources L , penalty) empty p ← -1 for each resource in [water, food, shelter] do resources p ← resource t if resource p ≥ resources L then empty p ← penalty resources p ̸ = resource L ← penalty end if end for return [empty p , resources p ] end function</p>
<p>Flowchart for the multi-objective reward function, which sets an agent's preferences at time t.The process starts by setting penalties.It then iterates over all resources (water, food, sleep).For each resource, the algorithm checks if it meets the required threshold.If it does not, a penalty is applied.The process continues to the next resource, and after processing all resources, it returns the final values.
Start: Set penaltiesIterate over resourcesIs resource sufficient?Apply penaltyNext resourceReturn valuesFigure 4:</p>
<p>Table 1 :
1
Estimated Mean Survival (EMM ±SE) at Trial 40 (Midpoint of Active Learning Phase, Trials 21-60).Contrasts are SL vs.Other Algorithms.
AlgorithmEMM Survival SE EMM ∆ EMM (SL -Comparison) SE contrastzp-valueSL40.780.08----SI37.680.083.100.0933.18 &lt; .0001BARL30.310.0810.470.09112.03 &lt; .0001BARL-UCB27.610.0813.170.09140.93 &lt; .0001
Note: EMMs derived from LME model for trials 21-60.SE EMM is the associated standard error.∆ EMM is the estimated difference between SL and the comparison algorithm in question.SE contrast is the standard error of this difference.</p>
<p>Table 2 :
2
Learning Rates (Mean Slope ±SE) in Ramp-up Phase (Trials 1-20).Contrasts are SL vs.Other Algorithms.Note: Estimated marginal trends (slopes) derived from the LME model for trials 1-20.SE slope is the associated standard error.∆ slope is the estimated difference between SL and the algorithm in question.SE contrast is the standard error of this difference.
AlgorithmMean Slope SE slope ∆ slope (SL -Comparison) SE contrastzp-valueSL0.2660.007----SI0.1070.0070.1590.01016.61 &lt; .0001BARL0.1130.0070.1520.01015.93 &lt; .0001BARL-UCB0.0500.0070.2160.01022.58 &lt; .0001</p>
<p>Table 3 :
3
Estimated Mean Survival (EMM ±SE) at Trial 20 (End of Ramp-up Phase).Contrasts are SL vs.Other Algorithms.Note: EMMs derived from the LME model for trials 1-20, evaluated at Trial 20.SE EMM is the association standard error.∆ EMM is the estimated difference between SL and the comparison algorithm in question.SE contrast is the standard error of this difference.
AlgorithmEMM Survival SE EMM ∆ EMM (SL -Comparison) SE contrastzp-valueSL27.420.08----SI24.800.082.620.1124.64 &lt; .0001BARL26.180.081.240.1111.68 &lt; .0001BARL-UCB24.850.082.570.1124.18 &lt; .0001</p>
<p>Table 4 :
4
Learning Rates (Mean Slope ±SE) in Active Learning Phase.Key contrast is SI vs. SL.Note: Displayes slopes (estimated marginal trends) within the LME model for trials 21-60.The contrast for SI is SI-SL.Other contrasts are Algorithm-SL.Thus, a positive value for SI-SL means SI had a steeper slope.SE slope is the standard error for each slope.∆ slope is the estimated difference (Algorithm-SL).SE contrast is the standard error of this difference.
AlgorithmMean Slope SE slope ∆ slope (SL -Comparison) SE contrastzp-valueSL0.6010.006----SI0.6230.0060.0220.0082.720.0334BARL0.1860.006-0.4150.008-51.29 &lt; .0001BARL-UCB0.1180.006-0.4830.008-59.75 &lt; .0001</p>
<p>Table 6 :
6
Paired t-test results comparing mean survival of SL and SI across 15 randomized grid configurations (N=15, df=14).Each condition included 200 agents.Trials 20, 40, and 110 correspond to early, mid, and late points in the experiment, respectively.Phase 1 spans Trials 1-20, Phase 2 spans Trials 21-60, and Phase 3 spans Trials 61-120.Cohen's d quantifies effect size, where 0.2, 0.5, and 0.8 are conventionally interpreted as small, medium, and large effects, respectively.
Comparison SL Mean SI Mean Diff.95% CIt(14) p-value Cohen's dTrial 2023.7123.08+0.63 [0.26, 1.01]3.630.0030.94Trial 4026.2325.04+1.19 [0.23, 2.16]2.650.0190.68Trial 11029.2529.35-0.10 [-0.68, 0.48] -0.360.725-0.09Phase 123.0222.75+0.27 [0.16, 0.38]5.39 &lt;0.0011.39Phase 225.9125.09+0.82 [0.37, 1.28]3.900.0021.01Phase 328.9128.67+0.24 [-0.01, 0.48] 2.090.0550.54</p>
<p>Table 7 :
7
A comparison of the number of recursive function calls with and without memoization.Figure 10: Flowchart representation of the Sophisticated Inference Tree Search algorithm in ActInf.This tree search process balances exploration (information gain about states and parameters) with goal-seeking behavior (reward value) by recursively evaluating actions using EFE.The algorithm begins by checking if the agent has moved in real time.If it has, it updates beliefs via Variational Free Energy (VFE) minimization and then evaluates possible actions based on EFE.If the current time step is within the planning horizon, the system simulates likely future states and checks whether past evaluations exist in short-term memory.If the state has been previously explored, stored evaluations are retrieved to optimize search efficiency.If no prior evaluation exists, the tree search recursively expands, exploring deeper future states until the optimal action is determined.When recursion completes, the best action sequence is returned.If the agent has not moved in real time, actions are evaluated externally, and search continues within the planning horizon.This process ensures the model dynamically balances exploration and exploitation in complex decision-making scenarios.
Search Depth With Memoization Without Memoization01111721253421311784214257160421
SI's Bellman optimality is limited by the pruning necessary for good performance, as noted byPaul et al.  (2023, Figure  1). Pruning terminates the tree search when the action probability falls below a threshold, but this approximation may cause the agent to miss preferred observations deeper in the tree.
One might consider making internal need states uncertain as well, which we note as an interesting possible direction for future work.
AcknowledgmentSJG is supported by funding from the Oppenheimer Memorial Trust.EAB is supported by the Dutch Research Council (NWO), grant number 019.223SG.002.R.S. is supported by the Laureate Institute for Brain Research.Computations were performed, in part, using facilities provided by the University of Cape Town's ICTS High Performance Computing team: hex.uct.ac.za.Travel and additional compute for this work has been funded by Conscium Ltd.Code and Data AvailabilityAll code for reproducing and customizing the simulations reported in this manuscript can be found at: https://github.com/sgrimbly/Sophisticated-Learning.AppendixThis appendix offers more detailed explanations of the algorithms and experimental environment used in this study, as well as additional analyses supporting findings reported in the main text.Detailed Algorithmic SpecificationsIn this section, we elaborate on the core algorithms discussed in the main paper.While the main text provides an overview of Active Inference (ActInf), Sophisticated Inference (SI), Bayes-Adaptive Reinforcement Learning (BARL), and our proposed Sophisticated Learning (SL) algorithm (as discussed in Sections 2.3 and 3.1), the following subsections offer detailed pseudocode.This level of detail is provided for reproducibility and for readers interested in the specific computational steps involved in the recursive tree search and learning mechanisms that were utilized.Sophisticated Inference (SI) Tree SearchThe SI algorithm, introduced in Section 2.3 of the main text, extends ActInf by employing a recursive tree search to evaluate policies based on their Expected Free Energy (EFE).This approach dynamically constructs policies rather than relying on a pre-specified policy space.The EFE calculation, as formulated, balances exploration (information gain about states and parameters) and exploitation (receipt of preferred observations).Algorithm 2 details the associated forward tree search mechanism.The SI search algorithm forms the basis from which SL was developed.Understanding this search process is therefore crucial for appreciating the extensions introduced in SL.In particular, it underlies the way SL incorporates active learning by simulating model parameter updates within the associated recursive structure (see Section 3.1 in the main text).Both algorithms also employ the same pruning mechanisms for tractability.These are implicitly handled within the 'ViableActions' and 'LikelyStates' components of the following pseudocode, which present the SI tree search process in detail.Bayes-Adaptive Reinforcement Learning (BARL) Tree SearchFor comparative analyses, we implemented the BARL method(Ross et al., 2007).The planning structure for this agent also utilized a recursive tree search, analogous to SI.However, BARL aims to maximize an explicit reward function integrated over the belief space of model parameters, rather than minimizing EFE.Algorithm 3 provides the pseudocode.The key distinction here is use of an explicit reward function (based on the time since each resource) with no intrinsic exploration component and direct updating of model parameters (concentration parameters a, b) within each recursive step of the planning phase.In some simulations, an Upper Confidence Bound (UCB) term was also incorporated into the 'Reward' calculation step for BARL to encourage directed exploration by adding a bonus for less visited state-action pairs, using counts N t .The addition of UCB provided a closer comparison to the ActInf algorithms, as performance within the testing environment benefited from their directed exploratory behavior.The following pseudo-code depiction of the BARL algorithm shows UCB included, with N t representing the number of visits to each state up until the current time point.Sophisticated Learning (SL) Tree Search and Backwards SmoothingThe SL algorithm detailed in Section 3.1 of the main text extends SI by integrating active learning into the planning process using insights from BARL.This is achieved by allowing the agent to counterfactually reason about how its model parameters would evolve based on futurehypothetical observations and then refining its understanding of past states through backward smoothing.Backward Smoothing Mechanism.Backward smoothing was conceptually introduced in Section 2.3 and depicted in the flowchart within Figure2.This mechanism allows the agent to retrospectively adjust its posterior beliefs over states at previous time steps within a simulated trajectory, given a sequence of actions and subsequent (hypothetical) observations.Algorithm 4 details this process.To explore this, we evaluated all four algorithms on a set of randomized grid configurations.These maintained the overall problem structure but varied the location of each resource in each season as well as the agent's start position.This allowed us to test whether the performance advantages seen for SL were due to general adaptability or specific features of the original environment.We also systematically varied one key structural parameter: the maximum placement distance of resources from the hill.This distance, which we term the 'Horizon', is defined by the Chebyshev distance (i.e., the maximum number of steps in any one cardinal or diagonal direction).We generated configurations with both a 'Horizon' of 3 and a 'Horizon' of 5.The 'Horizon=5' setting allowed resources to be placed anywhere on the 10x10 grid, while the 'Horizon=3' setting constrained placement to a more local 7x7 area around the hill.Initial exploration revealed that configurations with the larger 'Horizon=5' setting often resulted in less informative learning dynamics, as some resource locations were too distant to be reliably discovered and exploited within the trial period.This aligns with the theoretical expectation that agents with finite planning depths perform better when key locations are within a reasonable proximity.Consequently, to create a more challenging and meaningful test of adaptive learning, we focused our main robustness analysis on a dedicated set of 15 configurations where the 'Horizon' was fixed at 3. This approach ensured that performance differences reflect the efficiency of each algorithm in exploring a structurally coherent, non-trivial environment.Our analysis was based on a complete dataset of 200 simulation seeds for both the SL and SI algorithms across all 15 of these 'Horizon=3' configurations, with each seed being evaluated for 120 trials.Table5details these configurations.Comparison of SI and SL on Selected ConfigurationsTo quantitatively assess the performance difference between the SL and SI algorithms, we conducted paired t-tests on their mean survival rates.This comparison was made at specific trial points and averaged across the three learning phases (Phase 1: trials 1-20, Phase 2: trials 21-60, and Phase 3: trials 61-120).Figure 12: Early-phase survival curves for fifteen randomly sampled grid environments used in analyses of the robustness (generalizability) of observed performance differences between algorithms.Each plot shows the average survival across 60 trials for the SI and SL algorithms, aggregated over 200 random seeds per algorithm.Variability is visualized using standard deviation bands.Although the error bands could be further tightened with more runs, the observed early learning advantage -when interpreted alongside the statistical comparisons presented in the main text -provides robust evidence for the effect.
Stateof-the-art in artificial neural network applications: A survey. O I Abiodun, A Jantan, A E Omolara, K V Dada, N A Mohamed, H Arshad, 20184</p>
<p>Sample mean based index policies by o (log n) regret for the multi-armed bandit problem. R Agrawal, Advances in applied probability. 271995</p>
<p>Theory of choice in bandit, information sampling and foraging tasks. B B Averbeck, PLoS computational biology. 112015. e1004164</p>
<p>Unifying count-based exploration and intrinsic motivation. M Bellemare, S Srinivasan, G Ostrovski, T Schaul, D Saxton, R Munos, Advances in neural information processing systems. 292016</p>
<p>Dynamic programming and stochastic control processes. R Bellman, 10.1016/s0019-9958(58)80003-0Information and Control. 11958</p>
<p>Curiosity and exploration. D E Berlyne, 10.1126/science.153.3731.25arXiv:17196941966153</p>
<p>Learning perception and planning with deep active inference. O Catal, T Verbelen, J Nauta, C De Boom, B Dhoedt, 10.1109/ICASSP40776.2020.9054364ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE2020</p>
<p>O Çatal, S Wauthier, T Verbelen, C De Boom, B Dhoedt, arXiv:2003.03220Deep active inference for autonomous robot navigation. 2020arXiv preprint</p>
<p>Dopaminergic modulation of the exploration/exploitation trade-off in human decision-making. K Chakroun, D Mathar, A Wiehler, F Ganzer, J Peters, 20209e51260</p>
<p>Reframing the expected free energy: Four formulations and a unification. T Champion, H Bowman, D Marković, M Grześ, arXiv:2402.144602024</p>
<p>Optimal foraging, the marginal value theorem. E L Charnov, Theoretical population biology. 91976</p>
<p>A systematic empirical comparison of active inference and reinforcement learning models in accounting for decision-making under uncertainty. K P Chou, N Hakimi, T Y Hsu, R Smith, SSRN 51746692025</p>
<p>The influence of anxiety on exploration: A review of computational modeling studies. K P Chou, R C Wilson, R Smith, Neuroscience &amp; Biobehavioral Reviews. 2024. 105940</p>
<p>Reward maximization through discrete active inference. L Da Costa, N Sajid, T Parr, K Friston, R Smith, 10.1162/neco_a_01574Neural Computation. 352023</p>
<p>Deep active inference agents using montecarlo methods. Z Fountas, N Sajid, P Mediano, K Friston, Advances in neural information processing systems. 332020a</p>
<p>Z Fountas, N Sajid, P A M Mediano, K Friston, 10.48550/arXiv.2006.04176arXiv:2006.04176Deep active inference agents using monte-carlo methods URL. 2020bq-bio</p>
<p>The free-energy principle: a rough guide to the brain?. K Friston, 10.1016/j.tics.2009.04.005Trends in Cognitive Sciences. 132009</p>
<p>Free energy, value, and attractors. Computational and mathematical methods in medicine. K Friston, P Ao, 2012. 2012</p>
<p>. K Friston, L Costa, D Hafner, C Hesp, T Parr, Neural Computation. 332021Sophisticated inference</p>
<p>Action understanding and active inference. K Friston, J Mattout, J Kilner, 10.1007/s00422-011-0424-zBiological Cybernetics. 1042011</p>
<p>Bayesian reinforcement learning: A survey. M Ghavamzadeh, S Mannor, J Pineau, A Tamar, Found. Trends Mach. Learn. 82015</p>
<p>Information seeking, curiosity and attention: Computational and neural mechanisms. J Gottlieb, P Y Oudeyer, M Lopes, A Baranes, 10.1016/j.tics.2013.09.001arXiv:24126129201317</p>
<p>Scalable and efficient bayes-adaptive reinforcement learning based on monte-carlo tree search. A Guez, D Silver, P Dayan, 10.1613/jair.4117Journal of Artificial Intelligence Research. 482012</p>
<p>Near-optimal regret bounds for reinforcement learning. R Houthooft, X Chen, Y Duan, J Schulman, F De Turck, P Abbeel, T Jaksch, R Ortner, P Auer, Advances in neural information processing systems. 292016. 2010J. Mach. Learn. Res.</p>
<p>Bayesian reinforcement learning in factored pomdps. S Katt, F Oliehoek, C Amato, arXiv:1811.056122018arXiv preprint</p>
<p>Information-based objective functions for active data selection. D J C Mackay, 10.1162/neco.1992.4.4.590doi:10.1162/neco.1992.4.4.59019924</p>
<p>Directed exploration in reinforcement learning with transferred knowledge. T A Mann, Y Choe, Proceedings of the Tenth European Workshop on Reinforcement Learning, PMLR. the Tenth European Workshop on Reinforcement Learning, PMLR2013</p>
<p>An empirical evaluation of active inference in multi-armed bandits. D Marković, H Stojić, S Schwöbel, S J Kiebel, Neural Networks. 1442021</p>
<p>B Millidge, 10.48550/arXiv.1907.03876arXiv:1907.03876Deep active inference as variational policy gradients URL. 2019</p>
<p>B Millidge, arXiv:2107.00140Applications of the free energy principle to machine learning and neuroscience. 2021arXiv preprint</p>
<p>Whence the expected free energy. B Millidge, A Tschantz, C Buckley, Neural Computation. 332021</p>
<p>P Y Oudeyer, L B Smith, How Evolution May Work Through Curiosity-Driven Developmental Process. 2009</p>
<p>An online pomdp algorithm for complex multiagent environments. S Paquet, L Tobin, B Chaib-Draa, Proceedings of the fourth international joint conference on Autonomous agents and multiagent systems. the fourth international joint conference on Autonomous agents and multiagent systems2005</p>
<p>Generalised free energy and active inference. T Parr, K Friston, 10.1007/s00422-019-00805-wBiological Cybernetics. 1132019</p>
<p>Curiosity-driven exploration by selfsupervised prediction. D Pathak, P Agrawal, A A Efros, T Darrell, Proceedings of the 34th International Conference on Machine Learning, PMLR. D Precup, Y W Teh, the 34th International Conference on Machine Learning, PMLR2017</p>
<p>On predictive planning and counterfactual learning in active inference. A Paul, T Isomura, A Razi, 2024</p>
<p>On efficient computation in active inference. A Paul, N Sajid, L Da Costa, A Razi, 10.48550/arXiv.2307.00504arXiv:2307.005042023</p>
<p>Model-based bayesian reinforcement learning in partially observable domains. P Poupart, N Vlassis, Proc Int. Symp. on Artificial Intelligence and Mathematics. Int. Symp. on Artificial Intelligence and Mathematics2008</p>
<p>Bayes-adaptive pomdps. S Ross, B Chaib-Draa, J Pineau, Advances in neural information processing. 202007</p>
<p>A tutorial on thompson sampling. D J Russo, B Van Roy, A Kazerouni, I Osband, Z Wen, 10.1561/2200000070doi:10.1561/2200000070Found. Trends Mach. Learn. 112018</p>
<p>Active inference: Demystified and compared. N Sajid, P Ball, T Parr, K Friston, 10.1162/neco_a_01357Neural Comput. 332021</p>
<p>Computational mechanisms of curiosity and goal-directed exploration. P Schwartenbeck, J Passecker, T U Hauser, T H Fitzgerald, M Kronbichler, K J Friston, 10.7554/eLife.41703doi:10.7554/eLife.4170320198e41703</p>
<p>Active learning literature survey. B Settles, 2009</p>
<p>Monte-carlo planning in large pomdps. D Silver, J Veness, Advances in neural information processing systems. 232010</p>
<p>A step-by-step tutorial on active inference and its application to empirical data. R Smith, K J Friston, C J Whyte, 10.1016/j.jmp.2021.102632Journal of Mathematical Psychology. 1072022. 102632</p>
<p>. D W Stephens, J R Krebs, 1986Princeton university press6</p>
<p>. R Sutton, A Barto, 2018Reinforcement learning: An introduction</p>
<p>Linearly-solvable markov decision problems. E Todorov, Advances in Neural Information Processing Systems. MIT Press2006</p>
<p>Dissociable neural correlates of uncertainty underlie different exploration strategies. M S Tomov, V Q Truong, R A Hundia, S J Gershman, Nature communications. 112020. 2371</p>
<p>A Tschantz, B Millidge, A Seth, C Buckley, arXiv:2002.12636Reinforcement learning through active inference. 2020arXiv preprint</p>
<p>Expectation-maximization for bayes-adaptive pomdps. E P Vargo, R Cogill, Journal of the Operational Research Society. 662015</p>
<p>Foraging animals use dynamic bayesian updating to model meta-uncertainty in environment representations. J Webb, P Steffan, B Y Hayden, D Lee, C Kemere, M Mcginley, PLOS Computational Biology. 212025. e1012989</p>
<p>A causal role for right frontopolar cortex in directed, but not random. W K Zajkowski, M Kossut, R C Wilson, 2017e27430</p>
<p>Domain generalization: A survey. K Zhou, Z Liu, Y Qiao, T Xiang, C C Loy, 10.1109/tpami.2022.3195549doi:10.1109/tpami.2022.3195549IEEE Transactions on Pattern Analysis and Machine Intelligence. 2022</p>            </div>
        </div>

    </div>
</body>
</html>
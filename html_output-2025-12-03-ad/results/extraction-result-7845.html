<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7845 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7845</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7845</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-146.html">extraction-schema-146</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <p><strong>Paper ID:</strong> paper-280566467</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.19875v5.pdf" target="_blank">InfiniBench: A Benchmark for Large Multi-Modal Models in Long-Form Movies and TV Shows</a></p>
                <p><strong>Paper Abstract:</strong> Understanding long-form videos, such as movies and TV episodes ranging from tens of minutes to two hours, remains a significant challenge for multi-modal models. Existing benchmarks often fail to test the full range of cognitive skills needed to process these temporally rich and narratively complex inputs. Therefore, we introduce InfiniBench, a comprehensive benchmark designed to evaluate the capabilities of models in long video understanding rigorously. InfiniBench offers:(1) Over 1,000 hours of video content, with an average video length of 53 minutes. (2) The largest set of question-answer pairs for long video comprehension, totaling around 87.7 K. (3) Eight diverse skills that span both grounding-based (e.g., scene transitions, character actions) and reasoning-based (e.g., deep context understanding, multi-event linking). (4) Rich annotation formats, including both multiple-choice and open-ended questions. We conducted an in-depth evaluation across both commercial (GPT-4o, Gemini 2.0 Flash) and most recent open-source vision-language models such as Qwen2.5-VL, InternVL3.0). Results reveal that:(1) Models struggle across the board: Even the best model, GPT-4o, achieves only 47.1 % on grounding-based skills, with most models performing near or just above random chance. (2) Strong reliance on world knowledge: Models achieve surprisingly high scores using only metadata (e.g., video titles), highlighting a tendency to rely on pre-trained knowledge rather than actual visual or temporal understanding. (3) Multi-Modal Importance: When provided with full video and subtitle context, however, models show substantial improvements, confirming the critical role of multimodal input in video understanding. InfiniBench is publicly available at https://vision-cair.github.io/Infinibench</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7845.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7845.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o-mini judge vs. humans</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o-mini as an automated judge compared to human annotators</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper evaluates an LLM-based judge (GPT-4o-mini) scoring open-ended, reasoning-based answers and compares its scores to aggregated human ratings, reporting high linear and rank correlations and agreement statistics that support using the LLM as a proxy for human evaluation in this setting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>InfiniBench: A Benchmark for Large Multi-Modal Models in Long-Form Movies and TV Shows</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Scoring reasoning-based (open-ended) video QA responses</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>InfiniBench (human-evaluated subset: 10% of test set)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4o-mini</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>GPT-4o-mini used as a scoring LLM with a JSON-returning scoring prompt (scale 0–10) and five explicit criteria (factual correctness, relevance, proximity to expected answer, hallucination avoidance, completeness); judge consumes only predicted text and reference answers (not the original multimodal inputs).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Three independent human annotators per sample; averaged human score used as reference</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Pearson r (primary); also reported Spearman rho, Krippendorff's alpha, per-annotator Pearson</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.92</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>High linear agreement between LLM scores and averaged human ratings (Pearson r = 0.92) and high rank agreement (Spearman rho = 0.89); per-annotator Pearson correlations with the LLM ranged 0.88–0.93. The authors conclude the LLM judge 'exhibits near-human-level alignment' and 'aligns with human consensus nearly as well as humans agree among themselves.'</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Validated near-human alignment in this setting, enabling a reliable automated proxy for human scoring (improves evaluation robustness and consistency); authors highlight impartiality due to separation of generation and evaluation inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Evaluation performed on 10% of the test set; each sample rated by 3 human annotators (averaged). LLM judge prompted with a detailed scoring system (Figure 8) returning an integer 0–10 and short justification. Correlations (Pearson, Spearman) computed between LLM scores and averaged human scores; Krippendorff's alpha reported for agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'InfiniBench: A Benchmark for Large Multi-Modal Models in Long-Form Movies and TV Shows', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7845.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7845.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Inter-LLM judge agreement</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pairwise and multi-judge agreement among automated LLM judges (GPT-4o-mini, Gemini 2.0-flash, Qwen2.5-VL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper reports very strong agreement among multiple LLM-based judges when scoring reasoning outputs, via pairwise Pearson and Spearman correlations and an overall Krippendorff's alpha, supporting robustness of automated judging across different LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>InfiniBench: A Benchmark for Large Multi-Modal Models in Long-Form Movies and TV Shows</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Automated scoring of open-ended reasoning answers (comparison between different LLM judges)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>InfiniBench (validation/test splits used for judge comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4o-mini; Gemini 2.0-flash; Qwen2.5-VL (as judges)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Pairwise agreement computed between LLM judges using the same scoring protocol; reported pairwise Pearson and Spearman correlations (examples: GPT-4o-mini vs Gemini 2.0-flash: Pearson 0.97, Spearman 0.94; GPT-4o-mini vs Qwen2.5-VL: Pearson 0.95, Spearman 0.92). An overall Krippendorff's alpha across all judges was reported (0.8014) in an ablation/robustness analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Pairwise Pearson r / Spearman rho; Krippendorff's alpha (multi-judge)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.97</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Very strong inter-LLM agreement (pairwise Pearson up to 0.97, Spearman up to 0.94) and high multi-judge reliability (Krippendorff's alpha ≈ 0.80–0.86 reported in different analyses), indicating that different strong LLM judges produce highly consistent rankings/scores.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Consistency across different LLM judges supports robustness of automated evaluation; demonstrates that multiple LLMs can serve as mutually consistent evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Multi-judge study comparing LLM judges pairwise across validation/test splits; pairwise Pearson and Spearman correlations computed; Krippendorff's alpha reported across judges; some ablation analyses summarized in Table 5/Table 4.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'InfiniBench: A Benchmark for Large Multi-Modal Models in Long-Form Movies and TV Shows', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Judging llm-as-a-judge with mt-bench and chatbot arena <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7845",
    "paper_id": "paper-280566467",
    "extraction_schema_id": "extraction-schema-146",
    "extracted_data": [
        {
            "name_short": "GPT-4o-mini judge vs. humans",
            "name_full": "GPT-4o-mini as an automated judge compared to human annotators",
            "brief_description": "The paper evaluates an LLM-based judge (GPT-4o-mini) scoring open-ended, reasoning-based answers and compares its scores to aggregated human ratings, reporting high linear and rank correlations and agreement statistics that support using the LLM as a proxy for human evaluation in this setting.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "InfiniBench: A Benchmark for Large Multi-Modal Models in Long-Form Movies and TV Shows",
            "evaluation_task": "Scoring reasoning-based (open-ended) video QA responses",
            "dataset_name": "InfiniBench (human-evaluated subset: 10% of test set)",
            "judge_model_name": "GPT-4o-mini",
            "judge_model_details": "GPT-4o-mini used as a scoring LLM with a JSON-returning scoring prompt (scale 0–10) and five explicit criteria (factual correctness, relevance, proximity to expected answer, hallucination avoidance, completeness); judge consumes only predicted text and reference answers (not the original multimodal inputs).",
            "human_evaluator_type": "Three independent human annotators per sample; averaged human score used as reference",
            "agreement_metric": "Pearson r (primary); also reported Spearman rho, Krippendorff's alpha, per-annotator Pearson",
            "agreement_score": 0.92,
            "reported_loss_aspects": null,
            "qualitative_findings": "High linear agreement between LLM scores and averaged human ratings (Pearson r = 0.92) and high rank agreement (Spearman rho = 0.89); per-annotator Pearson correlations with the LLM ranged 0.88–0.93. The authors conclude the LLM judge 'exhibits near-human-level alignment' and 'aligns with human consensus nearly as well as humans agree among themselves.'",
            "advantages_of_llm_judge": "Validated near-human alignment in this setting, enabling a reliable automated proxy for human scoring (improves evaluation robustness and consistency); authors highlight impartiality due to separation of generation and evaluation inputs.",
            "experimental_setting": "Evaluation performed on 10% of the test set; each sample rated by 3 human annotators (averaged). LLM judge prompted with a detailed scoring system (Figure 8) returning an integer 0–10 and short justification. Correlations (Pearson, Spearman) computed between LLM scores and averaged human scores; Krippendorff's alpha reported for agreement.",
            "uuid": "e7845.0",
            "source_info": {
                "paper_title": "InfiniBench: A Benchmark for Large Multi-Modal Models in Long-Form Movies and TV Shows",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Inter-LLM judge agreement",
            "name_full": "Pairwise and multi-judge agreement among automated LLM judges (GPT-4o-mini, Gemini 2.0-flash, Qwen2.5-VL)",
            "brief_description": "The paper reports very strong agreement among multiple LLM-based judges when scoring reasoning outputs, via pairwise Pearson and Spearman correlations and an overall Krippendorff's alpha, supporting robustness of automated judging across different LLMs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "InfiniBench: A Benchmark for Large Multi-Modal Models in Long-Form Movies and TV Shows",
            "evaluation_task": "Automated scoring of open-ended reasoning answers (comparison between different LLM judges)",
            "dataset_name": "InfiniBench (validation/test splits used for judge comparisons)",
            "judge_model_name": "GPT-4o-mini; Gemini 2.0-flash; Qwen2.5-VL (as judges)",
            "judge_model_details": "Pairwise agreement computed between LLM judges using the same scoring protocol; reported pairwise Pearson and Spearman correlations (examples: GPT-4o-mini vs Gemini 2.0-flash: Pearson 0.97, Spearman 0.94; GPT-4o-mini vs Qwen2.5-VL: Pearson 0.95, Spearman 0.92). An overall Krippendorff's alpha across all judges was reported (0.8014) in an ablation/robustness analysis.",
            "human_evaluator_type": null,
            "agreement_metric": "Pairwise Pearson r / Spearman rho; Krippendorff's alpha (multi-judge)",
            "agreement_score": 0.97,
            "reported_loss_aspects": null,
            "qualitative_findings": "Very strong inter-LLM agreement (pairwise Pearson up to 0.97, Spearman up to 0.94) and high multi-judge reliability (Krippendorff's alpha ≈ 0.80–0.86 reported in different analyses), indicating that different strong LLM judges produce highly consistent rankings/scores.",
            "advantages_of_llm_judge": "Consistency across different LLM judges supports robustness of automated evaluation; demonstrates that multiple LLMs can serve as mutually consistent evaluators.",
            "experimental_setting": "Multi-judge study comparing LLM judges pairwise across validation/test splits; pairwise Pearson and Spearman correlations computed; Krippendorff's alpha reported across judges; some ablation analyses summarized in Table 5/Table 4.",
            "uuid": "e7845.1",
            "source_info": {
                "paper_title": "InfiniBench: A Benchmark for Large Multi-Modal Models in Long-Form Movies and TV Shows",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "rating": 2,
            "sanitized_title": "judging_llmasajudge_with_mtbench_and_chatbot_arena"
        }
    ],
    "cost": 0.01583375,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>InfiniBench: A Benchmark for Large Multi-Modal Models in Long-Form Movies and TV Shows
8 Nov 2025</p>
<p>Kirolos Ataallah 
Equal contribution</p>
<p>KAUST</p>
<p>Eslam Abdelrahman 
Equal contribution</p>
<p>KAUST</p>
<p>Mahmoud Ahmed 
KAUST</p>
<p>Chenhui Gou 
Monash University</p>
<p>Khushbu Pahwa 
RICE University</p>
<p>Jian Ding 
KAUST</p>
<p>Mohamed Elhoseiny 
KAUST</p>
<p>InfiniBench: A Benchmark for Large Multi-Modal Models in Long-Form Movies and TV Shows
8 Nov 20257BDF2A44174EA43317AFD82D406DC52CarXiv:2406.19875v5[cs.CV]</p>
<p>Abstract</p>
<p>Understanding long-form videos, such as movies and TV episodes ranging from tens of minutes to two hours, remains a significant challenge for multi-modal models.Existing benchmarks often fail to test the full range of cognitive skills needed to process these temporally rich and narratively complex inputs.Therefore, we introduce InfiniBench, a comprehensive benchmark designed to evaluate the capabilities of models in long video understanding rigorously.InfiniBench offers: (1) Over 1,000 hours of video content, with an average video length of 53 minutes.</p>
<p>(2) The largest set of question-answer pairs for long video comprehension, totaling around 87.7K.</p>
<p>(3) Eight diverse skills that span both grounding-based (e.g., scene transitions, character actions) and reasoning-based (e.g., deep context understanding, multi-event linking).(4) Rich annotation formats, including both multiple-choice and open-ended questions.We conducted an in-depth evaluation across both commercial (GPT-4o, Gemini 2.0 Flash) and most recent open-source vision-language models such as Qwen2.5-VL,InternVL3.0).Results reveal that: (1) Models struggle across the board: Even the best model, GPT-4o, achieves only 47.1% on grounding-based skills, with most models performing near or just above random chance.</p>
<p>(2) Strong reliance on world knowledge: Models achieve surprisingly high scores using only metadata (e.g., video titles), highlighting a tendency to rely on pre-trained knowledge rather than actual visual or temporal understanding.</p>
<p>(3) Multi-Modal Importance: When provided with full video and subtitle context, however, models show substantial improvements, confirming the critical role of multimodal input in video understanding.</p>
<p>Our findings underscore the inherent challenges in long-video comprehension and point to the need for substantial advancements in both grounding and reasoning capabilities in MLLMs.InfiniBench is publicly available at InfiniBench.</p>
<p>Khushbu Pahwa contributed to this work during her internship at KAUST.</p>
<p>Introduction</p>
<p>Recent progress in Multimodal large language models (MLMMs) has enabled impressive performance on image and short-video understanding by jointly processing visual and textual inputs (Ataallah et al., 2024a;Zhu et al., 2023;Zhang et al., 2023a;Chen et al., 2023;Lin et al., 2023;Liu et al., 2023;Maaz et al., 2023).To push toward richer video comprehension, several models have extended the context length of underlying LLMs, enabling them to process longer videos (Bai et al., 2023(Bai et al., , 2025;;Li et al., 2024a).However, current public benchmarks remain limited in both scope and realism.Most are restricted to short clips (a few seconds to minutes), and even larger-scale efforts like LVBench, with one-hour videos, lack narrative complexity and modality diversity (Wang et al., 2024b).In real-world scenarios, long-form videos especially movies and TV shows present fundamentally different challenges.These formats feature rich, structured storytelling with long-range temporal dependencies, evolving character arcs, and intricate causal relationships (Zhang et al., 2023b;Lei et al., 2019;Song et al., 2023) .Unlike short videos, they demand high-level skills such as intent inference, multi-event linking, and thematic summarization.Humans can easily integrate dispersed cues across modalities and time; current MLMMs cannot.</p>
<p>To bridge this gap, we introduce InfiniBench, a large-scale benchmark purpose-built to evaluate the long-video understanding capabilities of multimodal models.InfiniBench leverages over 1,000 hours of content drawn from movies and TV series with average video length (52.59 minutes) and the largest set of skill-targeted question-answer pairs (87.7K) among existing long and very long video benchmarks (see Table 1).</p>
<p>Crucially, InfiniBench is organized around eight core skills necessary for holistic long-video understanding.These are grouped into two categories:</p>
<p>(1) Grounding-based skills, which assess a model's ability to retrieve and organize visual and temporal information, and (2) Reasoning-based skills, which require causal inference, contextual understanding, and abstract reasoning.Key Insights from InfiniBench: Despite the scale and capabilities of modern MLMMs, all evaluated models struggle on InfiniBench.For instance, GPT-4o, which is the top-performing commercial model, achieves only 47.1% accuracy on grounding-based tasks, just modestly above the random baseline of 20%.This performance gap highlights how far even the best models are from mastering longvideo comprehension.To dissect model behavior further, we conduct controlled experiments that isolate the role of world knowledge vs. visual input.Surprisingly, models still achieve non-trivial accuracy when given only metadata (e.g., show title and episode number), indicating heavy reliance on pre-trained knowledge.Yet, the most substantial gains up to +15% in accuracy come from providing visual input, confirming that true video understanding requires visual grounding, not just memorized associations.Finally, we observe consistent underperformance across both skill groups, grounding and reasoning-based skills.Reasoning-based tasks remain especially challenging, with even commercial models failing to produce coherent, causal, or context-aware answers.These findings underscore the unique difficulty and diagnostic power of In-finiBench, which surfaces fundamental limitations in today's multi-modal models and points toward critical future directions.extended version of the related work can be found in Section C.</p>
<p>Recently, many multimodal large language models have aimed to extend their context lengths, such as Qwen2.5-VL(Bai et al., 2025), Qwen2-VL (Wang et al., 2024a), and LongVU (Wu et al., 2024), which can process videos over an hour long.This highlights the growing need for effective benchmarks.In contrast, short video benchmarks have been widely explored, covering tasks like moment retrieval (Liang et al., 2024;Lei et al., 2021), action classification (Caba Heilbron et al., 2015;Carreira and Zisserman, 2017;Soomro et al., 2012), video reasoning (Xie et al., 2024;Xiao et al., 2021), andvideo QA (Xu et al., 2017;Lei et al., 2019;Jang et al., 2017).Among longer video benchmarks, TVQA (Lei et al., 2019) is an early example with over 152.5 K QA pairs across 21.8 K clips (1.27 min avg.), totaling 461.2 hours.More recent datasets like MovieChat-1K (Song et al., 2023) offer 1,000 movie clips (9.4 min avg.) with 14K annotations.CRAFT (Ates et al., 2020) and GazeVQA (Ilaslan et al., 2023) extend to causal and embodied reasoning.Other long-form efforts include MLVU (Zhou et al., 2024), MoVQA (Zhang et al., 2023b), and Video-MME (Fu et al., 2024), though their average durations peak at 17 minutes.LVBench (Wang et al., 2024b) stands out with 1-hour videos, but includes only 103 clips totaling 117 hours.Domain-specific datasets like Ego4D QA on EgoSchema (Mangalam et al., 2023) focus on first-person views but use short (3-minute) clips.Motivated by these gaps, our benchmark evaluates grounding (temporal/visual anchoring) and reasoning (causality, narrative) skills to reflect human-like video understanding.As shown in Table 1, our dataset supports nearhour-long videos and offers the most significant total duration (∼1K hours) and the largest QApair count in terms of long video understanding (87.7K).</p>
<p>InfiniBench</p>
<p>Essential Skills for Long-Video Understanding</p>
<p>Understanding long videos requires more than event recognition, it demands tracking narratives, entities, and interactions over time.We categorize the core skills into two groups: 1-Grounding-Based Skills, focused on retrieving and organizing information directly from the video; and 2-Reasoning-Based Skills, which require inference, contextual understanding, and causal analysis.This division mirrors human comprehension: grounding what happens, then reasoning about why.In-finiBench leverages this structure to offer a comprehensive evaluation of long-video understanding.</p>
<p>Grounding-Based Skills</p>
<p>Grounding-based skills evaluate a model's ability to retrieve, order, and structure video content without requiring inference.Global Appearance: Tests the ability of the model to track changes in character appearance throughout the video.This requires long-term visual consistency and awareness of visual storytelling cues, e.g., "What is the change in outfit for (charactername) in this video?".</p>
<p>Reasoning-Based Skills</p>
<p>These skills assess deeper narrative understanding, where models must infer relationships, motivations, and implications not explicitly stated.They reflect how humans interpret stories by connecting context, prior knowledge, and unstated cues.</p>
<p>Linking Multiple Events: Evaluates the ability to connect distant or seemingly unrelated events by inferring causality.Example: "How does Chandler's previous experience with his own parents' separation influence his decision to break up with Janice?" A: "Chandler's experience with his parents' separation strongly influences his decision.</p>
<p>Remembering how he hated the man who came between his parents, Chandler decides to step aside for the sake of Janice's family's happiness, which leads him to break up with her."</p>
<p>Data Collection Pipeline</p>
<p>Building a rigorous long-video benchmark demands data that mirrors real-world storytelling, with multi-character dynamics, evolving plots, and implicit causal links.Our pipeline ensures diversity, depth, and relevance through three steps: 1-Video Selection (Sec.3.2.1):Choose movies and TV episodes that exhibit complex narratives.2-QA Generation (Sec.3.2.2):Craft question-answer pairs aligned with each of the eight skills.3-Refinement (Sec.3.2.3):Remove redundancies, and the questions that can be easily answered without the visual signal, e.g., from the subtitles only or from the model knowledge.</p>
<p>Why Movies and TV-Shows</p>
<p>Unlike vlogs or surveillance footage, which are often repetitive and low in narrative complexity, movies and TV shows offer the rich, structured storytelling essential for long-video understanding.They feature: 1-Diverse, Long-Term Contexts: Multi-layered plots and evolving character arcs require memory and integration across time.2-Non-Linear Storytelling: Flashbacks, subplots, and temporal shifts challenge models to reason over fragmented, distant events.These properties make them ideal for evaluating both grounding and reasoning skills in extended narratives.</p>
<p>Generating Question-Answer Pairs</p>
<p>We construct structured question-answer pairs tailored for long-video understanding, where models must reason over extended contexts, not just recognize isolated moments.Our pipeline, as shown in Figure 2, leverages the video visual-signal combined with transcripts and summaries.Why Transcripts?Transcripts offer richer context than subtitles, including scene descriptions and character actions (e.g., "Monica and Chandler enter the café").These cues allow for generating deeper, temporally grounded questions that reflect both visual and structural aspects of the story.For more details, see B.4 in the supplementary.For the TV shows, transcripts were sourced from reputable platforms such as (Forever Dreaming) and manually verified and aligned with the video.For movies, we utilize the transcripts provided by MovieNet (Huang et al., 2020), which have been pre-processed to ensure accurate alignment with both dialogue and subtitles, as detailed in Section A.5 (Huang et al., 2020).Chronological Understanding &amp; Linking Events: We employ GPT-4o to chronologically extract key events from the transcript.Then, we generate multiple-choice questions that assess the understanding of the full or partial chronological sequence.For linking events, we input GPT-4o the extracted events and asked the model to connect non-adjacent events by identifying causal relations and generate the Q/A pairs.Scene Transitions: We prompted GPT-4o to identify all scene locations appearing in the transcript in chronological order.Based on this information, we generated multiple-choice questions designed  to assess the correct sequential ordering of these locations in the video.</p>
<p>Character Tracking: To track character actions in the video, we input the transcript and the video summary into GPT-4o, which generates questionanswer pairs focused on identifying the sequence of actions performed by the main characters.Deep Context Understanding: For the deep context understanding question-answer pairs, we employed GPT-4o with the transcript and the summary to generate questions about the hidden motivations and social cues.Global Appearance: Our pipeline, as shown in Figure 2, integrates person detection (Khanam and Hussain, 2024) and face-matching using InsightFace (Guo et al., 2021) to identify characters in video frames.Reference images of the main cast were collected from IMDb and matched with detected faces, using an empirically determined threshold to filter out unmatched individuals.Cropped character images were saved chronologically in timestamped folders.To identify unique outfits, we extracted visual features using DINO-v2 (Oquab et al., 2024) and removed redundant frames based on similarity scores, reducing approximately 200 frames to 20 while conservatively preserving visual diversity.The remaining images were then processed with GPT-4o (OpenAI, 2024) to generate outfit descriptions.Multiple-choice questions were constructed by varying the sequence of outfits.In cases where a character wore a single outfit throughout, distractor options featuring incorrect outfits were introduced to maintain question complexity.Summarization &amp; Spoiler Questions:</p>
<p>We curate expert-written IMDb (IMDB) summaries for human-level summarization.Spoiler questions are sourced from flagged plot discussions, focusing on twists or endings.</p>
<p>Data Integrity and Diversity</p>
<p>To ensure the reliability and quality of our benchmark, the collected dataset undergoes several refinement steps aimed at eliminating redundancy, mitigating data leakage, and reinforcing multimodal reasoning.</p>
<p>1) Redundancy Removal.To avoid question duplication, we compute pairwise textual similarity using BGE-M3 embeddings (Chen et al., 2024a) and remove highly similar questions (see Section B.5 for details).Additionally, since movies often contain over 100 events, many of which are redundant, we apply the same similarity-based filtering to retain only 20 diverse and representative events per movie.</p>
<p>2) Filtering Internal Model's Knowledge.Large video-language models are trained on massive datasets and may memorize content from widely available media, raising the risk of data leakage.To address this, we evaluate three powerful models-GPT-4o, Gemini 2, and Qwen2.5-VL-onour dev and test splits using only the movie/TV show metadata (e.g., title, genre, year) as input.If all three models correctly answer a question using this metadata alone, we consider the question likely answerable from internal knowledge and remove it.This step filters out approximately 16% of the questions, resulting in a subset we refer to as the Knowledge-Filtered Set.</p>
<p>3) Making Vision Matters.To ensure that our benchmark requires visual understanding-beyond textual reasoning, we further filter out questions answerable using subtitles alone.Using the same three models, we provide only the subtitle stream and explicitly instruct the models to rely solely on this modality.As in the previous step, questions correctly answered by all three models are excluded.This removes an additional 16%, lead-ing to a total reduction of 32% across both filtering stages.The remaining set constitutes our final benchmark, on which all evaluations and ablations are performed unless otherwise noted.4) Human Verification.To further ensure the validity, clarity, and correctness of the dataset, we conducted manual verification on the test set, covering 20% of the entire benchmark.This verification process revealed a 90.11% accuracy rate for valid question-answer pairs across different skills, demonstrating high overall reliability.Importantly, any identified errors during this process were filtered out, ensuring that the final test subset reflects a clean and trustworthy evaluation set.Additional details can be found in Section B.6.The custom verification interface used in this process is shown in Figure 6.</p>
<p>Data Statistics</p>
<p>InfiniBench ultimately comprises 87.7K questions over videos averaging 53 minutes, with some running as long as 201 minutes, making it the largest and most challenging benchmark for long-video understanding.InfiniBench includes 923 episodes from six TV shows (Lei et al., 2019) based on the long-form version in GoldFish (Ataallah et al., 2024b) and filtered 296 movies that have subtitles from MovieNet (Huang et al., 2020).These movies offer a broad and challenging foundation for benchmarking long-video understanding.Figures 3 and  4 illustrate the per-skill distribution, source breakdown (TVQA (Lei et al., 2019), MovieNet (Huang et al., 2020)), and duration statistics across the dataset.We humanly verified 20% of the data, ensuring fair coverage across TV shows and movies.</p>
<p>The remaining 80 % is available for training video models.The verified portion is further divided into test and validation splits, 10% each.All reported results and ablation studies are conducted using the test split.</p>
<p>Experiments 4.1 Models</p>
<p>To assess the capabilities and limitations exposed by InfiniBench, we evaluate 13 state-of-the-art long-video MultiModal Large Language Models (MLLMs) spanning both open-source and commercial ecosystems.</p>
<p>Open-Source Models We benchmark eleven leading open-source MLLMs, each evaluated using their official implementations and following their official best practices and configuration, e.g., recommending sampling strategy and frame-rate.These models vary in architecture, context length, and vision-language alignment strategies, offering a diverse perspective on the current research landscape: Qwen2.5-VL(Bai et al., 2025), Qwen2-VL (Wang et al., 2024a), InternVL3 (Zhu et al., 2025b), InternVL2.5 (Chen et al., 2024b), In-ternVL2(InternVL2), LLaVA-OneVision (Li et al., 2024a), LongVU (Shen et al., 2024),VideoChat-Flash (Li et al., 2025), InternLM-XComposer-2.5 (Zhang et al., 2024), Goldfish (Ataallah et al., 2024b), and MiniGPT4-video (Ataallah et al., 2024a) Commercial Models We also evaluate two cuttingedge proprietary models via their official APIs: GPT-4o (OpenAI, 2024) and Gemini 2.0 Flash (Gemini, 2024).These models represent the frontier of closed-access MLLMs with advanced capabilities in video, text, and multi-turn interaction.</p>
<p>Their inclusion offers a valuable benchmark ceiling for current open-source efforts.</p>
<p>We followed the standard best practices for all the models based on their official implementation.More details can be seen in Section B.7.</p>
<p>Evaluation Setup</p>
<p>We adopt distinct evaluation protocols for grounding-based and reasoning-based skills, reflecting their differing task formats: multiplechoice (MCQ) and open-ended, respectively.Multiple-Choice (MCQ).For grounding-based skills, model responses are evaluated using standard accuracy.Models are prompted to output the correct option number, and accuracy is computed based on an exact match with the correct answer.Open-Ended.Reasoning-based skills are assessed using a GPT-4o mini, assigning a score from 0 to 10.This score reflects the overall quality of the response, based on five criteria: 1-Factual correctness.2-Relevance to the question.3-Proximity to the expected answer.4-Hallucination avoidance.5-Completeness.The scoring prompt is provided in Figure 8.</p>
<p>Findings and Insights</p>
<p>We analyze the results across 13 models to uncover key patterns in long-video understanding.Our findings highlight substantial performance gaps, modality dependencies, and emerging limitations in current MLLMs, even at the frontier.Table 3: InfiniBench leaderboard across eight skills.Models are ranked by their equally weighted performance for Grounding and reasoning skills using this formula 0.5 • acc 100 + 0.5 • score 10 .FPV (Frames Per Video), FPS (Frames Per Second), and FPW (Frames Per Window) are reported.All models use both video and subtitles as inputs.Random Per. is random performance.</p>
<p>World Knowledge &amp; Data Leakage</p>
<p>To isolate the influence of pre-trained knowledge from visual understanding, we conducted a controlled experiment where models were prompted using only video metadata-such as movie titles, release years, or TV episode identifiers-without access to any visual or subtitle input.As shown in Tables 2 and 6, several models perform surprisingly well under this setting.For example, GPT-4o achieves 36.8% and 5.0 in grounding and reasoning scores, respectively, indicating a substantial reliance on memorized knowledge from pretraining.To mitigate this issue, we apply a filtering strategy described in Section 3.2.3.Post-filtering, model performance under metadata-only input drops significantly-e.g., GPT-4o's accuracy decreases from 36.8% to 27.4%, approaching the random baseline of 20%.This validates the effectiveness of our filtering and suggests that the remaining questions require genuine understanding of the visual and textual input.Moreover, when full video and subtitle context is provided, model performance increases substantially (e.g., GPT-4o improves from 36.8% to 53.9%), reinforcing the role of multimodal input in successful reasoning.This finding raises an important question: "Do current models inherently favor memorized knowledge over true multi-modal understanding?"Our results suggest that overcoming this bias is key to advancing longvideo reasoning.</p>
<p>Models Are Struggling</p>
<p>Despite recent progress in vision-language modeling, none of the evaluated models-commercial or open-source-achieve strong performance on grounding-based skills.As shown in Table 3, the best-performing model, GPT-4o, reaches only 47.1% accuracy, approximately 2.5 times the random baseline of 20%.This substantial gap highlights a key limitation: even state-of-the-art models struggle to maintain accurate visual grounding over long temporal contexts.Reasoning-based performance is similarly low.GPT-4o achieves the highest reasoning score among commercial models with an average of only 6.5, followed by Gemini-2 at 5.4, and the best open-source model, InternVL3, at just 4.0.These results underscore the difficulty of our benchmark and point to the need for further advancements in long video understanding and multi-step reasoning capabilities.</p>
<p>Modality Benefits vs. Context Limitations</p>
<p>To further analyze the contribution of each modality, we compare model performance across three input settings: (1) metadata only, (2) subtitles only, and (3) video combined with subtitles.As shown in Tables 2 and 6, subtitles alone offer only marginal improvements over metadata for certain visually grounded skills such as Global Appearance, increasing accuracy from 22.2% to 29.1%.However, for multi-modal skills that require both visual and textual reasoning, e.g., Chronological Understanding, the performance improves substantially, from 35.4% to 68.5%.Surprisingly, adding video can be inconsistent: it significantly boosts performance on skills like Global Appearance and Spoiler Questions, but offers no benefit, or even degrades performance, for tasks such as Linking Events and Character Actions.We hypothesize that these inconsistencies stem from the tension between visual diversity and context length.Many skills require long-range contextual reasoning, which subtitles provide in a continuous form.In contrast, video inputs, sampled uniformly across the full episode, introduce fragmented and scattered visual information that can overwhelm the limited context window of current models.This fragmentation may distract the model, leading to worse performance than when using subtitles alone.This highlights a fundamental trade-off: while some skills benefit from multi-modal inputs, others rely more heavily on extended, coherent textual context, which can be disrupted by the inclusion of sampled visual frames.Overall, the results underscore a key limitation in existing models: constrained context windows hinder their ability to jointly reason over extended textual narratives and dispersed visual evidence.</p>
<p>Grounding &amp; Reasoning: Both Are Challenging</p>
<p>Across the eight skill types, both grounding and reasoning tasks remain far from solved.While commercial models outperform open-source ones by a notable margin, all models struggle across both skill categories, with no single model dominating in either domain.This reflects the intrinsic difficulty of the benchmark and underscores the need for significant model innovation, particularly in handling extended temporal structure, entity tracking, entity linking, and causal reasoning.Our results suggest that long-video understanding is far from solved and demands focused future efforts.</p>
<p>Impact of Different Frame Rates</p>
<p>To assess the influence of input frame rates, we conducted an ablation study using four open-source models (QwenVL2.5,QwenVL2.0,InternVL3.0,and Video-Flash).As shown in Table 4, increasing the number of input frames generally leads to better accuracy, although the degree of improvement varies between models.For example, both QwenVL2.5 and QwenVL2.0 show substantial improvements with higher frame rates.On the MCQ set, QwenVL2.5 performance increases from 23.6% to 27.4%, while QwenVL2.0 improves from 24.6% to 28.7% as the number of frames increases from 16 to 128.In contrast, Video-Flash and InternVL3.0exhibit minimal improvement even when the number of frames is increased by a factor of eight in InternVL3.0and by a factor of 62 in the case of VideoChat-Flash.The performance of QwenVL2.5 and QwenVL2.0 begins to degrade when entering the 768 frames, indicating that these models struggle when the context window is fully saturated.A likely explanation is that, these models are constrained to a certain number of input frames, limiting their capacity to exploit longer temporal context.In particular, reasoning-based skills benefit the most from higher frame rates, as the additional visual context reduces the likelihood of missing critical shots, thus improving model performance.In summary, while higher frame rates tend to improve accuracy, the degree of benefit is closely related to the architectural capacity of the model and the training strategy.</p>
<p>Evaluation Robustness</p>
<p>Self-enhancement Bias</p>
<p>The main results in Table 3 use GPT-4o as the judge model, which raises the potential concern of self-enhancement bias (Zheng et al., 2023), since GPT-4o serves as both the task model and evaluator.However, we argue that such bias is unlikely in our setup due to the clear separation between generation and evaluation.Specifically, during inference, the model accesses both video and subtitles to generate answers, whereas the judge model evaluates responses using only the generated text and reference answers.This separation in goals, modalities, and inputs minimizes the risk of any model-specific advantage transferring across tasks.</p>
<p>To further assess evaluation objectivity, we conducted a multi-judge study using two strong and di-</p>
<p>Reliability of the reasoning skills scoring</p>
<p>To assess the alignment between our LLM-based judge and human evaluators, we conducted a detailed comparison on 10% of the test set using standard statistical measures.Specifically, we collected ratings from three independent human annotators per sample and computed the average human score.</p>
<p>We then measured the Pearson correlation between the LLM scores and the averaged human ratings, obtaining a score of 0.92, indicating a high degree of linear agreement.Additionally, we report Spearman correlation of 0.89, confirming that the LLM's ranking of outputs closely matches human preferences.</p>
<p>To contextualize this alignment, we computed inter-annotator agreement using Krippendorff's alpha, which yielded a value of 0.86, showing that the LLM aligns with human consensus nearly as well as humans agree among themselves.We also measured per-annotator Pearson correlations with the LLM, which ranged from 0.88 to 0.93, further supporting the strong consistency between the LLM and individual human raters.These results demonstrate that our LLM judge exhibits near-humanlevel alignment, validating its use as a reliable proxy for human evaluation in our setting.</p>
<p>Conclusion</p>
<p>Our evaluation of current Multimodal Large Language Models (MLLMs) on the InfiniBench benchmark reveals key limitations in long-video understanding.Top models like GPT-4o perform modestly on grounding tasks and struggle with reasoning-based skills that require causal inference and context-aware understanding.Models show significant reliance on pre-trained knowledge, performing better with metadata alone, but improve substantially when provided full video and subtitle context, emphasizing the importance of multimodal inputs.We also observe that visual input diversity often conflicts with the limited context windows of current models, affecting performance on certain tasks.Overall, InfiniBench highlights critical gaps in existing models and directs future research toward improving visual grounding and multi-step reasoning for long-video comprehension.</p>
<p>Despite its strengths, our question-answer generation pipeline currently depends on the availability of transcripts, limiting its use to movies and TV shows.To broaden applicability, we will develop a more general pipeline that can handle arbitrary videos without preexisting transcripts.We also plan to leverage our dense, skill-based annotations to create multi-video benchmarks, such as seasonlevel evaluations, that extend each skill across related clips.Furthermore, our training split can support retrieval-based MLLM methods that focus on relevant frames (inspired by Video-XL (Liu et al., 2025)), and we aim to explore long-video reasoning approaches, such as those based on the GRPO algorithm (Shao et al., 2024), using samples from our dataset.</p>
<p>Acknowledgment</p>
<p>We would like to express our sincere gratitude to Habib Slim for his valuable assistance in preparing the visualizations accompanying this paper.His attention to detail and artistic insight significantly enhanced the overall quality and clarity of the work.</p>
<p>A Extended Experimental Results</p>
<p>A.1 Results on the Validation-set</p>
<p>We are organizing a challenge based on our benchmark; therefore, we will not release the test set.Accordingly, the follow-up works that will leverage our benchmark can use the results reported on the validation set reported in Table 7.As shown in 7, the same insights and conclusions are seen, similar to those reported on the test-set in Table 3.</p>
<p>A.2 Results on the Full Test-set Before Filtration</p>
<p>As discussed in Section 3.2.3,we have applied two types of filtration to make our benchmark more reliable and robust: 1) Filtering the internal model's knowledge by removing the questions that can be answered from the metadata solely.2) Making vision matters by removing the questions that can be answered from the subtitles only.The main results reported in Tables 3 and 7 are reported on the filtered version of our benchmark.</p>
<p>To demonstrate the importance of the filtration process, we demonstrate the results on the test-set before filtration in Table 8.As shown in Table 8 and in comparison to Table 3, all the models achieve a higher performance before filtration, due to the knowledge leakage, and the intensive dependency on the input text modality, subtitles.So by filtering these two factors, the performance of all the models drops significantly.We believe the filtered version is more reliable and truly assesses the model's capabilities.</p>
<p>A.3 Impact of the "I Don't Know" Option</p>
<p>As shown in Table 9, removing the "I don't know" option leads to an increase in model accuracy.Without this option, models are forced to select an answer, even when uncertain, which can occasionally result in correct guesses due to chance.In contrast, the inclusion of the "I don't know" option requires the model to explicitly acknowledge uncertainty when it lacks sufficient evidence to answer correctly, thereby increasing task difficulty and realism.Additionally, excluding the "I don't know" option reduces the number of choices per question, which increases the expected performance of random guessing.This factor should be considered when comparing results across configurations with and without this option.Leveraging the grounding information obtained during the verification process, we generated additional event-based questions.Specifically, we utilized the chronological events in conjunction with other grounding skills such as global appearance, scene transitions, and character actions to enrich the verified set.For example, in scene transitions skill, we can formulate systematic questions such as "What are the scene transitions that occur before the event "Ross watches TV with Rachel"?"This is feasible because we can extract the relevant transitions occurring before the event timestamp from the complete set of scene transitions using temporal annotations.The same approach is applied with global appearance and character actions; we formulated questions that refer to moments immediately preceding or following specific events in the video.These questions are only added to the verified set.</p>
<p>B.2 Data Diversity</p>
<p>Our benchmark is not narrowly scoped around human characters or interactions.Instead, it is built to holistically assess a wide range of long-video understanding capabilities:</p>
<ol>
<li>Diversity in Evaluated Skills Our benchmark explicitly targets a broad spectrum of cognitive and perceptual skills, not just character-centric ones.While certain tasks-such as Global Appearance and Character Actions-do emphasize characters, a substantial portion of the benchmark is designed around generic, high-level understanding of visual narratives, events, and environ-</li>
</ol>
<p>Diversity and Richness of Video Sources</p>
<p>Our dataset comprises a large, balanced mix of TV shows and over 296 movies spanning multiple decades, cultures, genres, and visual settings.This includes diverse environments such as nature, abstract spaces, and sci-fi/fantasy worlds, demonstrating that the benchmark is not narrowly human-centric.For example : Nature &amp; Survival: Movies like "Into the Wild" and "Life of Pi" focus on wilderness and animal interaction.Abstract &amp; Visual Reasoning: Movies such as "2001: A Space Odyssey", "Inception", and "Donnie Darko" challenge models with surreal, symbolic visuals.Space &amp; Environmental Focus: Movies such as "Gravity" and "The Thing" take place in isolated, non-social settings.Sci-Fi &amp; Fantasy Worlds: Movies such as "Avatar" and "The Matrix Reloaded" emphasize non-human environments and abstract reasoning.Minimal Dialogue / Object-Centric: "Cube", "Moon", and "Minority Report" require visual comprehension beyond character interaction.</p>
<p>Additionally, we argue that movies and TV shows are inherently more challenging than other long-form video domains, such as vlogs or surveillance footage.This is due to: 1) Multi-layered narratives require long-term memory and reasoning.2) Non-linear structures involving flashbacks and fragmented timelines.3) Visually rich environments that go beyond typical human-centric interactions.</p>
<p>These factors make them ideal testbeds for evaluating true long-video understanding.</p>
<p>B.3 Data Copyrights</p>
<p>Our benchmark provides only annotations and uses two widely adopted, publicly available video datasets that provide legally compliant, downsampled video content (e.g., 3 FPS, scene shots).Specifically, we use: (1) six full-season TV shows from the TVQA (Lei et al., 2019) dataset, based on the long-form version in GoldFish (Ataallah et al., 2024b), and (2) movies from the MovieNet (Huang et al., 2020) dataset.For the TV shows, transcripts were sourced from reputable platforms such as foreverdreaming and manually verified by the authors for alignment with the video.This usage is consistent with fair use for academic research.For movies, we use transcripts provided by MovieNet (Huang et al., 2020), which are already filtered and accurately aligned with dialogue and subtitles.</p>
<p>B.4 Transcripts vs. Subtitles</p>
<p>Figure 7 illustrates the distinction between transcripts and subtitles.</p>
<p>Transcripts are comprehensive documents authored by screenwriters, containing not only spoken dialogue but also detailed scene descriptions, setting and location information, character actions, and camera directions (e.g., angles, cuts, and shot composition).Effectively, transcripts serve as blueprints for both the visual and narrative structure of a movie or TV show.In our benchmark, transcripts are used exclusively during the data construction phase to generate rich and diverse questions.They are not provided to the model during inference or evaluation.</p>
<p>Subtitles, by contrast, are limited to the spoken dialogue extracted from the video's audio track.These are optionally provided to the model at inference time, serving as an auxiliary textual modality alongside the video frames.If available, subtitles can enrich the input context and help improve per-</p>
<p>Transcript of episode 1 season 1 of Friends TV shows:</p>
<p>[Scene: Central Perk, Chandler, Joey, Phoebe, and Monica are there.]Monica: There's nothing to tell!He's just some guy I work with!... (Ross gestures his consent.)Joey: Strip joint!C'mon, you're single!Have some hormones!(Rachel enters in a wet wedding dress and starts to search the room.)Subtitle of the same episode 1 00:00:54,012 --&gt; 00:00:57,641 There's nothing to tell.It's just some guy I work with. 2 00:00:57,892 --&gt; 00:00:59,962 There's gotta be something wrong with him.formance, particularly for dialogue-centric questions.</p>
<p>B.5 Duplications Filtering</p>
<p>To ensure that our dataset is free of duplicate or near-duplicate questions, we implemented a semantic similarity filtering pipeline.</p>
<p>We first embedded each question and its answer choices using M3-Embedding (Chen et al., 2024a), then computed pairwise cosine similarity to detect potential duplicates.We experimented with similarity thresholds of 90%, 95%, and 98%.As shown in Figure 5, the vast majority of questions are unique across all skill categories.Only two skills-Temporal Questions and Character Actions-produced potential duplicates.However, upon manual inspection, these were found to be false positives.For example, the following two questions were flagged as duplicates due to high embedding similarity, despite a critical semantic difference:</p>
<p>Q1: Did the event flashback to Phoebe completing a mile on a hippity-hop before turning thirty happen before the event Monica makes breakfast with chocolate-chip pancakes?Q2: Did the event flashback to Phoebe completing a mile on a hippity-hop before turning thirty happen after the event Monica makes breakfast with chocolate-chip pancakes?</p>
<p>Although these questions differ by a single word, that word fully reverses the temporal meaning, highlighting the importance of semantic context when evaluating similarity.Based on this analysis, we conclude that the dataset contains no true duplicates.The filtering pipeline successfully identifies and flags near-duplicates, and the observed false positives illustrate the subtlety and complexity of semantic variation in question phrasing.</p>
<p>B.6 Human Verification</p>
<p>To ensure the reliability of our benchmark, we conducted a human evaluation on 20% of the dataset.Annotators were tasked with verifying the correctness of the question-answer (Q/A) pairs across each skill category.Figure 6 shows the custombuilt verification interface developed to streamline the annotation process.During verification, annotators assessed each video using the following criteria:</p>
<p>• Global Appearance and Character Actions: Annotators were provided with the character's name and a list of appearance or action events extracted by our pipeline.Each event was marked as absent if not observed in the video; if present, the annotator recorded its corresponding timestamp.• Linking Events and Deep Context Understanding: Annotators confirmed the validity of each Q/A pair, ensuring the answer could only be derived from the visual or textual content of the video.Temporal grounding was also recorded when applicable.• Scene Transitions and Chronological Understanding: Annotators reviewed the sequence of collected events for each video, verifying their presence and providing timestamps for valid instances.• Spoiler Questions: Annotators ensured that each Q/A pair relied solely on information present in the video, excluding any external or prior knowledge.The collected timestamps were cross-referenced with the dataset to validate both the extracted events and their chronological consistency.This also enabled the generation of temporal certificates for each Q/A pair, following the methodology introduced in EgoSchema (Mangalam et al., 2023).A temporal certificate represents the number of video segments required to answer a given question.Following (Mangalam et al., 2023), video clips shorter than 0.1 seconds were discarded, and consecutive segments within a 5-second window were merged into a single clip.The total duration of these merged segments defines the certificate length.</p>
<p>Our human-verified test set contains an average of 3.23 temporal certificates per Q/A pair, with an average certificate length of 202.97 seconds, underscoring the benchmark's emphasis on comprehensive long-video understanding.The presence of timestamps for each answer further enhances the Note that Summarization questions do not require human verification, as they are sourced from the official IMDb website (IMDB).According to IMDb's contribution guidelines, all plot summaries must adhere to strict formatting and factual standards, and each submission is manually reviewed and approved by the IMDb team.This ensures the summaries are reliable, accurate, and effectively pre-verified by human annotators.</p>
<p>B.7 Models Configuration</p>
<p>GPT-4o (OpenAI, 2024): GPT-4o does not support direct ingestion of .mp4video files.To accommodate this, we sample up to 450 frames per video and provide the model with the corresponding subtitle text and question.Gemini-Flash 2.0 (Gemini, 2024): Developed by Google, Gemini-Flash 2.0 supports native video input.For TV shows, we supply the model with the full video and the associated question.For movies-which lack audio-we additionally provide the subtitle file alongside the video and question.Qwen2.5-VL(Bai et al., 2025) and Qwen2-VL (Wang et al., 2024a): These models accept up to 768 input frames, fitting within the memory limits of an NVIDIA A100 GPU (80 GB) under the authors' recommended inference settings.Subtitles are concatenated with the input question to provide multimodal context.Video-Chat-Flash (Li et al., 2025): Although the model supports up to 10,000 frames, we limit input to 1,000 frames due to memory constraints on the A100 GPU (80 GB).Other configurations follow the default setup.Subtitles are concatenated with the input question to enrich contextual understanding.InternVL3.0(Zhu et al., 2025a), In-ternVL2.5 (Chen et al., 2024b), and In-ternVL2 (InternVL2): For the InternVL family, we evaluate models using both 16 and 128 frames, the latter being the maximum supported without hallucination on an A100 GPU.Although these models can technically process more frames, exceeding 128 often introduces unstable outputs.Subtitles corresponding to the selected frames are appended to the input question.LLaVA-OneVision (Li et al., 2024a): This model supports both image and video inputs.We evaluate performance at 16 and 128 frames-found to be the upper bound for stable performance on an A100 GPU.Subtitles for selected frames are appended to the question.LongVU (Shen et al., 2024): LongVU is evaluated using 512 frames, staying within the memory and context window limits of the A100 GPU.Subtitles aligned with the selected frames are appended to the input question.InternLM-XComposer-2.5 (Zhang et al., 2024):</p>
<p>We use the default configuration with a 16-frame window and increase the number of clips per video to 120.Subtitles aligned with these clips are appended to the input question.All evaluations are conducted on an A100 GPU.Goldfish and MiniGPT-4-Video (Ataallah et al., 2024b): We use the Mistral backbone with default parameters, retrieving k = 3 video clips per query.Each 80-second video yields 60 frames (approx.1.3 FPS).Although the model supports up to 90 frames, we limit it to 60 to reserve space for the question, subtitles, and output tokens-reducing hallucinations due to context overflow.Subtitles are interleaved with the frames, following the MiniGPT-4-Video configuration.For MiniGPT-4-Video, we similarly sample 60 frames per video, interleave them with subtitles, and prompt the model with the question for generation.</p>
<p>B.8 Benchmark Examples</p>
<p>Here in this section, we are showing more examples of our benchmark skills, such as the Chronological Understanding in Fig. 9, linking multiple events in</p>
<p>B.9 Success and Failure Cases</p>
<p>In this section, we present examples of both success and failure cases in question generation using GPT-4o.Figure 18 illustrates cases involving the generation of chronological understanding questions, while Figure 17 showcases examples related to Linking Multiple Events questions.As highlighted in the human evaluation section B.6, such failure cases are infrequent, with 90.11% of the generated data verified as accurate.</p>
<p>B.10 Exact Prompts</p>
<p>This section elaborates on the specific prompts employed to generate questions for each skill category.The prompts, utilized within the GPT-4o framework, are depicted in Figures 19,21,20,22,23.These figures provide the exact phrasing and structure used for question generation, ensuring reproducibility and clarity in the benchmarking creation process.</p>
<p>C Extended Related Work</p>
<p>C.1 Semantic Roles in Vision and Language.</p>
<p>Early work like Grounding Semantic Roles in Images (Silberer and Pinkal, 2018) and Grounded Situation Recognition (Pratt et al., 2020) showed how semantic roles can be aligned with visual regions in images.This concept was extended to videos by Video Object Grounding (Sadhu et al., 2020) and Visual Semantic Role Labeling (Sadhu et al., 2021), emphasizing semantic alignment for event understanding.Building on these ideas, HERO (Li et al., 2020) proposed a hierarchical encoder that fuses visual and textual modalities, setting the stage for video-language pretraining.Recent models like VSGR (Mao et al., 2022) further reinforce this trajectory by explicitly building video scene graphs to reason over semantic elements and their relations.In this context, our work introduces a benchmark that evaluates models' ability to track and reason about evolving semantic roles across long-form, multimodal narratives.</p>
<p>C.2 Long Video Models.</p>
<p>Recent advancements in commercial AI models have introduced native support for long-form video understanding.For instance, Google's Gemini Flash family (Gemini, 2024) that provides a one million-token context window for coherent multihour understanding, and GPT-4o (OpenAI, 2024) that processes video frames in conjunction with subtitles within its 128K-token context window.In contrast to the commercial solutions, Open-source efforts rely on compression, memory, and retrieval techniques.Qwen2.5-VL(Bai et al., 2025) extends dynamic resolution temporally to spot key events in hours-long streams and ground them visually.Qwen2-VL (Wang et al., 2024a) uses Multimodal Rotary Position Embedding (M-RoPE) to encode up to 768 frames, preserving spatial and temporal structure.LongVU (Shen et al., 2024) applies cross-modal queries and inter-frame dependencies for adaptive compression, trimming redundancy while keeping salient content.InternLM-XComposer-2.5-OmniLive (Zhang et al., 2024) pairs a short-term memory buffer with on-the-fly compression into long-term storage for later retrieval.Goldfish (Ataallah et al., 2024b) segments videos into clips and retrieves the top-k most relevant, and LLaMA-VID (Li et al., 2023) represents each frame with only two tokens for maximal efficiency.VideoINSTA (Liao et al., 2024) enables long video understanding without pretraining by combining event-based temporal reasoning with LLMs and self-reflective spatial-temporal queries, achieving strong results on EgoSchema and NExT-QA.Despite these innovations, truly unrestricted long-form video understanding, with deep temporal dependencies and narrative complexity, remains an open challenge.</p>
<p>C.3 Video benchmarks.</p>
<p>Recently, many multimodal large language models have aimed to extend their context lengths, such as Qwen2.5-VL(Bai et al., 2025), Qwen2-VL (Wang et al., 2024a), and LongVU (Wu et al., 2024), which can process videos over an hour long.This highlights the growing need for effective benchmarks.In contrast, short video benchmarks have been widely explored, covering tasks like moment retrieval (Liang et al., 2024;Lei et al., 2021), action classification (Caba Heilbron et al., 2015;Carreira and Zisserman, 2017;Soomro et al., 2012), video reasoning (Xie et al., 2024;Xiao et al., 2021), andvideo QA (Xu et al., 2017;Lei et al., 2019;Jang et al., 2017).Among longer video benchmarks, TVQA (Lei et al., 2019) is an early example with over 152.5 K QA pairs across 21.8 K clips (1.27 min avg.), totaling 461.2 hours.More recent datasets like MovieChat-1K (Song et al., 2023) offer 1,000 movie clips (9.4 min avg.) with 14K annotations.CRAFT (Ates et al., 2020) and GazeVQA (Ilaslan et al., 2023) ex-tend to causal and embodied reasoning.Other long-form efforts include MLVU (Zhou et al., 2024), MoVQA (Zhang et al., 2023b), and Video-MME (Fu et al., 2024), though their average durations peak at 17 minutes.LVBench (Wang et al., 2024b) stands out with 1-hour videos, but includes only 103 clips totaling 117 hours.Domain-specific datasets like Ego4D QA on EgoSchema (Mangalam et al., 2023) focus on first-person views but use short (3-minute) clips.Motivated by these gaps, our benchmark evaluates grounding (temporal/visual anchoring) and reasoning (causality, narrative) skills to reflect human-like video understanding.As shown in Table 1, our dataset supports nearhour-long videos and offers the most significant total duration (∼1K hours) and the largest QApair count in terms of long video understanding (87.7K).</p>
<p>Scoring evaluation prompt (GPT4o-mini):</p>
<p>System prompt: You are an intelligent and fair evaluator AI that specializes in assessing the correctness and semantic alignment between ground truth answers and predicted responses for question-answering tasks, including those based on video content.Your role is to evaluate how well a predicted answer matches the correct (reference) answer based on the following detailed criteria: ## EVALUATION INSTRUCTIONS:</p>
<p>-Focus on <strong>semantic similarity</strong>, <strong>factual correctness</strong>, and <strong>completeness</strong>.</p>
<p>-Accept paraphrases, synonyms, or rephrasings <strong>as valid</strong>, as long as they preserve the original meaning.</p>
<p>-<strong>Do not penalize</strong> for stylistic differences or changes in tone, unless they impact factual accuracy.</p>
<p>-<strong>Penalize</strong> if:</p>
<p>-The predicted answer omits <strong>key factual elements</strong> present in the correct answer.</p>
<p>-The prediction includes <strong>hallucinated content</strong> or unfounded details.</p>
<p>-The prediction <strong>contradicts</strong> the correct answer.</p>
<p>-Use human-like judgment: apply reasoning beyond surface text similarity.</p>
<p>-When uncertain, provide a <strong>conservative but fair</strong> score.</p>
<p>-Use a scoring scale from <strong>0 (completely incorrect)</strong> to <strong>10 (perfect match)</strong>.## OUTPUT FORMAT: Return a JSON object with <strong>two fields</strong>:</p>
<p>-"score": an integer from 0 to 10 -"justification": a concise explanation (1-3 sentences) of your reasoning ### Example Output: { "score": 7, "justification": "The predicted answer captures the main idea, but it omits some key details about the setting described in the correct answer."'} Be fair, consistent, and concise.Follow the format exactly.</p>
<p>User prompt:</p>
<p>Please evaluate the following video-based question-answer pair: Question: {question} Correct Answer: {answer} Predicted Answer: {pred} Please return your evaluation in the specified JSON format with both a score and a justification.Monica cooks a gourmet meal for Steve (Jon Lovitz), a restaurateur looking for a new head chef.Steve is a massage client for Phoebe, and she makes the introduction between Monica and him.The job is perfect as Steve wants something eclectic and needs someone who can create the entire menu.As an audition, Monica is cooking dinner for him the coming week.She wants Phoebe to be there.Monica hires a professional waitress Wendy (for $10/hr.),which offends Rachel (Monica says that she needed a professional waitress).Wendy bails on Monica at the last minute.Monica begs Rachel and even says that she gave her shelter when she had nowhere else to go.. Eventually she offers Rachel $20/hr.He arrives stoned and wants to eat everything in sight, including taco shells and gummy bears.Phoebe tells Rachel who tries to handle the situation by offering Steve some wine.Eventually Monica realizes that Steve is super stoned.She tries to yank the gummy bears from Steve, and they end up falling in the punch bowl.. Dinner is a total disaster, and the gang tells her that she doesn't want to work for a guy like that.After working as a data processor for five years, Chandler gets promoted to supervisor.Chandler quits, claiming he only intended for his job to be temporary (and Chandler already has been there for over 5 yrs.To add a bit of mystery to the story.If he'd said 'the galaxy in the jewel on the cat's collar' , the movie would have ended much faster.Actually, Arquillian was indeed trying to tell Jay that the galaxy was on the cats collar.He just didn't have the correct vocabulary to do so.Note how he stumbles over the word \"war\".He almost certainly thinks \"belt\" is the correct word for \"collar\", which is understandable because the articles of clothing are identical, as the only differences are that one is worn around the waist and the other is worn around the neck.And the cat's name is Orion, so he's being accurately descriptive, not deceitful.It's likely that the Arquillian didn't understand much English and that the Jeweler's body had a translator in it when conversing with humans.It was likely damaged when Edgar stabbed it through the neck.The sea rescue brings the couple to the team's attention, setting off a series of events that act as catalysts for character development.The challenging medical mystery forces team members to confront their own abilities, resolve conflicts, and cope with Foreman's departure, leading to significant personal and professional growth.</p>
<p>In what ways do the sea rescue and the medical mystery serve as catalysts for character development within the Diagnostics team?</p>
<p>Linking multiple events</p>
<p>Dr. House's internal conflict at the end ties together the various events of the episode.The stress of the almost Sci-fi case, the emotional impact of Foreman's departure, and the unresolved medical mystery all contribute to House's turmoil, leaving him with an immense conflict that sets the stage for the next season.</p>
<p>How does Dr. House's internal conflict towards the end connect to the events of the episode?Why the answer is not valid ?</p>
<p>Because the couple traveled a great distance, time and danger to reach the hospital through the Coast Guards.This encourages the House team to do their best for this case.</p>
<p>Linking multiple events:</p>
<p>System prompt :</p>
<p>You play two roles: a human asking questions related to a video and an intelligent chatbot designed to help people find information from a given video.Your task is to generate question-answer pairs specifically related to linking multiple events in the video content.You will first play the role of a human who asks questions that link multiple events together in the video, and then play the role of an AI assistant that provides information based on the video content.</p>
<h2>TASK:</h2>
<p>Users will provide information about the video, and you will generate a conversation-like question-and-answer pairs specifically focusing on linking multiple events together in the video to make the questions comprehensive across the video.Generate TWENTY descriptive and conversational-style questions and their detailed answers based on the given information, specifically related to linking multiple events together in the video.</p>
<h2>INSTRUCTIONS:</h2>
<p>-The questions must be conversational, as if a human is asking them, and should directly relate to linking multiple events together in the video.</p>
<p>-The answers must be detailed, descriptive, and should directly reference the information provided.</p>
<p>-The number of events to link together can vary from 2 to any number of events.Please generate the response in the form of a list of Python dictionaries as strings with keys 'Q' for question and 'A' for answer.Each corresponding value should be the question-andanswer text respectively.</p>
<p>Character actions:</p>
<p>System prompt : You play two roles: a human asking questions related to a video and an intelligent chatbot designed to help people find information from a given video.Your task is to generate a question-answer pairs specifically related to each character actions through the whole video content.Your task is to first play the role of a human who asks questions about each character actions through the whole video content.and then play the role of an AI assistant that provides information based on the video content.</p>
<p>------##TASK: Users will provide information about a video, and you will generate a conversation-like question and answers pair specifically focusing on each character actions through the whole video content.Generate one question for each character that summarize all the actions did through the whole video content.</p>
<p>------##INSTRUCTIONS:</p>
<p>-The questions must be like a human conversation and directly related to each character actions through the whole video content.</p>
<p>-The answer must be detailed and descriptive that summarize all actions for each character in the video and should directly reference the information provided.</p>
<p>-Focus on both the visual and textual actions but focus more on the vision actions as these questions are designed for video understanding.##SAMPLE QUESTIONS: -{'Q1': 'What did ross do through this video?','A': 'At the beginning of the episode he drank coffee in central park , then went to his apartment then ate some pizza.'}-{'Q1': 'Summarize all actions that chandler did in this video.','A': 'At the beginning of the episode he read a magazine then went to his work by taxi , and finally he went to Monica's apartment to set with his friends.'}User prompt: This is the episode summary: {caption}.\n This is the episode script: {script}.\n Please generate the response in the form of list of Python dictionaries string with keys 'Q' for question and 'A' for answer.Each corresponding value should be the question-andanswer text, respectively.For the answer, please make it as a python list of actions in chronological order</p>
<p>Figure 1 :
1
Figure 1: Overview of InfiniBench skills: covers eight core skills, grouped into grounding-based (MCQ) and reasoning-based (open-ended) categories.</p>
<p>Figure 2 :
2
Figure 2: Full annotation pipeline for InfiniBench skill set.The upper section depicts the global appearance pipeline, while the lower section illustrates the question generation using GPT-4o.The gates for video summary and video transcript indicate that some skills utilize only the summary, others use only the transcript, and some use both.</p>
<p>Figure 3 :
3
Figure 3: InfiniBench skills statistics.(A) Number of questions per skill, (B) Number of videos per skill, and (C) Average video duration per skill</p>
<p>Figure 5 :
5
Figure 5: Duplication analysis for different thresholds using cosine similarity of text vector embeddings.</p>
<p>Figure 7 :
7
Figure 7: Difference between transcript and subtitles.</p>
<p>Figure.13, and deep context understanding in Figure.11, spoiler questions in Figure.15,Character Actions Figure.16,Scenetransitions in Figure.12, Global Appearance in Figure 10, and summarization in Figure.14.</p>
<p>Figure 8 :Figure 9 :Figure 10 :
8910
Figure 8: Detailed prompt for Scoring system evaluation.</p>
<p>Figure 11 :Figure 12 :Figure 13 :
111213
Figure 11: Example for the deep context understanding skill.</p>
<p>Figure 14 :
14
Figure 14: Example for the summarization skill.</p>
<p>Figure 15 :
15
Figure 15: Example for the spoiler questions in Movies.</p>
<p>Option 1 :Figure 16 :
116
Figure 16: Example for character actions questions.</p>
<p>Figure 17 :
17
Figure 17: Examples of success and failure cases in Linking Multiple Events questions.</p>
<p>[Figure 18 :
18
Figure 18: Examples of success and failure cases in Temporal Order of Events questions.</p>
<p>Figure 19 :
19
Figure19: Detailed prompt for Linking multiple events questions generation.</p>
<p>Figure 20: Detailed prompt for sequence of character actions questions generation.</p>
<p>Table 1 :
1
Comparison between InfiniBench and existing video QA benchmarks and datasets.InfiniBench contains the largest number of QA pairs for long and very long videos, as well as the longest total video duration.
CategoryBenchmarks# Questions # VideosAvg Video Duration (mins)# HoursQuestions Type MCQ Open Video Transcript Summary Auto Human QA Source AnnotationsTGIF-QA (Jang et al., 2017)165.2 K72.0 K0.0560.00✗✓✓✗✗✓✓ShortMSRVTT-QA (Xu et al., 2017) MV-Bench (Li et al., 2024b)243.6 K 4.0 K10.0 K 3.6 K0.25 0.27416.6 16.38✗ ✓✓ ✗✓ ✓✗ ✗✗ ✗✓ ✓✗ ✗TVQA (Lei et al., 2019)152.5 K21.8K1.27461.20✓✗✓✗✗✗✓Activity-QA (Yu et al., 2019)58.0 K58003.00290.00✗✓✓✗✗✗✓Egoschema (Mangalam et al., 2023)5.0 K50633.00253.15✓✗✓✗✗✓✓LongVideoBench (Wu et al., 2024)6.7 K37637.88494.21✓✗✓✗✗✗✓LongMoviechat (Song et al., 2023) MLVU (Zhou et al., 2024)14.0 K 3.1 K1000 17309.40 15.50156.67 446.92✗ ✓✓ ✓✓ ✓✗ ✗✗ ✗✗ ✓✓ ✓MoVQA (Zhang et al., 2023b)21.9 K10016.5327.55✓✗✓✗✗✗✓Video-MME (Fu et al., 2024)2.7 K90016.97254.55✓✗✓✗✗✗✓Very LongLVBench (Wang et al., 2024b)1.6 K10368.35117.33✓✗✓✗✗✗✓InfiniBench (Ours)87.7K121752.591066.70✓✓✓✓✓✓✓</p>
<p>Number of questions Number of videos Avg videos duration (mins)
119970561199113052.56 52.5649.2 49.2151427,63448.59 48.5929.96 29.9685121,942114349.06 49.0630.09 30.097525695888115049.25 49.2521,8261137 1137142126.02 126.02Chronological UnderstandingGlobal Appearance Linking Multiple Events Spoiler UnderstandingDeep Context UnderstandingScene TransitionsCharacter ActionsSummarization</p>
<p>Table 2 :
2
Analysis of the impact of filtering our benchmark by excluding the models' internal knowledge and emphasizing the visual questions.The reported numbers are on GPT4o.Table6in the Appendix, demonstrates the full results across more models.Random Per. is random performance.
Video SubtitlesMeta dataGlobalGrounding Skills Scene CharacterChronologicalSummar-Reasoning Skills Deep Context SpoilerLinkingAvg. Acc.Avg. ScoreAppearanceTransitionsActionsUnderstandingizationUnderstandingUnderstandingEventsRandom Per.---20.020.020.020.0N/AN/AN/AN/A20.0 N/A✓✓✗51.842.943.577.26.36.86.77.553.96.8w/o Filtering✗✓✗29.139.646.882.07.27.05.97.549.46.9✗✗✓25.726.139.156.53.45.23.97.436.85.0✓✓✗49.737.739.961.06.36.46.66.847.16.5w Filtering✗✓✗26.534.242.268.57.16.55.86.842.86.5✗✗✓22.219.931.935.43.34.83.56.627.44.6Grounding SkillsReasoning SkillsModelsFrame RateGlobal AppearanceScene TransitionsCharacter ActionsChronological UnderstandingSummar-izationDeep Context UnderstandingSpoiler UnderstandingLinking EventsAvg. Acc.Avg. ScoreOverallRandom Per.N/A20.020.020.020.0N/AN/AN/AN/A20.0 N/AN/AGPT-4o450 FPV49.737.739.961.06.36.46.66.847.16.556.0Gemini-2.01 FPS45.139.350.050.15.76.04.35.446.15.449.9InternVL3128 FPV34.327.720.531.13.83.73.35.328.44.034.4Qwen2.5VL768 FPV30.025.322.720.33.34.33.45.424.64.132.8Qwen2VL768 FPV23.528.230.227.42.24.33.55.027.33.832.5Goldfish60 FPW16.222.921.325.43.04.93.45.621.54.231.9VideoChat-Flash1000 FPV20.529.534.937.42.63.42.24.230.63.130.9InternVL2128 FPW26.624.921.526.62.93.53.05.024.93.630.4LLava-OneVision128 FPV21.024.723.330.32.03.72.75.124.83.429.4InternVL2.5128 FPV27.125.121.229.22.42.82.14.225.62.927.4InternLM-XComposer 16 FPW20.127.926.626.41.62.62.24.025.22.625.7LongVU512 FPV27.6721.027.919.01.72.92.73.623.92.725.6MiniGPT4-video60 FPV18.123.125.923.12.02.92.03.322.52.624.1</p>
<p>Table 4 :
4
Impact of varying frame rates on model performance.
ModelsFrame RateGlobalGrounding Skills Scene CharacterChronologicalSummar-Reasoning Skills Deep Context SpoilerLinkingAvg. Acc.Avg. ScoreAppearanceTransitionsActionsUnderstandingizationUnderstandingUnderstandingEventsBaseline RandomN/A20.020.020.020.0N/AN/AN/AN/A20.0 N/A76830.025.322.720.33.34.33.45.424.64.140031.327.124.023.33.54.33.35.426.44.1QwenVL2.5350 25630.9 33.327.8 29.824.1 24.623.9 23.83.4 3.34.2 4.13.7 3.45.6 5.426.6 27.94.2 4.112832.029.722.125.82.93.73.35.227.43.71628.325.121.219.82.23.22.94.823.63.376823.528.230.227.42.24.33.55.027.33.8QwenVL2.0256 12827.5 27.930.9 29.030.2 29.229.9 28.92.2 2.24.0 3.73.1 3.45.1 4.829.6 28.73.6 3.51626.422.623.925.41.83.03.14.624.63.1InterVL3128 1634.3 35.727.8 26.820.5 18.831.1 28.23.8 3.03.7 3.23.3 3.75.3 5.028.4 27.44.0 3.7100020.529.534.937.42.63.42.24.230.63.1VideoChat-Flash12818.030.033.536.82.42.82.24.029.62.91618.432.929.534.12.12.51.93.828.72.6verse alternatives: Qwen2.5-VL-7B-Instruct (open-source) and Gemini 2.0-flash (commercial). Pair-wise Pearson and Spearman correlations revealedstrong agreement: 1) GPT-4o-mini &amp; Gemini 2.0-flash: Pearson 0.97, Spearman 0.94 2) GPT-4o-Model / JudgeGPT-4o Gemini 2.0 Qwen 2.5GPT-4o6.777.676.76Gemini-2.05.616.765.78Goldfish (Mistral)4.644.535.06Qwen2.5VL4.564.585.36InternVL34.464.895.21Qwen2VL4.114.004.50InternVL23.904.014.50LLava-Onevision3.893.844.38Video-Flash3.463.843.73InternVL2.53.263.293.97MiniGPT4-video (Mistral)3.222.653.16LongVU3.003.243.41InternLM-XComposer2.993.063.28
mini &amp; Qwen2.5-VL:Pearson 0.95, Spearman 0.92 We also report a Krippendorff's alpha of 0.8014 across all judges, indicating high inter-rater reliability.These results demonstrate that GPT-4o is well-aligned with independent judges, supporting the robustness and impartiality of our evaluation.</p>
<p>Table 5 :
5
Ablation study demonstrates our LLM-based judge robustness.</p>
<p>Table 6 :
6
Analysis of the impact of filtering our benchmark by excluding the models' internal knowledge and emphasizing the visual questions.Random Per. is random performance.
ModelsFilterVideo SubtitlesMeta dataGlobalGrounding Skills Scene CharacterChronologicalSummar-Reasoning Skills Deep Context SpoilerLinkingAvg. Acc.Avg. ScoreAppearanceTransitionsActionsUnderstandingizationUnderstandingUnderstandingEventsRandom Per----20.020.020.020.0N/AN/AN/AN/A20.0 N/A✓✓✗51.842.943.577.26.36.86.77.553.96.8w/o Filtering✗✓✗29.139.646.882.07.27.05.97.549.46.9GPT4o✗✗✓25.726.139.156.53.45.23.97.436.85.0✓✓✗49.737.739.961.06.36.46.66.847.16.5w Filtering✗✓✗26.534.242.268.57.16.55.86.842.86.5✗✗✓22.219.931.935.43.34.83.56.627.44.6✓✓✗32.830.130.041.43.44.83.76.533.64.6w/o Filtering✗✓✗22.736.231.056.94.85.23.66.636.75.1Qwen2.5VL✗✗✓19.026.328.943.52.23.73.56.229.43.9✓✓✗30.025.322.720.43.34.33.45.424.64.1w Filtering✗✓✗18.424.221.523.94.24.13.75.421.94.3✗✗✓15.719.619.817.22.23.43.14.918.13.4✓✓✗35.927.824.841.23.94.13.66.232.44.4w/o Filtering✗✓✗30.834.324.846.54.24.23.86.434.04.7InternVL 3.0✗✗✓25.025.626.538.91.93.63.96.129.03.9✓✓✗34.327.820.531.13.83.73.35.228.44.0w Filtering✗✓✗29.231.220.833.94.23.93.55.428.84.2✗✗✓23.423.323.324.92.03.33.65.023.73.5✓✓✗23.227.627.041.12.04.03.26.129.73.9w/o Filtering✗✓✗20.827.829.843.44.05.13.76.630.44.8LlaVa-OneVision✗✗✓16.727.828.539.30.83.73.45.828.13.4✓✓✗21.124.723.330.32.03.72.75.124.83.4w Filtering✗✓✗18.524.525.728.93.94.53.35.624.44.3✗✗✓14.824.526.228.40.83.42.94.823.53.0Grounding SkillsReasoning SkillsModelsFrame RateGlobal AppearanceScene TransitionsCharacter ActionsChronological UnderstandingSummar-izationDeep Context UnderstandingSpoiler UnderstandingLinking EventsAvg. Acc.Avg. ScoreOverallRandom Per.N/A19.4816.6716.5240.56N/AN/AN/AN/A23.31 N/AN/AGPT-4o450 FPV54.2650.043.9649.666.26.495.617.0049.47 6.3356.36Gemini-2.01 FPS50.3950.7956.6136.605.705.403.415.9448.60 5.1149.86InternVL3128 FPV33.3328.5723.4636.293.913.793.415.2830.41 4.1035.69Qwen2VL768 FPV31.0123.8132.5948.412.264.232.935.1433.96 3.6435.18Qwen2.5VL768 FPV37.9822.2222.5327.173.224.133.675.3427.48 4.0934.19LLava-OneVision128 FPV37.2125.4020.1143.911.993.663.285.2131.66 3.5433.50Goldfish60 FPW20.9312.7021.9738.482.884.853.615.4823.52 4.2132.79InternVL2128 FPV24.0315.8722.1642.162.743.372.875.0226.06 3.5030.53VideoChat-Flash1000 FPV28.6820.6333.3329.732.453.242.354.2028.09 3.0629.35InternVL2.5128 FPV34.8820.6321.632.792.602.782.354.2027.48 2.9828.65InternLM-XComposer 16 FPW32.5614.2928.4930.111.722.502.114.1326.36 2.6226.26MiniGPT4-video60 FPV17.8311.1127.0040.12.042.872.153.3924.01 2.6125.07LongVU512 FPV20.9315.8721.4229.861.702.843.073.5322.02 2.7924.94</p>
<p>Table 7 :
7
InfiniBench leaderboard on the validation-set, across eight skills.Models are ranked by their equally weighted performance for Grounding and reasoning skills using this formula 0.5 • acc 100 + 0.5 • score 10 .FPV (Frames Per Video), FPS (Frames Per Second), and FPW (Frames Per Window) are reported.All models use both video and subtitles as inputs.Random Per. is random performance.
Grounding SkillsReasoning SkillsModelsFrame RateGlobal AppearanceScene TransitionsCharacter ActionsChronological UnderstandingSummar-izationDeep Context UnderstandingSpoiler UnderstandingLinking EventsAvg. Acc.Avg. ScoreOverallRandom Per.N/A20.0020.0020.0020.00N/AN/AN/AN/A20.00 N/AN/AGPT-4o450 FPV51.8342.9143.4977.156.326.826.737.4753.85 6.8461.10Gemini-2.01 FPS46.7344.5955.9467.625.806.564.366.1953.72 5.7355.50Qwen2.5VL768 FPV32.7830.1030.0441.363.354.843.716.4633.57 4.5939.74InternVL3128 FPV35.9327.7624.7541.173.864.063.576.1932.40 4.4238.30Qwen2VL768 FPV25.3731.3734.9136.682.234.873.596.0332.08 4.1836.94Goldfish60 FPW17.2824.4524.0329.323.025.453.616.5623.77 4.6635.19VideoChat-Flash1000 FPV21.4831.3737.4848.142.643.932.385.1034.62 3.5134.87LLava-OneVision128 FPV23.1527.5727.0441.072.044.043.186.1229.71 3.8534.08InternVL2128 FPV27.7226.1823.8930.472.893.743.165.9627.07 3.9433.22InternVL2.5128 FPV28.5827.1025.1834.002.463.102.305.1028.72 3.2430.56InternLM-XComposer 16 FPW22.5330.3330.3334.481.632.862.435.0229.42 2.9929.63LongVU512 FPV27.0422.3825.8921.491.703.263.004.1924.20 3.0427.29MiniGPT4-video60 FPV18.5225.8429.1825.792.093.092.213.8924.83 2.8226.52</p>
<p>Table 8 :
8
InfiniBench before filtration, Section 3.2.3,leaderboard across eight skills.Models are ranked by their equally weighted performance for Grounding and reasoning skills using this formula 0.5 • acc 100 + 0.5 • score 10 .FPV (Frames Per Video), FPS (Frames Per Second), and FPW (Frames Per Window) are reported.All models use both video and subtitles as inputs.Random Per. is random performance.
Models# FramesI don't know optionGlobalGrounding Skills Scene CharacterChronologicalAvg.AppearanceTransitionsActionsUnderstandingAcc.Qwen2.5-VL768✓ ✗30.0 31.425.3 23.322.7 23.920.3 24.324.6 25.7InternVL3.0128✓ ✗34.3 35.527.8 30.920.5 22.931.1 33.528.4 30.7Qwen2-VL768✓ ✗23.5 25.228.2 31.830.2 29.927.4 32.327.3 29.8Video-flash1000✓ ✗20.5 20.829.6 27.434.9 31.937.4 36.830.6 29.2InternLM-XC 16 FPW✓ ✗20.1 22.627.9 30.126.6 25.526.4 26.625.3 26.2InternVL2.5128✓ ✗27.1 27.725.1 27.721.2 22.229.2 28.925.6 26.7LongVU512✓ ✗27.7 27.721.0 23.528.0 29.519.0 23.523.9 26.0</p>
<p>Table 9 :
9
Analysis</p>
<p>of the impact of incorporating the "I don't know" option on model performance.Figure 6: Data verification Website.ments.These include: Chronological Understanding, Deep-Context Understanding, Spoiler Questions, Linking Multiple Events, Scene Transitions.These skills collectively test a model's capability in grounding, temporal reasoning, and narrative comprehension far beyond surface-level character recognition.</p>
<p>Table 10 :
10
Human verification for the keypoints used in the generation pipeline.The number of questions corresponds to the ones shown to the annotators, not the final count in our dataset.traceabilityandtrustworthiness of the verified set.Accuracy statistics across skill categories are reported in Table10, with an overall verification accuracy of 90.11%.Incorrect or ambiguous Q/A pairs were removed, and only validated examples were retained for the final test set.
Skill NameNumber of Questions Per Question Acc.(%)Character Actions137089.76Global Appearance75093.21Scene Transitions22485.23Chronological understandings22490.37Deep Context Understanding557494.14Linking Events458795.92Spoiler Questions15182.11</p>
<p>). Chandler goes to meet a career counselor.After 8 hrs. of aptitude, personality and intelligence tests he learns that he is fit for a career in data processing, for a large multinational corporation.he is disappointed as he always pictured himself doing something cool.When his boss calls and offers more money (&amp; more bonus.. Chandler resists, but the boss keeps throwing more and more numbers), Chandler caves and goes back to work.Chandler gets the corner office, and he shows it off to Phoebe.He has a view and an assistant.But Chandler has more responsibility now and starts spending more time &amp; late nights at work and yelling at his juniors.He doesn't like it.Ross has a date</p>
<p>with a beautiful colleague named Celia (Melora Hardin) (curator of insects at the museum) and gives new meaning to the term 'spanking the monkey' when she meets Marcel.The date goes bad when Marcel hands on Celia's hair and pulls it.Eventually Ross takes Celia to bed, and she wants him to talk dirty and he says 'Vulva'.Ross turns to Joey for advice as Celia wants him to talk dirty as foreplay.Joey gets Ross to practice on him..When Ross talks smack, Chandler overhears and amuses himself at their expense.Ross does well at the next date and talks very dirty (with theme, plot, motif and story-lines.at one point there were villagers), but eventually they get tired and cuddle.Phoebe takes out her anger at Steve at his next massage appointment by treating him to a bad massage (she elbows him on his back and pinches his skin so that it hurts).</p>
<p>For example, your response should look like this: [{\Q\: \Your question here...\, \A\: \Your answer here...},{\Q\: \Your question here...\, \A\: \Your answer here...}].Make sure to avoid to put double quotes inside string with double quotes, use single quotes instead.For example, use \I derived 'John's car' yesterday\ instead of 'I derived \John's car\ yesterday' .please only output the required format, do not include any additional information.Remember well the output format of ONLY a PYTHON LIST as output and DON'T output the python shell because I will use python ast library to parse your output list.## Few shot examples about the questions: -What is the influence of event A on event B? -How does event A lead to event B? -What is the relationship between event A and event B? -What is the impact of event A on event B? -What is the connection between event A, event B, and event C? User prompt: The user input is {summary}.Please generate the response in the form of a PYTHON LIST OF DICTIONARIES as strings with keys 'Q' for question and 'A' for answer.Each corresponding value should be the questionand-answer text respectively.For example, your response should look like this: [{'Q': 'Your question here...', 'A': 'Your answer here...'},{'Q': 'Your question here...', 'A': 'Your answer here...'}].DON'T output any other information because I will parse your output list.</p>
<p>Related WorkIn this section, we cover the existing video benchmarks and position our work among them. An
Temporal order of events:System prompt: You play two roles: a human asking questions related to a video and an intelligent chatbot designed to help people find information from a given video.##TASK: Users will provide an episode Screenplay Script.Your task is to extract the events from this Screenplay Script.Ensure that the events are listed in chronological order First read the Screenplay Script and think carefully to extract the all events.------##Few shot samples Episode Screenplay Script: {user Screenplay Script} Extract the events from this episode Screenplay Script: The response should be in the format: ['Event A', 'Event B', 'Event C', 'Event D',...], ensuring that the event B is after event A and before Event C. Remember well the output format of ONLY a PYTHON LIST of events and DON'T output the python shell because I will use python ast library to parse your output list.User prompt: Episode Screenplay Script: {script} Extract the events from the Screenplay Script in a list please provide the response in the format of PYTHON LIST of DON'T output any other information because I will parse your output list.DON'T output any ' or ' in your response but use /u2019 for ' and /u2019s for 's and /u2019t for 't and s/u2019 for s' or s'Scene transitions:System prompt: ##TASK: Users will provide an episode Screenplay Script.Your task is to extract scene transitions in from this script.First read the Screenplay Script and think carefully to extract the transitions.------##Few shot samples Episode Screenplay Script: {user Screenplay Script} Extract the scene transitions from this episode Screenplay Script: please provide the response in the format of PYTHON LIST of scene transitions like this example : ['scene A name', 'scene B name', 'scene C name',...], ensuring that the scene changed from A to B then C and so on.Remember well the output format of ONLY a PYTHON LIST of events and DON'T output the python shell because I will use python ast library to parse your output list.Scene names should be places name or location names where the scene is taking place such as home , cafe , bar , car and so on.User prompt:Episode Screenplay Script: {script} Extract the scene transitions from this Screenplay Script in a list please provide the response in the format of PYTHON LIST of scene transitions like this example : ['scene A name', 'scene B name', 'scene C name',...], ensuring that the scene changed from A to B then C and so on.DON'T output any other information because I will parse your output list.Deep context understanding:System prompt: You play two roles: a human asking questions related to a video and an intelligent chatbot designed to help people find information from a given video.##TASK: Your task is to first play the role of a human who asks questions related to deep context understanding in the video and then play the role of an AI assistant that provides information based on the video content.Users will provide human video summary and the video script,and you will generate a conversation-like question and answers pair specifically focusing on measuring the viewer's context understanding.##INSTRUCTIONS:-The questions must be conversational, as if a human is asking them, and should directly relate to deep context understanding for the video content.-The answers must be detailed, descriptive, and should directly reference the information provided.-The number of questions should be up to 20 questions and answers.-The questions should be tricky and hard to answer to measure the viewer's context understanding.-The answers must be detailed, descriptive, and should directly reference the information provided.-It will be good if most of the questions are related to the visual content of the video.-Again, the questions should be very tricky and hard to answer to measure the viewer's context understanding.Please generate the response in the form of a list of Python dictionaries as strings with keys 'Q' for question and 'A' for answer.Each corresponding value should be the questionand-answer text respectively.For example, your response should look like this: [{'Q': 'Your question here...', 'A': 'Your answer here...'},{'Q': 'Your question here...', 'A': 'Your answer here...'}].please only output the required format, do not include any additional information.If you want to type 's or 't and so on, please use \u2019s for 's and \u2019t for 't and so on.Test your output by using the python ast library to parse your output list.Remember well the output format of ONLY a PYTHON LIST as output User prompt: video summary: {caption}.video transcript: {script}.Please generate up to 20 questions and their answers in the form of list of Python dictionaries string with keys 'Q' for question and 'A' for answer.Each corresponding value should be the question-and-answer text respectively.For example, your response should look like this: [{'Q': 'Your question here...', 'A': 'Your answer here...'},{'Q': 'Your question here...', 'A': 'Your answer here...'}].
Kirolos Ataallah, Xiaoqian Shen, Eslam Abdelrahman, Essam Sleiman, Deyao Zhu, Jian Ding, Mohamed Elhoseiny, arXiv:2404.03413Minigpt4-video: Advancing multimodal llms for video understanding with interleaved visual-textual tokens. 2024aarXiv preprint</p>
<p>Goldfish: Vision-language understanding of arbitrarily long videos. Kirolos Ataallah, Xiaoqian Shen, Eslam Abdelrahman, Essam Sleiman, Mingchen Zhuge, Jian Ding, Deyao Zhu, Jürgen Schmidhuber, Mohamed Elhoseiny, arXiv:2407.126792024bPreprint</p>
<p>Ilker Kesen. Tayfun Ates, Cagatay Samil Atesoglu, Yigit, arXiv:2012.04293Mert Kobas, Erkut Erdem, Aykut Erdem, Tilbe Goksun, and Deniz Yuret. 2020. Craft: A benchmark for causal reasoning about forces and interactions. arXiv preprint</p>
<p>Qwen-vl: A versatile visionlanguage model for understanding, localization, text reading, and beyond. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, Jingren Zhou, arXiv:2308.129662023Preprint</p>
<p>Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, arXiv:2502.13923Yiheng Xu, and 8 others. 2025. Qwen2.5-vl technical report. Preprint</p>
<p>Activitynet: A large-scale video benchmark for human activity understanding. Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, Juan Carlos Niebles, Proceedings of the ieee conference on computer vision and pattern recognition. the ieee conference on computer vision and pattern recognition2015</p>
<p>Quo vadis, action recognition? a new model and the kinetics dataset. Joao Carreira, Andrew Zisserman, proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2017</p>
<p>Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation. Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, Zheng Liu, arXiv:2402.032162024aPreprint</p>
<p>Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, Mohamed Elhoseiny, arXiv:2310.09478Minigpt-v2: large language model as a unified interface for vision-language multi-task learning. 2023arXiv preprint</p>
<p>Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, arXiv:2412.05271Zhaoyang Liu, and 1 others. 2024barXiv preprint</p>
<p>Tv show transcripts. 2025-05-20</p>
<p>Mengdan Zhang, and 1 others. 2024. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, arXiv:2405.21075arXiv preprint</p>
<p>Google Gemini, Gemini technical report. 2024</p>
<p>Alexandros Lattas, and Stefanos Zafeiriou. 2021. Sample and computation redistribution for efficient face detection. Jia Guo, Jiankang Deng, arXiv:2105.04714arXiv preprint</p>
<p>Movienet: A holistic dataset for movie understanding. Qingqiu Huang, Yu Xiong, Anyi Rao, Jiaze Wang, Dahua Lin, Computer Vision-ECCV 2020: 16th European Conference. Glasgow, UKSpringer2020. August 23-28, 2020Proceedings, Part IV 16</p>
<p>Gazevqa: A video question answering dataset for multiview eye-gaze task. Muhammet Ilaslan, Chenan Song, Joya Chen, Difei Gao, Weixian Lei, Qianli Xu, Joo Lim, Mike Shou, 2023oriented collaborations</p>
<p>Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing</p>
<p>IMDB. Imdb website. InternVL2. Internvl2. </p>
<p>Tgif-qa: Toward spatiotemporal reasoning in visual question answering. Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, Gunhee Kim, arXiv:1704.044972017Preprint</p>
<p>Yolov11: An overview of the key architectural enhancements. Rahima Khanam, Muhammad Hussain, arXiv:2410.177252024Preprint</p>
<p>Qvhighlights: Detecting moments and highlights in videos via natural language queries. Jie Lei, Tamara L Berg, Mohit Bansal, arXiv:2107.096092021Preprint</p>
<p>Tvqa: Localized, compositional video question answering. Jie Lei, Licheng Yu, Mohit Bansal, Tamara L Berg, arXiv:1809.016962019Preprint</p>
<p>Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, Chunyuan Li, arXiv:2408.03326Llava-onevision: Easy visual task transfer. 2024aPreprint</p>
<p>Mvbench: A comprehensive multimodal video understanding benchmark. Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, Limin Wang, Yu Qiao, arXiv:2311.170052024bPreprint</p>
<p>Hero: Hierarchical encoder for video+ language omni-representation pretraining. Linjie Li, Yen-Chun Chen, Yu Cheng, Zhe Gan, Licheng Yu, Jingjing Liu, arXiv:2005.002002020arXiv preprint</p>
<p>Videochat-flash: Hierarchical compression for long-context video modeling. Xinhao Li, Yi Wang, Jiashuo Yu, Xiangyu Zeng, Yuhan Zhu, Haian Huang, Jianfei Gao, Kunchang Li, Yinan He, Chenting Wang, Yu Qiao, Yali Wang, Limin Wang, arXiv:2501.005742025Preprint</p>
<p>Llamavid: An image is worth 2 tokens in large language models. Yanwei Li, Chengyao Wang, Jiaya Jia, arXiv:2311.170432023Preprint</p>
<p>Tvr-ranking: A dataset for ranked video moment retrieval with imprecise queries. Renjie Liang, Li Li, Chongzhi Zhang, Jing Wang, Xizhou Zhu, Aixin Sun, arXiv:2407.065972024Preprint</p>
<p>Videoinsta: Zero-shot long video understanding via informative spatial-temporal reasoning with llms. Ruotong Liao, Max Erler, Huiyu Wang, Guangyao Zhai, Gengyuan Zhang, Yunpu Ma, Volker Tresp, Findings of the Association for Computational Linguistics: EMNLP 2024. Miami, Florida, USA2024Association for Computational Linguistics</p>
<p>Video-llava: Learning united visual representation by alignment before projection. Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Jin Peng, Li Yuan, arXiv:2311.101222023arXiv preprint</p>
<p>Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, arXiv:2304.08485Visual instruction tuning. 2023arXiv preprint</p>
<p>Video-xl-pro: Reconstructive token compression for extremely long video understanding. Xiangrui Liu, Yan Shu, Zheng Liu, Ao Li, Yang Tian, Bo Zhao, arXiv:2503.184782025arXiv preprint</p>
<p>Video-chatgpt: Towards detailed video understanding via large vision and language models. Muhammad Maaz, Hanoona Rasheed, Salman Khan, Fahad Shahbaz Khan, arXiv:2306.054242023arXiv preprint</p>
<p>Egoschema: A diagnostic benchmark for very long-form video language understanding. Karttikeya Mangalam, Raiymbek Akshulakov, Jitendra Malik, arXiv:2308.091262023arXiv preprint</p>
<p>Dynamic multistep reasoning based on video scene graph for video question answering. Jianguo Mao, Wenbin Jiang, Xiangdong Wang, Zhifan Feng, Yajuan Lyu, Hong Liu, Yong Zhu, Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies2022</p>
<p>Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, arXiv:2304.07193Ishan Misra, Michael Rabbat, Vasu Sharma, and 7 others. 2024. Dinov2: Learning robust visual features without supervision. 2024OpenAIPreprintGpt-4o technical report</p>
<p>Grounded situation recognition. Sarah Pratt, Mark Yatskar, Luca Weihs, Ali Farhadi, Aniruddha Kembhavi, Computer Vision-ECCV 2020: 16th European Conference. Glasgow, UKSpringer2020. August 23-28, 2020Proceedings, Part IV 16</p>
<p>Video object grounding using semantic roles in language description. Arka Sadhu, Kan Chen, Ram Nevatia, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2020</p>
<p>Ram Nevatia, and Aniruddha Kembhavi. 2021. Visual semantic role labeling for video understanding. Arka Sadhu, Tanmay Gupta, Mark Yatskar, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition</p>
<p>Deepseekmath: Pushing the limits of mathematical reasoning in open language models. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y K Li, Y Wu, Daya Guo, arXiv:2402.033002024Preprint</p>
<p>Mohamed Elhoseiny, and Vikas Chandra. 2024. Longvu: Spatiotemporal adaptive compression for long video-language understanding. Xiaoqian Shen, Yunyang Xiong, Changsheng Zhao, Lemeng Wu, Jun Chen, Chenchen Zhu, Zechun Liu, Fanyi Xiao, Balakrishnan Varadarajan, Florian Bordes, Zhuang Liu, Hu Xu, Hyunwoo J Kim, Bilge Soran, Raghuraman Krishnamoorthi, arXiv:2410.17434Preprint</p>
<p>Grounding semantic roles in images. Carina Silberer, Manfred Pinkal, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language Processing2018</p>
<p>Moviechat: From dense token to sparse memory for long video understanding. Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Xun Guo, Tian Ye, Yan Lu, arXiv:2307.164492023arXiv preprintJenq-Neng Hwang, and 1 others</p>
<p>Ucf101: A dataset of 101 human actions classes from videos in the wild. Khurram Soomro, Mubarak Amir Roshan Zamir, Shah, arXiv:1212.04022012Preprint</p>
<p>Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, Junyang Lin, arXiv:2409.121912024aPreprint</p>
<p>Weihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiaohan Zhang, Ji Qi, Xiaotao Gu, Shiyu Huang, Bin Xu, arXiv:2406.08035Yuxiao Dong, and 1 others. 2024b. Lvbench: An extreme long video understanding benchmark. arXiv preprint</p>
<p>Longvideobench: A benchmark for longcontext interleaved video-language understanding. Haoning Wu, Dongxu Li, Bei Chen, Junnan Li, The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2024</p>
<p>Next-qa:next phase of questionanswering to explaining temporal actions. Junbin Xiao, Xindi Shang, Angela Yao, Tat-Seng Chua, arXiv:2105.082762021Preprint</p>
<p>Xiangnan He, and Yueting Zhuang. 2017. Video question answering via gradually refined attention over appearance and motion. Binzhu Xie, Sicheng Zhang, Zitang Zhou, Bo Li, Yuanhan Zhang, Jack Hessel, Jingkang Yang, Ziwei Liu, ; Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, arXiv:2306.14899Proceedings of the 25th ACM international conference on Multimedia. the 25th ACM international conference on Multimedia2024PreprintFunqa: Towards surprising video comprehension</p>
<p>Activitynet-qa: A dataset for understanding complex web videos via question answering. Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, arXiv:1906.024672019PreprintYueting Zhuang, and Dacheng Tao</p>
<p>Videollama: An instruction-tuned audio-visual language model for video understanding. Hang Zhang, Xin Li, Lidong Bing, arXiv:2306.028582023aarXiv preprint</p>
<p>Hongjie Zhang, Yi Liu, Lu Dong, Yifei Huang, Zhen-Hua Ling, Yali Wang, Limin Wang, Yu Qiao, arXiv:2312.04817Movqa: A benchmark of versatile questionanswering for long-form movie understanding. 2023barXiv preprint</p>
<p>Pan Zhang, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Rui Qian, Lin Chen, Qipeng Guo, Haodong Duan, Bin Wang, Linke Ouyang, Songyang Zhang, Wenwei Zhang, Yining Li, Yang Gao, Peng Sun, Xinyue Zhang, Wei Li, Jingwen Li, arXiv:2407.03320Wenhai Wang, and 8 others. 2024. Internlm-xcomposer-2.5: A versatile large vision language model supporting long-contextual input and output. Preprint</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, 202336Advances in neural information processing systems</p>
<p>Mlvu: A comprehensive benchmark for multi-task long video understanding. Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, Zheng Liu, arXiv:2406.042642024Preprint</p>
<p>Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, Mohamed Elhoseiny, arXiv:2304.10592Minigpt-4: Enhancing vision-language understanding with advanced large language models. 2023arXiv preprint</p>
<p>Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Yuchen Hao Tian, Weijie Duan, Jie Su, Zhangwei Shao, Erfei Gao, Xuehui Cui, Yue Wang, Yangzhou Cao, Xingguang Liu, Hongjie Wei, Haomin Zhang, Wang, arXiv:2504.10479Weiye Xu, and 32 others. 2025a. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. Preprint</p>
<p>Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Yuchen Hao Tian, Weijie Duan, Su, arXiv:2504.10479Jie Shao, and 1 others. 2025b. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5345 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5345</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5345</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-111.html">extraction-schema-111</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-06a73ad09664435f8b3cd90293f4e05a047cf375</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/06a73ad09664435f8b3cd90293f4e05a047cf375" target="_blank">K-BERT: Enabling Language Representation with Knowledge Graph</a></p>
                <p><strong>Paper Venue:</strong> AAAI Conference on Artificial Intelligence</p>
                <p><strong>Paper TL;DR:</strong> This work proposes a knowledge-enabled language representation model (K-BERT) with knowledge graphs (KGs), in which triples are injected into the sentences as domain knowledge, which significantly outperforms BERT and reveals promising results in twelve NLP tasks.</p>
                <p><strong>Paper Abstract:</strong> Pre-trained language representation models, such as BERT, capture a general language representation from large-scale corpora, but lack domain-specific knowledge. When reading a domain text, experts make inferences with relevant knowledge. For machines to achieve this capability, we propose a knowledge-enabled language representation model (K-BERT) with knowledge graphs (KGs), in which triples are injected into the sentences as domain knowledge. However, too much knowledge incorporation may divert the sentence from its correct meaning, which is called knowledge noise (KN) issue. To overcome KN, K-BERT introduces soft-position and visible matrix to limit the impact of knowledge. K-BERT can easily inject domain knowledge into the models by being equipped with a KG without pre-training by itself because it is capable of loading model parameters from the pre-trained BERT. Our investigation reveals promising results in twelve NLP tasks. Especially in domain-specific tasks (including finance, law, and medicine), K-BERT significantly outperforms BERT, which demonstrates that K-BERT is an excellent choice for solving the knowledge-driven problems that require experts.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5345.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5345.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>K-BERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge-enabled Bidirectional Encoder Representation from Transformers (K-BERT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that converts KG triples into a knowledge-rich 'sentence tree' by injecting relevant triples as branches into the input sentence, then linearizes that tree into a token sequence while preserving original sentence structure via soft-position embeddings and controlling attention with a visible matrix and mask-self-attention so that pretrained BERT parameters can be reused.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Sentence-tree injection / linearized sentence-tree (K-Inject)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Given an input sentence, K-Query selects triples (wi, r, wk) for entities in the sentence from a KG. K-Inject stitches those triples as branches attached to the corresponding token node, producing a sentence tree (depth fixed to 1). The tree is flattened by inserting branch tokens immediately after their node (rearrangement by hard-position index) to produce a token sequence; soft-position embeddings are then assigned to restore original token order (allowing multiple tokens to share soft-position indices), and a visible matrix M marks which tokens are mutually visible (tokens in the same branch are visible; tokens in different branches are masked). A Mask-Transformer uses mask-self-attention (adds M to attention logits) so each token only attends to permitted tokens. This pipeline therefore converts KG structure into a textual token stream augmented with positional and visibility metadata for transformer consumption.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graph (triples: entity, relation, entity) — used CN-DBpedia, HowNet, MedicalKG</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Depth-1 branching (no iterative expansion), compatible with pretrained BERT vocab and parameters (avoids heterogeneous embedding-space by using same token embeddings), preserves original sentence structure via soft-position embeddings, interpretable and editable (KG entries are explicit), controlled knowledge flow via visible matrix (mitigates knowledge-noise), increases input length (branch tokens appended), mask-self-attention enforces locality of injected knowledge while allowing indirect propagation through original tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Fine-tuning across 12 Chinese NLP tasks: open-domain (sentiment classification: Book_review, Chnsenticorp, Shopping, Weibo; sentence-pair: XNLI, LCQMC; QA matching: NLPCC-DBQA; NER: MSRA-NER) and domain-specific tasks (Finance_Q&A, Law_Q&A, Finance_NER, Medicine_NER/CNER).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported metrics: accuracy for classification tasks (Acc.%); MRR% for NLPCC-DBQA; Precision/Recall/F1% for NER and Q&A. Representative numbers from paper: Medicine_NER (Google BERT F1 test 92.5% → K-BERT (MedicalKG) F1 test 94.2%); MSRA-NER (Google BERT F1 test 93.6% → K-BERT (CN-DBpedia) F1 test 95.7%); NLPCC-DBQA MRR test: Google BERT 93.3% → K-BERT (CN-DBpedia) 94.3%; XNLI Acc test: Google BERT 75.4% → K-BERT (HowNet) 76.1%. Also reports that K-BERT often converges faster (e.g., Law_Q&A peak at epoch 2 for K-BERT vs epoch 4 for BERT).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compared directly to baseline pretrained BERT variants (Google BERT; 'Our BERT' pre-trained on WikiZh+WebtextZh). K-BERT outperforms BERT especially on knowledge-driven, domain-specific tasks (1–2% absolute F1 gains reported) and sometimes on open-domain semantics tasks (HowNet better for semantic similarity; CN-DBpedia better for QA/NER). Ablation: removing soft-position or visible matrix reduces performance; removing visible matrix can make K-BERT worse than BERT in Law_Q&A (evidence of knowledge noise). Authors contrast K-BERT to approaches that simply use triples as corpus (COMET) or joint-embedding (word2vec+TransE) and argue K-BERT is more efficient and compatible with pretrained LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Knowledge Noise (KN) risk if visibility control absent or insufficient; depth limited to 1 (no iterative branch expansion), which constrains deeper graph context; injected branch tokens increase sequence length and computation; KG triple selection quality matters (authors note need for better K-Query filtering of unimportant triples as future work); authors avoid injecting KG during pretraining to prevent semantic collapse (close/equal vectors), so K-BERT relies on fine-tuning phase for KG use; potential memory/computation scaling concerns with very large KGs and large numbers of injected triples per sentence.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'K-BERT: Enabling Language Representation with Knowledge Graph', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5345.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5345.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>COMET (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>COMET: Commonsense transformers for automatic knowledge graph construction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A previously proposed approach that treats KG triples as text/corpus and uses them to train a language model (GPT) to learn commonsense knowledge directly from triples regarded as training sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>COMET: Commonsense transformers for automatic knowledge graph construction</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Triples-as-corpus linearization for LM pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Convert KG triples into linear textual sequences (treat triples as pseudo-sentences) and use that textualized corpus to train a generative LM (GPT), i.e., the KG is directly turned into training data for language modeling, rather than attached to natural sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graph (commonsense triples)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Simple and direct textualization of triples; does not explicitly preserve complex graph connectivity beyond the linearization; does not address heterogeneous embedding-space (relies on LM tokenization); easy to implement but may be inefficient and lose structural relations beyond single triple context.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Commonsense learning / KG completion and LM-based knowledge generation (as described by the cited COMET work); in this paper COMET is referenced as an approach to inject KG knowledge into LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported in this paper; authors of this paper state the approach (COMET) is 'very inefficient' when used to train GPT from triples (no numerical metrics provided here).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Authors contrast COMET unfavorably with K-BERT, noting COMET's inefficiency and that using triples as raw corpus is less efficient and less convenient than K-BERT's on-the-fly injection during fine-tuning which reuses pretrained BERT parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Described as inefficient for training large LMs directly from triples; likely to require large amounts of triple co-occurrence data to teach specific relations; may not preserve the relation to natural sentence context; authors imply structure and selective injection mechanisms (as in K-BERT) are advantageous.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'K-BERT: Enabling Language Representation with Knowledge Graph', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5345.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5345.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>THU-ERNIE (cited as ERNIE / THU-ERNIE)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ERNIE: Enhanced language representation with informative entities (THU-ERNIE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior approach that fuses entity information into the encoder by aggregating word and entity representations, aiming to inject knowledge into language models, but that work ignored explicit relations between entities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ERNIE: Enhanced language representation with informative entities</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Entity-fusion encoder modification (entity-aware encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Modify the transformer encoder to aggregate word-level and entity-level information so that entity signals are incorporated into contextual representations; does not convert graphs into text but fuses entity embeddings into LM internals.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graph entities (entity annotations), but relations not explicitly modeled in the cited approach</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Direct fusion of entity information into encoder; helps incorporate entity signals but ignores relation information between entities (limits expressive use of graph structure); may suffer from heterogeneous embedding-space if entities and words are embedded differently unless jointly learned.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Mentioned in related work as an approach to add KG information to LR models; not used in experiments in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported in this paper (THU-ERNIE only mentioned in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Authors position THU-ERNIE as a pioneer that fuses entities but note it ignores relations; K-BERT instead injects full triples (entities + relations) into sentence context and controls influence via soft-position and visible matrix.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Relations between entities ignored in THU-ERNIE according to the authors; potential HES if entity embeddings not aligned with word embeddings; may not be directly compatible with pretrained BERT parameters without architecture changes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'K-BERT: Enabling Language Representation with Knowledge Graph', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>COMET: Commonsense transformers for automatic knowledge graph construction <em>(Rating: 2)</em></li>
                <li>ERNIE: Enhanced language representation with informative entities <em>(Rating: 2)</em></li>
                <li>Knowledge graph and text jointly embedding <em>(Rating: 1)</em></li>
                <li>Representing text for joint embedding of text and knowledge bases <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5345",
    "paper_id": "paper-06a73ad09664435f8b3cd90293f4e05a047cf375",
    "extraction_schema_id": "extraction-schema-111",
    "extracted_data": [
        {
            "name_short": "K-BERT",
            "name_full": "Knowledge-enabled Bidirectional Encoder Representation from Transformers (K-BERT)",
            "brief_description": "A method that converts KG triples into a knowledge-rich 'sentence tree' by injecting relevant triples as branches into the input sentence, then linearizes that tree into a token sequence while preserving original sentence structure via soft-position embeddings and controlling attention with a visible matrix and mask-self-attention so that pretrained BERT parameters can be reused.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Sentence-tree injection / linearized sentence-tree (K-Inject)",
            "representation_description": "Given an input sentence, K-Query selects triples (wi, r, wk) for entities in the sentence from a KG. K-Inject stitches those triples as branches attached to the corresponding token node, producing a sentence tree (depth fixed to 1). The tree is flattened by inserting branch tokens immediately after their node (rearrangement by hard-position index) to produce a token sequence; soft-position embeddings are then assigned to restore original token order (allowing multiple tokens to share soft-position indices), and a visible matrix M marks which tokens are mutually visible (tokens in the same branch are visible; tokens in different branches are masked). A Mask-Transformer uses mask-self-attention (adds M to attention logits) so each token only attends to permitted tokens. This pipeline therefore converts KG structure into a textual token stream augmented with positional and visibility metadata for transformer consumption.",
            "graph_type": "Knowledge graph (triples: entity, relation, entity) — used CN-DBpedia, HowNet, MedicalKG",
            "representation_properties": "Depth-1 branching (no iterative expansion), compatible with pretrained BERT vocab and parameters (avoids heterogeneous embedding-space by using same token embeddings), preserves original sentence structure via soft-position embeddings, interpretable and editable (KG entries are explicit), controlled knowledge flow via visible matrix (mitigates knowledge-noise), increases input length (branch tokens appended), mask-self-attention enforces locality of injected knowledge while allowing indirect propagation through original tokens.",
            "evaluation_task": "Fine-tuning across 12 Chinese NLP tasks: open-domain (sentiment classification: Book_review, Chnsenticorp, Shopping, Weibo; sentence-pair: XNLI, LCQMC; QA matching: NLPCC-DBQA; NER: MSRA-NER) and domain-specific tasks (Finance_Q&A, Law_Q&A, Finance_NER, Medicine_NER/CNER).",
            "performance_metrics": "Reported metrics: accuracy for classification tasks (Acc.%); MRR% for NLPCC-DBQA; Precision/Recall/F1% for NER and Q&A. Representative numbers from paper: Medicine_NER (Google BERT F1 test 92.5% → K-BERT (MedicalKG) F1 test 94.2%); MSRA-NER (Google BERT F1 test 93.6% → K-BERT (CN-DBpedia) F1 test 95.7%); NLPCC-DBQA MRR test: Google BERT 93.3% → K-BERT (CN-DBpedia) 94.3%; XNLI Acc test: Google BERT 75.4% → K-BERT (HowNet) 76.1%. Also reports that K-BERT often converges faster (e.g., Law_Q&A peak at epoch 2 for K-BERT vs epoch 4 for BERT).",
            "comparison_to_other_representations": "Compared directly to baseline pretrained BERT variants (Google BERT; 'Our BERT' pre-trained on WikiZh+WebtextZh). K-BERT outperforms BERT especially on knowledge-driven, domain-specific tasks (1–2% absolute F1 gains reported) and sometimes on open-domain semantics tasks (HowNet better for semantic similarity; CN-DBpedia better for QA/NER). Ablation: removing soft-position or visible matrix reduces performance; removing visible matrix can make K-BERT worse than BERT in Law_Q&A (evidence of knowledge noise). Authors contrast K-BERT to approaches that simply use triples as corpus (COMET) or joint-embedding (word2vec+TransE) and argue K-BERT is more efficient and compatible with pretrained LMs.",
            "limitations_or_challenges": "Knowledge Noise (KN) risk if visibility control absent or insufficient; depth limited to 1 (no iterative branch expansion), which constrains deeper graph context; injected branch tokens increase sequence length and computation; KG triple selection quality matters (authors note need for better K-Query filtering of unimportant triples as future work); authors avoid injecting KG during pretraining to prevent semantic collapse (close/equal vectors), so K-BERT relies on fine-tuning phase for KG use; potential memory/computation scaling concerns with very large KGs and large numbers of injected triples per sentence.",
            "uuid": "e5345.0",
            "source_info": {
                "paper_title": "K-BERT: Enabling Language Representation with Knowledge Graph",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "COMET (as cited)",
            "name_full": "COMET: Commonsense transformers for automatic knowledge graph construction",
            "brief_description": "A previously proposed approach that treats KG triples as text/corpus and uses them to train a language model (GPT) to learn commonsense knowledge directly from triples regarded as training sentences.",
            "citation_title": "COMET: Commonsense transformers for automatic knowledge graph construction",
            "mention_or_use": "mention",
            "representation_name": "Triples-as-corpus linearization for LM pretraining",
            "representation_description": "Convert KG triples into linear textual sequences (treat triples as pseudo-sentences) and use that textualized corpus to train a generative LM (GPT), i.e., the KG is directly turned into training data for language modeling, rather than attached to natural sentences.",
            "graph_type": "Knowledge graph (commonsense triples)",
            "representation_properties": "Simple and direct textualization of triples; does not explicitly preserve complex graph connectivity beyond the linearization; does not address heterogeneous embedding-space (relies on LM tokenization); easy to implement but may be inefficient and lose structural relations beyond single triple context.",
            "evaluation_task": "Commonsense learning / KG completion and LM-based knowledge generation (as described by the cited COMET work); in this paper COMET is referenced as an approach to inject KG knowledge into LMs.",
            "performance_metrics": "Not reported in this paper; authors of this paper state the approach (COMET) is 'very inefficient' when used to train GPT from triples (no numerical metrics provided here).",
            "comparison_to_other_representations": "Authors contrast COMET unfavorably with K-BERT, noting COMET's inefficiency and that using triples as raw corpus is less efficient and less convenient than K-BERT's on-the-fly injection during fine-tuning which reuses pretrained BERT parameters.",
            "limitations_or_challenges": "Described as inefficient for training large LMs directly from triples; likely to require large amounts of triple co-occurrence data to teach specific relations; may not preserve the relation to natural sentence context; authors imply structure and selective injection mechanisms (as in K-BERT) are advantageous.",
            "uuid": "e5345.1",
            "source_info": {
                "paper_title": "K-BERT: Enabling Language Representation with Knowledge Graph",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "THU-ERNIE (cited as ERNIE / THU-ERNIE)",
            "name_full": "ERNIE: Enhanced language representation with informative entities (THU-ERNIE)",
            "brief_description": "A prior approach that fuses entity information into the encoder by aggregating word and entity representations, aiming to inject knowledge into language models, but that work ignored explicit relations between entities.",
            "citation_title": "ERNIE: Enhanced language representation with informative entities",
            "mention_or_use": "mention",
            "representation_name": "Entity-fusion encoder modification (entity-aware encoder)",
            "representation_description": "Modify the transformer encoder to aggregate word-level and entity-level information so that entity signals are incorporated into contextual representations; does not convert graphs into text but fuses entity embeddings into LM internals.",
            "graph_type": "Knowledge graph entities (entity annotations), but relations not explicitly modeled in the cited approach",
            "representation_properties": "Direct fusion of entity information into encoder; helps incorporate entity signals but ignores relation information between entities (limits expressive use of graph structure); may suffer from heterogeneous embedding-space if entities and words are embedded differently unless jointly learned.",
            "evaluation_task": "Mentioned in related work as an approach to add KG information to LR models; not used in experiments in this paper.",
            "performance_metrics": "Not reported in this paper (THU-ERNIE only mentioned in related work).",
            "comparison_to_other_representations": "Authors position THU-ERNIE as a pioneer that fuses entities but note it ignores relations; K-BERT instead injects full triples (entities + relations) into sentence context and controls influence via soft-position and visible matrix.",
            "limitations_or_challenges": "Relations between entities ignored in THU-ERNIE according to the authors; potential HES if entity embeddings not aligned with word embeddings; may not be directly compatible with pretrained BERT parameters without architecture changes.",
            "uuid": "e5345.2",
            "source_info": {
                "paper_title": "K-BERT: Enabling Language Representation with Knowledge Graph",
                "publication_date_yy_mm": "2019-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "COMET: Commonsense transformers for automatic knowledge graph construction",
            "rating": 2
        },
        {
            "paper_title": "ERNIE: Enhanced language representation with informative entities",
            "rating": 2
        },
        {
            "paper_title": "Knowledge graph and text jointly embedding",
            "rating": 1
        },
        {
            "paper_title": "Representing text for joint embedding of text and knowledge bases",
            "rating": 1
        }
    ],
    "cost": 0.011236,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>K-BERT: Enabling Language Representation with Knowledge Graph</h1>
<p>Weijie Liu, ${ }^{1}$ Peng Zhou, ${ }^{2}$ Zhe Zhao, ${ }^{2}$ Zhiruo Wang, ${ }^{3}$ Qi Ju, ${ }^{2, <em>}$ Haotang Deng, ${ }^{2}$ Ping Wang ${ }^{1, </em>}$<br>${ }^{1}$ Peking University, Beijing, China<br>${ }^{2}$ Tencent Research, Beijing, China<br>${ }^{3}$ Beijing Normal University, Beijing, China<br>{dataliu, pwang}@pku.edu.cn, SherronWang@gmail.com,<br>{rickzhou, nlpzhe, zhiruowang, damonju, haotangdeng}@tencent.com</p>
<h4>Abstract</h4>
<p>Pre-trained language representation models, such as BERT, capture a general language representation from large-scale corpora, but lack domain-specific knowledge. When reading a domain text, experts make inferences with relevant knowledge. For machines to achieve this capability, we propose a knowledge-enabled language representation model (K-BERT) with knowledge graphs (KGs), in which triples are injected into the sentences as domain knowledge. However, too much knowledge incorporation may divert the sentence from its correct meaning, which is called knowledge noise (KN) issue. To overcome KN, K-BERT introduces softposition and visible matrix to limit the impact of knowledge. K-BERT can easily inject domain knowledge into the models by being equipped with a KG without pre-training by itself because it is capable of loading model parameters from the pre-trained BERT. Our investigation reveals promising results in twelve NLP tasks. Especially in domain-specific tasks (including finance, law, and medicine), K-BERT significantly outperforms BERT, which demonstrates that K-BERT is an excellent choice for solving the knowledge-driven problems that require experts.</p>
<h2>Introduction</h2>
<p>Unsupervised pre-trained Language Representation (LR) models like BERT (Devlin et al. 2018) have achieved promising results in multiple NLP tasks. These models are pre-trained over large-scale open-domain corpora to obtain general language representations and then fine-tuned in specific downstream tasks for absorbing specific-domain knowledge. However, due to the domain-discrepancy between pre-training and fine-tuning, these models do not perform well on knowledge-driven tasks. For example, the Google BERT pre-trained on Wikipedia can not give full play to its value when dealing with electronic medical record (EMR) analysis task in the medical field.</p>
<p>When reading a text from a specific-domain, ordinary people can only comprehend words according to its context, while experts are able to make inferences with relevant domain knowledge. Publicly-provided models, like BERT,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>GPT (Radford et al. 2018), and XLNet (Yang et al. 2019), who were pre-trained over open-domain corpora, act just like an ordinary people. Even though they can refresh the state-of-the-art of GLUE (Wang et al. 2018) benchmark by learning from open-domain corpora, they may fail in some domain-specific tasks, due to little knowledge connection between specific and open domain. One way to solve this problem is to pre-train a model emphasizing domain-specific by ourselves, instead of using the publicly-provided ones. However, pre-training is time-consuming and computationally expensive, making it unacceptable to most users.</p>
<p>Moreover, although injecting domain-specific knowledge during pre-training is possible for LR models, this process of knowledge acquisition can be inefficient and expensive. For example, if we want the model to acquire the knowledge of "Paracetamol can treat cold", a large number of cooccurrences of "Paracetamol" and "cold" are required in the pre-training corpus. Instead of this strategy, what else can we do to make the model a domain expert? The knowledge graph (KG), which was called ontology in early research, serves as a good solution. As knowledge refined into a structured form, KGs over many fields have been constructed, e.g., SNOMED-CT (Bodenreider 2008) in medical field, HowNet (Dong, Dong, and Hao 2006) in Chinese conception. If KG can be integrated into the LR model, it will equip the model with domain knowledge, enhancing the model's performance over domain-specific tasks, while reducing the cost of pre-training on a large scale. Besides, the resulting models posses greater interpretability, for the injected knowledge is manually editable.</p>
<p>However, there are two challenges lies in the road of this knowledge integration: (1) Heterogeneous Embedding Space (HES): In general, the embedding vectors of words in text and entities in KG are obtained in separate ways, making their vector-space inconsistent; (2) Knowledge Noise (KN): Too much knowledge incorporation may divert the sentence from its correct meaning. To overcome these challenges, this paper propose a Knowledge-enabled Bidirectional Encoder Representation from Transformers (K-BERT). K-BERT is capable of loading any pre-trained BERT models due to they are identical in parameters. In addition, K-BERT can easily inject domain knowledge into</p>
<p>the models by equipped with a KG without pre-training. This characteristic of K-BERT is very convenient for users with limited computing resources. Experimental results on twelve Chinese NLP tasks demonstrate that the K-BERT gains superior performances on domain-specific tasks. The main contributions of this paper can be summarized as follows:</p>
<ul>
<li>This paper proposes a knowledge-enabled LR model, namely K-BERT, which is compatible with BERT and can incorporate domain knowledge without HES and KN issues;</li>
<li>With the delicate injection of KG, K-BERT significantly outperforms BERT not only on domain-specific tasks, but also plenty of those in the open-domain;</li>
<li>The codes of K-BERT and our self-developed knowledge graphs are publicly available at https://github.com/autoliuweijie/K-BERT.</li>
</ul>
<h2>Related Work</h2>
<p>Since Google Inc. launched BERT in 2018, many endeavors have been made for further optimization, basically focusing on the pre-training process and the encoder.</p>
<p>In optimizing pre-training process, Baidu-ERNIE (Sun et al. 2019) and BERT-WWM (Cui et al. 2019) adopt wholeword masking rather than single character masking for pretraining BERT in Chinese corpora. SpanBERT (Joshi et al. 2019) extended BERT by masking contiguous random spans and proposed a span boundary objective. RoBERTa (Liu et al. 2019) optimized the pre-training of BERT in three ways, i.e., deleting the target of the next sentence prediction, dynamically changing the masking strategy and using more and longer sentences for training. In optimizing the encoder of BERT, XLNet (Yang et al. 2019) replaced the Transformer in BERT with Transformer-XL (Dai et al. 2019) to improve its ability to process long sentences. THU-ERNIE (Zhang et al. 2019) modified the encoder of BERT to an aggregator for the mutual integration of word and entities.</p>
<p>While the pre-trained LR model is an emerging direction, there is little work on its fusion with KG. THU-ERNIE (Zhang et al. 2019) is a pioneer in this direction by fusing entity information, but the relations between entities are ignored by it. COMET (Bosselut et al. 2019) employed the triples in KG as corpus to train GPT (Radford et al. 2018) for common sense learning, which is very inefficient.</p>
<p>Before the emergence of pre-trained LR models, there were several studies that combined KG with word vectors. Wang et al. (2014) proposed a novel method of jointly embedding entities and words into the same continuous vector space basing on the idea of word2vec (Mikolov et al. 2013). Toutanova et al. (2015) proposed a model that captures the compositional structure of textual relations, and optimize entity, knowledge base, and textual relation representations in a joint manner. Han, Liu, and Sun (2016) applied a convolutional neural network and a KG completion task to learn the representation of text and knowledge jointly. Cao et al. (2018) carried out cross-lingual representation learning for words and entities via attentive distant supervision.</p>
<p>Input sentence: Tim Cook is currently visiting Beijing now
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The model structure of K-BERT: Compared to other RL models, the K-BERT is equipped with an editable KG, which can be adapted to its application domain. For example, for electronic medical record analysis, we can use a medical KG to grant the K-BERT with medical knowledge.</p>
<p>The major weakness of these methods is that they are still based on the idea of "word2vec + transE" (Bordes et al. 2013), rather than the pre-trained LR model. Although they use the method of joint representation to make the vector space of entities and words closer, there are still HES problems. What's more, for KGs with millions of entities, this idea makes the entity table very large, making it unusable because it exceeds the GPU's memory size.</p>
<h2>Methodology</h2>
<p>In this section, we detail the implementation of K-BERT and its overall framework is presented in Figure 1.</p>
<h2>Notation</h2>
<p>We denote a sentence $s=\left[w_{0}, w_{1}, w_{2}, \ldots, w_{n}\right]$ as a sequence of tokens, where $n$ is the length of this sentence. In this paper, English tokens are taken at the word-level, while Chinese tokens are at character-level. Each token $w_{i}$ is included in the vocabulary $\mathbb{V}, w_{i} \in \mathbb{V}$. KG, denoted as $\mathbb{K}$, is a collection of triples $\varepsilon=\left(w_{i}, r_{j}, w_{k}\right)$, where $w_{i}$ and $w_{k}$ are the name of entities, and $r_{j} \in \mathbb{V}$ is the relation between them. All the triples are in KG.</p>
<h2>Model architecture</h2>
<p>As shown in Figure 1, the model architecture of K-BERT consists of four modules, i.e., knowledge layer, embedding layer, seeing layer and mask-transformer. For an input sentence, the knowledge layer first injects relevant triples into</p>
<p>Embedding Representation
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The process of converting a sentence tree into an embedding representation and a visible matrix. In the sentence tree, the red number is the soft-position index, and the gray is the hard-position index. (1) For token embedding, the tokens in the sentence tree are flattened into a sequence of token embedding by their hard-position index; (2) The soft-position index is used as position embedding along with the token embedding; (3) In segment embedding, all the tokens in the fist sentence are tagged as "A"; (4) In the visible matrix, red means visible, and white means invisible. For example, the cell at row 4 , column 9 is white means that the "Apple(4)" cannot see "China(9)".
it from a KG, transforming the original sentence into a knowledge-rich sentence tree. The sentence tree is then simultaneously fed into the embedding layer and the seeing layer and then converted to a token-level embedding representation and a visible matrix. The visible matrix is used to control the visible area of each token, preventing changing the meaning of the original sentence due to too much knowledge injected.</p>
<h2>Knowledge layer</h2>
<p>The knowledge layer (KL) is used for sentence knowledge injection and sentence tree conversion. Specifically, given an input sentence $s=\left[w_{0}, w_{1}, w_{2}, \ldots, w_{n}\right]$ and a KG $\mathbb{K}$, KL outputs a sentence tree $t=$ $\left[w_{0}, w_{1}, \ldots, w_{i}\left[\left(r_{i 0}, w_{i 0}\right), \ldots,\left(r_{i k}, w_{i k}\right)\right], \ldots, w_{n}\right]$. This process can be divided into two steps: knowledge query (KQuery) and knowledge injection (K-Inject).</p>
<p>In K-Query, all the entity names involved in the sentence $s$ are selected to query their corresponding triples from $\mathbb{K}$. K-Query can be formulated as (1),</p>
<p>$$
E=K _Q u e r y(s, \mathbb{K})
$$</p>
<p>where $E=\left[\left(w_{i}, r_{i 0}, w_{i 0}\right), \ldots,\left(w_{i}, r_{i k}, w_{i k}\right)\right]$ is a collection of the corresponding triples.</p>
<p>Next, K-Inject injects the queried $E$ into the sentence $s$ by stitching the triples in $E$ to their corresponding position, and generates a sentence tree $t$. The structure of $t$ is illustrated in Figure 3.</p>
<p>In this paper, a sentence tree can have multiple branches, but its depth is fixed to 1 , which means that the entity names
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Structure of the sentence tree.
in triples will not derive branches iteratively. K-Inject can be formulated as (2),</p>
<p>$$
t=K _I n j e c t(s, E)
$$</p>
<h2>Embedding layer</h2>
<p>The function of the Embedding Layer (EL) is to convert the sentence tree into an embedding representation that can be fed into the Mask-Transformer. Similar to BERT, the embedding representation of K-BERT is the sum of three parts: token embedding, position embedding, and segment embedding, but differs in that the input of the K-BERT's embedding layer is a sentence tree, rather than a token sequence. Therefore, how to transform a sentence tree into a sequence while retaining its structural information is the key to KBERT.</p>
<p>Token embedding In this work, the token embedding is consistent with BERT, and the vocabulary provided by Google BERT is adopted in this paper. Each token in the sentence tree is converted to an embedding vector of dimension $H$ via a trainable lookup table. In addition, K-BERT</p>
<p>also uses $[C L S]$ as a classification tag and uses $[M A S K]$ to mask tokens. The difference between the token embeddings of K-BERT and BERT is that the tokens in the sentence tree require re-arrangement before the embedding operation. In our re-arrange strategy, tokens in the branch are inserted after the corresponding node, while subsequent tokens are moved backwards. As the example illustrated in Figure 2, the sentence tree is rearranged as "Tim Cook CEO Apple is visiting Beijing capital China is_a City now". Although this procedure is simple, but it makes the sentence unreadable and lost correct structural information. Fortunately, it can be solved by the soft-position and visible matrix.</p>
<p>Soft-position embedding For BERT, if there is no position embedding, it will be equivalent to a bag-of-word model, resulting in a lack of structural information (i.e., the order of tokens). All the structural information of the BERT's input sentence is contained in the position embedding, which allows us to add the missing structural information back to the unreadable rearranged sentence. Taking the sentence tree in Figure 2 as an example, after rearranging, $[C E O]$ and $[A p p l e]$ are inserted between $[C o o k]$ and $[i s]$, but the subject of $[i s]$ should be $[C o o k]$ instead of [Apple]. To solve this problem, we only need to set the position number of $[i s]$ to 3 instead of 5 . So when calculating the self-attention score in the transformer encoder, $[i s]$ is at the next position of $[C o o k]$ by the equivalent. However, another problem arises: the position numbers of $[i s]$ and $[C E O]$ are both 3 , which makes them close in position when calculating self-attention, but in fact, there is no connection between them. The solution to this problem is Mask-Self-Attention, which will be covered in the next subsection.</p>
<p>Segment embedding Like BERT, K-BERT also uses segmentation embedding to identify different sentences when multiple sentences are included. For example, when two sentences $\left[w_{00}, w_{01}, \ldots, w_{0 n}\right]$ and $\left[w_{10}, w_{11}, \ldots, w_{1 m}\right]$ are inputed, they are combined into one sentence $\left[[C L S], w_{00}, w_{01}, \ldots, w_{0 n},[S E P], w_{10}, w_{11}, \ldots, w_{1 m}\right]$ with $[S E P]$. For the combined sentence, it is marked with a sequence of segment tags, $[A, A, A, A, \ldots, A, B, B, \ldots, B]$.</p>
<h2>Seeing layer</h2>
<p>Seeing layer is the biggest difference between K-BERT and BERT, and also what makes this method so effective. The input to K-BERT is a sentence tree, where the branch is the knowledge gained from KG. However, the risk raised with knowledge is that it can lead to changes in the meaning of the original sentence, i.e., KN issue. For example, in the sentence tree in Figure 2, [China] only modifies [Beijing] and has nothing to do with [Apple]. Therefore, the representation of [Apple] should not be affected by [China]. On the other hand, the $[C L S]$ tag used for classification should not bypass the $[C o o k]$ to get the information of $[A p p l e]$, as this would bring the risk of semantic changes. To prevent this from happening, K-BERT's use a visible matrix $M$ to limit the visible area of each token so that [Apple] and [China], $[C L S]$ and $[A p p l e]$ are not visible to each other. The visible
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Illustration of the Mask-Transformer, which is a stack of multiple mask-self-attention blocks.
matrix $M$ is defined as (3),</p>
<p>$$
M_{i j}=\left{\begin{array}{cc}
0 &amp; w_{i} \ominus w_{j} \
-\infty &amp; w_{i} \oslash w_{j}
\end{array}\right.
$$</p>
<p>where, $w_{i} \ominus w_{j}$ indicates that $w_{i}$ and $w_{j}$ are in the same branch, while $w_{i} \oslash w_{j}$ are not. $i$ and $j$ are the hard-position index.</p>
<h2>Mask-Transformer</h2>
<p>To some degree, the visible matrix $M$ contains the structural information of the sentence tree. The Transformer (Vaswani et al. 2017) encoder in BERT cannot receive $M$ as an input, so we need to modify it to Mask-Transformer, which can limit the self-attention region according to $M$. Mask-Transformer is a stack of multiple mask-self-attention blocks. As BERT, we denote the number of layers (i.e., mask-self-attention blocks) as $L$, the hidden size as $H$, and the number of mask-self-attention heads as $A$.
Mask-Self-Attention: To prevent false semantic changes by taking advantage of the sentence structure information in $M$, we propose a mask-self-attention, which is an extension of self-attention. Formally, the mask-self-attention is (4).</p>
<p>$$
\begin{gathered}
Q^{i+1}, K^{i+1}, V^{i+1}=h^{i} W_{q}, h^{i} W_{k}, h^{i} W_{v} \
S^{i+1}=\operatorname{softmax}\left(\frac{Q^{i+1} K^{i+1}{ }^{\top}+M}{\sqrt{d_{k}}}\right) \
h^{i+1}=S^{i+1} V^{i+1}
\end{gathered}
$$</p>
<p>where $W_{q}, W_{k}$ and $W_{v}$ are trainable model parameters. $h^{i}$ is the hidden state of the $i$-th mask-self-attention blocks. $d_{k}$ is the scaling factor ${ }^{1} . M$ is the visible matrix calculated by the seeing layer. Intuitively, if $w_{k}$ is invisible to $w_{j}$, the $M_{j k}$ will mask the attention score $S_{j k}^{i+1}$ to 0 , which means $w_{k}$ make no contribution to the hidden state of $w_{j}$.</p>
<p>As the example illustrated in Figure 4, $h_{\text {[Apple] }}^{i}$ has no effect on $h_{\text {[CLS] }}^{i+1}$, because [Apple] is invisible to [CLS]. However, $h_{\text {[CLS] }}^{i+1}$ can obtain the information of $h_{\text {[Apple] }}^{i-1}$ indirectly</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>through $h_{[C o o k]}^{i+1}$, because $[A p p l e]$ is visible to $[C o o k]$ and $[C o o k]$ is visible to $[C L S]$. The advantage of this process is that $[A p p l e]$ enriches the representation of $[C o o k]$, but does not directly affect the meaning of the original sentence.</p>
<h2>Experiments</h2>
<p>In this section, we present the details of the K-BERT finetuning results on twelve Chinese NLP tasks, among which eight are open-domain, and four are specific-domain.</p>
<h2>Pre-training corpora</h2>
<p>In this paper, we adopt two Chinese corpora for pre-training, i.e., WikiZh ${ }^{2}$ and WebtextZh ${ }^{3}$.</p>
<ul>
<li>WikiZh WikiZh refers to the Chinese Wikipedia corpus, which is used to train Chinese BERT in (Devlin et al. 2018). WikiZh contains a total of 1 million well-formed Chinese entries with 12 million sentences and size of 1.2 G .</li>
<li>WebtextZh WebtextZh is a large-scale, high-quality Chinese question and answer ( $\mathrm{Q} \&amp; \mathrm{~A}$ ) corpus with 4.1 million entries and a size of 3.7 G . Each entry in WebtextZh belongs to a topic, with a total of 28,000 topics.</li>
</ul>
<h2>Knowledge graph</h2>
<p>We employ three Chinese KGs, CN-DBpedia ${ }^{4}$, HowNet ${ }^{5}$ and MedicalKG.</p>
<ul>
<li>CN-DBpedia CN-DBpedia (Xu et al. 2017) is a largescale open-domain encyclopedic KG developed by the Knowledge Work Laboratory of Fudan University, covering tens of millions of entities and hundreds of millions of relations. In this paper, we refine the official CN-DBpedia by eliminating those triples whose entity names are less than 2 in length or contain special characters. The refined CN-DBpedia contains a total of 5.17 million triples.</li>
<li>HowNet HowNet is a large-scale language knowledge base for Chinese vocabulary and concepts (Dong, Dong, and Hao 2006), in which each Chinese word is annotated with semantic units called sememes. If we take {word, contain, sememes $}$ as a triple, HowNet is a language KG. Similarly, we refine the official HowNet by eliminating those triples whose entity names are less than 2 in length or contain special characters. The refined HowNet contains a total of 52,576 triples.</li>
<li>MedicalKG MedicalKG is our self-developed Chinese medical concept KG, which contains four types of hypernym (symptoms, diseases, parts, and treatments). MedicalKG contains a total of 13,864 triples and is open source as part of K-BERT.</li>
</ul>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>Baselines</h2>
<p>In this paper, we compare K-BERT to two baselines:</p>
<ul>
<li>Google BERT ${ }^{6}$ The model was pre-trained on WikiZh and released by Google (Devlin et al. 2018).</li>
<li>Our BERT Our reimplementation of BERT with pretraining on WikiZh and WebtextZh, released in UER ${ }^{7}$.</li>
</ul>
<h2>Parameter settings and training details</h2>
<p>To reflect the role of KG in the RL model, we configure our K-BERT and BERT to the same parameter settings according to the basic version of Google BERT (Devlin et al. 2018). We denote the number of (mask-)self-attention layers and heads as $L$ and $A$ respectively, and the hidden dimension of embedding vectors as $H$. In detail, we have the following model configuration: $L=12, A=12$ and $H=768$. The total amounts of trainable parameters of both BERT and KBERT are the same (110M), which means that they are compatible with each other in model parameters.</p>
<p>For K-BERT pre-training, all settings are consistent with (Devlin et al. 2018). One thing to emphasize is that we don't add any KG to K-BERT during the pre-training phase. Because KG binds two related entity names together, thus making the pre-trained word vectors of the two are very close or even equal and resulting in a semantic loss. Therefore, in the pre-training phase, K-BERT and BERT are equivalent, and the latter's parameters can be assigned to the former. KG will be enabled during the fine-tuning and inferring phases.</p>
<h2>Open-domain tasks</h2>
<p>We first compare the performance of K-BERT with BERT on eight Chinese open-domain NLP tasks. Among these eight tasks, Book_review ${ }^{8}$, Chnsenticorp ${ }^{9}$, Shopping ${ }^{10}$, and Weibo ${ }^{11}$ are single-sentence classification tasks:</p>
<ul>
<li>Book_review This dataset contains 20,000 positive and 20,000 negative reviews collected from Douban ${ }^{12}$;</li>
<li>Chnsenticorp Chnsenticorp is a hotel review dataset with a total of 12,000 reviews, including 6,000 positive reviews and 6,000 negative reviews;</li>
<li>Shopping Shopping is a online shopping review dataset that contains 40,000 reviews, including 21,111 positive reviews and 18,889 negative reviews;</li>
<li>Weibo Weibo is a dataset with emotional annotations from Sina Weibo, including 60,000 positive samples and 60,000 negative samples.
XNLI (Conneau et al. 2018), LCQMC (Liu et al. 2018) are two-sentence classification tasks, NLPCC-DBQA ${ }^{13}$ is a Q\&amp;A matching task, and MSRA-NER (Levow 2006) is a Named Entity Recognition (NER) task:</li>
</ul>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>Table 1: Results of various models on sentence classification tasks on open-domain tasks (Acc. \%)</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Models $\backslash$ Datasets</th>
<th style="text-align: center;">Book_review</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Chnsenticorp</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Shopping</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Weibo</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">XNLI</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">LCQMC</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Dev</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">Dev</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">Dev</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">Dev</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">Dev</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">Dev</td>
<td style="text-align: center;">Test</td>
</tr>
<tr>
<td style="text-align: center;">Pre-trainied on WikiZh by Google.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Google BERT</td>
<td style="text-align: center;">88.3</td>
<td style="text-align: center;">87.5</td>
<td style="text-align: center;">93.3</td>
<td style="text-align: center;">94.3</td>
<td style="text-align: center;">96.7</td>
<td style="text-align: center;">96.3</td>
<td style="text-align: center;">98.2</td>
<td style="text-align: center;">98.3</td>
<td style="text-align: center;">76.0</td>
<td style="text-align: center;">75.4</td>
<td style="text-align: center;">88.4</td>
<td style="text-align: center;">86.2</td>
</tr>
<tr>
<td style="text-align: center;">K-BERT (HowNet)</td>
<td style="text-align: center;">88.6</td>
<td style="text-align: center;">87.2</td>
<td style="text-align: center;">94.6</td>
<td style="text-align: center;">95.6</td>
<td style="text-align: center;">97.1</td>
<td style="text-align: center;">97.0</td>
<td style="text-align: center;">98.3</td>
<td style="text-align: center;">98.3</td>
<td style="text-align: center;">76.8</td>
<td style="text-align: center;">76.1</td>
<td style="text-align: center;">88.9</td>
<td style="text-align: center;">86.9</td>
</tr>
<tr>
<td style="text-align: center;">K-BERT (CN-DBpedia)</td>
<td style="text-align: center;">88.6</td>
<td style="text-align: center;">87.3</td>
<td style="text-align: center;">93.9</td>
<td style="text-align: center;">95.3</td>
<td style="text-align: center;">96.6</td>
<td style="text-align: center;">96.5</td>
<td style="text-align: center;">98.3</td>
<td style="text-align: center;">98.3</td>
<td style="text-align: center;">76.5</td>
<td style="text-align: center;">76.0</td>
<td style="text-align: center;">88.6</td>
<td style="text-align: center;">87.0</td>
</tr>
<tr>
<td style="text-align: center;">Pre-trained on WikiZh and WebtextZh by us.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Our BERT</td>
<td style="text-align: center;">88.6</td>
<td style="text-align: center;">87.9</td>
<td style="text-align: center;">94.8</td>
<td style="text-align: center;">95.7</td>
<td style="text-align: center;">96.9</td>
<td style="text-align: center;">97.1</td>
<td style="text-align: center;">98.2</td>
<td style="text-align: center;">98.2</td>
<td style="text-align: center;">77.0</td>
<td style="text-align: center;">76.3</td>
<td style="text-align: center;">89.0</td>
<td style="text-align: center;">86.7</td>
</tr>
<tr>
<td style="text-align: center;">K-BERT (HowNet)</td>
<td style="text-align: center;">88.5</td>
<td style="text-align: center;">87.4</td>
<td style="text-align: center;">95.4</td>
<td style="text-align: center;">95.6</td>
<td style="text-align: center;">96.9</td>
<td style="text-align: center;">96.9</td>
<td style="text-align: center;">98.3</td>
<td style="text-align: center;">98.4</td>
<td style="text-align: center;">77.2</td>
<td style="text-align: center;">77.0</td>
<td style="text-align: center;">89.2</td>
<td style="text-align: center;">87.1</td>
</tr>
<tr>
<td style="text-align: center;">K-BERT (CN-DBpedia)</td>
<td style="text-align: center;">88.8</td>
<td style="text-align: center;">87.9</td>
<td style="text-align: center;">95.0</td>
<td style="text-align: center;">95.8</td>
<td style="text-align: center;">97.1</td>
<td style="text-align: center;">97.0</td>
<td style="text-align: center;">98.3</td>
<td style="text-align: center;">98.3</td>
<td style="text-align: center;">76.2</td>
<td style="text-align: center;">75.9</td>
<td style="text-align: center;">89.0</td>
<td style="text-align: center;">86.9</td>
</tr>
</tbody>
</table>
<p>Table 2: Results of various models on NLPCC-DBQA $(M R R \%)$ and MSRA-NER ( $F 1 \%$ ).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Models $\backslash$ Datasets</th>
<th style="text-align: center;">NLPCC-DBQA</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MSRA-NER</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Dev</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">Dev</td>
<td style="text-align: center;">Test</td>
</tr>
<tr>
<td style="text-align: center;">Pre-trained on WikiZh by Google.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Google BERT</td>
<td style="text-align: center;">93.4</td>
<td style="text-align: center;">93.3</td>
<td style="text-align: center;">94.5</td>
<td style="text-align: center;">93.6</td>
</tr>
<tr>
<td style="text-align: center;">K-BERT (HowNet)</td>
<td style="text-align: center;">93.2</td>
<td style="text-align: center;">93.1</td>
<td style="text-align: center;">95.8</td>
<td style="text-align: center;">94.5</td>
</tr>
<tr>
<td style="text-align: center;">K-BERT (CN-DBpedia)</td>
<td style="text-align: center;">94.5</td>
<td style="text-align: center;">94.3</td>
<td style="text-align: center;">96.6</td>
<td style="text-align: center;">95.7</td>
</tr>
<tr>
<td style="text-align: center;">Pre-trained on WikiZh and WebtextZh by us.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Our BERT</td>
<td style="text-align: center;">93.3</td>
<td style="text-align: center;">93.6</td>
<td style="text-align: center;">95.7</td>
<td style="text-align: center;">94.6</td>
</tr>
<tr>
<td style="text-align: center;">K-BERT (HowNet)</td>
<td style="text-align: center;">93.2</td>
<td style="text-align: center;">93.1</td>
<td style="text-align: center;">96.3</td>
<td style="text-align: center;">95.6</td>
</tr>
<tr>
<td style="text-align: center;">K-BERT (CN-DBpedia)</td>
<td style="text-align: center;">93.6</td>
<td style="text-align: center;">94.2</td>
<td style="text-align: center;">96.4</td>
<td style="text-align: center;">95.6</td>
</tr>
</tbody>
</table>
<ul>
<li>XNLI XNLI is a cross-language language understanding dataset in which each entry contains two sentences and the task is to determine their relation ("Entailment", "Contradict" or "Neutral" );</li>
<li>LCQMC LCQMC is a large-scale Chinese question matching corpus. The goal of this task is to determine if the two questions have a similar intent;</li>
<li>NLPCC-DBQA NLPCC-DBQA is a task to predict answers to each question from the given document;</li>
<li>MSRA-NER MSRA-NER is a NER dataset published by Microsoft. This task is to recognize the entity names in the text, including person names, place names, organization names, etc.
Each of the above datasets is divided into three parts: train, dev, and test. We use the train part to fine-tune the model and then evaluate its performance on the $d e v$ and test parts. The experimental results are shown in Table 1 and 2 , from which the results can be divided into three categories: (1) The KG has no significant effect on the tasks of sentiment analysis (i.e., Book_review, Chnsenticorp, Shopping and Weibo) because the sentiment of a sentence can be judged based on emotional words without any knowledge; (2) The language KG (HowNet) performs better than the
encyclopedic KG in terms of semantic similarity tasks (i.e., XNLI and LCQMC); (3) For Q\&amp;A and NER tasks (i.e., NLPCC-DBQA and MSRA-NER), the encyclopedic KG (CN-DBpedia) is more suitable than the language KG. Therefore, it is important to choose the right KG based on the type of task.</li>
</ul>
<p>In addition, it can be observed that the use of an additional corpus (WebtextZh) can also bring performance boost, but not as significant as KG. As MSRA-NER shown in Table 2, the CN-DBpedia improves $F 1$ from $93.6 \%$ to $95.7 \%$, while the WebtextZh only increases it to $94.6 \%$.</p>
<h2>Specific-domain tasks</h2>
<p>In fact, the task where K-BERT really shines is in specificdomain. Because KG is good at giving LR model with domain knowledge.</p>
<p>Domain Q\&amp;A We crawl about 770,000 and 36,000 Q\&amp;A samples from Baidu Zhidao ${ }^{14}$ in financial and legal domains, including questions, netizen answers, and best answers. Based on this, we built two datasets, i.e., Finance_Q\&amp;A and Law_Q\&amp;A. The task is to choose the best answer for the question from the netizen's answers.</p>
<p>Domain NER Finance_NER ${ }^{15}$ is a dataset including 3000 financial news articles manually labeled with over 65,000 name entities (people, location and organization). Medicine_NER is the Clinical Named Entity Recognition (CNER) task released in CCKS $2017^{16}$. The goal is to extract medical-related entity names from electronic medical records.</p>
<p>Similarly, the specific-domain datasets are split into three parts: train, dev, and test, which are used to fine-tune, select and test model, respectively. The test results of various models are illustrated in Table 3, where $P$., $R$. and $F 1$ refer to Precision, Recall and F1-score, respectively. Compared with BERT, K-BERT has a significant performance improvement in terms of domain tasks. As for $F 1$,</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 3: Results of various models on specific-domain tasks (\%).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Models $\backslash$ Datasets</th>
<th style="text-align: center;">Finance_Q\&amp;A</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Law_Q\&amp;A</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Finance.NER</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Medicine.NER</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$P$.</td>
<td style="text-align: center;">R.</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">$P$.</td>
<td style="text-align: center;">R.</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">$P$.</td>
<td style="text-align: center;">R.</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">$P$.</td>
<td style="text-align: center;">R.</td>
<td style="text-align: center;">F1</td>
</tr>
<tr>
<td style="text-align: center;">Pre-trained on WikiZh by Google.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Google BERT</td>
<td style="text-align: center;">81.9</td>
<td style="text-align: center;">86.0</td>
<td style="text-align: center;">83.9</td>
<td style="text-align: center;">83.1</td>
<td style="text-align: center;">90.1</td>
<td style="text-align: center;">86.4</td>
<td style="text-align: center;">84.8</td>
<td style="text-align: center;">87.4</td>
<td style="text-align: center;">86.1</td>
<td style="text-align: center;">91.9</td>
<td style="text-align: center;">93.1</td>
<td style="text-align: center;">92.5</td>
</tr>
<tr>
<td style="text-align: center;">K-BERT (HowNet)</td>
<td style="text-align: center;">83.3</td>
<td style="text-align: center;">84.4</td>
<td style="text-align: center;">83.9</td>
<td style="text-align: center;">83.7</td>
<td style="text-align: center;">91.2</td>
<td style="text-align: center;">87.3</td>
<td style="text-align: center;">86.3</td>
<td style="text-align: center;">89.0</td>
<td style="text-align: center;">87.6</td>
<td style="text-align: center;">93.2</td>
<td style="text-align: center;">93.3</td>
<td style="text-align: center;">93.3</td>
</tr>
<tr>
<td style="text-align: center;">K-BERT (CN-DBpedia)</td>
<td style="text-align: center;">81.5</td>
<td style="text-align: center;">88.6</td>
<td style="text-align: center;">84.9</td>
<td style="text-align: center;">82.1</td>
<td style="text-align: center;">93.8</td>
<td style="text-align: center;">87.5</td>
<td style="text-align: center;">86.1</td>
<td style="text-align: center;">88.7</td>
<td style="text-align: center;">87.4</td>
<td style="text-align: center;">93.9</td>
<td style="text-align: center;">93.8</td>
<td style="text-align: center;">93.8</td>
</tr>
<tr>
<td style="text-align: center;">K-BERT (MedicalKG)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">94.0</td>
<td style="text-align: center;">94.4</td>
<td style="text-align: center;">94.2</td>
</tr>
<tr>
<td style="text-align: center;">Pre-trained on WikiZh and WebtextZh by us.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Our BERT</td>
<td style="text-align: center;">82.1</td>
<td style="text-align: center;">86.5</td>
<td style="text-align: center;">84.2</td>
<td style="text-align: center;">83.2</td>
<td style="text-align: center;">91.7</td>
<td style="text-align: center;">87.2</td>
<td style="text-align: center;">84.9</td>
<td style="text-align: center;">87.4</td>
<td style="text-align: center;">86.1</td>
<td style="text-align: center;">91.8</td>
<td style="text-align: center;">93.5</td>
<td style="text-align: center;">92.7</td>
</tr>
<tr>
<td style="text-align: center;">K-BERT (HowNet)</td>
<td style="text-align: center;">82.8</td>
<td style="text-align: center;">85.8</td>
<td style="text-align: center;">84.3</td>
<td style="text-align: center;">83.0</td>
<td style="text-align: center;">92.4</td>
<td style="text-align: center;">87.5</td>
<td style="text-align: center;">86.3</td>
<td style="text-align: center;">88.5</td>
<td style="text-align: center;">87.3</td>
<td style="text-align: center;">93.5</td>
<td style="text-align: center;">93.8</td>
<td style="text-align: center;">93.7</td>
</tr>
<tr>
<td style="text-align: center;">K-BERT (CN-DBpedia)</td>
<td style="text-align: center;">81.9</td>
<td style="text-align: center;">87.1</td>
<td style="text-align: center;">84.4</td>
<td style="text-align: center;">83.1</td>
<td style="text-align: center;">92.6</td>
<td style="text-align: center;">87.6</td>
<td style="text-align: center;">86.3</td>
<td style="text-align: center;">88.6</td>
<td style="text-align: center;">87.4</td>
<td style="text-align: center;">93.9</td>
<td style="text-align: center;">94.3</td>
<td style="text-align: center;">94.1</td>
</tr>
<tr>
<td style="text-align: center;">K-BERT (MedicalKG)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">94.1</td>
<td style="text-align: center;">94.3</td>
<td style="text-align: center;">94.2</td>
</tr>
</tbody>
</table>
<p>K-BERT with CN-DBpedia can improve the performance of all tasks by $1 \sim 2 \%$. The unique gain benefits from the domain knowledge in KG. Furthermore, it can be observed from the Medicine_NER in Table 3 that the performance improvement using the MedicalKG is very obvious. From these results, we can conclude that KG, especially the domain KG, is very helpful for domain-specific tasks.</p>
<h2>Ablation studies</h2>
<p><img alt="img-4.jpeg" src="img-4.jpeg" />
(a) Law_Q\&amp;A
<img alt="img-5.jpeg" src="img-5.jpeg" />
(b) Medicine_NER</p>
<p>Figure 5: Ablation studies: (a) Law_Q\&amp;A with CNDBpedia; (b) Medicine_NER with MedicalKG;</p>
<p>In this subsection, we explore the effects of the softposition and visible matrix for K-BERT using two domainspecific tasks (Law_Q\&amp;A and Medicine_NER). "w/o softposition" refers to fine-tuning K-BERT with hard-position instead of soft-position. "w/o visible matrix" means that the all tokens are visible to each other. BERT is equivalent to the K-BERT without KG. As shown in Figure 5, we have the following observations: (1) In both tasks, without soft-position or visible matrix, the performance of K-BERT has declined; (2) In Law_Q\&amp;A (Figure 5(a)), K-BERT without visible matrix is worse than BERT, which confirms the existence of KN, i.e., improperly adding knowledge can lead to performance degradation; (3) In Figure 5(a), K-BERT reaches its peak at epoch 2, while BERT is at epoch 4 , which proves that K-BERT converges faster than BERT. In general, we can conclude that the soft-position and the visible matrix can make K-BERT more robust to KN interference and thus make more efficient use of knowledge.</p>
<h2>Conclusions</h2>
<p>In this paper, we propose K-BERT to enable language representation with knowledge graphs, achieving the capability of commonsense or domain knowledge. Specifically, K-BERT first injects knowledge from KG into a sentence, making it a knowledge-rich sentence tree. Next, soft-position and visible matrix are adapted to control the scope of knowledge, preventing it from deviating from its original meaning.</p>
<p>Despite the challenges of HES and KN, our investigation reveals promising results on twelve open-/specific- domain NLP tasks. Empirical results demonstrate that KG is especially helpful for knowledge-driven specific-domain tasks and can be used to solve problems that require domain experts. Besides, K-BERT is compatible with the model parameters of BERT, which means that users can directly adopt the available pre-trained BERT parameters (e.g., Google BERT, Baidu-ERNIE, etc.) on K-BERT without pre-training by themselves.</p>
<p>These positive results point to future work in (1) improving K-Query to enable filtering of unimportant triples based</p>
<p>on context; (2) extending this approach to other LR models such as ELMo (Peters et al. 2018), XLNet (Yang et al. 2019), etc;</p>
<h2>Acknowledgement</h2>
<p>This work is funded by the National Key R\&amp;D Program of China (2017YFB1200700), and 2019 Tencent Rhino-Bird Elite Training Program.</p>
<h2>References</h2>
<p>Bodenreider, O. 2008. Biomedical ontologies in action: role in knowledge management, data integration and decision support. Yearbook of medical informatics 17(01):6779.</p>
<p>Bordes, A.; Usunier, N.; Garcia-Duran, A.; Weston, J.; and Yakhnenko, O. 2013. Translating embeddings for modeling multi-relational data. In Advances in neural information processing systems, 2787-2795.
Bosselut, A.; Rashkin, H.; Sap, M.; Malaviya, C.; Celikyilmaz, A.; and Choi, Y. 2019. COMET: Commonsense transformers for automatic knowledge graph construction. arXiv preprint arXiv:1906.05317.
Cao, Y.; Hou, L.; Li, J.; Liu, Z.; Li, C.; Chen, X.; and Dong, T. 2018. Joint representation learning of cross-lingual words and entities via attentive distant supervision. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing.
Conneau, A.; Lample, G.; Rinott, R.; Williams, A.; Bowman, S. R.; Schwenk, H.; and Stoyanov, V. 2018. XNLI: Evaluating cross-lingual sentence representations. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing.
Cui, Y.; Che, W.; Liu, T.; Qin, B.; Yang, Z.; Wang, S.; and Hu, G. 2019. Pre-training with whole word masking for chinese bert. arXiv preprint arXiv:1906.08101.
Dai, Z.; Yang, Z.; Yang, Y.; Cohen, W. W.; Carbonell, J.; Le, Q. V.; and Salakhutdinov, R. 2019. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860.
Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018. BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.
Dong, Z.; Dong, Q.; and Hao, C. 2006. Hownet and the computation of meaning. Citeseer.
Han, X.; Liu, Z.; and Sun, M. 2016. Joint representation learning of text and knowledge for knowledge graph completion. arXiv preprint arXiv:1611.04125.
Joshi, M.; Chen, D.; Liu, Y.; Weld, D. S.; Zettlemoyer, L.; and Levy, O. 2019. SpanBERT: Improving pretraining by representing and predicting spans. arXiv preprint arXiv:1907.10529.
Levow, G. A. 2006. The 3rd international chinese language processing bakeoff. In Sighan Workshop on Chinese Language Processing.</p>
<p>Liu, X.; Chen, Q.; Deng, C.; Zeng, H.; Chen, J.; Li, D.; and Tang, B. 2018. LCQMC:a large-scale chinese question matching corpus. International conference on computational linguistics 1952-1962.
Liu, Y.; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.; Levy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V. 2019. RoBERTa: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.
Mikolov, T.; Chen, K.; Corrado, G.; and Dean, J. 2013. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.
Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. arXiv preprint arXiv:1802.05365.
Radford, A.; Narasimhan, K.; Salimans, T.; and Sutskever, I. 2018. Improving language understanding by generative pre-training. Technical report, OpenAI.
Sun, Y.; Wang, S.; Li, Y.; Feng, S.; Chen, X.; Zhang, H.; Tian, X.; Zhu, D.; Tian, H.; and Wu, H. 2019. ERNIE: Enhanced representation through knowledge integration. arXiv preprint arXiv:1904.09223.
Toutanova, K.; Chen, D.; Pantel, P.; Poon, H.; Choudhury, P.; and Gamon, M. 2015. Representing text for joint embedding of text and knowledge bases. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, 1499-1509.
Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. Attention is all you need. In Advances in neural information processing systems, 5998-6008.
Wang, Z.; Zhang, J.; Feng, J.; and Chen, Z. 2014. Knowledge graph and text jointly embedding. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), 1591-1601.
Wang, A.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and Bowman, S. R. 2018. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461.
Xu, B.; Xu, Y.; Liang, J.; Xie, C.; Liang, B.; Cui, W.; and Xiao, Y. 2017. Cn-dbpedia: A never-ending chinese knowledge extraction system. International conference industrial, engineering and other applications applied intelligent systems 428-438.
Yang, Z.; Dai, Z.; Yang, Y.; Carbonell, J.; Salakhutdinov, R.; and Le, Q. V. 2019. Xlnet: Generalized autoregressive pretraining for language understanding. arXiv preprint arXiv:1906.08237.
Zhang, Z.; Han, X.; Liu, Z.; Jiang, X.; Sun, M.; and Liu, Q. 2019. ERNIE: Enhanced language representation with informative entities. arXiv preprint arXiv:1905.07129.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{14}$ https://zhidao.baidu.com
${ }^{15} \mathrm{https}: / / e m b e d d i n g . g i t h u b . i o / e v a l u a t i o n / # e x t r i n s i c$
${ }^{16} \mathrm{https}: / /$ biendata.com/competition/CCKS2017_2/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{6}$ https://github.com/google-research/bert
${ }^{7}$ https://github.com/dbiir/UER-py
${ }^{8}$ https://embedding.github.io/evaluation/
${ }^{9}$ https://github.com/pengming617/bert_classification
${ }^{10} \mathrm{https}: / /$ share.weiyun.com/5xxYiig
${ }^{11}$ https://share.weiyun.com/5lEsv0w
${ }^{12}$ https://book.douban.com/
${ }^{13} \mathrm{http}: / /$ tcci.ccf.org.cn/conference/2016/dldoc/evagline2.pdf&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
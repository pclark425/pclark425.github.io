<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7062 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7062</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7062</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-131.html">extraction-schema-131</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <p><strong>Paper ID:</strong> paper-bda605928d6ebe4db906e69ab5d343df75918727</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/bda605928d6ebe4db906e69ab5d343df75918727" target="_blank">Large Language Model Guided Tree-of-Thought</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> Experimental results show that the ToT framework can significantly increase the success rate of Sudoku puzzle solving and the implemented ToT-based Sudoku solver is available on GitHub.</p>
                <p><strong>Paper Abstract:</strong> In this paper, we introduce the Tree-of-Thought (ToT) framework, a novel approach aimed at improving the problem-solving capabilities of auto-regressive large language models (LLMs). The ToT technique is inspired by the human mind's approach for solving complex reasoning tasks through trial and error. In this process, the human mind explores the solution space through a tree-like thought process, allowing for backtracking when necessary. To implement ToT as a software system, we augment an LLM with additional modules including a prompter agent, a checker module, a memory module, and a ToT controller. In order to solve a given problem, these modules engage in a multi-round conversation with the LLM. The memory module records the conversation and state history of the problem solving process, which allows the system to backtrack to the previous steps of the thought-process and explore other directions from there. To verify the effectiveness of the proposed technique, we implemented a ToT-based solver for the Sudoku Puzzle. Experimental results show that the ToT framework can significantly increase the success rate of Sudoku puzzle solving. Our implementation of the ToT-based Sudoku solver is available on GitHub: \url{https://github.com/jieyilong/tree-of-thought-puzzle-solver}.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7062.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7062.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ToT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree-of-Thought framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that augments an auto-regressive LLM with a prompter agent, checker module, memory module, and ToT controller to perform multi-round tree-search style reasoning with backtracking and correctness checks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An auto-regressive self-attention transformer LLM (OpenAI gpt-3.5-turbo) used as a heuristic to generate short-range next steps; called via API with temperature=1 in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer LLM augmented with Tree-of-Thought search components (prompter agent, checker, memory, ToT controller/policy)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Tree-of-Thought multi-round tree search: LLM generates short-range intermediate steps; a checker validates partial solutions; a ToT controller decides to continue or backtrack; prompter selects/generates prompts and in-context examples; multi-agent policy-gradient training (REINFORCE variant) proposed for prompter and controller.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Sudoku puzzles (3x3, 4x4, 5x5 benchmark sets)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Three benchmark sets, each of 10 Sudoku puzzles on grid sizes corresponding to n=3,4,5 (i.e., 9x9, 16x16, 25x25 in the generalized description), used to measure solver success rate.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Sudoku puzzle solving (constraint-satisfaction / logical reasoning with trial-and-error/backtracking)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>success rate (fraction of puzzles solved per benchmark set)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>3x3: 100% (10/10 solved); 4x4: 90% (9/10 solved; 1 failed before max rounds); 5x5: 80% (8/10 solved; 2 failed before max rounds). Maximum conversation rounds K=100 in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>ToT substantially outperformed zero-shot and CoT-based baselines: paper reports ToT success rate improves by 11% over the second-best on two benchmark sets and reports ToT is 80% higher than one-shot/few-shot on the 4x4 set and 60% higher on the 5x5 set (as stated in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Augmenting an LLM with a tree-search style multi-round interaction, correctness checker, memory, and backtracking controller significantly improves success on puzzles requiring trial-and-error and long-range planning (Sudoku) versus linear chain-of-thought prompting and zero-shot baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Current implementation uses a rule-based checker and a rule-based ToT controller which reduce adaptability and search efficiency for some instances; REINFORCE-based training is simple and may suffer stability issues; approach requires a checker (rule-based or trained verifier) for correctness checking and can require many conversational rounds (max rounds used=100).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model Guided Tree-of-Thought', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7062.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7062.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The auto-regressive transformer LLM used as the core language model/heuristic in the ToT experiments and all baseline solvers reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Auto-regressive transformer-based large language model by OpenAI, accessed via API (temperature set to 1 in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>autoregressive transformer</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Used to perform short-range next-step generation either via zero-shot, chain-of-thought (one-shot/few-shot) prompting or as the heuristic generator in the ToT multi-round tree search.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Sudoku puzzles (3x3, 4x4, 5x5)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Same Sudoku benchmark sets used to evaluate baseline prompting strategies and ToT.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Sudoku puzzle solving (logical deduction / constraint satisfaction)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>success rate (when used with different prompting/ToT strategies)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>As baseline LLM: zero-shot worst; one-shot/few-shot (CoT) boosted performance notably on 3x3 puzzles but dropped to ~50% on larger puzzles; when used inside ToT, achieved 100%/90%/80% on 3x3/4x4/5x5 respectively (as reported).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>gpt-3.5-turbo can generate useful short-range steps, but needs external mechanisms (checker, backtracking, multi-round search) to succeed on larger, long-range reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>As an auto-regressive LLM it generates linearly without native backtracking or explicit step-by-step correctness checking; performance on long-range, trial-and-error tasks degrades without augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model Guided Tree-of-Thought', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7062.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7062.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that elicits step-by-step intermediate reasoning from LLMs via in-context examples; used in the paper as one-shot and few-shot baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Technique applied to transformer LLMs to coax multi-step rationales via prompt examples (one-shot or few-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>prompting/in-context learning method for transformer LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Provide step-by-step worked examples in the prompt so the model outputs chain-of-thought style solutions (hand-crafted in-context examples).</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Sudoku puzzles (3x3, 4x4, 5x5) used as baselines in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Same three Sudoku benchmark sets used to compare CoT one-shot/few-shot prompting against ToT and zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Sudoku solving (multi-step logical reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>success rate</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>CoT-style one-shot/few-shot: significantly improves performance on small (3x3) puzzles; performance dropped to around ~50% success on larger puzzles (4x4/5x5) per the paper's reporting.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>CoT (one-shot/few-shot) outperforms zero-shot prompting but underperforms ToT on larger puzzles requiring backtracking/trial-and-error.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Chain-of-thought prompting helps with short-range reasoning and small instances but is insufficient for long-range tasks requiring exploration and backtracking.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires hand-crafted in-context examples (hard to scale); lacks explicit step-level correctness checking and backtracking, so minor errors amplify over long derivations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model Guided Tree-of-Thought', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7062.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7062.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-consistency decoding for chain-of-thought</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A technique that samples multiple chain-of-thought rationales from an LLM and chooses the final answer by majority/consistency across samples; mentioned in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>decoding/aggregation technique over multiple sampled reasoning chains</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Sample many independent chain-of-thought outputs and aggregate to pick the most consistent final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step reasoning tasks (general)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as a recent technique that improves CoT by leveraging multiple sampled rationales to increase answer consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Does not provide internal intermediate-step correction or backtracking; relies on sampling diversity and aggregation rather than explicit verification of partial solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model Guided Tree-of-Thought', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7062.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7062.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>STaR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-taught Reasoner (STaR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that has an LLM generate reasoning chains and then fine-tunes the model on those chains that yield correct answers, effectively bootstrapping reasoning capability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>fine-tuning on filtered chain-of-thought data</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Generate candidate reasoning chains, drop those with incorrect answers, fine-tune model on remaining valid chains.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step reasoning (general)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Described in related work as a bootstrapping approach to improve reasoning by fine-tuning on verified chains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Relies on ability to verify answers and assumes the existence of verifiable correctness signals for training.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model Guided Tree-of-Thought', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7062.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7062.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Verifier training (Cobbe et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Training verifiers to solve math word problems</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Train verifier models that judge whether an LLM's solution/rationale to a mathematical problem is logically correct; mentioned as an approach that could substitute rule-based checkers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Training verifiers to solve math word problems</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>neural verifier/classifier that judges correctness of model outputs</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Train a verifier to evaluate LLM responses and accept or reject reasoning chains based on predicted correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>mathematical word problem verification / solution checking</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as a promising way to perform correctness checking where rule-based checkers are difficult to implement; paper cited in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Verifier quality depends on training data; may be domain-specific and probabilistic rather than exact.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model Guided Tree-of-Thought', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7062.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7062.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PromptPG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PromptPG (dynamic prompt learning via policy gradient)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that learns to select in-context examples via policy gradient to improve performance on semi-structured mathematical reasoning; cited as inspiration for the prompter agent.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>policy-gradient based prompt/example selection</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Use a learned policy to pick in-context examples that maximize downstream LLM performance.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>mathematical reasoning / prompt engineering</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Used as prior work motivating the prompter agent design that selects in-context examples in the ToT framework.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Mentioned as prior work; limitations not discussed in detail in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model Guided Tree-of-Thought', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7062.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7062.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Approach that interleaves natural language reasoning traces and environment actions for agents powered by LLMs, enabling reasoning-and-acting behavior; cited in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ReAct: Synergizing reasoning and acting in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>agent architecture combining reasoning traces and actions</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Interleave chain-of-thought-like reasoning with actions to interact with environments and external tools.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>interactive reasoning and action tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Referenced as relevant work that augments LLMs with action-taking and memory; ToT differs by explicitly enabling backtracking/search control.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Not focused on explicit backtracking/search-tree style exploration as ToT is.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model Guided Tree-of-Thought', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7062.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7062.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that equips an agent with dynamic memory and self-reflection to iteratively improve reasoning traces and decisions; cited as related work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: an autonomous agent with dynamic memory and self-reflection</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>agent with dynamic memory and self-reflection loop</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Iterative self-reflection and memory updates to refine agent reasoning and choices.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>iterative reasoning and planning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as an approach that adds dynamic memory/self-reflection; ToT shares use of memory but emphasizes explicit checker and backtracking controller.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Mentioned as related work; specifics not covered in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model Guided Tree-of-Thought', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Star: Bootstrapping reasoning with reasoning <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 2)</em></li>
                <li>ReAct: Synergizing reasoning and acting in language models <em>(Rating: 2)</em></li>
                <li>Reflexion: an autonomous agent with dynamic memory and self-reflection <em>(Rating: 2)</em></li>
                <li>Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning <em>(Rating: 2)</em></li>
                <li>Gpt-4 technical report <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7062",
    "paper_id": "paper-bda605928d6ebe4db906e69ab5d343df75918727",
    "extraction_schema_id": "extraction-schema-131",
    "extracted_data": [
        {
            "name_short": "ToT",
            "name_full": "Tree-of-Thought framework",
            "brief_description": "A framework that augments an auto-regressive LLM with a prompter agent, checker module, memory module, and ToT controller to perform multi-round tree-search style reasoning with backtracking and correctness checks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo",
            "model_description": "An auto-regressive self-attention transformer LLM (OpenAI gpt-3.5-turbo) used as a heuristic to generate short-range next steps; called via API with temperature=1 in experiments.",
            "model_size": null,
            "architecture_type": "Transformer LLM augmented with Tree-of-Thought search components (prompter agent, checker, memory, ToT controller/policy)",
            "training_data": null,
            "reasoning_method": "Tree-of-Thought multi-round tree search: LLM generates short-range intermediate steps; a checker validates partial solutions; a ToT controller decides to continue or backtrack; prompter selects/generates prompts and in-context examples; multi-agent policy-gradient training (REINFORCE variant) proposed for prompter and controller.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "Sudoku puzzles (3x3, 4x4, 5x5 benchmark sets)",
            "benchmark_description": "Three benchmark sets, each of 10 Sudoku puzzles on grid sizes corresponding to n=3,4,5 (i.e., 9x9, 16x16, 25x25 in the generalized description), used to measure solver success rate.",
            "task_type": "Sudoku puzzle solving (constraint-satisfaction / logical reasoning with trial-and-error/backtracking)",
            "performance_metric": "success rate (fraction of puzzles solved per benchmark set)",
            "performance_value": "3x3: 100% (10/10 solved); 4x4: 90% (9/10 solved; 1 failed before max rounds); 5x5: 80% (8/10 solved; 2 failed before max rounds). Maximum conversation rounds K=100 in experiments.",
            "comparison_with_baseline": "ToT substantially outperformed zero-shot and CoT-based baselines: paper reports ToT success rate improves by 11% over the second-best on two benchmark sets and reports ToT is 80% higher than one-shot/few-shot on the 4x4 set and 60% higher on the 5x5 set (as stated in the paper).",
            "key_findings": "Augmenting an LLM with a tree-search style multi-round interaction, correctness checker, memory, and backtracking controller significantly improves success on puzzles requiring trial-and-error and long-range planning (Sudoku) versus linear chain-of-thought prompting and zero-shot baselines.",
            "limitations": "Current implementation uses a rule-based checker and a rule-based ToT controller which reduce adaptability and search efficiency for some instances; REINFORCE-based training is simple and may suffer stability issues; approach requires a checker (rule-based or trained verifier) for correctness checking and can require many conversational rounds (max rounds used=100).",
            "uuid": "e7062.0",
            "source_info": {
                "paper_title": "Large Language Model Guided Tree-of-Thought",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "gpt-3.5-turbo",
            "name_full": "OpenAI gpt-3.5-turbo",
            "brief_description": "The auto-regressive transformer LLM used as the core language model/heuristic in the ToT experiments and all baseline solvers reported in the paper.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo",
            "model_description": "Auto-regressive transformer-based large language model by OpenAI, accessed via API (temperature set to 1 in experiments).",
            "model_size": null,
            "architecture_type": "autoregressive transformer",
            "training_data": null,
            "reasoning_method": "Used to perform short-range next-step generation either via zero-shot, chain-of-thought (one-shot/few-shot) prompting or as the heuristic generator in the ToT multi-round tree search.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "Sudoku puzzles (3x3, 4x4, 5x5)",
            "benchmark_description": "Same Sudoku benchmark sets used to evaluate baseline prompting strategies and ToT.",
            "task_type": "Sudoku puzzle solving (logical deduction / constraint satisfaction)",
            "performance_metric": "success rate (when used with different prompting/ToT strategies)",
            "performance_value": "As baseline LLM: zero-shot worst; one-shot/few-shot (CoT) boosted performance notably on 3x3 puzzles but dropped to ~50% on larger puzzles; when used inside ToT, achieved 100%/90%/80% on 3x3/4x4/5x5 respectively (as reported).",
            "comparison_with_baseline": null,
            "key_findings": "gpt-3.5-turbo can generate useful short-range steps, but needs external mechanisms (checker, backtracking, multi-round search) to succeed on larger, long-range reasoning tasks.",
            "limitations": "As an auto-regressive LLM it generates linearly without native backtracking or explicit step-by-step correctness checking; performance on long-range, trial-and-error tasks degrades without augmentation.",
            "uuid": "e7062.1",
            "source_info": {
                "paper_title": "Large Language Model Guided Tree-of-Thought",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "CoT",
            "name_full": "Chain-of-Thought prompting",
            "brief_description": "A prompting technique that elicits step-by-step intermediate reasoning from LLMs via in-context examples; used in the paper as one-shot and few-shot baselines.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo",
            "model_description": "Technique applied to transformer LLMs to coax multi-step rationales via prompt examples (one-shot or few-shot).",
            "model_size": null,
            "architecture_type": "prompting/in-context learning method for transformer LLMs",
            "training_data": null,
            "reasoning_method": "Provide step-by-step worked examples in the prompt so the model outputs chain-of-thought style solutions (hand-crafted in-context examples).",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "Sudoku puzzles (3x3, 4x4, 5x5) used as baselines in this paper",
            "benchmark_description": "Same three Sudoku benchmark sets used to compare CoT one-shot/few-shot prompting against ToT and zero-shot.",
            "task_type": "Sudoku solving (multi-step logical reasoning)",
            "performance_metric": "success rate",
            "performance_value": "CoT-style one-shot/few-shot: significantly improves performance on small (3x3) puzzles; performance dropped to around ~50% success on larger puzzles (4x4/5x5) per the paper's reporting.",
            "comparison_with_baseline": "CoT (one-shot/few-shot) outperforms zero-shot prompting but underperforms ToT on larger puzzles requiring backtracking/trial-and-error.",
            "key_findings": "Chain-of-thought prompting helps with short-range reasoning and small instances but is insufficient for long-range tasks requiring exploration and backtracking.",
            "limitations": "Requires hand-crafted in-context examples (hard to scale); lacks explicit step-level correctness checking and backtracking, so minor errors amplify over long derivations.",
            "uuid": "e7062.2",
            "source_info": {
                "paper_title": "Large Language Model Guided Tree-of-Thought",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Self-consistency",
            "name_full": "Self-consistency decoding for chain-of-thought",
            "brief_description": "A technique that samples multiple chain-of-thought rationales from an LLM and chooses the final answer by majority/consistency across samples; mentioned in related work.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "architecture_type": "decoding/aggregation technique over multiple sampled reasoning chains",
            "training_data": null,
            "reasoning_method": "Sample many independent chain-of-thought outputs and aggregate to pick the most consistent final answer.",
            "external_tool_used": null,
            "external_tool_description": null,
            "benchmark_name": null,
            "benchmark_description": null,
            "task_type": "multi-step reasoning tasks (general)",
            "performance_metric": null,
            "performance_value": null,
            "comparison_with_baseline": null,
            "key_findings": "Mentioned as a recent technique that improves CoT by leveraging multiple sampled rationales to increase answer consistency.",
            "limitations": "Does not provide internal intermediate-step correction or backtracking; relies on sampling diversity and aggregation rather than explicit verification of partial solutions.",
            "uuid": "e7062.3",
            "source_info": {
                "paper_title": "Large Language Model Guided Tree-of-Thought",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "STaR",
            "name_full": "Self-taught Reasoner (STaR)",
            "brief_description": "A method that has an LLM generate reasoning chains and then fine-tunes the model on those chains that yield correct answers, effectively bootstrapping reasoning capability.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "architecture_type": "fine-tuning on filtered chain-of-thought data",
            "training_data": null,
            "reasoning_method": "Generate candidate reasoning chains, drop those with incorrect answers, fine-tune model on remaining valid chains.",
            "external_tool_used": null,
            "external_tool_description": null,
            "benchmark_name": null,
            "benchmark_description": null,
            "task_type": "multi-step reasoning (general)",
            "performance_metric": null,
            "performance_value": null,
            "comparison_with_baseline": null,
            "key_findings": "Described in related work as a bootstrapping approach to improve reasoning by fine-tuning on verified chains.",
            "limitations": "Relies on ability to verify answers and assumes the existence of verifiable correctness signals for training.",
            "uuid": "e7062.4",
            "source_info": {
                "paper_title": "Large Language Model Guided Tree-of-Thought",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Verifier training (Cobbe et al.)",
            "name_full": "Training verifiers to solve math word problems",
            "brief_description": "Train verifier models that judge whether an LLM's solution/rationale to a mathematical problem is logically correct; mentioned as an approach that could substitute rule-based checkers.",
            "citation_title": "Training verifiers to solve math word problems",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "architecture_type": "neural verifier/classifier that judges correctness of model outputs",
            "training_data": null,
            "reasoning_method": "Train a verifier to evaluate LLM responses and accept or reject reasoning chains based on predicted correctness.",
            "external_tool_used": null,
            "external_tool_description": null,
            "benchmark_name": null,
            "benchmark_description": null,
            "task_type": "mathematical word problem verification / solution checking",
            "performance_metric": null,
            "performance_value": null,
            "comparison_with_baseline": null,
            "key_findings": "Mentioned as a promising way to perform correctness checking where rule-based checkers are difficult to implement; paper cited in related work.",
            "limitations": "Verifier quality depends on training data; may be domain-specific and probabilistic rather than exact.",
            "uuid": "e7062.5",
            "source_info": {
                "paper_title": "Large Language Model Guided Tree-of-Thought",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "PromptPG",
            "name_full": "PromptPG (dynamic prompt learning via policy gradient)",
            "brief_description": "A method that learns to select in-context examples via policy gradient to improve performance on semi-structured mathematical reasoning; cited as inspiration for the prompter agent.",
            "citation_title": "Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "architecture_type": "policy-gradient based prompt/example selection",
            "training_data": null,
            "reasoning_method": "Use a learned policy to pick in-context examples that maximize downstream LLM performance.",
            "external_tool_used": null,
            "external_tool_description": null,
            "benchmark_name": null,
            "benchmark_description": null,
            "task_type": "mathematical reasoning / prompt engineering",
            "performance_metric": null,
            "performance_value": null,
            "comparison_with_baseline": null,
            "key_findings": "Used as prior work motivating the prompter agent design that selects in-context examples in the ToT framework.",
            "limitations": "Mentioned as prior work; limitations not discussed in detail in this paper.",
            "uuid": "e7062.6",
            "source_info": {
                "paper_title": "Large Language Model Guided Tree-of-Thought",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "ReAct",
            "name_full": "ReAct",
            "brief_description": "Approach that interleaves natural language reasoning traces and environment actions for agents powered by LLMs, enabling reasoning-and-acting behavior; cited in related work.",
            "citation_title": "ReAct: Synergizing reasoning and acting in language models",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "architecture_type": "agent architecture combining reasoning traces and actions",
            "training_data": null,
            "reasoning_method": "Interleave chain-of-thought-like reasoning with actions to interact with environments and external tools.",
            "external_tool_used": null,
            "external_tool_description": null,
            "benchmark_name": null,
            "benchmark_description": null,
            "task_type": "interactive reasoning and action tasks",
            "performance_metric": null,
            "performance_value": null,
            "comparison_with_baseline": null,
            "key_findings": "Referenced as relevant work that augments LLMs with action-taking and memory; ToT differs by explicitly enabling backtracking/search control.",
            "limitations": "Not focused on explicit backtracking/search-tree style exploration as ToT is.",
            "uuid": "e7062.7",
            "source_info": {
                "paper_title": "Large Language Model Guided Tree-of-Thought",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion",
            "brief_description": "An approach that equips an agent with dynamic memory and self-reflection to iteratively improve reasoning traces and decisions; cited as related work.",
            "citation_title": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "architecture_type": "agent with dynamic memory and self-reflection loop",
            "training_data": null,
            "reasoning_method": "Iterative self-reflection and memory updates to refine agent reasoning and choices.",
            "external_tool_used": null,
            "external_tool_description": null,
            "benchmark_name": null,
            "benchmark_description": null,
            "task_type": "iterative reasoning and planning tasks",
            "performance_metric": null,
            "performance_value": null,
            "comparison_with_baseline": null,
            "key_findings": "Cited as an approach that adds dynamic memory/self-reflection; ToT shares use of memory but emphasizes explicit checker and backtracking controller.",
            "limitations": "Mentioned as related work; specifics not covered in this paper.",
            "uuid": "e7062.8",
            "source_info": {
                "paper_title": "Large Language Model Guided Tree-of-Thought",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2
        },
        {
            "paper_title": "Star: Bootstrapping reasoning with reasoning",
            "rating": 2
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 2
        },
        {
            "paper_title": "ReAct: Synergizing reasoning and acting in language models",
            "rating": 2
        },
        {
            "paper_title": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "rating": 2
        },
        {
            "paper_title": "Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning",
            "rating": 2
        },
        {
            "paper_title": "Gpt-4 technical report",
            "rating": 1
        }
    ],
    "cost": 0.01830925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Large Language Model Guided Tree-of-Thought</h1>
<p>Jieyi Long<br>Theta Labs, Inc.<br>San Jose, CA 95128<br>jieyi@thetalabs.org</p>
<h4>Abstract</h4>
<p>In this paper, we introduce the Tree-of-Thought (ToT) framework, a novel approach aimed at improving the problem-solving capabilities of auto-regressive large language models (LLMs). The ToT technique is inspired by the human mind's approach for solving complex reasoning tasks through trial and error. In this process, the human mind explores the solution space through a tree-like thought process, allowing for backtracking when necessary. To implement ToT as a software system, we augment an LLM with additional modules including a prompter agent, a checker module, a memory module, and a ToT controller. In order to solve a given problem, these modules engage in a multi-round conversation with the LLM. The memory module records the conversation and state history of the problem solving process, which allows the system to backtrack to the previous steps of the thought-process and explore other directions from there. To verify the effectiveness of the proposed technique, we implemented a ToT-based solver for the Sudoku Puzzle. Experimental results show that the ToT framework can significantly increase the success rate of Sudoku puzzle solving. Our implementation of the ToT-based Sudoku solver is available on GitHub: https://github.com/jieyilong/tree-of-thought-puzzle-solver.</p>
<h2>1 Introduction</h2>
<p>Self-attention based auto-regressive large language models (LLMs) such as GPT-4 have recently taken the world by storm [1, 2, 3, 4, 5, 6]. These LLMs excel at a variety of tasks that previously thought as extremely difficult or even impossible. For example, they are able to handle various logical and mathematical reasoning tasks, particularly those that entail "short-range reasonings" necessitating only a few steps to arrive at conclusions [6, 7]. Such remarkable capabilities have even led to speculation that an early form of artificial general intelligence (AGI) may have already emerged [7]. However, today's LLMs still exhibit limitations in certain domains, especially for "long-range" reasoning tasks, where long-term planning and solution exploration are necessary [7]. When presenting a LLMs such as GPT-4 with a challenging problem solving task, especially the so called System-2 reasoning problems [8], the model does not always succeed. Although the generated answer may be indicative of the correct direction, the derivation process frequently includes logical errors. We hypothesize that there are two main contributing factors which limits the problem solving ability of LLMs:</p>
<p>Lack of correctness checking: To ensure correctness, a good practice for a human solver is to carry out verification procedures at every step of the problem-solving process, thereby ensuring the credibility of the final solution. In comparison, auto-regressive language models do not explicitly perform logical correctness checks as it generates a new token based on the previous tokens. This limits the model's capacity to rectify its own mistakes. A minor error could be amplified as the model generates more tokens, thereby leading to rapid solution quality deterioration and making it difficult to recover from mistakes.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: (a) Details of the Tree-of-Thought search strategy, where a solid arrow means a search step guided by the response from the LLM, and a dashed arrow indicates backtracking commanded by the ToT controller. (b) The software system implementing the Tree-of-Thought search strategy. It enhances the problem solving capability of an LLM by augmenting it with additional modules including a prompter agent, a checker module, a memory module, and a ToT controller.</p>
<p>Solution generated linearly: As mentioned above, LLMs typically generate a token based on the preceding sequence of tokens without backward editing. On the contrary, when a human solver attempts to solve a problem, she might backtrack to previous steps if a derivation step is incorrect, or if she becomes stuck and is unable to make further progress towards arriving at the final answer. Fields Medal winner Terence Tao once shared his experiences solving hard math problems ${ }^{1}$ : "When I was a kid, I had a romanticized notion of mathematics, that hard problems were solved in Eureka moments of inspiration... With me, it's always, Let's try this. That gets me part of the way, or that doesn't work. Now let's try this. Oh, there's a little shortcut here... You work on it long enough and you happen to make progress towards a hard problem by a back door at some point. At the end, it's usually, oh, I've solved the problem." The problem solving process as he described is a tree-like thinking process, rather than a linear chain-of-thought [9]. The limitation of linear response generation is also apparent from a computational complexity perspective. The number of computation steps an auto-regressive LLM can perform is polynomial in terms of its input length. Unless $\mathbf{P}=$ NP holds which contradicts the widely accepted belief, there would be problems in NP that is not solvable by auto-regressive LLMs.
Inspired by these two shortcomings of auto-regressive LLMs, we propose a novel framework which augments an LLM with several additional modules including an automatic "prompter agent". This framework employs a solution search strategy we call the Tree-of-Thought (ToT ${ }^{2}$ ). This strategy solves a problem through a multi-round conversation between the LLM and the prompter agent. Figure la provides a visual description of the ToT search strategy, in which the LLM plays a crucial role in guiding the search for solutions. To make it more concrete, let us assume the problem to be solved is an instance of the Sudoku puzzle. The "root" node represents the initial state, corresponding to when a human mind just reads through the problem description, and begins the thinking process. A blue node in the figure represents a valid partial solution, which can be used by the LLM as a basis to generate the next search step. In the context of Sudoku puzzle solving, this means presenting a partially filled Sudoku board to an LLM and letting the LLM fill in a few more cells. The rationale is that an LLM like GPT-4 has been trained on a vast amount of text corpus which includes many Sudoku puzzle solutions. Given a partially filled board, likely the LLM is able to recognize the pattern, and provide useful insights on how to proceed following the Sudoku rules. Hence, it is highly probable that a search guided by the LLM is significantly more efficient than a brute-force search. In the figure, the search steps guided by the LLM are represented by the solid arrows. However, these steps generated by the LLM are not guaranteed to be always logically correct. Thus, we introduce</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>a "checker module" to perform correctness checks. In Figure 1a, a gray node with an "X" marker represents a "dead-end", i.e. a partial solution that the checker module considers as invalid. For Sudoku, this means the partially filled board violates the Sudoku rules. If the current node is invalid, obviously we need to return to a parent or an ancestor node in order to correct the mistake. This can be coordinated by a module called the "ToT controller" which oversees the ToT search. With the backtracking capability, the system can regenerate the solution and thus recover from errors. In addition, even when the current node is valid, if the system remains stuck at it for too long, the ToT controller could issue a backtrack signal to explore other possible solutions. This is similar to a scenario where a human mind realizes that there is no viable path towards reaching the final solution through a particular direction, prompting her to change course and explore alternative routes. This process continues until either a full solution is found (represented by a green node in the figure), or a pre-specified maximum round of conversations is reached.</p>
<p>Note that while the above discussion utilized Sudoku solving as a tangible example to illustrate our main ideas, the ToT framework can potentially be applied to more general mathematical and logical reasoning tasks. For example, in the context of mathematical theorem proving, a full solution corresponds to the complete proof, encompassing a total of $n$ derivation steps. On the other hand, a partial solution refers to a subset of these steps, specifically the initial $k$ steps, where $k$ is less than $n$. The checker verifies the logically correctness of a given partial proof. In parallel, the prompter agent and the ToT controller can offer hints and suggestions to the LLM, encouraging it to think about the subsequent proving step, or explore different directions for theorem proving when necessary.</p>
<p>To evaluate the effectiveness of the ToT framework, we implemented a ToT-based Sudoku puzzle solver and evaluate it on a suite of Sudoku puzzle benchmarks we created. As shown by the experimental results in Section 4.2, the ToT framework can significantly increase the success rate of Sudoku puzzle solving.</p>
<p>The remainder of the paper is organized as follows. Section 2 reviews the related literature and compared our approach with the most relevant works. Section 3 provides the details of the ToT system architecture. Section 4 describes our implementation of a ToT-based Sudoku puzzle solver and presents the experimental results. Finally, Section 5 discusses the limitation of the present work, and potential future extensions of the ToT framework.</p>
<h1>2 Related Works</h1>
<p>Developing intelligent systems that can reason has long been one of the primary goals of artificial intelligence [10, 11, 12]. Recent advancements in large language models, particularly the discovery of their emergent properties and in-context learning abilities, have opened up a new avenue for machine reasoning [6, 7, 9]. It is discovered that prompting language models using chain-of-thought and other hints can elicit them to output step-by-step solutions for mathematical and logical reasoning tasks [9, 13]. Building on these findings, recent studies have also explored the practice of sampling multiple solutions and using self-consistency or complexity-based criteria to determine the optimal response [14, 15]. Experiments were also conducted to evaluate the performance of different prompts [15]. The self-taught reasoner (STaR) [16] is a technique which asks an LLM to generate reasoning chains and drop those producing incorrect answers. Then, the model is fine-tuned with the remaining valid reasoning chains.</p>
<p>Despite showing high potential, these techniques often necessitate human involvement. For example, chain-of-thought style prompting techniques require carefully hand-crafted examples and is thus difficult to scale. Consequently, researchers have started to explore the possibility of automatic prompt generation. Early exploration in this domain includes AutoPrompt [17], prefix-tuning [18], and parameter-efficient prompt tuning [19]. This research direction received even more attention lately. In a recent study [20], the authors experimented with training verifiers to check if the solution provided by an LLM to an given mathematical problem is logically correct. If the trained verifier can effectively judge the LLM outputs, it would provide another avenue for prompt evaluation. Automatic prompt engineer [21] examines a method to select the best prompt from a set of model-generated candidates. The three-phase augment-prune-select method was suggested in [22]. It first generates multiple chain-of-thought candidates, which was then pruned based on whether the derived answer matches with the ground truths. Finally, a policy gradient based method was used to select the optimal combination of several rationale chains from the pool for CoT prompting.</p>
<p>Very recently researchers have also turned their attention to augmenting LLM with additional agents for various purposes. This is also the research field that is most relevant to our current work. AutoGPT [23] is a program which combines GPT-4 with additional modules including an execution agent and a memory unit. It can chain together LLM "thoughts", in order to autonomously achieve whatever goal the user sets. PromptPG [24] proposes an approach that can learn to select in-context examples from a small amount of training data via policy gradient for prompt learning. The PromptPG agent learns to find optimal in-context examples from a candidate pool, with the goal of maximizing the prediction rewards on given training examples when interacting with the GPT-3 environment. DEPS [25] is a proposal that utilizes multi-step reasoning and sub-task error correction to tackle complex tasks with long-range dependencies. By being able to provide explanations for errors in sub-tasks within a trial, DEPS exhibits remarkable performance. ReAct [26] is an approach that employs emergent properties present in LLMs, such as traces of verbal reasoning, to enable agents to reason and take action, resulting in impressive performance on different text-based benchmarks. Building on top of ReAct, Reflexion [27] is an approach that equips an agent with dynamic memory and self-reflection capabilities, improving its existing reasoning trace and ability to choose task-specific actions. To achieve complete automation, a simple but effective heuristic was designed to enable the agent to identify hallucination instances and prevent repetitive action sequences. Our proposal shares some commonalities with these approaches, for example, the use of a memory module and additional agents for automatic prompt generation. However, our approach is unique in that it introduces a ToT controller which can explicitly conduct backtracking when necessary. This not only allows the system to recover from mistakes, but potentially can also enlarge the solution search space.</p>
<h1>3 Architecture</h1>
<h3>3.1 The Tree-of-Thought Framework</h3>
<p>Figure 1b depicts the software system that implements the ToT Framework. As mentioned earlier, it incorporates several components which enhance the problem solving capability of the LLM, including a prompter agent, a checker module, a memory module, and a ToT controller.</p>
<p>The problem solving process starts with the user inputting the problem description. The prompter agent then relays the problem to the LLM, with additional prompt text which encourages the LLM to come up with an intermediate solution instead of trying to reach the full solution in a single shot. After receiving the response from the LLM, the checker module is invoked to check the validity of the intermediate solution generated. If it passes the correctness check, the intermediate solution will be parsed and stored in the memory module. Then, based on the content of the memory module, the prompter agent generates a prompt to encourage the LLM to generate the next step. Conversely, if the LLM generates an invalid intermediate solution, the ToT controller will activate the prompter to offer hints to the LLM and request it to consider again. Note that in general, a valid intermediate solution does not always leads to the correct final solution. In order to prevent getting stuck, the ToT controller constantly monitors the search process and determines whether to continue trying from the current node or backtrack to a parent or an ancestor node and explore alternative directions.</p>
<p>The ToT strategy can be viewed as a tree-search algorithm using an LLM as a heuristic for generating the search steps. In this setting, the LLM is only used for the "short-range reasoning" tasks, i.e deriving the next intermediate solution, which is a type of tasks that have been shown to have a high success rate for LLMs [7]. On the other hand, by introducing the checker, the system have a higher likelihood to discover the mistakes it makes as it generates the solutions. Moreover, by allowing the system to backtrack from a valid but somewhat "hopeless" intermediate solution, the system is able to explore a larger solution space, which enhances the "long-range reasoning" capability of the system as a whole. The ToT framework thus combines the best of both world. Furthermore, this multi-round conversation technique increases the number of computation steps the system can perform. Thus, based on the time hierarchy theorem in computational complexity theory [28], the ToT framework can expand the range of problems that can potentially be solved compared to relying solely on a single round of conversation with an LLM.</p>
<h3>3.2 ToT Modules</h3>
<p>In this section we provide more details of the components of the ToT software system.</p>
<p>Checker Module. The checker module can either be rule-based or implemented as a deep neural network. For problems that have an explicit polynomial time algorithm for correctness checking (i.e. problems in NP), rule-based checkers can be implemented. Numerous important mathematical and logical problems are in this category, for example, equation solving, polynomial factoring, 3SAT, and puzzles like Sudoku. With a rule-based checker, the ToT software can be viewed as a hybrid system which allows explicitly encoding prior knowledge (e.g. the Sudoku rules) into a neural network powered system. An alternative is to train and use a neural network based classifier as the checker [20]. This is especially useful for problems where a rule-based checker is difficult to implement, e.g. checking whether a mathematical proof is correct.</p>
<p>Memory Module. The memory module can be used to store the entire conversation history between the LLM and the prompter agent, as well as other supplemental data useful for problem solving. The data stored can be served as the information source for the prompter agent to generate helpful hints for the LLM.</p>
<p>ToT Controller. The ToT controller oversees the entire ToT search. It can be implemented in a number of ways. It can be as simple as encoding two rules: 1) if the checker thinks the current partial solution is invalid, backtrack to the parent node, and 2) if the current partial solution is valid, but the ToT search tree has explored its $C$ children and yet failed to find the final solution, then backtrack to the parent node. Here $C$ is an pre-configured integer.</p>
<p>A more advanced version of the ToT controller can employ a policy network to determine the backtracking policy. The network's inputs include the recent search history comprised of the sequence of the last $k+1$ node visited in the search tree $s_{i-k}, . ., s_{i-1}, s_{i}$ ( $k$ is a hyper-parameter). The network also takes in $c_{i}$, a Boolean variable which indicates whether the checker module considers the current node $s_{i}$ is valid. We can sample from the policy to determine the next action $a_{i}$ :</p>
<p>$$
a_{i} \sim \pi_{\rho}^{t}\left(a \mid c_{i}, s_{i}, . ., s_{i-k}\right), a \in A_{\text {cand }}
$$</p>
<p>where $\pi_{\rho}^{t}$ represents the policy network of the ToT controller with parameters $\rho$. The set of candidate actions $A_{\text {cand }}$ includes simply staying at the current node to generate the next step, and backtracking to the parent or an ancestor node at most $L$ levels up in the search tree where $L$ is a hyper-parameter. Thus, we can use one-hot encoding for the actions, where backtracking $j$ levels up is represented by a vector where only the $j^{\text {th }}$ position is set to 1 . The action vector $a$ and checker output $c_{i}$ are processed by a feed-forward network (FFN) to for deep features extraction. A linear layer with learnable parameters $\mathbf{W}<em 1="1">{1}$ and $\mathbf{b}</em>}$ is added on top of the FFN to map its output to a vector $\mathbf{g}\left(a, c_{i}\right)$. The latest $k+1$ visited nodes are concatenated into a string, and then added with position embedding (PE), and finally inputted into a self-attention model [1]. The idea is that by adding position embedding, the attention model will be able to make decisions based on the sequence of the recent node visits. A linear layer with learnable parameters $\mathbf{W<em 2="2">{2}$ and $\mathbf{b}</em>\right)$. Finally, we calculate the inner-products of these two vectors, and use the softmax function to compute the probability of each action candidate:}$ is added on top of the attention model to transform its output to a vector $\mathbf{g}\left(s_{i}, . ., s_{i-k}\right)$ whose dimension matches with that of $\mathbf{g}\left(a, c_{i</p>
<p>$$
\begin{aligned}
\mathbf{g}\left(a, c_{i}\right) &amp; =\mathbf{W}<em i="i">{1} \cdot \operatorname{FFN}\left(a, c</em>}\right)+\mathbf{b<em i="i">{1} \
\mathbf{g}\left(s</em>}, . ., s_{i-k}\right) &amp; =\mathbf{W<em i-k="i-k">{2} \cdot \operatorname{Attention}\left(\operatorname{PE}\left(s</em>}\left||. .| \mid s_{i-1} \mid s_{i}\right)\right)+\mathbf{b<em _rho="\rho">{2}\right. \
\pi</em>
\end{aligned}
$$}^{t}\left(a \mid c_{i}, s_{i}, . ., s_{i-k}\right) &amp; =\frac{\exp \left(\mathbf{g}\left(a, c_{i}\right) \cdot \mathbf{g}\left(s_{i}, . ., s_{i-k}\right)\right)}{\sum_{a^{\prime} \in A_{\text {cand }}} \exp \left(\mathbf{g}\left(a^{\prime}, c_{i}\right) \cdot \mathbf{g}\left(s_{i}, . ., s_{i-k}\right)\right)</p>
<p>In the above formula, " $l$ " is the string concatenation operator. Section 3.3 will discuss the training algorithm for the ToT controller policy network.</p>
<p>Prompter Agent. The prompter agent gives hints to the LLM for it to generate the next search step. The most basic hint can be a generic prompt using the following template: generic_tmpl $=$ "For the given problem: [problem description], we have come up with a partial solution: [partial solution summary]. Please derive the next step on top of this partial solution, and return the next step in the following JSON format [next_step: <next_step>]". Note that the template requires the LLM to respond with a structured JSON string. This is a trick to make it easier for the checker to extract the next step from the LLM response. To create an actual prompt from this template, the prompter needs the [problem description] and the [partial solution summary], both of which can be queried from the memory module.</p>
<p>Algorithm 1 Policy Gradient based Training Algorithm for the ToT System
1: Input: training set $P_{\text {train }}$, num of training epochs $N$
2: procedure REINFORCE( $P_{\text {train }}, N$ )
3: randomly initialized the ToT Controller policy $\pi_{\rho}^{t}$
4: randomly initialized the Prompter agent policy $\pi_{\theta}^{p}$
5: for $e p o c h=1,2, . ., N$ do
6: $\pi_{w} \leftarrow \pi_{\rho}^{t}$ if $e p o c h$ is even, $\pi_{\theta}^{p}$ otherwise $\triangleright$ update the selected policy only, fix the other
7: for $p_{i} \in P_{\text {train }}$ do
8: $\quad r_{i} \leftarrow \operatorname{reward}\left(\operatorname{ToTSystem}\left(p_{i}\right)\right) \quad \triangleright$ attempt to solve problem $p_{i}$ and obtain reward $r_{i}$
9: $\quad w \leftarrow w+\alpha \nabla_{w} \log \pi_{w} r_{i}$
10: end for
11: end for
12: end procedure</p>
<p>Similar to the ToT controller, we can also implement the prompter agent as a policy network, which can generate prompts based on the current partial solution and the conversation history. First we define the prompt template as follows: prompt_tmpl $=$ generic_tmpl || "Here are a few examples: [in-context learning examples],", where || is the string concatenation operator. The variable [in context learning examples] are in-context learning examples for the problem being solved, which can be picked by the prompter policy network from a set of candidates, similar to the PromptPG approach [24]. The rationale is that given the current and recently attempted intermediate solution, some in-context examples might work better than others as hints for the next step. Given the recently visited node sequence $s_{i-k}, . ., s_{i-1}, s_{i}$, our goal is to select $l$ examples $e_{i}=\left{e_{i}^{1}, e_{i}^{2}, \ldots, e_{i}^{l} \mid e_{i}^{j} \in E_{\text {cand }}\right}$ where $E_{\text {cand }}$ is a pool of in-context learning example candidates. The examples are selected according on a policy:</p>
<p>$$
e_{i}^{j} \sim \pi_{\theta}^{p}\left(e \mid s_{i}, . ., s_{i-k}\right), e_{i}^{j} \in E_{\text {cand }} \text { for } j=1,2, \ldots, l
$$</p>
<p>where $\pi_{\theta}^{p}$ represents the policy network of the prompter agent with parameters $\theta$. With the set of selected examples, the prompter agent generates a prompt from the template: $p_{i}=$ prompter $\left(\right.$ prompt_tmpl, $\left.e_{i}, s_{i}\right)$, which can be fed into the LLM to obtain the next intermediate solution $s_{i+1}=L L M\left(p_{i}\right)$. The neural network architecture for the prompter's policy network is similar to that of the ToT controller. The only difference is that since the in-context examples are expressed in natural language, instead of FFN, we use an attention model to process them:</p>
<p>$$
\begin{aligned}
\mathbf{h}(e) &amp; =\mathbf{M}<em 1="1">{1} \cdot \operatorname{Attention}(e)+\mathbf{c}</em> \
\mathbf{h}\left(s_{i}, . ., s_{i-k}\right) &amp; =\mathbf{M}<em i-k="i-k">{2} \cdot \operatorname{Attention}\left(\operatorname{PE}\left(s</em>}\left|..\left|s_{i-1}\right| s_{i}\right)\right)+\mathbf{c<em _theta="\theta">{2}\right. \
\pi</em>
\end{aligned}
$$}^{p}\left(e \mid s_{i}, . ., s_{i-k}\right) &amp; =\frac{\exp \left(\mathbf{h}(e) \cdot \mathbf{h}\left(s_{i}, . ., s_{i-k}\right)\right)}{\sum_{e^{\prime} \in E_{\text {cand }}} \exp \left(\mathbf{h}\left(e^{\prime}\right) \cdot \mathbf{h}\left(s_{i}, . ., s_{i-k}\right)\right)</p>
<p>The prompter policy network can be trained together with the ToT controller using multi-agent reinforcement learning methods. The training algorithm of the prompter's policy network is discussed in Section 3.3.</p>
<h1>3.3 ToT System Training</h1>
<p>In the previous sections, we have described the multi-agent ToT framework. This section dives into how we can train the agents, in particular, the policy networks of the ToT controller and the prompter agent. While there are many multi-agent reinforcement learning algorithms (MARL) proposed in the literature [29, 30, 31], in this work we adopt a relatively simple approach which uses a modified version of the REINFORCE algorithm [32] to train the policy networks of the ToT controller and the prompter agent directly. The more advanced MARL algorithms will be explored in the future.
First, we define a run of the ToT system as the process where a user inputs the problem description, and the ToT system attempts to solve the problem until it thinks the problem is solved, or a prespecified maximum round of conversations is reached. Next, we define the reward $r$ of a run: if the problem is correctly solved, then $r=+1$. Otherwise, if the system outputs an incorrect solution, or the maximum round of conversations is reached, then $r=-1$.</p>
<div class="codehilite"><pre><span></span><code><span class="n">Algorithm</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="n">Problem</span><span class="w"> </span><span class="n">Solving</span><span class="w"> </span><span class="n">Using</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">ToT</span><span class="w"> </span><span class="n">System</span>
<span class="w">    </span><span class="n">Input</span><span class="p">:</span><span class="w"> </span><span class="n">problem</span><span class="w"> </span><span class="n">description</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">user</span><span class="w"> </span>\<span class="p">(</span><span class="n">p_</span><span class="p">{</span>\<span class="n">text</span><span class="w"> </span><span class="p">{</span><span class="n">user</span><span class="w"> </span><span class="p">}}</span>\<span class="p">),</span><span class="w"> </span><span class="nb">max</span><span class="w"> </span><span class="n">num</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">conversation</span><span class="w"> </span><span class="n">rounds</span><span class="w"> </span>\<span class="p">(</span><span class="n">K</span>\<span class="p">)</span>
<span class="w">    </span><span class="n">procedure</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">operatorname</span><span class="p">{</span><span class="n">SOLVE</span><span class="p">}</span>\<span class="n">left</span><span class="p">(</span><span class="n">p_</span><span class="p">{</span>\<span class="n">text</span><span class="w"> </span><span class="p">{</span><span class="n">user</span><span class="w"> </span><span class="p">}},</span><span class="w"> </span><span class="n">K</span>\<span class="n">right</span><span class="p">)</span>\<span class="p">)</span>
<span class="w">        </span><span class="n">prompt</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">leftarrow</span><span class="w"> </span>\<span class="n">operatorname</span><span class="p">{</span><span class="n">Prompter</span><span class="p">}</span>\<span class="n">left</span><span class="p">(</span><span class="n">p_</span><span class="p">{</span>\<span class="n">text</span><span class="w"> </span><span class="p">{</span><span class="n">user</span><span class="w"> </span><span class="p">}}</span>\<span class="n">right</span><span class="p">)</span>\<span class="p">)</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="nb">round</span><span class="w"> </span>\<span class="p">(</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="o">.</span><span class="w"> </span><span class="o">.</span><span class="p">,</span><span class="w"> </span><span class="n">K</span>\<span class="p">)</span><span class="w"> </span><span class="n">do</span>
<span class="w">            </span><span class="n">response</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">leftarrow</span><span class="w"> </span>\<span class="n">operatorname</span><span class="p">{</span><span class="n">LLM</span><span class="p">}(</span>\<span class="p">)</span><span class="w"> </span><span class="n">prompt</span><span class="w"> </span>\<span class="p">()</span>\<span class="p">)</span>
<span class="w">            </span><span class="n">result</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">leftarrow</span>\<span class="p">)</span><span class="w"> </span><span class="n">Checker</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="n">result</span><span class="o">.</span><span class="n">isValidFinalSolution</span><span class="p">()</span><span class="w"> </span><span class="n">then</span>
<span class="w">                </span><span class="k">return</span><span class="w"> </span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">solution</span><span class="p">)</span>
<span class="w">            </span><span class="n">end</span><span class="w"> </span><span class="k">if</span>
<span class="w">            </span><span class="n">memory</span><span class="o">.</span><span class="n">store</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
<span class="w">            </span><span class="n">ctrl_signal</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">leftarrow</span>\<span class="p">)</span><span class="w"> </span><span class="n">ToTController</span><span class="p">(</span><span class="n">memory</span><span class="p">)</span>
<span class="w">            </span><span class="n">prompt</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">leftarrow</span>\<span class="p">)</span><span class="w"> </span><span class="n">Prompter</span><span class="p">(</span><span class="n">memory</span><span class="p">,</span><span class="w"> </span><span class="n">ctrl_signal</span><span class="p">)</span>
<span class="w">        </span><span class="n">end</span><span class="w"> </span><span class="k">for</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="p">(</span><span class="n">nil</span><span class="p">)</span>
<span class="w">    </span><span class="n">end</span><span class="w"> </span><span class="n">procedure</span>
</code></pre></div>

<p>The training algorithm is provided in Algorithm 1. The algorithm takes two inputs, the training data set $P_{\text {train }}$, and the number of training epochs $N$ (Line 1-2). The two policy networks $\pi_{p}^{l}\left(a_{i} \mid s_{i}, . ., s_{i-k}\right)$ and $\pi_{p}^{p}\left(e_{i} \mid s_{i}, . ., s_{i-k}\right)$ are randomly initialized (Line 3-4). We train the two policy networks in turns, i.e. training one network with policy gradient while keeping the other fixed (Line 6). To be more specific, when the current epoch is an even number, we select the ToT controller policy $\pi_{p}^{l}$, and keep the parameters of the prompter agent fixed. Otherwise, we select the prompter agent policy $\pi_{p}^{p}$ and fix the ToT controller policy. Next, the algorithm updates the parameters of the selected policy network using the policy gradient method (Line 7-9). For each problem in the training data, we attempt to solve it with a ToT system run. Based on the result, we obtain the reward for that run (Line 8). The entire training algorithm runs for $N$ epochs.</p>
<h1>3.4 Problem Solving Using the ToT System</h1>
<p>After the ToT system is trained, we can use it for inference, i.e. problem solving. Algorithm 2 provides the pseudo code for solving problems using the ToT system. It starts with a user inputting description of the problem (Line 1-2). The prompter module then converts the user input into a prompt (Line 3) using a prompt template for user input, for example: user_input_prompt $=$ "For the given problem: [problem description], please derive the first step, and return the step in the following JSON format [next_step: <next_step>]".</p>
<p>Next, up to $K$ rounds of conversations with the LLM are conducted for problem solving (Line 4). In each round, the LLM first produces a response for the given prompt (Line 5). Then, the checker analyzes the response, and returns a result (Line 6). The result contains the partial solution extracted from the LLM response, as well as information like whether the checker considers the solution as a valid final solution, a valid intermediate solution, or an invalid partial solution, etc. If the solution is a valid final solution, the algorithm simply returns it (Line 7-9). Otherwise, the result is stored in the memory module (Line 10). Based on the content of the memory module, the ToT controller issues control signals, e.g. backtracking for $l$ levels, to the prompter (Line 11). Finally, based on the control signal, the prompter looks up the relevant information from the memory module, and produce the next prompt for the LLM (Line 12). If no valid final solution is found within $K$ rounds of conversations, the algorithm return nil indicating it fails to solve the problem (Line 14).</p>
<h2>4 Evaluation</h2>
<p>This section provides the evaluation methodology and experimental results for our proposed ToT framework. Our evaluation focuses on the ToT-based solver for the Sudoku problem. At the first glance, Sudoku problems seem to be just brain teasers with little practical importance. However, the generalized Sudoku problem on $n^{2} \times n^{2}$ grids of $n \times n$ blocks is known to be NP-complete [33]. If the ToT framework can solve instances of the generalized Sudoku problem (granted that it might takes an exponential number of rounds of conversations), in principle it can handle many</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Experimental results comparing the success rate of different LLM-based Sudoku puzzle solvers across three sets of benchmarks.
other mathematical and logical reasoning tasks. In fact, it is straightforward to re-purpose the implementation described below to solve other puzzles, such as 3SAT, 3-coloring, etc. Below we first describe the implementation details of the solver. Then, we present the test suite used in our evaluation, as well as the experimental results.</p>
<h1>4.1 ToT Solver for Sudoku Puzzles</h1>
<p>The ToT-based Sudoku solver follows the generic framework described in Section 3 with some specific tweaks for the Sudoku problem. It allows a user to input a Sudoku puzzle using natural languages, for example: "Please solve this 4x4 Sudoku puzzle [[3,<em>,</em>,2], $[1, <em>, 3, </em>],[<em>, 1, </em>, 3],[4, <em>, </em>, 1]]$ where * represents a cell to be filled".</p>
<p>We have implemented the ToT-based Sudoku solver as described in Section 4.1 in Python. We adopted a rule-based approach for the checker module since the Sudoku rules are precise and easy to check. The memory module stores the conversation history between the prompter and the LLM, as well as a search tree which maintains all the partially filled Sudoku board the LLM has generated so far. This way, when backtracking happens, the previous board configuration can be retrieved. The ToT controller in our implementation is also rule-based. It returns to the parent node in the search tree if either the current node considered invalid by the checker, or the search algorithm has explored more than 5 children of the current node. Finally the prompter agent uses a variation of the generic template mentioned above, with the [problem description] being the initial configuration of the Sudoku board input by the user, and [partial solution summary] being the partially filled board represented by the current node in the search tree. The LLM utilized in this study is the "gpt-3.5-turbo" model, which is accessible through the OpenAI API suite. The temperature parameter was set to 1 in our experiments.</p>
<h3>4.2 Experimental Results</h3>
<p>We have implemented four LLM-based Sudoku puzzle solvers and compared their performance: 1) zero-shot solver (zs) which directly posts the puzzle description to the LLM, 2) one-shot solver (os) which provides a chain-of-thought style step-by-step solution of a 3x3 Sudoku puzzle as an example in addition to the problem description, 3) few-shot solver (fs) which provides multiple examples with CoT-style solutions, and 4) our proposed Tree-of-Thought solver (tot). We constructed three benchmarks, comprising of ten $3 \times 3,4 \times 4$, and $5 \times 5$ Sudoku puzzles, respectively. The objective of a solver is to fill the $n \times n$ Sudoku grid with digits so that each row and column contain all of the digits from 1 to $n(n=3,4,5$ in our experiments).
Figure 2 compares the success rates of different LLM-based solvers across the three benchmarks. Here the term success rate refers to the fraction of problems in a benchmark set that are successfully solved by a solver. For example, if a solver is able to solve 4 out of 10 problems in the " $3 \times 3$ puzzles" benchmark set, then the success rate of this solver for this benchmark set is 0.4 . As expected, the zero-shot solver has the worst performance across all the three set of benchmarks. Adding CoT-style step-by-step examples significantly boosts the success rate, especially for the $3 \times 3$ puzzles. This is expected, since one can pretty much rely on "short-range" reasoning skills, which is a strength of the</p>
<p>LLM models, to solve a small-sized 3x3 Sudoku puzzle, espcially when CoT-style hints are provided. However, as the puzzle size gets bigger, the success rate of the one-shot and few-shot solver dropped to around 0.5 . This is because solving bigger puzzles requires trial and error, which is a capability LLMs generally lack of as discussed earlier.</p>
<p>In comparison, the ToT-based solver demonstrates superior performance when compared to the other solvers. For the 3x3 benchmark set, it was able to solve all the puzzles. The success rate improves by $11 \%$ compared to the second best for the two benchmark sets. For the $4 \times 4$ benchmark set, the ToT-based solver failed to find the solution for 1 out of the 10 puzzles before reaching the maximum round of conversations (which is set to 100 in our experiments). We suspect it is due to the limited capability of the rule-based ToT controller. In particular, the rule-based controller has no sense of whether the current partially-filled board can be completed without violating the Sudoku rules, which decreases the efficiency of the solution search. We expect a neural network based ToT controller will perform better, which we will verify in the future extension of this work. Despite this, the success rate of the ToT based solver is still $80 \%$ higher compared to that of the one-shot and few-shot based solvers. Finally, for the $5 \times 5$ puzzles, the ToT-based solver failed with 2 puzzles before reaching the maximum round of conversations. Nonetheless, the success rate is $60 \%$ higher compared to that of the one-shot and few-shot based solvers.</p>
<h1>5 Discussions and Future Works</h1>
<p>In this paper, we presented the Tree-of-Thought framework, which enhances LLMs with additional agents and memory modules, resulting in improved performance for mathematical problem-solving tasks. To evaluate the performance of this technique, we implemented a Sudoku puzzle solver based on the ToT framework. One of the limitations of the current implementation is that it utilizes a rule-based checker that contains custom logic, making it less easily adaptable to other problems. For more generic problems, for example, general mathematical and logical reasoning problems, where rule-based solution checking is difficult to implement, a future direction is to explore checkers based on neural network or other probabilistic models. Moreover, the experiments we conducted in this work also uses a rule-based ToT controller, which as we pointed out, has limited capabilities. In the future, we will implement the neural network based ToT controller which can hopefully enhance the system performance. Additionally, the policy-gradient based training algorithm proposed in this work is relatively simple and may be susceptible to training stability issues. To further optimize the ToT system, more advanced multi-agent reinforcement learning algorithms, particularly those designed for cooperative agents, could be adopted.
Another intriguing future direction is to investigate the potential of utilizing the "self-play" technique to enable the ToT system to develop novel problem solving strategies that are not found in the LLM's training text corpus. The self-play training method is a reinforcement learning technique which was popularized by the development of competitive game-playing agents such as AlphaGo and AlphaStar $[34,35,36]$, where an AI agent learns to improve its own strategy by playing against itself. Today's LLMs are typically trained using self-supervised learning techniques. They may have limitations when it comes to problem-solving, as they may not be able to generate samples (i.e. novel problem solving strategies) that fall outside the distribution of the training data. In other words, they may not be able to "think outside the box", which is a crucial human trait that facilitates the discovery of new knowledge. Compared to self-supervised learning, self-play based reinforcement learning enables the system to access a much broader solution space beyond the provided training examples, allowing for greater improvement. AlphaGo and similar systems have demonstrated the ability to devise strategies that surpass even those of human experts. Inspired by these examples, for ToT system training, instead of relying on the training data set $P_{\text {train }}$, we can introduce a "quizzer" module which can come up with problem descriptions on its own to train the ToT controller and the prompter agent. It is worth mentioning that one of the key enablers for training AlphaGo and similar system is that the environment reward can be precisely determined, as it is straightforward to determine whether the gameplay results in a win or a loss. The ToT framework incorporates a checker that can assess the correctness of the solution, functioning similarly to the environment, particularly for problems that have well-defined solution validation rules. Thus, the reinforcement learning training methods can be readily applied. We suspect that this self-driven learning approach, similar to the self-play method, could be an effective means of improving the ToT framework's problem-solving capabilities beyond the solution examples provided in the training text corpus for the LLMs.</p>
<h1>References</h1>
<p>[1] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2017.
[2] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding, 2019.
[3] Alec Radford and Karthik Narasimhan. Improving language understanding by generative pre-training. 2018.
[4] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019.
[5] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020.
[6] OpenAI. Gpt-4 technical report, 2023.
[7] Sbastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. Sparks of artificial general intelligence: Early experiments with gpt-4, 2023.
[8] Shu Tay, Paul Ryan, and Anthony C Ryan. Systems 1 and 2 thinking processes and cognitive reflection testing in medical students. Canadian Medical Education Journal, 2016:97-103, 112016.
[9] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023.
[10] Larry Wos, Ross Overbeck, Ewing Lusk, and Jim Boyle. Automated Reasoning: Introduction and Applications. Prentice Hall Professional Technical Reference, 1984.
[11] Frederick Hayes-Roth, Donald A. Waterman, and Douglas B. Lenat. Building Expert Systems. AddisonWesley Longman Publishing Co., Inc., USA, 1983.
[12] Ronald Fagin, Joseph Y. Halpern, Yoram Moses, and Moshe Y. Vardi. Reasoning About Knowledge. MIT Press, Cambridge, MA, USA, 2003.
[13] Iddo Drori, Sarah Zhang, Reece Shuttleworth, Leonard Tang, Albert Lu, Elizabeth Ke, Kevin Liu, Linda Chen, Sunny Tran, Newman Cheng, Roman Wang, Nikhil Singh, Taylor L. Patti, Jayson Lynch, Avi Shporer, Nakul Verma, Eugene Wu, and Gilbert Strang. A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level. Proceedings of the National Academy of Sciences, 119(32), aug 2022.
[14] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models, 2023.
[15] Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. Complexity-based prompting for multi-step reasoning, 2023.
[16] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. Star: Bootstrapping reasoning with reasoning, 2022.
[17] Taylor Shin, Yasaman Razeghi, Robert L. Logan IV au2, Eric Wallace, and Sameer Singh. Autoprompt: Eliciting knowledge from language models with automatically generated prompts, 2020.
[18] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation, 2021.
[19] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning, 2021.
[20] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021.
[21] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. Large language models are human-level prompt engineers, 2023.
[22] KaShun Shum, Shizhe Diao, and Tong Zhang. Automatic prompt augmentation and selection with chain-of-thought from labeled data, 2023.
[23] Auto-gpt: An autonomous gpt-4 experiment, 2023. https://github.com/Significant-Gravitas/ Auto-GPT.</p>
<p>[24] Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan. Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning, 2023.
[25] Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents, 2023.
[26] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models, 2023.
[27] Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic memory and self-reflection, 2023.
[28] Juris Hartmanis and R. Stearns. On the computational complexity of algorithms. Transactions of The American Mathematical Society - TRANS AMER MATH SOC, 117:285-285, 051965.
[29] Thanh Nguyen, Ngoc Duy Nguyen, and Saeid Nahavandi. Deep reinforcement learning for multiagent systems: A review of challenges, solutions, and applications. IEEE Transactions on Cybernetics, PP:1-14, 032020 .
[30] Afshin OroojlooyJadid and Davood Hajinezhad. A review of cooperative multi-agent deep reinforcement learning, 2021.
[31] Kaiqing Zhang, Zhuoran Yang, and Tamer Baar. Multi-agent reinforcement learning: A selective overview of theories and algorithms, 2021.
[32] Richard S. Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In Proceedings of the 12th International Conference on Neural Information Processing Systems, NIPS'99, page 1057-1063, Cambridge, MA, USA, 1999. MIT Press.
[33] Michael Haythorpe. Reducing the generalised sudoku problem to the hamiltonian cycle problem, 2016.
[34] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484-489, 2016.
[35] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen Simonyan, and Demis Hassabis. Mastering chess and shogi by self-play with a general reinforcement learning algorithm, 2017.
[36] Oriol Vinyals, Igor Babuschkin, Wojciech Czarnecki, Michal Mathieu, Andrew Dudzik, Junyoung Chung, David Choi, Richard Powell, Timo Ewalds, Petko Georgiev, Junhyuk Oh, Dan Horgan, Manuel Kroiss, Ivo Danihelka, Aja Huang, Laurent Sifre, Trevor Cai, John Agapiou, Max Jaderberg, and David Silver. Grandmaster level in starcraft ii using multi-agent reinforcement learning. Nature, 575, 112019.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://newsroom.ucla.edu/releases/Terence-Tao-Mozart-of-Math-7252
${ }^{2}$ The word "tot" means a very young child, which is an interesting analogy as this work is a preliminary exploration into the potential for automated problem-solving utilizing language models.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
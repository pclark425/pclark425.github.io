<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2615 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2615</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2615</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-66.html">extraction-schema-66</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <p><strong>Paper ID:</strong> paper-1c3c531fc0fbe79f97f367ed3648de8467caeeaa</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/1c3c531fc0fbe79f97f367ed3648de8467caeeaa" target="_blank">SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> SWE-agent is introduced: a system that facilitates LM agents to autonomously use computers to solve software engineering tasks and its custom agent-computer interface (ACI) significantly enhances an agent's ability to create and edit code files, navigate entire repositories, and execute tests and other programs.</p>
                <p><strong>Paper Abstract:</strong> Language model (LM) agents are increasingly being used to automate complicated tasks in digital environments. Just as humans benefit from powerful software applications, such as integrated development environments, for complex tasks like software engineering, we posit that LM agents represent a new category of end users with their own needs and abilities, and would benefit from specially-built interfaces to the software they use. We investigate how interface design affects the performance of language model agents. As a result of this exploration, we introduce SWE-agent: a system that facilitates LM agents to autonomously use computers to solve software engineering tasks. SWE-agent's custom agent-computer interface (ACI) significantly enhances an agent's ability to create and edit code files, navigate entire repositories, and execute tests and other programs. We evaluate SWE-agent on SWE-bench and HumanEvalFix, achieving state-of-the-art performance on both with a pass@1 rate of 12.5% and 87.7%, respectively, far exceeding the previous state-of-the-art achieved with non-interactive LMs. Finally, we provide insight on how the design of the ACI can impact agents' behavior and performance.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2615.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2615.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SWE-agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SWE-agent (Agent-Computer Interface for Software Engineering)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LM-based autonomous software engineering agent that combines a large language model with a tailored agent-computer interface (ACI) to search, navigate, edit, and execute code in repositories to resolve real-world GitHub issues.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SWE-agent</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>SWE-agent pairs a fixed large language model (e.g., GPT-4 Turbo, Claude 3 Opus) with a specially designed agent-computer interface (ACI). The ACI exposes a small, LM-friendly action set (search_dir, search_file, find_file, open, goto, scroll_up/down, edit, create, submit, plus selective shell commands), a concise, standardized feedback format (file viewer windows, summarized search results), history/context management, and guardrails (linting on edits, search result limits). The agent operates in ReAct fashion: it iteratively generates a thought and a command, executes the command in a sandboxed environment (Docker), inspects the returned observation (including test output), and issues further actions until it submits a patch.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Automated Software Engineering Agent / AI Scientist (software engineering)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Software engineering — repository-level bug localization, automated program repair, and end-to-end issue resolution on real-world Python codebases (SWE-bench) and short-form debugging tasks (HumanEvalFix).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Autonomously resolve issues collected from GitHub (SWE-bench): localize the root cause across files, optionally write reproduction scripts, produce multi-line edits to source files, run tests/reproduction scripts, and submit a patch. Tasks range from small self-contained functional bug fixes (SWE-bench Lite) to broader repository-level issues across 12 popular Python packages.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High relative complexity for code-generation benchmarks: cross-file reasoning, large search space (multiple files, symbols), need for correct multi-line edits with precise indentation and API usage, multi-step edit-execute-debug loops. Quantitative scope: SWE-bench contains 2,294 test instances; SWE-bench Lite is 300 self-contained instances. Evaluation requires passing human-written unit tests (deterministic execution environment).</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Pre-existing real-world codebases and human-written unit tests from GitHub (SWE-bench). Data are available (repository snapshots + issue descriptions). Reproduction scripts are often created by the agent; tests are provided and used as ground-truth evaluators. No additional costly experimental data collection required beyond repository checkout and test execution.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Uses large API-based LMs with very large context windows (GPT-4 Turbo: 128k tokens; Claude 3 Opus: 200k tokens). Per-instance budget capped at $4 for experiments; SWE-agent average API inference cost for successfully resolved instances: $1.59 (full SWE-bench, GPT-4 Turbo) and $1.67 (Lite). Agents interact iteratively (median successful run: 12 steps, median cost $1.21). Execution runs in Docker containers to safely run tests and scripts.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Well-defined evaluation: deterministic program execution with clear unit-test-based success metric (% Resolved / pass@1). Problem is discrete (sequence of agent actions), multi-step, and requires domain knowledge of codebase APIs; environment deterministic given same edits and tests. Open-ended in strategy (localization/reproduction/edit patterns vary) but success is measured by deterministic tests.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>% Resolved (pass@1): proportion of instances where all tests pass after the agent's patch; additionally pass@1 on HumanEvalFix tasks for language-specific debugging.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>On SWE-bench (full, 2,294 instances) SWE-agent w/ GPT-4 Turbo resolved 12.47% (286/2,294). On SWE-bench Lite (300 instances) SWE-agent w/ GPT-4 Turbo resolved 18.00% (54/300). HumanEvalFix pass@1 (SWE-agent w/ GPT-4 Turbo): Python 87.7%, JS 89.7%, Java 87.9%. Average API cost per successful instance (SWE-bench full, GPT-4 Turbo): $1.59.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Major failure modes: incorrect implementations or overly specific implementations (~52% of unresolved instances), cascading failed edits (~23.4% of failures). Editing is a frequent source of failure: 51.7% of trajectories had ≥1 failed edit (linting error). Recovery probability: any edit has a 90.5% eventual success chance; after one failed edit the chance of succeeding later drops to 57.2%. Other failure drivers: inefficient search/exhausting budget, insufficient generality of fixes, and repeated edits introducing syntax errors.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>ACI design choices: simple and compact actions that align with LM capabilities, concise informative feedback (file-windowed viewer, summarized search), guardrails such as linting that prevent cascading errors, context management that collapses older observations (keep last 5 full), and large LM context windows. Demonstrations also helped (Shell-only with demo better than without).</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>SWE-agent substantially outperforms baselines: on Lite, SWE-agent w/ GPT-4 Turbo: 18.00% resolved vs Shell-only agent 11.00% (both GPT-4 Turbo) and RAG 2.67% (GPT-4 Turbo). SWE-agent is costlier (8–13× higher cost than RAG) but yields ~6.7× improvement in % Resolved versus RAG. SWE-agent shows a 64% relative increase vs Shell-only when using same LM (GPT-4 Turbo). Claude 3 Opus variant: SWE-agent resolves 10.46% (full) and 13.00% (Lite).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2615.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2615.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Shell-only agent (InterCode)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Shell-only LM agent (InterCode environment)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LM agent that interacts directly with a Linux shell (bash) and standard command-line utilities for code navigation, inspection, and editing; used as an interactive baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Intercode: Standardizing and benchmarking interactive coding with execution feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Shell-only agent (InterCode-style)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>LM agent interacts with a raw shell environment using standard bash commands and utilities (cd, ls, cat, grep, sed, python, etc.) to localize, edit, and run code. There is no LM-tailored ACI; editing and navigation are achieved by conventional shell workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LM agent using standard shell interface / Interactive coding agent</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Software engineering tasks (SWE-bench) — repository-level bug localization and repair.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Resolve SWE-bench issues via multi-step shell interactions: search for files/strings, view file contents via shell tools, edit files via redirection/sed, run tests or reproduction scripts, and submit patches.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Same repository-level complexity as SWE-bench; agents struggle with verbosity of shell outputs, inefficient multi-command workflows, and arithmetic/formatting for line-oriented edits.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Uses the same GitHub repositories and tests as SWE-bench; data pre-existing and available.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Uses same LM backends as SWE-agent in experiments; average cost on Lite: $1.46 (Shell-only w/ GPT-4 Turbo). Demonstrations increase performance but also cost.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Deterministic program execution with unit-test evaluation; discrete action space (shell commands) that is more granular and noisy compared to LM-tailored ACI.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>% Resolved (pass@1) based on tests after final patch.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>On SWE-bench Lite, Shell-only agent w/ GPT-4 Turbo resolved 11.00%. Without demonstrations performance dropped to 7.33% (Lite).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Inefficient exploration (chains of cd/ls/cat), excessive/irrelevant search results from grep/find, brittle multi-step editing (redirection or sed), lack of immediate feedback for edits leading to confusion and cascading errors. These behaviors can exhaust budget or context window.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Demonstrations help; standard shell tools (grep/find) can be effective when results are limited, but the unstructured, verbose outputs and need for repeated arithmetic/formatting hinder agents.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Shell-only is substantially weaker than SWE-agent's ACI (e.g., 11% vs 18% on Lite with same LM), indicating that LM-centric interface design materially improves agent performance.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2615.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2615.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation (RAG) non-interactive baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A non-interactive pipeline where BM25 retrieves relevant repository files for a given issue and a model is prompted to generate a patch directly (no iterative execution-feedback loop).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>SWE-bench: Can language models resolve real-world github issues?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>RAG (retrieval-augmented generation) baseline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>BM25 retrieval of relevant files given the issue text, followed by a generation model that produces a patch in one shot (non-interactive). No multi-turn execution or environment interaction.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Non-interactive automated patch generation system / Retrieval-augmented generation</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Software engineering — repository-level patch generation for GitHub issues (SWE-bench).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Given an issue description and retrieved files, generate a patch that resolves the issue without interacting with an execution environment or running tests in a loop.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Lower operational complexity (single-shot generation) but faces the large search/repair space typical of repository-level bugs; relies on retrieval quality and the model's one-shot synthesis ability.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Uses retrieved repository files from SWE-bench; data pre-existing.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Cheap relative to interactive agents: RAG avg. cost (successful instances) reported as $0.13 (GPT-4 Turbo) and $0.25 (Claude 3 Opus) in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Well-defined test-based evaluation, deterministic execution when patch applied; no iterative feedback loop available to the generator.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>% Resolved (pass@1) after applying generated patch.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Poor relative performance: RAG w/ GPT-4 Turbo resolved 1.31% on full SWE-bench and 2.67% on Lite. RAG w/ Claude 3 Opus resolved 3.79% (full) and 4.33% (Lite).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Low one-shot patch quality and lack of iterative execution feedback lead to low resolution rates on repository-level issues. Retrieval can miss relevant cross-file context.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Low cost; retrieval quality and model's patch-generation capability are key. Works better on smaller/self-contained tasks but struggles on repository-level complexities.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>SWE-agent trades higher cost for substantially higher resolution rates (e.g., ~12–18% vs RAG's 1–4%).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2615.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2615.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced prior work on language agents that use verbal reinforcement learning to iteratively improve behavior (cited in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Reflexion (language-agent framework)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Prior research that equips language agents with a mechanism for verbal reinforcement learning (agents generate internal reflections and modify future behavior). The paper is cited as a relevant example of LM agents that use internal feedback to improve.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Language-agent framework / Agent learning method</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>General language-agent tasks (various interactive environments) — referenced in context of LM agents and iterative improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Designing agents that self-reflect and use verbalized feedback to improve multi-turn performance; not applied experimentally in this paper beyond citation.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2615.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2615.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>L2MAC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>L2MAC: Large language model automatic computer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced system for unbounded code generation where an LLM is used as an automatic 'computer' to generate code — cited in related work as another autonomous coding system.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>L2MAC: Large language model automatic computer for unbounded code generation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>L2MAC</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Referenced prior work that frames an LLM as an 'automatic computer' capable of unbounded code generation. Mentioned in the related work overview of LM-based agents for code.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Automated code generation / LM-as-executor</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Code generation and automated programming tasks (general).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Not detailed in this paper beyond citation; generally about enabling LLMs to perform extended code generation/execution workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2615.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2615.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MemGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MemGPT: Towards LLMs as operating systems</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced prior work that treats large language models as general-purpose computer agents with memory/system-like functionality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Memgpt: Towards llms as operating systems</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MemGPT</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited work proposing LLM-based systems that approximate operating-system-like behavior (memory, planning, self-improvement). Mentioned in the context of larger literature on LM agents controlling/composing tools.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LM-based autonomous agent / system orchestration</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>General agentic control of computer environments (broad tasks including code and system control).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Intercode: Standardizing and benchmarking interactive coding with execution feedback. <em>(Rating: 2)</em></li>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>Memgpt: Towards llms as operating systems <em>(Rating: 2)</em></li>
                <li>L2MAC: Large language model automatic computer for unbounded code generation. <em>(Rating: 2)</em></li>
                <li>Os-copilot: Towards generalist computer agents with self-improvement <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2615",
    "paper_id": "paper-1c3c531fc0fbe79f97f367ed3648de8467caeeaa",
    "extraction_schema_id": "extraction-schema-66",
    "extracted_data": [
        {
            "name_short": "SWE-agent",
            "name_full": "SWE-agent (Agent-Computer Interface for Software Engineering)",
            "brief_description": "An LM-based autonomous software engineering agent that combines a large language model with a tailored agent-computer interface (ACI) to search, navigate, edit, and execute code in repositories to resolve real-world GitHub issues.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "SWE-agent",
            "system_description": "SWE-agent pairs a fixed large language model (e.g., GPT-4 Turbo, Claude 3 Opus) with a specially designed agent-computer interface (ACI). The ACI exposes a small, LM-friendly action set (search_dir, search_file, find_file, open, goto, scroll_up/down, edit, create, submit, plus selective shell commands), a concise, standardized feedback format (file viewer windows, summarized search results), history/context management, and guardrails (linting on edits, search result limits). The agent operates in ReAct fashion: it iteratively generates a thought and a command, executes the command in a sandboxed environment (Docker), inspects the returned observation (including test output), and issues further actions until it submits a patch.",
            "system_type": "Automated Software Engineering Agent / AI Scientist (software engineering)",
            "problem_domain": "Software engineering — repository-level bug localization, automated program repair, and end-to-end issue resolution on real-world Python codebases (SWE-bench) and short-form debugging tasks (HumanEvalFix).",
            "problem_description": "Autonomously resolve issues collected from GitHub (SWE-bench): localize the root cause across files, optionally write reproduction scripts, produce multi-line edits to source files, run tests/reproduction scripts, and submit a patch. Tasks range from small self-contained functional bug fixes (SWE-bench Lite) to broader repository-level issues across 12 popular Python packages.",
            "problem_complexity": "High relative complexity for code-generation benchmarks: cross-file reasoning, large search space (multiple files, symbols), need for correct multi-line edits with precise indentation and API usage, multi-step edit-execute-debug loops. Quantitative scope: SWE-bench contains 2,294 test instances; SWE-bench Lite is 300 self-contained instances. Evaluation requires passing human-written unit tests (deterministic execution environment).",
            "data_availability": "Pre-existing real-world codebases and human-written unit tests from GitHub (SWE-bench). Data are available (repository snapshots + issue descriptions). Reproduction scripts are often created by the agent; tests are provided and used as ground-truth evaluators. No additional costly experimental data collection required beyond repository checkout and test execution.",
            "computational_requirements": "Uses large API-based LMs with very large context windows (GPT-4 Turbo: 128k tokens; Claude 3 Opus: 200k tokens). Per-instance budget capped at $4 for experiments; SWE-agent average API inference cost for successfully resolved instances: $1.59 (full SWE-bench, GPT-4 Turbo) and $1.67 (Lite). Agents interact iteratively (median successful run: 12 steps, median cost $1.21). Execution runs in Docker containers to safely run tests and scripts.",
            "problem_structure": "Well-defined evaluation: deterministic program execution with clear unit-test-based success metric (% Resolved / pass@1). Problem is discrete (sequence of agent actions), multi-step, and requires domain knowledge of codebase APIs; environment deterministic given same edits and tests. Open-ended in strategy (localization/reproduction/edit patterns vary) but success is measured by deterministic tests.",
            "success_metric": "% Resolved (pass@1): proportion of instances where all tests pass after the agent's patch; additionally pass@1 on HumanEvalFix tasks for language-specific debugging.",
            "success_rate": "On SWE-bench (full, 2,294 instances) SWE-agent w/ GPT-4 Turbo resolved 12.47% (286/2,294). On SWE-bench Lite (300 instances) SWE-agent w/ GPT-4 Turbo resolved 18.00% (54/300). HumanEvalFix pass@1 (SWE-agent w/ GPT-4 Turbo): Python 87.7%, JS 89.7%, Java 87.9%. Average API cost per successful instance (SWE-bench full, GPT-4 Turbo): $1.59.",
            "failure_modes": "Major failure modes: incorrect implementations or overly specific implementations (~52% of unresolved instances), cascading failed edits (~23.4% of failures). Editing is a frequent source of failure: 51.7% of trajectories had ≥1 failed edit (linting error). Recovery probability: any edit has a 90.5% eventual success chance; after one failed edit the chance of succeeding later drops to 57.2%. Other failure drivers: inefficient search/exhausting budget, insufficient generality of fixes, and repeated edits introducing syntax errors.",
            "success_factors": "ACI design choices: simple and compact actions that align with LM capabilities, concise informative feedback (file-windowed viewer, summarized search), guardrails such as linting that prevent cascading errors, context management that collapses older observations (keep last 5 full), and large LM context windows. Demonstrations also helped (Shell-only with demo better than without).",
            "comparative_results": "SWE-agent substantially outperforms baselines: on Lite, SWE-agent w/ GPT-4 Turbo: 18.00% resolved vs Shell-only agent 11.00% (both GPT-4 Turbo) and RAG 2.67% (GPT-4 Turbo). SWE-agent is costlier (8–13× higher cost than RAG) but yields ~6.7× improvement in % Resolved versus RAG. SWE-agent shows a 64% relative increase vs Shell-only when using same LM (GPT-4 Turbo). Claude 3 Opus variant: SWE-agent resolves 10.46% (full) and 13.00% (Lite).",
            "human_baseline": null,
            "uuid": "e2615.0",
            "source_info": {
                "paper_title": "SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Shell-only agent (InterCode)",
            "name_full": "Shell-only LM agent (InterCode environment)",
            "brief_description": "An LM agent that interacts directly with a Linux shell (bash) and standard command-line utilities for code navigation, inspection, and editing; used as an interactive baseline.",
            "citation_title": "Intercode: Standardizing and benchmarking interactive coding with execution feedback.",
            "mention_or_use": "use",
            "system_name": "Shell-only agent (InterCode-style)",
            "system_description": "LM agent interacts with a raw shell environment using standard bash commands and utilities (cd, ls, cat, grep, sed, python, etc.) to localize, edit, and run code. There is no LM-tailored ACI; editing and navigation are achieved by conventional shell workflows.",
            "system_type": "LM agent using standard shell interface / Interactive coding agent",
            "problem_domain": "Software engineering tasks (SWE-bench) — repository-level bug localization and repair.",
            "problem_description": "Resolve SWE-bench issues via multi-step shell interactions: search for files/strings, view file contents via shell tools, edit files via redirection/sed, run tests or reproduction scripts, and submit patches.",
            "problem_complexity": "Same repository-level complexity as SWE-bench; agents struggle with verbosity of shell outputs, inefficient multi-command workflows, and arithmetic/formatting for line-oriented edits.",
            "data_availability": "Uses the same GitHub repositories and tests as SWE-bench; data pre-existing and available.",
            "computational_requirements": "Uses same LM backends as SWE-agent in experiments; average cost on Lite: $1.46 (Shell-only w/ GPT-4 Turbo). Demonstrations increase performance but also cost.",
            "problem_structure": "Deterministic program execution with unit-test evaluation; discrete action space (shell commands) that is more granular and noisy compared to LM-tailored ACI.",
            "success_metric": "% Resolved (pass@1) based on tests after final patch.",
            "success_rate": "On SWE-bench Lite, Shell-only agent w/ GPT-4 Turbo resolved 11.00%. Without demonstrations performance dropped to 7.33% (Lite).",
            "failure_modes": "Inefficient exploration (chains of cd/ls/cat), excessive/irrelevant search results from grep/find, brittle multi-step editing (redirection or sed), lack of immediate feedback for edits leading to confusion and cascading errors. These behaviors can exhaust budget or context window.",
            "success_factors": "Demonstrations help; standard shell tools (grep/find) can be effective when results are limited, but the unstructured, verbose outputs and need for repeated arithmetic/formatting hinder agents.",
            "comparative_results": "Shell-only is substantially weaker than SWE-agent's ACI (e.g., 11% vs 18% on Lite with same LM), indicating that LM-centric interface design materially improves agent performance.",
            "human_baseline": null,
            "uuid": "e2615.1",
            "source_info": {
                "paper_title": "SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "RAG baseline",
            "name_full": "Retrieval-Augmented Generation (RAG) non-interactive baseline",
            "brief_description": "A non-interactive pipeline where BM25 retrieves relevant repository files for a given issue and a model is prompted to generate a patch directly (no iterative execution-feedback loop).",
            "citation_title": "SWE-bench: Can language models resolve real-world github issues?",
            "mention_or_use": "use",
            "system_name": "RAG (retrieval-augmented generation) baseline",
            "system_description": "BM25 retrieval of relevant files given the issue text, followed by a generation model that produces a patch in one shot (non-interactive). No multi-turn execution or environment interaction.",
            "system_type": "Non-interactive automated patch generation system / Retrieval-augmented generation",
            "problem_domain": "Software engineering — repository-level patch generation for GitHub issues (SWE-bench).",
            "problem_description": "Given an issue description and retrieved files, generate a patch that resolves the issue without interacting with an execution environment or running tests in a loop.",
            "problem_complexity": "Lower operational complexity (single-shot generation) but faces the large search/repair space typical of repository-level bugs; relies on retrieval quality and the model's one-shot synthesis ability.",
            "data_availability": "Uses retrieved repository files from SWE-bench; data pre-existing.",
            "computational_requirements": "Cheap relative to interactive agents: RAG avg. cost (successful instances) reported as $0.13 (GPT-4 Turbo) and $0.25 (Claude 3 Opus) in the experiments.",
            "problem_structure": "Well-defined test-based evaluation, deterministic execution when patch applied; no iterative feedback loop available to the generator.",
            "success_metric": "% Resolved (pass@1) after applying generated patch.",
            "success_rate": "Poor relative performance: RAG w/ GPT-4 Turbo resolved 1.31% on full SWE-bench and 2.67% on Lite. RAG w/ Claude 3 Opus resolved 3.79% (full) and 4.33% (Lite).",
            "failure_modes": "Low one-shot patch quality and lack of iterative execution feedback lead to low resolution rates on repository-level issues. Retrieval can miss relevant cross-file context.",
            "success_factors": "Low cost; retrieval quality and model's patch-generation capability are key. Works better on smaller/self-contained tasks but struggles on repository-level complexities.",
            "comparative_results": "SWE-agent trades higher cost for substantially higher resolution rates (e.g., ~12–18% vs RAG's 1–4%).",
            "human_baseline": null,
            "uuid": "e2615.2",
            "source_info": {
                "paper_title": "SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion: Language agents with verbal reinforcement learning",
            "brief_description": "A referenced prior work on language agents that use verbal reinforcement learning to iteratively improve behavior (cited in related work).",
            "citation_title": "Reflexion: Language agents with verbal reinforcement learning",
            "mention_or_use": "mention",
            "system_name": "Reflexion (language-agent framework)",
            "system_description": "Prior research that equips language agents with a mechanism for verbal reinforcement learning (agents generate internal reflections and modify future behavior). The paper is cited as a relevant example of LM agents that use internal feedback to improve.",
            "system_type": "Language-agent framework / Agent learning method",
            "problem_domain": "General language-agent tasks (various interactive environments) — referenced in context of LM agents and iterative improvement.",
            "problem_description": "Designing agents that self-reflect and use verbalized feedback to improve multi-turn performance; not applied experimentally in this paper beyond citation.",
            "problem_complexity": null,
            "data_availability": null,
            "computational_requirements": null,
            "problem_structure": null,
            "success_metric": null,
            "success_rate": null,
            "failure_modes": null,
            "success_factors": null,
            "comparative_results": null,
            "human_baseline": null,
            "uuid": "e2615.3",
            "source_info": {
                "paper_title": "SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "L2MAC",
            "name_full": "L2MAC: Large language model automatic computer",
            "brief_description": "A referenced system for unbounded code generation where an LLM is used as an automatic 'computer' to generate code — cited in related work as another autonomous coding system.",
            "citation_title": "L2MAC: Large language model automatic computer for unbounded code generation.",
            "mention_or_use": "mention",
            "system_name": "L2MAC",
            "system_description": "Referenced prior work that frames an LLM as an 'automatic computer' capable of unbounded code generation. Mentioned in the related work overview of LM-based agents for code.",
            "system_type": "Automated code generation / LM-as-executor",
            "problem_domain": "Code generation and automated programming tasks (general).",
            "problem_description": "Not detailed in this paper beyond citation; generally about enabling LLMs to perform extended code generation/execution workflows.",
            "problem_complexity": null,
            "data_availability": null,
            "computational_requirements": null,
            "problem_structure": null,
            "success_metric": null,
            "success_rate": null,
            "failure_modes": null,
            "success_factors": null,
            "comparative_results": null,
            "human_baseline": null,
            "uuid": "e2615.4",
            "source_info": {
                "paper_title": "SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "MemGPT",
            "name_full": "MemGPT: Towards LLMs as operating systems",
            "brief_description": "A referenced prior work that treats large language models as general-purpose computer agents with memory/system-like functionality.",
            "citation_title": "Memgpt: Towards llms as operating systems",
            "mention_or_use": "mention",
            "system_name": "MemGPT",
            "system_description": "Cited work proposing LLM-based systems that approximate operating-system-like behavior (memory, planning, self-improvement). Mentioned in the context of larger literature on LM agents controlling/composing tools.",
            "system_type": "LM-based autonomous agent / system orchestration",
            "problem_domain": "General agentic control of computer environments (broad tasks including code and system control).",
            "problem_description": null,
            "problem_complexity": null,
            "data_availability": null,
            "computational_requirements": null,
            "problem_structure": null,
            "success_metric": null,
            "success_rate": null,
            "failure_modes": null,
            "success_factors": null,
            "comparative_results": null,
            "human_baseline": null,
            "uuid": "e2615.5",
            "source_info": {
                "paper_title": "SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Intercode: Standardizing and benchmarking interactive coding with execution feedback.",
            "rating": 2
        },
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Memgpt: Towards llms as operating systems",
            "rating": 2
        },
        {
            "paper_title": "L2MAC: Large language model automatic computer for unbounded code generation.",
            "rating": 2
        },
        {
            "paper_title": "Os-copilot: Towards generalist computer agents with self-improvement",
            "rating": 1
        }
    ],
    "cost": 0.01914,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering</h1>
<p>John Yang<em> Carlos E. Jimenez</em> Alexander Wettig Kilian Lieret<br>Shunyu Yao Karthik Narasimhan Ofir Press<br>Princeton Language and Intelligence, Princeton University</p>
<h4>Abstract</h4>
<p>Language model (LM) agents are increasingly being used to automate complicated tasks in digital environments. Just as humans benefit from powerful software applications, such as integrated development environments, for complex tasks like software engineering, we posit that LM agents represent a new category of end users with their own needs and abilities, and would benefit from specially-built interfaces to the software they use. We investigate how interface design affects the performance of language model agents. As a result of this exploration, we introduce SWE-agent: a system that facilitates LM agents to autonomously use computers to solve software engineering tasks. SWE-agent's custom agent-computer interface (ACI) significantly enhances an agent's ability to create and edit code files, navigate entire repositories, and execute tests and other programs. We evaluate SWE-agent on SWE-bench and HumanEvalFix, achieving state-of-the-art performance on both with a pass@1 rate of $12.5 \%$ and $87.7 \%$, respectively, far exceeding the previous state-of-the-art achieved with non-interactive LMs. Finally, we provide insight on how the design of the ACI can impact agents' behavior and performance.</p>
<h2>1 Introduction</h2>
<p>Recent work has demonstrated the efficacy of LM agents for code generation with execution feedback [39]. However, applying agents to more complex code tasks like software engineering remains unexplored. To solve programming tasks, LM agents are typically designed to use existing applications, such as the Linux shell or Python interpreter [53, 57, 59]. However, to perform more complex programming tasks such as software engineering [20], human engineers benefit from sophisticated applications like VSCode with powerful tools and extensions. Inspired by human-computer interaction (HCI) studies on the efficacy of user interfaces for humans [7], we investigate whether LM agents could similarly benefit from better-designed interfaces for performing software engineering tasks.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: SWE-agent is an LM interacting with a computer through an agent-computer interface (ACI), which includes the commands the agent uses and the format of the feedback from the computer.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Consider the simple setting of an agent interacting directly with a Linux shell [59]. In practice, we find that LM agents can struggle to reliably take actions in this environment. For example, it fails to provide simple commands to edit a small file segment, and does not provide any feedback if the user makes an invalid edit. These deficits substantially hamper performance, motivating the need for an agent-computer interface (ACI), i.e., an abstraction layer between the LM agent and computer, to enhance the LM agent’s abilities in computer environments (Figure 1).</p>
<p>From this effort, we introduce SWE-agent, an agent composed of an LM and ACI, that can interact with a computer to solve challenging real-world software engineering problems, such as those proposed in SWE-bench [20]. In contrast to the Linux Shell’s granular, highly configurable action space, SWE-agent’s ACI instead offers a small set of simple actions for viewing, searching through and editing files. The ACI uses guardrails to prevent common mistakes, and an agent receives specific, concise feedback about a command’s effects at every turn. We show that ACIs tailored specifically for LMs outperform existing user interfaces (UIs) designed for human users, such as the Linux shell.</p>
<p>Using GPT-4 Turbo as a base LM, SWE-agent solves 12.47% of the 2,294 SWE-bench test tasks, substantially outperforming the previous best resolve rate of 3.8% by a non-interactive, retrieval-augmented system [20]. We perform an ablation study on a subset of 300 SWE-bench test instances (SWE-bench Lite) to analyze our ACI design choices. The results show that SWE-agent solves 10.7 percentage points more instances than the baseline agent, which uses only the default Linux shell. Although our ACI was developed for GPT-4 Turbo, we show that it is portable to a different LM; SWE-agent with Claude 3 Opus can solve 10.5% of the benchmark tasks.</p>
<p>Our contributions are twofold. First, we introduce the concept of the agent-computer interface (ACI) and demonstrate how careful ACI design can substantially improve LM agent performance without modifying the underlying LM’s weights. Second, we build, evaluate, and open-source SWE-agent, a system that provides LMs an ACI for solving real-world software engineering tasks. Unlike prior works that independently explore the merits of tool use, prompting techniques, and code execution in interactive settings, our approach unifies these factors within the ACI framework. We show that crafting LM-centric interactive components has meaningful effects on downstream task performance.</p>
<h2>2 The Agent-Computer Interface</h2>
<p>An LM acts as an agent when it interacts with an environment by iteratively taking actions and receiving feedback [42; 62]. Typically, the environment has hard constraints, as in robotics, where agents control actuators in the physical world. On the other hand, digital environments can be molded by abstractions in the form of application programming interfaces and user interfaces for software and humans respectively. Naturally, existing interfaces have been designed with one of these users in mind. We argue that LM agents represent a new category of end user, with their own needs and abilities. We refer to the interface LM agents use to interact with computers as the agent-computer interface (ACI). Figure 2 illustrates how ACIs provide LM agents with important functionality to interface with computers, similar to how code editors also help humans use computers more effectively.</p>
<p>Figure 2: Specialized applications like IDEs (e.g., VSCode, PyCharm) make scientists and software engineers more efficient and effective at computer tasks. Similarly, ACI design aims to create a suitable interface that makes LM agents more effective at digital work such as software engineering.</p>
<p>Disparities in humans’ and LMs’ abilities and limitations motivates different interface design guidelines. For instance, the current generation of LMs lack the visual understanding abilities to directly operate GUI-based applications with rich visual components and signals. However, many of the features provided by these applications, such as syntax checking and navigation tools, could be useful to LM agents if they were presented in a suitable manner. Additionally, humans can flexibly ignore unnecessary information, whereas all content has a fixed cost in memory and computation for LMs</p>
<p>and distracting context can harm performance [27]. Therefore, LM agents may be more effective at interacting with computers when provided an interface that was built informed by these differences.</p>
<p>Ultimately, a well-designed ACI should help the LM agent understand the state of the application given previous changes, manage history to avoid unnecessary context from prior observations, and provide actions that models can use efficiently and reliably. The ACI specifies both the commands available to the LM and how the environment state is communicated back to the LM. It also tracks the history of all previous commands and observations and, at each step, manages how these should be formatted and combined with high-level instructions into a single input for the LM.</p>
<p>In this paper, we assume a fixed LM and focus on designing the ACI to improve its performance. This means that we shape the actions, their documentation, and environment feedback to complement an LM's limitations and abilities. We draw inspiration from the field of HCI, where user studies elicit insights about how compatible different interfaces are with respect to human intuition and performance [7]. We use two approaches to enhance performance on a development set: (1) manually inspect agent behavior to identify difficulties and propose improvements, and (2) run a grid search to select the best ACI configuration.</p>
<p>Taking these two actions resulted in several insights about design principles that seem especially important for building effective ACIs:</p>
<ol>
<li>Actions should be simple and easy to understand for agents. Many bash commands have documentation that includes dozens of options. Simple commands with a few options and concise documentation are easier for agents to use, reducing the need for demonstrations or fine-tuning. This is a defining principle for all SWE-agent commands that we describe in Section 3.</li>
<li>Actions should be compact and efficient. Important operations (e.g., file navigation, editing) should be consolidated into as few actions as possible. Efficient actions help agents make meaningful progress towards a goal in a single step. A poor design would therefore have many simple actions that must be composed across multiple turns for a higher order operation to take effect. We show this idea in action in the Editing and Search interface analyses in Section 5.1.</li>
<li>Environment feedback should be informative but concise. High quality feedback should provide the agent with substantive information about the current environment state (and the effect of the agent's recent actions) without unnecessary details. For instance, when editing a file, updating the agent about revised content is helpful. Figures 3a, 3b and Table 3 show this.</li>
<li>Guardrails mitigate error propagation and hasten recovery. Like humans, LMs make mistakes when editing or searching and can struggle to recover from these errors. Building in guardrails, such as a code syntax checker that automatically detects mistakes, can help agents recognize and quickly correct errors. We show the effect of editing guardrails in Table 3.</li>
</ol>
<p>Analysis and ablation studies in Section 5 demonstrate how alternative ACIs affect LM performance. Our studies shows how these principles appear recurrently across actions, feedback, and workflows.</p>
<h1>3 SWE-agent: Designing an ACI for Software Engineering</h1>
<p>Here we describe how SWE-agent provides an ACI for LMs to act as software engineering agents, enabling them to effectively search, navigate, edit, and execute code commands. The ACI comprises several principal components, including search/navigation, file viewer, file editor, and context management. At each step, SWE-agent generates a thought and a command, then incorporates the feedback from the command's execution in the environment (ReAct; Yao et al. [62]). Built atop the Linux shell, SWE-agent also allows access to common Linux commands and utilities when needed.</p>
<p>Search and navigation. Navigating codebases requires finding the relevant file and content. A common strategy to do this involves looking up terms that might be useful, e.g., files, functions, or class definitions mentioned in an issue. We introduce the special commands find_file, search_file, and search_dir, which output a summary of search results when searching for filenames and strings within files or directories. Figure 10 shows examples of these search result formats. The find_file command searches for filenames in the repository, while the search_file and search_dir locates strings in a file(s) of a subdirectory. Our interface encourages efficient searches by suppressing verbose results. The search commands return at most 50</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: The file viewer and edit command are integrated. (a) The file viewer shows the agent the open file's content with line numbers. (b) The agent invokes the edit function to replace lines 404-407 in the open file. After the edit, the file viewer shows the agent the now updated version of the file.
results for each search query; if a search exceeds this number, we do not report the results and instead suggest that the agent write a more specific query.</p>
<p>File viewer. After finding a file they want to view, agents use the interactive file viewer by calling the command open on the relevant file path. The file viewer presents a window of at most 100 lines of the file at a time. The agent can move this window with the commands scroll_down and scroll_up or access a specific line with the goto command. To facilitate in-file navigation and code localization, we display: the full path of the open file, the total number of lines in the file, the number of lines omitted before and after the current window, and the line number (prepended to each visible line). Figure 3a shows an example of this interface.</p>
<p>File editor. We provide a few commands that let LMs create and edit files. The edit command works in conjunction with the file viewer, allowing agents to replace a specific range of lines in the open file. This command takes 3 required arguments: the start line, end line, and replacement text. In a single step, agents can replace all lines between the start and end lines with the replacement text, as shown in Figure 3b. After edits are applied, the file viewer automatically displays the updated content, helping the agent observe the effects of its edit immediately without invoking additional commands. Figure 3b shows an example agent response, including a file edit.</p>
<p>Similar to how humans can use tools like syntax highlighting to help them notice format errors when editing files in an IDE, we integrate a code linter into the edit function to alert the agent of mistakes it may have introduced when editing a file. Select errors from the linter are shown to the agent along with a snippet of the file contents before/after the error was introduced. Invalid edits are discarded, and the agent is asked to try editing the file again.</p>
<p>Context management. The SWE-agent system uses informative prompts, error messages, and history processors to keep agent context concise and informative. Agents receive instructions, documentation, and demonstrations on the correct use of bash and ACI commands. At each step, the system instructs them to generate both a thought and an action [62]. Malformed generations trigger an error response, shown in Figure 32, asking the agent to try again, which is repeated until a valid generation is received. Once received, all past error messages except the first are omitted.</p>
<p>The agent's environment responses display computer output using the template shown in Figure 30; however, if no output is generated, a specific message ("Your command ran successfully and did not produce any output") is included to enhance clarity. To further improve context relevance, observations preceding the last 5 are each collapsed into a single line, shown in Figure 31. By removing most content from prior observations, we maintain essential information about the plan and action history while reducing unnecessary context, which allows for more interaction cycles and avoids showing outdated file information. §A provides further implementation details.</p>
<h1>4 Experimental Setup</h1>
<p>Datasets. We primarily evaluate on the SWE-bench dataset, which includes 2,294 task instances from 12 different repositories of popular Python packages [20]. We report our main agent results on the full SWE-bench test set and ablations and analysis on the SWE-bench Lite test set, unless</p>
<p>otherwise specified. SWE-bench Lite is a canonical subset of 300 instances from SWE-bench that focus on evaluating self-contained functional bug fixes. We also test SWE-agent's basic code editing abilities with HumanEvalFix, a short-form code debugging benchmark [32].</p>
<p>Models. All results, ablations, and analyses are based on two leading LMs, GPT-4 Turbo (gpt-4-1106-preview) [34] and Claude 3 Opus (claude-3-opus-20240229) [6]. We experimented with a number of additional closed and open source models, including Llama 3 and DeepSeek Coder [14], but found their performance in the agent setting to be subpar. Many LMs' context window is too small, such as Llama 3's context window of 8k. GPT-4 Turbo and Claude 3 Opus have 128k and 200k token context windows, respectively, which provides sufficient room for the LM to interact for several turns after being fed the system prompt, issue description, and optionally, a demonstration.</p>
<p>Baselines. We compare SWE-agent to two baselines. The first setting is the non-interactive, retrievalaugmented generation (RAG) baselines established in Jimenez et al. [20]. Here, a BM25 retrieval system retrieves the most relevant codebase files using the issue as the query; given these files, the model is asked to directly generate a patch file that resolves the issue.</p>
<p>The second setting, called Shell-only, is adapted from the interactive coding framework introduced in Yang et al. [59]. Following the InterCode environment, this baseline system asks the LM to resolve the issue by interacting with a shell process on Linux. Like SWE-agent, model prediction is generated automatically based on the final state of the codebase after interaction.</p>
<p>Metrics. We report \% Resolved or pass@1 as the main metric, which is the proportion of instances for which all tests pass successfully after the model generated patch is applied to the repository [20]. We also report the $\$$ Avg. Cost metric, the API inference cost incurred by SWE-agent averaged over all successfully resolved instances. Due to budget constraints, we set the per-instance budget to $\$ 4$; if a run exceeded this budget, existing edits were submitted automatically.</p>
<p>Configuration search. During the design process of SWE-agent, we arrived at the final ACI design through qualitative analysis of system behavior on a small set of hand-picked examples from the development split of SWE-bench. For the remaining hyperparameter choices, we performed a sweep over the window size, history processing, and decoding temperature, shown in §B.1.</p>
<h1>5 Results</h1>
<p>Across all systems, SWE-agent w/ GPT-4 Turbo achieves the best performance all-around, successfully solving $12.47 \%(286 / 2,294)$ of the full SWE-bench test set and $18.00 \%(54 / 300)$ of the Lite split. As shown in Table 1, compared to RAG on Lite, SWE-agent is 8-13x more costly but yields a 6.7 -fold improved \% Resolved rate. An LM-friendly ACI's value is confirmed by SWE-agent's $64 \%$ relative increase compared to Shell-only, both with GPT-4 Turbo.</p>
<p>In Table 2, SWE-agent yields strong performance on HumanEvalFix with $88.3 \%$ pass@1 rate. Figure 4 reveals that average performance variance is relatively low, but per-instance resolution can change considerably. More results are given in the appendix: $\S$ B. 2 shows that the success rate is uncorrelated to the issue age (controlling for possible test pollution), B. 5 presents more details on performance variance and pass@ $k$, and B. 7 discusses extra evaluation details.</p>
<h3>5.1 Analysis of ACI Design</h3>
<p>We perform several ablations of the SWE-agent interface, specifically with respect to the SWE-agent w/ GPT-4 configuration, summarized in Table 3. Our case studies shed light on interesting agent behavior along with the impact of different ACI designs.</p>
<p>Human user interfaces are not always suitable as agent-computer interfaces. Current LMs are vulnerable to a number of pitfalls when searching for relevant content in a Linux shell environment. Some exploration patterns (e.g., chains of cd, ls, cat) are extremely inefficient. grep or find look ups can perform better but occasionally produce many lines of irrelevant results. We hypothesize that better localization is possible with faster navigation and a more informative search interface.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 1: Main results for SWE-agent performance on the full and Lite splits of the SWE-bench test set. We benchmark models in the SWE-agent, Basic CLI, and Retrieval Augmented Generation (RAG) settings established in SWE-bench [20].</p>
<table>
<thead>
<tr>
<th></th>
<th>SWE-bench</th>
<th></th>
<th>SWE-bench Lite</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Model</td>
<td>% Resolved</td>
<td>$ Avg. Cost</td>
<td>% Resolved</td>
<td>$ Avg. Cost</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>RAG</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>w/ GPT-4 Turbo</td>
<td>1.31</td>
<td>0.13</td>
<td>2.67</td>
<td>0.13</td>
</tr>
<tr>
<td>w/ Claude 3 Opus</td>
<td>3.79</td>
<td>0.25</td>
<td>4.33</td>
<td>0.25</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Shell-only agent</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>w/ GPT-4 Turbo</td>
<td>-</td>
<td>-</td>
<td>11.00</td>
<td>1.46</td>
</tr>
<tr>
<td>w/o Demonstration</td>
<td>-</td>
<td>-</td>
<td>7.33</td>
<td>0.79</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>SWE-agent</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>w/ GPT-4 Turbo</td>
<td>12.47</td>
<td>1.59</td>
<td>18.00</td>
<td>1.67</td>
</tr>
<tr>
<td>w/ Claude 3 Opus</td>
<td>10.46</td>
<td>2.59</td>
<td>13.00</td>
<td>2.18</td>
</tr>
</tbody>
</table>
<p>Table 2: Pass@1 results on HumanEvalFix [32]. Except for SWE-agent, we use scores as reported in Yu et al. [65].</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Python</th>
<th>JS</th>
<th>Java</th>
</tr>
</thead>
<tbody>
<tr>
<td>CodeLLaMa-instruct-13B</td>
<td>29.2</td>
<td>19.5</td>
<td>32.3</td>
</tr>
<tr>
<td>GPT-4</td>
<td>47.0</td>
<td>48.2</td>
<td>50.0</td>
</tr>
<tr>
<td>DeepseekCoder-CodeAlpaca-6.7B</td>
<td>49.4</td>
<td>51.8</td>
<td>45.1</td>
</tr>
<tr>
<td>WaveCoder-DS-6.7B</td>
<td>57.9</td>
<td>52.4</td>
<td>57.3</td>
</tr>
<tr>
<td>SWE-agent w/ GPT-4 Turbo</td>
<td>87.7</td>
<td>89.7</td>
<td>87.9</td>
</tr>
</tbody>
</table>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: SWE-agent w/ GPT-4 Turbo Pass@k performance across 6 runs on SWE-bench Lite.</p>
<p>Table 3: SWE-bench Lite performance under ablations to the SWE-agent interface, which is denoted by . We consider different approaches to searching and editing (see Figures 5 and 6, respectively). We also verify how varying the file viewer window size affects performance, and we ablate the effect of different context management approaches.</p>
<table>
<thead>
<tr>
<th>Editor</th>
<th></th>
<th>Search</th>
<th></th>
<th>File Viewer</th>
<th></th>
<th>Context</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>edit action</td>
<td>15.0 ↓3.0</td>
<td>Summarized</td>
<td>18.0</td>
<td>30 lines</td>
<td>14.3 ↓3.7</td>
<td>Last 5 Obs.</td>
<td>18.0</td>
</tr>
<tr>
<td>w/ linting</td>
<td>18.0</td>
<td>Iterative</td>
<td>12.0 ↓6.0</td>
<td>100 lines</td>
<td>18.0</td>
<td>Full history</td>
<td>15.0 ↓3.0</td>
</tr>
<tr>
<td>No edit</td>
<td>10.3 ↓7.7</td>
<td>No search</td>
<td>15.7 ↓2.3</td>
<td>Full file</td>
<td>12.7 ↓5.3</td>
<td>w/o demo.</td>
<td>16.3 ↓1.7</td>
</tr>
</tbody>
</table>
<p>Figure 5 compares the Shell-only setting to two different search interfaces. <em>Iterative</em> search, directly inspired by traditional user interfaces for search, e.g., Vim or VSCode, shows results one by one via the file viewer. Agents can look through results using next and prev actions. Each result displays the matching line along with n surrounding lines of context. An advantage is that an agent can begin editing directly after seeing the relevant code in its search. However, when given a large number of search results, agents tend to look through every match exhaustively, calling next until each result has been inspected. This inefficient behavior can exhaust an agent's cost budget or context window, leading to even worse performance than the not having additional search tools at all (15.7% ↓2.3 for No search vs. 12.0% ↓6.0 with Iterative search).</p>
<p><strong>Compact, efficient file editing is critical to performance.</strong> SWE-agent's file editor and viewer are designed to consolidate the editing process into a single command that enables easy multi-line edits with consistent feedback and automatically updates the agent's view of the file after editing. In the No edit setting, editing options are restrictive and prone to errors; the primary methods available are either replacing entire files through redirection and overwriting or using utilities like sed for single-line or search-and-replace edits. Both methods have significant drawbacks. Redirection involves copying and rewriting entire files for even minor changes, which is both inefficient and error-prone. Although sed can facilitate specific edits, executing multi-line edits is cumbersome and can lead to unintended consequences that are challenging to detect. Moreover, both strategies</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Three different Search interfaces for task instance pvlib_pvlib-python-1224. In Shell-only, an agent performs localization using only standard bash commands and utilities. Compared to Iterative search, Summarized search shows an exhaustive list of search results and provides guidance on refining under-specified queries.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Three different Edit interfaces for task instance sympy_sympy-24102. Editing with bash commands requires several actions to successfully modify a file. The Editing component defines an edit command that leverages the File Viewer component to replace the bash style of editing workflow with a single command. Linting is beneficial for stymieing cascading errors that often start with an error-introducing edit by the agent.
lack immediate feedback about file updates, making these silent operations potentially confusing for models to interpret and increasing the risk of errors. Without SWE-agent's file editor interface, performance drops to $\left(10.3 \% \downarrow 11\right)$. We also find that agents are sensitive to the number of lines the file viewer displays. Either too little content ( 30 lines, $14.3 \% \downarrow 17$ ) or too much (entire file, $12.7 \%$ $\downarrow 53$ ) lowers performance.
Guardrails can improve error recovery. A prominent failure mode occurs when models repeatedly edit the same code snippet. The usual suspect for this behavior is an agent introducing a syntax error (e.g., incorrect indentation, extra parenthesis) via an errant edit. As discussed in Section 3, we add an intervention to the edit logic that lets a modification apply only if it does not produce major errors. We compare this interface with the No edit and edit w/o linting alternatives in Figure 6. This intervention improves performance considerably (without linting, $15.0 \% \downarrow 10$ ).</p>
<h1>5.2 Analysis of Agent Behavior</h1>
<p>Recurring problem-solving patterns emerge when LMs are equipped with a useful, intuitive ACI. We describe several model behaviors and problem-solving patterns that can be discerned from model performance and each model's corresponding trajectories.
Reproduction and/or localization is the first step. SWE-agent usually begins with either writing reproduction code and/or localizing the issue's cause to specific lines of code. As shown in Figure 7, all trajectories begin with either create (reproduction) or find_file/search_dir (localization). To reproduce, models will create a new file, add reproduction code to it with an edit, then run with python; this is the most popular triple of actions in Table 8. Using this feedback along with file</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: The frequency with which actions are invoked at each turn by SWE-agent w/ GPT-4 for task instances that it solved on the SWE-bench full test set (286 trajectories).</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8: Failure mode distribution for SWEagent w/ GPT-4 Turbo trajectories of unresolved instances. Each instance is labeled automatically using an LM with the categories from Table 9.</p>
<p>names and symbols in the issue description, an agent will start with a broad, directory-level keyword search, before then zooming into specific files and lines. This is reflected in Figure 22, where the most likely actions following localization sequences like (python, find_file) and (search_dir, open) are search_file and goto, indicative of how an agent "zooms in" on a bug. Extensive analysis on correlations between different groups of actions are discussed in §B.3.3</p>
<p>Remaining turns are mostly “edit, then execute” loops. As exhibited in Figure 7, from turn 5 onwards, the most frequent two actions for all turns are edit and python. Captured as high probability next actions following (edit, python) in Figure 22, additional localization operations are often interspersed across these later turns, where agents might look at more in-file code with search_file, scroll_up/down, or other files altogether with search_dir, find_file. This behavior usually arises in response to new information from re-running the reproduction script. Submissions are distributed normally from turn 10 onwards, although resolved task instances correlate more with earlier submits (see §B.3.1). A walk-through of common trajectory phases is in §B.3.2.</p>
<p>Editing remains challenging for agents. A non-trivial minority of edit actions raise a linting error; out of 2,294 task instances, 1,185 (51.7%) of SWE-agent w/ GPT-4 Turbo trajectories have 1+ failed edits. While agents generally recover more often than not from failed edits, the odds of recovery decrease as the agent accumulates more failed edits. Recovery refers to a sequence of consecutive failed edits followed immediately by a successful edit. Any attempt at editing has a 90.5% chance of eventually being successful. This probability drops off to 57.2% after a single failed edit. More editing phenomena are discussed in §B.3.3, and data about agents’ generated fixes are in §B.6.</p>
<p>Agents succeed quickly and fail slowly. We find that runs submitted relatively early are much more likely to be successful compared to those submitted after a larger number of steps or cost. We show in Table 15 the distribution of resolved and unresolved instances, including only instances that did not exhaust their budget. We observe that successful runs complete earlier and at a cheaper cost than unsuccessful ones. In general, successful instances solved by SWE-agent w/ GPT 4 finish with a median cost of $1.21 and 12 steps compared to a mean of $2.52 and 21 steps for unsuccessful ones. Furthermore, we find that 93.0% of resolved instances are submitted before exhausting their cost budget, compared to 69.0% of instances overall. For these reasons, we suspect that increasing the maximum budget or token limit are unlikely to substantially increase performance. More statistics about how trajectories typically conclude are in §B.9.</p>
<p>Most failures are incorrect implementations. We use GPT-4o to automatically categorize unresolved trajectories (SWE-agent w/ GPT-4 Turbo on SWE-bench Lite, $n=248$) into one of 9 manually defined categories described in Table 9. On a hand-labeled validation set, the LM’s judgment agrees with the authors’ on 87% of instances. From Figure 8, about half (52.0%) of unresolved instances fall into the Incorrect Implementation or Overly Specific Implementation categories, suggesting that agents’ proposed solutions often simply fail to functionally address the issue or are insufficiently general solutions. Cascading failed edits make up another 23.4% of failures. More details in §B.4.</p>
<h1>6 Related Work</h1>
<h3>6.1 Software Engineering Benchmarks</h3>
<p>Code generation benchmarks, which evaluate models on the task of synthesizing code from natural language descriptions, have served as a long-standing bellwether for measuring LM performance [5, $1,15,30]$. Subsequent works have built upon the code generation task formulation to contribute new benchmarks that translate problems to different (programming) languages [3, 49], incorporate third-party libraries [25, 29], introduce derivative code completion tasks [18, 32], increase test coverage [26], change the edit scope [8, 9, 64], and add robustness to dataset contamination [19]. Code generation problems are largely self-contained, with short problem descriptions ( $\sim 100$ lines) and corresponding solutions that are similarly brief, requiring nothing more complex than basic language primitives. Tests are either handwritten or generated synthetically via fuzz testing. In recent months, the rapid development of LMs has begun to saturate many of these benchmarks. For instance, the top method solves $94.4 \%$ of HumanEval [70].</p>
<p>Gauging future trends with the code generation task paradigm can be limited by the simplicity of this setting and cost of human-in-the-loop problem creation. In response, recent efforts have demonstrated that software engineering (SE) can serve as a diverse, challenging testbed for LM evaluation [68, 20, 28]. Repository-level code editing introduces many reasoning challenges grounded in real SE subtasks, such as spotting errant code and identifying cross-file relationships and understanding codebasespecific symbols and conventions. As a field, SE has generally studied tasks in a more isolated manner; prior benchmarks tended to frame problems in isolation from the rest of a codebase [21, 23].</p>
<p>We use SWE-bench because it unites many separate SE tasks, such as automated program repair [10, 40, 55], bug localization [4, 58], and testing [22, 46, 56] under a single task formulation that faithfully mirrors practical SE. Furthermore, SWE-bench task instances are diverse, having been automatically collected from real GitHub issues across 12 different repositories. In addition, SWEbench performance is based on rigorous, execution-based evaluation with human-written unit tests.</p>
<h3>6.2 Language Models as Agents</h3>
<p>The co-emergence of stronger LMs, increasingly challenging benchmarks, and practical use cases have together motivated a paradigm shift in LMs' inference setting. Instead of traditional zero/fewshot generation, LM agents [17, 42, 47, 54] that interact with a real/virtual world have proliferated as the default setting for web navigation [24, 33, 36, 41, 45, 61, 62, 71], computer control [35, 53, 57], and code generation tasks $[16,50,63]$.</p>
<p>Interaction and code generation are increasingly used together, with code as the modality of choice for actions [48, 59], tool construction [13, 51, 69], and reasoning [39, 66, 67]. Coding agents have also been applied to offensive security [11, 37, 60], theorem proving [44], and clinical tasks [38, 43, 52]. To the best of our knowledge, SWE-agent is the first work to explore language agents for end-to-end software engineering (SE).</p>
<h2>7 Discussion</h2>
<p>We introduce SWE-agent, an agent composed of an LM and ACI capable of autonomously solving software engineering tasks. Through our design methodology, results, and analysis, we demonstrate the value of ACIs tailored to leverage LMs' strengths and mitigate their weaknesses. Beyond empirical applications, we hope the further study of ACIs can also make principled use of and contribute to our understanding of language models and agents, analogous to the synergy between human-computer interaction (HCI) and psychology [2]. Humans and LMs have different characteristics, training objectives, specialities, and limitations [12, 31], and the interaction design processes can be seen as systematic behavioral experimentation that could reveal more insights into these differences towards establishing a comparative understanding of human and artificial intelligence.</p>
<h1>Acknowledgements</h1>
<p>We thank Austin W. Hanjie, Sam Ainsworth, Xindi Wu, Yuhan Liu, Mengzhou Xia, Dan Friedman, Tianyu Gao, Adithya Bhaskar, Aatmik Gupta, Louisa Nyhus, Alisa Liu, Ori Yoran and Richard Zhu for their valuable feedback and advice. We would also like to thank the broader Princeton Language and Intelligence community for supporting our work. We acknowledge support from an Oracle Collaborative Research award and the National Science Foundation under Grant No. 2239363. Any opinions, findings, conclusions, or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation</p>
<h2>References</h2>
<p>[1] J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry, Q. Le, and C. Sutton. Program synthesis with large language models, 2021.
[2] J. M. Carroll. Human-computer interaction: psychology as a science of design. Annual review of psychology, 48(1):61-83, 1997.
[3] F. Cassano, J. Gouwar, D. Nguyen, S. Nguyen, L. Phipps-Costin, D. Pinckney, M.-H. Yee, Y. Zi, C. J. Anderson, M. Q. Feldman, A. Guha, M. Greenberg, and A. Jangda. Multipl-e: A scalable and extensible approach to benchmarking neural code generation, 2022.
[4] S. Chakraborty, Y. Li, M. Irvine, R. Saha, and B. Ray. Entropy guided spectrum based bug localization using statistical language model. arXiv preprint arXiv:1802.06947, 2018.
[5] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, and J. K. et. al. Evaluating large language models trained on code, 2021.
[6] W.-L. Chiang, L. Zheng, Y. Sheng, A. N. Angelopoulos, T. Li, D. Li, H. Zhang, B. Zhu, M. Jordan, J. E. Gonzalez, and I. Stoica. Chatbot arena: An open platform for evaluating llms by human preference, 2024.
[7] A. Cooper, R. Reimann, and D. Cronin. About face 3: the essentials of interaction design. John Wiley \&amp; Sons, Inc., USA, 2007. ISBN 9780470084113.
[8] Y. Ding, Z. Wang, W. U. Ahmad, H. Ding, M. Tan, N. Jain, M. K. Ramanathan, R. Nallapati, P. Bhatia, D. Roth, and B. Xiang. Crosscodeeval: A diverse and multilingual benchmark for cross-file code completion. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. URL https://openreview.net/forum? id=wgDcbBMSfh.
[9] X. Du, M. Liu, K. Wang, H. Wang, J. Liu, Y. Chen, J. Feng, C. Sha, X. Peng, and Y. Lou. Classeval: A manually-crafted benchmark for evaluating llms on class-level code generation, 2023.
[10] Z. Fan, X. Gao, M. Mirchev, A. Roychoudhury, and S. H. Tan. Automated repair of programs from large language models, 2023.
[11] R. Fang, R. Bindu, A. Gupta, Q. Zhan, and D. Kang. Llm agents can autonomously hack websites, 2024.
[12] T. L. Griffiths. Understanding human intelligence through human limitations. Trends in Cognitive Sciences, 24(11):873-883, 2020.
[13] Y. Gu, Y. Shu, H. Yu, X. Liu, Y. Dong, J. Tang, J. Srinivasa, H. Latapie, and Y. Su. Middleware for llms: Tools are instrumental for language agents in complex environments, 2024.
[14] D. Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen, X. Bi, Y. Wu, Y. K. Li, F. Luo, Y. Xiong, and W. Liang. Deepseek-coder: When the large language model meets programming - the rise of code intelligence. CoRR, abs/2401.14196, 2024. URL https: //arxiv.org/abs/2401.14196.</p>
<p>[15] D. Hendrycks, S. Basart, S. Kadavath, M. Mazeika, A. Arora, E. Guo, C. Burns, S. Puranik, H. He, D. Song, and J. Steinhardt. Measuring coding challenge competence with apps, 2021.
[16] S. Holt, M. R. Luyten, and M. van der Schaar. L2MAC: Large language model automatic computer for unbounded code generation. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=EhrzQwsV4K.
[17] S. Hong, M. Zhuge, J. Chen, X. Zheng, Y. Cheng, C. Zhang, J. Wang, Z. Wang, S. K. S. Yau, Z. Lin, L. Zhou, C. Ran, L. Xiao, C. Wu, and J. Schmidhuber. Metagpt: Meta programming for a multi-agent collaborative framework, 2023.
[18] Q. Huang, J. Vora, P. Liang, and J. Leskovec. Mlagentbench: Evaluating language agents on machine learning experimentation, 2024.
[19] N. Jain, K. Han, A. Gu, W.-D. Li, F. Yan, T. Zhang, S. Wang, A. Solar-Lezama, K. Sen, and I. Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code, 2024.
[20] C. E. Jimenez, J. Yang, A. Wettig, S. Yao, K. Pei, O. Press, and K. R. Narasimhan. SWE-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum? id=VTF8yNQM66.
[21] R. Just, D. Jalali, and M. D. Ernst. Defects4J: A Database of existing faults to enable controlled testing studies for Java programs. In ISSTA 2014, Proceedings of the 2014 International Symposium on Software Testing and Analysis, pages 437-440, San Jose, CA, USA, July 2014. Tool demo.
[22] S. Kang, J. Yoon, and S. Yoo. Large language models are few-shot testers: Exploring llm-based general bug reproduction, 2023.
[23] R.-M. Karampatsis and C. Sutton. How often do single-statement bugs occur? the manysstubs4j dataset. 2020 IEEE/ACM 17th International Conference on Mining Software Repositories (MSR), pages 573-577, 2019. URL https://api.semanticscholar.org/CorpusID: 173188438 .
[24] J. Y. Koh, R. Lo, L. Jang, V. Duvvur, M. C. Lim, P.-Y. Huang, G. Neubig, S. Zhou, R. Salakhutdinov, and D. Fried. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks, 2024.
[25] Y. Lai, C. Li, Y. Wang, T. Zhang, R. Zhong, L. Zettlemoyer, S. W. tau Yih, D. Fried, S. Wang, and T. Yu. Ds-1000: A natural and reliable benchmark for data science code generation, 2022.
[26] J. Liu, C. S. Xia, Y. Wang, and L. Zhang. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. arXiv preprint arXiv:2305.01210, 2023.
[27] N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni, and P. Liang. Lost in the middle: How language models use long contexts, 2023.
[28] T. Liu, C. Xu, and J. McAuley. Repobench: Benchmarking repository-level code autocompletion systems. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=pPjZIOuQuF.
[29] Y. Liu, X. Tang, Z. Cai, J. Lu, Y. Zhang, Y. Shao, Z. Deng, H. Hu, K. An, R. Huang, S. Si, S. Chen, H. Zhao, L. Chen, Y. Wang, T. Liu, Z. Jiang, B. Chang, Y. Qin, W. Zhou, Y. Zhao, A. Cohan, and M. Gerstein. Ml-bench: Evaluating large language models for code generation in repository-level machine learning tasks, 2024.
[30] S. Lu, D. Guo, S. Ren, J. Huang, A. Svyatkovskiy, A. Blanco, C. Clement, D. Drain, D. Jiang, D. Tang, G. Li, L. Zhou, L. Shou, L. Zhou, M. Tufano, M. Gong, M. Zhou, N. Duan, N. Sundaresan, S. K. Deng, S. Fu, and S. Liu. Codexglue: A machine learning benchmark dataset for code understanding and generation, 2021.</p>
<p>[31] R. T. McCoy, S. Yao, D. Friedman, M. Hardy, and T. L. Griffiths. Embers of autoregression: Understanding large language models through the problem they are trained to solve. arXiv preprint arXiv:2309.13638, 2023.
[32] N. Muennighoff, Q. Liu, A. R. Zebaze, Q. Zheng, B. Hui, T. Y. Zhuo, S. Singh, X. Tang, L. V. Werra, and S. Longpre. Octopack: Instruction tuning code large language models. In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id=mw1PWNSWZP.
[33] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim, C. Hesse, S. Jain, V. Kosaraju, W. Saunders, X. Jiang, K. Cobbe, T. Eloundou, G. Krueger, K. Button, M. Knight, B. Chess, and J. Schulman. Webgpt: Browser-assisted question-answering with human feedback, 2022.
[34] OpenAI, J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, R. Avila, I. Babuschkin, S. Balaji, V. Balcom, P. Baltescu, H. Bao, M. Bavarian, J. Belgum, I. Bello, J. Berdine, G. Bernadett-Shapiro, C. Berner, L. Bogdonoff, O. Boiko, M. Boyd, A.-L. Brakman, G. Brockman, T. Brooks, M. Brundage, K. Button, T. Cai, R. Campbell, A. Cann, B. Carey, C. Carlson, R. Carmichael, B. Chan, C. Chang, F. Chantzis, D. Chen, S. Chen, R. Chen, J. Chen, M. Chen, B. Chess, C. Cho, C. Chu, H. W. Chung, D. Cummings, J. Currier, Y. Dai, C. Decareaux, T. Degry, N. Deutsch, D. Deville, A. Dhar, D. Dohan, S. Dowling, S. Dunning, A. Ecoffet, A. Eleti, T. Eloundou, D. Farhi, L. Fedus, N. Felix, S. P. Fishman, J. Forte, I. Fulford, L. Gao, E. Georges, C. Gibson, V. Goel, T. Gogineni, G. Goh, R. Gontijo-Lopes, J. Gordon, M. Grafstein, S. Gray, R. Greene, J. Gross, S. S. Gu, Y. Guo, C. Hallacy, J. Han, J. Harris, Y. He, M. Heaton, J. Heidecke, C. Hesse, A. Hickey, W. Hickey, P. Hoeschele, B. Houghton, K. Hsu, S. Hu, X. Hu, J. Huizinga, S. Jain, S. Jain, J. Jang, A. Jiang, R. Jiang, H. Jin, D. Jin, S. Jomoto, B. Jonn, H. Jun, T. Kaftan, Łukasz Kaiser, A. Kamali, I. Kanitscheider, N. S. Keskar, T. Khan, L. Kilpatrick, J. W. Kim, C. Kim, Y. Kim, J. H. Kirchner, J. Kiros, M. Knight, D. Kokotajlo, Łukasz Kondraciuk, A. Kondrich, A. Konstantinidis, K. Kosic, G. Krueger, V. Kuo, M. Lampe, I. Lan, T. Lee, J. Leike, J. Leung, D. Levy, C. M. Li, R. Lim, M. Lin, S. Lin, M. Litwin, T. Lopez, R. Lowe, P. Lue, A. Makanju, K. Malfacini, S. Manning, T. Markov, Y. Markovski, B. Martin, K. Mayer, A. Mayne, B. McGrew, S. M. McKinney, C. McLeavey, P. McMillan, J. McNeil, D. Medina, A. Mehta, J. Menick, L. Metz, A. Mishchenko, P. Mishkin, V. Monaco, E. Morikawa, D. Mossing, T. Mu, M. Murati, O. Murk, D. Mély, A. Nair, R. Nakano, R. Nayak, A. Neelakantan, R. Ngo, H. Noh, L. Ouyang, C. O’Keefe, J. Pachocki, A. Paino, J. Palermo, A. Pantuliano, G. Parascandolo, J. Parish, E. Parparita, A. Passos, M. Pavlov, A. Peng, A. Perelman, F. de Avila Belbute Peres, M. Petrov, H. P. de Oliveira Pinto, Michael, Pokorny, M. Pokrass, V. H. Pong, T. Powell, A. Power, B. Power, E. Proehl, R. Puri, A. Radford, J. Rae, A. Ramesh, C. Raymond, F. Real, K. Rimbach, C. Ross, B. Rotsted, H. Roussez, N. Ryder, M. Saltarelli, T. Sanders, S. Santurkar, G. Sastry, H. Schmidt, D. Schnurr, J. Schulman, D. Selsam, K. Sheppard, T. Sherbakov, J. Shieh, S. Shoker, P. Shyam, S. Sidor, E. Sigler, M. Simens, J. Sitkin, K. Slama, I. Sohl, B. Sokolowsky, Y. Song, N. Staudacher, F. P. Such, N. Summers, I. Sutskever, J. Tang, N. Tezak, M. B. Thompson, P. Tillet, A. Tootoonchian, E. Tseng, P. Tuggle, N. Turley, J. Tworek, J. F. C. Uribe, A. Vallone, A. Vijayvergiya, C. Voss, C. Wainwright, J. J. Wang, A. Wang, B. Wang, J. Ward, J. Wei, C. Weinmann, A. Welihinda, P. Welinder, J. Weng, L. Weng, M. Wiethoff, D. Willner, C. Winter, S. Wolrich, H. Wong, L. Workman, S. Wu, J. Wu, M. Wu, K. Xiao, T. Xu, S. Yoo, K. Yu, Q. Yuan, W. Zaremba, R. Zellers, C. Zhang, M. Zhang, S. Zhao, T. Zheng, J. Zhuang, W. Zhuk, and B. Zoph. Gpt-4 technical report, 2023.
[35] C. Packer, S. Wooders, K. Lin, V. Fang, S. G. Patil, I. Stoica, and J. E. Gonzalez. Memgpt: Towards llms as operating systems, 2024.
[36] O. Press, M. Zhang, S. Min, L. Schmidt, N. Smith, and M. Lewis. Measuring and narrowing the compositionality gap in language models. In H. Bouamor, J. Pino, and K. Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 5687-5711, Singapore, Dec. 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp. 378. URL https://aclanthology.org/2023.findings-emnlp. 378.
[37] M. Shao, B. Chen, S. Jancheska, B. Dolan-Gavitt, S. Garg, R. Karri, and M. Shafique. An empirical evaluation of llms for solving offensive security challenges, 2024.</p>
<p>[38] W. Shi, R. Xu, Y. Zhuang, Y. Yu, J. Zhang, H. Wu, Y. Zhu, J. Ho, C. Yang, and M. D. Wang. Ehragent: Code empowers large language models for few-shot complex tabular reasoning on electronic health records, 2024.
[39] N. Shinn, F. Cassano, E. Berman, A. Gopinath, K. Narasimhan, and S. Yao. Reflexion: Language agents with verbal reinforcement learning, 2023.
[40] D. Sobania, M. Briesch, C. Hanna, and J. Petke. An analysis of the automatic bug fixing performance of chatgpt, 2023.
[41] A. Sridhar, R. Lo, F. F. Xu, H. Zhu, and S. Zhou. Hierarchical prompting assists large language model on web navigation, 2023.
[42] T. Sumers, S. Yao, K. Narasimhan, and T. L. Griffiths. Cognitive architectures for language agents, 2023.
[43] X. Tang, A. Zou, Z. Zhang, Z. Li, Y. Zhao, X. Zhang, A. Cohan, and M. Gerstein. Medagents: Large language models as collaborators for zero-shot medical reasoning, 2024.
[44] A. Thakur, G. Tsoukalas, Y. Wen, J. Xin, and S. Chaudhuri. An in-context learning agent for formal theorem-proving, 2024.
[45] R. Thoppilan, D. D. Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T. Cheng, A. Jin, T. Bos, L. Baker, Y. Du, Y. Li, H. Lee, H. S. Zheng, A. Ghafouri, M. Menegali, Y. Huang, M. Krikun, D. Lepikhin, J. Qin, D. Chen, Y. Xu, Z. Chen, A. Roberts, M. Bosma, V. Zhao, Y. Zhou, C.-C. Chang, I. Krivokon, W. Rusch, M. Pickett, P. Srinivasan, L. Man, K. Meier-Hellstern, M. R. Morris, T. Doshi, R. D. Santos, T. Duke, J. Soraker, B. Zevenbergen, V. Prabhakaran, M. Diaz, B. Hutchinson, K. Olson, A. Molina, E. Hoffman-John, J. Lee, L. Aroyo, R. Rajakumar, A. Butryna, M. Lamm, V. Kuzmina, J. Fenton, A. Cohen, R. Bernstein, R. Kurzweil, B. AgueraArcas, C. Cui, M. Croak, E. Chi, and Q. Le. Lamda: Language models for dialog applications, 2022.
[46] J. Wang, Y. Huang, C. Chen, Z. Liu, S. Wang, and Q. Wang. Software testing with large language model: Survey, landscape, and vision, 2023.
[47] L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen, J. Tang, X. Chen, Y. Lin, W. X. Zhao, Z. Wei, and J. Wen. A survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6), Mar. 2024. ISSN 2095-2236. doi: 10.1007/s11704-024-40231-1. URL http://dx.doi.org/10.1007/s11704-024-40231-1.
[48] X. Wang, Y. Chen, L. Yuan, Y. Zhang, Y. Li, H. Peng, and H. Ji. Executable code actions elicit better llm agents, 2024.
[49] Z. Wang, G. Cuenca, S. Zhou, F. F. Xu, and G. Neubig. Mconala: A benchmark for code generation from multiple natural languages, 2023.
[50] Z. Wang, S. Zhou, D. Fried, and G. Neubig. Execution-based evaluation for open-domain code generation, 2023.
[51] Z. Wang, D. Fried, and G. Neubig. Trove: Inducing verifiable and efficient toolboxes for solving programmatic tasks, 2024.
[52] M. Wornow, A. Narayan, K. Opsahl-Ong, Q. McIntyre, N. H. Shah, and C. Re. Automating the enterprise with foundation models, 2024.
[53] Z. Wu, C. Han, Z. Ding, Z. Weng, Z. Liu, S. Yao, T. Yu, and L. Kong. Os-copilot: Towards generalist computer agents with self-improvement, 2024.
[54] Z. Xi, W. Chen, X. Guo, W. He, Y. Ding, B. Hong, M. Zhang, J. Wang, S. Jin, E. Zhou, R. Zheng, X. Fan, X. Wang, L. Xiong, Y. Zhou, W. Wang, C. Jiang, Y. Zou, X. Liu, Z. Yin, S. Dou, R. Weng, W. Cheng, Q. Zhang, W. Qin, Y. Zheng, X. Qiu, X. Huang, and T. Gui. The rise and potential of large language model based agents: A survey, 2023.</p>
<p>[55] C. S. Xia and L. Zhang. Less training, more repairing please: revisiting automated program repair via zero-shot learning. In Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, pages 959-971, 2022.
[56] C. S. Xia, M. Paltenghi, J. L. Tian, M. Pradel, and L. Zhang. Universal fuzzing via large language models. In Proceedings of the 46th IEEE/ACM International Conference on Software Engineering, 2023.
[57] T. Xie, D. Zhang, J. Chen, X. Li, S. Zhao, R. Cao, T. J. Hua, Z. Cheng, D. Shin, F. Lei, Y. Liu, Y. Xu, S. Zhou, S. Savarese, C. Xiong, V. Zhong, and T. Yu. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments, 2024.
[58] A. Z. H. Yang, C. Le Goues, R. Martins, and V. Hellendoorn. Large language models for test-free fault localization. In Proceedings of the IEEE/ACM 46th International Conference on Software Engineering, ICSE '24, New York, NY, USA, 2024. Association for Computing Machinery. ISBN 9798400702174. doi: 10.1145/3597503.3623342. URL https://doi . org/10.1145/3597503.3623342.
[59] J. Yang, A. Prabhakar, K. R. Narasimhan, and S. Yao. Intercode: Standardizing and benchmarking interactive coding with execution feedback. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. URL https://openreview.net/forum?id=fvKaLFlns8.
[60] J. Yang, A. Prabhakar, S. Yao, K. Pei, and K. R. Narasimhan. Language agents as hackers: Evaluating cybersecurity skills with capture the flag. In Multi-Agent Security Workshop@ NeurIPS'23, 2023.
[61] S. Yao, H. Chen, J. Yang, and K. Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents, 2023.
[62] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. R. Narasimhan, and Y. Cao. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=WE_vluYUL-X.
[63] P. Yin, W.-D. Li, K. Xiao, A. Rao, Y. Wen, K. Shi, J. Howland, P. Bailey, M. Catasta, H. Michalewski, A. Polozov, and C. Sutton. Natural language to code generation in interactive data science notebooks, 2022.
[64] H. Yu, B. Shen, D. Ran, J. Zhang, Q. Zhang, Y. Ma, G. Liang, Y. Li, T. Xie, and Q. Wang. Codereval: A benchmark of pragmatic code generation with generative pre-trained models. In International Conference on Software Engineering, 2023. URL https://api. semanticscholar.org/CorpusID:256459413.
[65] Z. Yu, X. Zhang, N. Shang, Y. Huang, C. Xu, Y. Zhao, W. Hu, and Q. Yin. Wavecoder: Widespread and versatile enhanced instruction tuning with refined data generation. arXiv preprint arXiv:2312.14187, 2023.
[66] E. Zelikman, Q. Huang, G. Poesia, N. D. Goodman, and N. Haber. Parsel: Algorithmic reasoning with language models by composing decompositions, 2022. URL https://arxiv.org/ abs/2212.10561.
[67] E. Zelikman, E. Lorch, L. Mackey, and A. T. Kalai. Self-taught optimizer (stop): Recursively self-improving code generation, 2024.
[68] F. Zhang, B. Chen, Y. Zhang, J. Keung, J. Liu, D. Zan, Y. Mao, J.-G. Lou, and W. Chen. Repocoder: Repository-level code completion through iterative retrieval and generation. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. URL https://openreview.net/forum?id=q09vTY1Cqh.
[69] S. Zhang, J. Zhang, J. Liu, L. Song, C. Wang, R. Krishna, and Q. Wu. Training language model agents without modifying language models, 2024.</p>
<p>[70] A. Zhou, K. Yan, M. Shlapentokh-Rothman, H. Wang, and Y.-X. Wang. Language agent tree search unifies reasoning acting and planning in language models, 2023.
[71] S. Zhou, F. F. Xu, H. Zhu, X. Zhou, R. Lo, A. Sridhar, X. Cheng, Y. Bisk, D. Fried, U. Alon, and G. Neubig. Webarena: A realistic web environment for building autonomous agents, 2023.</p>
<h1>Appendix</h1>
<p>In the appendix, we provide additional analyses and more extensive discussions about SWE-agent, agent-computer interface (ACI) design, and model performance on various evaluation benchmarks. We also provide several thorough case studies of SWE-agent behavior on select task instances. Data, code, and leaderboard at swe-agent.com.</p>
<h2>A SWE-agent Design</h2>
<p>In this section, we go into greater discussion about the design methodology, appearance, and implementation of each of the SWE-agent components. As described in Section 3, the SWE-agent interface consists of several components that enable agents to accomplish key subtasks that are fundamental to solving software engineering problems. These are generally the following:</p>
<ol>
<li>Localization: Identify file(s)/line(s) causing the issue.</li>
<li>Editing: Generate fixes addressing the given issue.</li>
<li>Testing: Write new scripts or modify existing test files to reproduce the issue and/or verify if fixes are correct.</li>
</ol>
<p>To enable LM-based agents to efficiently carry out these individual functions and progress towards the overarching goal of resolving a codebase issue, we provide a file viewer, file editor, search / navigation system, and context management system. In Section A.1, we provide a thorough breakdown of each of these components. In Section A.2, we discuss the technical design decisions and challenges of building SWE-agent. In Section A.3, we discuss how SWE-agent is configured to support the final interface, along with how SWE-agent is built to enable easy extensibility and customization to alter the interface.</p>
<h2>A. 1 ACI Design</h2>
<p>In this section, we revisit each component discussed in Section 3. Per section, we first briefly review the component. We then discuss the underlying motivation for the component with respect to existing software tools. Finally, we note any additional thoughts that influenced the design process of the component with some occasional discussion of what aspects of the component heavily influence language model behavior.</p>
<p>For a quick, text-free overview, comprehensive documentation for all commands, their usage, and docstrings are included in Table 4. Figure 9 visualizes the message history for SWE-agent. Each prompt template is discussed thoroughly in Section C.</p>
<p>File viewer. As discussed in Section 3, the File Viewer is fundamental to a language agent's ability to understand file content and understand how different programmatic entities relate to one another. The File Viewer refers to an interface that consists of the four commands, as shown in Table 4, and a customized standard output for displaying $n$ lines of a file at a time. Using the file viewer, an agent can look at $n$ lines of a file at a time and jump around the file. The File Viewer enables agents to perform fine-grained localization steps and also understand relationships between intra-file entities.
First, we discuss why existing software systems and graphical user interfaces are sub-optimal for LM use. In a Shell-only setting, there are several commands that can be used to inspect file content. However, out of the box command line tools are sub-optimal or limiting for language agents for</p>
<p>Table 4: In additional to the standard Linux Bash commands, we provide SWE-agent with specialized tools, including an interactive file viewer, search functionalities, and edit tools for the open file. Required arguments are enclosed in $&lt;&gt;$ and optional arguments are in []. The last column shows the documentation presented to the LM.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Category</th>
<th style="text-align: center;">Command</th>
<th style="text-align: center;">Documentation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">File viewer</td>
<td style="text-align: center;">open <path> <br> [<line_number>]</td>
<td style="text-align: center;">Opens the file at the given path in the editor. If line_number is provided, the window will move to include that line.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">goto <line_number></td>
<td style="text-align: center;">Moves the window to show line_number.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">scroll_down</td>
<td style="text-align: center;">Moves the window up 100 lines.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">scroll_up</td>
<td style="text-align: center;">Moves the window down 100 lines.</td>
</tr>
<tr>
<td style="text-align: center;">Search tools</td>
<td style="text-align: center;">search_file <search_term> [<file>]</td>
<td style="text-align: center;">Searches for search_term in file. If file is not provided, searches in the current open file.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">search_dir <search_term> <br> [<dir>]</td>
<td style="text-align: center;">Searches for search_term in all files in dir. If dir is not provided, searches in the current directory.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">find_file <file_name> [<dir>]</td>
<td style="text-align: center;">Finds all files with the given name in dir. If dir is not provided, searches in the current directory.</td>
</tr>
<tr>
<td style="text-align: center;">File editing</td>
<td style="text-align: center;">edit $&lt;\mathrm{n}&gt;:&lt;\mathrm{m}&gt;$ <br> <replacement_text> end_of_edit</td>
<td style="text-align: center;">Replaces lines n through m (inclusive) with the given text in the open file. All of the replacement_text will be entered, so make sure your indentation is formatted properly. Python files will be checked for syntax errors after the edit. If an error is found, the edit will not be executed. Reading the error message and modifying your command is recommended as issuing the same command will return the same error.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">create <filename></td>
<td style="text-align: center;">Creates and opens a new file with the given name.</td>
</tr>
<tr>
<td style="text-align: center;">Task</td>
<td style="text-align: center;">submit</td>
<td style="text-align: center;">Generates and submits the patch from all previous edits and closes the shell.</td>
</tr>
</tbody>
</table>
<p>several reasons. First, commands that print files to standard output (e.g. cat, printf) can easily flood a language agent's context window with too much file content, the majority of which is usually irrelevant to the issue. Enabling a language agent to filter out distractions and focus on relevant code snippets is crucial to generating effective edits. While commands like head and tail reduce length to the first/last n lines, it is not intuitive to use bash commands to perform in-file navigation. It is either impossible or requires a long list of arguments to show specific file lines. Furthermore, since such Bash commands are stateless, "scrolling" up/down relative to the current file position typically requires regenerating the same lengthy command with minor changes. Interactive tools like more and less accommodate this, but (1) representing navigation actions (multiple key up/down clicks) is intuitive for humans, but is verbose and costly for language agents, and (2) even if jumping to a specific line number is allowed, it is not possible to quickly identify what classes/methods/symbols are declared in a file and then immediately go to their definitions.</p>
<p>There are a couple features of the File Viewer interface that make it friendlier and more operable than the Shell-only setting. First, the File Viewer standard output contextualizes code snippets with prepended line numbers and indicators of the number of lines above/below the current region. These details give a more focused view of a file without compromising easy viewing of other parts of the codebase. This kind of file presentation also makes precise and consistent editing commands possible, as we discuss more thoroughly in the following section.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 10: The File Viewer and Search components of the SWE-agent interface. The corresponding commands for each component are shown in blue. These examples are copied from trajectories generated by SWE-agent w/ GPT-4 Turbo on the pvlib_pvlib-python-1603 task instance.</p>
<p>Another advantage of the File Viewer is that the commands are designed to be complementary and grounded in the File Viewer standard output. This saves the model from having to do repetitive or additional actions that unnecessarily increase the potential for error. As a concrete example, if an agent used a sed command to view the first 100 lines of a file and wants to look at the next 100 lines, it will have to recalculate parameters such as the start line and end line and reflect these updates correctly in the subsequent generation. As a rule of thumb, reducing the need for models to do this arithmetic by constructing actions and standard output that complement one another and build upon the effects of prior actions is highly preferable.</p>
<p>File editor. The File Editor, working in conjunction with the File Viewer, primarily refers to the edit command and the guardrails it enforces to protect models against self-incurred cascading edit errors. Editing and testing are crucial to language agents' success on programming tasks, and a well-designed interface directly influences how well an agent's capabilities can be elicited. In other words, a bad interface undermines model performance.</p>
<p>As discussed in Section 3, editing can be very difficult in a Shell-only setting. Built in commands (e.g., sed) often require a lengthy list of arguments, and the mis-specification of an argument can easily throw a model off track as it attempts to correct self-incurred errors. We also observe that when agents use such commands directly, they struggle with the arithmetic skills required to generate an edit. Details such as including the correct indentation level, inserting delimiters at specific points in a line, and adhering to stylistic preferences of the codebase all require some amount of planning or calculation. Similar to the Shell-only file viewing process, file editing may also require repeating many commands. For instance, performing a multi-line edit can only be represented as multiple sed calls with requisite, delicate tweaks to the arguments for every turn. Furthermore, as referenced in Section 5.1, editing in Shell-only is usually a "silent" procedure. Confirming whether an edit succeeded and viewing its effects requires additional steps that can bloat the editing process with extra, needless commands.</p>
<p>The edit command, documented in Table 4, addresses the Shell-only failure modes by being grounded in the File Viewer standard output. The line numbers argument eliminates the need for any additional arithmetic, and the find-and-replace edit mechanism is a format that existing models are more used to. With this functionality, agents can also perform multi-line edits in a single action.</p>
<p>Finally, as mentioned in Section 5.2, an important feature of the edit command is that it does not apply changes which incur a linting error. A fair and verified assumption we make when considering this feature is that the original codebase associated with each task instance is well-formed. In other</p>
<p>words, we assume that codebase maintainers will only push syntactically sound code that can be compiled successfully. When an agent issues an edit, it is applied to the codebase. Then, we run the following linting command (CURRENT_FILE refers to the file that is currently open):
flake8 --isolated --select=F821,F822,F831,E111,E112,E113,E999,E902 "\$CURRENT_FILE" 2&gt;\&amp;1</p>
<p>The arguments for select are error codes that refer to syntax issues such as indentation. F821 and F822 indicate undefined names/symbols. F831 indicates a duplicate argument in a function definition. E111, E112, E113 are indentation errors. E999 denotes a syntax error and an E902 occurs if flake8 cannot read the source file.</p>
<p>If the edit does not introduce any of these errors, this command will produce no output. The edit is kept and the updated file content is shown using the File Viewer centered around the lines the edit occurred. If however the linting command produces output, which indicates the edit introduces a syntax error, the edit is reverted. In place of the file viewer, a message shown in Figure 11 is displayed to the agent which shows the error that was caused, what the edit would have looked like, and the original file content. During the development process, we experimented with variations to this message, including the omission of one or more parts. Our takeaway was that having all three messages is helpful. Without the error type, the agent might misdiagnose what the mistake was. Without a snippet of the changed file content, the agent will re-issue the same command more frequently. Without a snippet of the original file content, the agent has to attend to the same content from several turns ago; agents also sometimes generate edit's with respect to wrong, errant file content because it is from a more recent turn.</p>
<h1>Linting Error Message</h1>
<div class="codehilite"><pre><span></span><code>Your proposed edit has introduced new syntax error(s). Please
    \ understand the fixes and retry your edit commmand.
ERRORS:
<span class="k">-</span> F821 undefined name &#39;orientation_strategy&#39;
<span class="k">-</span> F821 undefined name &#39;orientation_strategy&#39;
This is how your edit would have looked if applied
[File: /pvlib__pvlib-python/pvlib/modelchain.py (1890 lines total)]
(64 more lines above)
65:def basic_chain(times, latitude, longitude,
66: module_parameters, temperature_model_parameters,
67: inverter_parameters,
68: irradiance=None, weather=None,
69: surface_tilt=None, surface_azimuth=None,
70: transposition_model=&#39;haydavies&#39;,
71: solar_position_method=&#39;nrel_numpy&#39;,
72: airmass_model=&#39;kastenyoung1989&#39;,
73: altitude=None, pressure=None,
74: **kwargs):
(1816 more lines below)
</code></pre></div>

<p>This is the original code before your edit
[File: /pvlib__pvlib-python/pvlib/modelchain.py (1891 lines total)]
(64 more lines above)
65:def basic_chain(times, latitude, longitude,
66: module_parameters, temperature_model_parameters,
67: inverter_parameters,
68: irradiance=None, weather=None,
69: surface_tilt=None, surface_azimuth=None,
70: orientation_strategy=None,
71: transposition_model='haydavies',</p>
<table>
<thead>
<tr>
<th style="text-align: left;">72:</th>
<th style="text-align: left;">solar_position_method='nrel_numpy',</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">73:</td>
<td style="text-align: left;">airmass_model='kastenyoung1989',</td>
</tr>
<tr>
<td style="text-align: left;">74:</td>
<td style="text-align: left;">altitude=None, pressure=None,</td>
</tr>
<tr>
<td style="text-align: left;">75:</td>
<td style="text-align: left;">**kwargs):</td>
</tr>
<tr>
<td style="text-align: left;">(1816 more lines below)</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>Your changes have NOT been applied. Please fix your edit command and try again.
You either need to 1) Specify the correct start/end line arguments or
$\rightarrow$ 2) Correct your edit code.
DO NOT re-run the same failed edit command. Running it again will
$\rightarrow$ lead to the same error.</p>
<p>Figure 11: A linting error message. This is emitted if a model generates an edit command that introduces a syntax error into the codebase. The error message shows the before and after of the proposed edit along with what error messages were thrown. The problem with this edit is that it omits the orientation_strategy field in its edit of the basic_chain method definition.
The editing guardrail has a drawback. To a certain degree, it forces some edits to be done in a particular order. For instance, in Figure 11, if the model's intention was in fact to remove the orientation_strategy argument, due to the SWE-agent editing guardrails, it would have to remove all references from the function implementation either at the same time in a single action, or before removing it from the method header if split into two separate actions. For this particular scenario, the latter is necessary because the file snippet is not large enough to show the entirety of the basic_chain implementation. This example highlights the trade-offs between the flexibility and guardrails of a command. Deciding whether to introduce a guardrail depends on how well it reduces common model errors compared to whether such restrictions hamper models' preferred workflows.</p>
<p>Search \&amp; navigation. The File Viewer and File Editor together allow agents to make edits, write tests, and perform localization at a file level. The Search \&amp; navigation module complements these capabilities by giving agents the tools to perform keyword-driven localization at both a directory level and file level.</p>
<p>As discussed, the main struggles with using built in Shell-only search commands such as grep and find are (1) given a general enough term, they are prone to producing too many search results that can consume an inordinate amount of space in the context window, and (2) they are highly configurable, making search result outcomes potentially inconsistent in appearance. The alternative to these search utilities is to navigate the file system directly with cd and look at what's in each folder with variations of ls and cat; this kind of approach can take a large number of turns without yielding any particularly useful information.
Figure 10 visualizes the standard output for the three different search commands. The search_dir and find_file helps agents perform directory level searches. The reason we provide two commands is due to the kinds of keywords that are present in an issue description (e.g., class references, file names). The search_file command allows agents to search for terms at a file-level, which is helpful for efficient fine-grained localization. Taking a step back, the goal of these search commands is to make it easy for the agent to utilize any signal (e.g., line number, stack trace, natural language) about the root cause of an issue that may be present in the issue description or codebase. Once again, simpler command usage patterns with consistent output formats are easier for agents to use and reduces the chance for mistakes or irrelevant outputs.
The main guardrail in place for all three search commands is curbing the number of search results to 50 or fewer. The downside is that reporting an error forces the model to generate another search query which can be an expensive operation. This reflects a trade-off between keeping observations concise and making additional calls to the base LM.</p>
<h1>A. 2 Implementation</h1>
<p>The SWE-agent codebase is generally composed of three modules: the environment, the agent, and the logging mechanism for saving task episodes into trajectories and patch generations.</p>
<p>Environment. The SWE-agent environment is heavily influenced by the InterCode library [59]. For the general pipeline of agent interactions with the environment, our work directly adopts InterCode's interactive coding task formulation. The environment integrates large parts of the interaction handling logic from the InterCode-Bash environment, which is essentially the Shell-only setting referenced in the paper. As a part of this adoption, SWE-agent also uses Docker containers to ensure reproducible and safe execution. Because of this, SWE-agent's infrastructure makes it easy for a user to swap out the Dockerfile (a domain specific language for defining a container) to support other codebases and programming languages beyond the scope of SWE-bench task instances. One difference is that SWE-agent makes minor adjustments to the underlying communication logic that transfers actions and observations between the Docker container and agent entity.
Agent. Beyond serving as an agentic wrapper for facilitating multi-turn queries from an LM, the agent module defines the functions that render the ACI (e.g., context management, commands, interface logic, input/output format) and supports inference for closed/open, API-based/local language models. The main workflow is to define an interface as a class and/or set of commands, which can then be specified via a configuration file, discussed more thoroughly in Section A.3. The commands for the top performing SWE-agent with GPT 4 configuration are shown in Table 4.
Logging. For each task episode, the main artifacts produced are the trajectory, which contains a history of the interactions between the agent and environment, and the final patch generation, which can represents a summary of the changes proposed by the agent during the interaction. The patch generation can be used directly for SWE-bench [20] evaluation.</p>
<h1>A. 3 Configuration</h1>
<p>The SWE-agent system is instantiated by three components: an LM, a SWE-bench style dataset or GitHub issue, and a configuration file. The configuration file serves to specify the design of the ACI. Iteratively refining the configuration file is the main way we achieved better agent performance and carried out different analyses for the main paper. In this section, we will present a thorough review of what a SWE-agent configuration file looks like.
An agent-computer interface is generally made up of four categories of configurable components:</p>
<ol>
<li>Prompt templates: These prompt templates are used to inform the language model of the task setting, show the list of available commands, augment environment responses with the values of state variables, and provide the initial task setting.</li>
<li>Command files: These files contain the source code of bash or Python functions and scripts. Commands are easily modified, added, and removed through manipulating these files' code contents directly. Documentation added in these files can also be injected into prompts to inform the model of the available commands.</li>
<li>Control flow: Methods for parsing model responses and processing history can be specified through these configuration arguments.</li>
<li>Environment variables: Initial values of variables that may interact with commands and the shell can also be specified in the configuration.</li>
</ol>
<p>In the following Figure 12, we include an annotated example of the contents of a configuration file.</p>
<div class="codehilite"><pre><span></span><code>Configuration (.yaml)
<span class="gh">#</span> Prompt Templates: Control how observations of environment are shown
    to agent
system_template: |
    First &quot;system&quot; message shown to agent
instance_template: |
    Instance prompt, contains task instance-specific content
next_step_template: |-
    Format template of per-turn observation (Contains standard output
    from agent&#39;s action)
next_step_no_output_template: |-
</code></pre></div>

<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>https://github.com/meta-llama/llama3
Token counts for different models are not directly comparable since they use different tokenizers.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
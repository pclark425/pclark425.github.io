<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8159 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8159</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8159</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-149.html">extraction-schema-149</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <p><strong>Paper ID:</strong> paper-258865170</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2305.15054v2.pdf" target="_blank">A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis</a></p>
                <p><strong>Paper Abstract:</strong> Mathematical reasoning in large language models (LMs) has garnered significant attention in recent work, but there is a limited understanding of how these models process and store information related to arithmetic tasks within their architecture. In order to improve our understanding of this aspect of language models, we present a mechanistic interpretation of Transformer-based LMs on arithmetic questions using a causal mediation analysis framework. By intervening on the activations of specific model components and measuring the resulting changes in predicted probabilities, we identify the subset of parameters responsible for specific predictions. This provides insights into how information related to arithmetic is processed by LMs. Our experimental results indicate that LMs process the input by transmitting the information relevant to the query from mid-sequence early layers to the final token using the attention mechanism. Then, this information is processed by a set of MLP modules, which generate result-related information that is incorporated into the residual stream. To assess the specificity of the observed activation dynamics, we compare the effects of different model components on arithmetic queries with other tasks, including number retrieval from prompts and factual knowledge questions.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8159.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8159.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-J</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-J-6B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 6-billion-parameter decoder-only Transformer language model used in this study as the primary model for mechanistic analysis of arithmetic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GPT-J-6B: A 6 billion parameter autoregressive language model.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-J-6B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only Transformer (GPT-style) with parallel attention and rotary positional encodings; ~6B parameters; pre-trained (Wang & Komatsuzaki, 2021).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Two-operand arithmetic (+, -, ×, ÷) and three-operand two-operator combinations; experiments with Arabic numerals and numeral words; also compared to number retrieval and factual (LAMA) tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Processing pipeline: early-layer MLPs represent operands/operators at their token positions; attention (mid-to-late layers) routes operand/operator information to the last-token residual stream; last-token mid-to-late MLPs (notably layers ~19-20 in GPT-J) incorporate result-related information into the residual stream used for next-token prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Causal mediation analysis via activation patching: store m^{(l)}_t and a^{(l)}_t from a reference prompt and overwrite corresponding MLP/attention outputs during a second forward pass; compute indirect effect (IE) (Eq. 2). Also neuron-level interventions (set individual neuron activations) and alternative IE using log-probability differences.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Arithmetic (two-operand) overall accuracy ≈ 67.8%; per-operator: + ≈ 69.3%, - ≈ 78.0%, × ≈ 82.8%, ÷ ≈ 40.8%. Using numeral words overall ≈ 81.3% (notable gains for + and ÷). Factual (LAMA) accuracy ≈ 65.0%. Number-retrieval task accuracy ≈ 86.7%. Three-operand pre-trained accuracy <10% for GPT-J (poor).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Weak performance on division and multi-operator (three-operand) queries; tokenization issues for multi-digit numbers (necessitating restricted result set); context-dependent and inconsistent behavior; pre-trained models often fail on three-operand queries (improved by fine-tuning). Interventions at some earlier layers can cause undesired prediction changes.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Layerwise IE heatmaps show (1) high IE at first-layer MLPs at operand/operator tokens; (2) high IE at attention modules at the final token (layers ~11-18); (3) strong IE peaks at last-token MLPs in mid-late layers (layers ~19-20) when results vary; RI metric shows large contribution of last-token late MLPs (GPT-J RI(M_late_-1) ≈ 40.2% when results vary, dropping to ≈ 4.4% when results are fixed). Neuron-level overlap (top neurons) between Arabic and word numerals is high (~50%), but low overlap with number-retrieval and factual tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Number retrieval and factual tasks show different activation patterns (early MLPs at subject/entity tokens dominate) and low RI for last-token late MLPs (e.g., factual RI ≈ 4.2%), indicating the last-token MLP role is specific to result computation, not generic numeric prediction. Interventions reveal some layers (14-17) can induce undesired (correct→wrong) changes, so not all high-IE sites reliably compute results.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8159.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8159.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pythia-2.8B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pythia 2.8B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 2.8-billion-parameter decoder-only Transformer model used to validate findings and to run fine-tuning experiments for three-operand arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Pythia: A suite for analyzing large language models across training and scaling.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Pythia 2.8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only Transformer series model (2.8B parameters) designed for analysis; pre-trained (Biderman et al., 2023).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Two-operand arithmetic (+, -, ×, ÷) and three-operand two-operator combinations; experiments include pre- and post-fine-tuning three-operand evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Same pipeline observed: early MLPs encode operands/operators, attention moves information to final token, last-token mid-late MLPs produce result-related activations; before fine-tuning the three-operand signal is weak at mid-late MLPs but emerges after fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Causal mediation activation patching on MLPs and attention outputs at token positions; neuron-level interventions and RI metric computed; fine-tuning on small set of three-operand queries followed by same interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Two-operand overall accuracy ≈ 59.9% (per-operator breakdown in paper). Three-operand pre-trained overall ≈ 0.9%; after fine-tuning on 29 templates (1000 samples each, 2 epochs with Adafactor, lr 1e-5) three-operand accuracy improved to ≈ 39.7%.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Very poor out-of-the-box three-operand performance (near-zero) prior to task-specific fine-tuning; model requires fine-tuning to develop mid-late MLP activation site for multi-operator computations.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Layerwise IE before fine-tuning shows only last-layer activation at last token; after fine-tuning a mid-late last-token MLP activation site appears (matching patterns seen in two-operand experiments); RI values and heatmaps consistent with mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Pre-fine-tuning absence of mid-late MLP activation site indicates mechanism may not be learned for complex multi-step arithmetic without task-specific training; some arithmetic capabilities may be acquired or strengthened via fine-tuning rather than present in pretraining.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8159.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8159.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA 7B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7-billion-parameter open foundation Transformer model used to replicate the paper's mechanistic observations across model families.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Llama: Open and efficient foundation language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA 7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only Transformer foundation model with 7B parameters (Touvron et al., 2023); used here unmodified and also as base for Goat.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Two-operand arithmetic (+, -, ×, ÷) evaluated with same templates and settings as other models.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Observed similar information flow: early MLPs at operand/operator tokens, attention conveying to last token, and last-token mid-late MLPs contributing to result-related activations (albeit layer indices differ by architecture/size).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Causal mediation activation patching across MLPs and attention outputs; RI metric computed to compare late MLP contribution; comparisons to number retrieval and factual tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported high arithmetic accuracy in experiments: overall ≈ 97.2% on the two-operand set used in the paper (per-table values), with high per-operator scores (near 100% for some operations on tested templates).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Differences in tokenization (LLaMA tokenizer treats digits as separate tokens) required result-space restrictions (single-digit results) for fair comparison; errors still present for multi-operator/more complex queries if not fine-tuned.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>RI and IE measurements on LLaMA mirror those on GPT-J and Pythia: significant last-token mid-late MLP contributions for arithmetic, and attention modules conveying operand/operator information.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Tokenizer differences required restricting result sets, complicating direct comparison; despite high overall scores here, generalization to more complex or out-of-distribution arithmetic remains unproven.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8159.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8159.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Goat</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Goat (fine-tuned LLaMA on arithmetic tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A LLaMA-based model fine-tuned specifically on arithmetic tasks, used to assess how task-specific tuning affects arithmetic mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Goat: Fine-tuned LLaMA outperforms GPT-4 on arithmetic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Goat (fine-tuned LLaMA)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLaMA-weight-based model further fine-tuned on arithmetic training data to improve arithmetic accuracy (Liu & Low, 2023).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Two-operand arithmetic (+, -, ×, ÷) and evaluated in same experimental framework to compare activation patterns after targeted fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Fine-tuning concentrates arithmetic-relevant information into similar activation sites (last-token mid-late MLPs) observed in pre-trained models; fine-tuning can amplify the MLP-based result representation.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Same causal mediation activation patching and IE/RIs as other models to compare pre- and post-fine-tuning activation patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported overall arithmetic accuracy ≈ 85.6% on the paper's evaluation set (per-table values), with strong increases relative to baseline on some operations.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Although fine-tuning improves accuracy, some operations (e.g., division) and multi-operator queries can still be error-prone; potential overfitting to template distributions is a concern.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>RI and IE measures for Goat match the qualitative pattern: early MLPs/attention route information and late MLPs encode result; magnitudes reflect improved task competence after fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Fine-tuned arithmetic skills may not generalize beyond the fine-tuning distribution (templates and numeric ranges used); the paper notes tokenizer and result-space restrictions remain relevant.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8159.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8159.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Causal Mediation Analysis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Causal Mediation Analysis (activation patching intervention)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A causality-grounded experimental framework used here to attribute causal effect of specific model components on arithmetic predictions by intervening on internal activations and measuring output probability changes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Investigating gender bias in language models using causal mediation analysis</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>applies to studied models (GPT-J, Pythia, LLaMA, Goat)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not a model but a methodology: treat model components as mediators in a causal graph; interventions overwrite stored activations from a reference input into a target run and measure indirect effect on predicted probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Used to study two- and three-operand arithmetic tasks and to compare to number retrieval and factual tasks within the same causal framework.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Enables identification of where in the network result-related vs operand-related information is encoded (e.g., last-token MLPs vs early MLPs/attention).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Activation patching: save m^{(l)}_t and ā^{(l)}_t from run p1, run p2 while replacing selected module outputs with the saved values; compute IE as in Eq. 2 (and alternative log-probability metric) to quantify causal contributions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not a predictive metric but provides per-component IE scores; computed RI over sets of MLPs to quantify relative contribution (e.g., GPT-J RI(M_late_-1) ≈ 40.2% when results vary).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Interventions can produce both desired (wrong→correct) and undesired (correct→wrong) prediction flips depending on layer; IE depends on choice of reference and target inputs; coarse module-level interventions may mask head- or neuron-level heterogeneity.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Consistent, reproducible IE heatmaps across multiple architectures showing the same three activation sites (early MLPs at operand/operator tokens, attention at last token, mid-late last-token MLPs) and RI reductions when result is fixed, supporting causal role of last-token MLPs in result computation.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Activation-level interventions treat full module outputs as units; the paper notes limitation of not analyzing individual attention heads and that module-level effects may hide finer-grained circuit structure. IE magnitude depends on metric choice (probability vs log-prob), though qualitative conclusions are stable.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8159.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8159.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Last-token mid-late MLPs (M_late_-1)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Last-token mid-to-late-layer MLP modules (M_late_-1 activation site)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A specific subset of MLP modules at the final token in mid-to-late layers identified as the site where result-related information is incorporated into the residual stream for arithmetic predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>applies to studied models (observed in GPT-J, Pythia, LLaMA, Goat)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>MLP output vectors at the final input token, specifically in middle-to-late layers (e.g., layers ~19-20 in GPT-J), that show high indirect effect on arithmetic result probabilities when intervened.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Two-operand and three-operand arithmetic where results vary between comparison prompts; also observed to emerge after fine-tuning for three-operand tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Represent result-related information (r) as activations inserted into the residual stream at the final token; distinct from early-layer operand representations and from attention-mediated transport.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Module-level activation patching of MLP outputs at the last token; neuron-level interventions within these MLPs (ranking top neurons by IE and computing overlaps across tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Quantified by IE values and RI metric: e.g., GPT-J RI(M_late_-1) ≈ 40.2% (result-varying) vs ≈ 4.4% (result-fixed). Across models RI values range ≈ 27.8%–43.2% (when result varies) and drop to single-digit percentages when result is fixed.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Not all last-token MLPs are equally beneficial: specific layers (19-20) tended to produce desired (wrong→correct) changes, whereas slightly earlier layers (14-17) sometimes produced undesired flips; thus interventions there can worsen predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Strong IE peaks at these MLPs when r differs across prompts; RI drops dramatically when r is held constant, demonstrating these MLPs encode result-specific information. Neuron overlap analysis shows task-specific neuron sets within these MLPs.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>On non-arithmetic numeric tasks (number retrieval) and factual prediction, these last-token MLPs have far lower relative importance (e.g., RI ≈ 8.7% for number retrieval, ≈ 4.2% for factual), indicating the role is task-specific and not a generic numeric-storage location.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8159.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8159.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Neuron-level interventions</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neuron-level activation patching and ranking</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Finer-grained interventions that overwrite individual neuron activations within MLP modules and rank neurons by their indirect effect to identify task-specific neuron sets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>applies primarily to GPT-J (layer 19 analysis) but conceptually to other models</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Per-dimension interventions on the output vector of an MLP layer (each neuron = dimension) at a specific token/layer; used to compute neuron IE and rank neurons by average effect.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Applied to two-operand arithmetic (Arabic and numeral words), number retrieval, and factual prediction tasks for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Identifies neurons within a last-token MLP that are most causally responsible for shifting probability mass toward particular arithmetic results; reveals partially overlapping but mostly distinct neuron sets across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>For each neuron, set activation to value from reference run and compute IE; rank neurons by average IE and compute overlap of top-k (top 400) neurons across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Overlap statistics used as evidence: ~50% overlap between Arabic and word-based arithmetic top-400 neurons; ~22-23% overlap between arithmetic and number retrieval; ~9-10% overlap with factual — near random expectation (~9.8%).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Neuron-level ranking ignores effect magnitude heterogeneity and downstream nonlinear interactions; overlapping top neurons does not imply identical causal roles; measuring only top indices can miss distributed contributions.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Overlap patterns support that arithmetic representations (even across numeral encodings) share significant neuronal substrates, while number retrieval and factual tasks use largely distinct neurons within the same MLP layers.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Top-neuron overlaps with number retrieval and factual tasks are low, but this does not rule out some shared subspace or distributed encoding; moreover, interventions on single neurons may have small effects due to redundancy.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8159.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8159.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Relative Importance (RI)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Relative Importance metric for MLP subsets</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A quantitative metric defined in this paper to measure the relative contribution of a subset of MLP modules (e.g., mid-late last-token MLPs) to the overall IE across all MLPs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>applies to studied models (GPT-J, Pythia, LLaMA, Goat)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>RI(M*) = sum_{m in M*} log(IE(m)+1) / sum_{m in M} log(IE(m)+1), used to quantify fraction of total IE attributable to subset M*.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Used to quantify importance of last-token mid-late MLPs for two-operand and three-operand arithmetic, and to compare with number retrieval and factual tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Operationalizes contribution of identified activation sites (e.g., M_late_-1) to result computation relative to the whole model's MLPs.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Calculated from IE measurements obtained via activation patching across MLP modules.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Example values: GPT-J RI(M_late_-1) ≈ 40.2% (result-varying) vs 4.4% (result-fixed); Pythia 2.8B ≈ 43.2% vs 5.8%; LLaMA 7B ≈ 36.1% vs 7.5%; Goat ≈ 33.5% vs 7.4%. After fine-tuning Pythia on 3-operand tasks RI increased (pre/post fine-tune reported).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>RI depends on IE estimates which in turn depend on sampled prompt pairs and metric choice; small IE values and log transform stabilize but choice of M* can affect interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Large RI values for last-token mid-late MLPs when results vary and the dramatic RI drop when results are fixed provide quantitative evidence these MLPs specifically encode result information.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>RI is an aggregate statistic and may conceal that only a few neurons or heads within M* are doing the computation; it does not identify head-level contributors.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Locating and editing factual associations in GPT <em>(Rating: 2)</em></li>
                <li>Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space <em>(Rating: 2)</em></li>
                <li>A mathematical framework for transformer circuits <em>(Rating: 2)</em></li>
                <li>Investigating gender bias in language models using causal mediation analysis <em>(Rating: 2)</em></li>
                <li>Goat: Fine-tuned LLaMA outperforms GPT-4 on arithmetic tasks <em>(Rating: 2)</em></li>
                <li>Pythia: A suite for analyzing large language models across training and scaling. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8159",
    "paper_id": "paper-258865170",
    "extraction_schema_id": "extraction-schema-149",
    "extracted_data": [
        {
            "name_short": "GPT-J",
            "name_full": "GPT-J-6B",
            "brief_description": "A 6-billion-parameter decoder-only Transformer language model used in this study as the primary model for mechanistic analysis of arithmetic reasoning.",
            "citation_title": "GPT-J-6B: A 6 billion parameter autoregressive language model.",
            "mention_or_use": "use",
            "model_name": "GPT-J-6B",
            "model_description": "Decoder-only Transformer (GPT-style) with parallel attention and rotary positional encodings; ~6B parameters; pre-trained (Wang & Komatsuzaki, 2021).",
            "arithmetic_task_type": "Two-operand arithmetic (+, -, ×, ÷) and three-operand two-operator combinations; experiments with Arabic numerals and numeral words; also compared to number retrieval and factual (LAMA) tasks.",
            "mechanism_or_representation": "Processing pipeline: early-layer MLPs represent operands/operators at their token positions; attention (mid-to-late layers) routes operand/operator information to the last-token residual stream; last-token mid-to-late MLPs (notably layers ~19-20 in GPT-J) incorporate result-related information into the residual stream used for next-token prediction.",
            "probing_or_intervention_method": "Causal mediation analysis via activation patching: store m^{(l)}_t and a^{(l)}_t from a reference prompt and overwrite corresponding MLP/attention outputs during a second forward pass; compute indirect effect (IE) (Eq. 2). Also neuron-level interventions (set individual neuron activations) and alternative IE using log-probability differences.",
            "performance_metrics": "Arithmetic (two-operand) overall accuracy ≈ 67.8%; per-operator: + ≈ 69.3%, - ≈ 78.0%, × ≈ 82.8%, ÷ ≈ 40.8%. Using numeral words overall ≈ 81.3% (notable gains for + and ÷). Factual (LAMA) accuracy ≈ 65.0%. Number-retrieval task accuracy ≈ 86.7%. Three-operand pre-trained accuracy &lt;10% for GPT-J (poor).",
            "error_types_or_failure_modes": "Weak performance on division and multi-operator (three-operand) queries; tokenization issues for multi-digit numbers (necessitating restricted result set); context-dependent and inconsistent behavior; pre-trained models often fail on three-operand queries (improved by fine-tuning). Interventions at some earlier layers can cause undesired prediction changes.",
            "evidence_for_mechanism": "Layerwise IE heatmaps show (1) high IE at first-layer MLPs at operand/operator tokens; (2) high IE at attention modules at the final token (layers ~11-18); (3) strong IE peaks at last-token MLPs in mid-late layers (layers ~19-20) when results vary; RI metric shows large contribution of last-token late MLPs (GPT-J RI(M_late_-1) ≈ 40.2% when results vary, dropping to ≈ 4.4% when results are fixed). Neuron-level overlap (top neurons) between Arabic and word numerals is high (~50%), but low overlap with number-retrieval and factual tasks.",
            "counterexamples_or_challenges": "Number retrieval and factual tasks show different activation patterns (early MLPs at subject/entity tokens dominate) and low RI for last-token late MLPs (e.g., factual RI ≈ 4.2%), indicating the last-token MLP role is specific to result computation, not generic numeric prediction. Interventions reveal some layers (14-17) can induce undesired (correct→wrong) changes, so not all high-IE sites reliably compute results.",
            "uuid": "e8159.0"
        },
        {
            "name_short": "Pythia-2.8B",
            "name_full": "Pythia 2.8B",
            "brief_description": "A 2.8-billion-parameter decoder-only Transformer model used to validate findings and to run fine-tuning experiments for three-operand arithmetic.",
            "citation_title": "Pythia: A suite for analyzing large language models across training and scaling.",
            "mention_or_use": "use",
            "model_name": "Pythia 2.8B",
            "model_description": "Decoder-only Transformer series model (2.8B parameters) designed for analysis; pre-trained (Biderman et al., 2023).",
            "arithmetic_task_type": "Two-operand arithmetic (+, -, ×, ÷) and three-operand two-operator combinations; experiments include pre- and post-fine-tuning three-operand evaluation.",
            "mechanism_or_representation": "Same pipeline observed: early MLPs encode operands/operators, attention moves information to final token, last-token mid-late MLPs produce result-related activations; before fine-tuning the three-operand signal is weak at mid-late MLPs but emerges after fine-tuning.",
            "probing_or_intervention_method": "Causal mediation activation patching on MLPs and attention outputs at token positions; neuron-level interventions and RI metric computed; fine-tuning on small set of three-operand queries followed by same interventions.",
            "performance_metrics": "Two-operand overall accuracy ≈ 59.9% (per-operator breakdown in paper). Three-operand pre-trained overall ≈ 0.9%; after fine-tuning on 29 templates (1000 samples each, 2 epochs with Adafactor, lr 1e-5) three-operand accuracy improved to ≈ 39.7%.",
            "error_types_or_failure_modes": "Very poor out-of-the-box three-operand performance (near-zero) prior to task-specific fine-tuning; model requires fine-tuning to develop mid-late MLP activation site for multi-operator computations.",
            "evidence_for_mechanism": "Layerwise IE before fine-tuning shows only last-layer activation at last token; after fine-tuning a mid-late last-token MLP activation site appears (matching patterns seen in two-operand experiments); RI values and heatmaps consistent with mechanism.",
            "counterexamples_or_challenges": "Pre-fine-tuning absence of mid-late MLP activation site indicates mechanism may not be learned for complex multi-step arithmetic without task-specific training; some arithmetic capabilities may be acquired or strengthened via fine-tuning rather than present in pretraining.",
            "uuid": "e8159.1"
        },
        {
            "name_short": "LLaMA-7B",
            "name_full": "LLaMA 7B",
            "brief_description": "A 7-billion-parameter open foundation Transformer model used to replicate the paper's mechanistic observations across model families.",
            "citation_title": "Llama: Open and efficient foundation language models.",
            "mention_or_use": "use",
            "model_name": "LLaMA 7B",
            "model_description": "Decoder-only Transformer foundation model with 7B parameters (Touvron et al., 2023); used here unmodified and also as base for Goat.",
            "arithmetic_task_type": "Two-operand arithmetic (+, -, ×, ÷) evaluated with same templates and settings as other models.",
            "mechanism_or_representation": "Observed similar information flow: early MLPs at operand/operator tokens, attention conveying to last token, and last-token mid-late MLPs contributing to result-related activations (albeit layer indices differ by architecture/size).",
            "probing_or_intervention_method": "Causal mediation activation patching across MLPs and attention outputs; RI metric computed to compare late MLP contribution; comparisons to number retrieval and factual tasks.",
            "performance_metrics": "Reported high arithmetic accuracy in experiments: overall ≈ 97.2% on the two-operand set used in the paper (per-table values), with high per-operator scores (near 100% for some operations on tested templates).",
            "error_types_or_failure_modes": "Differences in tokenization (LLaMA tokenizer treats digits as separate tokens) required result-space restrictions (single-digit results) for fair comparison; errors still present for multi-operator/more complex queries if not fine-tuned.",
            "evidence_for_mechanism": "RI and IE measurements on LLaMA mirror those on GPT-J and Pythia: significant last-token mid-late MLP contributions for arithmetic, and attention modules conveying operand/operator information.",
            "counterexamples_or_challenges": "Tokenizer differences required restricting result sets, complicating direct comparison; despite high overall scores here, generalization to more complex or out-of-distribution arithmetic remains unproven.",
            "uuid": "e8159.2"
        },
        {
            "name_short": "Goat",
            "name_full": "Goat (fine-tuned LLaMA on arithmetic tasks)",
            "brief_description": "A LLaMA-based model fine-tuned specifically on arithmetic tasks, used to assess how task-specific tuning affects arithmetic mechanisms.",
            "citation_title": "Goat: Fine-tuned LLaMA outperforms GPT-4 on arithmetic tasks.",
            "mention_or_use": "use",
            "model_name": "Goat (fine-tuned LLaMA)",
            "model_description": "LLaMA-weight-based model further fine-tuned on arithmetic training data to improve arithmetic accuracy (Liu & Low, 2023).",
            "arithmetic_task_type": "Two-operand arithmetic (+, -, ×, ÷) and evaluated in same experimental framework to compare activation patterns after targeted fine-tuning.",
            "mechanism_or_representation": "Fine-tuning concentrates arithmetic-relevant information into similar activation sites (last-token mid-late MLPs) observed in pre-trained models; fine-tuning can amplify the MLP-based result representation.",
            "probing_or_intervention_method": "Same causal mediation activation patching and IE/RIs as other models to compare pre- and post-fine-tuning activation patterns.",
            "performance_metrics": "Reported overall arithmetic accuracy ≈ 85.6% on the paper's evaluation set (per-table values), with strong increases relative to baseline on some operations.",
            "error_types_or_failure_modes": "Although fine-tuning improves accuracy, some operations (e.g., division) and multi-operator queries can still be error-prone; potential overfitting to template distributions is a concern.",
            "evidence_for_mechanism": "RI and IE measures for Goat match the qualitative pattern: early MLPs/attention route information and late MLPs encode result; magnitudes reflect improved task competence after fine-tuning.",
            "counterexamples_or_challenges": "Fine-tuned arithmetic skills may not generalize beyond the fine-tuning distribution (templates and numeric ranges used); the paper notes tokenizer and result-space restrictions remain relevant.",
            "uuid": "e8159.3"
        },
        {
            "name_short": "Causal Mediation Analysis",
            "name_full": "Causal Mediation Analysis (activation patching intervention)",
            "brief_description": "A causality-grounded experimental framework used here to attribute causal effect of specific model components on arithmetic predictions by intervening on internal activations and measuring output probability changes.",
            "citation_title": "Investigating gender bias in language models using causal mediation analysis",
            "mention_or_use": "use",
            "model_name": "applies to studied models (GPT-J, Pythia, LLaMA, Goat)",
            "model_description": "Not a model but a methodology: treat model components as mediators in a causal graph; interventions overwrite stored activations from a reference input into a target run and measure indirect effect on predicted probabilities.",
            "arithmetic_task_type": "Used to study two- and three-operand arithmetic tasks and to compare to number retrieval and factual tasks within the same causal framework.",
            "mechanism_or_representation": "Enables identification of where in the network result-related vs operand-related information is encoded (e.g., last-token MLPs vs early MLPs/attention).",
            "probing_or_intervention_method": "Activation patching: save m^{(l)}_t and ā^{(l)}_t from run p1, run p2 while replacing selected module outputs with the saved values; compute IE as in Eq. 2 (and alternative log-probability metric) to quantify causal contributions.",
            "performance_metrics": "Not a predictive metric but provides per-component IE scores; computed RI over sets of MLPs to quantify relative contribution (e.g., GPT-J RI(M_late_-1) ≈ 40.2% when results vary).",
            "error_types_or_failure_modes": "Interventions can produce both desired (wrong→correct) and undesired (correct→wrong) prediction flips depending on layer; IE depends on choice of reference and target inputs; coarse module-level interventions may mask head- or neuron-level heterogeneity.",
            "evidence_for_mechanism": "Consistent, reproducible IE heatmaps across multiple architectures showing the same three activation sites (early MLPs at operand/operator tokens, attention at last token, mid-late last-token MLPs) and RI reductions when result is fixed, supporting causal role of last-token MLPs in result computation.",
            "counterexamples_or_challenges": "Activation-level interventions treat full module outputs as units; the paper notes limitation of not analyzing individual attention heads and that module-level effects may hide finer-grained circuit structure. IE magnitude depends on metric choice (probability vs log-prob), though qualitative conclusions are stable.",
            "uuid": "e8159.4"
        },
        {
            "name_short": "Last-token mid-late MLPs (M_late_-1)",
            "name_full": "Last-token mid-to-late-layer MLP modules (M_late_-1 activation site)",
            "brief_description": "A specific subset of MLP modules at the final token in mid-to-late layers identified as the site where result-related information is incorporated into the residual stream for arithmetic predictions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "applies to studied models (observed in GPT-J, Pythia, LLaMA, Goat)",
            "model_description": "MLP output vectors at the final input token, specifically in middle-to-late layers (e.g., layers ~19-20 in GPT-J), that show high indirect effect on arithmetic result probabilities when intervened.",
            "arithmetic_task_type": "Two-operand and three-operand arithmetic where results vary between comparison prompts; also observed to emerge after fine-tuning for three-operand tasks.",
            "mechanism_or_representation": "Represent result-related information (r) as activations inserted into the residual stream at the final token; distinct from early-layer operand representations and from attention-mediated transport.",
            "probing_or_intervention_method": "Module-level activation patching of MLP outputs at the last token; neuron-level interventions within these MLPs (ranking top neurons by IE and computing overlaps across tasks).",
            "performance_metrics": "Quantified by IE values and RI metric: e.g., GPT-J RI(M_late_-1) ≈ 40.2% (result-varying) vs ≈ 4.4% (result-fixed). Across models RI values range ≈ 27.8%–43.2% (when result varies) and drop to single-digit percentages when result is fixed.",
            "error_types_or_failure_modes": "Not all last-token MLPs are equally beneficial: specific layers (19-20) tended to produce desired (wrong→correct) changes, whereas slightly earlier layers (14-17) sometimes produced undesired flips; thus interventions there can worsen predictions.",
            "evidence_for_mechanism": "Strong IE peaks at these MLPs when r differs across prompts; RI drops dramatically when r is held constant, demonstrating these MLPs encode result-specific information. Neuron overlap analysis shows task-specific neuron sets within these MLPs.",
            "counterexamples_or_challenges": "On non-arithmetic numeric tasks (number retrieval) and factual prediction, these last-token MLPs have far lower relative importance (e.g., RI ≈ 8.7% for number retrieval, ≈ 4.2% for factual), indicating the role is task-specific and not a generic numeric-storage location.",
            "uuid": "e8159.5"
        },
        {
            "name_short": "Neuron-level interventions",
            "name_full": "Neuron-level activation patching and ranking",
            "brief_description": "Finer-grained interventions that overwrite individual neuron activations within MLP modules and rank neurons by their indirect effect to identify task-specific neuron sets.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "applies primarily to GPT-J (layer 19 analysis) but conceptually to other models",
            "model_description": "Per-dimension interventions on the output vector of an MLP layer (each neuron = dimension) at a specific token/layer; used to compute neuron IE and rank neurons by average effect.",
            "arithmetic_task_type": "Applied to two-operand arithmetic (Arabic and numeral words), number retrieval, and factual prediction tasks for comparison.",
            "mechanism_or_representation": "Identifies neurons within a last-token MLP that are most causally responsible for shifting probability mass toward particular arithmetic results; reveals partially overlapping but mostly distinct neuron sets across tasks.",
            "probing_or_intervention_method": "For each neuron, set activation to value from reference run and compute IE; rank neurons by average IE and compute overlap of top-k (top 400) neurons across tasks.",
            "performance_metrics": "Overlap statistics used as evidence: ~50% overlap between Arabic and word-based arithmetic top-400 neurons; ~22-23% overlap between arithmetic and number retrieval; ~9-10% overlap with factual — near random expectation (~9.8%).",
            "error_types_or_failure_modes": "Neuron-level ranking ignores effect magnitude heterogeneity and downstream nonlinear interactions; overlapping top neurons does not imply identical causal roles; measuring only top indices can miss distributed contributions.",
            "evidence_for_mechanism": "Overlap patterns support that arithmetic representations (even across numeral encodings) share significant neuronal substrates, while number retrieval and factual tasks use largely distinct neurons within the same MLP layers.",
            "counterexamples_or_challenges": "Top-neuron overlaps with number retrieval and factual tasks are low, but this does not rule out some shared subspace or distributed encoding; moreover, interventions on single neurons may have small effects due to redundancy.",
            "uuid": "e8159.6"
        },
        {
            "name_short": "Relative Importance (RI)",
            "name_full": "Relative Importance metric for MLP subsets",
            "brief_description": "A quantitative metric defined in this paper to measure the relative contribution of a subset of MLP modules (e.g., mid-late last-token MLPs) to the overall IE across all MLPs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "applies to studied models (GPT-J, Pythia, LLaMA, Goat)",
            "model_description": "RI(M*) = sum_{m in M*} log(IE(m)+1) / sum_{m in M} log(IE(m)+1), used to quantify fraction of total IE attributable to subset M*.",
            "arithmetic_task_type": "Used to quantify importance of last-token mid-late MLPs for two-operand and three-operand arithmetic, and to compare with number retrieval and factual tasks.",
            "mechanism_or_representation": "Operationalizes contribution of identified activation sites (e.g., M_late_-1) to result computation relative to the whole model's MLPs.",
            "probing_or_intervention_method": "Calculated from IE measurements obtained via activation patching across MLP modules.",
            "performance_metrics": "Example values: GPT-J RI(M_late_-1) ≈ 40.2% (result-varying) vs 4.4% (result-fixed); Pythia 2.8B ≈ 43.2% vs 5.8%; LLaMA 7B ≈ 36.1% vs 7.5%; Goat ≈ 33.5% vs 7.4%. After fine-tuning Pythia on 3-operand tasks RI increased (pre/post fine-tune reported).",
            "error_types_or_failure_modes": "RI depends on IE estimates which in turn depend on sampled prompt pairs and metric choice; small IE values and log transform stabilize but choice of M* can affect interpretation.",
            "evidence_for_mechanism": "Large RI values for last-token mid-late MLPs when results vary and the dramatic RI drop when results are fixed provide quantitative evidence these MLPs specifically encode result information.",
            "counterexamples_or_challenges": "RI is an aggregate statistic and may conceal that only a few neurons or heads within M* are doing the computation; it does not identify head-level contributors.",
            "uuid": "e8159.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Locating and editing factual associations in GPT",
            "rating": 2,
            "sanitized_title": "locating_and_editing_factual_associations_in_gpt"
        },
        {
            "paper_title": "Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space",
            "rating": 2,
            "sanitized_title": "transformer_feedforward_layers_build_predictions_by_promoting_concepts_in_the_vocabulary_space"
        },
        {
            "paper_title": "A mathematical framework for transformer circuits",
            "rating": 2,
            "sanitized_title": "a_mathematical_framework_for_transformer_circuits"
        },
        {
            "paper_title": "Investigating gender bias in language models using causal mediation analysis",
            "rating": 2,
            "sanitized_title": "investigating_gender_bias_in_language_models_using_causal_mediation_analysis"
        },
        {
            "paper_title": "Goat: Fine-tuned LLaMA outperforms GPT-4 on arithmetic tasks",
            "rating": 2,
            "sanitized_title": "goat_finetuned_llama_outperforms_gpt4_on_arithmetic_tasks"
        },
        {
            "paper_title": "Pythia: A suite for analyzing large language models across training and scaling.",
            "rating": 2,
            "sanitized_title": "pythia_a_suite_for_analyzing_large_language_models_across_training_and_scaling"
        }
    ],
    "cost": 0.0173545,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis
20 Oct 2023</p>
<p>Alessandro Stolfo stolfoa@ethz.ch 
ETH Zürich</p>
<p>Yonatan Belinkov belinkov@technion.ac.il 
Technion -IIT
Israel</p>
<p>Mrinmaya Sachan msachan@ethz.ch 
ETH Zürich</p>
<p>A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis
20 Oct 2023F3EE1A02DBD30485E3EBD6B975383EFFarXiv:2305.15054v2[cs.CL]
Mathematical reasoning in large language models (LMs) has garnered significant attention in recent work, but there is a limited understanding of how these models process and store information related to arithmetic tasks within their architecture.In order to improve our understanding of this aspect of language models, we present a mechanistic interpretation of Transformer-based LMs on arithmetic questions using a causal mediation analysis framework.By intervening on the activations of specific model components and measuring the resulting changes in predicted probabilities, we identify the subset of parameters responsible for specific predictions.This provides insights into how information related to arithmetic is processed by LMs.Our experimental results indicate that LMs process the input by transmitting the information relevant to the query from mid-sequence early layers to the final token using the attention mechanism.Then, this information is processed by a set of MLP modules, which generate result-related information that is incorporated into the residual stream.To assess the specificity of the observed activation dynamics, we compare the effects of different model components on arithmetic queries with other tasks, including number retrieval from prompts and factual knowledge questions. 1</p>
<p>Introduction</p>
<p>Mathematical reasoning with Transformer-based models (Vaswani et al., 2017) is challenging as it requires an understanding of the quantities and the mathematical concepts involved.While large language models (LMs) have recently achieved impressive performance on a set of math-based tasks (Wei et al., 2022a;Chowdhery et al., 2022;OpenAI, 2023), their behavior has been shown to be inconsistent and context-dependent (Bubeck et al., 2023).Recent literature shows a multitude 1 Our code and data is available at https://github.com/alestolfo/lm-arithmetic.We trace the flow of numerical information within Transformerbased LMs: given an input query, the model processes the representations of numbers and operators with early layers (A).Then, the relevant information is conveyed by the attention mechanism to the end of the input sequence (B).Here, it is processed by late MLP modules, which output result-related information into the residual stream (C).</p>
<p>of works proposing methods to improve the performance of large LMs on math benchmark datasets through enhanced pre-training (Spokoyny et al., 2022;Lewkowycz et al., 2022;Liu and Low, 2023) or specific prompting techniques (Wei et al., 2022b;Kojima et al., 2022;Yang et al., 2023, inter alia).</p>
<p>However, there is a limited understanding of the inner workings of these models and how they store and process information to correctly perform mathbased tasks.Insights into the mechanics behind LMs' reasoning are key to improvements such as inference-time correction of the model's behavior (Li et al., 2023) and safer deployment.Therefore, research in this direction is critical for the development of more faithful and accurate next-generation LM-based reasoning systems.</p>
<p>In this paper, we present a set of analyses aimed at mechanistically interpreting LMs on the task of answering simple arithmetic questions (e.g., "What is the product of 11 and 17?").In particular, we hypothesize that the computations involved in reasoning about such arithmetic problems are carried out by a specific subset of the network.Then, we test this hypothesis by adopting a causal mediation analysis framework (Vig et al., 2020;Meng et al., 2022), where the model is seen as a causal graph going from inputs to outputs, and the model components (e.g., neurons or layers) are seen as mediators (Pearl, 2001).Within this framework, we assess the impact of a mediator on the observed output behavior by conducting controlled interventions on the activations of specific subsets of the model and examining the resulting changes in the probabilities assigned to different numerical predictions.Through this experimental procedure, we track the flow of information within the model and identify the model components that encode information about the result of arithmetic queries.Our findings show that the model processes the input by conveying information about the operator and the operands from mid-sequence early layers to the final token using attention.At this location, the information is processed by a set of MLP modules, which output result-related information into the residual stream (shown in Figure 1).We verify this finding for biand tri-variate arithmetic queries across four pretrained language models with different sizes: 2.8B, 6B, and 7B parameters.Finally, we compare the effect of different model components on answering arithmetic questions to two additional tasks: a synthetic task that involves retrieving a number from the prompt and answering questions related to factual knowledge.This comparison validates the specificity of the activation dynamics observed on arithmetic queries.</p>
<p>Related Work</p>
<p>Mechanistic Interpretability.The objective of mechanistic interpretability is to reverse engineer model computation into components, aiming to discover, comprehend, and validate the algorithms (called circuits in certain works) implemented by the model weights (Räuker et al., 2023).Early work in this area analyzed the activation values of single neurons when generating text using LSTMs (Karpathy et al., 2015).A multitude of studies have later focused on interpreting weights and intermediate representations in neural networks (Olah et al., 2017(Olah et al., , 2018(Olah et al., , 2020;;Voss et al., 2021;Goh et al., 2021) and on how information is processed by Transformer-based (Vaswani et al., 2017) language models (Geva et al., 2021(Geva et al., , 2022(Geva et al., , 2023;;Olsson et al., 2022;Nanda et al., 2023).Although not strictly mechanistic, other recent studies have analyzed the hidden representations and behavior of inner components of large LMs (Belrose et al., 2023;Gurnee et al., 2023;Bills et al., 2023).</p>
<p>Causality-based Interpretability.Causal mediation analysis is an important tool that is used to effectively attribute the causal effect of mediators on an outcome variable (Pearl, 2001).This paradigm was applied to investigate LMs by Vig et al. (2020), who proposed a framework based on causal mediation analysis to investigate gender bias.Variants of this approach were later applied to mechanistically interpret the inner workings of pre-trained LMs on other tasks such as subject-verb agreement (Finlayson et al., 2021), natural language inference (Geiger et al., 2021), indirect object identification (Wang et al., 2022), and to study their retention of factual knowledge (Meng et al., 2022).</p>
<p>Math and Arithmetic Reasoning.A growing body of work has proposed methods to analyze the performance and robustness of large LMs on tasks involving mathematical reasoning (Pal and Baral, 2021;Piękos et al., 2021;Razeghi et al., 2022;Cobbe et al., 2021;Mishra et al., 2022).In this area, Stolfo et al. (2023) use a causally-grounded approach to quantify the robustness of large LMs.However, the proposed formulation is limited to behavioral investigation with no insights into the models' inner mechanisms.To the best of our knowledge, our study represents the first attempt to connect the area of mechanistic interpretability to the investigation of the mathematical reasoning abilities in Transformer-based LMs.</p>
<p>Methodology</p>
<p>Background and Task</p>
<p>We denote an autoregressive language model as G : X → P. The model operates over a vocabulary V and takes a token sequence x = [x 1 , ..., x T ] ∈ X , where each x i ∈ V .G generates a probability distribution P ∈ P : R |V | → [0, 1] that predicts possible next tokens following the sequence x.In this work, we study decoder-only Transformer-based models (Vaswani et al., 2017).Specifically, we focus on models that represent a slight variation of the standard GPT paradigm, as they utilize parallel attention (Wang and Komatsuzaki, 2021) and rotary positional encodings (Su et al., 2022).The internal computation of the model's hidden states h (l) t at position t ∈ {1, . . ., T } of the input sequence is carried out as follows:
h (l) t = h (l−1) t + a (l) t + m (l) t (1) a (l) t = A (l) h (l−1) 1 , . . . , h (l−1) t m (l) t = W (l) proj σ W (l) f c h (l−1) t =: MLP (l) (h (l−1) t ),
where at layer l, σ is the sigmoid nonlinearity, W (l) f c and W (l) proj are two matrices that parameterize the multilayer perceptron (MLP) of the Transformer block and A (l) is the attention mechanism. 2e consider the task of computing the result of arithmetic operations.Each arithmetic query consists of a list of operands N = (n 1 , n 2 , . . . ) and a function f O representing the application of a set of arithmetic operators (+, −, ×, ÷).We denote as r = f O (N ) the result obtained by applying the operators to the operands.Each query is rendered as a natural language question through a prompt p(N, f O ) ∈ X such as "How much is n 1 plus n 2 ?" (in this case, f O (n 1 , n 2 ) = n 1 + n 2 ).The prompt is then fed to the language model to produce a probability distribution P over V .Our aim is to investigate whether certain hidden state variables are more important than others during the process of computing the result r.</p>
<p>Experimental Procedure</p>
<p>We see the model G as a causal graph (Pearl, 2009), framing internal model components, such as specific neurons, as mediators positioned along the causal path connecting model inputs and outputs.Following a causal mediation analysis procedure, we then quantify the contribution of particular model components by intervening on their activation values and measuring the change in the model's output.Previous work has isolated the effect of every single neuron within a model (Vig et al., 2020;Finlayson et al., 2021).However, this approach becomes impractical for models with billions of parameters.Therefore, for our main experiments, the elements that we consider as variables along the causal path described by the model are the outputs of the MLP (l) and A (l) functions at each token t, i.e., m</p>
<p>t and a (l) t .To quantify the importance of modules MLP (l)  and A (l) in mediating the model's predictions at position t, we use the following procedure.</p>
<ol>
<li>
<p>Given f O , we sample two sets of operands N , N ′ , and we obtain r = f O (N ) and r ′ = f O (N ′ ).Then, two input questions with only the operands differing, p 1 = p(N, f O ) and p 2 = p(N ′ , f O ), are passed through the model.</p>
</li>
<li>
<p>During the forward pass with input p 1 , we store the activation values m(l) t := MLP (l) (h
(l−1) t
), and ā(l) t := A (l) (h
(l−1) 1 , . . . , h (l−1) t
) .</p>
</li>
<li>
<p>We perform an additional forward pass using p 2 , but this time we intervene on components MLP (l) and A (l) at position t, setting their activation values to m(l) t , and ā(l) t , respectively.This process is illustrated in Figure 2.More specifically, we compute the indirect effect (IE) of a specific mediating component by quantifying its contribution in skewing P towards the correct result.Consider a generic activation variable z ∈ {m
(1) 1 , . . . , m (L) t , a (1) 1 , . . . , a (L)
t }.We denote the model's output probability following an intervention on z as P * z .Then, we compute the IE as:
IE(z) = 1 2 P * z (r) − P(r) P(r) + P(r ′ ) − P * z (r ′ ) P * z (r ′ )(2)
where the two terms in the sum represent the relative change in the probability assigned by the model to r and r ′ , caused by the intervention performed.</p>
</li>
</ol>
<p>The larger the measured IE, the larger the contribution of component z in shifting probability mass from the clean-run result r ′ to result r corresponding to the alternative input p 1 . 3e additionally measure the mediation effect of each component with respect to the operation f O .We achieve this by fixing the operands and changing the operator across the two input prompts.More formally, in step 1, we sample a list of operands N and two operators f O and f ′ O .Then, we generate two prompts
p 1 = p(N, f O ) and p 2 = p(N, f ′ O ) (e.g., "
What is the sum of 11 and 7?" and "What is the product of 11 and 7?").Finally, we carry out the procedure in steps 2-4.</p>
<p>Experimental Setup</p>
<p>We present the results of our analyses in the main paper for GPT-J (Wang and Komatsuzaki, 2021), a 6B-parameter pre-trained LM (Gao et al., 2020).Additionally, we validate our findings on Pythia 2.8B (Biderman et al., 2023), LLaMA 7B (Touvron et al., 2023), and Goat, a version of LLaMA finetuned on arithmetic tasks (Liu and Low, 2023).We report the detailed results for these models in Appendix J.</p>
<p>In our experiments, we focus on two-and threeoperand arithmetic problems.Similar to previous work (Razeghi et al., 2022;Karpas et al., 2022), for single-operator two-operand queries, we use a set of six diverse templates representing a question involving each of the four arithmetic operators.For the three-operand queries, we use one template for each of the 29 possible two-operator combinations.Details about the templates are reported in Appendix A. In the bi-variate case, for each of the four operators f O ∈ {+, −, ×, ÷} and for each of the templates, we generate 50 pairs of prompts by sampling two pairs of operands (n 1 , n 2 ) ∈ S 2 and
(n ′ 1 , n ′ 2 ) ∈ S 2
, where S ⊂ V ∩ N.For the operandrelated experiment, we sample (n 1 , n 2 ) and a second operation f ′ O .In both cases, we ensure that the result r falls within S. 4 In the three-operand case, we generate 15 pairs of prompts for each of the 29 templates, following the same procedure.In order to ensure that the model achieves a meaningful task performance, we use a two-shot prompt in which we include two exemplars of question-answer for the same operation that is being queried.We report the accuracy results in Appendix B.</p>
<p>Causal Effects on Arithmetic Queries</p>
<p>Our analyses address the following question:</p>
<p>Q1 What are the components of the model that mediate predictions involving arithmetic computations?</p>
<p>We address this question by first studying the flow of information throughout the model by measuring the effect of each component (MLP and attention block) at each point of the input sequence for two-operand queries ( §4.1).Then, we distinguish between model components that carry information about the result and about the operands of the arithmetic computations ( §4.2 and §4.3).Finally, we consider queries involving three operands ( §4.4) and present a measure to quantify the changes in information flow ( §4.5).</p>
<p>Tracing the Information Flow</p>
<p>We measure the indirect effect of each MLP and attention block at different positions along the input sequence.The output of these modules can be seen as new information being incorporated into the residual stream.This new information can be produced at any point of the sequence and then conveyed to the end of the sequence for the prediction of the next token.By studying the IE at different locations within the model, we can identify the modules that generate new information relevant to the model's prediction.The results are reported in Figures 3a and 3b for MLP and attention, respectively.</p>
<p>(e) (f) Our analysis reveals four primary activation sites: the MLP module situated at the first layer corresponding to the tokens of the two operands; the intermediate attention blocks at the last token of the sequence; and the MLP modules in the middleto-late layers, also located at the last token of the sequence.It is expected to observe a high effect for the first MLPs associated with the tokens that vary (i.e., the operands), as such modules are likely to affect the representation of the tokens, which is subsequently used for the next token prediction.On the other hand, of particular interest is the high effect detected at the attention modules in layers 11-18 and in the MLPs around layer 20.
(g) (h) (a) (b) (c) (d)
As for the flow of information tied to the operator, the activations display a parallel pattern: high effect is registered at early MLPs associated with the operator tokens and at the same last-token MLP and attention locations.We report the visualization of the operator-related results in Appendix C.</p>
<p>A possible explanation of the model's behavior on this task is that the attention mechanism facilitates the propagation of operand-and operatorrelated information from the first layers early in the sequence to the last token.Here, this information is processed by the MLP modules, which incorporate the information about the result of the computation in the residual stream.This hypothesis aligns with the existing theory that attributes the responsibility of moving and extracting infor-mation within Transformer-based models to the attention mechanism (Elhage et al., 2021;Geva et al., 2023), while the feed-forward layers are associated with performing computations, retrieving facts and information (Geva et al., 2022;Din et al., 2023;Meng et al., 2022).We test the validity of this hypothesis in the following section.</p>
<p>Operand-and Result-related Effects</p>
<p>Our objective is to verify whether the contribution to the model's prediction of each component measured in Figures 3a and 3b is due to (1) the component representing information related to the operands, or (2) the component encoding information about the result of the computation.To this end, we formulate a variant of our previous experimental procedure.In particular, we condition the sampling of the second pair of operands (n ′ 1 , n ′ 2 ) on the constraint r = r ′ .That is, we generate the two input questions p 1 and p 2 , such that their result is the same (e.g., "What is the sum of 25 and 7?" and "What is the sum of 14 and 18?").In case number (1), we would expect a component to have high IE both in the result-varying setting and when r = r ′ , as the operands are modified in both scenarios.In case (2), we expect a subset of the model to have a large effect when the operands are sampled without constraints but a low effect for the fixed-result setting.</p>
<p>We report the results in Figure 3c and 3d.By comparing Figures 3a and 3c, two notable observations emerge.First, the high effect in the early layers corresponding to the operand tokens is observed in both the result-preserving and the resultvarying scenarios.Second, the last-token mid-late MLPs that lead to a high effect on the model's prediction following a result change, dramatically decrease their effect on the model's output in the result-preserving setting, as described in scenario (2).These observations point to the conclusion that the MLP blocks around layer 20 incorporate resultrelevant information.As for the contribution of the attention mechanism (Figures 3b and 3d), we do not observe a substantial difference in the layers with the highest IE between the two settings, which aligns this scenario to the description of case (1).These results are consistent with our hypothesis that operand-related information is transferred by the attention mechanism to the end of the sequence and then processed by the MLPs to obtain the result of the computation.</p>
<p>Zooming in on the Last Token</p>
<p>In Figures 3e-3h, we show a re-scaled version of the IE measurements for the layers at the end of the input sequence.While the large difference in magnitude was already evident in the previously considered visualizations, in Figures 3e and 3f we notice that the MLPs with the highest effect in the two settings differ: the main contribution to the model's output when the results are not fixed is given by layers 19 and 20, while in the resultpreserving setting the effect is largest at layers 14-18.For the attention (Figures 3g and 3h), we do not observe a significant change in the shape of the curve describing the IE across different layers, with layer 13 producing the largest contribution.</p>
<p>We interpret this as additional evidence indicating that the last-token MLPs at layers 19-20 encode information about r, while the attention modules carry information related to the operands.</p>
<p>Three-operand Queries &amp; Fine-tuning</p>
<p>We extend our analyses by including three-operand arithmetic queries such as "What is the difference between n 1 and the ratio between n 2 and n 3 ?". Answering correctly this type of questions represents a challenging task for pre-trained language models, and we observe poor accuracy (below 10%) with GPT-J.Thus, we opt for fine-tuning the model on a small set of three-operand queries.The model that we consider for this analysis is Pythia 2.8B, as its smaller size allows for less computationally demanding training than the 6B-parameter GPT-J.</p>
<p>After fine-tuning, the model attains an accuracy of ∼40%.We provide the details about the training procedure in Appendix F. We carry out the experimental procedure as in Section 4.1.In particular, we compare the information flow in the MLPs of the model before and after fine-tuning (Figure 4).In the non-fine-tuned version of the model, the only relevant activation site, besides the early layers at the operand tokens, is the very last layer at the last token.In the fine-tuned model, on the other hand, we notice the emergence of the mid-late MLP activation site that was previously observed in the two-operand setting.</p>
<p>Quantifying the Change of the Information Flow</p>
<p>Denote the set of MLPs in the model by M. We define the relative importance (RI) of a specific subset M * ⊆ M of MLP modules as
RI(M * ) = m∈M * log(IE(m) + 1) m∈M log(IE(m) + 1)
.</p>
<p>In order to quantitatively show the difference in the activation sites observed in Figure 3, we compute the RI measure for the set where the subscript −1 indicates the last token of the input sequence and L is the number of layers in the model.This quantity represents the relative contribution of the mid-late last-token MLPs compared to all the MLP blocks in the model.For the two-operand setting, we carry out the experimental procedure described in Section 3 for three additional models: Pythia 2.8B, LLaMA 7B, and Goat.5 Furthermore, we repeat the analyses on GPT-J using a different number representation: instead of Arabic numbers (e.g., the token 2), we represent quantities using numeral words (e.g., the token two).For the three-operand setting, we report the results for Pythia 2.8B before and after fine-tuning.We measure the effects using both randomly sampled and result-preserving operand pairs, comparing the RI measure in the two settings.The results (Table 1) exhibit consistency across all these four additional experiments.These quantitative measurements further highlight the influence of last-token late MLP modules on the prediction of r.We provide in Appendix J the heatmap illustrations of the effects for these additional studies.
M late −1 = {m (⌊L/2⌋) −1 , m (⌊L/2⌋+1) −1 , . . . , m (L) −1 },</p>
<p>Causal Effects on Different Tasks</p>
<p>In order to understand whether the patterns in the effect of the model components that we observed so far are specific to arithmetic queries, we compare our observations on arithmetic queries to two different tasks: the retrieval of a number from the prompt ( §5.1), and the prediction of factual knowl- Q2 Are the activation patterns observed so far specific to the arithmetic setting?</p>
<p>Information Flow on Number Retrieval</p>
<p>We consider a simple synthetic task involving numerical predictions.We construct a set of templates of the form "Paul has n 1 e 1 and n 2 e 2 .How many e q does Paul have?", where n 1 , n 2 are two randomly sampled numbers, e 1 and e 2 are two entity names sampled at random,6 and e q ∈ {e 1 , e 2 }.In this case, the two input prompts p 1 and p 2 differ solely in the value of e q .To provide the correct answer to a query, the model has simply to retrieve the correct number from the prompt.With this task, we aim to analyze the model's behavior in a setting involving numerical predictions but not requiring any kind of arithmetic computation.We report the indirect effect measured for the MLPs modules of GPT-J in Figure 5.In this setting, we observe an unsurprising high-effect activation site corresponding to the tokens of the entity e q and a lower-effect site at the end of the input in layers 14-20.The latter site appears in the set of the model components that were shown to be active on arithmetic queries.However, computing the relative importance of the late MLPs on this task shows that this second activation site is responsible for only RI(M late −1 ) = 8.7% of the overall log IE.The low RI, compared to the higher values measured on arithmetic queries, suggests that the function of the last-token late MLPs is not dictated by the numerical type of prediction, but rather by their involvement in processing the input information.This finding is aligned with our theory that sees M late −1 as the location where information about r is included in the residual stream.</p>
<p>Information Flow on Factual Predictions</p>
<p>We carry out our experimental procedure using data from the LAMA benchmark (Petroni et al., 2019), which consists of natural language templates representing knowledge-base relations, such as "[subject] is the capital of [object]".By instantiating a template with a specific subject (e.g., "Paris"), we prompt the model to predict the correct object ("France").Similar to our approach with arithmetic questions, we create pairs of factual queries that differ solely in the subject.In particular, we sample pairs of entities from the set of entities compatible for a given relation (e.g., cities for the relation "is the capital of").Details about the data used for this procedure are provided in Appendix H.We then measure the indirect effect following the formulation in Equation 2, where the correct object corresponds to the correct numerical outcome in the arithmetic scenario.</p>
<p>From the results (Figure 6), we notice that a main activation site emerges in early layers at the tokens corresponding to the subject of the query.These findings are consistent with previous works (Meng et al., 2022;Geva et al., 2023), which hypothesize that language models store and retrieve factual associations in early MLPs located at the subject tokens.We compute the RI metric for the late MLP modules, which quantitatively validates the contribution of the early MLP activation site by attaining a low value of RI(M late −1 ) = 4.2%.The large IE observed at mid-sequence early MLPs represents a difference in the information flow with respect to the arithmetic scenario, where the modules with the highest influence on the model's prediction are located at the end of the sequence.This difference serves as additional evidence highlighting the specificity of the model's activation patterns when answering arithmetic queries.</p>
<p>Neuron-level Interventions</p>
<p>The experimental results in Sections 5.1 and 5.2 showed a quantitative difference in the contributions of last-token mid-late MLPs between arithmetic queries and two tasks that do not involve arithmetic computation.Now, we investigate whether the components active within M late −1 on the different types of tasks are different.We carry out a finer-grained analysis in which we consider independently each neuron in an MLP module (i.e., each dimension in the output vector of the function MLP (l) ) at a specific layer l.In particular, following the same procedure as for layer-level experiments, we intervene on each neuron by setting its activation to the value it would take if the input query contained different operands (or a different entity).We then compute the corresponding indirect effect as in Eq. 2. We carry out this procedure for arithmetic queries using Arabic numerals (Ar) and numeral words (W), for the number retrieval task (NR), and for factual knowledge queries (F). 7e rank the neurons according to the average effect measured for each of these four settings and compute the overlap in the top 400 neurons (roughly 10%, as GPT-J has a hidden dimension of 4096).</p>
<p>We carry out this procedure for layer l = 19, as it exhibits the largest IE within M late −1 on all the tasks considered.The heatmap in Figure 7 illustrates the results.We observe a consistent overlap (50%) between the top neurons active for the arithmetic queries using Arabic and word-based representa-tions.Interestingly, the size of the neuron overlap between arithmetic queries and number retrieval is considerably lower (22% and 23%), even though both tasks involve the prediction of numerical quantities.Finally, the overlaps between the top neurons for the arithmetic operations and the factual predictions (between 9% and 10%) are not larger than for random rankings: the expected overlap ratio between the top 400 indices in two random rankings of size 4096 is 9.8% (Antverg and Belinkov, 2022).These results support the hypothesis that the model's circuits responsible for different kinds of prediction, though possibly relying on similar subsets of layers, are distinct.However, it is important to note that this measurement does not take into account the magnitude of the effect.</p>
<p>Conclusion</p>
<p>We proposed the use of causal mediation analysis to mechanistically investigate how LMs process information related to arithmetic.Through controlled interventions on specific subsets of the model, we assessed the impact of these mediators on the model's predictions.</p>
<p>We posited that models produce predictions to arithmetic queries by conveying the math-relevant information from the mid-sequence early layers to the last token, where this information is then processed by late MLP modules.We carried out a causality-grounded experimental procedure on four different Transformer-based LMs, and we provided empirical evidence supporting our hypothesis.Furthermore, we showed that the information flow we observed in our experiments is specific to arithmetic queries, compared to two other tasks that do not involve arithmetic computation.</p>
<p>Our findings suggest potential avenues for research into model pruning and more targeted training/fine-tuning by concentrating on specific model components associated with certain queries or computations.Moreover, our results offer insights that may guide further studies into using LMs' hidden representations to correct the model's behavior on math-based tasks at inference time (Li et al., 2023) and to estimate the probability of the model's predictions to be true (Burns et al., 2023).</p>
<p>Limitations</p>
<p>The scope of our work is investigating arithmetic reasoning and we experiment with the four fundamental arithmetic operators.Addition, subtraction, multiplication, and division form the cornerstone of arithmetic calculations and serve as the basis for a wide range of mathematical computations.Thus, exploring their mechanisms in language models provides a starting point to explore more complex forms of mathematical processing.Studying a broader set of mathematical operators represents an interesting avenue for further investigation.</p>
<p>Our work focuses on synthetically-generated queries that are derived from natural language descriptions of the four basic arithmetic operators.To broaden the scope, future research can expand the analysis of model activations to encompass mathbased queries described in real-life settings, such as math word problems.</p>
<p>Finally, a limitation of our work concerns the analysis of different attention heads.In our experiments, we consider the output of an attention module as a whole.Future research could focus on identifying the specific heads that are responsible for forwarding particular types of information in order to offer a more detailed understanding of their individual contributions.</p>
<p>A Prompt Templates</p>
<p>In Tables 2 and 3, we report the question templates from Karpas et al. (2022), which we used as prompts for the model for two-and three-operand queries, respectively.For three-operand queries, we use one query template for each of the 29 possible two-operation combinations.</p>
<p>B Performance of the Models</p>
<p>In Table 4, we report the accuracy of the models on the arithmetic queries that we use for our analyses.The higher accuracy obtained using numeral words is likely given by the smaller set of possible solutions considered (we used S = {"one", "two", . . ., "twenty"}, as the numeral words corresponding to larger numbers get split into multiple tokens by the tokenizer).The accuracy of GPT-J on the factual queries from the LAMA benchmark is 65.0% (we constrain the vocabulary to the set of all possible objects for all the relations considered).On the synthetic number retrieval task, GPT-J's accuracy is 86.7%.</p>
<p>C Flow of Operator-related Information</p>
<p>The measurements of the indirect effect for each model component when fixing the operand and varying the operator in the two input prompts p 1 and p 2 reveal how the model processes the information related to the operator.We report in Figure 8 the heatmap visualizations of these results for twooperand queries.Similar to the operand-related information, we observe a high effect in three activation locations: early MLP blocks corresponding to the operand tokens; middle-to-early attention modules at the last token; and middle-to-late MLP modules at the last token.These results align with our hypothesis that arithmetic-related information is transferred to the end of the sequence by the attention mechanism, where it is then processed by late MLP layers.In this setting, we measure RI(M late −1 ) =31.4%.</p>
<p>D Effects for Each Operator</p>
<p>For each of the four operators, we report the indirect effect measured for the last-token MLP modules in GPT-J in Figure 9.The results for each of the four operators show a common spike in the effect at layers 19-20.This indicates the presence of a specific part of the model relevant to the numerical predictions of the bi-variate arithmetic questions, irrespective of the operator involved.We also notice a difference in the magnitude of the effects, which is linked to the capability of the model to correctly answer the query.</p>
<p>E Changes in the Model's Prediction</p>
<p>We measured the influence of the model components in terms of probability changes.Now, we study the dynamics of the actual model predictions.</p>
<p>In particular, considering the scenario in which r = r ′ , we verify whether the intervention leads to a change in the model's prediction.That is, we What is the difference between n1 and the product of n2 and n3? n1*(n2-n3)</p>
<p>How much is n1 times the difference between n2 and n3?The results reported in Figure 10 show an increase in the desired change in prediction at layers 19-20, while the undesired change in prediction is higher for layers 14-17.This means that interventions on the MLPs at layers 19-20 are more likely to lead to a correct adjustment of the prediction, while the opposite is true for earlier layers (14-15 in particular).This finding is consistent with our previous observations and we see this as additional evidence highlighting the influence of the MLPs at layers 19-20 on the prediction of r.</p>
<p>F Fine-tuning Details</p>
<p>We fine-tune Pythia 2.8B on three-operand queries.We train the model for 2 epochs on a set of queries obtained by sampling 1000 triples of operands for each of the 29 templates.We use Adafactor (Shazeer and Stern, 2018) a learning rate of 10 −5 , linearly decaying, and a batch size of 8. We make sure that there is no overlap between the set of</p>
<p>G Computing Infrastructure</p>
<p>The experiments for all models are carried out using a single Nvidia A100 GPU with 80GB of memory.</p>
<p>Figure 1 :
1
Figure1: Visualization of our findings.We trace the flow of numerical information within Transformerbased LMs: given an input query, the model processes the representations of numbers and operators with early layers (A).Then, the relevant information is conveyed by the attention mechanism to the end of the input sequence (B).Here, it is processed by late MLP modules, which output result-related information into the residual stream (C).</p>
<p>Figure 2 :
2
Figure2: By intervening on the activation values of specific components within a language model and computing the corresponding effects, we identify the subset of parameters responsible for specific predictions.</p>
<ol>
<li>We measure the causal effect of the intervention on variables m (l) t and a (l) t on the model's prediction by computing the change in the probability values assigned to the results r and r ′ .</li>
</ol>
<p>Figure 3 :
3
Figure 3: Indirect effect (IE) measured within GPT-J.Figures (a) and (b) illustrate the flow of information related to both the operands and the result of the queries, while the effect displayed in Figures (c) and (d) is related to the operands only (the result is kept unchanged).Figures(e-h) show a re-scaled visualization of the effects at the last token for each of the four heatmaps (a-d).The difference in the effect registered for the MLPs at layers 15-25 between figures (a) and (c) illustrates the role of these components in producing result-related information.</p>
<p>Figure 4 :
4
Figure 4: Indirect effect (IE) on three-operand queries for different MLP modules in Pythia 2.8B before and after fine-tuning.The effect produced by the last-token mid-late MLP activation site emerges with fine-tuning.Results for the attention are reported in Appendix J.</p>
<p>Figure 5 :
5
Figure 5: Indirect effect measured on the MLPs of GPT-J for predictions on the number retrieval task.</p>
<p>Figure 6 :
6
Figure 6: Indirect effect measured on the MLPs of GPT-J for predictions to factual queries.</p>
<p>Figure 7 :
7
Figure 7: Overlap ratio in the top 400 neurons with the largest effect on predicting answers to factual queries involving Arabic Numerals (Ar) and numeral words (W), number retrieval (NR), and factual knowledge (F).The results are obtained with GPT-J.</p>
<p>Figure 8 :
8
Figure 8: effect (IE) measured in GPT-J when varying the word describing the operator involved in the input query.Similar to the operands case, we observe a high contribution produced by middle-to-late MLP modules at the end of the input sequence.</p>
<p>Figure 9 :
9
Figure9: Indirect effect of the MLPs at the last token in each layer in GPT-J, for each of the four arithmetic operators.We observe a peak in the effect at layer 19 for all four types of operation.</p>
<p>x∈S P * (x) = r) and undesired (arg max x∈S P(x) = r) changes.</p>
<p>Figure 10 :Figure 11 :
1011
Figure 10: Desired (wrong to correct) and undesired (correct to wrong) change in the prediction induced by the intervention on the MLPs in GPT-J.The layers at which the two types of prediction change peak correspond to the layers with the largest corresponding IE.</p>
<p>Figure 12 :Figure 15 :Figure 16 :Figure 17 :Figure 18 :Figure 19 :
121516171819
Figure 12: Indirect effect (IE) measured for the attention modules in GPT-J on factual knowledge queries.</p>
<p>Table 1 :
1
Relative importance (RI) measurements for the last-token late MLP activation site.The decrease in the RI observed when fixing the result of the two pairs of operands used for the interventions quantitatively confirms the role of this subset of the model in incorporating result-related information.
|N | ModelRI(M late −1 )RI(M late −1 ) Result FixedGPT-J40.2%4.4%Pythia 2.8B43.2%5.8%2LLaMA 7B36.1%7.5%Goat33.5%7.4%GPT-J (Words)27.8%4.5%3Pythia 2.8B Pythia 2.8B (FT)13.5% 24.7%6.7% 13.6%</p>
<p>Table 2 :
2
Question templates for two-operand arithmetic queries.
Typeadditionsubtraction1Q: How much is n1 plus n2? A:Q: How much is n1 minus n2? A:2Q: What is n1 plus n2? A:Q: What is n1 minus n2? A:3Q: What is the result of n1 plus n2? A:Q: What is the result of n1 minus n2?3Q: What is the sum of n1 and n2? A:Q: What is the difference between A: n1 and n2? A:5The sum of n1 and n2 isThe difference between n1 and n2 is6n1 + n2 =n1 -n2 =multiplicationdivision1Q: How much is n1 times n2? A:Q: How much is n1 over n2? A:2Q: What is n1 times n2? A:Q: What is n1 over n2? A:3Q: What is the result of n1 times n2? A: Q: What is the result of n1 over n2? A:4Q: What is the product of n1 and n2? A: Q: What the ratio between n1 and n2? A:5The product of n1 and n2 isThe ratio of n1 and n2 is6n1 * n2 =n1 / n2 =FormulaTemplate(n1+n2)<em>n3 Sum n1 and n2 and multiply by n3n1+n2</em>n3What is the sum of n1 and the product of n2 and n3?(n1-n2)<em>n3What is the product of n1 minus n2 and n3?n1/(n2/n3)How much is n1 divided by the ratio between n2 and n3?n1-n2</em>n3</p>
<p>Table 3 :
3
Karpas et al. (2022)s of three-operand queries.For the full list, we refer toKarpas et al. (2022).
ModelOperation Accuracy (%)+69.3−78.0GPT-J×82.8÷40.8Overall67.8+95.5−86.7GPT-J (Numeral Words)×83.3÷59.7Overall81.3+57.4−77.5Pythia 2.8B×64.7÷40.2Overall59.9+100.0−99.8LLaMA×100.0÷88.7Overall97.2+100.0−100.0Goat×91.4÷54.0Overall85.6Pythia 2.8B (3 Operands)Overall0.9Pythia 2.8B Fine-tuned (3 Operands)Overall39.7</p>
<p>Table 4 :
4
Accuracy of the models analyzed in the paper on various types of arithmetic queries.</p>
<p>For brevity, layer normalization (Ba et al., 2016) is omitted as it is not essential for our analysis.
As an alternative metric to quantify the IE, we experiment using the difference in log probabilities (Appendix I). The results obtained with the two metrics show consistency and lead to the same conclusions.
Unless otherwise specified, we use S = {1, 2, . . . , 300}, as larger integers get split into multiple tokens by the tokenizer.
The LLaMA tokenizer considers each digit as an independent token in the vocabulary. This makes it problematic to compare the probability value assigned by the model to multi-digit numbers. Therefore, we restrict the set of possible results to the set of single-digit numbers.
We sample entities from a list containing names of animals, fruits, office tools, and other everyday items and objects.
To have the same result space for all the arithmetic queries (Ar and NW) and for the number retrieval task, we restrict the set S to {1, . . . , 20} (or the corresponding numeral words).
AcknowledgmentsAS is supported by armasuisse Science and Technology through a CYD Doctoral Fellowship.YB is supported by an AI Alignment grant from Open Philanthropy, the Israel Science Foundation (grant No. 448/20), and an Azrieli Foundation Early Career Faculty Fellowship.MS acknowledges support from the Swiss National Science Foundation (Project No. 197155), a Responsible AI grant by the Haslerstiftung, and an ETH Grant (ETH-19 21-1).We are grateful to Vilém Zouhar and Neel Nanda for the insightful discussions.I Log Probability to Quantify the IEIn order to validate whether the measurements of the indirect effect are specific to the metric that we describe in Equation 2, we quantify the IE using the absolute difference in the log of the probability values assigned by the model to the results r and r ′ .More formally, we computewhereThe results are reported in Figure13.The activation sites that we observe are the same as reported in Section 4.1: first-layer MLP at the operand tokens and last-token MLP and attention modules.J Additional Information Flow VisualizationsWe include the IE measurements for the attention modules of GPT-J on the number retrieval task (Figure11) and on the factual knowledge queries (Figure12), and for Pythia 2.8B on three-operand arithmetic queries before and after fine-tuning (Figure15).Additionally, we report the heatmap visualizations of the indirect effect measured for the following models: Pythia 2.8B (Figure16), LLaMA 7B (Figure17), Goat (Figure18), and GPT-J using word numerals (Figure19).Finally, we visualize in Figure14the IE of MLPs and attention modules for the fine-tuned Pythia 2.8B in the fixed-result case.
On the pitfalls of analyzing individual neurons in language models. Omer Antverg, Yonatan Belinkov, International Conference on Learning Representations. 2022</p>
<p>Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E Hinton, arXiv:1607.06450Layer normalization. 2016arXiv preprint</p>
<p>Eliciting latent predictions from transformers with the tuned lens. Nora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor Ostrovsky, Lev Mckinney, Stella Biderman, Jacob Steinhardt, arXiv:2303.081122023arXiv preprint</p>
<p>Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, O' Kyle, Eric Brien, Mohammad Hallahan, Shivanshu Aflah Khan, Purohit, Sai Usvsn, Edward Prashanth, Raff, arXiv:2304.01373Pythia: A suite for analyzing large language models across training and scaling. 2023arXiv preprint</p>
<p>Language models can explain neurons in language models. Steven Bills, Nick Cammarata, Dan Mossing, Henk Tillman, Leo Gao, Gabriel Goh, Ilya Sutskever, Jan Leike, Jeff Wu, William Saunders, 2023</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023arXiv preprint</p>
<p>Discovering latent knowledge in language models without supervision. Collin Burns, Haotian Ye, Dan Klein, Jacob Steinhardt, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, arXiv:2204.02311Palm: Scaling language modeling with pathways. 2022arXiv preprint</p>
<p>. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, 2021Training verifiers to solve math word problems</p>
<p>Alexander Yom Din, Taelin Karidi, Leshem Choshen, Mor Geva, arXiv:2303.09435Jump to conclusions: Short-Cutting transformers with linear transformations. 2023arXiv preprint</p>
<p>. Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova Dassarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlishand Chris Olah. 2021. A mathematical framework for transformer circuits. Transformer Circuits Thread</p>
<p>Causal analysis of syntactic agreement mechanisms in neural language models. Matthew Finlayson, Aaron Mueller, Sebastian Gehrmann, Stuart Shieber, Tal Linzen, Yonatan Belinkov, 10.18653/v1/2021.acl-long.144Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline. Association for Computational Linguistics20211</p>
<p>Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, arXiv:2101.00027The pile: An 800gb dataset of diverse text for language modeling. 2020arXiv preprint</p>
<p>Causal abstractions of neural networks. Atticus Geiger, Hanson Lu, Thomas Icard, Christopher Potts, Advances in Neural Information Processing Systems. 202134</p>
<p>Dissecting recall of factual associations in auto-regressive language models. Mor Geva, Jasmijn Bastings, Katja Filippova, Amir Globerson, arXiv:2304.147672023arXiv preprint</p>
<p>Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space. Mor Geva, Avi Caciularu, Kevin Wang, Yoav Goldberg, 10.18653/v1/2022.emnlp-main.3Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Transformer feed-forward layers are keyvalue memories. Mor Geva, Roei Schuster, Jonathan Berant, Omer Levy, 10.18653/v1/2021.emnlp-main.446Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican Republic. Association for Computational Linguistics2021Online and Punta Cana</p>
<p>Alec Radford, and Chris Olah. 2021. Multimodal neurons in artificial neural networks. Gabriel Goh, Nick Cammarata, Chelsea Voss, Shan Carter, Michael Petrov, Ludwig Schubert, Distill. 63e30</p>
<p>Finding neurons in a haystack: Case studies with sparse probing. Wes Gurnee, Neel Nanda, Matthew Pauly, Katherine Harvey, Dmitrii Troitskii, Dimitris Bertsimas, arXiv:2305.016102023arXiv preprint</p>
<p>MRKL systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning. Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofit Bata, Yoav Levine, Kevin Leyton-Brown, arXiv:2205.004452022arXiv preprint</p>
<p>Andrej Karpathy, Justin Johnson, Li Fei-Fei, arXiv:1506.02078Visualizing and understanding recurrent networks. 2015arXiv preprint</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, ( Shixiang, Machel Shane) Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in Neural Information Processing Systems. Curran Associates, Inc202235</p>
<p>Solving quantitative reasoning problems with language models. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag. Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra2022</p>
<p>Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, Martin Wattenberg, Inference-time intervention: Eliciting truthful answers from a language model. 2023</p>
<p>Tiedong Liu, Bryan Kian, Hsiang Low, arXiv:2305.14201Goat: Fine-tuned LLaMA outperforms GPT-4 on arithmetic tasks. 2023arXiv preprint</p>
<p>Locating and editing factual associations in GPT. Kevin Meng, David Bau, Alex Andonian, Yonatan Belinkov, Advances in Neural Information Processing Systems. 202235</p>
<p>NumGLUE: A suite of fundamental yet challenging mathematical reasoning tasks. Swaroop Mishra, Arindam Mitra, Neeraj Varshney, Bhavdeep Sachdeva, Peter Clark, Chitta Baral, Ashwin Kalyan, 10.18653/v1/2022.acl-long.246Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20221</p>
<p>Neel Nanda, Lawrence Chan, Tom Liberum, Jess Smith, Jacob Steinhardt, arXiv:2301.05217Progress measures for grokking via mechanistic interpretability. 2023arXiv preprint</p>
<p>Zoom in: An introduction to circuits. Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, Shan Carter, 10.23915/distill.00024.0012020</p>
<p>. Chris Olah, Alexander Mordvintsev, Ludwig Schubert, Feature visualization. Distill. 211e72017</p>
<p>The building blocks of interpretability. Chris Olah, Arvind Satyanarayan, Ian Johnson, Shan Carter, Ludwig Schubert, Katherine Ye, Alexander Mordvintsev, Distill. 33e102018</p>
<p>. Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova Dassarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlishand Chris Olah. 2022. In-context learning and induction heads. Transformer Circuits Thread</p>
<p>GPT-4 technical report. 2023OpenAI</p>
<p>Investigating numeracy learning ability of a text-to-text transfer model. Kuntal Kumar, Pal , Chitta Baral, 10.18653/v1/2021.findings-emnlp.265Findings of the Association for Computational Linguistics: EMNLP 2021. Punta Cana, Dominican RepublicAssociation for Computational Linguistics2021</p>
<p>Direct and indirect effects. Judea Pearl, UAI '01: Proceedings of the 17th Conference in Uncertainty in Artificial Intelligence, University of Washington. Seattle, Washington, USAMorgan Kaufmann2001. August 2-5, 2001</p>
<p>Causality. Judea Pearl, 2009Cambridge University Press</p>
<p>Language models as knowledge bases?. Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander Miller, 10.18653/v1/D19-1250Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Measuring and improving BERT's mathematical abilities by predicting the order of reasoning. Piotr Piękos, Mateusz Malinowski, Henryk Michalewski, 10.18653/v1/2021.acl-short.49Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline. Association for Computational Linguistics20212Short Papers)</p>
<p>Impact of pretraining term frequencies on few-shot numerical reasoning. Yasaman Razeghi, Robert L Logan, I V , Matt Gardner, Sameer Singh, 10.18653/v1/2022.findings-emnlp.59Findings of the Association for Computational Linguistics: EMNLP 2022. Abu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Toward transparent AI: A survey on interpreting the inner structures of deep neural networks. Tilman Räuker, Anson Ho, Stephen Casper, Dylan Hadfield-Menell, 2023</p>
<p>Adafactor: Adaptive learning rates with sublinear memory cost. Noam Shazeer, Mitchell Stern, International Conference on Machine Learning. PMLR2018</p>
<p>Masked measurement prediction: Learning to jointly predict quantities and units from textual context. Daniel Spokoyny, Ivan Lee, Jin Zhao, Taylor Berg-Kirkpatrick, 10.18653/v1/2022.findings-naacl.2Findings of the Association for Computational Linguistics: NAACL 2022. Seattle, United StatesAssociation for Computational Linguistics2022</p>
<p>A causal framework to quantify the robustness of mathematical reasoning with language models. Alessandro Stolfo, Zhijing Jin, Kumar Shridhar, Bernhard Schoelkopf, Mrinmaya Sachan, 10.18653/v1/2023.acl-long.32Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231Long Papers)</p>
<p>Roformer: Enhanced transformer with rotary position embedding. Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, Yunfeng Liu, 2022</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Azhar, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, Advances in neural information processing systems. 201730</p>
<p>Investigating gender bias in language models using causal mediation analysis. Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer, Stuart Shieber, Advances in Neural Information Processing Systems. 202033</p>
<p>Chelsea Voss, Nick Cammarata, Gabriel Goh, Michael Petrov, Ludwig Schubert, Ben Egan, Swee Kiat Lim, and Chris Olah. 2021. Visualizing weights. 6</p>
<p>Ben Wang, Aran Komatsuzaki, GPT-J-6B: A 6 billion parameter autoregressive language model. 2021</p>
<p>Interpretability in the wild: A circuit for indirect object identification in GPT-2 small. Kevin Ro, Wang , Alexandre Variengien, Arthur Conmy, Buck Shlegeris, Jacob Steinhardt, NeurIPS ML Safety Workshop. 2022</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, arXiv:2206.07682Emergent abilities of large language models. 2022aarXiv preprint</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H Chi, Quoc Le, Denny Zhou, CoRR, abs/2201.119032022b</p>
<p>Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Denny Quoc V Le, Xinyun Zhou, Chen, arXiv:2309.03409Large language models as optimizers. 2023arXiv preprint</p>
<p>We sample pairs of subject entities from the set of entities compatible for a given relation (e.g., cities for the relation "is the capital of"). For each relation. we sample 100 pairs of subject entities</p>            </div>
        </div>

    </div>
</body>
</html>